[Zeus Master] Job(cifar100,shufflenetv2,adadelta,0.8,bs128~100) x 100
[Zeus Master] Batch sizes: [128]
[Zeus Master] Learning rates: [0.01, 0.1]
[run job] Launching job with BS 128: and LR: 0.01
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835', 'ZEUS_JOB_ID': 'rec00+try01', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_TARGET_METRIC': '0.8', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_USE_OPTIMAL_PL': 'True'}
[run job] cwd=/workspace/zeus/examples/cifar100
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '0.01']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/rec00+try01.train.log'
2022-12-06 03:37:19,582 [ZeusDataLoader(train)] Distributed data parallel: OFF
2022-12-06 03:37:19,623 [ZeusDataLoader(train)] Power limit range: [175000, 150000, 125000, 100000]
2022-12-06 03:37:19,624 [ZeusDataLoader(train)] Power profiling: ON
2022-12-06 03:37:22,214 [ZeusDataLoader(train)] Up to epoch 0: time=0.00, energy=0.00, cost=0.00
2022-12-06 03:37:22,215 [ZeusDataLoader(train)] Epoch 1 begin.
Files already downloaded and verified
Files already downloaded and verified
2022-12-06 03:37:22,383 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:37:22.398 [ZeusMonitor] Monitor started.
2022-12-06 08:37:22.398 [ZeusMonitor] Running indefinitely. 2022-12-06 08:37:22.398 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 08:37:22.398 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e1+gpu0.power.log
2022-12-06 03:37:22,452 [ZeusDataLoader(train)] [GPU_0] Set GPU power limit to 175W.
2022-12-06 03:37:22,452 [ZeusDataLoader(train)] Warm-up started with power limit 175W
2022-12-06 03:37:25,371 [ZeusDataLoader(train)] Profile started with power limit 175W
2022-12-06 03:37:34,204 [ZeusDataLoader(train)] Profile done with power limit 175W
2022-12-06 03:37:34,426 [ZeusDataLoader(train)] [GPU_0] Set GPU power limit to 150W.
2022-12-06 03:37:34,426 [ZeusDataLoader(train)] Warm-up started with power limit 150W
2022-12-06 03:37:36,729 [ZeusDataLoader(train)] Profile started with power limit 150W
2022-12-06 03:37:45,552 [ZeusDataLoader(train)] Profile done with power limit 150W
2022-12-06 03:37:45,775 [ZeusDataLoader(train)] [GPU_0] Set GPU power limit to 125W.
2022-12-06 03:37:45,775 [ZeusDataLoader(train)] Warm-up started with power limit 125W
2022-12-06 03:37:48,097 [ZeusDataLoader(train)] Profile started with power limit 125W
2022-12-06 03:37:56,923 [ZeusDataLoader(train)] Profile done with power limit 125W
2022-12-06 03:37:57,149 [ZeusDataLoader(train)] [GPU_0] Set GPU power limit to 100W.
2022-12-06 03:37:57,149 [ZeusDataLoader(train)] Warm-up started with power limit 100W
2022-12-06 03:37:59,768 [ZeusDataLoader(train)] Profile started with power limit 100W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 4.8435
Training Epoch: 0 [384/50048]	Loss: 4.9288
Training Epoch: 0 [512/50048]	Loss: 4.7388
Training Epoch: 0 [640/50048]	Loss: 4.9147
Training Epoch: 0 [768/50048]	Loss: 4.8302
Training Epoch: 0 [896/50048]	Loss: 4.8936
Training Epoch: 0 [1024/50048]	Loss: 4.7801
Training Epoch: 0 [1152/50048]	Loss: 4.8947
Training Epoch: 0 [1280/50048]	Loss: 4.8014
Training Epoch: 0 [1408/50048]	Loss: 4.7598
Training Epoch: 0 [1536/50048]	Loss: 4.6527
Training Epoch: 0 [1664/50048]	Loss: 4.9387
Training Epoch: 0 [1792/50048]	Loss: 4.6624
Training Epoch: 0 [1920/50048]	Loss: 4.8130
Training Epoch: 0 [2048/50048]	Loss: 4.6574
Training Epoch: 0 [2176/50048]	Loss: 4.9271
Training Epoch: 0 [2304/50048]	Loss: 4.8007
Training Epoch: 0 [2432/50048]	Loss: 4.6673
Training Epoch: 0 [2560/50048]	Loss: 4.6633
Training Epoch: 0 [2688/50048]	Loss: 4.6848
Training Epoch: 0 [2816/50048]	Loss: 4.6425
Training Epoch: 0 [2944/50048]	Loss: 4.6134
Training Epoch: 0 [3072/50048]	Loss: 4.6258
Training Epoch: 0 [3200/50048]	Loss: 4.7523
Training Epoch: 0 [3328/50048]	Loss: 4.6538
Training Epoch: 0 [3456/50048]	Loss: 4.4921
Training Epoch: 0 [3584/50048]	Loss: 4.5436
Training Epoch: 0 [3712/50048]	Loss: 4.6053
Training Epoch: 0 [3840/50048]	Loss: 4.5095
Training Epoch: 0 [3968/50048]	Loss: 4.6131
Training Epoch: 0 [4096/50048]	Loss: 4.6529
Training Epoch: 0 [4224/50048]	Loss: 4.6396
Training Epoch: 0 [4352/50048]	Loss: 4.6302
Training Epoch: 0 [4480/50048]	Loss: 4.5185
Training Epoch: 0 [4608/50048]	Loss: 4.4671
Training Epoch: 0 [4736/50048]	Loss: 4.4941
Training Epoch: 0 [4864/50048]	Loss: 4.4424
Training Epoch: 0 [4992/50048]	Loss: 4.5448
Training Epoch: 0 [5120/50048]	Loss: 4.5122
Training Epoch: 0 [5248/50048]	Loss: 4.4714
Training Epoch: 0 [5376/50048]	Loss: 4.3930
Training Epoch: 0 [5504/50048]	Loss: 4.4211
Training Epoch: 0 [5632/50048]	Loss: 4.4188
Training Epoch: 0 [5760/50048]	Loss: 4.5209
Training Epoch: 0 [5888/50048]	Loss: 4.4019
Training Epoch: 0 [6016/50048]	Loss: 4.2172
Training Epoch: 0 [6144/50048]	Loss: 4.1616
Training Epoch: 0 [6272/50048]	Loss: 4.6102
Training Epoch: 0 [6400/50048]	Loss: 4.2738
Training Epoch: 0 [6528/50048]	Loss: 4.2269
Training Epoch: 0 [6656/50048]	Loss: 4.4696
Training Epoch: 0 [6784/50048]	Loss: 4.3207
Training Epoch: 0 [6912/50048]	Loss: 4.4155
Training Epoch: 0 [7040/50048]	Loss: 4.2409
Training Epoch: 0 [7168/50048]	Loss: 4.3105
Training Epoch: 0 [7296/50048]	Loss: 4.4554
Training Epoch: 0 [7424/50048]	Loss: 4.3159
Training Epoch: 0 [7552/50048]	Loss: 4.2177
Training Epoch: 0 [7680/50048]	Loss: 4.3220
Training Epoch: 0 [7808/50048]	Loss: 4.3650
Training Epoch: 0 [7936/50048]	Loss: 4.3066
Training Epoch: 0 [8064/50048]	Loss: 4.1930
Training Epoch: 0 [8192/50048]	Loss: 4.2341
Training Epoch: 0 [8320/50048]	Loss: 4.3697
Training Epoch: 0 [8448/50048]	Loss: 4.2056
Training Epoch: 0 [8576/50048]	Loss: 4.0952
Training Epoch: 0 [8704/50048]	Loss: 4.2379
Training Epoch: 0 [8832/50048]	Loss: 4.1552
Training Epoch: 0 [8960/50048]	Loss: 4.2699
Training Epoch: 0 [9088/50048]	Loss: 4.1984
Training Epoch: 0 [9216/50048]	Loss: 4.3054
Training Epoch: 0 [9344/50048]	Loss: 4.1669
Training Epoch: 0 [9472/50048]	Loss: 4.2059
Training Epoch: 0 [9600/50048]	Loss: 4.1632
Training Epoch: 0 [9728/50048]	Loss: 4.2692
Training Epoch: 0 [9856/50048]	Loss: 4.1813
Training Epoch: 0 [9984/50048]	Loss: 4.1919
Training Epoch: 0 [10112/50048]	Loss: 4.2036
Training Epoch: 0 [10240/50048]	Loss: 4.3449
Training Epoch: 0 [10368/50048]	Loss: 4.2525
Training Epoch: 0 [10496/50048]	Loss: 4.1559
Training Epoch: 0 [10624/50048]	Loss: 4.1621
Training Epoch: 0 [10752/50048]	Loss: 4.0623
Training Epoch: 0 [10880/50048]	Loss: 4.1906
Training Epoch: 0 [11008/50048]	Loss: 3.9873
Training Epoch: 0 [11136/50048]	Loss: 4.2051
Training Epoch: 0 [11264/50048]	Loss: 4.2709
Training Epoch: 0 [11392/50048]	Loss: 4.1644
Training Epoch: 0 [11520/50048]	Loss: 4.0619
Training Epoch: 0 [11648/50048]	Loss: 4.1670
Training Epoch: 0 [11776/50048]	Loss: 4.0203
Training Epoch: 0 [11904/50048]	Loss: 4.1485
Training Epoch: 0 [12032/50048]	Loss: 4.1332
Training Epoch: 0 [12160/50048]	Loss: 4.1971
Training Epoch: 0 [12288/50048]	Loss: 3.9671
Training Epoch: 0 [12416/50048]	Loss: 4.0972
Training Epoch: 0 [12544/50048]	Loss: 3.9831
Training Epoch: 0 [12672/50048]	Loss: 4.1126
Training Epoch: 0 [12800/50048]	Loss: 4.0302
Training Epoch: 0 [12928/50048]	Loss: 3.9101
Training Epoch: 0 [13056/50048]	Loss: 4.0283
Training Epoch: 0 [13184/50048]	Loss: 4.1833
Training Epoch: 0 [13312/50048]	Loss: 4.1190
Training Epoch: 0 [13440/50048]	Loss: 4.1729
Training Epoch: 0 [13568/50048]	Loss: 4.1557
Training Epoch: 0 [13696/50048]	Loss: 3.7941
Training Epoch: 0 [13824/50048]	Loss: 4.1144
Training Epoch: 0 [13952/50048]	Loss: 4.0506
Training Epoch: 0 [14080/50048]	Loss: 3.9215
Training Epoch: 0 [14208/50048]	Loss: 4.2000
Training Epoch: 0 [14336/50048]	Loss: 4.0980
Training Epoch: 0 [14464/50048]	Loss: 4.0295
Training Epoch: 0 [14592/50048]	Loss: 4.0596
Training Epoch: 0 [14720/50048]	Loss: 4.0017
Training Epoch: 0 [14848/50048]	Loss: 3.8204
Training Epoch: 0 [14976/50048]	Loss: 4.0849
Training Epoch: 0 [15104/50048]	Loss: 3.9978
Training Epoch: 0 [15232/50048]	Loss: 4.1194
Training Epoch: 0 [15360/50048]	Loss: 4.1990
Training Epoch: 0 [15488/50048]	Loss: 3.9731
Training Epoch: 0 [15616/50048]	Loss: 4.0834
Training Epoch: 0 [15744/50048]	Loss: 4.1262
Training Epoch: 0 [15872/50048]	Loss: 4.2181
Training Epoch: 0 [16000/50048]	Loss: 4.0509
Training Epoch: 0 [16128/50048]	Loss: 3.9741
Training Epoch: 0 [16256/50048]	Loss: 3.9875
Training Epoch: 0 [16384/50048]	Loss: 4.0326
Training Epoch: 0 [16512/50048]	Loss: 4.0952
Training Epoch: 0 [16640/50048]	Loss: 3.9216
Training Epoch: 0 [16768/50048]	Loss: 3.7876
Training Epoch: 0 [16896/50048]	Loss: 4.1014
Training Epoch: 0 [17024/50048]	Loss: 3.9089
Training Epoch: 0 [17152/50048]	Loss: 3.8983
Training Epoch: 0 [17280/50048]	Loss: 4.2146
Training Epoch: 0 [17408/50048]	Loss: 4.0153
Training Epoch: 0 [17536/50048]	Loss: 4.0004
Training Epoch: 0 [17664/50048]	Loss: 3.9106
Training Epoch: 0 [17792/50048]	Loss: 4.0456
Training Epoch: 0 [17920/50048]	Loss: 4.1351
Training Epoch: 0 [18048/50048]	Loss: 3.9395
Training Epoch: 0 [18176/50048]	Loss: 3.8865
Training Epoch: 0 [18304/50048]	Loss: 4.1854
Training Epoch: 0 [18432/50048]	Loss: 4.0411
Training Epoch: 0 [18560/50048]	Loss: 3.8461
Training Epoch: 0 [18688/50048]	Loss: 3.9325
Training Epoch: 0 [18816/50048]	Loss: 3.9839
Training Epoch: 0 [18944/50048]	Loss: 3.9057
Training Epoch: 0 [19072/50048]	Loss: 4.1382
Training Epoch: 0 [19200/50048]	Loss: 3.9787
Training Epoch: 0 [19328/50048]	Loss: 3.8871
Training Epoch: 0 [19456/50048]	Loss: 3.8124
Training Epoch: 0 [19584/50048]	Loss: 3.9326
Training Epoch: 0 [19712/50048]	Loss: 3.9366
Training Epoch: 0 [19840/50048]	Loss: 3.9943
Training Epoch: 0 [19968/50048]	Loss: 4.1104
Training Epoch: 0 [20096/50048]	Loss: 3.8829
Training Epoch: 0 [20224/50048]	Loss: 3.8983
Training Epoch: 0 [20352/50048]	Loss: 3.8229
Training Epoch: 0 [20480/50048]	Loss: 4.0611
Training Epoch: 0 [20608/50048]	Loss: 3.8127
Training Epoch: 0 [20736/50048]	Loss: 3.9059
Training Epoch: 0 [20864/50048]	Loss: 3.8566
Training Epoch: 0 [20992/50048]	Loss: 3.8854
Training Epoch: 0 [21120/50048]	Loss: 4.0797
Training Epoch: 0 [21248/50048]	Loss: 4.1650
Training Epoch: 0 [21376/50048]	Loss: 4.0222
Training Epoch: 0 [21504/50048]	Loss: 3.8271
Training Epoch: 0 [21632/50048]	Loss: 3.8602
Training Epoch: 0 [21760/50048]	Loss: 3.9180
Training Epoch: 0 [21888/50048]	Loss: 3.9109
Training Epoch: 0 [22016/50048]	Loss: 3.9981
Training Epoch: 0 [22144/50048]	Loss: 3.9956
Training Epoch: 0 [22272/50048]	Loss: 3.7504
Training Epoch: 0 [22400/50048]	Loss: 3.9970
Training Epoch: 0 [22528/50048]	Loss: 3.9603
Training Epoch: 0 [22656/50048]	Loss: 3.7607
Training Epoch: 0 [22784/50048]	Loss: 3.7711
Training Epoch: 0 [22912/50048]	Loss: 3.7897
Training Epoch: 0 [23040/50048]	Loss: 3.8245
Training Epoch: 0 [23168/50048]	Loss: 3.9410
Training Epoch: 0 [23296/50048]	Loss: 3.8535
Training Epoch: 0 [23424/50048]	Loss: 3.9406
2022-12-06 03:38:09,917 [ZeusDataLoader(train)] Profile done with power limit 100W
2022-12-06 03:38:09,917 [ZeusDataLoader(train)] This was the last power limit to explore.
2022-12-06 03:38:09,917 [ZeusDataLoader(train)] Cost-optimal power limit is 175W
2022-12-06 03:38:09,923 [ZeusDataLoader(train)] [GPU_0] Set GPU power limit to 175W.
Training Epoch: 0 [23552/50048]	Loss: 3.8643
Training Epoch: 0 [23680/50048]	Loss: 3.9109
Training Epoch: 0 [23808/50048]	Loss: 3.8139
Training Epoch: 0 [23936/50048]	Loss: 3.9618
Training Epoch: 0 [24064/50048]	Loss: 3.9664
Training Epoch: 0 [24192/50048]	Loss: 3.7696
Training Epoch: 0 [24320/50048]	Loss: 3.8760
Training Epoch: 0 [24448/50048]	Loss: 3.7611
Training Epoch: 0 [24576/50048]	Loss: 3.9871
Training Epoch: 0 [24704/50048]	Loss: 3.9421
Training Epoch: 0 [24832/50048]	Loss: 4.0361
Training Epoch: 0 [24960/50048]	Loss: 3.9046
Training Epoch: 0 [25088/50048]	Loss: 3.7889
Training Epoch: 0 [25216/50048]	Loss: 3.8981
Training Epoch: 0 [25344/50048]	Loss: 3.8466
Training Epoch: 0 [25472/50048]	Loss: 3.8063
Training Epoch: 0 [25600/50048]	Loss: 3.9701
Training Epoch: 0 [25728/50048]	Loss: 3.8353
Training Epoch: 0 [25856/50048]	Loss: 3.8149
Training Epoch: 0 [25984/50048]	Loss: 3.7688
Training Epoch: 0 [26112/50048]	Loss: 3.7983
Training Epoch: 0 [26240/50048]	Loss: 4.0749
Training Epoch: 0 [26368/50048]	Loss: 3.8945
Training Epoch: 0 [26496/50048]	Loss: 3.8358
Training Epoch: 0 [26624/50048]	Loss: 3.8261
Training Epoch: 0 [26752/50048]	Loss: 4.0062
Training Epoch: 0 [26880/50048]	Loss: 3.7935
Training Epoch: 0 [27008/50048]	Loss: 3.7633
Training Epoch: 0 [27136/50048]	Loss: 3.7113
Training Epoch: 0 [27264/50048]	Loss: 3.8684
Training Epoch: 0 [27392/50048]	Loss: 3.9926
Training Epoch: 0 [27520/50048]	Loss: 3.8484
Training Epoch: 0 [27648/50048]	Loss: 3.7646
Training Epoch: 0 [27776/50048]	Loss: 3.8427
Training Epoch: 0 [27904/50048]	Loss: 3.9871
Training Epoch: 0 [28032/50048]	Loss: 3.7975
Training Epoch: 0 [28160/50048]	Loss: 3.8799
Training Epoch: 0 [28288/50048]	Loss: 3.7153
Training Epoch: 0 [28416/50048]	Loss: 3.6557
Training Epoch: 0 [28544/50048]	Loss: 3.7592
Training Epoch: 0 [28672/50048]	Loss: 3.7656
Training Epoch: 0 [28800/50048]	Loss: 3.8657
Training Epoch: 0 [28928/50048]	Loss: 3.8071
Training Epoch: 0 [29056/50048]	Loss: 3.9118
Training Epoch: 0 [29184/50048]	Loss: 3.9093
Training Epoch: 0 [29312/50048]	Loss: 3.6686
Training Epoch: 0 [29440/50048]	Loss: 3.8824
Training Epoch: 0 [29568/50048]	Loss: 3.8845
Training Epoch: 0 [29696/50048]	Loss: 3.8164
Training Epoch: 0 [29824/50048]	Loss: 3.8233
Training Epoch: 0 [29952/50048]	Loss: 3.8822
Training Epoch: 0 [30080/50048]	Loss: 3.8976
Training Epoch: 0 [30208/50048]	Loss: 4.0729
Training Epoch: 0 [30336/50048]	Loss: 3.9108
Training Epoch: 0 [30464/50048]	Loss: 3.9715
Training Epoch: 0 [30592/50048]	Loss: 3.7589
Training Epoch: 0 [30720/50048]	Loss: 3.7741
Training Epoch: 0 [30848/50048]	Loss: 3.9444
Training Epoch: 0 [30976/50048]	Loss: 3.6622
Training Epoch: 0 [31104/50048]	Loss: 3.8587
Training Epoch: 0 [31232/50048]	Loss: 3.8977
Training Epoch: 0 [31360/50048]	Loss: 3.7491
Training Epoch: 0 [31488/50048]	Loss: 3.9758
Training Epoch: 0 [31616/50048]	Loss: 3.8842
Training Epoch: 0 [31744/50048]	Loss: 3.8734
Training Epoch: 0 [31872/50048]	Loss: 3.8206
Training Epoch: 0 [32000/50048]	Loss: 3.9437
Training Epoch: 0 [32128/50048]	Loss: 3.5359
Training Epoch: 0 [32256/50048]	Loss: 3.8026
Training Epoch: 0 [32384/50048]	Loss: 3.6316
Training Epoch: 0 [32512/50048]	Loss: 3.5308
Training Epoch: 0 [32640/50048]	Loss: 3.8235
Training Epoch: 0 [32768/50048]	Loss: 3.7212
Training Epoch: 0 [32896/50048]	Loss: 3.7927
Training Epoch: 0 [33024/50048]	Loss: 3.7380
Training Epoch: 0 [33152/50048]	Loss: 3.9095
Training Epoch: 0 [33280/50048]	Loss: 3.6673
Training Epoch: 0 [33408/50048]	Loss: 3.7757
Training Epoch: 0 [33536/50048]	Loss: 3.9509
Training Epoch: 0 [33664/50048]	Loss: 3.6855
Training Epoch: 0 [33792/50048]	Loss: 3.5812
Training Epoch: 0 [33920/50048]	Loss: 3.7013
Training Epoch: 0 [34048/50048]	Loss: 3.8390
Training Epoch: 0 [34176/50048]	Loss: 3.8159
Training Epoch: 0 [34304/50048]	Loss: 3.6672
Training Epoch: 0 [34432/50048]	Loss: 3.8274
Training Epoch: 0 [34560/50048]	Loss: 3.8250
Training Epoch: 0 [34688/50048]	Loss: 3.8810
Training Epoch: 0 [34816/50048]	Loss: 3.8944
Training Epoch: 0 [34944/50048]	Loss: 3.7602
Training Epoch: 0 [35072/50048]	Loss: 3.6946
Training Epoch: 0 [35200/50048]	Loss: 3.6020
Training Epoch: 0 [35328/50048]	Loss: 3.7586
Training Epoch: 0 [35456/50048]	Loss: 3.6909
Training Epoch: 0 [35584/50048]	Loss: 3.7577
Training Epoch: 0 [35712/50048]	Loss: 3.7129
Training Epoch: 0 [35840/50048]	Loss: 3.7869
Training Epoch: 0 [35968/50048]	Loss: 3.7794
Training Epoch: 0 [36096/50048]	Loss: 3.6503
Training Epoch: 0 [36224/50048]	Loss: 3.6310
Training Epoch: 0 [36352/50048]	Loss: 3.5848
Training Epoch: 0 [36480/50048]	Loss: 3.6819
Training Epoch: 0 [36608/50048]	Loss: 3.9046
Training Epoch: 0 [36736/50048]	Loss: 3.7390
Training Epoch: 0 [36864/50048]	Loss: 3.6438
Training Epoch: 0 [36992/50048]	Loss: 3.5703
Training Epoch: 0 [37120/50048]	Loss: 3.7700
Training Epoch: 0 [37248/50048]	Loss: 3.4926
Training Epoch: 0 [37376/50048]	Loss: 3.7522
Training Epoch: 0 [37504/50048]	Loss: 3.7952
Training Epoch: 0 [37632/50048]	Loss: 3.7496
Training Epoch: 0 [37760/50048]	Loss: 3.5659
Training Epoch: 0 [37888/50048]	Loss: 4.0376
Training Epoch: 0 [38016/50048]	Loss: 3.6980
Training Epoch: 0 [38144/50048]	Loss: 3.7362
Training Epoch: 0 [38272/50048]	Loss: 3.8462
Training Epoch: 0 [38400/50048]	Loss: 3.7081
Training Epoch: 0 [38528/50048]	Loss: 3.7997
Training Epoch: 0 [38656/50048]	Loss: 3.5574
Training Epoch: 0 [38784/50048]	Loss: 3.5492
Training Epoch: 0 [38912/50048]	Loss: 3.6697
Training Epoch: 0 [39040/50048]	Loss: 3.7287
Training Epoch: 0 [39168/50048]	Loss: 3.8738
Training Epoch: 0 [39296/50048]	Loss: 3.6982
Training Epoch: 0 [39424/50048]	Loss: 3.6193
Training Epoch: 0 [39552/50048]	Loss: 3.4170
Training Epoch: 0 [39680/50048]	Loss: 3.6245
Training Epoch: 0 [39808/50048]	Loss: 3.6168
Training Epoch: 0 [39936/50048]	Loss: 3.8573
Training Epoch: 0 [40064/50048]	Loss: 3.5993
Training Epoch: 0 [40192/50048]	Loss: 3.5882
Training Epoch: 0 [40320/50048]	Loss: 3.6096
Training Epoch: 0 [40448/50048]	Loss: 3.6836
Training Epoch: 0 [40576/50048]	Loss: 3.6570
Training Epoch: 0 [40704/50048]	Loss: 3.6165
Training Epoch: 0 [40832/50048]	Loss: 3.6212
Training Epoch: 0 [40960/50048]	Loss: 3.7581
Training Epoch: 0 [41088/50048]	Loss: 3.6348
Training Epoch: 0 [41216/50048]	Loss: 3.5273
Training Epoch: 0 [41344/50048]	Loss: 3.8449
Training Epoch: 0 [41472/50048]	Loss: 3.6414
Training Epoch: 0 [41600/50048]	Loss: 3.4935
Training Epoch: 0 [41728/50048]	Loss: 3.5770
Training Epoch: 0 [41856/50048]	Loss: 3.7009
Training Epoch: 0 [41984/50048]	Loss: 3.7140
Training Epoch: 0 [42112/50048]	Loss: 3.7106
Training Epoch: 0 [42240/50048]	Loss: 3.7336
Training Epoch: 0 [42368/50048]	Loss: 3.6270
Training Epoch: 0 [42496/50048]	Loss: 3.9793
Training Epoch: 0 [42624/50048]	Loss: 3.7786
Training Epoch: 0 [42752/50048]	Loss: 3.7118
Training Epoch: 0 [42880/50048]	Loss: 3.7870
Training Epoch: 0 [43008/50048]	Loss: 3.5945
Training Epoch: 0 [43136/50048]	Loss: 3.5300
Training Epoch: 0 [43264/50048]	Loss: 3.6822
Training Epoch: 0 [43392/50048]	Loss: 3.5669
Training Epoch: 0 [43520/50048]	Loss: 3.7098
Training Epoch: 0 [43648/50048]	Loss: 3.4017
Training Epoch: 0 [43776/50048]	Loss: 3.4242
Training Epoch: 0 [43904/50048]	Loss: 3.6905
Training Epoch: 0 [44032/50048]	Loss: 3.5728
Training Epoch: 0 [44160/50048]	Loss: 3.6371
Training Epoch: 0 [44288/50048]	Loss: 3.5776
Training Epoch: 0 [44416/50048]	Loss: 3.6444
Training Epoch: 0 [44544/50048]	Loss: 3.5272
Training Epoch: 0 [44672/50048]	Loss: 3.8313
Training Epoch: 0 [44800/50048]	Loss: 3.5127
Training Epoch: 0 [44928/50048]	Loss: 3.5348
Training Epoch: 0 [45056/50048]	Loss: 3.5642
Training Epoch: 0 [45184/50048]	Loss: 3.9738
Training Epoch: 0 [45312/50048]	Loss: 3.7332
Training Epoch: 0 [45440/50048]	Loss: 3.4813
Training Epoch: 0 [45568/50048]	Loss: 3.5173
Training Epoch: 0 [45696/50048]	Loss: 3.4757
Training Epoch: 0 [45824/50048]	Loss: 3.8533
Training Epoch: 0 [45952/50048]	Loss: 3.5821
Training Epoch: 0 [46080/50048]	Loss: 3.6292
Training Epoch: 0 [46208/50048]	Loss: 3.6048
Training Epoch: 0 [46336/50048]	Loss: 3.5592
Training Epoch: 0 [46464/50048]	Loss: 3.4717
Training Epoch: 0 [46592/50048]	Loss: 3.6937
Training Epoch: 0 [46720/50048]	Loss: 3.6116
2022-12-06 03:38:51,397 [ZeusDataLoader(train)] train epoch 1 done: time=89.17 energy=10350.75
2022-12-06 03:38:51,398 [ZeusDataLoader(eval)] Epoch 1 begin.
Training Epoch: 0 [46848/50048]	Loss: 3.6880
Training Epoch: 0 [46976/50048]	Loss: 3.8422
Training Epoch: 0 [47104/50048]	Loss: 3.6543
Training Epoch: 0 [47232/50048]	Loss: 3.4804
Training Epoch: 0 [47360/50048]	Loss: 3.3983
Training Epoch: 0 [47488/50048]	Loss: 3.4912
Training Epoch: 0 [47616/50048]	Loss: 3.5999
Training Epoch: 0 [47744/50048]	Loss: 3.8371
Training Epoch: 0 [47872/50048]	Loss: 3.5818
Training Epoch: 0 [48000/50048]	Loss: 3.6314
Training Epoch: 0 [48128/50048]	Loss: 3.8301
Training Epoch: 0 [48256/50048]	Loss: 3.5323
Training Epoch: 0 [48384/50048]	Loss: 3.6270
Training Epoch: 0 [48512/50048]	Loss: 3.6664
Training Epoch: 0 [48640/50048]	Loss: 3.5828
Training Epoch: 0 [48768/50048]	Loss: 3.7472
Training Epoch: 0 [48896/50048]	Loss: 3.4922
Training Epoch: 0 [49024/50048]	Loss: 3.5861
Training Epoch: 0 [49152/50048]	Loss: 3.6141
Training Epoch: 0 [49280/50048]	Loss: 3.4746
Training Epoch: 0 [49408/50048]	Loss: 3.2971
Training Epoch: 0 [49536/50048]	Loss: 3.6190
Training Epoch: 0 [49664/50048]	Loss: 3.4953
Training Epoch: 0 [49792/50048]	Loss: 3.4885
Training Epoch: 0 [49920/50048]	Loss: 3.6251
Training Epoch: 0 [50048/50048]	Loss: 3.6901
2022-12-06 08:38:55.133 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 03:38:55,170 [ZeusDataLoader(eval)] Power profiling done.
2022-12-06 03:38:55,170 [ZeusDataLoader(eval)] Saved /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+lr0.0100000.power.json: {"job_id": "rec00+try01", "train_power": {"175000": 120.25616099179643, "150000": 120.91003189146555, "125000": 121.11901311576284, "100000": 98.69303482927677}, "train_throughput": {"175000": 4.544606647863609, "150000": 4.5365038969654385, "125000": 4.536710151601713, "100000": 3.9448251884904706}, "eval_power": {"175000": 120.03848402041383}, "eval_throughput": {"175000": 20.996480248389204}, "optimal_pl": 175000}
2022-12-06 03:38:55,170 [ZeusDataLoader(eval)] eval epoch 1 done: time=3.76 energy=451.65
2022-12-06 03:38:55,171 [ZeusDataLoader(train)] Up to epoch 1: time=92.94, energy=10802.39, cost=13533.09
2022-12-06 03:38:55,171 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 03:38:55,171 [ZeusDataLoader(train)] Expected next epoch: time=182.73, energy=21600.41, cost=26789.47
2022-12-06 03:38:55,172 [ZeusDataLoader(train)] Epoch 2 begin.
Validation Epoch: 0, Average loss: 0.0296, Accuracy: 0.1244
2022-12-06 03:38:55,355 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 03:38:55,355 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 08:38:55.359 [ZeusMonitor] Monitor started.
2022-12-06 08:38:55.359 [ZeusMonitor] Running indefinitely. 2022-12-06 08:38:55.359 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 08:38:55.359 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e2+gpu0.power.log
Training Epoch: 1 [128/50048]	Loss: 3.6274
Training Epoch: 1 [256/50048]	Loss: 3.5683
Training Epoch: 1 [384/50048]	Loss: 3.4121
Training Epoch: 1 [512/50048]	Loss: 3.7262
Training Epoch: 1 [640/50048]	Loss: 3.4131
Training Epoch: 1 [768/50048]	Loss: 3.5608
Training Epoch: 1 [896/50048]	Loss: 3.5404
Training Epoch: 1 [1024/50048]	Loss: 3.5555
Training Epoch: 1 [1152/50048]	Loss: 3.4636
Training Epoch: 1 [1280/50048]	Loss: 3.5978
Training Epoch: 1 [1408/50048]	Loss: 3.5713
Training Epoch: 1 [1536/50048]	Loss: 3.7066
Training Epoch: 1 [1664/50048]	Loss: 3.4684
Training Epoch: 1 [1792/50048]	Loss: 3.2665
Training Epoch: 1 [1920/50048]	Loss: 3.5246
Training Epoch: 1 [2048/50048]	Loss: 3.5504
Training Epoch: 1 [2176/50048]	Loss: 3.8406
Training Epoch: 1 [2304/50048]	Loss: 3.5866
Training Epoch: 1 [2432/50048]	Loss: 3.6324
Training Epoch: 1 [2560/50048]	Loss: 3.4259
Training Epoch: 1 [2688/50048]	Loss: 3.5122
Training Epoch: 1 [2816/50048]	Loss: 3.4046
Training Epoch: 1 [2944/50048]	Loss: 3.5822
Training Epoch: 1 [3072/50048]	Loss: 3.4541
Training Epoch: 1 [3200/50048]	Loss: 3.5672
Training Epoch: 1 [3328/50048]	Loss: 3.3778
Training Epoch: 1 [3456/50048]	Loss: 3.4347
Training Epoch: 1 [3584/50048]	Loss: 3.5731
Training Epoch: 1 [3712/50048]	Loss: 3.5257
Training Epoch: 1 [3840/50048]	Loss: 3.5251
Training Epoch: 1 [3968/50048]	Loss: 3.4635
Training Epoch: 1 [4096/50048]	Loss: 3.4653
Training Epoch: 1 [4224/50048]	Loss: 3.7184
Training Epoch: 1 [4352/50048]	Loss: 3.6361
Training Epoch: 1 [4480/50048]	Loss: 3.1860
Training Epoch: 1 [4608/50048]	Loss: 3.4050
Training Epoch: 1 [4736/50048]	Loss: 3.6187
Training Epoch: 1 [4864/50048]	Loss: 3.4041
Training Epoch: 1 [4992/50048]	Loss: 3.6025
Training Epoch: 1 [5120/50048]	Loss: 3.3573
Training Epoch: 1 [5248/50048]	Loss: 3.5703
Training Epoch: 1 [5376/50048]	Loss: 3.4273
Training Epoch: 1 [5504/50048]	Loss: 3.3778
Training Epoch: 1 [5632/50048]	Loss: 3.3684
Training Epoch: 1 [5760/50048]	Loss: 3.6469
Training Epoch: 1 [5888/50048]	Loss: 3.4024
Training Epoch: 1 [6016/50048]	Loss: 3.5515
Training Epoch: 1 [6144/50048]	Loss: 3.4913
Training Epoch: 1 [6272/50048]	Loss: 3.2938
Training Epoch: 1 [6400/50048]	Loss: 3.5035
Training Epoch: 1 [6528/50048]	Loss: 3.6024
Training Epoch: 1 [6656/50048]	Loss: 3.2780
Training Epoch: 1 [6784/50048]	Loss: 3.5628
Training Epoch: 1 [6912/50048]	Loss: 3.3098
Training Epoch: 1 [7040/50048]	Loss: 3.4674
Training Epoch: 1 [7168/50048]	Loss: 3.4149
Training Epoch: 1 [7296/50048]	Loss: 3.5371
Training Epoch: 1 [7424/50048]	Loss: 3.4372
Training Epoch: 1 [7552/50048]	Loss: 3.4494
Training Epoch: 1 [7680/50048]	Loss: 3.5068
Training Epoch: 1 [7808/50048]	Loss: 3.4848
Training Epoch: 1 [7936/50048]	Loss: 3.3650
Training Epoch: 1 [8064/50048]	Loss: 3.2190
Training Epoch: 1 [8192/50048]	Loss: 3.4382
Training Epoch: 1 [8320/50048]	Loss: 3.6511
Training Epoch: 1 [8448/50048]	Loss: 3.4814
Training Epoch: 1 [8576/50048]	Loss: 3.2674
Training Epoch: 1 [8704/50048]	Loss: 3.4945
Training Epoch: 1 [8832/50048]	Loss: 3.5541
Training Epoch: 1 [8960/50048]	Loss: 3.4640
Training Epoch: 1 [9088/50048]	Loss: 3.6605
Training Epoch: 1 [9216/50048]	Loss: 3.8131
Training Epoch: 1 [9344/50048]	Loss: 3.4243
Training Epoch: 1 [9472/50048]	Loss: 3.5947
Training Epoch: 1 [9600/50048]	Loss: 3.6759
Training Epoch: 1 [9728/50048]	Loss: 3.3638
Training Epoch: 1 [9856/50048]	Loss: 3.3650
Training Epoch: 1 [9984/50048]	Loss: 3.3961
Training Epoch: 1 [10112/50048]	Loss: 3.3376
Training Epoch: 1 [10240/50048]	Loss: 3.6300
Training Epoch: 1 [10368/50048]	Loss: 3.4114
Training Epoch: 1 [10496/50048]	Loss: 3.5850
Training Epoch: 1 [10624/50048]	Loss: 3.5128
Training Epoch: 1 [10752/50048]	Loss: 3.4937
Training Epoch: 1 [10880/50048]	Loss: 3.4357
Training Epoch: 1 [11008/50048]	Loss: 3.5475
Training Epoch: 1 [11136/50048]	Loss: 3.3886
Training Epoch: 1 [11264/50048]	Loss: 3.6611
Training Epoch: 1 [11392/50048]	Loss: 3.4547
Training Epoch: 1 [11520/50048]	Loss: 3.5236
Training Epoch: 1 [11648/50048]	Loss: 3.4221
Training Epoch: 1 [11776/50048]	Loss: 3.5153
Training Epoch: 1 [11904/50048]	Loss: 3.4421
Training Epoch: 1 [12032/50048]	Loss: 3.2218
Training Epoch: 1 [12160/50048]	Loss: 3.3565
Training Epoch: 1 [12288/50048]	Loss: 3.5067
Training Epoch: 1 [12416/50048]	Loss: 3.3906
Training Epoch: 1 [12544/50048]	Loss: 3.2869
Training Epoch: 1 [12672/50048]	Loss: 3.2649
Training Epoch: 1 [12800/50048]	Loss: 3.4605
Training Epoch: 1 [12928/50048]	Loss: 3.3206
Training Epoch: 1 [13056/50048]	Loss: 3.1955
Training Epoch: 1 [13184/50048]	Loss: 3.4942
Training Epoch: 1 [13312/50048]	Loss: 3.2390
Training Epoch: 1 [13440/50048]	Loss: 3.5947
Training Epoch: 1 [13568/50048]	Loss: 3.5776
Training Epoch: 1 [13696/50048]	Loss: 3.4724
Training Epoch: 1 [13824/50048]	Loss: 3.5227
Training Epoch: 1 [13952/50048]	Loss: 3.3047
Training Epoch: 1 [14080/50048]	Loss: 3.2648
Training Epoch: 1 [14208/50048]	Loss: 3.5435
Training Epoch: 1 [14336/50048]	Loss: 3.3155
Training Epoch: 1 [14464/50048]	Loss: 3.3629
Training Epoch: 1 [14592/50048]	Loss: 3.6081
Training Epoch: 1 [14720/50048]	Loss: 3.3405
Training Epoch: 1 [14848/50048]	Loss: 3.5226
Training Epoch: 1 [14976/50048]	Loss: 3.4746
Training Epoch: 1 [15104/50048]	Loss: 3.2972
Training Epoch: 1 [15232/50048]	Loss: 3.4353
Training Epoch: 1 [15360/50048]	Loss: 3.4151
Training Epoch: 1 [15488/50048]	Loss: 3.6291
Training Epoch: 1 [15616/50048]	Loss: 3.4438
Training Epoch: 1 [15744/50048]	Loss: 3.3700
Training Epoch: 1 [15872/50048]	Loss: 3.2273
Training Epoch: 1 [16000/50048]	Loss: 3.6814
Training Epoch: 1 [16128/50048]	Loss: 3.2098
Training Epoch: 1 [16256/50048]	Loss: 3.3394
Training Epoch: 1 [16384/50048]	Loss: 3.4270
Training Epoch: 1 [16512/50048]	Loss: 3.3668
Training Epoch: 1 [16640/50048]	Loss: 3.3073
Training Epoch: 1 [16768/50048]	Loss: 3.4463
Training Epoch: 1 [16896/50048]	Loss: 3.1747
Training Epoch: 1 [17024/50048]	Loss: 3.6207
Training Epoch: 1 [17152/50048]	Loss: 3.3757
Training Epoch: 1 [17280/50048]	Loss: 3.4379
Training Epoch: 1 [17408/50048]	Loss: 3.3215
Training Epoch: 1 [17536/50048]	Loss: 3.3929
Training Epoch: 1 [17664/50048]	Loss: 3.2467
Training Epoch: 1 [17792/50048]	Loss: 3.1160
Training Epoch: 1 [17920/50048]	Loss: 3.3393
Training Epoch: 1 [18048/50048]	Loss: 3.4545
Training Epoch: 1 [18176/50048]	Loss: 3.2472
Training Epoch: 1 [18304/50048]	Loss: 3.2314
Training Epoch: 1 [18432/50048]	Loss: 3.3238
Training Epoch: 1 [18560/50048]	Loss: 3.4389
Training Epoch: 1 [18688/50048]	Loss: 3.4509
Training Epoch: 1 [18816/50048]	Loss: 3.3132
Training Epoch: 1 [18944/50048]	Loss: 3.2967
Training Epoch: 1 [19072/50048]	Loss: 3.2684
Training Epoch: 1 [19200/50048]	Loss: 3.4943
Training Epoch: 1 [19328/50048]	Loss: 3.5592
Training Epoch: 1 [19456/50048]	Loss: 3.4181
Training Epoch: 1 [19584/50048]	Loss: 3.2482
Training Epoch: 1 [19712/50048]	Loss: 3.1740
Training Epoch: 1 [19840/50048]	Loss: 3.1718
Training Epoch: 1 [19968/50048]	Loss: 3.3039
Training Epoch: 1 [20096/50048]	Loss: 3.4380
Training Epoch: 1 [20224/50048]	Loss: 3.6075
Training Epoch: 1 [20352/50048]	Loss: 3.3171
Training Epoch: 1 [20480/50048]	Loss: 3.3828
Training Epoch: 1 [20608/50048]	Loss: 3.5688
Training Epoch: 1 [20736/50048]	Loss: 3.3552
Training Epoch: 1 [20864/50048]	Loss: 3.4074
Training Epoch: 1 [20992/50048]	Loss: 3.3935
Training Epoch: 1 [21120/50048]	Loss: 3.2039
Training Epoch: 1 [21248/50048]	Loss: 3.2570
Training Epoch: 1 [21376/50048]	Loss: 3.4141
Training Epoch: 1 [21504/50048]	Loss: 3.3199
Training Epoch: 1 [21632/50048]	Loss: 3.1702
Training Epoch: 1 [21760/50048]	Loss: 3.3307
Training Epoch: 1 [21888/50048]	Loss: 3.4025
Training Epoch: 1 [22016/50048]	Loss: 3.5036
Training Epoch: 1 [22144/50048]	Loss: 3.4190
Training Epoch: 1 [22272/50048]	Loss: 3.1890
Training Epoch: 1 [22400/50048]	Loss: 3.3661
Training Epoch: 1 [22528/50048]	Loss: 3.2773
Training Epoch: 1 [22656/50048]	Loss: 3.0800
Training Epoch: 1 [22784/50048]	Loss: 3.3434
Training Epoch: 1 [22912/50048]	Loss: 3.4151
Training Epoch: 1 [23040/50048]	Loss: 3.2346
Training Epoch: 1 [23168/50048]	Loss: 3.6423
Training Epoch: 1 [23296/50048]	Loss: 3.2852
Training Epoch: 1 [23424/50048]	Loss: 3.1444
Training Epoch: 1 [23552/50048]	Loss: 3.3626
Training Epoch: 1 [23680/50048]	Loss: 3.3846
Training Epoch: 1 [23808/50048]	Loss: 3.3733
Training Epoch: 1 [23936/50048]	Loss: 3.2448
Training Epoch: 1 [24064/50048]	Loss: 3.4607
Training Epoch: 1 [24192/50048]	Loss: 3.1092
Training Epoch: 1 [24320/50048]	Loss: 3.0939
Training Epoch: 1 [24448/50048]	Loss: 3.3031
Training Epoch: 1 [24576/50048]	Loss: 3.2666
Training Epoch: 1 [24704/50048]	Loss: 3.3723
Training Epoch: 1 [24832/50048]	Loss: 3.0830
Training Epoch: 1 [24960/50048]	Loss: 3.2881
Training Epoch: 1 [25088/50048]	Loss: 3.2481
Training Epoch: 1 [25216/50048]	Loss: 3.3455
Training Epoch: 1 [25344/50048]	Loss: 3.4028
Training Epoch: 1 [25472/50048]	Loss: 3.2435
Training Epoch: 1 [25600/50048]	Loss: 3.3971
Training Epoch: 1 [25728/50048]	Loss: 3.4087
Training Epoch: 1 [25856/50048]	Loss: 3.4004
Training Epoch: 1 [25984/50048]	Loss: 3.3741
Training Epoch: 1 [26112/50048]	Loss: 3.2355
Training Epoch: 1 [26240/50048]	Loss: 3.3880
Training Epoch: 1 [26368/50048]	Loss: 3.4852
Training Epoch: 1 [26496/50048]	Loss: 3.1744
Training Epoch: 1 [26624/50048]	Loss: 3.3586
Training Epoch: 1 [26752/50048]	Loss: 3.2003
Training Epoch: 1 [26880/50048]	Loss: 3.1677
Training Epoch: 1 [27008/50048]	Loss: 3.2709
Training Epoch: 1 [27136/50048]	Loss: 3.2406
Training Epoch: 1 [27264/50048]	Loss: 3.3050
Training Epoch: 1 [27392/50048]	Loss: 3.2874
Training Epoch: 1 [27520/50048]	Loss: 3.3720
Training Epoch: 1 [27648/50048]	Loss: 3.1328
Training Epoch: 1 [27776/50048]	Loss: 3.2337
Training Epoch: 1 [27904/50048]	Loss: 3.2900
Training Epoch: 1 [28032/50048]	Loss: 3.1981
Training Epoch: 1 [28160/50048]	Loss: 3.0837
Training Epoch: 1 [28288/50048]	Loss: 3.3535
Training Epoch: 1 [28416/50048]	Loss: 3.2856
Training Epoch: 1 [28544/50048]	Loss: 3.4139
Training Epoch: 1 [28672/50048]	Loss: 3.1041
Training Epoch: 1 [28800/50048]	Loss: 3.6507
Training Epoch: 1 [28928/50048]	Loss: 3.4967
Training Epoch: 1 [29056/50048]	Loss: 3.3852
Training Epoch: 1 [29184/50048]	Loss: 3.3667
Training Epoch: 1 [29312/50048]	Loss: 3.3442
Training Epoch: 1 [29440/50048]	Loss: 3.3281
Training Epoch: 1 [29568/50048]	Loss: 3.3822
Training Epoch: 1 [29696/50048]	Loss: 3.1994
Training Epoch: 1 [29824/50048]	Loss: 3.1626
Training Epoch: 1 [29952/50048]	Loss: 3.3771
Training Epoch: 1 [30080/50048]	Loss: 3.2297
Training Epoch: 1 [30208/50048]	Loss: 3.3883
Training Epoch: 1 [30336/50048]	Loss: 3.0377
Training Epoch: 1 [30464/50048]	Loss: 3.3105
Training Epoch: 1 [30592/50048]	Loss: 3.2238
Training Epoch: 1 [30720/50048]	Loss: 3.4595
Training Epoch: 1 [30848/50048]	Loss: 3.2647
Training Epoch: 1 [30976/50048]	Loss: 3.1293
Training Epoch: 1 [31104/50048]	Loss: 3.1765
Training Epoch: 1 [31232/50048]	Loss: 3.5155
Training Epoch: 1 [31360/50048]	Loss: 3.0699
Training Epoch: 1 [31488/50048]	Loss: 3.2144
Training Epoch: 1 [31616/50048]	Loss: 3.3507
Training Epoch: 1 [31744/50048]	Loss: 3.2866
Training Epoch: 1 [31872/50048]	Loss: 3.3080
Training Epoch: 1 [32000/50048]	Loss: 3.2117
Training Epoch: 1 [32128/50048]	Loss: 3.2314
Training Epoch: 1 [32256/50048]	Loss: 3.1073
Training Epoch: 1 [32384/50048]	Loss: 3.2682
Training Epoch: 1 [32512/50048]	Loss: 3.3063
Training Epoch: 1 [32640/50048]	Loss: 3.1617
Training Epoch: 1 [32768/50048]	Loss: 3.1030
Training Epoch: 1 [32896/50048]	Loss: 3.1923
Training Epoch: 1 [33024/50048]	Loss: 3.2609
Training Epoch: 1 [33152/50048]	Loss: 3.4217
Training Epoch: 1 [33280/50048]	Loss: 3.1583
Training Epoch: 1 [33408/50048]	Loss: 3.2919
Training Epoch: 1 [33536/50048]	Loss: 3.0105
Training Epoch: 1 [33664/50048]	Loss: 3.2626
Training Epoch: 1 [33792/50048]	Loss: 3.1736
Training Epoch: 1 [33920/50048]	Loss: 3.4015
Training Epoch: 1 [34048/50048]	Loss: 3.1528
Training Epoch: 1 [34176/50048]	Loss: 3.3384
Training Epoch: 1 [34304/50048]	Loss: 3.1308
Training Epoch: 1 [34432/50048]	Loss: 3.4186
Training Epoch: 1 [34560/50048]	Loss: 3.0399
Training Epoch: 1 [34688/50048]	Loss: 3.2026
Training Epoch: 1 [34816/50048]	Loss: 3.3295
Training Epoch: 1 [34944/50048]	Loss: 3.1977
Training Epoch: 1 [35072/50048]	Loss: 3.4055
Training Epoch: 1 [35200/50048]	Loss: 3.3316
Training Epoch: 1 [35328/50048]	Loss: 3.3350
Training Epoch: 1 [35456/50048]	Loss: 3.2443
Training Epoch: 1 [35584/50048]	Loss: 3.2071
Training Epoch: 1 [35712/50048]	Loss: 3.1915
Training Epoch: 1 [35840/50048]	Loss: 3.2399
Training Epoch: 1 [35968/50048]	Loss: 3.0404
Training Epoch: 1 [36096/50048]	Loss: 3.1683
Training Epoch: 1 [36224/50048]	Loss: 3.4198
Training Epoch: 1 [36352/50048]	Loss: 3.2175
Training Epoch: 1 [36480/50048]	Loss: 3.1643
Training Epoch: 1 [36608/50048]	Loss: 3.1890
Training Epoch: 1 [36736/50048]	Loss: 3.2975
Training Epoch: 1 [36864/50048]	Loss: 3.1489
Training Epoch: 1 [36992/50048]	Loss: 3.1281
Training Epoch: 1 [37120/50048]	Loss: 3.1549
Training Epoch: 1 [37248/50048]	Loss: 3.3126
Training Epoch: 1 [37376/50048]	Loss: 3.0686
Training Epoch: 1 [37504/50048]	Loss: 2.9740
Training Epoch: 1 [37632/50048]	Loss: 3.0766
Training Epoch: 1 [37760/50048]	Loss: 3.1890
Training Epoch: 1 [37888/50048]	Loss: 3.1973
Training Epoch: 1 [38016/50048]	Loss: 3.0787
Training Epoch: 1 [38144/50048]	Loss: 3.1524
Training Epoch: 1 [38272/50048]	Loss: 3.2366
Training Epoch: 1 [38400/50048]	Loss: 3.3575
Training Epoch: 1 [38528/50048]	Loss: 3.1107
Training Epoch: 1 [38656/50048]	Loss: 3.2420
Training Epoch: 1 [38784/50048]	Loss: 3.3870
Training Epoch: 1 [38912/50048]	Loss: 2.9262
Training Epoch: 1 [39040/50048]	Loss: 3.4052
Training Epoch: 1 [39168/50048]	Loss: 2.9804
Training Epoch: 1 [39296/50048]	Loss: 3.2533
Training Epoch: 1 [39424/50048]	Loss: 3.1215
Training Epoch: 1 [39552/50048]	Loss: 3.2001
Training Epoch: 1 [39680/50048]	Loss: 3.2455
Training Epoch: 1 [39808/50048]	Loss: 3.2054
Training Epoch: 1 [39936/50048]	Loss: 3.2921
Training Epoch: 1 [40064/50048]	Loss: 3.0647
Training Epoch: 1 [40192/50048]	Loss: 3.2807
Training Epoch: 1 [40320/50048]	Loss: 3.2145
Training Epoch: 1 [40448/50048]	Loss: 3.2814
Training Epoch: 1 [40576/50048]	Loss: 3.1124
Training Epoch: 1 [40704/50048]	Loss: 3.2114
Training Epoch: 1 [40832/50048]	Loss: 3.2413
Training Epoch: 1 [40960/50048]	Loss: 3.1572
Training Epoch: 1 [41088/50048]	Loss: 3.0792
Training Epoch: 1 [41216/50048]	Loss: 3.2843
Training Epoch: 1 [41344/50048]	Loss: 3.0939
Training Epoch: 1 [41472/50048]	Loss: 2.9513
Training Epoch: 1 [41600/50048]	Loss: 3.1290
Training Epoch: 1 [41728/50048]	Loss: 2.9887
Training Epoch: 1 [41856/50048]	Loss: 3.1604
Training Epoch: 1 [41984/50048]	Loss: 3.2418
Training Epoch: 1 [42112/50048]	Loss: 3.0727
Training Epoch: 1 [42240/50048]	Loss: 3.2289
Training Epoch: 1 [42368/50048]	Loss: 3.2857
Training Epoch: 1 [42496/50048]	Loss: 3.4173
Training Epoch: 1 [42624/50048]	Loss: 3.1580
Training Epoch: 1 [42752/50048]	Loss: 3.0655
Training Epoch: 1 [42880/50048]	Loss: 3.4347
Training Epoch: 1 [43008/50048]	Loss: 3.2903
Training Epoch: 1 [43136/50048]	Loss: 3.2102
Training Epoch: 1 [43264/50048]	Loss: 2.9893
Training Epoch: 1 [43392/50048]	Loss: 3.0296
Training Epoch: 1 [43520/50048]	Loss: 3.3029
Training Epoch: 1 [43648/50048]	Loss: 3.0243
Training Epoch: 1 [43776/50048]	Loss: 3.0243
Training Epoch: 1 [43904/50048]	Loss: 3.1782
Training Epoch: 1 [44032/50048]	Loss: 3.1179
Training Epoch: 1 [44160/50048]	Loss: 2.8773
Training Epoch: 1 [44288/50048]	Loss: 3.1939
Training Epoch: 1 [44416/50048]	Loss: 3.1957
Training Epoch: 1 [44544/50048]	Loss: 3.1315
Training Epoch: 1 [44672/50048]	Loss: 2.8215
Training Epoch: 1 [44800/50048]	Loss: 3.2119
Training Epoch: 1 [44928/50048]	Loss: 3.4265
Training Epoch: 1 [45056/50048]	Loss: 3.2776
Training Epoch: 1 [45184/50048]	Loss: 3.2922
Training Epoch: 1 [45312/50048]	Loss: 3.1155
Training Epoch: 1 [45440/50048]	Loss: 2.9086
Training Epoch: 1 [45568/50048]	Loss: 3.4429
Training Epoch: 1 [45696/50048]	Loss: 3.2244
Training Epoch: 1 [45824/50048]	Loss: 3.0110
Training Epoch: 1 [45952/50048]	Loss: 3.0597
Training Epoch: 1 [46080/50048]	Loss: 3.0288
Training Epoch: 1 [46208/50048]	Loss: 3.0108
Training Epoch: 1 [46336/50048]	Loss: 3.1683
Training Epoch: 1 [46464/50048]	Loss: 2.8605
Training Epoch: 1 [46592/50048]	Loss: 3.0797
Training Epoch: 1 [46720/50048]	Loss: 3.1272
2022-12-06 03:40:21,575 [ZeusDataLoader(train)] train epoch 2 done: time=86.39 energy=10494.22
2022-12-06 03:40:21,577 [ZeusDataLoader(eval)] Epoch 2 begin.
Training Epoch: 1 [46848/50048]	Loss: 3.3136
Training Epoch: 1 [46976/50048]	Loss: 3.2232
Training Epoch: 1 [47104/50048]	Loss: 3.3167
Training Epoch: 1 [47232/50048]	Loss: 3.0895
Training Epoch: 1 [47360/50048]	Loss: 3.0147
Training Epoch: 1 [47488/50048]	Loss: 3.4858
Training Epoch: 1 [47616/50048]	Loss: 3.2157
Training Epoch: 1 [47744/50048]	Loss: 3.0803
Training Epoch: 1 [47872/50048]	Loss: 3.4321
Training Epoch: 1 [48000/50048]	Loss: 3.1254
Training Epoch: 1 [48128/50048]	Loss: 3.1556
Training Epoch: 1 [48256/50048]	Loss: 3.2349
Training Epoch: 1 [48384/50048]	Loss: 3.1490
Training Epoch: 1 [48512/50048]	Loss: 3.0601
Training Epoch: 1 [48640/50048]	Loss: 3.3424
Training Epoch: 1 [48768/50048]	Loss: 3.3763
Training Epoch: 1 [48896/50048]	Loss: 3.1785
Training Epoch: 1 [49024/50048]	Loss: 3.3648
Training Epoch: 1 [49152/50048]	Loss: 3.2717
Training Epoch: 1 [49280/50048]	Loss: 3.0488
Training Epoch: 1 [49408/50048]	Loss: 3.1762
Training Epoch: 1 [49536/50048]	Loss: 2.8847
Training Epoch: 1 [49664/50048]	Loss: 3.2812
Training Epoch: 1 [49792/50048]	Loss: 3.1115
Training Epoch: 1 [49920/50048]	Loss: 3.0777
Training Epoch: 1 [50048/50048]	Loss: 3.2627
2022-12-06 08:40:25.256 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 03:40:25,273 [ZeusDataLoader(eval)] eval epoch 2 done: time=3.69 energy=441.70
2022-12-06 03:40:25,273 [ZeusDataLoader(train)] Up to epoch 2: time=183.02, energy=21738.31, cost=26883.06
2022-12-06 03:40:25,273 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 03:40:25,273 [ZeusDataLoader(train)] Expected next epoch: time=272.81, energy=32536.33, cost=40139.45
2022-12-06 03:40:25,274 [ZeusDataLoader(train)] Epoch 3 begin.
Validation Epoch: 1, Average loss: 0.0246, Accuracy: 0.2235
2022-12-06 03:40:25,415 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 03:40:25,416 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 08:40:25.420 [ZeusMonitor] Monitor started.
2022-12-06 08:40:25.420 [ZeusMonitor] Running indefinitely. 2022-12-06 08:40:25.420 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 08:40:25.420 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e3+gpu0.power.log
Training Epoch: 2 [128/50048]	Loss: 3.2202
Training Epoch: 2 [256/50048]	Loss: 2.8536
Training Epoch: 2 [384/50048]	Loss: 3.3489
Training Epoch: 2 [512/50048]	Loss: 3.1227
Training Epoch: 2 [640/50048]	Loss: 2.7623
Training Epoch: 2 [768/50048]	Loss: 2.8423
Training Epoch: 2 [896/50048]	Loss: 3.1452
Training Epoch: 2 [1024/50048]	Loss: 3.1384
Training Epoch: 2 [1152/50048]	Loss: 3.0186
Training Epoch: 2 [1280/50048]	Loss: 3.2281
Training Epoch: 2 [1408/50048]	Loss: 3.1644
Training Epoch: 2 [1536/50048]	Loss: 2.9927
Training Epoch: 2 [1664/50048]	Loss: 3.0233
Training Epoch: 2 [1792/50048]	Loss: 2.9259
Training Epoch: 2 [1920/50048]	Loss: 3.0595
Training Epoch: 2 [2048/50048]	Loss: 2.9136
Training Epoch: 2 [2176/50048]	Loss: 3.0923
Training Epoch: 2 [2304/50048]	Loss: 2.8347
Training Epoch: 2 [2432/50048]	Loss: 3.1166
Training Epoch: 2 [2560/50048]	Loss: 3.3305
Training Epoch: 2 [2688/50048]	Loss: 2.9324
Training Epoch: 2 [2816/50048]	Loss: 2.8474
Training Epoch: 2 [2944/50048]	Loss: 2.7245
Training Epoch: 2 [3072/50048]	Loss: 2.9690
Training Epoch: 2 [3200/50048]	Loss: 3.2347
Training Epoch: 2 [3328/50048]	Loss: 3.0366
Training Epoch: 2 [3456/50048]	Loss: 2.8913
Training Epoch: 2 [3584/50048]	Loss: 3.2661
Training Epoch: 2 [3712/50048]	Loss: 2.7453
Training Epoch: 2 [3840/50048]	Loss: 2.9194
Training Epoch: 2 [3968/50048]	Loss: 2.9331
Training Epoch: 2 [4096/50048]	Loss: 2.9961
Training Epoch: 2 [4224/50048]	Loss: 3.1963
Training Epoch: 2 [4352/50048]	Loss: 3.0023
Training Epoch: 2 [4480/50048]	Loss: 2.9887
Training Epoch: 2 [4608/50048]	Loss: 2.8076
Training Epoch: 2 [4736/50048]	Loss: 2.8563
Training Epoch: 2 [4864/50048]	Loss: 3.0433
Training Epoch: 2 [4992/50048]	Loss: 2.9787
Training Epoch: 2 [5120/50048]	Loss: 3.0136
Training Epoch: 2 [5248/50048]	Loss: 3.1259
Training Epoch: 2 [5376/50048]	Loss: 2.8691
Training Epoch: 2 [5504/50048]	Loss: 3.1475
Training Epoch: 2 [5632/50048]	Loss: 3.2907
Training Epoch: 2 [5760/50048]	Loss: 2.9270
Training Epoch: 2 [5888/50048]	Loss: 2.9585
Training Epoch: 2 [6016/50048]	Loss: 3.0219
Training Epoch: 2 [6144/50048]	Loss: 3.1072
Training Epoch: 2 [6272/50048]	Loss: 3.1406
Training Epoch: 2 [6400/50048]	Loss: 2.8384
Training Epoch: 2 [6528/50048]	Loss: 3.0659
Training Epoch: 2 [6656/50048]	Loss: 3.2164
Training Epoch: 2 [6784/50048]	Loss: 3.0320
Training Epoch: 2 [6912/50048]	Loss: 3.0477
Training Epoch: 2 [7040/50048]	Loss: 2.9698
Training Epoch: 2 [7168/50048]	Loss: 3.1553
Training Epoch: 2 [7296/50048]	Loss: 3.0048
Training Epoch: 2 [7424/50048]	Loss: 2.9341
Training Epoch: 2 [7552/50048]	Loss: 3.3617
Training Epoch: 2 [7680/50048]	Loss: 3.0527
Training Epoch: 2 [7808/50048]	Loss: 3.0184
Training Epoch: 2 [7936/50048]	Loss: 3.0593
Training Epoch: 2 [8064/50048]	Loss: 2.9905
Training Epoch: 2 [8192/50048]	Loss: 3.1720
Training Epoch: 2 [8320/50048]	Loss: 2.9236
Training Epoch: 2 [8448/50048]	Loss: 2.9915
Training Epoch: 2 [8576/50048]	Loss: 2.9047
Training Epoch: 2 [8704/50048]	Loss: 2.8877
Training Epoch: 2 [8832/50048]	Loss: 3.1775
Training Epoch: 2 [8960/50048]	Loss: 2.8481
Training Epoch: 2 [9088/50048]	Loss: 3.0111
Training Epoch: 2 [9216/50048]	Loss: 3.1254
Training Epoch: 2 [9344/50048]	Loss: 2.9853
Training Epoch: 2 [9472/50048]	Loss: 2.9069
Training Epoch: 2 [9600/50048]	Loss: 2.8551
Training Epoch: 2 [9728/50048]	Loss: 3.1052
Training Epoch: 2 [9856/50048]	Loss: 2.7715
Training Epoch: 2 [9984/50048]	Loss: 2.8366
Training Epoch: 2 [10112/50048]	Loss: 2.8606
Training Epoch: 2 [10240/50048]	Loss: 2.8371
Training Epoch: 2 [10368/50048]	Loss: 2.9383
Training Epoch: 2 [10496/50048]	Loss: 3.0899
Training Epoch: 2 [10624/50048]	Loss: 2.9230
Training Epoch: 2 [10752/50048]	Loss: 2.9815
Training Epoch: 2 [10880/50048]	Loss: 2.7977
Training Epoch: 2 [11008/50048]	Loss: 2.8100
Training Epoch: 2 [11136/50048]	Loss: 2.7812
Training Epoch: 2 [11264/50048]	Loss: 2.9465
Training Epoch: 2 [11392/50048]	Loss: 2.8818
Training Epoch: 2 [11520/50048]	Loss: 3.0824
Training Epoch: 2 [11648/50048]	Loss: 2.8065
Training Epoch: 2 [11776/50048]	Loss: 2.9166
Training Epoch: 2 [11904/50048]	Loss: 2.9692
Training Epoch: 2 [12032/50048]	Loss: 2.9692
Training Epoch: 2 [12160/50048]	Loss: 2.9863
Training Epoch: 2 [12288/50048]	Loss: 2.9636
Training Epoch: 2 [12416/50048]	Loss: 3.1874
Training Epoch: 2 [12544/50048]	Loss: 2.9338
Training Epoch: 2 [12672/50048]	Loss: 3.1349
Training Epoch: 2 [12800/50048]	Loss: 2.8429
Training Epoch: 2 [12928/50048]	Loss: 2.8641
Training Epoch: 2 [13056/50048]	Loss: 2.9372
Training Epoch: 2 [13184/50048]	Loss: 2.8252
Training Epoch: 2 [13312/50048]	Loss: 3.0680
Training Epoch: 2 [13440/50048]	Loss: 3.0363
Training Epoch: 2 [13568/50048]	Loss: 3.0133
Training Epoch: 2 [13696/50048]	Loss: 2.9543
Training Epoch: 2 [13824/50048]	Loss: 2.9164
Training Epoch: 2 [13952/50048]	Loss: 3.0124
Training Epoch: 2 [14080/50048]	Loss: 3.0421
Training Epoch: 2 [14208/50048]	Loss: 3.0849
Training Epoch: 2 [14336/50048]	Loss: 2.8676
Training Epoch: 2 [14464/50048]	Loss: 2.8984
Training Epoch: 2 [14592/50048]	Loss: 2.8398
Training Epoch: 2 [14720/50048]	Loss: 2.9927
Training Epoch: 2 [14848/50048]	Loss: 2.7877
Training Epoch: 2 [14976/50048]	Loss: 2.9819
Training Epoch: 2 [15104/50048]	Loss: 2.8012
Training Epoch: 2 [15232/50048]	Loss: 3.0193
Training Epoch: 2 [15360/50048]	Loss: 3.0918
Training Epoch: 2 [15488/50048]	Loss: 3.2448
Training Epoch: 2 [15616/50048]	Loss: 3.0967
Training Epoch: 2 [15744/50048]	Loss: 3.2051
Training Epoch: 2 [15872/50048]	Loss: 3.0239
Training Epoch: 2 [16000/50048]	Loss: 3.2578
Training Epoch: 2 [16128/50048]	Loss: 3.1764
Training Epoch: 2 [16256/50048]	Loss: 2.9567
Training Epoch: 2 [16384/50048]	Loss: 3.0556
Training Epoch: 2 [16512/50048]	Loss: 2.9020
Training Epoch: 2 [16640/50048]	Loss: 2.8366
Training Epoch: 2 [16768/50048]	Loss: 2.9959
Training Epoch: 2 [16896/50048]	Loss: 3.0058
Training Epoch: 2 [17024/50048]	Loss: 2.8800
Training Epoch: 2 [17152/50048]	Loss: 3.0544
Training Epoch: 2 [17280/50048]	Loss: 3.1484
Training Epoch: 2 [17408/50048]	Loss: 2.9009
Training Epoch: 2 [17536/50048]	Loss: 2.9035
Training Epoch: 2 [17664/50048]	Loss: 2.8679
Training Epoch: 2 [17792/50048]	Loss: 2.8739
Training Epoch: 2 [17920/50048]	Loss: 2.7440
Training Epoch: 2 [18048/50048]	Loss: 2.7903
Training Epoch: 2 [18176/50048]	Loss: 3.2697
Training Epoch: 2 [18304/50048]	Loss: 3.1287
Training Epoch: 2 [18432/50048]	Loss: 2.9871
Training Epoch: 2 [18560/50048]	Loss: 2.7953
Training Epoch: 2 [18688/50048]	Loss: 2.9980
Training Epoch: 2 [18816/50048]	Loss: 2.9551
Training Epoch: 2 [18944/50048]	Loss: 3.0903
Training Epoch: 2 [19072/50048]	Loss: 2.9072
Training Epoch: 2 [19200/50048]	Loss: 3.0147
Training Epoch: 2 [19328/50048]	Loss: 3.0241
Training Epoch: 2 [19456/50048]	Loss: 3.0507
Training Epoch: 2 [19584/50048]	Loss: 3.0211
Training Epoch: 2 [19712/50048]	Loss: 2.7258
Training Epoch: 2 [19840/50048]	Loss: 2.8685
Training Epoch: 2 [19968/50048]	Loss: 2.9085
Training Epoch: 2 [20096/50048]	Loss: 3.1270
Training Epoch: 2 [20224/50048]	Loss: 2.7326
Training Epoch: 2 [20352/50048]	Loss: 2.7296
Training Epoch: 2 [20480/50048]	Loss: 2.6560
Training Epoch: 2 [20608/50048]	Loss: 3.2175
Training Epoch: 2 [20736/50048]	Loss: 2.9714
Training Epoch: 2 [20864/50048]	Loss: 2.8926
Training Epoch: 2 [20992/50048]	Loss: 3.0502
Training Epoch: 2 [21120/50048]	Loss: 2.8671
Training Epoch: 2 [21248/50048]	Loss: 2.7489
Training Epoch: 2 [21376/50048]	Loss: 2.9328
Training Epoch: 2 [21504/50048]	Loss: 2.8808
Training Epoch: 2 [21632/50048]	Loss: 3.0734
Training Epoch: 2 [21760/50048]	Loss: 3.1006
Training Epoch: 2 [21888/50048]	Loss: 3.0324
Training Epoch: 2 [22016/50048]	Loss: 2.9153
Training Epoch: 2 [22144/50048]	Loss: 3.0161
Training Epoch: 2 [22272/50048]	Loss: 2.9304
Training Epoch: 2 [22400/50048]	Loss: 2.9760
Training Epoch: 2 [22528/50048]	Loss: 2.9435
Training Epoch: 2 [22656/50048]	Loss: 3.1275
Training Epoch: 2 [22784/50048]	Loss: 2.9438
Training Epoch: 2 [22912/50048]	Loss: 2.7802
Training Epoch: 2 [23040/50048]	Loss: 2.9337
Training Epoch: 2 [23168/50048]	Loss: 3.0371
Training Epoch: 2 [23296/50048]	Loss: 3.0075
Training Epoch: 2 [23424/50048]	Loss: 2.8944
Training Epoch: 2 [23552/50048]	Loss: 2.9772
Training Epoch: 2 [23680/50048]	Loss: 3.0297
Training Epoch: 2 [23808/50048]	Loss: 3.1591
Training Epoch: 2 [23936/50048]	Loss: 3.1480
Training Epoch: 2 [24064/50048]	Loss: 2.6522
Training Epoch: 2 [24192/50048]	Loss: 2.9638
Training Epoch: 2 [24320/50048]	Loss: 3.0498
Training Epoch: 2 [24448/50048]	Loss: 2.7622
Training Epoch: 2 [24576/50048]	Loss: 3.0665
Training Epoch: 2 [24704/50048]	Loss: 2.7992
Training Epoch: 2 [24832/50048]	Loss: 2.9590
Training Epoch: 2 [24960/50048]	Loss: 2.7524
Training Epoch: 2 [25088/50048]	Loss: 2.8211
Training Epoch: 2 [25216/50048]	Loss: 3.1104
Training Epoch: 2 [25344/50048]	Loss: 2.8308
Training Epoch: 2 [25472/50048]	Loss: 2.9053
Training Epoch: 2 [25600/50048]	Loss: 2.9589
Training Epoch: 2 [25728/50048]	Loss: 2.8930
Training Epoch: 2 [25856/50048]	Loss: 3.1018
Training Epoch: 2 [25984/50048]	Loss: 3.0038
Training Epoch: 2 [26112/50048]	Loss: 2.9828
Training Epoch: 2 [26240/50048]	Loss: 2.9683
Training Epoch: 2 [26368/50048]	Loss: 3.0571
Training Epoch: 2 [26496/50048]	Loss: 3.1120
Training Epoch: 2 [26624/50048]	Loss: 3.1447
Training Epoch: 2 [26752/50048]	Loss: 2.9965
Training Epoch: 2 [26880/50048]	Loss: 3.0623
Training Epoch: 2 [27008/50048]	Loss: 3.1674
Training Epoch: 2 [27136/50048]	Loss: 2.7916
Training Epoch: 2 [27264/50048]	Loss: 2.8488
Training Epoch: 2 [27392/50048]	Loss: 3.1236
Training Epoch: 2 [27520/50048]	Loss: 2.9044
Training Epoch: 2 [27648/50048]	Loss: 2.9975
Training Epoch: 2 [27776/50048]	Loss: 2.8420
Training Epoch: 2 [27904/50048]	Loss: 3.1397
Training Epoch: 2 [28032/50048]	Loss: 2.8842
Training Epoch: 2 [28160/50048]	Loss: 3.0524
Training Epoch: 2 [28288/50048]	Loss: 2.7681
Training Epoch: 2 [28416/50048]	Loss: 2.9415
Training Epoch: 2 [28544/50048]	Loss: 2.8347
Training Epoch: 2 [28672/50048]	Loss: 3.0661
Training Epoch: 2 [28800/50048]	Loss: 2.8776
Training Epoch: 2 [28928/50048]	Loss: 2.9542
Training Epoch: 2 [29056/50048]	Loss: 2.9258
Training Epoch: 2 [29184/50048]	Loss: 2.9020
Training Epoch: 2 [29312/50048]	Loss: 2.7704
Training Epoch: 2 [29440/50048]	Loss: 3.0009
Training Epoch: 2 [29568/50048]	Loss: 2.9124
Training Epoch: 2 [29696/50048]	Loss: 2.7183
Training Epoch: 2 [29824/50048]	Loss: 2.8713
Training Epoch: 2 [29952/50048]	Loss: 2.9727
Training Epoch: 2 [30080/50048]	Loss: 2.8821
Training Epoch: 2 [30208/50048]	Loss: 3.1376
Training Epoch: 2 [30336/50048]	Loss: 3.0173
Training Epoch: 2 [30464/50048]	Loss: 2.9467
Training Epoch: 2 [30592/50048]	Loss: 2.9562
Training Epoch: 2 [30720/50048]	Loss: 2.6286
Training Epoch: 2 [30848/50048]	Loss: 2.9535
Training Epoch: 2 [30976/50048]	Loss: 2.8394
Training Epoch: 2 [31104/50048]	Loss: 2.8743
Training Epoch: 2 [31232/50048]	Loss: 2.7361
Training Epoch: 2 [31360/50048]	Loss: 3.0070
Training Epoch: 2 [31488/50048]	Loss: 2.9560
Training Epoch: 2 [31616/50048]	Loss: 2.8351
Training Epoch: 2 [31744/50048]	Loss: 2.7199
Training Epoch: 2 [31872/50048]	Loss: 2.8001
Training Epoch: 2 [32000/50048]	Loss: 3.0717
Training Epoch: 2 [32128/50048]	Loss: 2.8973
Training Epoch: 2 [32256/50048]	Loss: 2.9059
Training Epoch: 2 [32384/50048]	Loss: 2.6193
Training Epoch: 2 [32512/50048]	Loss: 2.7902
Training Epoch: 2 [32640/50048]	Loss: 2.9337
Training Epoch: 2 [32768/50048]	Loss: 2.9030
Training Epoch: 2 [32896/50048]	Loss: 2.7665
Training Epoch: 2 [33024/50048]	Loss: 2.9632
Training Epoch: 2 [33152/50048]	Loss: 2.7699
Training Epoch: 2 [33280/50048]	Loss: 2.9202
Training Epoch: 2 [33408/50048]	Loss: 2.8392
Training Epoch: 2 [33536/50048]	Loss: 3.0529
Training Epoch: 2 [33664/50048]	Loss: 2.8877
Training Epoch: 2 [33792/50048]	Loss: 2.9937
Training Epoch: 2 [33920/50048]	Loss: 2.7906
Training Epoch: 2 [34048/50048]	Loss: 2.9500
Training Epoch: 2 [34176/50048]	Loss: 2.9647
Training Epoch: 2 [34304/50048]	Loss: 3.0268
Training Epoch: 2 [34432/50048]	Loss: 2.8163
Training Epoch: 2 [34560/50048]	Loss: 3.0223
Training Epoch: 2 [34688/50048]	Loss: 2.9044
Training Epoch: 2 [34816/50048]	Loss: 2.7618
Training Epoch: 2 [34944/50048]	Loss: 3.0460
Training Epoch: 2 [35072/50048]	Loss: 2.9161
Training Epoch: 2 [35200/50048]	Loss: 2.7384
Training Epoch: 2 [35328/50048]	Loss: 2.8623
Training Epoch: 2 [35456/50048]	Loss: 2.6756
Training Epoch: 2 [35584/50048]	Loss: 2.7612
Training Epoch: 2 [35712/50048]	Loss: 3.1077
Training Epoch: 2 [35840/50048]	Loss: 2.7618
Training Epoch: 2 [35968/50048]	Loss: 2.9337
Training Epoch: 2 [36096/50048]	Loss: 2.9004
Training Epoch: 2 [36224/50048]	Loss: 2.7377
Training Epoch: 2 [36352/50048]	Loss: 2.9208
Training Epoch: 2 [36480/50048]	Loss: 2.6575
Training Epoch: 2 [36608/50048]	Loss: 2.7950
Training Epoch: 2 [36736/50048]	Loss: 2.7904
Training Epoch: 2 [36864/50048]	Loss: 2.8748
Training Epoch: 2 [36992/50048]	Loss: 2.6945
Training Epoch: 2 [37120/50048]	Loss: 3.1138
Training Epoch: 2 [37248/50048]	Loss: 3.0483
Training Epoch: 2 [37376/50048]	Loss: 2.8962
Training Epoch: 2 [37504/50048]	Loss: 2.8084
Training Epoch: 2 [37632/50048]	Loss: 2.9675
Training Epoch: 2 [37760/50048]	Loss: 2.8287
Training Epoch: 2 [37888/50048]	Loss: 2.6208
Training Epoch: 2 [38016/50048]	Loss: 2.7281
Training Epoch: 2 [38144/50048]	Loss: 2.9975
Training Epoch: 2 [38272/50048]	Loss: 2.7212
Training Epoch: 2 [38400/50048]	Loss: 2.8056
Training Epoch: 2 [38528/50048]	Loss: 2.9593
Training Epoch: 2 [38656/50048]	Loss: 2.8131
Training Epoch: 2 [38784/50048]	Loss: 2.7244
Training Epoch: 2 [38912/50048]	Loss: 2.8202
Training Epoch: 2 [39040/50048]	Loss: 2.8251
Training Epoch: 2 [39168/50048]	Loss: 2.7459
Training Epoch: 2 [39296/50048]	Loss: 3.1521
Training Epoch: 2 [39424/50048]	Loss: 2.6623
Training Epoch: 2 [39552/50048]	Loss: 2.7032
Training Epoch: 2 [39680/50048]	Loss: 2.6657
Training Epoch: 2 [39808/50048]	Loss: 2.9340
Training Epoch: 2 [39936/50048]	Loss: 2.8557
Training Epoch: 2 [40064/50048]	Loss: 3.0365
Training Epoch: 2 [40192/50048]	Loss: 2.8758
Training Epoch: 2 [40320/50048]	Loss: 2.8176
Training Epoch: 2 [40448/50048]	Loss: 2.7095
Training Epoch: 2 [40576/50048]	Loss: 2.8398
Training Epoch: 2 [40704/50048]	Loss: 2.9880
Training Epoch: 2 [40832/50048]	Loss: 2.9075
Training Epoch: 2 [40960/50048]	Loss: 2.9001
Training Epoch: 2 [41088/50048]	Loss: 2.6592
Training Epoch: 2 [41216/50048]	Loss: 2.7203
Training Epoch: 2 [41344/50048]	Loss: 2.8163
Training Epoch: 2 [41472/50048]	Loss: 2.8145
Training Epoch: 2 [41600/50048]	Loss: 2.7297
Training Epoch: 2 [41728/50048]	Loss: 2.8395
Training Epoch: 2 [41856/50048]	Loss: 2.7016
Training Epoch: 2 [41984/50048]	Loss: 3.1455
Training Epoch: 2 [42112/50048]	Loss: 2.9393
Training Epoch: 2 [42240/50048]	Loss: 2.8216
Training Epoch: 2 [42368/50048]	Loss: 3.2168
Training Epoch: 2 [42496/50048]	Loss: 2.6927
Training Epoch: 2 [42624/50048]	Loss: 2.6545
Training Epoch: 2 [42752/50048]	Loss: 2.7322
Training Epoch: 2 [42880/50048]	Loss: 3.0378
Training Epoch: 2 [43008/50048]	Loss: 2.5335
Training Epoch: 2 [43136/50048]	Loss: 2.7753
Training Epoch: 2 [43264/50048]	Loss: 2.8040
Training Epoch: 2 [43392/50048]	Loss: 2.6110
Training Epoch: 2 [43520/50048]	Loss: 2.7358
Training Epoch: 2 [43648/50048]	Loss: 2.7267
Training Epoch: 2 [43776/50048]	Loss: 2.6728
Training Epoch: 2 [43904/50048]	Loss: 2.7422
Training Epoch: 2 [44032/50048]	Loss: 2.6502
Training Epoch: 2 [44160/50048]	Loss: 2.6870
Training Epoch: 2 [44288/50048]	Loss: 2.8079
Training Epoch: 2 [44416/50048]	Loss: 2.6990
Training Epoch: 2 [44544/50048]	Loss: 2.8716
Training Epoch: 2 [44672/50048]	Loss: 2.7264
Training Epoch: 2 [44800/50048]	Loss: 2.9084
Training Epoch: 2 [44928/50048]	Loss: 2.6938
Training Epoch: 2 [45056/50048]	Loss: 2.6918
Training Epoch: 2 [45184/50048]	Loss: 2.7320
Training Epoch: 2 [45312/50048]	Loss: 2.9019
Training Epoch: 2 [45440/50048]	Loss: 3.1379
Training Epoch: 2 [45568/50048]	Loss: 2.6803
Training Epoch: 2 [45696/50048]	Loss: 2.8809
Training Epoch: 2 [45824/50048]	Loss: 2.8251
Training Epoch: 2 [45952/50048]	Loss: 2.9395
Training Epoch: 2 [46080/50048]	Loss: 2.8980
Training Epoch: 2 [46208/50048]	Loss: 2.8909
Training Epoch: 2 [46336/50048]	Loss: 3.0907
Training Epoch: 2 [46464/50048]	Loss: 2.8208
Training Epoch: 2 [46592/50048]	Loss: 2.7669
Training Epoch: 2 [46720/50048]	Loss: 2.7328
2022-12-06 03:41:52,133 [ZeusDataLoader(train)] train epoch 3 done: time=86.85 energy=10530.52
2022-12-06 03:41:52,135 [ZeusDataLoader(eval)] Epoch 3 begin.
Training Epoch: 2 [46848/50048]	Loss: 2.5870
Training Epoch: 2 [46976/50048]	Loss: 2.7009
Training Epoch: 2 [47104/50048]	Loss: 2.5719
Training Epoch: 2 [47232/50048]	Loss: 2.6810
Training Epoch: 2 [47360/50048]	Loss: 2.7889
Training Epoch: 2 [47488/50048]	Loss: 2.6786
Training Epoch: 2 [47616/50048]	Loss: 2.6864
Training Epoch: 2 [47744/50048]	Loss: 2.6610
Training Epoch: 2 [47872/50048]	Loss: 2.5605
Training Epoch: 2 [48000/50048]	Loss: 2.7530
Training Epoch: 2 [48128/50048]	Loss: 3.0282
Training Epoch: 2 [48256/50048]	Loss: 2.9463
Training Epoch: 2 [48384/50048]	Loss: 2.6747
Training Epoch: 2 [48512/50048]	Loss: 2.7779
Training Epoch: 2 [48640/50048]	Loss: 2.9132
Training Epoch: 2 [48768/50048]	Loss: 2.6601
Training Epoch: 2 [48896/50048]	Loss: 2.9537
Training Epoch: 2 [49024/50048]	Loss: 2.7698
Training Epoch: 2 [49152/50048]	Loss: 2.6946
Training Epoch: 2 [49280/50048]	Loss: 2.7016
Training Epoch: 2 [49408/50048]	Loss: 2.7597
Training Epoch: 2 [49536/50048]	Loss: 2.7945
Training Epoch: 2 [49664/50048]	Loss: 2.5672
Training Epoch: 2 [49792/50048]	Loss: 2.6141
Training Epoch: 2 [49920/50048]	Loss: 2.7478
Training Epoch: 2 [50048/50048]	Loss: 2.5301
2022-12-06 08:41:55.858 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 03:41:55,899 [ZeusDataLoader(eval)] eval epoch 3 done: time=3.75 energy=451.97
2022-12-06 03:41:55,899 [ZeusDataLoader(train)] Up to epoch 3: time=273.62, energy=32720.80, cost=40302.14
2022-12-06 03:41:55,899 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 03:41:55,899 [ZeusDataLoader(train)] Expected next epoch: time=363.42, energy=43518.82, cost=53558.53
2022-12-06 03:41:55,900 [ZeusDataLoader(train)] Epoch 4 begin.
Validation Epoch: 2, Average loss: 0.0223, Accuracy: 0.2779
2022-12-06 03:41:56,041 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 03:41:56,042 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 08:41:56.045 [ZeusMonitor] Monitor started.
2022-12-06 08:41:56.046 [ZeusMonitor] Running indefinitely. 2022-12-06 08:41:56.046 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 08:41:56.046 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e4+gpu0.power.log
Training Epoch: 3 [128/50048]	Loss: 2.8830
Training Epoch: 3 [256/50048]	Loss: 2.4669
Training Epoch: 3 [384/50048]	Loss: 2.5638
Training Epoch: 3 [512/50048]	Loss: 2.6792
Training Epoch: 3 [640/50048]	Loss: 2.5007
Training Epoch: 3 [768/50048]	Loss: 2.4819
Training Epoch: 3 [896/50048]	Loss: 2.8496
Training Epoch: 3 [1024/50048]	Loss: 2.7332
Training Epoch: 3 [1152/50048]	Loss: 2.7282
Training Epoch: 3 [1280/50048]	Loss: 2.9113
Training Epoch: 3 [1408/50048]	Loss: 2.4986
Training Epoch: 3 [1536/50048]	Loss: 2.6565
Training Epoch: 3 [1664/50048]	Loss: 2.6019
Training Epoch: 3 [1792/50048]	Loss: 2.6563
Training Epoch: 3 [1920/50048]	Loss: 2.6751
Training Epoch: 3 [2048/50048]	Loss: 2.8382
Training Epoch: 3 [2176/50048]	Loss: 2.6148
Training Epoch: 3 [2304/50048]	Loss: 2.8592
Training Epoch: 3 [2432/50048]	Loss: 2.6493
Training Epoch: 3 [2560/50048]	Loss: 2.4455
Training Epoch: 3 [2688/50048]	Loss: 2.5195
Training Epoch: 3 [2816/50048]	Loss: 2.8076
Training Epoch: 3 [2944/50048]	Loss: 2.8514
Training Epoch: 3 [3072/50048]	Loss: 2.6281
Training Epoch: 3 [3200/50048]	Loss: 2.8381
Training Epoch: 3 [3328/50048]	Loss: 2.9033
Training Epoch: 3 [3456/50048]	Loss: 2.6590
Training Epoch: 3 [3584/50048]	Loss: 2.8404
Training Epoch: 3 [3712/50048]	Loss: 2.6327
Training Epoch: 3 [3840/50048]	Loss: 2.7910
Training Epoch: 3 [3968/50048]	Loss: 2.6669
Training Epoch: 3 [4096/50048]	Loss: 2.6575
Training Epoch: 3 [4224/50048]	Loss: 2.6157
Training Epoch: 3 [4352/50048]	Loss: 2.8360
Training Epoch: 3 [4480/50048]	Loss: 2.5828
Training Epoch: 3 [4608/50048]	Loss: 2.6190
Training Epoch: 3 [4736/50048]	Loss: 2.3270
Training Epoch: 3 [4864/50048]	Loss: 2.7455
Training Epoch: 3 [4992/50048]	Loss: 2.6292
Training Epoch: 3 [5120/50048]	Loss: 2.6460
Training Epoch: 3 [5248/50048]	Loss: 2.5787
Training Epoch: 3 [5376/50048]	Loss: 2.7532
Training Epoch: 3 [5504/50048]	Loss: 2.3122
Training Epoch: 3 [5632/50048]	Loss: 2.5357
Training Epoch: 3 [5760/50048]	Loss: 2.9619
Training Epoch: 3 [5888/50048]	Loss: 2.6707
Training Epoch: 3 [6016/50048]	Loss: 2.7866
Training Epoch: 3 [6144/50048]	Loss: 2.6991
Training Epoch: 3 [6272/50048]	Loss: 2.5711
Training Epoch: 3 [6400/50048]	Loss: 2.4121
Training Epoch: 3 [6528/50048]	Loss: 2.8062
Training Epoch: 3 [6656/50048]	Loss: 2.6847
Training Epoch: 3 [6784/50048]	Loss: 2.6682
Training Epoch: 3 [6912/50048]	Loss: 2.8908
Training Epoch: 3 [7040/50048]	Loss: 2.8245
Training Epoch: 3 [7168/50048]	Loss: 2.9683
Training Epoch: 3 [7296/50048]	Loss: 2.7801
Training Epoch: 3 [7424/50048]	Loss: 2.6382
Training Epoch: 3 [7552/50048]	Loss: 2.7411
Training Epoch: 3 [7680/50048]	Loss: 2.8410
Training Epoch: 3 [7808/50048]	Loss: 2.3490
Training Epoch: 3 [7936/50048]	Loss: 2.5990
Training Epoch: 3 [8064/50048]	Loss: 2.7401
Training Epoch: 3 [8192/50048]	Loss: 2.6858
Training Epoch: 3 [8320/50048]	Loss: 2.6564
Training Epoch: 3 [8448/50048]	Loss: 2.6825
Training Epoch: 3 [8576/50048]	Loss: 2.7359
Training Epoch: 3 [8704/50048]	Loss: 2.6941
Training Epoch: 3 [8832/50048]	Loss: 2.4115
Training Epoch: 3 [8960/50048]	Loss: 2.6353
Training Epoch: 3 [9088/50048]	Loss: 2.4778
Training Epoch: 3 [9216/50048]	Loss: 2.5279
Training Epoch: 3 [9344/50048]	Loss: 2.6357
Training Epoch: 3 [9472/50048]	Loss: 2.8585
Training Epoch: 3 [9600/50048]	Loss: 2.8064
Training Epoch: 3 [9728/50048]	Loss: 2.6743
Training Epoch: 3 [9856/50048]	Loss: 2.7344
Training Epoch: 3 [9984/50048]	Loss: 2.7869
Training Epoch: 3 [10112/50048]	Loss: 2.5175
Training Epoch: 3 [10240/50048]	Loss: 2.5358
Training Epoch: 3 [10368/50048]	Loss: 2.5016
Training Epoch: 3 [10496/50048]	Loss: 2.7907
Training Epoch: 3 [10624/50048]	Loss: 2.6641
Training Epoch: 3 [10752/50048]	Loss: 2.6717
Training Epoch: 3 [10880/50048]	Loss: 2.7529
Training Epoch: 3 [11008/50048]	Loss: 2.4591
Training Epoch: 3 [11136/50048]	Loss: 2.9296
Training Epoch: 3 [11264/50048]	Loss: 2.6797
Training Epoch: 3 [11392/50048]	Loss: 2.5718
Training Epoch: 3 [11520/50048]	Loss: 2.6824
Training Epoch: 3 [11648/50048]	Loss: 2.7402
Training Epoch: 3 [11776/50048]	Loss: 2.6209
Training Epoch: 3 [11904/50048]	Loss: 2.7585
Training Epoch: 3 [12032/50048]	Loss: 2.5624
Training Epoch: 3 [12160/50048]	Loss: 2.7344
Training Epoch: 3 [12288/50048]	Loss: 2.7338
Training Epoch: 3 [12416/50048]	Loss: 2.4785
Training Epoch: 3 [12544/50048]	Loss: 2.6580
Training Epoch: 3 [12672/50048]	Loss: 2.7415
Training Epoch: 3 [12800/50048]	Loss: 2.6492
Training Epoch: 3 [12928/50048]	Loss: 2.5475
Training Epoch: 3 [13056/50048]	Loss: 2.6493
Training Epoch: 3 [13184/50048]	Loss: 2.4059
Training Epoch: 3 [13312/50048]	Loss: 2.9971
Training Epoch: 3 [13440/50048]	Loss: 2.7411
Training Epoch: 3 [13568/50048]	Loss: 2.5152
Training Epoch: 3 [13696/50048]	Loss: 2.5502
Training Epoch: 3 [13824/50048]	Loss: 2.4868
Training Epoch: 3 [13952/50048]	Loss: 2.7956
Training Epoch: 3 [14080/50048]	Loss: 2.5514
Training Epoch: 3 [14208/50048]	Loss: 2.7821
Training Epoch: 3 [14336/50048]	Loss: 2.6576
Training Epoch: 3 [14464/50048]	Loss: 2.6390
Training Epoch: 3 [14592/50048]	Loss: 2.6532
Training Epoch: 3 [14720/50048]	Loss: 2.3246
Training Epoch: 3 [14848/50048]	Loss: 2.7810
Training Epoch: 3 [14976/50048]	Loss: 2.7010
Training Epoch: 3 [15104/50048]	Loss: 2.6635
Training Epoch: 3 [15232/50048]	Loss: 2.7952
Training Epoch: 3 [15360/50048]	Loss: 2.8638
Training Epoch: 3 [15488/50048]	Loss: 2.4966
Training Epoch: 3 [15616/50048]	Loss: 2.6930
Training Epoch: 3 [15744/50048]	Loss: 2.7455
Training Epoch: 3 [15872/50048]	Loss: 2.7516
Training Epoch: 3 [16000/50048]	Loss: 2.6641
Training Epoch: 3 [16128/50048]	Loss: 2.8482
Training Epoch: 3 [16256/50048]	Loss: 2.5599
Training Epoch: 3 [16384/50048]	Loss: 2.5753
Training Epoch: 3 [16512/50048]	Loss: 2.8191
Training Epoch: 3 [16640/50048]	Loss: 2.6780
Training Epoch: 3 [16768/50048]	Loss: 3.0050
Training Epoch: 3 [16896/50048]	Loss: 2.5717
Training Epoch: 3 [17024/50048]	Loss: 2.4873
Training Epoch: 3 [17152/50048]	Loss: 2.6043
Training Epoch: 3 [17280/50048]	Loss: 2.6473
Training Epoch: 3 [17408/50048]	Loss: 2.3781
Training Epoch: 3 [17536/50048]	Loss: 2.5125
Training Epoch: 3 [17664/50048]	Loss: 2.5661
Training Epoch: 3 [17792/50048]	Loss: 2.7396
Training Epoch: 3 [17920/50048]	Loss: 2.4684
Training Epoch: 3 [18048/50048]	Loss: 2.6743
Training Epoch: 3 [18176/50048]	Loss: 2.7312
Training Epoch: 3 [18304/50048]	Loss: 2.7692
Training Epoch: 3 [18432/50048]	Loss: 2.6798
Training Epoch: 3 [18560/50048]	Loss: 2.5210
Training Epoch: 3 [18688/50048]	Loss: 2.4906
Training Epoch: 3 [18816/50048]	Loss: 2.5333
Training Epoch: 3 [18944/50048]	Loss: 2.5362
Training Epoch: 3 [19072/50048]	Loss: 2.7143
Training Epoch: 3 [19200/50048]	Loss: 2.5709
Training Epoch: 3 [19328/50048]	Loss: 2.5046
Training Epoch: 3 [19456/50048]	Loss: 2.9686
Training Epoch: 3 [19584/50048]	Loss: 2.3204
Training Epoch: 3 [19712/50048]	Loss: 2.5125
Training Epoch: 3 [19840/50048]	Loss: 2.9326
Training Epoch: 3 [19968/50048]	Loss: 2.7831
Training Epoch: 3 [20096/50048]	Loss: 2.4302
Training Epoch: 3 [20224/50048]	Loss: 2.8097
Training Epoch: 3 [20352/50048]	Loss: 2.5611
Training Epoch: 3 [20480/50048]	Loss: 2.4887
Training Epoch: 3 [20608/50048]	Loss: 2.6641
Training Epoch: 3 [20736/50048]	Loss: 2.7022
Training Epoch: 3 [20864/50048]	Loss: 2.5228
Training Epoch: 3 [20992/50048]	Loss: 2.6913
Training Epoch: 3 [21120/50048]	Loss: 2.4172
Training Epoch: 3 [21248/50048]	Loss: 2.9391
Training Epoch: 3 [21376/50048]	Loss: 2.8346
Training Epoch: 3 [21504/50048]	Loss: 2.7463
Training Epoch: 3 [21632/50048]	Loss: 2.5723
Training Epoch: 3 [21760/50048]	Loss: 2.7578
Training Epoch: 3 [21888/50048]	Loss: 2.6671
Training Epoch: 3 [22016/50048]	Loss: 2.4953
Training Epoch: 3 [22144/50048]	Loss: 2.3748
Training Epoch: 3 [22272/50048]	Loss: 2.6106
Training Epoch: 3 [22400/50048]	Loss: 2.6754
Training Epoch: 3 [22528/50048]	Loss: 2.7208
Training Epoch: 3 [22656/50048]	Loss: 2.8375
Training Epoch: 3 [22784/50048]	Loss: 2.6796
Training Epoch: 3 [22912/50048]	Loss: 2.6577
Training Epoch: 3 [23040/50048]	Loss: 2.4431
Training Epoch: 3 [23168/50048]	Loss: 2.4143
Training Epoch: 3 [23296/50048]	Loss: 2.7553
Training Epoch: 3 [23424/50048]	Loss: 2.4137
Training Epoch: 3 [23552/50048]	Loss: 2.5624
Training Epoch: 3 [23680/50048]	Loss: 2.5936
Training Epoch: 3 [23808/50048]	Loss: 2.5656
Training Epoch: 3 [23936/50048]	Loss: 2.6322
Training Epoch: 3 [24064/50048]	Loss: 2.6671
Training Epoch: 3 [24192/50048]	Loss: 2.4861
Training Epoch: 3 [24320/50048]	Loss: 2.5479
Training Epoch: 3 [24448/50048]	Loss: 2.5681
Training Epoch: 3 [24576/50048]	Loss: 2.6964
Training Epoch: 3 [24704/50048]	Loss: 2.6200
Training Epoch: 3 [24832/50048]	Loss: 2.5004
Training Epoch: 3 [24960/50048]	Loss: 2.5757
Training Epoch: 3 [25088/50048]	Loss: 2.3046
Training Epoch: 3 [25216/50048]	Loss: 2.4202
Training Epoch: 3 [25344/50048]	Loss: 2.5145
Training Epoch: 3 [25472/50048]	Loss: 2.6360
Training Epoch: 3 [25600/50048]	Loss: 2.2741
Training Epoch: 3 [25728/50048]	Loss: 2.9602
Training Epoch: 3 [25856/50048]	Loss: 2.6092
Training Epoch: 3 [25984/50048]	Loss: 2.5407
Training Epoch: 3 [26112/50048]	Loss: 2.4346
Training Epoch: 3 [26240/50048]	Loss: 2.7551
Training Epoch: 3 [26368/50048]	Loss: 2.6306
Training Epoch: 3 [26496/50048]	Loss: 2.7157
Training Epoch: 3 [26624/50048]	Loss: 2.6259
Training Epoch: 3 [26752/50048]	Loss: 2.5024
Training Epoch: 3 [26880/50048]	Loss: 2.9700
Training Epoch: 3 [27008/50048]	Loss: 2.5854
Training Epoch: 3 [27136/50048]	Loss: 2.7150
Training Epoch: 3 [27264/50048]	Loss: 2.6593
Training Epoch: 3 [27392/50048]	Loss: 2.4760
Training Epoch: 3 [27520/50048]	Loss: 2.5503
Training Epoch: 3 [27648/50048]	Loss: 2.6642
Training Epoch: 3 [27776/50048]	Loss: 2.5216
Training Epoch: 3 [27904/50048]	Loss: 2.6413
Training Epoch: 3 [28032/50048]	Loss: 2.4742
Training Epoch: 3 [28160/50048]	Loss: 2.4232
Training Epoch: 3 [28288/50048]	Loss: 2.7592
Training Epoch: 3 [28416/50048]	Loss: 2.7326
Training Epoch: 3 [28544/50048]	Loss: 2.4707
Training Epoch: 3 [28672/50048]	Loss: 2.5821
Training Epoch: 3 [28800/50048]	Loss: 2.4866
Training Epoch: 3 [28928/50048]	Loss: 2.5993
Training Epoch: 3 [29056/50048]	Loss: 2.7558
Training Epoch: 3 [29184/50048]	Loss: 2.8162
Training Epoch: 3 [29312/50048]	Loss: 2.4261
Training Epoch: 3 [29440/50048]	Loss: 2.6460
Training Epoch: 3 [29568/50048]	Loss: 2.8081
Training Epoch: 3 [29696/50048]	Loss: 2.4475
Training Epoch: 3 [29824/50048]	Loss: 2.3714
Training Epoch: 3 [29952/50048]	Loss: 2.6195
Training Epoch: 3 [30080/50048]	Loss: 2.4156
Training Epoch: 3 [30208/50048]	Loss: 2.7081
Training Epoch: 3 [30336/50048]	Loss: 2.6262
Training Epoch: 3 [30464/50048]	Loss: 2.6107
Training Epoch: 3 [30592/50048]	Loss: 2.6332
Training Epoch: 3 [30720/50048]	Loss: 2.4279
Training Epoch: 3 [30848/50048]	Loss: 2.6814
Training Epoch: 3 [30976/50048]	Loss: 2.6291
Training Epoch: 3 [31104/50048]	Loss: 2.6073
Training Epoch: 3 [31232/50048]	Loss: 2.3173
Training Epoch: 3 [31360/50048]	Loss: 2.5034
Training Epoch: 3 [31488/50048]	Loss: 2.6581
Training Epoch: 3 [31616/50048]	Loss: 2.9100
Training Epoch: 3 [31744/50048]	Loss: 2.5319
Training Epoch: 3 [31872/50048]	Loss: 2.6417
Training Epoch: 3 [32000/50048]	Loss: 2.3521
Training Epoch: 3 [32128/50048]	Loss: 2.5075
Training Epoch: 3 [32256/50048]	Loss: 2.6348
Training Epoch: 3 [32384/50048]	Loss: 2.4599
Training Epoch: 3 [32512/50048]	Loss: 2.7745
Training Epoch: 3 [32640/50048]	Loss: 2.6441
Training Epoch: 3 [32768/50048]	Loss: 2.6308
Training Epoch: 3 [32896/50048]	Loss: 2.3855
Training Epoch: 3 [33024/50048]	Loss: 2.1453
Training Epoch: 3 [33152/50048]	Loss: 2.5711
Training Epoch: 3 [33280/50048]	Loss: 2.6063
Training Epoch: 3 [33408/50048]	Loss: 2.4537
Training Epoch: 3 [33536/50048]	Loss: 2.3848
Training Epoch: 3 [33664/50048]	Loss: 2.6183
Training Epoch: 3 [33792/50048]	Loss: 2.2638
Training Epoch: 3 [33920/50048]	Loss: 2.6019
Training Epoch: 3 [34048/50048]	Loss: 2.4501
Training Epoch: 3 [34176/50048]	Loss: 2.7380
Training Epoch: 3 [34304/50048]	Loss: 2.6052
Training Epoch: 3 [34432/50048]	Loss: 2.5317
Training Epoch: 3 [34560/50048]	Loss: 2.4898
Training Epoch: 3 [34688/50048]	Loss: 2.5322
Training Epoch: 3 [34816/50048]	Loss: 2.7716
Training Epoch: 3 [34944/50048]	Loss: 2.5394
Training Epoch: 3 [35072/50048]	Loss: 2.3488
Training Epoch: 3 [35200/50048]	Loss: 2.5412
Training Epoch: 3 [35328/50048]	Loss: 2.5484
Training Epoch: 3 [35456/50048]	Loss: 2.5041
Training Epoch: 3 [35584/50048]	Loss: 2.3622
Training Epoch: 3 [35712/50048]	Loss: 2.6594
Training Epoch: 3 [35840/50048]	Loss: 2.6811
Training Epoch: 3 [35968/50048]	Loss: 2.6339
Training Epoch: 3 [36096/50048]	Loss: 2.4978
Training Epoch: 3 [36224/50048]	Loss: 2.5210
Training Epoch: 3 [36352/50048]	Loss: 2.5453
Training Epoch: 3 [36480/50048]	Loss: 2.3894
Training Epoch: 3 [36608/50048]	Loss: 2.8121
Training Epoch: 3 [36736/50048]	Loss: 2.5537
Training Epoch: 3 [36864/50048]	Loss: 2.4024
Training Epoch: 3 [36992/50048]	Loss: 2.6435
Training Epoch: 3 [37120/50048]	Loss: 2.3337
Training Epoch: 3 [37248/50048]	Loss: 2.5425
Training Epoch: 3 [37376/50048]	Loss: 2.4732
Training Epoch: 3 [37504/50048]	Loss: 2.7329
Training Epoch: 3 [37632/50048]	Loss: 2.7947
Training Epoch: 3 [37760/50048]	Loss: 2.4158
Training Epoch: 3 [37888/50048]	Loss: 2.2567
Training Epoch: 3 [38016/50048]	Loss: 2.4547
Training Epoch: 3 [38144/50048]	Loss: 2.3640
Training Epoch: 3 [38272/50048]	Loss: 2.6294
Training Epoch: 3 [38400/50048]	Loss: 2.7889
Training Epoch: 3 [38528/50048]	Loss: 2.5311
Training Epoch: 3 [38656/50048]	Loss: 2.6147
Training Epoch: 3 [38784/50048]	Loss: 2.4124
Training Epoch: 3 [38912/50048]	Loss: 2.7100
Training Epoch: 3 [39040/50048]	Loss: 2.6129
Training Epoch: 3 [39168/50048]	Loss: 2.5664
Training Epoch: 3 [39296/50048]	Loss: 2.5224
Training Epoch: 3 [39424/50048]	Loss: 2.5713
Training Epoch: 3 [39552/50048]	Loss: 2.6097
Training Epoch: 3 [39680/50048]	Loss: 2.5125
Training Epoch: 3 [39808/50048]	Loss: 2.5503
Training Epoch: 3 [39936/50048]	Loss: 2.5470
Training Epoch: 3 [40064/50048]	Loss: 2.1545
Training Epoch: 3 [40192/50048]	Loss: 2.6053
Training Epoch: 3 [40320/50048]	Loss: 2.4993
Training Epoch: 3 [40448/50048]	Loss: 2.5047
Training Epoch: 3 [40576/50048]	Loss: 2.4525
Training Epoch: 3 [40704/50048]	Loss: 2.4209
Training Epoch: 3 [40832/50048]	Loss: 2.5488
Training Epoch: 3 [40960/50048]	Loss: 2.3744
Training Epoch: 3 [41088/50048]	Loss: 2.9066
Training Epoch: 3 [41216/50048]	Loss: 2.3621
Training Epoch: 3 [41344/50048]	Loss: 2.8309
Training Epoch: 3 [41472/50048]	Loss: 2.5221
Training Epoch: 3 [41600/50048]	Loss: 2.3015
Training Epoch: 3 [41728/50048]	Loss: 2.4731
Training Epoch: 3 [41856/50048]	Loss: 2.4158
Training Epoch: 3 [41984/50048]	Loss: 2.5615
Training Epoch: 3 [42112/50048]	Loss: 2.3018
Training Epoch: 3 [42240/50048]	Loss: 2.5157
Training Epoch: 3 [42368/50048]	Loss: 2.3415
Training Epoch: 3 [42496/50048]	Loss: 2.4281
Training Epoch: 3 [42624/50048]	Loss: 2.7520
Training Epoch: 3 [42752/50048]	Loss: 2.4962
Training Epoch: 3 [42880/50048]	Loss: 2.5222
Training Epoch: 3 [43008/50048]	Loss: 2.3349
Training Epoch: 3 [43136/50048]	Loss: 2.4613
Training Epoch: 3 [43264/50048]	Loss: 2.5360
Training Epoch: 3 [43392/50048]	Loss: 2.4607
Training Epoch: 3 [43520/50048]	Loss: 2.5414
Training Epoch: 3 [43648/50048]	Loss: 2.4926
Training Epoch: 3 [43776/50048]	Loss: 2.4157
Training Epoch: 3 [43904/50048]	Loss: 2.3424
Training Epoch: 3 [44032/50048]	Loss: 2.6592
Training Epoch: 3 [44160/50048]	Loss: 2.6112
Training Epoch: 3 [44288/50048]	Loss: 2.5229
Training Epoch: 3 [44416/50048]	Loss: 2.3073
Training Epoch: 3 [44544/50048]	Loss: 2.4146
Training Epoch: 3 [44672/50048]	Loss: 2.4642
Training Epoch: 3 [44800/50048]	Loss: 2.4798
Training Epoch: 3 [44928/50048]	Loss: 2.1856
Training Epoch: 3 [45056/50048]	Loss: 2.7756
Training Epoch: 3 [45184/50048]	Loss: 2.6482
Training Epoch: 3 [45312/50048]	Loss: 2.2447
Training Epoch: 3 [45440/50048]	Loss: 2.5403
Training Epoch: 3 [45568/50048]	Loss: 2.3083
Training Epoch: 3 [45696/50048]	Loss: 2.4741
Training Epoch: 3 [45824/50048]	Loss: 2.3303
Training Epoch: 3 [45952/50048]	Loss: 2.6192
Training Epoch: 3 [46080/50048]	Loss: 2.6181
Training Epoch: 3 [46208/50048]	Loss: 2.5372
Training Epoch: 3 [46336/50048]	Loss: 2.5669
Training Epoch: 3 [46464/50048]	Loss: 2.4756
Training Epoch: 3 [46592/50048]	Loss: 2.1453
Training Epoch: 3 [46720/50048]	Loss: 2.4079
2022-12-06 03:43:22,464 [ZeusDataLoader(train)] train epoch 4 done: time=86.55 energy=10512.71
2022-12-06 03:43:22,466 [ZeusDataLoader(eval)] Epoch 4 begin.
Training Epoch: 3 [46848/50048]	Loss: 2.4681
Training Epoch: 3 [46976/50048]	Loss: 2.6071
Training Epoch: 3 [47104/50048]	Loss: 2.3811
Training Epoch: 3 [47232/50048]	Loss: 2.5121
Training Epoch: 3 [47360/50048]	Loss: 2.6721
Training Epoch: 3 [47488/50048]	Loss: 2.6039
Training Epoch: 3 [47616/50048]	Loss: 2.6132
Training Epoch: 3 [47744/50048]	Loss: 2.6588
Training Epoch: 3 [47872/50048]	Loss: 2.5992
Training Epoch: 3 [48000/50048]	Loss: 2.5591
Training Epoch: 3 [48128/50048]	Loss: 2.6286
Training Epoch: 3 [48256/50048]	Loss: 2.2470
Training Epoch: 3 [48384/50048]	Loss: 2.5348
Training Epoch: 3 [48512/50048]	Loss: 2.4907
Training Epoch: 3 [48640/50048]	Loss: 2.5702
Training Epoch: 3 [48768/50048]	Loss: 2.6869
Training Epoch: 3 [48896/50048]	Loss: 2.5089
Training Epoch: 3 [49024/50048]	Loss: 2.6057
Training Epoch: 3 [49152/50048]	Loss: 2.6411
Training Epoch: 3 [49280/50048]	Loss: 2.5877
Training Epoch: 3 [49408/50048]	Loss: 2.4351
Training Epoch: 3 [49536/50048]	Loss: 2.5759
Training Epoch: 3 [49664/50048]	Loss: 2.4632
Training Epoch: 3 [49792/50048]	Loss: 2.3986
Training Epoch: 3 [49920/50048]	Loss: 2.5809
Training Epoch: 3 [50048/50048]	Loss: 2.2487
2022-12-06 08:43:26.167 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 03:43:26,189 [ZeusDataLoader(eval)] eval epoch 4 done: time=3.71 energy=452.33
2022-12-06 03:43:26,189 [ZeusDataLoader(train)] Up to epoch 4: time=363.89, energy=43685.84, cost=53683.16
2022-12-06 03:43:26,189 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 03:43:26,190 [ZeusDataLoader(train)] Expected next epoch: time=453.69, energy=54483.85, cost=66939.54
2022-12-06 03:43:26,190 [ZeusDataLoader(train)] Epoch 5 begin.
Validation Epoch: 3, Average loss: 0.0208, Accuracy: 0.3185
2022-12-06 03:43:26,363 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 03:43:26,364 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 08:43:26.366 [ZeusMonitor] Monitor started.
2022-12-06 08:43:26.366 [ZeusMonitor] Running indefinitely. 2022-12-06 08:43:26.366 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 08:43:26.366 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e5+gpu0.power.log
Training Epoch: 4 [128/50048]	Loss: 2.4359
Training Epoch: 4 [256/50048]	Loss: 2.4347
Training Epoch: 4 [384/50048]	Loss: 2.6570
Training Epoch: 4 [512/50048]	Loss: 2.4178
Training Epoch: 4 [640/50048]	Loss: 2.5673
Training Epoch: 4 [768/50048]	Loss: 2.4223
Training Epoch: 4 [896/50048]	Loss: 2.3785
Training Epoch: 4 [1024/50048]	Loss: 2.4595
Training Epoch: 4 [1152/50048]	Loss: 2.2717
Training Epoch: 4 [1280/50048]	Loss: 1.9903
Training Epoch: 4 [1408/50048]	Loss: 2.0465
Training Epoch: 4 [1536/50048]	Loss: 2.2801
Training Epoch: 4 [1664/50048]	Loss: 2.4930
Training Epoch: 4 [1792/50048]	Loss: 2.3522
Training Epoch: 4 [1920/50048]	Loss: 2.2277
Training Epoch: 4 [2048/50048]	Loss: 2.2462
Training Epoch: 4 [2176/50048]	Loss: 2.4095
Training Epoch: 4 [2304/50048]	Loss: 2.2483
Training Epoch: 4 [2432/50048]	Loss: 2.4758
Training Epoch: 4 [2560/50048]	Loss: 2.3568
Training Epoch: 4 [2688/50048]	Loss: 2.4047
Training Epoch: 4 [2816/50048]	Loss: 2.3651
Training Epoch: 4 [2944/50048]	Loss: 2.1323
Training Epoch: 4 [3072/50048]	Loss: 2.2811
Training Epoch: 4 [3200/50048]	Loss: 2.5051
Training Epoch: 4 [3328/50048]	Loss: 2.5721
Training Epoch: 4 [3456/50048]	Loss: 2.2586
Training Epoch: 4 [3584/50048]	Loss: 2.4453
Training Epoch: 4 [3712/50048]	Loss: 2.5132
Training Epoch: 4 [3840/50048]	Loss: 2.3559
Training Epoch: 4 [3968/50048]	Loss: 2.2432
Training Epoch: 4 [4096/50048]	Loss: 2.4107
Training Epoch: 4 [4224/50048]	Loss: 2.3508
Training Epoch: 4 [4352/50048]	Loss: 2.2185
Training Epoch: 4 [4480/50048]	Loss: 2.4859
Training Epoch: 4 [4608/50048]	Loss: 2.3043
Training Epoch: 4 [4736/50048]	Loss: 2.5604
Training Epoch: 4 [4864/50048]	Loss: 2.3902
Training Epoch: 4 [4992/50048]	Loss: 2.2306
Training Epoch: 4 [5120/50048]	Loss: 2.4694
Training Epoch: 4 [5248/50048]	Loss: 2.2137
Training Epoch: 4 [5376/50048]	Loss: 2.5484
Training Epoch: 4 [5504/50048]	Loss: 2.2126
Training Epoch: 4 [5632/50048]	Loss: 2.6053
Training Epoch: 4 [5760/50048]	Loss: 2.1434
Training Epoch: 4 [5888/50048]	Loss: 2.4812
Training Epoch: 4 [6016/50048]	Loss: 2.5014
Training Epoch: 4 [6144/50048]	Loss: 2.5367
Training Epoch: 4 [6272/50048]	Loss: 2.2864
Training Epoch: 4 [6400/50048]	Loss: 2.0983
Training Epoch: 4 [6528/50048]	Loss: 2.2672
Training Epoch: 4 [6656/50048]	Loss: 2.4204
Training Epoch: 4 [6784/50048]	Loss: 2.3878
Training Epoch: 4 [6912/50048]	Loss: 2.4578
Training Epoch: 4 [7040/50048]	Loss: 2.0508
Training Epoch: 4 [7168/50048]	Loss: 2.3836
Training Epoch: 4 [7296/50048]	Loss: 2.3975
Training Epoch: 4 [7424/50048]	Loss: 2.2179
Training Epoch: 4 [7552/50048]	Loss: 2.6084
Training Epoch: 4 [7680/50048]	Loss: 2.6247
Training Epoch: 4 [7808/50048]	Loss: 2.5366
Training Epoch: 4 [7936/50048]	Loss: 2.5893
Training Epoch: 4 [8064/50048]	Loss: 2.4949
Training Epoch: 4 [8192/50048]	Loss: 2.3804
Training Epoch: 4 [8320/50048]	Loss: 2.3698
Training Epoch: 4 [8448/50048]	Loss: 2.4091
Training Epoch: 4 [8576/50048]	Loss: 2.4977
Training Epoch: 4 [8704/50048]	Loss: 2.2726
Training Epoch: 4 [8832/50048]	Loss: 2.3142
Training Epoch: 4 [8960/50048]	Loss: 2.5016
Training Epoch: 4 [9088/50048]	Loss: 2.2534
Training Epoch: 4 [9216/50048]	Loss: 2.7083
Training Epoch: 4 [9344/50048]	Loss: 2.5224
Training Epoch: 4 [9472/50048]	Loss: 2.4041
Training Epoch: 4 [9600/50048]	Loss: 2.3631
Training Epoch: 4 [9728/50048]	Loss: 2.1917
Training Epoch: 4 [9856/50048]	Loss: 2.4641
Training Epoch: 4 [9984/50048]	Loss: 2.6762
Training Epoch: 4 [10112/50048]	Loss: 2.4553
Training Epoch: 4 [10240/50048]	Loss: 2.3376
Training Epoch: 4 [10368/50048]	Loss: 2.3800
Training Epoch: 4 [10496/50048]	Loss: 2.5746
Training Epoch: 4 [10624/50048]	Loss: 2.2749
Training Epoch: 4 [10752/50048]	Loss: 2.3933
Training Epoch: 4 [10880/50048]	Loss: 2.5429
Training Epoch: 4 [11008/50048]	Loss: 2.5448
Training Epoch: 4 [11136/50048]	Loss: 2.2755
Training Epoch: 4 [11264/50048]	Loss: 2.5346
Training Epoch: 4 [11392/50048]	Loss: 2.4046
Training Epoch: 4 [11520/50048]	Loss: 2.4082
Training Epoch: 4 [11648/50048]	Loss: 2.2385
Training Epoch: 4 [11776/50048]	Loss: 2.5580
Training Epoch: 4 [11904/50048]	Loss: 2.4252
Training Epoch: 4 [12032/50048]	Loss: 2.3626
Training Epoch: 4 [12160/50048]	Loss: 2.3240
Training Epoch: 4 [12288/50048]	Loss: 2.5051
Training Epoch: 4 [12416/50048]	Loss: 2.6115
Training Epoch: 4 [12544/50048]	Loss: 2.5358
Training Epoch: 4 [12672/50048]	Loss: 2.3531
Training Epoch: 4 [12800/50048]	Loss: 2.3494
Training Epoch: 4 [12928/50048]	Loss: 2.1842
Training Epoch: 4 [13056/50048]	Loss: 2.3912
Training Epoch: 4 [13184/50048]	Loss: 2.4326
Training Epoch: 4 [13312/50048]	Loss: 2.3849
Training Epoch: 4 [13440/50048]	Loss: 2.2606
Training Epoch: 4 [13568/50048]	Loss: 2.2743
Training Epoch: 4 [13696/50048]	Loss: 2.3253
Training Epoch: 4 [13824/50048]	Loss: 2.3918
Training Epoch: 4 [13952/50048]	Loss: 2.5810
Training Epoch: 4 [14080/50048]	Loss: 2.4661
Training Epoch: 4 [14208/50048]	Loss: 2.4193
Training Epoch: 4 [14336/50048]	Loss: 2.3692
Training Epoch: 4 [14464/50048]	Loss: 2.3270
Training Epoch: 4 [14592/50048]	Loss: 2.1642
Training Epoch: 4 [14720/50048]	Loss: 2.4849
Training Epoch: 4 [14848/50048]	Loss: 2.4614
Training Epoch: 4 [14976/50048]	Loss: 2.5363
Training Epoch: 4 [15104/50048]	Loss: 2.5048
Training Epoch: 4 [15232/50048]	Loss: 2.5210
Training Epoch: 4 [15360/50048]	Loss: 2.4170
Training Epoch: 4 [15488/50048]	Loss: 2.2824
Training Epoch: 4 [15616/50048]	Loss: 2.2736
Training Epoch: 4 [15744/50048]	Loss: 1.9521
Training Epoch: 4 [15872/50048]	Loss: 2.2033
Training Epoch: 4 [16000/50048]	Loss: 2.3179
Training Epoch: 4 [16128/50048]	Loss: 2.2780
Training Epoch: 4 [16256/50048]	Loss: 2.5405
Training Epoch: 4 [16384/50048]	Loss: 2.3046
Training Epoch: 4 [16512/50048]	Loss: 2.6871
Training Epoch: 4 [16640/50048]	Loss: 2.4757
Training Epoch: 4 [16768/50048]	Loss: 2.1637
Training Epoch: 4 [16896/50048]	Loss: 2.2575
Training Epoch: 4 [17024/50048]	Loss: 2.3350
Training Epoch: 4 [17152/50048]	Loss: 2.3097
Training Epoch: 4 [17280/50048]	Loss: 2.3273
Training Epoch: 4 [17408/50048]	Loss: 2.4307
Training Epoch: 4 [17536/50048]	Loss: 2.2396
Training Epoch: 4 [17664/50048]	Loss: 2.4159
Training Epoch: 4 [17792/50048]	Loss: 2.8346
Training Epoch: 4 [17920/50048]	Loss: 2.2410
Training Epoch: 4 [18048/50048]	Loss: 2.1510
Training Epoch: 4 [18176/50048]	Loss: 2.4786
Training Epoch: 4 [18304/50048]	Loss: 2.2368
Training Epoch: 4 [18432/50048]	Loss: 2.1953
Training Epoch: 4 [18560/50048]	Loss: 2.3626
Training Epoch: 4 [18688/50048]	Loss: 2.5085
Training Epoch: 4 [18816/50048]	Loss: 2.0720
Training Epoch: 4 [18944/50048]	Loss: 2.2858
Training Epoch: 4 [19072/50048]	Loss: 2.4895
Training Epoch: 4 [19200/50048]	Loss: 2.5271
Training Epoch: 4 [19328/50048]	Loss: 2.3105
Training Epoch: 4 [19456/50048]	Loss: 2.4687
Training Epoch: 4 [19584/50048]	Loss: 2.3317
Training Epoch: 4 [19712/50048]	Loss: 2.3702
Training Epoch: 4 [19840/50048]	Loss: 2.4182
Training Epoch: 4 [19968/50048]	Loss: 2.4775
Training Epoch: 4 [20096/50048]	Loss: 2.3727
Training Epoch: 4 [20224/50048]	Loss: 2.3539
Training Epoch: 4 [20352/50048]	Loss: 2.2484
Training Epoch: 4 [20480/50048]	Loss: 2.5526
Training Epoch: 4 [20608/50048]	Loss: 2.3203
Training Epoch: 4 [20736/50048]	Loss: 2.3740
Training Epoch: 4 [20864/50048]	Loss: 2.3070
Training Epoch: 4 [20992/50048]	Loss: 2.2168
Training Epoch: 4 [21120/50048]	Loss: 2.3894
Training Epoch: 4 [21248/50048]	Loss: 2.3787
Training Epoch: 4 [21376/50048]	Loss: 2.3008
Training Epoch: 4 [21504/50048]	Loss: 2.3981
Training Epoch: 4 [21632/50048]	Loss: 2.1753
Training Epoch: 4 [21760/50048]	Loss: 2.3823
Training Epoch: 4 [21888/50048]	Loss: 2.5930
Training Epoch: 4 [22016/50048]	Loss: 2.3681
Training Epoch: 4 [22144/50048]	Loss: 2.2498
Training Epoch: 4 [22272/50048]	Loss: 2.4437
Training Epoch: 4 [22400/50048]	Loss: 2.0795
Training Epoch: 4 [22528/50048]	Loss: 2.3237
Training Epoch: 4 [22656/50048]	Loss: 2.5638
Training Epoch: 4 [22784/50048]	Loss: 2.4162
Training Epoch: 4 [22912/50048]	Loss: 2.1223
Training Epoch: 4 [23040/50048]	Loss: 2.3692
Training Epoch: 4 [23168/50048]	Loss: 1.9338
Training Epoch: 4 [23296/50048]	Loss: 2.3988
Training Epoch: 4 [23424/50048]	Loss: 2.3395
Training Epoch: 4 [23552/50048]	Loss: 2.4541
Training Epoch: 4 [23680/50048]	Loss: 2.3035
Training Epoch: 4 [23808/50048]	Loss: 2.4969
Training Epoch: 4 [23936/50048]	Loss: 2.6141
Training Epoch: 4 [24064/50048]	Loss: 2.2419
Training Epoch: 4 [24192/50048]	Loss: 2.3368
Training Epoch: 4 [24320/50048]	Loss: 2.3580
Training Epoch: 4 [24448/50048]	Loss: 2.3488
Training Epoch: 4 [24576/50048]	Loss: 2.3504
Training Epoch: 4 [24704/50048]	Loss: 2.4309
Training Epoch: 4 [24832/50048]	Loss: 2.3872
Training Epoch: 4 [24960/50048]	Loss: 2.2203
Training Epoch: 4 [25088/50048]	Loss: 2.5148
Training Epoch: 4 [25216/50048]	Loss: 2.2126
Training Epoch: 4 [25344/50048]	Loss: 2.4787
Training Epoch: 4 [25472/50048]	Loss: 2.1496
Training Epoch: 4 [25600/50048]	Loss: 2.1312
Training Epoch: 4 [25728/50048]	Loss: 2.1969
Training Epoch: 4 [25856/50048]	Loss: 2.3979
Training Epoch: 4 [25984/50048]	Loss: 2.1580
Training Epoch: 4 [26112/50048]	Loss: 2.4720
Training Epoch: 4 [26240/50048]	Loss: 2.4353
Training Epoch: 4 [26368/50048]	Loss: 2.3655
Training Epoch: 4 [26496/50048]	Loss: 2.3671
Training Epoch: 4 [26624/50048]	Loss: 2.0848
Training Epoch: 4 [26752/50048]	Loss: 2.4201
Training Epoch: 4 [26880/50048]	Loss: 2.4206
Training Epoch: 4 [27008/50048]	Loss: 2.3606
Training Epoch: 4 [27136/50048]	Loss: 2.5056
Training Epoch: 4 [27264/50048]	Loss: 2.2062
Training Epoch: 4 [27392/50048]	Loss: 2.1937
Training Epoch: 4 [27520/50048]	Loss: 2.5174
Training Epoch: 4 [27648/50048]	Loss: 2.5479
Training Epoch: 4 [27776/50048]	Loss: 2.3909
Training Epoch: 4 [27904/50048]	Loss: 2.4657
Training Epoch: 4 [28032/50048]	Loss: 2.3861
Training Epoch: 4 [28160/50048]	Loss: 2.2712
Training Epoch: 4 [28288/50048]	Loss: 2.4066
Training Epoch: 4 [28416/50048]	Loss: 2.2197
Training Epoch: 4 [28544/50048]	Loss: 2.1406
Training Epoch: 4 [28672/50048]	Loss: 2.4313
Training Epoch: 4 [28800/50048]	Loss: 2.5093
Training Epoch: 4 [28928/50048]	Loss: 2.1333
Training Epoch: 4 [29056/50048]	Loss: 2.5151
Training Epoch: 4 [29184/50048]	Loss: 2.3205
Training Epoch: 4 [29312/50048]	Loss: 2.2143
Training Epoch: 4 [29440/50048]	Loss: 2.2100
Training Epoch: 4 [29568/50048]	Loss: 2.2862
Training Epoch: 4 [29696/50048]	Loss: 2.3097
Training Epoch: 4 [29824/50048]	Loss: 2.2091
Training Epoch: 4 [29952/50048]	Loss: 2.3723
Training Epoch: 4 [30080/50048]	Loss: 2.4106
Training Epoch: 4 [30208/50048]	Loss: 2.5515
Training Epoch: 4 [30336/50048]	Loss: 2.3650
Training Epoch: 4 [30464/50048]	Loss: 2.3482
Training Epoch: 4 [30592/50048]	Loss: 2.2321
Training Epoch: 4 [30720/50048]	Loss: 2.1629
Training Epoch: 4 [30848/50048]	Loss: 2.1260
Training Epoch: 4 [30976/50048]	Loss: 2.5008
Training Epoch: 4 [31104/50048]	Loss: 2.3783
Training Epoch: 4 [31232/50048]	Loss: 2.1402
Training Epoch: 4 [31360/50048]	Loss: 2.2272
Training Epoch: 4 [31488/50048]	Loss: 2.2812
Training Epoch: 4 [31616/50048]	Loss: 2.2891
Training Epoch: 4 [31744/50048]	Loss: 2.3607
Training Epoch: 4 [31872/50048]	Loss: 2.3607
Training Epoch: 4 [32000/50048]	Loss: 2.5028
Training Epoch: 4 [32128/50048]	Loss: 2.2482
Training Epoch: 4 [32256/50048]	Loss: 2.3576
Training Epoch: 4 [32384/50048]	Loss: 2.4097
Training Epoch: 4 [32512/50048]	Loss: 2.1234
Training Epoch: 4 [32640/50048]	Loss: 2.4088
Training Epoch: 4 [32768/50048]	Loss: 2.1401
Training Epoch: 4 [32896/50048]	Loss: 2.6594
Training Epoch: 4 [33024/50048]	Loss: 2.5054
Training Epoch: 4 [33152/50048]	Loss: 2.2241
Training Epoch: 4 [33280/50048]	Loss: 2.0738
Training Epoch: 4 [33408/50048]	Loss: 2.2603
Training Epoch: 4 [33536/50048]	Loss: 2.4456
Training Epoch: 4 [33664/50048]	Loss: 2.1060
Training Epoch: 4 [33792/50048]	Loss: 2.1954
Training Epoch: 4 [33920/50048]	Loss: 2.0878
Training Epoch: 4 [34048/50048]	Loss: 2.1238
Training Epoch: 4 [34176/50048]	Loss: 2.2478
Training Epoch: 4 [34304/50048]	Loss: 2.6005
Training Epoch: 4 [34432/50048]	Loss: 2.2321
Training Epoch: 4 [34560/50048]	Loss: 2.1220
Training Epoch: 4 [34688/50048]	Loss: 2.2155
Training Epoch: 4 [34816/50048]	Loss: 2.2948
Training Epoch: 4 [34944/50048]	Loss: 2.4388
Training Epoch: 4 [35072/50048]	Loss: 2.0760
Training Epoch: 4 [35200/50048]	Loss: 2.3311
Training Epoch: 4 [35328/50048]	Loss: 2.0050
Training Epoch: 4 [35456/50048]	Loss: 2.2731
Training Epoch: 4 [35584/50048]	Loss: 2.1682
Training Epoch: 4 [35712/50048]	Loss: 2.1323
Training Epoch: 4 [35840/50048]	Loss: 2.4779
Training Epoch: 4 [35968/50048]	Loss: 2.1075
Training Epoch: 4 [36096/50048]	Loss: 2.3254
Training Epoch: 4 [36224/50048]	Loss: 2.1435
Training Epoch: 4 [36352/50048]	Loss: 2.1666
Training Epoch: 4 [36480/50048]	Loss: 2.5347
Training Epoch: 4 [36608/50048]	Loss: 2.1483
Training Epoch: 4 [36736/50048]	Loss: 2.2517
Training Epoch: 4 [36864/50048]	Loss: 2.1766
Training Epoch: 4 [36992/50048]	Loss: 2.4688
Training Epoch: 4 [37120/50048]	Loss: 2.5504
Training Epoch: 4 [37248/50048]	Loss: 2.5095
Training Epoch: 4 [37376/50048]	Loss: 2.5658
Training Epoch: 4 [37504/50048]	Loss: 2.2565
Training Epoch: 4 [37632/50048]	Loss: 2.4344
Training Epoch: 4 [37760/50048]	Loss: 2.0760
Training Epoch: 4 [37888/50048]	Loss: 2.4827
Training Epoch: 4 [38016/50048]	Loss: 2.1731
Training Epoch: 4 [38144/50048]	Loss: 2.3788
Training Epoch: 4 [38272/50048]	Loss: 2.2269
Training Epoch: 4 [38400/50048]	Loss: 2.0518
Training Epoch: 4 [38528/50048]	Loss: 2.3472
Training Epoch: 4 [38656/50048]	Loss: 2.1684
Training Epoch: 4 [38784/50048]	Loss: 2.2592
Training Epoch: 4 [38912/50048]	Loss: 2.3664
Training Epoch: 4 [39040/50048]	Loss: 1.9412
Training Epoch: 4 [39168/50048]	Loss: 2.3841
Training Epoch: 4 [39296/50048]	Loss: 2.2379
Training Epoch: 4 [39424/50048]	Loss: 2.4032
Training Epoch: 4 [39552/50048]	Loss: 2.4162
Training Epoch: 4 [39680/50048]	Loss: 2.2201
Training Epoch: 4 [39808/50048]	Loss: 2.2609
Training Epoch: 4 [39936/50048]	Loss: 2.3184
Training Epoch: 4 [40064/50048]	Loss: 2.3584
Training Epoch: 4 [40192/50048]	Loss: 2.2151
Training Epoch: 4 [40320/50048]	Loss: 2.2514
Training Epoch: 4 [40448/50048]	Loss: 2.5259
Training Epoch: 4 [40576/50048]	Loss: 2.1078
Training Epoch: 4 [40704/50048]	Loss: 2.1946
Training Epoch: 4 [40832/50048]	Loss: 2.0160
Training Epoch: 4 [40960/50048]	Loss: 2.2468
Training Epoch: 4 [41088/50048]	Loss: 2.1050
Training Epoch: 4 [41216/50048]	Loss: 2.3622
Training Epoch: 4 [41344/50048]	Loss: 2.2391
Training Epoch: 4 [41472/50048]	Loss: 2.2815
Training Epoch: 4 [41600/50048]	Loss: 2.2866
Training Epoch: 4 [41728/50048]	Loss: 2.1265
Training Epoch: 4 [41856/50048]	Loss: 2.2508
Training Epoch: 4 [41984/50048]	Loss: 2.1606
Training Epoch: 4 [42112/50048]	Loss: 2.0540
Training Epoch: 4 [42240/50048]	Loss: 2.0380
Training Epoch: 4 [42368/50048]	Loss: 2.4855
Training Epoch: 4 [42496/50048]	Loss: 2.2962
Training Epoch: 4 [42624/50048]	Loss: 2.4379
Training Epoch: 4 [42752/50048]	Loss: 2.3235
Training Epoch: 4 [42880/50048]	Loss: 2.0864
Training Epoch: 4 [43008/50048]	Loss: 2.5919
Training Epoch: 4 [43136/50048]	Loss: 2.1235
Training Epoch: 4 [43264/50048]	Loss: 2.3163
Training Epoch: 4 [43392/50048]	Loss: 2.3973
Training Epoch: 4 [43520/50048]	Loss: 2.0875
Training Epoch: 4 [43648/50048]	Loss: 2.0776
Training Epoch: 4 [43776/50048]	Loss: 1.8942
Training Epoch: 4 [43904/50048]	Loss: 2.2727
Training Epoch: 4 [44032/50048]	Loss: 2.5061
Training Epoch: 4 [44160/50048]	Loss: 2.5078
Training Epoch: 4 [44288/50048]	Loss: 2.1687
Training Epoch: 4 [44416/50048]	Loss: 2.0830
Training Epoch: 4 [44544/50048]	Loss: 2.2038
Training Epoch: 4 [44672/50048]	Loss: 2.2488
Training Epoch: 4 [44800/50048]	Loss: 2.3612
Training Epoch: 4 [44928/50048]	Loss: 2.3721
Training Epoch: 4 [45056/50048]	Loss: 2.1875
Training Epoch: 4 [45184/50048]	Loss: 2.3074
Training Epoch: 4 [45312/50048]	Loss: 2.3973
Training Epoch: 4 [45440/50048]	Loss: 2.3836
Training Epoch: 4 [45568/50048]	Loss: 2.4180
Training Epoch: 4 [45696/50048]	Loss: 2.2364
Training Epoch: 4 [45824/50048]	Loss: 2.2118
Training Epoch: 4 [45952/50048]	Loss: 2.2513
Training Epoch: 4 [46080/50048]	Loss: 2.3055
Training Epoch: 4 [46208/50048]	Loss: 2.4464
Training Epoch: 4 [46336/50048]	Loss: 2.1407
Training Epoch: 4 [46464/50048]	Loss: 2.5412
Training Epoch: 4 [46592/50048]	Loss: 2.2810
Training Epoch: 4 [46720/50048]	Loss: 2.4049
2022-12-06 03:44:52,702 [ZeusDataLoader(train)] train epoch 5 done: time=86.50 energy=10519.66
2022-12-06 03:44:52,703 [ZeusDataLoader(eval)] Epoch 5 begin.
Training Epoch: 4 [46848/50048]	Loss: 2.3124
Training Epoch: 4 [46976/50048]	Loss: 2.4968
Training Epoch: 4 [47104/50048]	Loss: 2.2670
Training Epoch: 4 [47232/50048]	Loss: 2.5217
Training Epoch: 4 [47360/50048]	Loss: 2.3658
Training Epoch: 4 [47488/50048]	Loss: 2.1797
Training Epoch: 4 [47616/50048]	Loss: 2.1564
Training Epoch: 4 [47744/50048]	Loss: 2.2053
Training Epoch: 4 [47872/50048]	Loss: 2.1602
Training Epoch: 4 [48000/50048]	Loss: 2.4338
Training Epoch: 4 [48128/50048]	Loss: 2.4673
Training Epoch: 4 [48256/50048]	Loss: 2.1455
Training Epoch: 4 [48384/50048]	Loss: 2.5157
Training Epoch: 4 [48512/50048]	Loss: 2.2498
Training Epoch: 4 [48640/50048]	Loss: 2.3169
Training Epoch: 4 [48768/50048]	Loss: 2.1420
Training Epoch: 4 [48896/50048]	Loss: 2.5885
Training Epoch: 4 [49024/50048]	Loss: 2.2155
Training Epoch: 4 [49152/50048]	Loss: 2.0630
Training Epoch: 4 [49280/50048]	Loss: 2.2859
Training Epoch: 4 [49408/50048]	Loss: 2.2230
Training Epoch: 4 [49536/50048]	Loss: 2.4839
Training Epoch: 4 [49664/50048]	Loss: 2.2748
Training Epoch: 4 [49792/50048]	Loss: 2.3186
Training Epoch: 4 [49920/50048]	Loss: 2.5533
Training Epoch: 4 [50048/50048]	Loss: 2.4071
2022-12-06 08:44:56.378 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 03:44:56,387 [ZeusDataLoader(eval)] eval epoch 5 done: time=3.67 energy=441.74
2022-12-06 03:44:56,388 [ZeusDataLoader(train)] Up to epoch 5: time=454.06, energy=54647.24, cost=67054.29
2022-12-06 03:44:56,388 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 03:44:56,388 [ZeusDataLoader(train)] Expected next epoch: time=543.86, energy=65445.26, cost=80310.68
2022-12-06 03:44:56,389 [ZeusDataLoader(train)] Epoch 6 begin.
Validation Epoch: 4, Average loss: 0.0182, Accuracy: 0.3829
2022-12-06 03:44:56,532 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 03:44:56,533 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 08:44:56.537 [ZeusMonitor] Monitor started.
2022-12-06 08:44:56.537 [ZeusMonitor] Running indefinitely. 2022-12-06 08:44:56.537 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 08:44:56.537 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e6+gpu0.power.log
Training Epoch: 5 [128/50048]	Loss: 2.1819
Training Epoch: 5 [256/50048]	Loss: 2.1589
Training Epoch: 5 [384/50048]	Loss: 2.0831
Training Epoch: 5 [512/50048]	Loss: 2.0086
Training Epoch: 5 [640/50048]	Loss: 2.0779
Training Epoch: 5 [768/50048]	Loss: 1.8666
Training Epoch: 5 [896/50048]	Loss: 2.0279
Training Epoch: 5 [1024/50048]	Loss: 1.9438
Training Epoch: 5 [1152/50048]	Loss: 2.2665
Training Epoch: 5 [1280/50048]	Loss: 2.1328
Training Epoch: 5 [1408/50048]	Loss: 2.2638
Training Epoch: 5 [1536/50048]	Loss: 2.2807
Training Epoch: 5 [1664/50048]	Loss: 2.1507
Training Epoch: 5 [1792/50048]	Loss: 2.2043
Training Epoch: 5 [1920/50048]	Loss: 1.8078
Training Epoch: 5 [2048/50048]	Loss: 2.1277
Training Epoch: 5 [2176/50048]	Loss: 2.3201
Training Epoch: 5 [2304/50048]	Loss: 2.1512
Training Epoch: 5 [2432/50048]	Loss: 2.1823
Training Epoch: 5 [2560/50048]	Loss: 2.2664
Training Epoch: 5 [2688/50048]	Loss: 2.3915
Training Epoch: 5 [2816/50048]	Loss: 2.4282
Training Epoch: 5 [2944/50048]	Loss: 2.2810
Training Epoch: 5 [3072/50048]	Loss: 2.3270
Training Epoch: 5 [3200/50048]	Loss: 2.0120
Training Epoch: 5 [3328/50048]	Loss: 2.4869
Training Epoch: 5 [3456/50048]	Loss: 1.9560
Training Epoch: 5 [3584/50048]	Loss: 2.2492
Training Epoch: 5 [3712/50048]	Loss: 2.2019
Training Epoch: 5 [3840/50048]	Loss: 2.1383
Training Epoch: 5 [3968/50048]	Loss: 1.8321
Training Epoch: 5 [4096/50048]	Loss: 2.4676
Training Epoch: 5 [4224/50048]	Loss: 2.0101
Training Epoch: 5 [4352/50048]	Loss: 2.0435
Training Epoch: 5 [4480/50048]	Loss: 1.9463
Training Epoch: 5 [4608/50048]	Loss: 2.2227
Training Epoch: 5 [4736/50048]	Loss: 2.0170
Training Epoch: 5 [4864/50048]	Loss: 2.0151
Training Epoch: 5 [4992/50048]	Loss: 2.2406
Training Epoch: 5 [5120/50048]	Loss: 2.4745
Training Epoch: 5 [5248/50048]	Loss: 1.9820
Training Epoch: 5 [5376/50048]	Loss: 2.2558
Training Epoch: 5 [5504/50048]	Loss: 2.1792
Training Epoch: 5 [5632/50048]	Loss: 2.4341
Training Epoch: 5 [5760/50048]	Loss: 2.1510
Training Epoch: 5 [5888/50048]	Loss: 2.1618
Training Epoch: 5 [6016/50048]	Loss: 2.0167
Training Epoch: 5 [6144/50048]	Loss: 2.1758
Training Epoch: 5 [6272/50048]	Loss: 2.3574
Training Epoch: 5 [6400/50048]	Loss: 2.0805
Training Epoch: 5 [6528/50048]	Loss: 1.9295
Training Epoch: 5 [6656/50048]	Loss: 1.7811
Training Epoch: 5 [6784/50048]	Loss: 2.3504
Training Epoch: 5 [6912/50048]	Loss: 2.1560
Training Epoch: 5 [7040/50048]	Loss: 2.1440
Training Epoch: 5 [7168/50048]	Loss: 2.3143
Training Epoch: 5 [7296/50048]	Loss: 2.1422
Training Epoch: 5 [7424/50048]	Loss: 2.4452
Training Epoch: 5 [7552/50048]	Loss: 2.0172
Training Epoch: 5 [7680/50048]	Loss: 2.1271
Training Epoch: 5 [7808/50048]	Loss: 1.9208
Training Epoch: 5 [7936/50048]	Loss: 2.0282
Training Epoch: 5 [8064/50048]	Loss: 2.1206
Training Epoch: 5 [8192/50048]	Loss: 2.1666
Training Epoch: 5 [8320/50048]	Loss: 2.2057
Training Epoch: 5 [8448/50048]	Loss: 1.8636
Training Epoch: 5 [8576/50048]	Loss: 2.2212
Training Epoch: 5 [8704/50048]	Loss: 2.1317
Training Epoch: 5 [8832/50048]	Loss: 2.1283
Training Epoch: 5 [8960/50048]	Loss: 2.1890
Training Epoch: 5 [9088/50048]	Loss: 2.0972
Training Epoch: 5 [9216/50048]	Loss: 2.1739
Training Epoch: 5 [9344/50048]	Loss: 2.0615
Training Epoch: 5 [9472/50048]	Loss: 2.2086
Training Epoch: 5 [9600/50048]	Loss: 2.0856
Training Epoch: 5 [9728/50048]	Loss: 2.1231
Training Epoch: 5 [9856/50048]	Loss: 2.4472
Training Epoch: 5 [9984/50048]	Loss: 2.1626
Training Epoch: 5 [10112/50048]	Loss: 2.4519
Training Epoch: 5 [10240/50048]	Loss: 2.3023
Training Epoch: 5 [10368/50048]	Loss: 2.0967
Training Epoch: 5 [10496/50048]	Loss: 2.1618
Training Epoch: 5 [10624/50048]	Loss: 2.1696
Training Epoch: 5 [10752/50048]	Loss: 2.2322
Training Epoch: 5 [10880/50048]	Loss: 2.1500
Training Epoch: 5 [11008/50048]	Loss: 2.1919
Training Epoch: 5 [11136/50048]	Loss: 2.2569
Training Epoch: 5 [11264/50048]	Loss: 2.0660
Training Epoch: 5 [11392/50048]	Loss: 2.1620
Training Epoch: 5 [11520/50048]	Loss: 1.8023
Training Epoch: 5 [11648/50048]	Loss: 2.3658
Training Epoch: 5 [11776/50048]	Loss: 2.1888
Training Epoch: 5 [11904/50048]	Loss: 2.1647
Training Epoch: 5 [12032/50048]	Loss: 2.2400
Training Epoch: 5 [12160/50048]	Loss: 1.9432
Training Epoch: 5 [12288/50048]	Loss: 2.2231
Training Epoch: 5 [12416/50048]	Loss: 2.0417
Training Epoch: 5 [12544/50048]	Loss: 2.2010
Training Epoch: 5 [12672/50048]	Loss: 2.2416
Training Epoch: 5 [12800/50048]	Loss: 2.0087
Training Epoch: 5 [12928/50048]	Loss: 2.1836
Training Epoch: 5 [13056/50048]	Loss: 2.2250
Training Epoch: 5 [13184/50048]	Loss: 2.5494
Training Epoch: 5 [13312/50048]	Loss: 2.1351
Training Epoch: 5 [13440/50048]	Loss: 2.1345
Training Epoch: 5 [13568/50048]	Loss: 1.9583
Training Epoch: 5 [13696/50048]	Loss: 2.2551
Training Epoch: 5 [13824/50048]	Loss: 1.9817
Training Epoch: 5 [13952/50048]	Loss: 2.0494
Training Epoch: 5 [14080/50048]	Loss: 2.2277
Training Epoch: 5 [14208/50048]	Loss: 2.3282
Training Epoch: 5 [14336/50048]	Loss: 2.1813
Training Epoch: 5 [14464/50048]	Loss: 2.5166
Training Epoch: 5 [14592/50048]	Loss: 2.1838
Training Epoch: 5 [14720/50048]	Loss: 2.0982
Training Epoch: 5 [14848/50048]	Loss: 2.2270
Training Epoch: 5 [14976/50048]	Loss: 2.1476
Training Epoch: 5 [15104/50048]	Loss: 2.1287
Training Epoch: 5 [15232/50048]	Loss: 2.4667
Training Epoch: 5 [15360/50048]	Loss: 2.1597
Training Epoch: 5 [15488/50048]	Loss: 1.9928
Training Epoch: 5 [15616/50048]	Loss: 2.0977
Training Epoch: 5 [15744/50048]	Loss: 2.2655
Training Epoch: 5 [15872/50048]	Loss: 2.1474
Training Epoch: 5 [16000/50048]	Loss: 2.3922
Training Epoch: 5 [16128/50048]	Loss: 2.0296
Training Epoch: 5 [16256/50048]	Loss: 2.0479
Training Epoch: 5 [16384/50048]	Loss: 2.2778
Training Epoch: 5 [16512/50048]	Loss: 2.2966
Training Epoch: 5 [16640/50048]	Loss: 2.1185
Training Epoch: 5 [16768/50048]	Loss: 2.2621
Training Epoch: 5 [16896/50048]	Loss: 2.1369
Training Epoch: 5 [17024/50048]	Loss: 2.4491
Training Epoch: 5 [17152/50048]	Loss: 2.3304
Training Epoch: 5 [17280/50048]	Loss: 2.2387
Training Epoch: 5 [17408/50048]	Loss: 2.0689
Training Epoch: 5 [17536/50048]	Loss: 2.3387
Training Epoch: 5 [17664/50048]	Loss: 2.4368
Training Epoch: 5 [17792/50048]	Loss: 2.0748
Training Epoch: 5 [17920/50048]	Loss: 2.1401
Training Epoch: 5 [18048/50048]	Loss: 2.1998
Training Epoch: 5 [18176/50048]	Loss: 1.9818
Training Epoch: 5 [18304/50048]	Loss: 1.9580
Training Epoch: 5 [18432/50048]	Loss: 2.0521
Training Epoch: 5 [18560/50048]	Loss: 2.2152
Training Epoch: 5 [18688/50048]	Loss: 2.4793
Training Epoch: 5 [18816/50048]	Loss: 2.1705
Training Epoch: 5 [18944/50048]	Loss: 2.3026
Training Epoch: 5 [19072/50048]	Loss: 1.9644
Training Epoch: 5 [19200/50048]	Loss: 2.3637
Training Epoch: 5 [19328/50048]	Loss: 2.0412
Training Epoch: 5 [19456/50048]	Loss: 2.1099
Training Epoch: 5 [19584/50048]	Loss: 2.3466
Training Epoch: 5 [19712/50048]	Loss: 2.2494
Training Epoch: 5 [19840/50048]	Loss: 2.1910
Training Epoch: 5 [19968/50048]	Loss: 2.0141
Training Epoch: 5 [20096/50048]	Loss: 2.2802
Training Epoch: 5 [20224/50048]	Loss: 2.2380
Training Epoch: 5 [20352/50048]	Loss: 2.3301
Training Epoch: 5 [20480/50048]	Loss: 2.0307
Training Epoch: 5 [20608/50048]	Loss: 2.1132
Training Epoch: 5 [20736/50048]	Loss: 2.0511
Training Epoch: 5 [20864/50048]	Loss: 2.2912
Training Epoch: 5 [20992/50048]	Loss: 2.2262
Training Epoch: 5 [21120/50048]	Loss: 1.9053
Training Epoch: 5 [21248/50048]	Loss: 2.1686
Training Epoch: 5 [21376/50048]	Loss: 2.1045
Training Epoch: 5 [21504/50048]	Loss: 2.3233
Training Epoch: 5 [21632/50048]	Loss: 2.0574
Training Epoch: 5 [21760/50048]	Loss: 2.2912
Training Epoch: 5 [21888/50048]	Loss: 2.4717
Training Epoch: 5 [22016/50048]	Loss: 2.0604
Training Epoch: 5 [22144/50048]	Loss: 2.2970
Training Epoch: 5 [22272/50048]	Loss: 2.1060
Training Epoch: 5 [22400/50048]	Loss: 2.1702
Training Epoch: 5 [22528/50048]	Loss: 2.3479
Training Epoch: 5 [22656/50048]	Loss: 2.0720
Training Epoch: 5 [22784/50048]	Loss: 2.1792
Training Epoch: 5 [22912/50048]	Loss: 2.2770
Training Epoch: 5 [23040/50048]	Loss: 2.2193
Training Epoch: 5 [23168/50048]	Loss: 1.9362
Training Epoch: 5 [23296/50048]	Loss: 2.2577
Training Epoch: 5 [23424/50048]	Loss: 2.2212
Training Epoch: 5 [23552/50048]	Loss: 2.2914
Training Epoch: 5 [23680/50048]	Loss: 2.0037
Training Epoch: 5 [23808/50048]	Loss: 2.4917
Training Epoch: 5 [23936/50048]	Loss: 2.1995
Training Epoch: 5 [24064/50048]	Loss: 2.1945
Training Epoch: 5 [24192/50048]	Loss: 2.0890
Training Epoch: 5 [24320/50048]	Loss: 2.2684
Training Epoch: 5 [24448/50048]	Loss: 2.1392
Training Epoch: 5 [24576/50048]	Loss: 1.8652
Training Epoch: 5 [24704/50048]	Loss: 2.1444
Training Epoch: 5 [24832/50048]	Loss: 1.9370
Training Epoch: 5 [24960/50048]	Loss: 2.1410
Training Epoch: 5 [25088/50048]	Loss: 2.2703
Training Epoch: 5 [25216/50048]	Loss: 1.9952
Training Epoch: 5 [25344/50048]	Loss: 2.2682
Training Epoch: 5 [25472/50048]	Loss: 2.1208
Training Epoch: 5 [25600/50048]	Loss: 2.2531
Training Epoch: 5 [25728/50048]	Loss: 2.1535
Training Epoch: 5 [25856/50048]	Loss: 2.1591
Training Epoch: 5 [25984/50048]	Loss: 1.9431
Training Epoch: 5 [26112/50048]	Loss: 1.9157
Training Epoch: 5 [26240/50048]	Loss: 2.1992
Training Epoch: 5 [26368/50048]	Loss: 1.9558
Training Epoch: 5 [26496/50048]	Loss: 1.9862
Training Epoch: 5 [26624/50048]	Loss: 2.1997
Training Epoch: 5 [26752/50048]	Loss: 2.3267
Training Epoch: 5 [26880/50048]	Loss: 2.0671
Training Epoch: 5 [27008/50048]	Loss: 2.2999
Training Epoch: 5 [27136/50048]	Loss: 1.8715
Training Epoch: 5 [27264/50048]	Loss: 2.2365
Training Epoch: 5 [27392/50048]	Loss: 2.1365
Training Epoch: 5 [27520/50048]	Loss: 2.2124
Training Epoch: 5 [27648/50048]	Loss: 1.9771
Training Epoch: 5 [27776/50048]	Loss: 1.8188
Training Epoch: 5 [27904/50048]	Loss: 2.3147
Training Epoch: 5 [28032/50048]	Loss: 2.0280
Training Epoch: 5 [28160/50048]	Loss: 2.1474
Training Epoch: 5 [28288/50048]	Loss: 1.9347
Training Epoch: 5 [28416/50048]	Loss: 2.0001
Training Epoch: 5 [28544/50048]	Loss: 2.0328
Training Epoch: 5 [28672/50048]	Loss: 2.5961
Training Epoch: 5 [28800/50048]	Loss: 1.9558
Training Epoch: 5 [28928/50048]	Loss: 2.1453
Training Epoch: 5 [29056/50048]	Loss: 2.3298
Training Epoch: 5 [29184/50048]	Loss: 2.0190
Training Epoch: 5 [29312/50048]	Loss: 2.2298
Training Epoch: 5 [29440/50048]	Loss: 2.3165
Training Epoch: 5 [29568/50048]	Loss: 2.0043
Training Epoch: 5 [29696/50048]	Loss: 2.1966
Training Epoch: 5 [29824/50048]	Loss: 2.0640
Training Epoch: 5 [29952/50048]	Loss: 2.1665
Training Epoch: 5 [30080/50048]	Loss: 1.9769
Training Epoch: 5 [30208/50048]	Loss: 2.1605
Training Epoch: 5 [30336/50048]	Loss: 2.0563
Training Epoch: 5 [30464/50048]	Loss: 2.0434
Training Epoch: 5 [30592/50048]	Loss: 2.0276
Training Epoch: 5 [30720/50048]	Loss: 1.9932
Training Epoch: 5 [30848/50048]	Loss: 2.1560
Training Epoch: 5 [30976/50048]	Loss: 1.9466
Training Epoch: 5 [31104/50048]	Loss: 2.1635
Training Epoch: 5 [31232/50048]	Loss: 2.2527
Training Epoch: 5 [31360/50048]	Loss: 1.9169
Training Epoch: 5 [31488/50048]	Loss: 2.1397
Training Epoch: 5 [31616/50048]	Loss: 2.0968
Training Epoch: 5 [31744/50048]	Loss: 2.1869
Training Epoch: 5 [31872/50048]	Loss: 2.1414
Training Epoch: 5 [32000/50048]	Loss: 2.1253
Training Epoch: 5 [32128/50048]	Loss: 2.3797
Training Epoch: 5 [32256/50048]	Loss: 1.8936
Training Epoch: 5 [32384/50048]	Loss: 2.2205
Training Epoch: 5 [32512/50048]	Loss: 2.4278
Training Epoch: 5 [32640/50048]	Loss: 2.0254
Training Epoch: 5 [32768/50048]	Loss: 1.8737
Training Epoch: 5 [32896/50048]	Loss: 2.3086
Training Epoch: 5 [33024/50048]	Loss: 2.0514
Training Epoch: 5 [33152/50048]	Loss: 2.1918
Training Epoch: 5 [33280/50048]	Loss: 2.1514
Training Epoch: 5 [33408/50048]	Loss: 1.9634
Training Epoch: 5 [33536/50048]	Loss: 1.9012
Training Epoch: 5 [33664/50048]	Loss: 1.8908
Training Epoch: 5 [33792/50048]	Loss: 1.8485
Training Epoch: 5 [33920/50048]	Loss: 1.9870
Training Epoch: 5 [34048/50048]	Loss: 2.1699
Training Epoch: 5 [34176/50048]	Loss: 1.8584
Training Epoch: 5 [34304/50048]	Loss: 2.3025
Training Epoch: 5 [34432/50048]	Loss: 2.0624
Training Epoch: 5 [34560/50048]	Loss: 1.9582
Training Epoch: 5 [34688/50048]	Loss: 2.2683
Training Epoch: 5 [34816/50048]	Loss: 2.0129
Training Epoch: 5 [34944/50048]	Loss: 2.2177
Training Epoch: 5 [35072/50048]	Loss: 2.2434
Training Epoch: 5 [35200/50048]	Loss: 1.9130
Training Epoch: 5 [35328/50048]	Loss: 2.2609
Training Epoch: 5 [35456/50048]	Loss: 1.9837
Training Epoch: 5 [35584/50048]	Loss: 2.2615
Training Epoch: 5 [35712/50048]	Loss: 2.2676
Training Epoch: 5 [35840/50048]	Loss: 2.0972
Training Epoch: 5 [35968/50048]	Loss: 2.0626
Training Epoch: 5 [36096/50048]	Loss: 2.0090
Training Epoch: 5 [36224/50048]	Loss: 1.7605
Training Epoch: 5 [36352/50048]	Loss: 1.9653
Training Epoch: 5 [36480/50048]	Loss: 2.0571
Training Epoch: 5 [36608/50048]	Loss: 2.1677
Training Epoch: 5 [36736/50048]	Loss: 2.0670
Training Epoch: 5 [36864/50048]	Loss: 2.2030
Training Epoch: 5 [36992/50048]	Loss: 2.3347
Training Epoch: 5 [37120/50048]	Loss: 1.9293
Training Epoch: 5 [37248/50048]	Loss: 2.2538
Training Epoch: 5 [37376/50048]	Loss: 1.7960
Training Epoch: 5 [37504/50048]	Loss: 1.8358
Training Epoch: 5 [37632/50048]	Loss: 2.1427
Training Epoch: 5 [37760/50048]	Loss: 2.1976
Training Epoch: 5 [37888/50048]	Loss: 2.1978
Training Epoch: 5 [38016/50048]	Loss: 2.2360
Training Epoch: 5 [38144/50048]	Loss: 2.1251
Training Epoch: 5 [38272/50048]	Loss: 2.1251
Training Epoch: 5 [38400/50048]	Loss: 2.1315
Training Epoch: 5 [38528/50048]	Loss: 2.0235
Training Epoch: 5 [38656/50048]	Loss: 1.8944
Training Epoch: 5 [38784/50048]	Loss: 2.0877
Training Epoch: 5 [38912/50048]	Loss: 1.9669
Training Epoch: 5 [39040/50048]	Loss: 2.0186
Training Epoch: 5 [39168/50048]	Loss: 2.1660
Training Epoch: 5 [39296/50048]	Loss: 2.1414
Training Epoch: 5 [39424/50048]	Loss: 2.0664
Training Epoch: 5 [39552/50048]	Loss: 2.1458
Training Epoch: 5 [39680/50048]	Loss: 2.1816
Training Epoch: 5 [39808/50048]	Loss: 2.2579
Training Epoch: 5 [39936/50048]	Loss: 2.0569
Training Epoch: 5 [40064/50048]	Loss: 2.0518
Training Epoch: 5 [40192/50048]	Loss: 1.8563
Training Epoch: 5 [40320/50048]	Loss: 2.3228
Training Epoch: 5 [40448/50048]	Loss: 1.9074
Training Epoch: 5 [40576/50048]	Loss: 2.0542
Training Epoch: 5 [40704/50048]	Loss: 2.1468
Training Epoch: 5 [40832/50048]	Loss: 2.0425
Training Epoch: 5 [40960/50048]	Loss: 1.8534
Training Epoch: 5 [41088/50048]	Loss: 2.2845
Training Epoch: 5 [41216/50048]	Loss: 2.0259
Training Epoch: 5 [41344/50048]	Loss: 2.1338
Training Epoch: 5 [41472/50048]	Loss: 2.2275
Training Epoch: 5 [41600/50048]	Loss: 1.7654
Training Epoch: 5 [41728/50048]	Loss: 2.2444
Training Epoch: 5 [41856/50048]	Loss: 1.9932
Training Epoch: 5 [41984/50048]	Loss: 2.0489
Training Epoch: 5 [42112/50048]	Loss: 1.9665
Training Epoch: 5 [42240/50048]	Loss: 2.3257
Training Epoch: 5 [42368/50048]	Loss: 1.6960
Training Epoch: 5 [42496/50048]	Loss: 2.3224
Training Epoch: 5 [42624/50048]	Loss: 2.1299
Training Epoch: 5 [42752/50048]	Loss: 2.1866
Training Epoch: 5 [42880/50048]	Loss: 2.0958
Training Epoch: 5 [43008/50048]	Loss: 1.8900
Training Epoch: 5 [43136/50048]	Loss: 1.8554
Training Epoch: 5 [43264/50048]	Loss: 2.2333
Training Epoch: 5 [43392/50048]	Loss: 2.1386
Training Epoch: 5 [43520/50048]	Loss: 2.0103
Training Epoch: 5 [43648/50048]	Loss: 2.3105
Training Epoch: 5 [43776/50048]	Loss: 2.4197
Training Epoch: 5 [43904/50048]	Loss: 2.0230
Training Epoch: 5 [44032/50048]	Loss: 2.0422
Training Epoch: 5 [44160/50048]	Loss: 1.9307
Training Epoch: 5 [44288/50048]	Loss: 2.2639
Training Epoch: 5 [44416/50048]	Loss: 2.0135
Training Epoch: 5 [44544/50048]	Loss: 2.1135
Training Epoch: 5 [44672/50048]	Loss: 2.0365
Training Epoch: 5 [44800/50048]	Loss: 2.2302
Training Epoch: 5 [44928/50048]	Loss: 2.2568
Training Epoch: 5 [45056/50048]	Loss: 2.3334
Training Epoch: 5 [45184/50048]	Loss: 2.1739
Training Epoch: 5 [45312/50048]	Loss: 2.2481
Training Epoch: 5 [45440/50048]	Loss: 2.1352
Training Epoch: 5 [45568/50048]	Loss: 1.7551
Training Epoch: 5 [45696/50048]	Loss: 2.1934
Training Epoch: 5 [45824/50048]	Loss: 1.9062
Training Epoch: 5 [45952/50048]	Loss: 2.0382
Training Epoch: 5 [46080/50048]	Loss: 2.0677
Training Epoch: 5 [46208/50048]	Loss: 2.1523
Training Epoch: 5 [46336/50048]	Loss: 2.1690
Training Epoch: 5 [46464/50048]	Loss: 2.1794
Training Epoch: 5 [46592/50048]	Loss: 1.8926
Training Epoch: 5 [46720/50048]	Loss: 2.0051
2022-12-06 03:46:22,796 [ZeusDataLoader(train)] train epoch 6 done: time=86.40 energy=10501.78
2022-12-06 03:46:22,798 [ZeusDataLoader(eval)] Epoch 6 begin.
Training Epoch: 5 [46848/50048]	Loss: 1.9696
Training Epoch: 5 [46976/50048]	Loss: 2.2425
Training Epoch: 5 [47104/50048]	Loss: 2.1292
Training Epoch: 5 [47232/50048]	Loss: 1.9598
Training Epoch: 5 [47360/50048]	Loss: 1.7619
Training Epoch: 5 [47488/50048]	Loss: 1.8927
Training Epoch: 5 [47616/50048]	Loss: 2.0445
Training Epoch: 5 [47744/50048]	Loss: 2.3575
Training Epoch: 5 [47872/50048]	Loss: 2.1024
Training Epoch: 5 [48000/50048]	Loss: 1.9830
Training Epoch: 5 [48128/50048]	Loss: 2.1210
Training Epoch: 5 [48256/50048]	Loss: 1.8132
Training Epoch: 5 [48384/50048]	Loss: 2.3246
Training Epoch: 5 [48512/50048]	Loss: 2.2167
Training Epoch: 5 [48640/50048]	Loss: 2.2621
Training Epoch: 5 [48768/50048]	Loss: 2.1550
Training Epoch: 5 [48896/50048]	Loss: 2.1092
Training Epoch: 5 [49024/50048]	Loss: 2.2867
Training Epoch: 5 [49152/50048]	Loss: 2.2394
Training Epoch: 5 [49280/50048]	Loss: 1.9406
Training Epoch: 5 [49408/50048]	Loss: 2.1764
Training Epoch: 5 [49536/50048]	Loss: 2.2007
Training Epoch: 5 [49664/50048]	Loss: 1.9584
Training Epoch: 5 [49792/50048]	Loss: 2.2836
Training Epoch: 5 [49920/50048]	Loss: 2.2084
Training Epoch: 5 [50048/50048]	Loss: 1.9158
2022-12-06 08:46:26.517 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 03:46:26,543 [ZeusDataLoader(eval)] eval epoch 6 done: time=3.74 energy=454.71
2022-12-06 03:46:26,543 [ZeusDataLoader(train)] Up to epoch 6: time=544.20, energy=65603.73, cost=80419.24
2022-12-06 03:46:26,544 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 03:46:26,544 [ZeusDataLoader(train)] Expected next epoch: time=634.00, energy=76401.74, cost=93675.62
2022-12-06 03:46:26,545 [ZeusDataLoader(train)] Epoch 7 begin.
Validation Epoch: 5, Average loss: 0.0166, Accuracy: 0.4266
2022-12-06 03:46:26,715 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 03:46:26,716 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 08:46:26.718 [ZeusMonitor] Monitor started.
2022-12-06 08:46:26.718 [ZeusMonitor] Running indefinitely. 2022-12-06 08:46:26.718 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 08:46:26.718 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e7+gpu0.power.log
Training Epoch: 6 [128/50048]	Loss: 1.9849
Training Epoch: 6 [256/50048]	Loss: 1.8363
Training Epoch: 6 [384/50048]	Loss: 2.2685
Training Epoch: 6 [512/50048]	Loss: 2.0289
Training Epoch: 6 [640/50048]	Loss: 1.9079
Training Epoch: 6 [768/50048]	Loss: 1.9064
Training Epoch: 6 [896/50048]	Loss: 1.9776
Training Epoch: 6 [1024/50048]	Loss: 1.9539
Training Epoch: 6 [1152/50048]	Loss: 1.8614
Training Epoch: 6 [1280/50048]	Loss: 2.0897
Training Epoch: 6 [1408/50048]	Loss: 2.1025
Training Epoch: 6 [1536/50048]	Loss: 1.9308
Training Epoch: 6 [1664/50048]	Loss: 2.0627
Training Epoch: 6 [1792/50048]	Loss: 1.8166
Training Epoch: 6 [1920/50048]	Loss: 1.9518
Training Epoch: 6 [2048/50048]	Loss: 2.0197
Training Epoch: 6 [2176/50048]	Loss: 2.0631
Training Epoch: 6 [2304/50048]	Loss: 2.0991
Training Epoch: 6 [2432/50048]	Loss: 2.0136
Training Epoch: 6 [2560/50048]	Loss: 2.3544
Training Epoch: 6 [2688/50048]	Loss: 1.9490
Training Epoch: 6 [2816/50048]	Loss: 2.1254
Training Epoch: 6 [2944/50048]	Loss: 1.8419
Training Epoch: 6 [3072/50048]	Loss: 1.8880
Training Epoch: 6 [3200/50048]	Loss: 2.1803
Training Epoch: 6 [3328/50048]	Loss: 2.1991
Training Epoch: 6 [3456/50048]	Loss: 1.7041
Training Epoch: 6 [3584/50048]	Loss: 2.0034
Training Epoch: 6 [3712/50048]	Loss: 1.7913
Training Epoch: 6 [3840/50048]	Loss: 2.1703
Training Epoch: 6 [3968/50048]	Loss: 1.9221
Training Epoch: 6 [4096/50048]	Loss: 2.0319
Training Epoch: 6 [4224/50048]	Loss: 2.0161
Training Epoch: 6 [4352/50048]	Loss: 1.8352
Training Epoch: 6 [4480/50048]	Loss: 2.1203
Training Epoch: 6 [4608/50048]	Loss: 1.9892
Training Epoch: 6 [4736/50048]	Loss: 1.8751
Training Epoch: 6 [4864/50048]	Loss: 1.7349
Training Epoch: 6 [4992/50048]	Loss: 2.0027
Training Epoch: 6 [5120/50048]	Loss: 2.0809
Training Epoch: 6 [5248/50048]	Loss: 1.9126
Training Epoch: 6 [5376/50048]	Loss: 1.7312
Training Epoch: 6 [5504/50048]	Loss: 2.3278
Training Epoch: 6 [5632/50048]	Loss: 2.1869
Training Epoch: 6 [5760/50048]	Loss: 2.1491
Training Epoch: 6 [5888/50048]	Loss: 1.9546
Training Epoch: 6 [6016/50048]	Loss: 1.7384
Training Epoch: 6 [6144/50048]	Loss: 2.0769
Training Epoch: 6 [6272/50048]	Loss: 2.0261
Training Epoch: 6 [6400/50048]	Loss: 1.9820
Training Epoch: 6 [6528/50048]	Loss: 2.1887
Training Epoch: 6 [6656/50048]	Loss: 2.0597
Training Epoch: 6 [6784/50048]	Loss: 2.0185
Training Epoch: 6 [6912/50048]	Loss: 1.8605
Training Epoch: 6 [7040/50048]	Loss: 2.2008
Training Epoch: 6 [7168/50048]	Loss: 2.0568
Training Epoch: 6 [7296/50048]	Loss: 1.7208
Training Epoch: 6 [7424/50048]	Loss: 1.8549
Training Epoch: 6 [7552/50048]	Loss: 1.8830
Training Epoch: 6 [7680/50048]	Loss: 1.8813
Training Epoch: 6 [7808/50048]	Loss: 2.1244
Training Epoch: 6 [7936/50048]	Loss: 1.9869
Training Epoch: 6 [8064/50048]	Loss: 2.1036
Training Epoch: 6 [8192/50048]	Loss: 1.9875
Training Epoch: 6 [8320/50048]	Loss: 2.0520
Training Epoch: 6 [8448/50048]	Loss: 2.1508
Training Epoch: 6 [8576/50048]	Loss: 2.2233
Training Epoch: 6 [8704/50048]	Loss: 2.2484
Training Epoch: 6 [8832/50048]	Loss: 2.0445
Training Epoch: 6 [8960/50048]	Loss: 1.8909
Training Epoch: 6 [9088/50048]	Loss: 2.1103
Training Epoch: 6 [9216/50048]	Loss: 2.0171
Training Epoch: 6 [9344/50048]	Loss: 2.2529
Training Epoch: 6 [9472/50048]	Loss: 2.1396
Training Epoch: 6 [9600/50048]	Loss: 2.0280
Training Epoch: 6 [9728/50048]	Loss: 2.3328
Training Epoch: 6 [9856/50048]	Loss: 1.6728
Training Epoch: 6 [9984/50048]	Loss: 1.6540
Training Epoch: 6 [10112/50048]	Loss: 1.9987
Training Epoch: 6 [10240/50048]	Loss: 2.0121
Training Epoch: 6 [10368/50048]	Loss: 2.0820
Training Epoch: 6 [10496/50048]	Loss: 1.9470
Training Epoch: 6 [10624/50048]	Loss: 2.0091
Training Epoch: 6 [10752/50048]	Loss: 1.8858
Training Epoch: 6 [10880/50048]	Loss: 1.7067
Training Epoch: 6 [11008/50048]	Loss: 2.1641
Training Epoch: 6 [11136/50048]	Loss: 2.0805
Training Epoch: 6 [11264/50048]	Loss: 1.9330
Training Epoch: 6 [11392/50048]	Loss: 1.7787
Training Epoch: 6 [11520/50048]	Loss: 2.1264
Training Epoch: 6 [11648/50048]	Loss: 2.0460
Training Epoch: 6 [11776/50048]	Loss: 1.9637
Training Epoch: 6 [11904/50048]	Loss: 1.9610
Training Epoch: 6 [12032/50048]	Loss: 1.9055
Training Epoch: 6 [12160/50048]	Loss: 2.2770
Training Epoch: 6 [12288/50048]	Loss: 2.1268
Training Epoch: 6 [12416/50048]	Loss: 1.9581
Training Epoch: 6 [12544/50048]	Loss: 1.8558
Training Epoch: 6 [12672/50048]	Loss: 1.7835
Training Epoch: 6 [12800/50048]	Loss: 1.9318
Training Epoch: 6 [12928/50048]	Loss: 2.1596
Training Epoch: 6 [13056/50048]	Loss: 1.8786
Training Epoch: 6 [13184/50048]	Loss: 2.1230
Training Epoch: 6 [13312/50048]	Loss: 2.1997
Training Epoch: 6 [13440/50048]	Loss: 2.0658
Training Epoch: 6 [13568/50048]	Loss: 1.9317
Training Epoch: 6 [13696/50048]	Loss: 1.9543
Training Epoch: 6 [13824/50048]	Loss: 2.2432
Training Epoch: 6 [13952/50048]	Loss: 2.2729
Training Epoch: 6 [14080/50048]	Loss: 1.9764
Training Epoch: 6 [14208/50048]	Loss: 1.8146
Training Epoch: 6 [14336/50048]	Loss: 2.1859
Training Epoch: 6 [14464/50048]	Loss: 1.9807
Training Epoch: 6 [14592/50048]	Loss: 1.8303
Training Epoch: 6 [14720/50048]	Loss: 1.7743
Training Epoch: 6 [14848/50048]	Loss: 1.7939
Training Epoch: 6 [14976/50048]	Loss: 1.8313
Training Epoch: 6 [15104/50048]	Loss: 1.8832
Training Epoch: 6 [15232/50048]	Loss: 2.0823
Training Epoch: 6 [15360/50048]	Loss: 1.9851
Training Epoch: 6 [15488/50048]	Loss: 1.9199
Training Epoch: 6 [15616/50048]	Loss: 2.2436
Training Epoch: 6 [15744/50048]	Loss: 1.8970
Training Epoch: 6 [15872/50048]	Loss: 2.0260
Training Epoch: 6 [16000/50048]	Loss: 1.8395
Training Epoch: 6 [16128/50048]	Loss: 1.7281
Training Epoch: 6 [16256/50048]	Loss: 2.0768
Training Epoch: 6 [16384/50048]	Loss: 2.0827
Training Epoch: 6 [16512/50048]	Loss: 1.8336
Training Epoch: 6 [16640/50048]	Loss: 2.0967
Training Epoch: 6 [16768/50048]	Loss: 2.2849
Training Epoch: 6 [16896/50048]	Loss: 1.7063
Training Epoch: 6 [17024/50048]	Loss: 1.7147
Training Epoch: 6 [17152/50048]	Loss: 1.9621
Training Epoch: 6 [17280/50048]	Loss: 2.0997
Training Epoch: 6 [17408/50048]	Loss: 2.0783
Training Epoch: 6 [17536/50048]	Loss: 1.8963
Training Epoch: 6 [17664/50048]	Loss: 1.9693
Training Epoch: 6 [17792/50048]	Loss: 2.0426
Training Epoch: 6 [17920/50048]	Loss: 1.9184
Training Epoch: 6 [18048/50048]	Loss: 2.0810
Training Epoch: 6 [18176/50048]	Loss: 2.1465
Training Epoch: 6 [18304/50048]	Loss: 1.6397
Training Epoch: 6 [18432/50048]	Loss: 2.2370
Training Epoch: 6 [18560/50048]	Loss: 2.0191
Training Epoch: 6 [18688/50048]	Loss: 1.9415
Training Epoch: 6 [18816/50048]	Loss: 1.8015
Training Epoch: 6 [18944/50048]	Loss: 2.2657
Training Epoch: 6 [19072/50048]	Loss: 2.1408
Training Epoch: 6 [19200/50048]	Loss: 2.0412
Training Epoch: 6 [19328/50048]	Loss: 1.9041
Training Epoch: 6 [19456/50048]	Loss: 1.8155
Training Epoch: 6 [19584/50048]	Loss: 2.0279
Training Epoch: 6 [19712/50048]	Loss: 1.8688
Training Epoch: 6 [19840/50048]	Loss: 2.0213
Training Epoch: 6 [19968/50048]	Loss: 2.0536
Training Epoch: 6 [20096/50048]	Loss: 1.9708
Training Epoch: 6 [20224/50048]	Loss: 1.8979
Training Epoch: 6 [20352/50048]	Loss: 1.8062
Training Epoch: 6 [20480/50048]	Loss: 2.0422
Training Epoch: 6 [20608/50048]	Loss: 1.9392
Training Epoch: 6 [20736/50048]	Loss: 1.9274
Training Epoch: 6 [20864/50048]	Loss: 1.9377
Training Epoch: 6 [20992/50048]	Loss: 2.1202
Training Epoch: 6 [21120/50048]	Loss: 2.0509
Training Epoch: 6 [21248/50048]	Loss: 2.2283
Training Epoch: 6 [21376/50048]	Loss: 1.9986
Training Epoch: 6 [21504/50048]	Loss: 2.0503
Training Epoch: 6 [21632/50048]	Loss: 1.8319
Training Epoch: 6 [21760/50048]	Loss: 2.1521
Training Epoch: 6 [21888/50048]	Loss: 2.0507
Training Epoch: 6 [22016/50048]	Loss: 2.2730
Training Epoch: 6 [22144/50048]	Loss: 1.9946
Training Epoch: 6 [22272/50048]	Loss: 1.8726
Training Epoch: 6 [22400/50048]	Loss: 1.8185
Training Epoch: 6 [22528/50048]	Loss: 1.9639
Training Epoch: 6 [22656/50048]	Loss: 2.1186
Training Epoch: 6 [22784/50048]	Loss: 2.2856
Training Epoch: 6 [22912/50048]	Loss: 1.7529
Training Epoch: 6 [23040/50048]	Loss: 2.0568
Training Epoch: 6 [23168/50048]	Loss: 1.8886
Training Epoch: 6 [23296/50048]	Loss: 2.0498
Training Epoch: 6 [23424/50048]	Loss: 2.0758
Training Epoch: 6 [23552/50048]	Loss: 1.9503
Training Epoch: 6 [23680/50048]	Loss: 1.9519
Training Epoch: 6 [23808/50048]	Loss: 2.3019
Training Epoch: 6 [23936/50048]	Loss: 2.0290
Training Epoch: 6 [24064/50048]	Loss: 1.9784
Training Epoch: 6 [24192/50048]	Loss: 1.7969
Training Epoch: 6 [24320/50048]	Loss: 2.1509
Training Epoch: 6 [24448/50048]	Loss: 2.0967
Training Epoch: 6 [24576/50048]	Loss: 2.2140
Training Epoch: 6 [24704/50048]	Loss: 1.7881
Training Epoch: 6 [24832/50048]	Loss: 2.0318
Training Epoch: 6 [24960/50048]	Loss: 1.8398
Training Epoch: 6 [25088/50048]	Loss: 2.1654
Training Epoch: 6 [25216/50048]	Loss: 1.9775
Training Epoch: 6 [25344/50048]	Loss: 1.9919
Training Epoch: 6 [25472/50048]	Loss: 1.9684
Training Epoch: 6 [25600/50048]	Loss: 1.7825
Training Epoch: 6 [25728/50048]	Loss: 2.2457
Training Epoch: 6 [25856/50048]	Loss: 1.8492
Training Epoch: 6 [25984/50048]	Loss: 1.7061
Training Epoch: 6 [26112/50048]	Loss: 2.1247
Training Epoch: 6 [26240/50048]	Loss: 2.0999
Training Epoch: 6 [26368/50048]	Loss: 2.1845
Training Epoch: 6 [26496/50048]	Loss: 1.9711
Training Epoch: 6 [26624/50048]	Loss: 1.9342
Training Epoch: 6 [26752/50048]	Loss: 1.9528
Training Epoch: 6 [26880/50048]	Loss: 2.0103
Training Epoch: 6 [27008/50048]	Loss: 1.9006
Training Epoch: 6 [27136/50048]	Loss: 2.1477
Training Epoch: 6 [27264/50048]	Loss: 1.6714
Training Epoch: 6 [27392/50048]	Loss: 1.6564
Training Epoch: 6 [27520/50048]	Loss: 1.9883
Training Epoch: 6 [27648/50048]	Loss: 2.1308
Training Epoch: 6 [27776/50048]	Loss: 2.0144
Training Epoch: 6 [27904/50048]	Loss: 1.9667
Training Epoch: 6 [28032/50048]	Loss: 1.6898
Training Epoch: 6 [28160/50048]	Loss: 1.8271
Training Epoch: 6 [28288/50048]	Loss: 1.8686
Training Epoch: 6 [28416/50048]	Loss: 1.9671
Training Epoch: 6 [28544/50048]	Loss: 1.8851
Training Epoch: 6 [28672/50048]	Loss: 1.8158
Training Epoch: 6 [28800/50048]	Loss: 1.9139
Training Epoch: 6 [28928/50048]	Loss: 2.0096
Training Epoch: 6 [29056/50048]	Loss: 2.0775
Training Epoch: 6 [29184/50048]	Loss: 2.2918
Training Epoch: 6 [29312/50048]	Loss: 1.9017
Training Epoch: 6 [29440/50048]	Loss: 1.9455
Training Epoch: 6 [29568/50048]	Loss: 2.0361
Training Epoch: 6 [29696/50048]	Loss: 1.7534
Training Epoch: 6 [29824/50048]	Loss: 1.7599
Training Epoch: 6 [29952/50048]	Loss: 1.9551
Training Epoch: 6 [30080/50048]	Loss: 2.0517
Training Epoch: 6 [30208/50048]	Loss: 1.6364
Training Epoch: 6 [30336/50048]	Loss: 1.9349
Training Epoch: 6 [30464/50048]	Loss: 2.1459
Training Epoch: 6 [30592/50048]	Loss: 1.8921
Training Epoch: 6 [30720/50048]	Loss: 1.6909
Training Epoch: 6 [30848/50048]	Loss: 1.9365
Training Epoch: 6 [30976/50048]	Loss: 2.0855
Training Epoch: 6 [31104/50048]	Loss: 2.0936
Training Epoch: 6 [31232/50048]	Loss: 1.7797
Training Epoch: 6 [31360/50048]	Loss: 1.8156
Training Epoch: 6 [31488/50048]	Loss: 2.1067
Training Epoch: 6 [31616/50048]	Loss: 2.0088
Training Epoch: 6 [31744/50048]	Loss: 1.9811
Training Epoch: 6 [31872/50048]	Loss: 1.8231
Training Epoch: 6 [32000/50048]	Loss: 1.9094
Training Epoch: 6 [32128/50048]	Loss: 2.0363
Training Epoch: 6 [32256/50048]	Loss: 2.0271
Training Epoch: 6 [32384/50048]	Loss: 2.1346
Training Epoch: 6 [32512/50048]	Loss: 2.0046
Training Epoch: 6 [32640/50048]	Loss: 1.9168
Training Epoch: 6 [32768/50048]	Loss: 1.8834
Training Epoch: 6 [32896/50048]	Loss: 1.8470
Training Epoch: 6 [33024/50048]	Loss: 1.9348
Training Epoch: 6 [33152/50048]	Loss: 2.2122
Training Epoch: 6 [33280/50048]	Loss: 2.1780
Training Epoch: 6 [33408/50048]	Loss: 1.9158
Training Epoch: 6 [33536/50048]	Loss: 2.0821
Training Epoch: 6 [33664/50048]	Loss: 2.1131
Training Epoch: 6 [33792/50048]	Loss: 2.0474
Training Epoch: 6 [33920/50048]	Loss: 1.7747
Training Epoch: 6 [34048/50048]	Loss: 2.1022
Training Epoch: 6 [34176/50048]	Loss: 2.2319
Training Epoch: 6 [34304/50048]	Loss: 1.9683
Training Epoch: 6 [34432/50048]	Loss: 1.9928
Training Epoch: 6 [34560/50048]	Loss: 1.8835
Training Epoch: 6 [34688/50048]	Loss: 1.9003
Training Epoch: 6 [34816/50048]	Loss: 2.1771
Training Epoch: 6 [34944/50048]	Loss: 1.8889
Training Epoch: 6 [35072/50048]	Loss: 2.1183
Training Epoch: 6 [35200/50048]	Loss: 2.0139
Training Epoch: 6 [35328/50048]	Loss: 2.0219
Training Epoch: 6 [35456/50048]	Loss: 2.0383
Training Epoch: 6 [35584/50048]	Loss: 1.8449
Training Epoch: 6 [35712/50048]	Loss: 1.8898
Training Epoch: 6 [35840/50048]	Loss: 2.1476
Training Epoch: 6 [35968/50048]	Loss: 1.9972
Training Epoch: 6 [36096/50048]	Loss: 1.7664
Training Epoch: 6 [36224/50048]	Loss: 1.7768
Training Epoch: 6 [36352/50048]	Loss: 2.0001
Training Epoch: 6 [36480/50048]	Loss: 2.1812
Training Epoch: 6 [36608/50048]	Loss: 2.2117
Training Epoch: 6 [36736/50048]	Loss: 2.0467
Training Epoch: 6 [36864/50048]	Loss: 2.1039
Training Epoch: 6 [36992/50048]	Loss: 1.8876
Training Epoch: 6 [37120/50048]	Loss: 2.1471
Training Epoch: 6 [37248/50048]	Loss: 2.0805
Training Epoch: 6 [37376/50048]	Loss: 2.0135
Training Epoch: 6 [37504/50048]	Loss: 1.9604
Training Epoch: 6 [37632/50048]	Loss: 2.0349
Training Epoch: 6 [37760/50048]	Loss: 2.0066
Training Epoch: 6 [37888/50048]	Loss: 1.7443
Training Epoch: 6 [38016/50048]	Loss: 2.1122
Training Epoch: 6 [38144/50048]	Loss: 2.0078
Training Epoch: 6 [38272/50048]	Loss: 1.9753
Training Epoch: 6 [38400/50048]	Loss: 2.4208
Training Epoch: 6 [38528/50048]	Loss: 1.7933
Training Epoch: 6 [38656/50048]	Loss: 1.8529
Training Epoch: 6 [38784/50048]	Loss: 2.1805
Training Epoch: 6 [38912/50048]	Loss: 1.6795
Training Epoch: 6 [39040/50048]	Loss: 1.9361
Training Epoch: 6 [39168/50048]	Loss: 2.0649
Training Epoch: 6 [39296/50048]	Loss: 2.0212
Training Epoch: 6 [39424/50048]	Loss: 1.8581
Training Epoch: 6 [39552/50048]	Loss: 2.0204
Training Epoch: 6 [39680/50048]	Loss: 1.7418
Training Epoch: 6 [39808/50048]	Loss: 2.0454
Training Epoch: 6 [39936/50048]	Loss: 2.2023
Training Epoch: 6 [40064/50048]	Loss: 1.8855
Training Epoch: 6 [40192/50048]	Loss: 2.2352
Training Epoch: 6 [40320/50048]	Loss: 1.9845
Training Epoch: 6 [40448/50048]	Loss: 1.9181
Training Epoch: 6 [40576/50048]	Loss: 1.8299
Training Epoch: 6 [40704/50048]	Loss: 1.9733
Training Epoch: 6 [40832/50048]	Loss: 1.7987
Training Epoch: 6 [40960/50048]	Loss: 1.9314
Training Epoch: 6 [41088/50048]	Loss: 1.9928
Training Epoch: 6 [41216/50048]	Loss: 2.0481
Training Epoch: 6 [41344/50048]	Loss: 1.8565
Training Epoch: 6 [41472/50048]	Loss: 1.7362
Training Epoch: 6 [41600/50048]	Loss: 1.8277
Training Epoch: 6 [41728/50048]	Loss: 2.0786
Training Epoch: 6 [41856/50048]	Loss: 2.0971
Training Epoch: 6 [41984/50048]	Loss: 1.9061
Training Epoch: 6 [42112/50048]	Loss: 1.9329
Training Epoch: 6 [42240/50048]	Loss: 1.9176
Training Epoch: 6 [42368/50048]	Loss: 2.0953
Training Epoch: 6 [42496/50048]	Loss: 1.6155
Training Epoch: 6 [42624/50048]	Loss: 1.8852
Training Epoch: 6 [42752/50048]	Loss: 2.0698
Training Epoch: 6 [42880/50048]	Loss: 1.8727
Training Epoch: 6 [43008/50048]	Loss: 2.1190
Training Epoch: 6 [43136/50048]	Loss: 2.1116
Training Epoch: 6 [43264/50048]	Loss: 2.1166
Training Epoch: 6 [43392/50048]	Loss: 1.7785
Training Epoch: 6 [43520/50048]	Loss: 1.8522
Training Epoch: 6 [43648/50048]	Loss: 2.0129
Training Epoch: 6 [43776/50048]	Loss: 1.7266
Training Epoch: 6 [43904/50048]	Loss: 2.1138
Training Epoch: 6 [44032/50048]	Loss: 2.0482
Training Epoch: 6 [44160/50048]	Loss: 1.9083
Training Epoch: 6 [44288/50048]	Loss: 1.9366
Training Epoch: 6 [44416/50048]	Loss: 2.0070
Training Epoch: 6 [44544/50048]	Loss: 1.9909
Training Epoch: 6 [44672/50048]	Loss: 2.2381
Training Epoch: 6 [44800/50048]	Loss: 1.8249
Training Epoch: 6 [44928/50048]	Loss: 1.7436
Training Epoch: 6 [45056/50048]	Loss: 1.7818
Training Epoch: 6 [45184/50048]	Loss: 1.9833
Training Epoch: 6 [45312/50048]	Loss: 2.0493
Training Epoch: 6 [45440/50048]	Loss: 2.1682
Training Epoch: 6 [45568/50048]	Loss: 1.9877
Training Epoch: 6 [45696/50048]	Loss: 1.8768
Training Epoch: 6 [45824/50048]	Loss: 1.9448
Training Epoch: 6 [45952/50048]	Loss: 1.9197
Training Epoch: 6 [46080/50048]	Loss: 2.1124
Training Epoch: 6 [46208/50048]	Loss: 1.8555
Training Epoch: 6 [46336/50048]	Loss: 1.8120
Training Epoch: 6 [46464/50048]	Loss: 2.0464
Training Epoch: 6 [46592/50048]	Loss: 1.9596
Training Epoch: 6 [46720/50048]	Loss: 2.1054
2022-12-06 03:47:52,998 [ZeusDataLoader(train)] train epoch 7 done: time=86.44 energy=10511.98
2022-12-06 03:47:53,000 [ZeusDataLoader(eval)] Epoch 7 begin.
Training Epoch: 6 [46848/50048]	Loss: 1.7941
Training Epoch: 6 [46976/50048]	Loss: 1.8082
Training Epoch: 6 [47104/50048]	Loss: 1.8045
Training Epoch: 6 [47232/50048]	Loss: 2.0296
Training Epoch: 6 [47360/50048]	Loss: 1.7776
Training Epoch: 6 [47488/50048]	Loss: 1.8556
Training Epoch: 6 [47616/50048]	Loss: 1.8787
Training Epoch: 6 [47744/50048]	Loss: 1.9408
Training Epoch: 6 [47872/50048]	Loss: 1.8575
Training Epoch: 6 [48000/50048]	Loss: 2.1411
Training Epoch: 6 [48128/50048]	Loss: 2.3488
Training Epoch: 6 [48256/50048]	Loss: 2.0575
Training Epoch: 6 [48384/50048]	Loss: 1.9070
Training Epoch: 6 [48512/50048]	Loss: 1.7480
Training Epoch: 6 [48640/50048]	Loss: 1.8472
Training Epoch: 6 [48768/50048]	Loss: 1.8512
Training Epoch: 6 [48896/50048]	Loss: 1.7871
Training Epoch: 6 [49024/50048]	Loss: 1.8450
Training Epoch: 6 [49152/50048]	Loss: 1.8620
Training Epoch: 6 [49280/50048]	Loss: 1.9841
Training Epoch: 6 [49408/50048]	Loss: 1.8584
Training Epoch: 6 [49536/50048]	Loss: 1.8050
Training Epoch: 6 [49664/50048]	Loss: 1.9992
Training Epoch: 6 [49792/50048]	Loss: 1.7324
Training Epoch: 6 [49920/50048]	Loss: 2.0998
Training Epoch: 6 [50048/50048]	Loss: 1.9739
2022-12-06 08:47:56.730 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 03:47:56,775 [ZeusDataLoader(eval)] eval epoch 7 done: time=3.77 energy=453.68
2022-12-06 03:47:56,776 [ZeusDataLoader(train)] Up to epoch 7: time=634.41, energy=76569.39, cost=93795.41
2022-12-06 03:47:56,776 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 03:47:56,776 [ZeusDataLoader(train)] Expected next epoch: time=724.21, energy=87367.40, cost=107051.79
2022-12-06 03:47:56,777 [ZeusDataLoader(train)] Epoch 8 begin.
Validation Epoch: 6, Average loss: 0.0164, Accuracy: 0.4432
2022-12-06 03:47:56,955 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 03:47:56,955 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 08:47:56.959 [ZeusMonitor] Monitor started.
2022-12-06 08:47:56.959 [ZeusMonitor] Running indefinitely. 2022-12-06 08:47:56.960 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 08:47:56.960 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e8+gpu0.power.log
Training Epoch: 7 [128/50048]	Loss: 1.7165
Training Epoch: 7 [256/50048]	Loss: 1.8100
Training Epoch: 7 [384/50048]	Loss: 1.8745
Training Epoch: 7 [512/50048]	Loss: 1.9740
Training Epoch: 7 [640/50048]	Loss: 1.9248
Training Epoch: 7 [768/50048]	Loss: 2.0041
Training Epoch: 7 [896/50048]	Loss: 1.9185
Training Epoch: 7 [1024/50048]	Loss: 2.1210
Training Epoch: 7 [1152/50048]	Loss: 1.7638
Training Epoch: 7 [1280/50048]	Loss: 1.9280
Training Epoch: 7 [1408/50048]	Loss: 1.9374
Training Epoch: 7 [1536/50048]	Loss: 1.8462
Training Epoch: 7 [1664/50048]	Loss: 1.9454
Training Epoch: 7 [1792/50048]	Loss: 1.8234
Training Epoch: 7 [1920/50048]	Loss: 1.7187
Training Epoch: 7 [2048/50048]	Loss: 1.8016
Training Epoch: 7 [2176/50048]	Loss: 1.8677
Training Epoch: 7 [2304/50048]	Loss: 2.0372
Training Epoch: 7 [2432/50048]	Loss: 1.7712
Training Epoch: 7 [2560/50048]	Loss: 1.7507
Training Epoch: 7 [2688/50048]	Loss: 1.5657
Training Epoch: 7 [2816/50048]	Loss: 1.8676
Training Epoch: 7 [2944/50048]	Loss: 2.0338
Training Epoch: 7 [3072/50048]	Loss: 1.7378
Training Epoch: 7 [3200/50048]	Loss: 1.7167
Training Epoch: 7 [3328/50048]	Loss: 1.9010
Training Epoch: 7 [3456/50048]	Loss: 1.6475
Training Epoch: 7 [3584/50048]	Loss: 1.8360
Training Epoch: 7 [3712/50048]	Loss: 1.8929
Training Epoch: 7 [3840/50048]	Loss: 1.7167
Training Epoch: 7 [3968/50048]	Loss: 1.9420
Training Epoch: 7 [4096/50048]	Loss: 1.8057
Training Epoch: 7 [4224/50048]	Loss: 1.8923
Training Epoch: 7 [4352/50048]	Loss: 1.7491
Training Epoch: 7 [4480/50048]	Loss: 1.9271
Training Epoch: 7 [4608/50048]	Loss: 1.9018
Training Epoch: 7 [4736/50048]	Loss: 1.9309
Training Epoch: 7 [4864/50048]	Loss: 1.7955
Training Epoch: 7 [4992/50048]	Loss: 1.8310
Training Epoch: 7 [5120/50048]	Loss: 1.8010
Training Epoch: 7 [5248/50048]	Loss: 1.5269
Training Epoch: 7 [5376/50048]	Loss: 1.8321
Training Epoch: 7 [5504/50048]	Loss: 2.0415
Training Epoch: 7 [5632/50048]	Loss: 1.9151
Training Epoch: 7 [5760/50048]	Loss: 1.8713
Training Epoch: 7 [5888/50048]	Loss: 1.4604
Training Epoch: 7 [6016/50048]	Loss: 2.1289
Training Epoch: 7 [6144/50048]	Loss: 1.8778
Training Epoch: 7 [6272/50048]	Loss: 1.8957
Training Epoch: 7 [6400/50048]	Loss: 2.0669
Training Epoch: 7 [6528/50048]	Loss: 1.7780
Training Epoch: 7 [6656/50048]	Loss: 1.6044
Training Epoch: 7 [6784/50048]	Loss: 1.8495
Training Epoch: 7 [6912/50048]	Loss: 1.6756
Training Epoch: 7 [7040/50048]	Loss: 1.7719
Training Epoch: 7 [7168/50048]	Loss: 1.9154
Training Epoch: 7 [7296/50048]	Loss: 1.7406
Training Epoch: 7 [7424/50048]	Loss: 2.0132
Training Epoch: 7 [7552/50048]	Loss: 1.8085
Training Epoch: 7 [7680/50048]	Loss: 1.7911
Training Epoch: 7 [7808/50048]	Loss: 2.0182
Training Epoch: 7 [7936/50048]	Loss: 1.7975
Training Epoch: 7 [8064/50048]	Loss: 1.8610
Training Epoch: 7 [8192/50048]	Loss: 2.0776
Training Epoch: 7 [8320/50048]	Loss: 1.5512
Training Epoch: 7 [8448/50048]	Loss: 1.7731
Training Epoch: 7 [8576/50048]	Loss: 1.7114
Training Epoch: 7 [8704/50048]	Loss: 1.7010
Training Epoch: 7 [8832/50048]	Loss: 2.1163
Training Epoch: 7 [8960/50048]	Loss: 1.9288
Training Epoch: 7 [9088/50048]	Loss: 1.7665
Training Epoch: 7 [9216/50048]	Loss: 1.6436
Training Epoch: 7 [9344/50048]	Loss: 1.8746
Training Epoch: 7 [9472/50048]	Loss: 1.9608
Training Epoch: 7 [9600/50048]	Loss: 1.7660
Training Epoch: 7 [9728/50048]	Loss: 1.7618
Training Epoch: 7 [9856/50048]	Loss: 1.7905
Training Epoch: 7 [9984/50048]	Loss: 2.0344
Training Epoch: 7 [10112/50048]	Loss: 1.5867
Training Epoch: 7 [10240/50048]	Loss: 1.6888
Training Epoch: 7 [10368/50048]	Loss: 1.6173
Training Epoch: 7 [10496/50048]	Loss: 1.9642
Training Epoch: 7 [10624/50048]	Loss: 1.9369
Training Epoch: 7 [10752/50048]	Loss: 1.7834
Training Epoch: 7 [10880/50048]	Loss: 1.7719
Training Epoch: 7 [11008/50048]	Loss: 1.7487
Training Epoch: 7 [11136/50048]	Loss: 1.9342
Training Epoch: 7 [11264/50048]	Loss: 1.8299
Training Epoch: 7 [11392/50048]	Loss: 1.9116
Training Epoch: 7 [11520/50048]	Loss: 2.2384
Training Epoch: 7 [11648/50048]	Loss: 1.6616
Training Epoch: 7 [11776/50048]	Loss: 1.7104
Training Epoch: 7 [11904/50048]	Loss: 1.9393
Training Epoch: 7 [12032/50048]	Loss: 1.7802
Training Epoch: 7 [12160/50048]	Loss: 1.9046
Training Epoch: 7 [12288/50048]	Loss: 1.8581
Training Epoch: 7 [12416/50048]	Loss: 1.9056
Training Epoch: 7 [12544/50048]	Loss: 1.7605
Training Epoch: 7 [12672/50048]	Loss: 1.9239
Training Epoch: 7 [12800/50048]	Loss: 1.7678
Training Epoch: 7 [12928/50048]	Loss: 2.1911
Training Epoch: 7 [13056/50048]	Loss: 1.7611
Training Epoch: 7 [13184/50048]	Loss: 1.9813
Training Epoch: 7 [13312/50048]	Loss: 2.1598
Training Epoch: 7 [13440/50048]	Loss: 1.9239
Training Epoch: 7 [13568/50048]	Loss: 2.0106
Training Epoch: 7 [13696/50048]	Loss: 1.7648
Training Epoch: 7 [13824/50048]	Loss: 2.1530
Training Epoch: 7 [13952/50048]	Loss: 1.9602
Training Epoch: 7 [14080/50048]	Loss: 2.1826
Training Epoch: 7 [14208/50048]	Loss: 1.9208
Training Epoch: 7 [14336/50048]	Loss: 2.0553
Training Epoch: 7 [14464/50048]	Loss: 1.9173
Training Epoch: 7 [14592/50048]	Loss: 1.9458
Training Epoch: 7 [14720/50048]	Loss: 1.9472
Training Epoch: 7 [14848/50048]	Loss: 1.6635
Training Epoch: 7 [14976/50048]	Loss: 1.7398
Training Epoch: 7 [15104/50048]	Loss: 1.7690
Training Epoch: 7 [15232/50048]	Loss: 1.9992
Training Epoch: 7 [15360/50048]	Loss: 1.8778
Training Epoch: 7 [15488/50048]	Loss: 2.0146
Training Epoch: 7 [15616/50048]	Loss: 1.6177
Training Epoch: 7 [15744/50048]	Loss: 1.9741
Training Epoch: 7 [15872/50048]	Loss: 1.6269
Training Epoch: 7 [16000/50048]	Loss: 2.0493
Training Epoch: 7 [16128/50048]	Loss: 1.7692
Training Epoch: 7 [16256/50048]	Loss: 1.7790
Training Epoch: 7 [16384/50048]	Loss: 1.8846
Training Epoch: 7 [16512/50048]	Loss: 2.0533
Training Epoch: 7 [16640/50048]	Loss: 1.8147
Training Epoch: 7 [16768/50048]	Loss: 1.7354
Training Epoch: 7 [16896/50048]	Loss: 1.9100
Training Epoch: 7 [17024/50048]	Loss: 2.0925
Training Epoch: 7 [17152/50048]	Loss: 1.8553
Training Epoch: 7 [17280/50048]	Loss: 2.2055
Training Epoch: 7 [17408/50048]	Loss: 1.8281
Training Epoch: 7 [17536/50048]	Loss: 1.8650
Training Epoch: 7 [17664/50048]	Loss: 1.8616
Training Epoch: 7 [17792/50048]	Loss: 1.6568
Training Epoch: 7 [17920/50048]	Loss: 1.9333
Training Epoch: 7 [18048/50048]	Loss: 1.8346
Training Epoch: 7 [18176/50048]	Loss: 1.8339
Training Epoch: 7 [18304/50048]	Loss: 1.6491
Training Epoch: 7 [18432/50048]	Loss: 1.7657
Training Epoch: 7 [18560/50048]	Loss: 1.9152
Training Epoch: 7 [18688/50048]	Loss: 1.6413
Training Epoch: 7 [18816/50048]	Loss: 1.6559
Training Epoch: 7 [18944/50048]	Loss: 1.9900
Training Epoch: 7 [19072/50048]	Loss: 1.9394
Training Epoch: 7 [19200/50048]	Loss: 2.0269
Training Epoch: 7 [19328/50048]	Loss: 1.8186
Training Epoch: 7 [19456/50048]	Loss: 2.0852
Training Epoch: 7 [19584/50048]	Loss: 1.9644
Training Epoch: 7 [19712/50048]	Loss: 1.8386
Training Epoch: 7 [19840/50048]	Loss: 1.9345
Training Epoch: 7 [19968/50048]	Loss: 1.8322
Training Epoch: 7 [20096/50048]	Loss: 2.0001
Training Epoch: 7 [20224/50048]	Loss: 1.9811
Training Epoch: 7 [20352/50048]	Loss: 2.0832
Training Epoch: 7 [20480/50048]	Loss: 1.8901
Training Epoch: 7 [20608/50048]	Loss: 1.9587
Training Epoch: 7 [20736/50048]	Loss: 1.9222
Training Epoch: 7 [20864/50048]	Loss: 2.0196
Training Epoch: 7 [20992/50048]	Loss: 1.7683
Training Epoch: 7 [21120/50048]	Loss: 1.8334
Training Epoch: 7 [21248/50048]	Loss: 1.9402
Training Epoch: 7 [21376/50048]	Loss: 1.9985
Training Epoch: 7 [21504/50048]	Loss: 1.4879
Training Epoch: 7 [21632/50048]	Loss: 1.7600
Training Epoch: 7 [21760/50048]	Loss: 1.5834
Training Epoch: 7 [21888/50048]	Loss: 1.6786
Training Epoch: 7 [22016/50048]	Loss: 1.6390
Training Epoch: 7 [22144/50048]	Loss: 1.5789
Training Epoch: 7 [22272/50048]	Loss: 1.8428
Training Epoch: 7 [22400/50048]	Loss: 1.9969
Training Epoch: 7 [22528/50048]	Loss: 2.0233
Training Epoch: 7 [22656/50048]	Loss: 1.8139
Training Epoch: 7 [22784/50048]	Loss: 1.7850
Training Epoch: 7 [22912/50048]	Loss: 1.8455
Training Epoch: 7 [23040/50048]	Loss: 1.6244
Training Epoch: 7 [23168/50048]	Loss: 1.9086
Training Epoch: 7 [23296/50048]	Loss: 1.8477
Training Epoch: 7 [23424/50048]	Loss: 2.0272
Training Epoch: 7 [23552/50048]	Loss: 1.9902
Training Epoch: 7 [23680/50048]	Loss: 1.9503
Training Epoch: 7 [23808/50048]	Loss: 1.8371
Training Epoch: 7 [23936/50048]	Loss: 1.6743
Training Epoch: 7 [24064/50048]	Loss: 1.7364
Training Epoch: 7 [24192/50048]	Loss: 1.7413
Training Epoch: 7 [24320/50048]	Loss: 1.6213
Training Epoch: 7 [24448/50048]	Loss: 2.0989
Training Epoch: 7 [24576/50048]	Loss: 1.8016
Training Epoch: 7 [24704/50048]	Loss: 1.8831
Training Epoch: 7 [24832/50048]	Loss: 1.9523
Training Epoch: 7 [24960/50048]	Loss: 2.0854
Training Epoch: 7 [25088/50048]	Loss: 2.3022
Training Epoch: 7 [25216/50048]	Loss: 1.7192
Training Epoch: 7 [25344/50048]	Loss: 1.7623
Training Epoch: 7 [25472/50048]	Loss: 1.6405
Training Epoch: 7 [25600/50048]	Loss: 1.9950
Training Epoch: 7 [25728/50048]	Loss: 1.7269
Training Epoch: 7 [25856/50048]	Loss: 1.8305
Training Epoch: 7 [25984/50048]	Loss: 1.9428
Training Epoch: 7 [26112/50048]	Loss: 1.8617
Training Epoch: 7 [26240/50048]	Loss: 1.7570
Training Epoch: 7 [26368/50048]	Loss: 1.9077
Training Epoch: 7 [26496/50048]	Loss: 1.7492
Training Epoch: 7 [26624/50048]	Loss: 1.8777
Training Epoch: 7 [26752/50048]	Loss: 1.9627
Training Epoch: 7 [26880/50048]	Loss: 1.9346
Training Epoch: 7 [27008/50048]	Loss: 2.0165
Training Epoch: 7 [27136/50048]	Loss: 1.9490
Training Epoch: 7 [27264/50048]	Loss: 1.8068
Training Epoch: 7 [27392/50048]	Loss: 1.9438
Training Epoch: 7 [27520/50048]	Loss: 2.0460
Training Epoch: 7 [27648/50048]	Loss: 1.8289
Training Epoch: 7 [27776/50048]	Loss: 1.8895
Training Epoch: 7 [27904/50048]	Loss: 1.6779
Training Epoch: 7 [28032/50048]	Loss: 1.9854
Training Epoch: 7 [28160/50048]	Loss: 1.8989
Training Epoch: 7 [28288/50048]	Loss: 1.5975
Training Epoch: 7 [28416/50048]	Loss: 1.6784
Training Epoch: 7 [28544/50048]	Loss: 1.5846
Training Epoch: 7 [28672/50048]	Loss: 2.0167
Training Epoch: 7 [28800/50048]	Loss: 2.0173
Training Epoch: 7 [28928/50048]	Loss: 1.8750
Training Epoch: 7 [29056/50048]	Loss: 1.6959
Training Epoch: 7 [29184/50048]	Loss: 1.6753
Training Epoch: 7 [29312/50048]	Loss: 1.9129
Training Epoch: 7 [29440/50048]	Loss: 1.6667
Training Epoch: 7 [29568/50048]	Loss: 1.7144
Training Epoch: 7 [29696/50048]	Loss: 1.8936
Training Epoch: 7 [29824/50048]	Loss: 2.1125
Training Epoch: 7 [29952/50048]	Loss: 1.8014
Training Epoch: 7 [30080/50048]	Loss: 1.8978
Training Epoch: 7 [30208/50048]	Loss: 1.6337
Training Epoch: 7 [30336/50048]	Loss: 1.7834
Training Epoch: 7 [30464/50048]	Loss: 1.9429
Training Epoch: 7 [30592/50048]	Loss: 1.6249
Training Epoch: 7 [30720/50048]	Loss: 1.9635
Training Epoch: 7 [30848/50048]	Loss: 1.6391
Training Epoch: 7 [30976/50048]	Loss: 1.8127
Training Epoch: 7 [31104/50048]	Loss: 1.9350
Training Epoch: 7 [31232/50048]	Loss: 1.9156
Training Epoch: 7 [31360/50048]	Loss: 1.9451
Training Epoch: 7 [31488/50048]	Loss: 1.8651
Training Epoch: 7 [31616/50048]	Loss: 2.2501
Training Epoch: 7 [31744/50048]	Loss: 1.8041
Training Epoch: 7 [31872/50048]	Loss: 1.8695
Training Epoch: 7 [32000/50048]	Loss: 1.7748
Training Epoch: 7 [32128/50048]	Loss: 1.5977
Training Epoch: 7 [32256/50048]	Loss: 2.0013
Training Epoch: 7 [32384/50048]	Loss: 1.6059
Training Epoch: 7 [32512/50048]	Loss: 1.8570
Training Epoch: 7 [32640/50048]	Loss: 1.8625
Training Epoch: 7 [32768/50048]	Loss: 1.9106
Training Epoch: 7 [32896/50048]	Loss: 1.8536
Training Epoch: 7 [33024/50048]	Loss: 1.8000
Training Epoch: 7 [33152/50048]	Loss: 1.8203
Training Epoch: 7 [33280/50048]	Loss: 1.8228
Training Epoch: 7 [33408/50048]	Loss: 1.8397
Training Epoch: 7 [33536/50048]	Loss: 1.8457
Training Epoch: 7 [33664/50048]	Loss: 1.9650
Training Epoch: 7 [33792/50048]	Loss: 1.7223
Training Epoch: 7 [33920/50048]	Loss: 2.0536
Training Epoch: 7 [34048/50048]	Loss: 1.8307
Training Epoch: 7 [34176/50048]	Loss: 1.6761
Training Epoch: 7 [34304/50048]	Loss: 1.7138
Training Epoch: 7 [34432/50048]	Loss: 1.9157
Training Epoch: 7 [34560/50048]	Loss: 1.9567
Training Epoch: 7 [34688/50048]	Loss: 2.0492
Training Epoch: 7 [34816/50048]	Loss: 1.8798
Training Epoch: 7 [34944/50048]	Loss: 1.9171
Training Epoch: 7 [35072/50048]	Loss: 1.8965
Training Epoch: 7 [35200/50048]	Loss: 1.8149
Training Epoch: 7 [35328/50048]	Loss: 1.8575
Training Epoch: 7 [35456/50048]	Loss: 1.8203
Training Epoch: 7 [35584/50048]	Loss: 1.7809
Training Epoch: 7 [35712/50048]	Loss: 1.8326
Training Epoch: 7 [35840/50048]	Loss: 2.0695
Training Epoch: 7 [35968/50048]	Loss: 1.6539
Training Epoch: 7 [36096/50048]	Loss: 1.7840
Training Epoch: 7 [36224/50048]	Loss: 1.8878
Training Epoch: 7 [36352/50048]	Loss: 1.6872
Training Epoch: 7 [36480/50048]	Loss: 1.6412
Training Epoch: 7 [36608/50048]	Loss: 1.9220
Training Epoch: 7 [36736/50048]	Loss: 1.7978
Training Epoch: 7 [36864/50048]	Loss: 1.5891
Training Epoch: 7 [36992/50048]	Loss: 1.7467
Training Epoch: 7 [37120/50048]	Loss: 1.9214
Training Epoch: 7 [37248/50048]	Loss: 1.6970
Training Epoch: 7 [37376/50048]	Loss: 1.9179
Training Epoch: 7 [37504/50048]	Loss: 1.8814
Training Epoch: 7 [37632/50048]	Loss: 1.8824
Training Epoch: 7 [37760/50048]	Loss: 1.7485
Training Epoch: 7 [37888/50048]	Loss: 1.8426
Training Epoch: 7 [38016/50048]	Loss: 1.7440
Training Epoch: 7 [38144/50048]	Loss: 1.5946
Training Epoch: 7 [38272/50048]	Loss: 1.8158
Training Epoch: 7 [38400/50048]	Loss: 1.8439
Training Epoch: 7 [38528/50048]	Loss: 2.0212
Training Epoch: 7 [38656/50048]	Loss: 1.7606
Training Epoch: 7 [38784/50048]	Loss: 1.8859
Training Epoch: 7 [38912/50048]	Loss: 2.0093
Training Epoch: 7 [39040/50048]	Loss: 1.8285
Training Epoch: 7 [39168/50048]	Loss: 2.1748
Training Epoch: 7 [39296/50048]	Loss: 1.9006
Training Epoch: 7 [39424/50048]	Loss: 1.9879
Training Epoch: 7 [39552/50048]	Loss: 1.7483
Training Epoch: 7 [39680/50048]	Loss: 1.8019
Training Epoch: 7 [39808/50048]	Loss: 1.6834
Training Epoch: 7 [39936/50048]	Loss: 2.0183
Training Epoch: 7 [40064/50048]	Loss: 2.1977
Training Epoch: 7 [40192/50048]	Loss: 1.9980
Training Epoch: 7 [40320/50048]	Loss: 1.8111
Training Epoch: 7 [40448/50048]	Loss: 1.8907
Training Epoch: 7 [40576/50048]	Loss: 1.8424
Training Epoch: 7 [40704/50048]	Loss: 1.9820
Training Epoch: 7 [40832/50048]	Loss: 1.6536
Training Epoch: 7 [40960/50048]	Loss: 1.8923
Training Epoch: 7 [41088/50048]	Loss: 1.7367
Training Epoch: 7 [41216/50048]	Loss: 1.6796
Training Epoch: 7 [41344/50048]	Loss: 1.8329
Training Epoch: 7 [41472/50048]	Loss: 1.9376
Training Epoch: 7 [41600/50048]	Loss: 1.6573
Training Epoch: 7 [41728/50048]	Loss: 1.8079
Training Epoch: 7 [41856/50048]	Loss: 1.8342
Training Epoch: 7 [41984/50048]	Loss: 1.8443
Training Epoch: 7 [42112/50048]	Loss: 1.8801
Training Epoch: 7 [42240/50048]	Loss: 1.8057
Training Epoch: 7 [42368/50048]	Loss: 1.6401
Training Epoch: 7 [42496/50048]	Loss: 1.9123
Training Epoch: 7 [42624/50048]	Loss: 1.7008
Training Epoch: 7 [42752/50048]	Loss: 1.9046
Training Epoch: 7 [42880/50048]	Loss: 1.4930
Training Epoch: 7 [43008/50048]	Loss: 1.7537
Training Epoch: 7 [43136/50048]	Loss: 1.7927
Training Epoch: 7 [43264/50048]	Loss: 1.9406
Training Epoch: 7 [43392/50048]	Loss: 2.1326
Training Epoch: 7 [43520/50048]	Loss: 1.5546
Training Epoch: 7 [43648/50048]	Loss: 1.7332
Training Epoch: 7 [43776/50048]	Loss: 1.6996
Training Epoch: 7 [43904/50048]	Loss: 1.9346
Training Epoch: 7 [44032/50048]	Loss: 1.9644
Training Epoch: 7 [44160/50048]	Loss: 1.9384
Training Epoch: 7 [44288/50048]	Loss: 1.8937
Training Epoch: 7 [44416/50048]	Loss: 1.6386
Training Epoch: 7 [44544/50048]	Loss: 1.8910
Training Epoch: 7 [44672/50048]	Loss: 1.7788
Training Epoch: 7 [44800/50048]	Loss: 1.8020
Training Epoch: 7 [44928/50048]	Loss: 1.9731
Training Epoch: 7 [45056/50048]	Loss: 1.7353
Training Epoch: 7 [45184/50048]	Loss: 2.0665
Training Epoch: 7 [45312/50048]	Loss: 1.9713
Training Epoch: 7 [45440/50048]	Loss: 1.7777
Training Epoch: 7 [45568/50048]	Loss: 1.7948
Training Epoch: 7 [45696/50048]	Loss: 1.9159
Training Epoch: 7 [45824/50048]	Loss: 1.7649
Training Epoch: 7 [45952/50048]	Loss: 1.7486
Training Epoch: 7 [46080/50048]	Loss: 2.1120
Training Epoch: 7 [46208/50048]	Loss: 1.8913
Training Epoch: 7 [46336/50048]	Loss: 1.9549
Training Epoch: 7 [46464/50048]	Loss: 1.7488
Training Epoch: 7 [46592/50048]	Loss: 1.8925
Training Epoch: 7 [46720/50048]	Loss: 1.8031
2022-12-06 03:49:23,278 [ZeusDataLoader(train)] train epoch 8 done: time=86.49 energy=10510.32
2022-12-06 03:49:23,279 [ZeusDataLoader(eval)] Epoch 8 begin.
Training Epoch: 7 [46848/50048]	Loss: 1.9596
Training Epoch: 7 [46976/50048]	Loss: 2.0205
Training Epoch: 7 [47104/50048]	Loss: 1.6516
Training Epoch: 7 [47232/50048]	Loss: 1.7190
Training Epoch: 7 [47360/50048]	Loss: 1.7280
Training Epoch: 7 [47488/50048]	Loss: 1.8015
Training Epoch: 7 [47616/50048]	Loss: 1.8971
Training Epoch: 7 [47744/50048]	Loss: 1.8683
Training Epoch: 7 [47872/50048]	Loss: 1.6534
Training Epoch: 7 [48000/50048]	Loss: 1.4693
Training Epoch: 7 [48128/50048]	Loss: 2.0091
Training Epoch: 7 [48256/50048]	Loss: 1.8521
Training Epoch: 7 [48384/50048]	Loss: 2.0233
Training Epoch: 7 [48512/50048]	Loss: 1.7267
Training Epoch: 7 [48640/50048]	Loss: 1.9118
Training Epoch: 7 [48768/50048]	Loss: 2.0614
Training Epoch: 7 [48896/50048]	Loss: 1.8787
Training Epoch: 7 [49024/50048]	Loss: 2.0606
Training Epoch: 7 [49152/50048]	Loss: 1.9618
Training Epoch: 7 [49280/50048]	Loss: 1.7094
Training Epoch: 7 [49408/50048]	Loss: 1.7835
Training Epoch: 7 [49536/50048]	Loss: 1.8524
Training Epoch: 7 [49664/50048]	Loss: 1.5184
Training Epoch: 7 [49792/50048]	Loss: 1.8196
Training Epoch: 7 [49920/50048]	Loss: 1.9153
Training Epoch: 7 [50048/50048]	Loss: 2.0445
2022-12-06 08:49:26.999 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 03:49:27,032 [ZeusDataLoader(eval)] eval epoch 8 done: time=3.74 energy=454.10
2022-12-06 03:49:27,032 [ZeusDataLoader(train)] Up to epoch 8: time=724.64, energy=87533.81, cost=107173.15
2022-12-06 03:49:27,032 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 03:49:27,032 [ZeusDataLoader(train)] Expected next epoch: time=814.44, energy=98331.82, cost=120429.53
2022-12-06 03:49:27,033 [ZeusDataLoader(train)] Epoch 9 begin.
Validation Epoch: 7, Average loss: 0.0153, Accuracy: 0.4738
2022-12-06 03:49:27,177 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 03:49:27,178 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 08:49:27.181 [ZeusMonitor] Monitor started.
2022-12-06 08:49:27.182 [ZeusMonitor] Running indefinitely. 2022-12-06 08:49:27.182 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 08:49:27.182 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e9+gpu0.power.log
Training Epoch: 8 [128/50048]	Loss: 1.9322
Training Epoch: 8 [256/50048]	Loss: 1.7227
Training Epoch: 8 [384/50048]	Loss: 1.7993
Training Epoch: 8 [512/50048]	Loss: 2.0305
Training Epoch: 8 [640/50048]	Loss: 1.9459
Training Epoch: 8 [768/50048]	Loss: 1.8383
Training Epoch: 8 [896/50048]	Loss: 1.8420
Training Epoch: 8 [1024/50048]	Loss: 1.8125
Training Epoch: 8 [1152/50048]	Loss: 1.7545
Training Epoch: 8 [1280/50048]	Loss: 1.6408
Training Epoch: 8 [1408/50048]	Loss: 1.6717
Training Epoch: 8 [1536/50048]	Loss: 1.6613
Training Epoch: 8 [1664/50048]	Loss: 1.7789
Training Epoch: 8 [1792/50048]	Loss: 1.7692
Training Epoch: 8 [1920/50048]	Loss: 2.0806
Training Epoch: 8 [2048/50048]	Loss: 1.7200
Training Epoch: 8 [2176/50048]	Loss: 1.7968
Training Epoch: 8 [2304/50048]	Loss: 1.7564
Training Epoch: 8 [2432/50048]	Loss: 2.1896
Training Epoch: 8 [2560/50048]	Loss: 1.6884
Training Epoch: 8 [2688/50048]	Loss: 1.8177
Training Epoch: 8 [2816/50048]	Loss: 1.5602
Training Epoch: 8 [2944/50048]	Loss: 1.7345
Training Epoch: 8 [3072/50048]	Loss: 1.8232
Training Epoch: 8 [3200/50048]	Loss: 1.8911
Training Epoch: 8 [3328/50048]	Loss: 1.5106
Training Epoch: 8 [3456/50048]	Loss: 1.6420
Training Epoch: 8 [3584/50048]	Loss: 1.7950
Training Epoch: 8 [3712/50048]	Loss: 1.6559
Training Epoch: 8 [3840/50048]	Loss: 1.8526
Training Epoch: 8 [3968/50048]	Loss: 1.7261
Training Epoch: 8 [4096/50048]	Loss: 1.8650
Training Epoch: 8 [4224/50048]	Loss: 1.5292
Training Epoch: 8 [4352/50048]	Loss: 1.7387
Training Epoch: 8 [4480/50048]	Loss: 1.6130
Training Epoch: 8 [4608/50048]	Loss: 1.5769
Training Epoch: 8 [4736/50048]	Loss: 1.6910
Training Epoch: 8 [4864/50048]	Loss: 1.8558
Training Epoch: 8 [4992/50048]	Loss: 1.6807
Training Epoch: 8 [5120/50048]	Loss: 1.4688
Training Epoch: 8 [5248/50048]	Loss: 1.8767
Training Epoch: 8 [5376/50048]	Loss: 1.8038
Training Epoch: 8 [5504/50048]	Loss: 1.5678
Training Epoch: 8 [5632/50048]	Loss: 1.6971
Training Epoch: 8 [5760/50048]	Loss: 1.6180
Training Epoch: 8 [5888/50048]	Loss: 1.4719
Training Epoch: 8 [6016/50048]	Loss: 1.4710
Training Epoch: 8 [6144/50048]	Loss: 1.6971
Training Epoch: 8 [6272/50048]	Loss: 1.4292
Training Epoch: 8 [6400/50048]	Loss: 1.5802
Training Epoch: 8 [6528/50048]	Loss: 1.9659
Training Epoch: 8 [6656/50048]	Loss: 1.8765
Training Epoch: 8 [6784/50048]	Loss: 1.8302
Training Epoch: 8 [6912/50048]	Loss: 1.8573
Training Epoch: 8 [7040/50048]	Loss: 1.9107
Training Epoch: 8 [7168/50048]	Loss: 1.8095
Training Epoch: 8 [7296/50048]	Loss: 1.8572
Training Epoch: 8 [7424/50048]	Loss: 1.6483
Training Epoch: 8 [7552/50048]	Loss: 1.7047
Training Epoch: 8 [7680/50048]	Loss: 1.6973
Training Epoch: 8 [7808/50048]	Loss: 2.0300
Training Epoch: 8 [7936/50048]	Loss: 1.7623
Training Epoch: 8 [8064/50048]	Loss: 1.5994
Training Epoch: 8 [8192/50048]	Loss: 1.7402
Training Epoch: 8 [8320/50048]	Loss: 1.7067
Training Epoch: 8 [8448/50048]	Loss: 1.5958
Training Epoch: 8 [8576/50048]	Loss: 1.8203
Training Epoch: 8 [8704/50048]	Loss: 1.8459
Training Epoch: 8 [8832/50048]	Loss: 1.8453
Training Epoch: 8 [8960/50048]	Loss: 1.6597
Training Epoch: 8 [9088/50048]	Loss: 1.6985
Training Epoch: 8 [9216/50048]	Loss: 1.5473
Training Epoch: 8 [9344/50048]	Loss: 1.8893
Training Epoch: 8 [9472/50048]	Loss: 1.7317
Training Epoch: 8 [9600/50048]	Loss: 1.6810
Training Epoch: 8 [9728/50048]	Loss: 2.0819
Training Epoch: 8 [9856/50048]	Loss: 1.6877
Training Epoch: 8 [9984/50048]	Loss: 1.6846
Training Epoch: 8 [10112/50048]	Loss: 1.7953
Training Epoch: 8 [10240/50048]	Loss: 1.8616
Training Epoch: 8 [10368/50048]	Loss: 1.7930
Training Epoch: 8 [10496/50048]	Loss: 1.7994
Training Epoch: 8 [10624/50048]	Loss: 1.6073
Training Epoch: 8 [10752/50048]	Loss: 1.6931
Training Epoch: 8 [10880/50048]	Loss: 1.7377
Training Epoch: 8 [11008/50048]	Loss: 1.6208
Training Epoch: 8 [11136/50048]	Loss: 1.9222
Training Epoch: 8 [11264/50048]	Loss: 1.8281
Training Epoch: 8 [11392/50048]	Loss: 1.8396
Training Epoch: 8 [11520/50048]	Loss: 1.8206
Training Epoch: 8 [11648/50048]	Loss: 1.7093
Training Epoch: 8 [11776/50048]	Loss: 1.6161
Training Epoch: 8 [11904/50048]	Loss: 1.5143
Training Epoch: 8 [12032/50048]	Loss: 1.6844
Training Epoch: 8 [12160/50048]	Loss: 1.5961
Training Epoch: 8 [12288/50048]	Loss: 1.7953
Training Epoch: 8 [12416/50048]	Loss: 1.7440
Training Epoch: 8 [12544/50048]	Loss: 1.8794
Training Epoch: 8 [12672/50048]	Loss: 1.7128
Training Epoch: 8 [12800/50048]	Loss: 1.7721
Training Epoch: 8 [12928/50048]	Loss: 1.6850
Training Epoch: 8 [13056/50048]	Loss: 1.7072
Training Epoch: 8 [13184/50048]	Loss: 1.6552
Training Epoch: 8 [13312/50048]	Loss: 1.6246
Training Epoch: 8 [13440/50048]	Loss: 1.7756
Training Epoch: 8 [13568/50048]	Loss: 1.7369
Training Epoch: 8 [13696/50048]	Loss: 1.6300
Training Epoch: 8 [13824/50048]	Loss: 1.7766
Training Epoch: 8 [13952/50048]	Loss: 2.0507
Training Epoch: 8 [14080/50048]	Loss: 1.6377
Training Epoch: 8 [14208/50048]	Loss: 1.9332
Training Epoch: 8 [14336/50048]	Loss: 1.7933
Training Epoch: 8 [14464/50048]	Loss: 1.7688
Training Epoch: 8 [14592/50048]	Loss: 1.9173
Training Epoch: 8 [14720/50048]	Loss: 1.6499
Training Epoch: 8 [14848/50048]	Loss: 1.6808
Training Epoch: 8 [14976/50048]	Loss: 1.5185
Training Epoch: 8 [15104/50048]	Loss: 1.8151
Training Epoch: 8 [15232/50048]	Loss: 1.9957
Training Epoch: 8 [15360/50048]	Loss: 1.7563
Training Epoch: 8 [15488/50048]	Loss: 1.6762
Training Epoch: 8 [15616/50048]	Loss: 1.4885
Training Epoch: 8 [15744/50048]	Loss: 1.8052
Training Epoch: 8 [15872/50048]	Loss: 1.7727
Training Epoch: 8 [16000/50048]	Loss: 1.4797
Training Epoch: 8 [16128/50048]	Loss: 1.8289
Training Epoch: 8 [16256/50048]	Loss: 1.5829
Training Epoch: 8 [16384/50048]	Loss: 1.6309
Training Epoch: 8 [16512/50048]	Loss: 1.4835
Training Epoch: 8 [16640/50048]	Loss: 2.1154
Training Epoch: 8 [16768/50048]	Loss: 1.5838
Training Epoch: 8 [16896/50048]	Loss: 1.6709
Training Epoch: 8 [17024/50048]	Loss: 2.0234
Training Epoch: 8 [17152/50048]	Loss: 2.0706
Training Epoch: 8 [17280/50048]	Loss: 1.8769
Training Epoch: 8 [17408/50048]	Loss: 1.8686
Training Epoch: 8 [17536/50048]	Loss: 1.5098
Training Epoch: 8 [17664/50048]	Loss: 1.6572
Training Epoch: 8 [17792/50048]	Loss: 1.5103
Training Epoch: 8 [17920/50048]	Loss: 1.7019
Training Epoch: 8 [18048/50048]	Loss: 2.1198
Training Epoch: 8 [18176/50048]	Loss: 1.8396
Training Epoch: 8 [18304/50048]	Loss: 1.8501
Training Epoch: 8 [18432/50048]	Loss: 1.8095
Training Epoch: 8 [18560/50048]	Loss: 1.8456
Training Epoch: 8 [18688/50048]	Loss: 1.6202
Training Epoch: 8 [18816/50048]	Loss: 1.6353
Training Epoch: 8 [18944/50048]	Loss: 1.7666
Training Epoch: 8 [19072/50048]	Loss: 1.5416
Training Epoch: 8 [19200/50048]	Loss: 1.8339
Training Epoch: 8 [19328/50048]	Loss: 1.5597
Training Epoch: 8 [19456/50048]	Loss: 1.6266
Training Epoch: 8 [19584/50048]	Loss: 1.5597
Training Epoch: 8 [19712/50048]	Loss: 1.6252
Training Epoch: 8 [19840/50048]	Loss: 1.9785
Training Epoch: 8 [19968/50048]	Loss: 1.7718
Training Epoch: 8 [20096/50048]	Loss: 1.5488
Training Epoch: 8 [20224/50048]	Loss: 1.6179
Training Epoch: 8 [20352/50048]	Loss: 1.7550
Training Epoch: 8 [20480/50048]	Loss: 1.8609
Training Epoch: 8 [20608/50048]	Loss: 1.6351
Training Epoch: 8 [20736/50048]	Loss: 1.9610
Training Epoch: 8 [20864/50048]	Loss: 1.7543
Training Epoch: 8 [20992/50048]	Loss: 1.7827
Training Epoch: 8 [21120/50048]	Loss: 1.7532
Training Epoch: 8 [21248/50048]	Loss: 1.3008
Training Epoch: 8 [21376/50048]	Loss: 1.6414
Training Epoch: 8 [21504/50048]	Loss: 1.8128
Training Epoch: 8 [21632/50048]	Loss: 1.4867
Training Epoch: 8 [21760/50048]	Loss: 1.5983
Training Epoch: 8 [21888/50048]	Loss: 1.8453
Training Epoch: 8 [22016/50048]	Loss: 1.8318
Training Epoch: 8 [22144/50048]	Loss: 1.5935
Training Epoch: 8 [22272/50048]	Loss: 1.8444
Training Epoch: 8 [22400/50048]	Loss: 1.6505
Training Epoch: 8 [22528/50048]	Loss: 1.6106
Training Epoch: 8 [22656/50048]	Loss: 1.9509
Training Epoch: 8 [22784/50048]	Loss: 1.7665
Training Epoch: 8 [22912/50048]	Loss: 1.8812
Training Epoch: 8 [23040/50048]	Loss: 1.7967
Training Epoch: 8 [23168/50048]	Loss: 1.5606
Training Epoch: 8 [23296/50048]	Loss: 1.6420
Training Epoch: 8 [23424/50048]	Loss: 1.6262
Training Epoch: 8 [23552/50048]	Loss: 1.6409
Training Epoch: 8 [23680/50048]	Loss: 1.8287
Training Epoch: 8 [23808/50048]	Loss: 1.9502
Training Epoch: 8 [23936/50048]	Loss: 1.5233
Training Epoch: 8 [24064/50048]	Loss: 1.7201
Training Epoch: 8 [24192/50048]	Loss: 1.5442
Training Epoch: 8 [24320/50048]	Loss: 1.8739
Training Epoch: 8 [24448/50048]	Loss: 1.8092
Training Epoch: 8 [24576/50048]	Loss: 1.7814
Training Epoch: 8 [24704/50048]	Loss: 1.5861
Training Epoch: 8 [24832/50048]	Loss: 1.8288
Training Epoch: 8 [24960/50048]	Loss: 1.7502
Training Epoch: 8 [25088/50048]	Loss: 1.9670
Training Epoch: 8 [25216/50048]	Loss: 1.6861
Training Epoch: 8 [25344/50048]	Loss: 1.5956
Training Epoch: 8 [25472/50048]	Loss: 1.6795
Training Epoch: 8 [25600/50048]	Loss: 1.7469
Training Epoch: 8 [25728/50048]	Loss: 1.7286
Training Epoch: 8 [25856/50048]	Loss: 1.5819
Training Epoch: 8 [25984/50048]	Loss: 1.8462
Training Epoch: 8 [26112/50048]	Loss: 1.7180
Training Epoch: 8 [26240/50048]	Loss: 1.6397
Training Epoch: 8 [26368/50048]	Loss: 1.8349
Training Epoch: 8 [26496/50048]	Loss: 1.6925
Training Epoch: 8 [26624/50048]	Loss: 1.7064
Training Epoch: 8 [26752/50048]	Loss: 1.8289
Training Epoch: 8 [26880/50048]	Loss: 1.8553
Training Epoch: 8 [27008/50048]	Loss: 1.6598
Training Epoch: 8 [27136/50048]	Loss: 1.5313
Training Epoch: 8 [27264/50048]	Loss: 1.8471
Training Epoch: 8 [27392/50048]	Loss: 1.8623
Training Epoch: 8 [27520/50048]	Loss: 1.7388
Training Epoch: 8 [27648/50048]	Loss: 1.5268
Training Epoch: 8 [27776/50048]	Loss: 1.8805
Training Epoch: 8 [27904/50048]	Loss: 1.6631
Training Epoch: 8 [28032/50048]	Loss: 1.6689
Training Epoch: 8 [28160/50048]	Loss: 1.6928
Training Epoch: 8 [28288/50048]	Loss: 1.4816
Training Epoch: 8 [28416/50048]	Loss: 1.6331
Training Epoch: 8 [28544/50048]	Loss: 1.8148
Training Epoch: 8 [28672/50048]	Loss: 1.7227
Training Epoch: 8 [28800/50048]	Loss: 1.6066
Training Epoch: 8 [28928/50048]	Loss: 1.3634
Training Epoch: 8 [29056/50048]	Loss: 1.7724
Training Epoch: 8 [29184/50048]	Loss: 1.8816
Training Epoch: 8 [29312/50048]	Loss: 1.7568
Training Epoch: 8 [29440/50048]	Loss: 2.1116
Training Epoch: 8 [29568/50048]	Loss: 1.7636
Training Epoch: 8 [29696/50048]	Loss: 1.4556
Training Epoch: 8 [29824/50048]	Loss: 1.6488
Training Epoch: 8 [29952/50048]	Loss: 2.0329
Training Epoch: 8 [30080/50048]	Loss: 1.6879
Training Epoch: 8 [30208/50048]	Loss: 1.4470
Training Epoch: 8 [30336/50048]	Loss: 1.8171
Training Epoch: 8 [30464/50048]	Loss: 2.1400
Training Epoch: 8 [30592/50048]	Loss: 1.7416
Training Epoch: 8 [30720/50048]	Loss: 1.7663
Training Epoch: 8 [30848/50048]	Loss: 1.7336
Training Epoch: 8 [30976/50048]	Loss: 1.8410
Training Epoch: 8 [31104/50048]	Loss: 1.7336
Training Epoch: 8 [31232/50048]	Loss: 1.8374
Training Epoch: 8 [31360/50048]	Loss: 1.5769
Training Epoch: 8 [31488/50048]	Loss: 1.6641
Training Epoch: 8 [31616/50048]	Loss: 1.5950
Training Epoch: 8 [31744/50048]	Loss: 1.5744
Training Epoch: 8 [31872/50048]	Loss: 1.5386
Training Epoch: 8 [32000/50048]	Loss: 1.5489
Training Epoch: 8 [32128/50048]	Loss: 1.8815
Training Epoch: 8 [32256/50048]	Loss: 1.8742
Training Epoch: 8 [32384/50048]	Loss: 1.7249
Training Epoch: 8 [32512/50048]	Loss: 1.5042
Training Epoch: 8 [32640/50048]	Loss: 1.9154
Training Epoch: 8 [32768/50048]	Loss: 1.4804
Training Epoch: 8 [32896/50048]	Loss: 1.7724
Training Epoch: 8 [33024/50048]	Loss: 1.7763
Training Epoch: 8 [33152/50048]	Loss: 1.4864
Training Epoch: 8 [33280/50048]	Loss: 1.8030
Training Epoch: 8 [33408/50048]	Loss: 1.9385
Training Epoch: 8 [33536/50048]	Loss: 1.9403
Training Epoch: 8 [33664/50048]	Loss: 1.7879
Training Epoch: 8 [33792/50048]	Loss: 1.9180
Training Epoch: 8 [33920/50048]	Loss: 1.8136
Training Epoch: 8 [34048/50048]	Loss: 1.8016
Training Epoch: 8 [34176/50048]	Loss: 1.7308
Training Epoch: 8 [34304/50048]	Loss: 1.6830
Training Epoch: 8 [34432/50048]	Loss: 1.6621
Training Epoch: 8 [34560/50048]	Loss: 1.8336
Training Epoch: 8 [34688/50048]	Loss: 1.7999
Training Epoch: 8 [34816/50048]	Loss: 1.8975
Training Epoch: 8 [34944/50048]	Loss: 1.9552
Training Epoch: 8 [35072/50048]	Loss: 1.6193
Training Epoch: 8 [35200/50048]	Loss: 1.8932
Training Epoch: 8 [35328/50048]	Loss: 1.5323
Training Epoch: 8 [35456/50048]	Loss: 1.6123
Training Epoch: 8 [35584/50048]	Loss: 1.5090
Training Epoch: 8 [35712/50048]	Loss: 1.8955
Training Epoch: 8 [35840/50048]	Loss: 1.9092
Training Epoch: 8 [35968/50048]	Loss: 1.7597
Training Epoch: 8 [36096/50048]	Loss: 1.8571
Training Epoch: 8 [36224/50048]	Loss: 1.9678
Training Epoch: 8 [36352/50048]	Loss: 1.8310
Training Epoch: 8 [36480/50048]	Loss: 1.7113
Training Epoch: 8 [36608/50048]	Loss: 1.7985
Training Epoch: 8 [36736/50048]	Loss: 1.5351
Training Epoch: 8 [36864/50048]	Loss: 1.5447
Training Epoch: 8 [36992/50048]	Loss: 1.9650
Training Epoch: 8 [37120/50048]	Loss: 1.8079
Training Epoch: 8 [37248/50048]	Loss: 1.5268
Training Epoch: 8 [37376/50048]	Loss: 1.8649
Training Epoch: 8 [37504/50048]	Loss: 1.6882
Training Epoch: 8 [37632/50048]	Loss: 1.7143
Training Epoch: 8 [37760/50048]	Loss: 1.4455
Training Epoch: 8 [37888/50048]	Loss: 1.9003
Training Epoch: 8 [38016/50048]	Loss: 1.7490
Training Epoch: 8 [38144/50048]	Loss: 1.7666
Training Epoch: 8 [38272/50048]	Loss: 1.6570
Training Epoch: 8 [38400/50048]	Loss: 1.6756
Training Epoch: 8 [38528/50048]	Loss: 1.8057
Training Epoch: 8 [38656/50048]	Loss: 1.7693
Training Epoch: 8 [38784/50048]	Loss: 1.8805
Training Epoch: 8 [38912/50048]	Loss: 1.6238
Training Epoch: 8 [39040/50048]	Loss: 1.8710
Training Epoch: 8 [39168/50048]	Loss: 1.5180
Training Epoch: 8 [39296/50048]	Loss: 2.0600
Training Epoch: 8 [39424/50048]	Loss: 1.6500
Training Epoch: 8 [39552/50048]	Loss: 1.6676
Training Epoch: 8 [39680/50048]	Loss: 1.9310
Training Epoch: 8 [39808/50048]	Loss: 1.8341
Training Epoch: 8 [39936/50048]	Loss: 1.8698
Training Epoch: 8 [40064/50048]	Loss: 1.9442
Training Epoch: 8 [40192/50048]	Loss: 1.7050
Training Epoch: 8 [40320/50048]	Loss: 1.5916
Training Epoch: 8 [40448/50048]	Loss: 1.6963
Training Epoch: 8 [40576/50048]	Loss: 1.7942
Training Epoch: 8 [40704/50048]	Loss: 1.4952
Training Epoch: 8 [40832/50048]	Loss: 1.6626
Training Epoch: 8 [40960/50048]	Loss: 1.6466
Training Epoch: 8 [41088/50048]	Loss: 1.7816
Training Epoch: 8 [41216/50048]	Loss: 1.4071
Training Epoch: 8 [41344/50048]	Loss: 1.6409
Training Epoch: 8 [41472/50048]	Loss: 1.6349
Training Epoch: 8 [41600/50048]	Loss: 1.4906
Training Epoch: 8 [41728/50048]	Loss: 1.7495
Training Epoch: 8 [41856/50048]	Loss: 2.0520
Training Epoch: 8 [41984/50048]	Loss: 1.8264
Training Epoch: 8 [42112/50048]	Loss: 1.6855
Training Epoch: 8 [42240/50048]	Loss: 1.7611
Training Epoch: 8 [42368/50048]	Loss: 1.7873
Training Epoch: 8 [42496/50048]	Loss: 1.7928
Training Epoch: 8 [42624/50048]	Loss: 1.6602
Training Epoch: 8 [42752/50048]	Loss: 1.7200
Training Epoch: 8 [42880/50048]	Loss: 1.7665
Training Epoch: 8 [43008/50048]	Loss: 1.7585
Training Epoch: 8 [43136/50048]	Loss: 1.5750
Training Epoch: 8 [43264/50048]	Loss: 1.6243
Training Epoch: 8 [43392/50048]	Loss: 1.7236
Training Epoch: 8 [43520/50048]	Loss: 1.8310
Training Epoch: 8 [43648/50048]	Loss: 1.7001
Training Epoch: 8 [43776/50048]	Loss: 2.0459
Training Epoch: 8 [43904/50048]	Loss: 2.0293
Training Epoch: 8 [44032/50048]	Loss: 1.6588
Training Epoch: 8 [44160/50048]	Loss: 1.5243
Training Epoch: 8 [44288/50048]	Loss: 1.6694
Training Epoch: 8 [44416/50048]	Loss: 1.8890
Training Epoch: 8 [44544/50048]	Loss: 1.6970
Training Epoch: 8 [44672/50048]	Loss: 2.0819
Training Epoch: 8 [44800/50048]	Loss: 1.7674
Training Epoch: 8 [44928/50048]	Loss: 2.0853
Training Epoch: 8 [45056/50048]	Loss: 1.5341
Training Epoch: 8 [45184/50048]	Loss: 1.9517
Training Epoch: 8 [45312/50048]	Loss: 1.7859
Training Epoch: 8 [45440/50048]	Loss: 1.6407
Training Epoch: 8 [45568/50048]	Loss: 1.8078
Training Epoch: 8 [45696/50048]	Loss: 1.7024
Training Epoch: 8 [45824/50048]	Loss: 1.6457
Training Epoch: 8 [45952/50048]	Loss: 1.9294
Training Epoch: 8 [46080/50048]	Loss: 1.4667
Training Epoch: 8 [46208/50048]	Loss: 1.6776
Training Epoch: 8 [46336/50048]	Loss: 1.5615
Training Epoch: 8 [46464/50048]	Loss: 1.7318
Training Epoch: 8 [46592/50048]	Loss: 2.0187
Training Epoch: 8 [46720/50048]	Loss: 1.6116
2022-12-06 03:50:53,449 [ZeusDataLoader(train)] train epoch 9 done: time=86.41 energy=10501.04
2022-12-06 03:50:53,450 [ZeusDataLoader(eval)] Epoch 9 begin.
Training Epoch: 8 [46848/50048]	Loss: 2.0532
Training Epoch: 8 [46976/50048]	Loss: 1.8628
Training Epoch: 8 [47104/50048]	Loss: 1.8195
Training Epoch: 8 [47232/50048]	Loss: 1.6912
Training Epoch: 8 [47360/50048]	Loss: 1.7843
Training Epoch: 8 [47488/50048]	Loss: 1.6014
Training Epoch: 8 [47616/50048]	Loss: 1.7908
Training Epoch: 8 [47744/50048]	Loss: 1.7780
Training Epoch: 8 [47872/50048]	Loss: 1.6530
Training Epoch: 8 [48000/50048]	Loss: 1.4838
Training Epoch: 8 [48128/50048]	Loss: 1.7670
Training Epoch: 8 [48256/50048]	Loss: 1.7648
Training Epoch: 8 [48384/50048]	Loss: 1.9923
Training Epoch: 8 [48512/50048]	Loss: 1.8636
Training Epoch: 8 [48640/50048]	Loss: 1.8009
Training Epoch: 8 [48768/50048]	Loss: 1.7799
Training Epoch: 8 [48896/50048]	Loss: 1.5848
Training Epoch: 8 [49024/50048]	Loss: 1.4763
Training Epoch: 8 [49152/50048]	Loss: 1.7709
Training Epoch: 8 [49280/50048]	Loss: 1.8291
Training Epoch: 8 [49408/50048]	Loss: 1.5276
Training Epoch: 8 [49536/50048]	Loss: 1.8096
Training Epoch: 8 [49664/50048]	Loss: 1.7623
Training Epoch: 8 [49792/50048]	Loss: 1.7607
Training Epoch: 8 [49920/50048]	Loss: 1.6234
Training Epoch: 8 [50048/50048]	Loss: 1.5809
2022-12-06 08:50:57.163 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 03:50:57,175 [ZeusDataLoader(eval)] eval epoch 9 done: time=3.72 energy=452.35
2022-12-06 03:50:57,176 [ZeusDataLoader(train)] Up to epoch 9: time=814.76, energy=98487.19, cost=120535.50
2022-12-06 03:50:57,176 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 03:50:57,176 [ZeusDataLoader(train)] Expected next epoch: time=904.56, energy=109285.21, cost=133791.89
2022-12-06 03:50:57,177 [ZeusDataLoader(train)] Epoch 10 begin.
Validation Epoch: 8, Average loss: 0.0142, Accuracy: 0.5096
2022-12-06 03:50:57,360 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 03:50:57,361 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 08:50:57.363 [ZeusMonitor] Monitor started.
2022-12-06 08:50:57.363 [ZeusMonitor] Running indefinitely. 2022-12-06 08:50:57.363 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 08:50:57.363 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e10+gpu0.power.log
Training Epoch: 9 [128/50048]	Loss: 1.7105
Training Epoch: 9 [256/50048]	Loss: 1.5528
Training Epoch: 9 [384/50048]	Loss: 1.5466
Training Epoch: 9 [512/50048]	Loss: 1.7357
Training Epoch: 9 [640/50048]	Loss: 1.8327
Training Epoch: 9 [768/50048]	Loss: 1.6471
Training Epoch: 9 [896/50048]	Loss: 1.7263
Training Epoch: 9 [1024/50048]	Loss: 1.6226
Training Epoch: 9 [1152/50048]	Loss: 1.6346
Training Epoch: 9 [1280/50048]	Loss: 1.8761
Training Epoch: 9 [1408/50048]	Loss: 1.6848
Training Epoch: 9 [1536/50048]	Loss: 1.5146
Training Epoch: 9 [1664/50048]	Loss: 1.5623
Training Epoch: 9 [1792/50048]	Loss: 1.4157
Training Epoch: 9 [1920/50048]	Loss: 1.8285
Training Epoch: 9 [2048/50048]	Loss: 1.7299
Training Epoch: 9 [2176/50048]	Loss: 1.7145
Training Epoch: 9 [2304/50048]	Loss: 1.5370
Training Epoch: 9 [2432/50048]	Loss: 1.4618
Training Epoch: 9 [2560/50048]	Loss: 1.7439
Training Epoch: 9 [2688/50048]	Loss: 1.7557
Training Epoch: 9 [2816/50048]	Loss: 1.6754
Training Epoch: 9 [2944/50048]	Loss: 1.6659
Training Epoch: 9 [3072/50048]	Loss: 1.6236
Training Epoch: 9 [3200/50048]	Loss: 1.4189
Training Epoch: 9 [3328/50048]	Loss: 1.5550
Training Epoch: 9 [3456/50048]	Loss: 1.8522
Training Epoch: 9 [3584/50048]	Loss: 1.8861
Training Epoch: 9 [3712/50048]	Loss: 1.5532
Training Epoch: 9 [3840/50048]	Loss: 1.5144
Training Epoch: 9 [3968/50048]	Loss: 1.6594
Training Epoch: 9 [4096/50048]	Loss: 1.6459
Training Epoch: 9 [4224/50048]	Loss: 1.9245
Training Epoch: 9 [4352/50048]	Loss: 1.5449
Training Epoch: 9 [4480/50048]	Loss: 1.6446
Training Epoch: 9 [4608/50048]	Loss: 1.8411
Training Epoch: 9 [4736/50048]	Loss: 1.5685
Training Epoch: 9 [4864/50048]	Loss: 1.6163
Training Epoch: 9 [4992/50048]	Loss: 1.6011
Training Epoch: 9 [5120/50048]	Loss: 1.8418
Training Epoch: 9 [5248/50048]	Loss: 1.3510
Training Epoch: 9 [5376/50048]	Loss: 1.7952
Training Epoch: 9 [5504/50048]	Loss: 1.4344
Training Epoch: 9 [5632/50048]	Loss: 1.7638
Training Epoch: 9 [5760/50048]	Loss: 1.5373
Training Epoch: 9 [5888/50048]	Loss: 1.5409
Training Epoch: 9 [6016/50048]	Loss: 1.5572
Training Epoch: 9 [6144/50048]	Loss: 1.4470
Training Epoch: 9 [6272/50048]	Loss: 1.6289
Training Epoch: 9 [6400/50048]	Loss: 1.8451
Training Epoch: 9 [6528/50048]	Loss: 1.6898
Training Epoch: 9 [6656/50048]	Loss: 1.9207
Training Epoch: 9 [6784/50048]	Loss: 1.7507
Training Epoch: 9 [6912/50048]	Loss: 1.7119
Training Epoch: 9 [7040/50048]	Loss: 1.5972
Training Epoch: 9 [7168/50048]	Loss: 1.6374
Training Epoch: 9 [7296/50048]	Loss: 1.4056
Training Epoch: 9 [7424/50048]	Loss: 1.7237
Training Epoch: 9 [7552/50048]	Loss: 1.4606
Training Epoch: 9 [7680/50048]	Loss: 1.6141
Training Epoch: 9 [7808/50048]	Loss: 1.6545
Training Epoch: 9 [7936/50048]	Loss: 1.8313
Training Epoch: 9 [8064/50048]	Loss: 1.4740
Training Epoch: 9 [8192/50048]	Loss: 1.8077
Training Epoch: 9 [8320/50048]	Loss: 1.6066
Training Epoch: 9 [8448/50048]	Loss: 1.5280
Training Epoch: 9 [8576/50048]	Loss: 1.5935
Training Epoch: 9 [8704/50048]	Loss: 1.6676
Training Epoch: 9 [8832/50048]	Loss: 1.5024
Training Epoch: 9 [8960/50048]	Loss: 1.7217
Training Epoch: 9 [9088/50048]	Loss: 1.6293
Training Epoch: 9 [9216/50048]	Loss: 1.8001
Training Epoch: 9 [9344/50048]	Loss: 1.4228
Training Epoch: 9 [9472/50048]	Loss: 1.5879
Training Epoch: 9 [9600/50048]	Loss: 1.4748
Training Epoch: 9 [9728/50048]	Loss: 1.7756
Training Epoch: 9 [9856/50048]	Loss: 1.8779
Training Epoch: 9 [9984/50048]	Loss: 1.6792
Training Epoch: 9 [10112/50048]	Loss: 1.8625
Training Epoch: 9 [10240/50048]	Loss: 1.5827
Training Epoch: 9 [10368/50048]	Loss: 1.8090
Training Epoch: 9 [10496/50048]	Loss: 1.6816
Training Epoch: 9 [10624/50048]	Loss: 1.6942
Training Epoch: 9 [10752/50048]	Loss: 1.8621
Training Epoch: 9 [10880/50048]	Loss: 1.8068
Training Epoch: 9 [11008/50048]	Loss: 1.6002
Training Epoch: 9 [11136/50048]	Loss: 1.6367
Training Epoch: 9 [11264/50048]	Loss: 1.8128
Training Epoch: 9 [11392/50048]	Loss: 1.6123
Training Epoch: 9 [11520/50048]	Loss: 1.5303
Training Epoch: 9 [11648/50048]	Loss: 1.8146
Training Epoch: 9 [11776/50048]	Loss: 1.5672
Training Epoch: 9 [11904/50048]	Loss: 1.6944
Training Epoch: 9 [12032/50048]	Loss: 1.7591
Training Epoch: 9 [12160/50048]	Loss: 1.7335
Training Epoch: 9 [12288/50048]	Loss: 1.8145
Training Epoch: 9 [12416/50048]	Loss: 1.5860
Training Epoch: 9 [12544/50048]	Loss: 1.4905
Training Epoch: 9 [12672/50048]	Loss: 1.4631
Training Epoch: 9 [12800/50048]	Loss: 1.6202
Training Epoch: 9 [12928/50048]	Loss: 1.5787
Training Epoch: 9 [13056/50048]	Loss: 1.5474
Training Epoch: 9 [13184/50048]	Loss: 1.7719
Training Epoch: 9 [13312/50048]	Loss: 1.3853
Training Epoch: 9 [13440/50048]	Loss: 1.7978
Training Epoch: 9 [13568/50048]	Loss: 1.6336
Training Epoch: 9 [13696/50048]	Loss: 2.0120
Training Epoch: 9 [13824/50048]	Loss: 1.4438
Training Epoch: 9 [13952/50048]	Loss: 1.6870
Training Epoch: 9 [14080/50048]	Loss: 1.5711
Training Epoch: 9 [14208/50048]	Loss: 1.7142
Training Epoch: 9 [14336/50048]	Loss: 1.5497
Training Epoch: 9 [14464/50048]	Loss: 1.4443
Training Epoch: 9 [14592/50048]	Loss: 1.8044
Training Epoch: 9 [14720/50048]	Loss: 1.6658
Training Epoch: 9 [14848/50048]	Loss: 1.7170
Training Epoch: 9 [14976/50048]	Loss: 1.6874
Training Epoch: 9 [15104/50048]	Loss: 1.5019
Training Epoch: 9 [15232/50048]	Loss: 1.8721
Training Epoch: 9 [15360/50048]	Loss: 1.6900
Training Epoch: 9 [15488/50048]	Loss: 2.0074
Training Epoch: 9 [15616/50048]	Loss: 1.5546
Training Epoch: 9 [15744/50048]	Loss: 1.6901
Training Epoch: 9 [15872/50048]	Loss: 1.7370
Training Epoch: 9 [16000/50048]	Loss: 1.6187
Training Epoch: 9 [16128/50048]	Loss: 1.9742
Training Epoch: 9 [16256/50048]	Loss: 1.9412
Training Epoch: 9 [16384/50048]	Loss: 1.4938
Training Epoch: 9 [16512/50048]	Loss: 1.6627
Training Epoch: 9 [16640/50048]	Loss: 1.4800
Training Epoch: 9 [16768/50048]	Loss: 1.5549
Training Epoch: 9 [16896/50048]	Loss: 1.7256
Training Epoch: 9 [17024/50048]	Loss: 1.5688
Training Epoch: 9 [17152/50048]	Loss: 1.8459
Training Epoch: 9 [17280/50048]	Loss: 1.3925
Training Epoch: 9 [17408/50048]	Loss: 1.3774
Training Epoch: 9 [17536/50048]	Loss: 1.8246
Training Epoch: 9 [17664/50048]	Loss: 1.6834
Training Epoch: 9 [17792/50048]	Loss: 1.4134
Training Epoch: 9 [17920/50048]	Loss: 1.6894
Training Epoch: 9 [18048/50048]	Loss: 1.7018
Training Epoch: 9 [18176/50048]	Loss: 1.5828
Training Epoch: 9 [18304/50048]	Loss: 1.8018
Training Epoch: 9 [18432/50048]	Loss: 1.6010
Training Epoch: 9 [18560/50048]	Loss: 1.5058
Training Epoch: 9 [18688/50048]	Loss: 1.4679
Training Epoch: 9 [18816/50048]	Loss: 1.7309
Training Epoch: 9 [18944/50048]	Loss: 1.6032
Training Epoch: 9 [19072/50048]	Loss: 1.7808
Training Epoch: 9 [19200/50048]	Loss: 1.5404
Training Epoch: 9 [19328/50048]	Loss: 1.5448
Training Epoch: 9 [19456/50048]	Loss: 1.8479
Training Epoch: 9 [19584/50048]	Loss: 1.5122
Training Epoch: 9 [19712/50048]	Loss: 1.5250
Training Epoch: 9 [19840/50048]	Loss: 1.7946
Training Epoch: 9 [19968/50048]	Loss: 1.9027
Training Epoch: 9 [20096/50048]	Loss: 1.8251
Training Epoch: 9 [20224/50048]	Loss: 1.6305
Training Epoch: 9 [20352/50048]	Loss: 1.6649
Training Epoch: 9 [20480/50048]	Loss: 1.5673
Training Epoch: 9 [20608/50048]	Loss: 1.6669
Training Epoch: 9 [20736/50048]	Loss: 1.5422
Training Epoch: 9 [20864/50048]	Loss: 1.4711
Training Epoch: 9 [20992/50048]	Loss: 1.6777
Training Epoch: 9 [21120/50048]	Loss: 1.5411
Training Epoch: 9 [21248/50048]	Loss: 1.6544
Training Epoch: 9 [21376/50048]	Loss: 1.8214
Training Epoch: 9 [21504/50048]	Loss: 1.4931
Training Epoch: 9 [21632/50048]	Loss: 1.8081
Training Epoch: 9 [21760/50048]	Loss: 1.5122
Training Epoch: 9 [21888/50048]	Loss: 1.7803
Training Epoch: 9 [22016/50048]	Loss: 1.4829
Training Epoch: 9 [22144/50048]	Loss: 1.4905
Training Epoch: 9 [22272/50048]	Loss: 1.6453
Training Epoch: 9 [22400/50048]	Loss: 1.4937
Training Epoch: 9 [22528/50048]	Loss: 1.7292
Training Epoch: 9 [22656/50048]	Loss: 1.4803
Training Epoch: 9 [22784/50048]	Loss: 1.5804
Training Epoch: 9 [22912/50048]	Loss: 2.1676
Training Epoch: 9 [23040/50048]	Loss: 1.7257
Training Epoch: 9 [23168/50048]	Loss: 1.4428
Training Epoch: 9 [23296/50048]	Loss: 1.7209
Training Epoch: 9 [23424/50048]	Loss: 1.3495
Training Epoch: 9 [23552/50048]	Loss: 1.6882
Training Epoch: 9 [23680/50048]	Loss: 1.6363
Training Epoch: 9 [23808/50048]	Loss: 1.3396
Training Epoch: 9 [23936/50048]	Loss: 1.6125
Training Epoch: 9 [24064/50048]	Loss: 1.6039
Training Epoch: 9 [24192/50048]	Loss: 1.5695
Training Epoch: 9 [24320/50048]	Loss: 1.8438
Training Epoch: 9 [24448/50048]	Loss: 1.5148
Training Epoch: 9 [24576/50048]	Loss: 1.6239
Training Epoch: 9 [24704/50048]	Loss: 1.5475
Training Epoch: 9 [24832/50048]	Loss: 1.3812
Training Epoch: 9 [24960/50048]	Loss: 1.5864
Training Epoch: 9 [25088/50048]	Loss: 1.5203
Training Epoch: 9 [25216/50048]	Loss: 1.5814
Training Epoch: 9 [25344/50048]	Loss: 1.4758
Training Epoch: 9 [25472/50048]	Loss: 1.5721
Training Epoch: 9 [25600/50048]	Loss: 1.6257
Training Epoch: 9 [25728/50048]	Loss: 1.5178
Training Epoch: 9 [25856/50048]	Loss: 1.5478
Training Epoch: 9 [25984/50048]	Loss: 1.6759
Training Epoch: 9 [26112/50048]	Loss: 1.4628
Training Epoch: 9 [26240/50048]	Loss: 1.6844
Training Epoch: 9 [26368/50048]	Loss: 1.6671
Training Epoch: 9 [26496/50048]	Loss: 1.6012
Training Epoch: 9 [26624/50048]	Loss: 1.4741
Training Epoch: 9 [26752/50048]	Loss: 1.5785
Training Epoch: 9 [26880/50048]	Loss: 1.8597
Training Epoch: 9 [27008/50048]	Loss: 1.7393
Training Epoch: 9 [27136/50048]	Loss: 1.6228
Training Epoch: 9 [27264/50048]	Loss: 1.8121
Training Epoch: 9 [27392/50048]	Loss: 1.6075
Training Epoch: 9 [27520/50048]	Loss: 1.6125
Training Epoch: 9 [27648/50048]	Loss: 1.8642
Training Epoch: 9 [27776/50048]	Loss: 1.4593
Training Epoch: 9 [27904/50048]	Loss: 1.5256
Training Epoch: 9 [28032/50048]	Loss: 1.6278
Training Epoch: 9 [28160/50048]	Loss: 1.5719
Training Epoch: 9 [28288/50048]	Loss: 1.6997
Training Epoch: 9 [28416/50048]	Loss: 1.7180
Training Epoch: 9 [28544/50048]	Loss: 1.7170
Training Epoch: 9 [28672/50048]	Loss: 1.8475
Training Epoch: 9 [28800/50048]	Loss: 1.5732
Training Epoch: 9 [28928/50048]	Loss: 1.8462
Training Epoch: 9 [29056/50048]	Loss: 1.8282
Training Epoch: 9 [29184/50048]	Loss: 1.6939
Training Epoch: 9 [29312/50048]	Loss: 1.5383
Training Epoch: 9 [29440/50048]	Loss: 1.6697
Training Epoch: 9 [29568/50048]	Loss: 1.7059
Training Epoch: 9 [29696/50048]	Loss: 1.6449
Training Epoch: 9 [29824/50048]	Loss: 1.2313
Training Epoch: 9 [29952/50048]	Loss: 1.6137
Training Epoch: 9 [30080/50048]	Loss: 1.7191
Training Epoch: 9 [30208/50048]	Loss: 1.4351
Training Epoch: 9 [30336/50048]	Loss: 1.7184
Training Epoch: 9 [30464/50048]	Loss: 1.5117
Training Epoch: 9 [30592/50048]	Loss: 1.6653
Training Epoch: 9 [30720/50048]	Loss: 1.5638
Training Epoch: 9 [30848/50048]	Loss: 1.4578
Training Epoch: 9 [30976/50048]	Loss: 1.6730
Training Epoch: 9 [31104/50048]	Loss: 1.5083
Training Epoch: 9 [31232/50048]	Loss: 1.4678
Training Epoch: 9 [31360/50048]	Loss: 1.7350
Training Epoch: 9 [31488/50048]	Loss: 1.6634
Training Epoch: 9 [31616/50048]	Loss: 1.7986
Training Epoch: 9 [31744/50048]	Loss: 1.7564
Training Epoch: 9 [31872/50048]	Loss: 1.9356
Training Epoch: 9 [32000/50048]	Loss: 1.8223
Training Epoch: 9 [32128/50048]	Loss: 1.5566
Training Epoch: 9 [32256/50048]	Loss: 1.6301
Training Epoch: 9 [32384/50048]	Loss: 1.6181
Training Epoch: 9 [32512/50048]	Loss: 1.5255
Training Epoch: 9 [32640/50048]	Loss: 1.5917
Training Epoch: 9 [32768/50048]	Loss: 1.4165
Training Epoch: 9 [32896/50048]	Loss: 1.6807
Training Epoch: 9 [33024/50048]	Loss: 1.6768
Training Epoch: 9 [33152/50048]	Loss: 1.7463
Training Epoch: 9 [33280/50048]	Loss: 1.5257
Training Epoch: 9 [33408/50048]	Loss: 1.8004
Training Epoch: 9 [33536/50048]	Loss: 1.4576
Training Epoch: 9 [33664/50048]	Loss: 1.4831
Training Epoch: 9 [33792/50048]	Loss: 1.4656
Training Epoch: 9 [33920/50048]	Loss: 1.7039
Training Epoch: 9 [34048/50048]	Loss: 1.9174
Training Epoch: 9 [34176/50048]	Loss: 1.2520
Training Epoch: 9 [34304/50048]	Loss: 1.9611
Training Epoch: 9 [34432/50048]	Loss: 1.7852
Training Epoch: 9 [34560/50048]	Loss: 1.5032
Training Epoch: 9 [34688/50048]	Loss: 1.5376
Training Epoch: 9 [34816/50048]	Loss: 1.9412
Training Epoch: 9 [34944/50048]	Loss: 1.7589
Training Epoch: 9 [35072/50048]	Loss: 1.5258
Training Epoch: 9 [35200/50048]	Loss: 1.5735
Training Epoch: 9 [35328/50048]	Loss: 1.3224
Training Epoch: 9 [35456/50048]	Loss: 1.6483
Training Epoch: 9 [35584/50048]	Loss: 1.3950
Training Epoch: 9 [35712/50048]	Loss: 1.8387
Training Epoch: 9 [35840/50048]	Loss: 1.7241
Training Epoch: 9 [35968/50048]	Loss: 1.5891
Training Epoch: 9 [36096/50048]	Loss: 1.5581
Training Epoch: 9 [36224/50048]	Loss: 1.6908
Training Epoch: 9 [36352/50048]	Loss: 1.4952
Training Epoch: 9 [36480/50048]	Loss: 1.9119
Training Epoch: 9 [36608/50048]	Loss: 1.5244
Training Epoch: 9 [36736/50048]	Loss: 1.6487
Training Epoch: 9 [36864/50048]	Loss: 1.5373
Training Epoch: 9 [36992/50048]	Loss: 1.7055
Training Epoch: 9 [37120/50048]	Loss: 1.7486
Training Epoch: 9 [37248/50048]	Loss: 1.8449
Training Epoch: 9 [37376/50048]	Loss: 1.6975
Training Epoch: 9 [37504/50048]	Loss: 1.7473
Training Epoch: 9 [37632/50048]	Loss: 1.6609
Training Epoch: 9 [37760/50048]	Loss: 1.4256
Training Epoch: 9 [37888/50048]	Loss: 1.4998
Training Epoch: 9 [38016/50048]	Loss: 1.9009
Training Epoch: 9 [38144/50048]	Loss: 1.3919
Training Epoch: 9 [38272/50048]	Loss: 1.5171
Training Epoch: 9 [38400/50048]	Loss: 1.7406
Training Epoch: 9 [38528/50048]	Loss: 1.7349
Training Epoch: 9 [38656/50048]	Loss: 1.5684
Training Epoch: 9 [38784/50048]	Loss: 1.6149
Training Epoch: 9 [38912/50048]	Loss: 1.6347
Training Epoch: 9 [39040/50048]	Loss: 1.4781
Training Epoch: 9 [39168/50048]	Loss: 1.7291
Training Epoch: 9 [39296/50048]	Loss: 1.5473
Training Epoch: 9 [39424/50048]	Loss: 1.5833
Training Epoch: 9 [39552/50048]	Loss: 1.8393
Training Epoch: 9 [39680/50048]	Loss: 1.8811
Training Epoch: 9 [39808/50048]	Loss: 1.7546
Training Epoch: 9 [39936/50048]	Loss: 1.7169
Training Epoch: 9 [40064/50048]	Loss: 1.7907
Training Epoch: 9 [40192/50048]	Loss: 1.5630
Training Epoch: 9 [40320/50048]	Loss: 1.7354
Training Epoch: 9 [40448/50048]	Loss: 1.5025
Training Epoch: 9 [40576/50048]	Loss: 1.6945
Training Epoch: 9 [40704/50048]	Loss: 1.8039
Training Epoch: 9 [40832/50048]	Loss: 1.7873
Training Epoch: 9 [40960/50048]	Loss: 1.7463
Training Epoch: 9 [41088/50048]	Loss: 1.7137
Training Epoch: 9 [41216/50048]	Loss: 1.7171
Training Epoch: 9 [41344/50048]	Loss: 1.6133
Training Epoch: 9 [41472/50048]	Loss: 1.6663
Training Epoch: 9 [41600/50048]	Loss: 1.4157
Training Epoch: 9 [41728/50048]	Loss: 1.5439
Training Epoch: 9 [41856/50048]	Loss: 1.5955
Training Epoch: 9 [41984/50048]	Loss: 1.3445
Training Epoch: 9 [42112/50048]	Loss: 1.5269
Training Epoch: 9 [42240/50048]	Loss: 1.4753
Training Epoch: 9 [42368/50048]	Loss: 1.7272
Training Epoch: 9 [42496/50048]	Loss: 1.4595
Training Epoch: 9 [42624/50048]	Loss: 1.6828
Training Epoch: 9 [42752/50048]	Loss: 1.4627
Training Epoch: 9 [42880/50048]	Loss: 1.7159
Training Epoch: 9 [43008/50048]	Loss: 2.0554
Training Epoch: 9 [43136/50048]	Loss: 1.5444
Training Epoch: 9 [43264/50048]	Loss: 1.8809
Training Epoch: 9 [43392/50048]	Loss: 1.8044
Training Epoch: 9 [43520/50048]	Loss: 1.6159
Training Epoch: 9 [43648/50048]	Loss: 1.8269
Training Epoch: 9 [43776/50048]	Loss: 1.6036
Training Epoch: 9 [43904/50048]	Loss: 1.5451
Training Epoch: 9 [44032/50048]	Loss: 1.7143
Training Epoch: 9 [44160/50048]	Loss: 1.6139
Training Epoch: 9 [44288/50048]	Loss: 1.7161
Training Epoch: 9 [44416/50048]	Loss: 1.4333
Training Epoch: 9 [44544/50048]	Loss: 1.6045
Training Epoch: 9 [44672/50048]	Loss: 1.6328
Training Epoch: 9 [44800/50048]	Loss: 1.6091
Training Epoch: 9 [44928/50048]	Loss: 1.7052
Training Epoch: 9 [45056/50048]	Loss: 1.7973
Training Epoch: 9 [45184/50048]	Loss: 1.6132
Training Epoch: 9 [45312/50048]	Loss: 1.7355
Training Epoch: 9 [45440/50048]	Loss: 1.5303
Training Epoch: 9 [45568/50048]	Loss: 1.7480
Training Epoch: 9 [45696/50048]	Loss: 1.4874
Training Epoch: 9 [45824/50048]	Loss: 1.4457
Training Epoch: 9 [45952/50048]	Loss: 1.5737
Training Epoch: 9 [46080/50048]	Loss: 1.5467
Training Epoch: 9 [46208/50048]	Loss: 1.5470
Training Epoch: 9 [46336/50048]	Loss: 1.7049
Training Epoch: 9 [46464/50048]	Loss: 1.8370
Training Epoch: 9 [46592/50048]	Loss: 1.8474
Training Epoch: 9 [46720/50048]	Loss: 1.5940
2022-12-06 03:52:23,644 [ZeusDataLoader(train)] train epoch 10 done: time=86.46 energy=10502.32
2022-12-06 03:52:23,645 [ZeusDataLoader(eval)] Epoch 10 begin.
Training Epoch: 9 [46848/50048]	Loss: 1.4616
Training Epoch: 9 [46976/50048]	Loss: 1.7500
Training Epoch: 9 [47104/50048]	Loss: 1.5981
Training Epoch: 9 [47232/50048]	Loss: 1.8229
Training Epoch: 9 [47360/50048]	Loss: 1.5836
Training Epoch: 9 [47488/50048]	Loss: 1.6615
Training Epoch: 9 [47616/50048]	Loss: 1.5055
Training Epoch: 9 [47744/50048]	Loss: 1.8978
Training Epoch: 9 [47872/50048]	Loss: 1.7443
Training Epoch: 9 [48000/50048]	Loss: 1.3456
Training Epoch: 9 [48128/50048]	Loss: 1.4905
Training Epoch: 9 [48256/50048]	Loss: 1.4007
Training Epoch: 9 [48384/50048]	Loss: 1.7946
Training Epoch: 9 [48512/50048]	Loss: 1.3189
Training Epoch: 9 [48640/50048]	Loss: 1.5840
Training Epoch: 9 [48768/50048]	Loss: 1.5305
Training Epoch: 9 [48896/50048]	Loss: 1.6994
Training Epoch: 9 [49024/50048]	Loss: 1.6479
Training Epoch: 9 [49152/50048]	Loss: 1.5025
Training Epoch: 9 [49280/50048]	Loss: 1.5335
Training Epoch: 9 [49408/50048]	Loss: 1.5126
Training Epoch: 9 [49536/50048]	Loss: 1.8196
Training Epoch: 9 [49664/50048]	Loss: 1.6857
Training Epoch: 9 [49792/50048]	Loss: 1.4475
Training Epoch: 9 [49920/50048]	Loss: 1.4961
Training Epoch: 9 [50048/50048]	Loss: 1.8401
2022-12-06 08:52:27.321 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 03:52:27,336 [ZeusDataLoader(eval)] eval epoch 10 done: time=3.68 energy=452.30
2022-12-06 03:52:27,336 [ZeusDataLoader(train)] Up to epoch 10: time=904.90, energy=109441.81, cost=133899.95
2022-12-06 03:52:27,337 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 03:52:27,337 [ZeusDataLoader(train)] Expected next epoch: time=994.70, energy=120239.83, cost=147156.33
2022-12-06 03:52:27,338 [ZeusDataLoader(train)] Epoch 11 begin.
Validation Epoch: 9, Average loss: 0.0141, Accuracy: 0.5106
2022-12-06 03:52:27,511 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 03:52:27,512 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 08:52:27.513 [ZeusMonitor] Monitor started.
2022-12-06 08:52:27.514 [ZeusMonitor] Running indefinitely. 2022-12-06 08:52:27.514 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 08:52:27.514 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e11+gpu0.power.log
Training Epoch: 10 [128/50048]	Loss: 1.5449
Training Epoch: 10 [256/50048]	Loss: 1.5318
Training Epoch: 10 [384/50048]	Loss: 1.6691
Training Epoch: 10 [512/50048]	Loss: 1.4741
Training Epoch: 10 [640/50048]	Loss: 1.6129
Training Epoch: 10 [768/50048]	Loss: 1.5979
Training Epoch: 10 [896/50048]	Loss: 1.3821
Training Epoch: 10 [1024/50048]	Loss: 1.8527
Training Epoch: 10 [1152/50048]	Loss: 1.3771
Training Epoch: 10 [1280/50048]	Loss: 1.4051
Training Epoch: 10 [1408/50048]	Loss: 1.4630
Training Epoch: 10 [1536/50048]	Loss: 1.4610
Training Epoch: 10 [1664/50048]	Loss: 1.7349
Training Epoch: 10 [1792/50048]	Loss: 1.6481
Training Epoch: 10 [1920/50048]	Loss: 1.4528
Training Epoch: 10 [2048/50048]	Loss: 1.2852
Training Epoch: 10 [2176/50048]	Loss: 1.6831
Training Epoch: 10 [2304/50048]	Loss: 1.4863
Training Epoch: 10 [2432/50048]	Loss: 1.2811
Training Epoch: 10 [2560/50048]	Loss: 1.5599
Training Epoch: 10 [2688/50048]	Loss: 1.6889
Training Epoch: 10 [2816/50048]	Loss: 1.3789
Training Epoch: 10 [2944/50048]	Loss: 1.5024
Training Epoch: 10 [3072/50048]	Loss: 1.7039
Training Epoch: 10 [3200/50048]	Loss: 1.7467
Training Epoch: 10 [3328/50048]	Loss: 1.6429
Training Epoch: 10 [3456/50048]	Loss: 1.4841
Training Epoch: 10 [3584/50048]	Loss: 1.6487
Training Epoch: 10 [3712/50048]	Loss: 1.3902
Training Epoch: 10 [3840/50048]	Loss: 1.6518
Training Epoch: 10 [3968/50048]	Loss: 1.4516
Training Epoch: 10 [4096/50048]	Loss: 1.6685
Training Epoch: 10 [4224/50048]	Loss: 1.7452
Training Epoch: 10 [4352/50048]	Loss: 1.5285
Training Epoch: 10 [4480/50048]	Loss: 1.3359
Training Epoch: 10 [4608/50048]	Loss: 1.5583
Training Epoch: 10 [4736/50048]	Loss: 1.6242
Training Epoch: 10 [4864/50048]	Loss: 1.3714
Training Epoch: 10 [4992/50048]	Loss: 1.5808
Training Epoch: 10 [5120/50048]	Loss: 1.5340
Training Epoch: 10 [5248/50048]	Loss: 1.5957
Training Epoch: 10 [5376/50048]	Loss: 1.6570
Training Epoch: 10 [5504/50048]	Loss: 1.6018
Training Epoch: 10 [5632/50048]	Loss: 1.5228
Training Epoch: 10 [5760/50048]	Loss: 1.5981
Training Epoch: 10 [5888/50048]	Loss: 1.4562
Training Epoch: 10 [6016/50048]	Loss: 1.6912
Training Epoch: 10 [6144/50048]	Loss: 1.3371
Training Epoch: 10 [6272/50048]	Loss: 1.4148
Training Epoch: 10 [6400/50048]	Loss: 1.7760
Training Epoch: 10 [6528/50048]	Loss: 1.4694
Training Epoch: 10 [6656/50048]	Loss: 1.6939
Training Epoch: 10 [6784/50048]	Loss: 1.5528
Training Epoch: 10 [6912/50048]	Loss: 1.5804
Training Epoch: 10 [7040/50048]	Loss: 1.5307
Training Epoch: 10 [7168/50048]	Loss: 1.5224
Training Epoch: 10 [7296/50048]	Loss: 1.4480
Training Epoch: 10 [7424/50048]	Loss: 1.5171
Training Epoch: 10 [7552/50048]	Loss: 1.5368
Training Epoch: 10 [7680/50048]	Loss: 1.6638
Training Epoch: 10 [7808/50048]	Loss: 1.9025
Training Epoch: 10 [7936/50048]	Loss: 1.3438
Training Epoch: 10 [8064/50048]	Loss: 1.4852
Training Epoch: 10 [8192/50048]	Loss: 1.5207
Training Epoch: 10 [8320/50048]	Loss: 1.5988
Training Epoch: 10 [8448/50048]	Loss: 1.3533
Training Epoch: 10 [8576/50048]	Loss: 1.4924
Training Epoch: 10 [8704/50048]	Loss: 1.3994
Training Epoch: 10 [8832/50048]	Loss: 1.6575
Training Epoch: 10 [8960/50048]	Loss: 1.6353
Training Epoch: 10 [9088/50048]	Loss: 1.3881
Training Epoch: 10 [9216/50048]	Loss: 1.6397
Training Epoch: 10 [9344/50048]	Loss: 1.3636
Training Epoch: 10 [9472/50048]	Loss: 1.5971
Training Epoch: 10 [9600/50048]	Loss: 1.3634
Training Epoch: 10 [9728/50048]	Loss: 1.4227
Training Epoch: 10 [9856/50048]	Loss: 1.5523
Training Epoch: 10 [9984/50048]	Loss: 1.5065
Training Epoch: 10 [10112/50048]	Loss: 1.5239
Training Epoch: 10 [10240/50048]	Loss: 1.5344
Training Epoch: 10 [10368/50048]	Loss: 1.6072
Training Epoch: 10 [10496/50048]	Loss: 1.8064
Training Epoch: 10 [10624/50048]	Loss: 1.5400
Training Epoch: 10 [10752/50048]	Loss: 1.7962
Training Epoch: 10 [10880/50048]	Loss: 1.6712
Training Epoch: 10 [11008/50048]	Loss: 1.8172
Training Epoch: 10 [11136/50048]	Loss: 1.7200
Training Epoch: 10 [11264/50048]	Loss: 1.6396
Training Epoch: 10 [11392/50048]	Loss: 1.5352
Training Epoch: 10 [11520/50048]	Loss: 1.5016
Training Epoch: 10 [11648/50048]	Loss: 1.4634
Training Epoch: 10 [11776/50048]	Loss: 1.4876
Training Epoch: 10 [11904/50048]	Loss: 1.4881
Training Epoch: 10 [12032/50048]	Loss: 1.5282
Training Epoch: 10 [12160/50048]	Loss: 1.5491
Training Epoch: 10 [12288/50048]	Loss: 1.3862
Training Epoch: 10 [12416/50048]	Loss: 1.6886
Training Epoch: 10 [12544/50048]	Loss: 1.5792
Training Epoch: 10 [12672/50048]	Loss: 1.5153
Training Epoch: 10 [12800/50048]	Loss: 1.6467
Training Epoch: 10 [12928/50048]	Loss: 1.5574
Training Epoch: 10 [13056/50048]	Loss: 1.7207
Training Epoch: 10 [13184/50048]	Loss: 1.6603
Training Epoch: 10 [13312/50048]	Loss: 1.7847
Training Epoch: 10 [13440/50048]	Loss: 1.5345
Training Epoch: 10 [13568/50048]	Loss: 1.5363
Training Epoch: 10 [13696/50048]	Loss: 1.6130
Training Epoch: 10 [13824/50048]	Loss: 1.7425
Training Epoch: 10 [13952/50048]	Loss: 1.5524
Training Epoch: 10 [14080/50048]	Loss: 1.5646
Training Epoch: 10 [14208/50048]	Loss: 1.4361
Training Epoch: 10 [14336/50048]	Loss: 1.4299
Training Epoch: 10 [14464/50048]	Loss: 1.6430
Training Epoch: 10 [14592/50048]	Loss: 1.5767
Training Epoch: 10 [14720/50048]	Loss: 1.3540
Training Epoch: 10 [14848/50048]	Loss: 1.6708
Training Epoch: 10 [14976/50048]	Loss: 1.5319
Training Epoch: 10 [15104/50048]	Loss: 1.6096
Training Epoch: 10 [15232/50048]	Loss: 1.6292
Training Epoch: 10 [15360/50048]	Loss: 1.6928
Training Epoch: 10 [15488/50048]	Loss: 1.6513
Training Epoch: 10 [15616/50048]	Loss: 1.6673
Training Epoch: 10 [15744/50048]	Loss: 1.4592
Training Epoch: 10 [15872/50048]	Loss: 1.7117
Training Epoch: 10 [16000/50048]	Loss: 1.6940
Training Epoch: 10 [16128/50048]	Loss: 1.5601
Training Epoch: 10 [16256/50048]	Loss: 1.4545
Training Epoch: 10 [16384/50048]	Loss: 1.5858
Training Epoch: 10 [16512/50048]	Loss: 1.5453
Training Epoch: 10 [16640/50048]	Loss: 1.8170
Training Epoch: 10 [16768/50048]	Loss: 1.5487
Training Epoch: 10 [16896/50048]	Loss: 1.9373
Training Epoch: 10 [17024/50048]	Loss: 1.5551
Training Epoch: 10 [17152/50048]	Loss: 1.6524
Training Epoch: 10 [17280/50048]	Loss: 1.6396
Training Epoch: 10 [17408/50048]	Loss: 1.5403
Training Epoch: 10 [17536/50048]	Loss: 1.5634
Training Epoch: 10 [17664/50048]	Loss: 1.5623
Training Epoch: 10 [17792/50048]	Loss: 1.6487
Training Epoch: 10 [17920/50048]	Loss: 1.3400
Training Epoch: 10 [18048/50048]	Loss: 1.5780
Training Epoch: 10 [18176/50048]	Loss: 1.4009
Training Epoch: 10 [18304/50048]	Loss: 1.5615
Training Epoch: 10 [18432/50048]	Loss: 1.4420
Training Epoch: 10 [18560/50048]	Loss: 1.5674
Training Epoch: 10 [18688/50048]	Loss: 1.4987
Training Epoch: 10 [18816/50048]	Loss: 1.6524
Training Epoch: 10 [18944/50048]	Loss: 1.6000
Training Epoch: 10 [19072/50048]	Loss: 1.7369
Training Epoch: 10 [19200/50048]	Loss: 1.5498
Training Epoch: 10 [19328/50048]	Loss: 1.5203
Training Epoch: 10 [19456/50048]	Loss: 1.7188
Training Epoch: 10 [19584/50048]	Loss: 1.5415
Training Epoch: 10 [19712/50048]	Loss: 1.7451
Training Epoch: 10 [19840/50048]	Loss: 1.6411
Training Epoch: 10 [19968/50048]	Loss: 1.6003
Training Epoch: 10 [20096/50048]	Loss: 1.5815
Training Epoch: 10 [20224/50048]	Loss: 1.6481
Training Epoch: 10 [20352/50048]	Loss: 1.3944
Training Epoch: 10 [20480/50048]	Loss: 1.6298
Training Epoch: 10 [20608/50048]	Loss: 1.5736
Training Epoch: 10 [20736/50048]	Loss: 1.5278
Training Epoch: 10 [20864/50048]	Loss: 1.5839
Training Epoch: 10 [20992/50048]	Loss: 1.5700
Training Epoch: 10 [21120/50048]	Loss: 1.4022
Training Epoch: 10 [21248/50048]	Loss: 1.8256
Training Epoch: 10 [21376/50048]	Loss: 1.5104
Training Epoch: 10 [21504/50048]	Loss: 1.6821
Training Epoch: 10 [21632/50048]	Loss: 1.5717
Training Epoch: 10 [21760/50048]	Loss: 1.4767
Training Epoch: 10 [21888/50048]	Loss: 1.3533
Training Epoch: 10 [22016/50048]	Loss: 1.6462
Training Epoch: 10 [22144/50048]	Loss: 1.4465
Training Epoch: 10 [22272/50048]	Loss: 1.5059
Training Epoch: 10 [22400/50048]	Loss: 1.5868
Training Epoch: 10 [22528/50048]	Loss: 1.8355
Training Epoch: 10 [22656/50048]	Loss: 1.6727
Training Epoch: 10 [22784/50048]	Loss: 1.5795
Training Epoch: 10 [22912/50048]	Loss: 1.3993
Training Epoch: 10 [23040/50048]	Loss: 1.4190
Training Epoch: 10 [23168/50048]	Loss: 1.6931
Training Epoch: 10 [23296/50048]	Loss: 1.5910
Training Epoch: 10 [23424/50048]	Loss: 1.3501
Training Epoch: 10 [23552/50048]	Loss: 1.4550
Training Epoch: 10 [23680/50048]	Loss: 1.2688
Training Epoch: 10 [23808/50048]	Loss: 1.6891
Training Epoch: 10 [23936/50048]	Loss: 1.7658
Training Epoch: 10 [24064/50048]	Loss: 1.5767
Training Epoch: 10 [24192/50048]	Loss: 1.6113
Training Epoch: 10 [24320/50048]	Loss: 1.6243
Training Epoch: 10 [24448/50048]	Loss: 1.6129
Training Epoch: 10 [24576/50048]	Loss: 1.6763
Training Epoch: 10 [24704/50048]	Loss: 1.4251
Training Epoch: 10 [24832/50048]	Loss: 1.4303
Training Epoch: 10 [24960/50048]	Loss: 1.6016
Training Epoch: 10 [25088/50048]	Loss: 1.6212
Training Epoch: 10 [25216/50048]	Loss: 1.5740
Training Epoch: 10 [25344/50048]	Loss: 1.3624
Training Epoch: 10 [25472/50048]	Loss: 1.7456
Training Epoch: 10 [25600/50048]	Loss: 1.3858
Training Epoch: 10 [25728/50048]	Loss: 1.4452
Training Epoch: 10 [25856/50048]	Loss: 1.5105
Training Epoch: 10 [25984/50048]	Loss: 1.4365
Training Epoch: 10 [26112/50048]	Loss: 1.6102
Training Epoch: 10 [26240/50048]	Loss: 1.2844
Training Epoch: 10 [26368/50048]	Loss: 1.5206
Training Epoch: 10 [26496/50048]	Loss: 1.5973
Training Epoch: 10 [26624/50048]	Loss: 1.4936
Training Epoch: 10 [26752/50048]	Loss: 1.8541
Training Epoch: 10 [26880/50048]	Loss: 1.4294
Training Epoch: 10 [27008/50048]	Loss: 1.4846
Training Epoch: 10 [27136/50048]	Loss: 1.6307
Training Epoch: 10 [27264/50048]	Loss: 1.3509
Training Epoch: 10 [27392/50048]	Loss: 1.6744
Training Epoch: 10 [27520/50048]	Loss: 1.3849
Training Epoch: 10 [27648/50048]	Loss: 1.4663
Training Epoch: 10 [27776/50048]	Loss: 1.4512
Training Epoch: 10 [27904/50048]	Loss: 1.4102
Training Epoch: 10 [28032/50048]	Loss: 1.4470
Training Epoch: 10 [28160/50048]	Loss: 1.4963
Training Epoch: 10 [28288/50048]	Loss: 1.5593
Training Epoch: 10 [28416/50048]	Loss: 1.8410
Training Epoch: 10 [28544/50048]	Loss: 1.6062
Training Epoch: 10 [28672/50048]	Loss: 1.6737
Training Epoch: 10 [28800/50048]	Loss: 1.6000
Training Epoch: 10 [28928/50048]	Loss: 1.7237
Training Epoch: 10 [29056/50048]	Loss: 1.4981
Training Epoch: 10 [29184/50048]	Loss: 1.6969
Training Epoch: 10 [29312/50048]	Loss: 1.7486
Training Epoch: 10 [29440/50048]	Loss: 1.5737
Training Epoch: 10 [29568/50048]	Loss: 1.3969
Training Epoch: 10 [29696/50048]	Loss: 1.7441
Training Epoch: 10 [29824/50048]	Loss: 1.6048
Training Epoch: 10 [29952/50048]	Loss: 1.5375
Training Epoch: 10 [30080/50048]	Loss: 1.3512
Training Epoch: 10 [30208/50048]	Loss: 1.4277
Training Epoch: 10 [30336/50048]	Loss: 1.3735
Training Epoch: 10 [30464/50048]	Loss: 1.5706
Training Epoch: 10 [30592/50048]	Loss: 1.5373
Training Epoch: 10 [30720/50048]	Loss: 1.5489
Training Epoch: 10 [30848/50048]	Loss: 1.4885
Training Epoch: 10 [30976/50048]	Loss: 1.5260
Training Epoch: 10 [31104/50048]	Loss: 1.4854
Training Epoch: 10 [31232/50048]	Loss: 1.7092
Training Epoch: 10 [31360/50048]	Loss: 1.5550
Training Epoch: 10 [31488/50048]	Loss: 1.8081
Training Epoch: 10 [31616/50048]	Loss: 1.7000
Training Epoch: 10 [31744/50048]	Loss: 1.4678
Training Epoch: 10 [31872/50048]	Loss: 1.7665
Training Epoch: 10 [32000/50048]	Loss: 1.5238
Training Epoch: 10 [32128/50048]	Loss: 1.6358
Training Epoch: 10 [32256/50048]	Loss: 1.3468
Training Epoch: 10 [32384/50048]	Loss: 1.5381
Training Epoch: 10 [32512/50048]	Loss: 1.3696
Training Epoch: 10 [32640/50048]	Loss: 1.5778
Training Epoch: 10 [32768/50048]	Loss: 1.6332
Training Epoch: 10 [32896/50048]	Loss: 1.4612
Training Epoch: 10 [33024/50048]	Loss: 1.4922
Training Epoch: 10 [33152/50048]	Loss: 1.2535
Training Epoch: 10 [33280/50048]	Loss: 1.5957
Training Epoch: 10 [33408/50048]	Loss: 1.7328
Training Epoch: 10 [33536/50048]	Loss: 1.5668
Training Epoch: 10 [33664/50048]	Loss: 1.3935
Training Epoch: 10 [33792/50048]	Loss: 1.8233
Training Epoch: 10 [33920/50048]	Loss: 1.6084
Training Epoch: 10 [34048/50048]	Loss: 1.6872
Training Epoch: 10 [34176/50048]	Loss: 1.5187
Training Epoch: 10 [34304/50048]	Loss: 1.4877
Training Epoch: 10 [34432/50048]	Loss: 1.7185
Training Epoch: 10 [34560/50048]	Loss: 1.6664
Training Epoch: 10 [34688/50048]	Loss: 1.5115
Training Epoch: 10 [34816/50048]	Loss: 1.5041
Training Epoch: 10 [34944/50048]	Loss: 1.7334
Training Epoch: 10 [35072/50048]	Loss: 1.4256
Training Epoch: 10 [35200/50048]	Loss: 1.5933
Training Epoch: 10 [35328/50048]	Loss: 1.5746
Training Epoch: 10 [35456/50048]	Loss: 1.6925
Training Epoch: 10 [35584/50048]	Loss: 1.4833
Training Epoch: 10 [35712/50048]	Loss: 1.7929
Training Epoch: 10 [35840/50048]	Loss: 1.6577
Training Epoch: 10 [35968/50048]	Loss: 1.6709
Training Epoch: 10 [36096/50048]	Loss: 1.8532
Training Epoch: 10 [36224/50048]	Loss: 1.5695
Training Epoch: 10 [36352/50048]	Loss: 1.4640
Training Epoch: 10 [36480/50048]	Loss: 1.4765
Training Epoch: 10 [36608/50048]	Loss: 1.3452
Training Epoch: 10 [36736/50048]	Loss: 1.5974
Training Epoch: 10 [36864/50048]	Loss: 1.6174
Training Epoch: 10 [36992/50048]	Loss: 1.4838
Training Epoch: 10 [37120/50048]	Loss: 1.2608
Training Epoch: 10 [37248/50048]	Loss: 1.3857
Training Epoch: 10 [37376/50048]	Loss: 1.5263
Training Epoch: 10 [37504/50048]	Loss: 1.7811
Training Epoch: 10 [37632/50048]	Loss: 1.3608
Training Epoch: 10 [37760/50048]	Loss: 1.6892
Training Epoch: 10 [37888/50048]	Loss: 1.7908
Training Epoch: 10 [38016/50048]	Loss: 1.6295
Training Epoch: 10 [38144/50048]	Loss: 1.6491
Training Epoch: 10 [38272/50048]	Loss: 1.6584
Training Epoch: 10 [38400/50048]	Loss: 1.6944
Training Epoch: 10 [38528/50048]	Loss: 1.3961
Training Epoch: 10 [38656/50048]	Loss: 1.3918
Training Epoch: 10 [38784/50048]	Loss: 1.7181
Training Epoch: 10 [38912/50048]	Loss: 1.3622
Training Epoch: 10 [39040/50048]	Loss: 1.7189
Training Epoch: 10 [39168/50048]	Loss: 1.9430
Training Epoch: 10 [39296/50048]	Loss: 1.5617
Training Epoch: 10 [39424/50048]	Loss: 1.6211
Training Epoch: 10 [39552/50048]	Loss: 1.4305
Training Epoch: 10 [39680/50048]	Loss: 1.6849
Training Epoch: 10 [39808/50048]	Loss: 1.7795
Training Epoch: 10 [39936/50048]	Loss: 1.7094
Training Epoch: 10 [40064/50048]	Loss: 1.6740
Training Epoch: 10 [40192/50048]	Loss: 1.3294
Training Epoch: 10 [40320/50048]	Loss: 1.4263
Training Epoch: 10 [40448/50048]	Loss: 1.2989
Training Epoch: 10 [40576/50048]	Loss: 1.6090
Training Epoch: 10 [40704/50048]	Loss: 1.4865
Training Epoch: 10 [40832/50048]	Loss: 1.4626
Training Epoch: 10 [40960/50048]	Loss: 1.5800
Training Epoch: 10 [41088/50048]	Loss: 1.2623
Training Epoch: 10 [41216/50048]	Loss: 1.6324
Training Epoch: 10 [41344/50048]	Loss: 1.4559
Training Epoch: 10 [41472/50048]	Loss: 1.5926
Training Epoch: 10 [41600/50048]	Loss: 1.4954
Training Epoch: 10 [41728/50048]	Loss: 1.7185
Training Epoch: 10 [41856/50048]	Loss: 1.4038
Training Epoch: 10 [41984/50048]	Loss: 1.4151
Training Epoch: 10 [42112/50048]	Loss: 1.5369
Training Epoch: 10 [42240/50048]	Loss: 1.5109
Training Epoch: 10 [42368/50048]	Loss: 1.6297
Training Epoch: 10 [42496/50048]	Loss: 1.3803
Training Epoch: 10 [42624/50048]	Loss: 1.6047
Training Epoch: 10 [42752/50048]	Loss: 1.5791
Training Epoch: 10 [42880/50048]	Loss: 1.6647
Training Epoch: 10 [43008/50048]	Loss: 1.7876
Training Epoch: 10 [43136/50048]	Loss: 1.3889
Training Epoch: 10 [43264/50048]	Loss: 1.5874
Training Epoch: 10 [43392/50048]	Loss: 1.5812
Training Epoch: 10 [43520/50048]	Loss: 1.7254
Training Epoch: 10 [43648/50048]	Loss: 1.4547
Training Epoch: 10 [43776/50048]	Loss: 1.7303
Training Epoch: 10 [43904/50048]	Loss: 1.4334
Training Epoch: 10 [44032/50048]	Loss: 1.6086
Training Epoch: 10 [44160/50048]	Loss: 1.5149
Training Epoch: 10 [44288/50048]	Loss: 1.7825
Training Epoch: 10 [44416/50048]	Loss: 1.6608
Training Epoch: 10 [44544/50048]	Loss: 1.5725
Training Epoch: 10 [44672/50048]	Loss: 1.3949
Training Epoch: 10 [44800/50048]	Loss: 1.5545
Training Epoch: 10 [44928/50048]	Loss: 1.4607
Training Epoch: 10 [45056/50048]	Loss: 1.6236
Training Epoch: 10 [45184/50048]	Loss: 1.1387
Training Epoch: 10 [45312/50048]	Loss: 1.5274
Training Epoch: 10 [45440/50048]	Loss: 1.6298
Training Epoch: 10 [45568/50048]	Loss: 1.4486
Training Epoch: 10 [45696/50048]	Loss: 1.6245
2022-12-06 03:53:53,729 [ZeusDataLoader(train)] train epoch 11 done: time=86.38 energy=10495.44
2022-12-06 03:53:53,731 [ZeusDataLoader(eval)] Epoch 11 begin.
Training Epoch: 10 [45824/50048]	Loss: 1.5224
Training Epoch: 10 [45952/50048]	Loss: 1.4222
Training Epoch: 10 [46080/50048]	Loss: 1.2508
Training Epoch: 10 [46208/50048]	Loss: 1.2394
Training Epoch: 10 [46336/50048]	Loss: 1.7605
Training Epoch: 10 [46464/50048]	Loss: 1.7168
Training Epoch: 10 [46592/50048]	Loss: 1.5993
Training Epoch: 10 [46720/50048]	Loss: 1.6182
Training Epoch: 10 [46848/50048]	Loss: 1.7013
Training Epoch: 10 [46976/50048]	Loss: 1.5223
Training Epoch: 10 [47104/50048]	Loss: 1.6685
Training Epoch: 10 [47232/50048]	Loss: 1.4202
Training Epoch: 10 [47360/50048]	Loss: 1.7711
Training Epoch: 10 [47488/50048]	Loss: 1.5772
Training Epoch: 10 [47616/50048]	Loss: 1.3399
Training Epoch: 10 [47744/50048]	Loss: 1.6098
Training Epoch: 10 [47872/50048]	Loss: 1.5104
Training Epoch: 10 [48000/50048]	Loss: 1.5233
Training Epoch: 10 [48128/50048]	Loss: 1.5997
Training Epoch: 10 [48256/50048]	Loss: 1.6828
Training Epoch: 10 [48384/50048]	Loss: 1.6945
Training Epoch: 10 [48512/50048]	Loss: 1.7536
Training Epoch: 10 [48640/50048]	Loss: 1.6159
Training Epoch: 10 [48768/50048]	Loss: 1.4277
Training Epoch: 10 [48896/50048]	Loss: 1.5142
Training Epoch: 10 [49024/50048]	Loss: 1.5275
Training Epoch: 10 [49152/50048]	Loss: 1.7402
Training Epoch: 10 [49280/50048]	Loss: 1.6853
Training Epoch: 10 [49408/50048]	Loss: 1.5163
Training Epoch: 10 [49536/50048]	Loss: 1.5981
Training Epoch: 10 [49664/50048]	Loss: 1.2262
Training Epoch: 10 [49792/50048]	Loss: 1.6731
Training Epoch: 10 [49920/50048]	Loss: 1.5885
Training Epoch: 10 [50048/50048]	Loss: 1.9291
2022-12-06 08:53:57.413 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 03:53:57,440 [ZeusDataLoader(eval)] eval epoch 11 done: time=3.70 energy=453.68
2022-12-06 03:53:57,441 [ZeusDataLoader(train)] Up to epoch 11: time=994.98, energy=120390.94, cost=147256.63
2022-12-06 03:53:57,441 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 03:53:57,441 [ZeusDataLoader(train)] Expected next epoch: time=1084.78, energy=131188.95, cost=160513.01
2022-12-06 03:53:57,442 [ZeusDataLoader(train)] Epoch 12 begin.
Validation Epoch: 10, Average loss: 0.0137, Accuracy: 0.5235
2022-12-06 03:53:57,622 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 03:53:57,623 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 08:53:57.624 [ZeusMonitor] Monitor started.
2022-12-06 08:53:57.624 [ZeusMonitor] Running indefinitely. 2022-12-06 08:53:57.624 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 08:53:57.624 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e12+gpu0.power.log
Training Epoch: 11 [128/50048]	Loss: 1.3096
Training Epoch: 11 [256/50048]	Loss: 1.3337
Training Epoch: 11 [384/50048]	Loss: 1.4075
Training Epoch: 11 [512/50048]	Loss: 1.4771
Training Epoch: 11 [640/50048]	Loss: 1.1665
Training Epoch: 11 [768/50048]	Loss: 1.0968
Training Epoch: 11 [896/50048]	Loss: 1.4068
Training Epoch: 11 [1024/50048]	Loss: 1.3886
Training Epoch: 11 [1152/50048]	Loss: 1.4992
Training Epoch: 11 [1280/50048]	Loss: 1.3864
Training Epoch: 11 [1408/50048]	Loss: 1.3555
Training Epoch: 11 [1536/50048]	Loss: 1.3289
Training Epoch: 11 [1664/50048]	Loss: 1.2951
Training Epoch: 11 [1792/50048]	Loss: 1.3052
Training Epoch: 11 [1920/50048]	Loss: 1.4494
Training Epoch: 11 [2048/50048]	Loss: 1.5357
Training Epoch: 11 [2176/50048]	Loss: 1.3530
Training Epoch: 11 [2304/50048]	Loss: 1.3404
Training Epoch: 11 [2432/50048]	Loss: 1.1931
Training Epoch: 11 [2560/50048]	Loss: 1.4443
Training Epoch: 11 [2688/50048]	Loss: 1.5239
Training Epoch: 11 [2816/50048]	Loss: 1.3398
Training Epoch: 11 [2944/50048]	Loss: 1.5176
Training Epoch: 11 [3072/50048]	Loss: 1.3178
Training Epoch: 11 [3200/50048]	Loss: 1.4507
Training Epoch: 11 [3328/50048]	Loss: 1.4590
Training Epoch: 11 [3456/50048]	Loss: 1.5682
Training Epoch: 11 [3584/50048]	Loss: 1.5075
Training Epoch: 11 [3712/50048]	Loss: 1.3529
Training Epoch: 11 [3840/50048]	Loss: 1.6859
Training Epoch: 11 [3968/50048]	Loss: 1.6749
Training Epoch: 11 [4096/50048]	Loss: 1.4594
Training Epoch: 11 [4224/50048]	Loss: 1.4532
Training Epoch: 11 [4352/50048]	Loss: 1.4575
Training Epoch: 11 [4480/50048]	Loss: 1.3762
Training Epoch: 11 [4608/50048]	Loss: 1.6398
Training Epoch: 11 [4736/50048]	Loss: 1.6933
Training Epoch: 11 [4864/50048]	Loss: 1.4293
Training Epoch: 11 [4992/50048]	Loss: 1.5067
Training Epoch: 11 [5120/50048]	Loss: 1.7516
Training Epoch: 11 [5248/50048]	Loss: 1.4062
Training Epoch: 11 [5376/50048]	Loss: 1.1707
Training Epoch: 11 [5504/50048]	Loss: 1.6611
Training Epoch: 11 [5632/50048]	Loss: 1.4817
Training Epoch: 11 [5760/50048]	Loss: 1.2744
Training Epoch: 11 [5888/50048]	Loss: 1.3799
Training Epoch: 11 [6016/50048]	Loss: 1.2939
Training Epoch: 11 [6144/50048]	Loss: 1.3456
Training Epoch: 11 [6272/50048]	Loss: 1.2585
Training Epoch: 11 [6400/50048]	Loss: 1.2877
Training Epoch: 11 [6528/50048]	Loss: 1.2994
Training Epoch: 11 [6656/50048]	Loss: 1.5861
Training Epoch: 11 [6784/50048]	Loss: 1.1509
Training Epoch: 11 [6912/50048]	Loss: 1.5435
Training Epoch: 11 [7040/50048]	Loss: 1.4640
Training Epoch: 11 [7168/50048]	Loss: 1.3033
Training Epoch: 11 [7296/50048]	Loss: 1.4168
Training Epoch: 11 [7424/50048]	Loss: 1.5522
Training Epoch: 11 [7552/50048]	Loss: 1.3256
Training Epoch: 11 [7680/50048]	Loss: 1.6851
Training Epoch: 11 [7808/50048]	Loss: 1.4329
Training Epoch: 11 [7936/50048]	Loss: 1.4950
Training Epoch: 11 [8064/50048]	Loss: 1.5223
Training Epoch: 11 [8192/50048]	Loss: 1.5055
Training Epoch: 11 [8320/50048]	Loss: 1.3922
Training Epoch: 11 [8448/50048]	Loss: 1.4517
Training Epoch: 11 [8576/50048]	Loss: 1.2513
Training Epoch: 11 [8704/50048]	Loss: 1.3653
Training Epoch: 11 [8832/50048]	Loss: 1.5078
Training Epoch: 11 [8960/50048]	Loss: 1.3492
Training Epoch: 11 [9088/50048]	Loss: 1.3735
Training Epoch: 11 [9216/50048]	Loss: 1.6406
Training Epoch: 11 [9344/50048]	Loss: 1.4875
Training Epoch: 11 [9472/50048]	Loss: 1.3359
Training Epoch: 11 [9600/50048]	Loss: 1.2138
Training Epoch: 11 [9728/50048]	Loss: 1.5761
Training Epoch: 11 [9856/50048]	Loss: 1.5650
Training Epoch: 11 [9984/50048]	Loss: 1.5484
Training Epoch: 11 [10112/50048]	Loss: 1.5839
Training Epoch: 11 [10240/50048]	Loss: 1.4380
Training Epoch: 11 [10368/50048]	Loss: 1.2808
Training Epoch: 11 [10496/50048]	Loss: 1.4912
Training Epoch: 11 [10624/50048]	Loss: 1.5407
Training Epoch: 11 [10752/50048]	Loss: 1.5892
Training Epoch: 11 [10880/50048]	Loss: 1.3682
Training Epoch: 11 [11008/50048]	Loss: 1.3457
Training Epoch: 11 [11136/50048]	Loss: 1.4803
Training Epoch: 11 [11264/50048]	Loss: 1.4060
Training Epoch: 11 [11392/50048]	Loss: 1.4658
Training Epoch: 11 [11520/50048]	Loss: 1.4117
Training Epoch: 11 [11648/50048]	Loss: 1.6826
Training Epoch: 11 [11776/50048]	Loss: 1.4525
Training Epoch: 11 [11904/50048]	Loss: 1.6704
Training Epoch: 11 [12032/50048]	Loss: 1.3147
Training Epoch: 11 [12160/50048]	Loss: 1.3234
Training Epoch: 11 [12288/50048]	Loss: 1.4415
Training Epoch: 11 [12416/50048]	Loss: 1.5778
Training Epoch: 11 [12544/50048]	Loss: 1.7854
Training Epoch: 11 [12672/50048]	Loss: 1.3005
Training Epoch: 11 [12800/50048]	Loss: 1.5408
Training Epoch: 11 [12928/50048]	Loss: 1.4837
Training Epoch: 11 [13056/50048]	Loss: 1.6150
Training Epoch: 11 [13184/50048]	Loss: 1.4690
Training Epoch: 11 [13312/50048]	Loss: 1.4312
Training Epoch: 11 [13440/50048]	Loss: 1.5051
Training Epoch: 11 [13568/50048]	Loss: 1.5149
Training Epoch: 11 [13696/50048]	Loss: 1.5324
Training Epoch: 11 [13824/50048]	Loss: 1.3742
Training Epoch: 11 [13952/50048]	Loss: 1.5202
Training Epoch: 11 [14080/50048]	Loss: 1.5274
Training Epoch: 11 [14208/50048]	Loss: 1.4838
Training Epoch: 11 [14336/50048]	Loss: 1.5352
Training Epoch: 11 [14464/50048]	Loss: 1.5237
Training Epoch: 11 [14592/50048]	Loss: 1.4553
Training Epoch: 11 [14720/50048]	Loss: 1.6021
Training Epoch: 11 [14848/50048]	Loss: 1.2273
Training Epoch: 11 [14976/50048]	Loss: 1.7160
Training Epoch: 11 [15104/50048]	Loss: 1.4451
Training Epoch: 11 [15232/50048]	Loss: 1.2824
Training Epoch: 11 [15360/50048]	Loss: 1.6256
Training Epoch: 11 [15488/50048]	Loss: 1.2359
Training Epoch: 11 [15616/50048]	Loss: 1.3269
Training Epoch: 11 [15744/50048]	Loss: 1.4907
Training Epoch: 11 [15872/50048]	Loss: 1.4696
Training Epoch: 11 [16000/50048]	Loss: 1.5773
Training Epoch: 11 [16128/50048]	Loss: 1.4995
Training Epoch: 11 [16256/50048]	Loss: 1.1575
Training Epoch: 11 [16384/50048]	Loss: 1.3591
Training Epoch: 11 [16512/50048]	Loss: 1.3719
Training Epoch: 11 [16640/50048]	Loss: 1.3303
Training Epoch: 11 [16768/50048]	Loss: 1.4224
Training Epoch: 11 [16896/50048]	Loss: 1.4645
Training Epoch: 11 [17024/50048]	Loss: 1.5318
Training Epoch: 11 [17152/50048]	Loss: 1.4442
Training Epoch: 11 [17280/50048]	Loss: 1.3702
Training Epoch: 11 [17408/50048]	Loss: 1.4343
Training Epoch: 11 [17536/50048]	Loss: 1.2655
Training Epoch: 11 [17664/50048]	Loss: 1.4752
Training Epoch: 11 [17792/50048]	Loss: 1.6983
Training Epoch: 11 [17920/50048]	Loss: 1.4280
Training Epoch: 11 [18048/50048]	Loss: 1.5162
Training Epoch: 11 [18176/50048]	Loss: 1.2222
Training Epoch: 11 [18304/50048]	Loss: 1.4597
Training Epoch: 11 [18432/50048]	Loss: 1.6980
Training Epoch: 11 [18560/50048]	Loss: 1.5793
Training Epoch: 11 [18688/50048]	Loss: 1.7966
Training Epoch: 11 [18816/50048]	Loss: 1.5283
Training Epoch: 11 [18944/50048]	Loss: 1.2781
Training Epoch: 11 [19072/50048]	Loss: 1.2751
Training Epoch: 11 [19200/50048]	Loss: 1.4686
Training Epoch: 11 [19328/50048]	Loss: 1.3623
Training Epoch: 11 [19456/50048]	Loss: 1.3462
Training Epoch: 11 [19584/50048]	Loss: 1.8542
Training Epoch: 11 [19712/50048]	Loss: 1.7098
Training Epoch: 11 [19840/50048]	Loss: 1.6903
Training Epoch: 11 [19968/50048]	Loss: 1.5923
Training Epoch: 11 [20096/50048]	Loss: 1.3331
Training Epoch: 11 [20224/50048]	Loss: 1.6875
Training Epoch: 11 [20352/50048]	Loss: 1.5207
Training Epoch: 11 [20480/50048]	Loss: 1.5301
Training Epoch: 11 [20608/50048]	Loss: 1.5806
Training Epoch: 11 [20736/50048]	Loss: 1.7289
Training Epoch: 11 [20864/50048]	Loss: 1.6836
Training Epoch: 11 [20992/50048]	Loss: 1.5400
Training Epoch: 11 [21120/50048]	Loss: 1.2800
Training Epoch: 11 [21248/50048]	Loss: 1.4642
Training Epoch: 11 [21376/50048]	Loss: 1.6419
Training Epoch: 11 [21504/50048]	Loss: 1.5967
Training Epoch: 11 [21632/50048]	Loss: 1.6436
Training Epoch: 11 [21760/50048]	Loss: 1.5490
Training Epoch: 11 [21888/50048]	Loss: 1.5225
Training Epoch: 11 [22016/50048]	Loss: 1.4409
Training Epoch: 11 [22144/50048]	Loss: 1.4920
Training Epoch: 11 [22272/50048]	Loss: 1.4410
Training Epoch: 11 [22400/50048]	Loss: 1.7483
Training Epoch: 11 [22528/50048]	Loss: 1.6434
Training Epoch: 11 [22656/50048]	Loss: 1.5529
Training Epoch: 11 [22784/50048]	Loss: 1.4245
Training Epoch: 11 [22912/50048]	Loss: 1.5227
Training Epoch: 11 [23040/50048]	Loss: 1.3555
Training Epoch: 11 [23168/50048]	Loss: 1.4861
Training Epoch: 11 [23296/50048]	Loss: 1.5662
Training Epoch: 11 [23424/50048]	Loss: 1.4515
Training Epoch: 11 [23552/50048]	Loss: 1.6463
Training Epoch: 11 [23680/50048]	Loss: 1.3879
Training Epoch: 11 [23808/50048]	Loss: 1.4862
Training Epoch: 11 [23936/50048]	Loss: 1.4606
Training Epoch: 11 [24064/50048]	Loss: 1.5232
Training Epoch: 11 [24192/50048]	Loss: 1.4079
Training Epoch: 11 [24320/50048]	Loss: 1.3671
Training Epoch: 11 [24448/50048]	Loss: 1.4223
Training Epoch: 11 [24576/50048]	Loss: 1.7098
Training Epoch: 11 [24704/50048]	Loss: 1.5176
Training Epoch: 11 [24832/50048]	Loss: 1.6261
Training Epoch: 11 [24960/50048]	Loss: 1.4882
Training Epoch: 11 [25088/50048]	Loss: 1.5005
Training Epoch: 11 [25216/50048]	Loss: 1.5669
Training Epoch: 11 [25344/50048]	Loss: 1.5298
Training Epoch: 11 [25472/50048]	Loss: 1.2169
Training Epoch: 11 [25600/50048]	Loss: 1.5450
Training Epoch: 11 [25728/50048]	Loss: 1.7864
Training Epoch: 11 [25856/50048]	Loss: 1.6544
Training Epoch: 11 [25984/50048]	Loss: 1.2877
Training Epoch: 11 [26112/50048]	Loss: 1.5623
Training Epoch: 11 [26240/50048]	Loss: 1.5698
Training Epoch: 11 [26368/50048]	Loss: 1.3530
Training Epoch: 11 [26496/50048]	Loss: 1.3323
Training Epoch: 11 [26624/50048]	Loss: 1.2699
Training Epoch: 11 [26752/50048]	Loss: 1.3380
Training Epoch: 11 [26880/50048]	Loss: 1.4804
Training Epoch: 11 [27008/50048]	Loss: 1.4368
Training Epoch: 11 [27136/50048]	Loss: 1.5004
Training Epoch: 11 [27264/50048]	Loss: 1.4372
Training Epoch: 11 [27392/50048]	Loss: 1.5427
Training Epoch: 11 [27520/50048]	Loss: 1.6941
Training Epoch: 11 [27648/50048]	Loss: 1.3466
Training Epoch: 11 [27776/50048]	Loss: 1.3925
Training Epoch: 11 [27904/50048]	Loss: 1.5149
Training Epoch: 11 [28032/50048]	Loss: 1.6253
Training Epoch: 11 [28160/50048]	Loss: 1.6845
Training Epoch: 11 [28288/50048]	Loss: 1.4558
Training Epoch: 11 [28416/50048]	Loss: 1.4928
Training Epoch: 11 [28544/50048]	Loss: 1.3960
Training Epoch: 11 [28672/50048]	Loss: 1.3672
Training Epoch: 11 [28800/50048]	Loss: 1.4166
Training Epoch: 11 [28928/50048]	Loss: 1.4028
Training Epoch: 11 [29056/50048]	Loss: 1.5848
Training Epoch: 11 [29184/50048]	Loss: 1.5092
Training Epoch: 11 [29312/50048]	Loss: 1.6741
Training Epoch: 11 [29440/50048]	Loss: 1.6270
Training Epoch: 11 [29568/50048]	Loss: 1.7604
Training Epoch: 11 [29696/50048]	Loss: 1.5393
Training Epoch: 11 [29824/50048]	Loss: 1.4845
Training Epoch: 11 [29952/50048]	Loss: 1.3503
Training Epoch: 11 [30080/50048]	Loss: 1.2883
Training Epoch: 11 [30208/50048]	Loss: 1.5856
Training Epoch: 11 [30336/50048]	Loss: 1.3996
Training Epoch: 11 [30464/50048]	Loss: 1.5661
Training Epoch: 11 [30592/50048]	Loss: 1.6195
Training Epoch: 11 [30720/50048]	Loss: 1.2149
Training Epoch: 11 [30848/50048]	Loss: 1.4765
Training Epoch: 11 [30976/50048]	Loss: 1.3761
Training Epoch: 11 [31104/50048]	Loss: 1.4472
Training Epoch: 11 [31232/50048]	Loss: 1.5676
Training Epoch: 11 [31360/50048]	Loss: 1.3582
Training Epoch: 11 [31488/50048]	Loss: 1.4554
Training Epoch: 11 [31616/50048]	Loss: 1.6589
Training Epoch: 11 [31744/50048]	Loss: 1.5167
Training Epoch: 11 [31872/50048]	Loss: 1.5866
Training Epoch: 11 [32000/50048]	Loss: 1.5642
Training Epoch: 11 [32128/50048]	Loss: 1.4777
Training Epoch: 11 [32256/50048]	Loss: 1.5286
Training Epoch: 11 [32384/50048]	Loss: 1.6309
Training Epoch: 11 [32512/50048]	Loss: 1.4072
Training Epoch: 11 [32640/50048]	Loss: 1.5022
Training Epoch: 11 [32768/50048]	Loss: 1.4767
Training Epoch: 11 [32896/50048]	Loss: 1.1280
Training Epoch: 11 [33024/50048]	Loss: 1.5815
Training Epoch: 11 [33152/50048]	Loss: 1.7351
Training Epoch: 11 [33280/50048]	Loss: 1.6900
Training Epoch: 11 [33408/50048]	Loss: 1.5684
Training Epoch: 11 [33536/50048]	Loss: 1.2949
Training Epoch: 11 [33664/50048]	Loss: 1.5098
Training Epoch: 11 [33792/50048]	Loss: 1.3094
Training Epoch: 11 [33920/50048]	Loss: 1.7847
Training Epoch: 11 [34048/50048]	Loss: 1.5538
Training Epoch: 11 [34176/50048]	Loss: 1.5241
Training Epoch: 11 [34304/50048]	Loss: 1.4455
Training Epoch: 11 [34432/50048]	Loss: 1.5708
Training Epoch: 11 [34560/50048]	Loss: 1.3641
Training Epoch: 11 [34688/50048]	Loss: 1.5101
Training Epoch: 11 [34816/50048]	Loss: 1.3612
Training Epoch: 11 [34944/50048]	Loss: 1.6892
Training Epoch: 11 [35072/50048]	Loss: 1.4382
Training Epoch: 11 [35200/50048]	Loss: 1.3366
Training Epoch: 11 [35328/50048]	Loss: 1.5078
Training Epoch: 11 [35456/50048]	Loss: 1.6144
Training Epoch: 11 [35584/50048]	Loss: 1.5520
Training Epoch: 11 [35712/50048]	Loss: 1.3962
Training Epoch: 11 [35840/50048]	Loss: 1.5889
Training Epoch: 11 [35968/50048]	Loss: 1.4407
Training Epoch: 11 [36096/50048]	Loss: 1.3018
Training Epoch: 11 [36224/50048]	Loss: 1.5184
Training Epoch: 11 [36352/50048]	Loss: 1.5380
Training Epoch: 11 [36480/50048]	Loss: 1.5405
Training Epoch: 11 [36608/50048]	Loss: 1.3315
Training Epoch: 11 [36736/50048]	Loss: 1.6009
Training Epoch: 11 [36864/50048]	Loss: 1.4243
Training Epoch: 11 [36992/50048]	Loss: 1.6455
Training Epoch: 11 [37120/50048]	Loss: 1.5585
Training Epoch: 11 [37248/50048]	Loss: 1.2692
Training Epoch: 11 [37376/50048]	Loss: 1.5672
Training Epoch: 11 [37504/50048]	Loss: 1.6316
Training Epoch: 11 [37632/50048]	Loss: 1.5292
Training Epoch: 11 [37760/50048]	Loss: 1.6470
Training Epoch: 11 [37888/50048]	Loss: 1.2775
Training Epoch: 11 [38016/50048]	Loss: 1.6420
Training Epoch: 11 [38144/50048]	Loss: 1.2004
Training Epoch: 11 [38272/50048]	Loss: 1.5832
Training Epoch: 11 [38400/50048]	Loss: 1.3110
Training Epoch: 11 [38528/50048]	Loss: 1.1514
Training Epoch: 11 [38656/50048]	Loss: 1.4320
Training Epoch: 11 [38784/50048]	Loss: 1.6679
Training Epoch: 11 [38912/50048]	Loss: 1.6071
Training Epoch: 11 [39040/50048]	Loss: 1.5206
Training Epoch: 11 [39168/50048]	Loss: 1.4267
Training Epoch: 11 [39296/50048]	Loss: 1.3886
Training Epoch: 11 [39424/50048]	Loss: 1.5510
Training Epoch: 11 [39552/50048]	Loss: 1.3640
Training Epoch: 11 [39680/50048]	Loss: 1.3908
Training Epoch: 11 [39808/50048]	Loss: 1.6411
Training Epoch: 11 [39936/50048]	Loss: 1.4567
Training Epoch: 11 [40064/50048]	Loss: 1.3454
Training Epoch: 11 [40192/50048]	Loss: 1.4934
Training Epoch: 11 [40320/50048]	Loss: 1.5814
Training Epoch: 11 [40448/50048]	Loss: 1.3380
Training Epoch: 11 [40576/50048]	Loss: 1.4219
Training Epoch: 11 [40704/50048]	Loss: 1.6347
Training Epoch: 11 [40832/50048]	Loss: 1.6795
Training Epoch: 11 [40960/50048]	Loss: 1.6437
Training Epoch: 11 [41088/50048]	Loss: 1.4402
Training Epoch: 11 [41216/50048]	Loss: 1.6527
Training Epoch: 11 [41344/50048]	Loss: 1.4178
Training Epoch: 11 [41472/50048]	Loss: 1.8319
Training Epoch: 11 [41600/50048]	Loss: 1.6345
Training Epoch: 11 [41728/50048]	Loss: 1.4189
Training Epoch: 11 [41856/50048]	Loss: 1.4161
Training Epoch: 11 [41984/50048]	Loss: 1.4427
Training Epoch: 11 [42112/50048]	Loss: 1.7059
Training Epoch: 11 [42240/50048]	Loss: 1.2671
Training Epoch: 11 [42368/50048]	Loss: 1.5873
Training Epoch: 11 [42496/50048]	Loss: 1.6483
Training Epoch: 11 [42624/50048]	Loss: 1.8041
Training Epoch: 11 [42752/50048]	Loss: 1.4707
Training Epoch: 11 [42880/50048]	Loss: 1.5240
Training Epoch: 11 [43008/50048]	Loss: 1.5857
Training Epoch: 11 [43136/50048]	Loss: 1.4907
Training Epoch: 11 [43264/50048]	Loss: 1.3295
Training Epoch: 11 [43392/50048]	Loss: 1.5140
Training Epoch: 11 [43520/50048]	Loss: 1.4599
Training Epoch: 11 [43648/50048]	Loss: 1.4296
Training Epoch: 11 [43776/50048]	Loss: 1.4932
Training Epoch: 11 [43904/50048]	Loss: 1.5631
Training Epoch: 11 [44032/50048]	Loss: 1.1751
Training Epoch: 11 [44160/50048]	Loss: 1.4596
Training Epoch: 11 [44288/50048]	Loss: 1.5443
Training Epoch: 11 [44416/50048]	Loss: 1.7198
Training Epoch: 11 [44544/50048]	Loss: 1.3965
Training Epoch: 11 [44672/50048]	Loss: 1.6865
Training Epoch: 11 [44800/50048]	Loss: 1.4243
Training Epoch: 11 [44928/50048]	Loss: 1.4799
Training Epoch: 11 [45056/50048]	Loss: 1.3332
Training Epoch: 11 [45184/50048]	Loss: 1.4469
Training Epoch: 11 [45312/50048]	Loss: 1.4056
Training Epoch: 11 [45440/50048]	Loss: 1.4766
Training Epoch: 11 [45568/50048]	Loss: 1.3016
Training Epoch: 11 [45696/50048]	Loss: 1.2689
2022-12-06 03:55:23,921 [ZeusDataLoader(train)] train epoch 12 done: time=86.47 energy=10512.10
2022-12-06 03:55:23,922 [ZeusDataLoader(eval)] Epoch 12 begin.
Training Epoch: 11 [45824/50048]	Loss: 1.6406
Training Epoch: 11 [45952/50048]	Loss: 1.4765
Training Epoch: 11 [46080/50048]	Loss: 1.2375
Training Epoch: 11 [46208/50048]	Loss: 1.5715
Training Epoch: 11 [46336/50048]	Loss: 1.3317
Training Epoch: 11 [46464/50048]	Loss: 1.5628
Training Epoch: 11 [46592/50048]	Loss: 1.4009
Training Epoch: 11 [46720/50048]	Loss: 1.6337
Training Epoch: 11 [46848/50048]	Loss: 1.1367
Training Epoch: 11 [46976/50048]	Loss: 1.5426
Training Epoch: 11 [47104/50048]	Loss: 1.6195
Training Epoch: 11 [47232/50048]	Loss: 1.5197
Training Epoch: 11 [47360/50048]	Loss: 1.5741
Training Epoch: 11 [47488/50048]	Loss: 1.5547
Training Epoch: 11 [47616/50048]	Loss: 1.3270
Training Epoch: 11 [47744/50048]	Loss: 1.3595
Training Epoch: 11 [47872/50048]	Loss: 1.3074
Training Epoch: 11 [48000/50048]	Loss: 1.5837
Training Epoch: 11 [48128/50048]	Loss: 1.5205
Training Epoch: 11 [48256/50048]	Loss: 1.4675
Training Epoch: 11 [48384/50048]	Loss: 1.6431
Training Epoch: 11 [48512/50048]	Loss: 1.5461
Training Epoch: 11 [48640/50048]	Loss: 1.3237
Training Epoch: 11 [48768/50048]	Loss: 1.4390
Training Epoch: 11 [48896/50048]	Loss: 1.6111
Training Epoch: 11 [49024/50048]	Loss: 1.5134
Training Epoch: 11 [49152/50048]	Loss: 1.2651
Training Epoch: 11 [49280/50048]	Loss: 1.6171
Training Epoch: 11 [49408/50048]	Loss: 1.2884
Training Epoch: 11 [49536/50048]	Loss: 1.6033
Training Epoch: 11 [49664/50048]	Loss: 1.3813
Training Epoch: 11 [49792/50048]	Loss: 1.8112
Training Epoch: 11 [49920/50048]	Loss: 1.6434
Training Epoch: 11 [50048/50048]	Loss: 1.6097
2022-12-06 08:55:27.615 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 03:55:27,637 [ZeusDataLoader(eval)] eval epoch 12 done: time=3.71 energy=454.51
2022-12-06 03:55:27,638 [ZeusDataLoader(train)] Up to epoch 12: time=1085.16, energy=131357.55, cost=160630.23
2022-12-06 03:55:27,638 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 03:55:27,638 [ZeusDataLoader(train)] Expected next epoch: time=1174.96, energy=142155.57, cost=173886.61
2022-12-06 03:55:27,639 [ZeusDataLoader(train)] Epoch 13 begin.
Validation Epoch: 11, Average loss: 0.0136, Accuracy: 0.5270
2022-12-06 03:55:27,815 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 03:55:27,816 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 08:55:27.826 [ZeusMonitor] Monitor started.
2022-12-06 08:55:27.826 [ZeusMonitor] Running indefinitely. 2022-12-06 08:55:27.826 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 08:55:27.826 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e13+gpu0.power.log
Training Epoch: 12 [128/50048]	Loss: 1.6233
Training Epoch: 12 [256/50048]	Loss: 1.3449
Training Epoch: 12 [384/50048]	Loss: 1.4549
Training Epoch: 12 [512/50048]	Loss: 1.2807
Training Epoch: 12 [640/50048]	Loss: 1.4385
Training Epoch: 12 [768/50048]	Loss: 1.5474
Training Epoch: 12 [896/50048]	Loss: 1.2425
Training Epoch: 12 [1024/50048]	Loss: 1.2620
Training Epoch: 12 [1152/50048]	Loss: 1.5234
Training Epoch: 12 [1280/50048]	Loss: 1.2960
Training Epoch: 12 [1408/50048]	Loss: 1.4000
Training Epoch: 12 [1536/50048]	Loss: 1.3490
Training Epoch: 12 [1664/50048]	Loss: 1.3453
Training Epoch: 12 [1792/50048]	Loss: 1.3457
Training Epoch: 12 [1920/50048]	Loss: 1.2890
Training Epoch: 12 [2048/50048]	Loss: 1.3118
Training Epoch: 12 [2176/50048]	Loss: 1.4616
Training Epoch: 12 [2304/50048]	Loss: 1.3806
Training Epoch: 12 [2432/50048]	Loss: 1.2946
Training Epoch: 12 [2560/50048]	Loss: 1.3491
Training Epoch: 12 [2688/50048]	Loss: 1.3088
Training Epoch: 12 [2816/50048]	Loss: 1.2937
Training Epoch: 12 [2944/50048]	Loss: 1.4407
Training Epoch: 12 [3072/50048]	Loss: 1.4805
Training Epoch: 12 [3200/50048]	Loss: 1.3927
Training Epoch: 12 [3328/50048]	Loss: 1.3839
Training Epoch: 12 [3456/50048]	Loss: 1.3268
Training Epoch: 12 [3584/50048]	Loss: 1.3500
Training Epoch: 12 [3712/50048]	Loss: 1.1497
Training Epoch: 12 [3840/50048]	Loss: 1.1970
Training Epoch: 12 [3968/50048]	Loss: 1.5085
Training Epoch: 12 [4096/50048]	Loss: 1.5331
Training Epoch: 12 [4224/50048]	Loss: 1.3901
Training Epoch: 12 [4352/50048]	Loss: 1.3273
Training Epoch: 12 [4480/50048]	Loss: 1.2367
Training Epoch: 12 [4608/50048]	Loss: 1.4001
Training Epoch: 12 [4736/50048]	Loss: 1.2072
Training Epoch: 12 [4864/50048]	Loss: 1.5494
Training Epoch: 12 [4992/50048]	Loss: 1.3165
Training Epoch: 12 [5120/50048]	Loss: 1.3218
Training Epoch: 12 [5248/50048]	Loss: 1.4604
Training Epoch: 12 [5376/50048]	Loss: 1.3992
Training Epoch: 12 [5504/50048]	Loss: 1.6703
Training Epoch: 12 [5632/50048]	Loss: 1.4779
Training Epoch: 12 [5760/50048]	Loss: 1.1457
Training Epoch: 12 [5888/50048]	Loss: 1.3029
Training Epoch: 12 [6016/50048]	Loss: 1.3939
Training Epoch: 12 [6144/50048]	Loss: 1.5682
Training Epoch: 12 [6272/50048]	Loss: 1.6668
Training Epoch: 12 [6400/50048]	Loss: 1.4293
Training Epoch: 12 [6528/50048]	Loss: 1.6751
Training Epoch: 12 [6656/50048]	Loss: 1.4112
Training Epoch: 12 [6784/50048]	Loss: 1.6385
Training Epoch: 12 [6912/50048]	Loss: 1.4398
Training Epoch: 12 [7040/50048]	Loss: 1.3117
Training Epoch: 12 [7168/50048]	Loss: 1.4163
Training Epoch: 12 [7296/50048]	Loss: 1.3943
Training Epoch: 12 [7424/50048]	Loss: 1.2665
Training Epoch: 12 [7552/50048]	Loss: 1.5157
Training Epoch: 12 [7680/50048]	Loss: 1.1847
Training Epoch: 12 [7808/50048]	Loss: 1.5804
Training Epoch: 12 [7936/50048]	Loss: 1.4725
Training Epoch: 12 [8064/50048]	Loss: 1.1388
Training Epoch: 12 [8192/50048]	Loss: 1.3850
Training Epoch: 12 [8320/50048]	Loss: 1.6495
Training Epoch: 12 [8448/50048]	Loss: 1.7928
Training Epoch: 12 [8576/50048]	Loss: 1.4789
Training Epoch: 12 [8704/50048]	Loss: 1.1779
Training Epoch: 12 [8832/50048]	Loss: 1.4248
Training Epoch: 12 [8960/50048]	Loss: 1.4475
Training Epoch: 12 [9088/50048]	Loss: 1.4263
Training Epoch: 12 [9216/50048]	Loss: 1.3535
Training Epoch: 12 [9344/50048]	Loss: 1.2774
Training Epoch: 12 [9472/50048]	Loss: 1.3544
Training Epoch: 12 [9600/50048]	Loss: 1.4075
Training Epoch: 12 [9728/50048]	Loss: 1.3673
Training Epoch: 12 [9856/50048]	Loss: 1.1675
Training Epoch: 12 [9984/50048]	Loss: 1.3125
Training Epoch: 12 [10112/50048]	Loss: 1.3163
Training Epoch: 12 [10240/50048]	Loss: 1.2898
Training Epoch: 12 [10368/50048]	Loss: 1.8672
Training Epoch: 12 [10496/50048]	Loss: 1.5374
Training Epoch: 12 [10624/50048]	Loss: 1.6234
Training Epoch: 12 [10752/50048]	Loss: 1.4844
Training Epoch: 12 [10880/50048]	Loss: 1.4834
Training Epoch: 12 [11008/50048]	Loss: 1.5308
Training Epoch: 12 [11136/50048]	Loss: 1.3568
Training Epoch: 12 [11264/50048]	Loss: 1.3952
Training Epoch: 12 [11392/50048]	Loss: 1.5199
Training Epoch: 12 [11520/50048]	Loss: 1.5426
Training Epoch: 12 [11648/50048]	Loss: 1.3074
Training Epoch: 12 [11776/50048]	Loss: 1.5021
Training Epoch: 12 [11904/50048]	Loss: 1.3721
Training Epoch: 12 [12032/50048]	Loss: 1.3588
Training Epoch: 12 [12160/50048]	Loss: 1.4343
Training Epoch: 12 [12288/50048]	Loss: 1.4072
Training Epoch: 12 [12416/50048]	Loss: 1.3097
Training Epoch: 12 [12544/50048]	Loss: 1.5776
Training Epoch: 12 [12672/50048]	Loss: 1.5304
Training Epoch: 12 [12800/50048]	Loss: 1.4169
Training Epoch: 12 [12928/50048]	Loss: 1.2334
Training Epoch: 12 [13056/50048]	Loss: 1.5135
Training Epoch: 12 [13184/50048]	Loss: 1.2007
Training Epoch: 12 [13312/50048]	Loss: 1.3430
Training Epoch: 12 [13440/50048]	Loss: 1.4402
Training Epoch: 12 [13568/50048]	Loss: 1.4524
Training Epoch: 12 [13696/50048]	Loss: 1.5934
Training Epoch: 12 [13824/50048]	Loss: 1.6745
Training Epoch: 12 [13952/50048]	Loss: 1.3852
Training Epoch: 12 [14080/50048]	Loss: 1.4457
Training Epoch: 12 [14208/50048]	Loss: 1.2612
Training Epoch: 12 [14336/50048]	Loss: 1.4201
Training Epoch: 12 [14464/50048]	Loss: 1.4622
Training Epoch: 12 [14592/50048]	Loss: 1.2445
Training Epoch: 12 [14720/50048]	Loss: 1.4058
Training Epoch: 12 [14848/50048]	Loss: 1.3039
Training Epoch: 12 [14976/50048]	Loss: 1.1807
Training Epoch: 12 [15104/50048]	Loss: 1.2549
Training Epoch: 12 [15232/50048]	Loss: 1.2666
Training Epoch: 12 [15360/50048]	Loss: 1.1790
Training Epoch: 12 [15488/50048]	Loss: 1.4244
Training Epoch: 12 [15616/50048]	Loss: 1.3496
Training Epoch: 12 [15744/50048]	Loss: 1.4940
Training Epoch: 12 [15872/50048]	Loss: 1.2450
Training Epoch: 12 [16000/50048]	Loss: 1.2950
Training Epoch: 12 [16128/50048]	Loss: 1.5604
Training Epoch: 12 [16256/50048]	Loss: 1.4617
Training Epoch: 12 [16384/50048]	Loss: 1.5474
Training Epoch: 12 [16512/50048]	Loss: 1.4337
Training Epoch: 12 [16640/50048]	Loss: 1.4529
Training Epoch: 12 [16768/50048]	Loss: 1.3021
Training Epoch: 12 [16896/50048]	Loss: 1.5681
Training Epoch: 12 [17024/50048]	Loss: 1.4717
Training Epoch: 12 [17152/50048]	Loss: 1.2589
Training Epoch: 12 [17280/50048]	Loss: 1.3822
Training Epoch: 12 [17408/50048]	Loss: 1.4882
Training Epoch: 12 [17536/50048]	Loss: 1.3428
Training Epoch: 12 [17664/50048]	Loss: 1.2991
Training Epoch: 12 [17792/50048]	Loss: 1.5551
Training Epoch: 12 [17920/50048]	Loss: 1.3435
Training Epoch: 12 [18048/50048]	Loss: 1.4992
Training Epoch: 12 [18176/50048]	Loss: 1.3769
Training Epoch: 12 [18304/50048]	Loss: 1.2764
Training Epoch: 12 [18432/50048]	Loss: 1.2808
Training Epoch: 12 [18560/50048]	Loss: 1.5784
Training Epoch: 12 [18688/50048]	Loss: 1.3462
Training Epoch: 12 [18816/50048]	Loss: 1.3538
Training Epoch: 12 [18944/50048]	Loss: 1.3991
Training Epoch: 12 [19072/50048]	Loss: 1.2250
Training Epoch: 12 [19200/50048]	Loss: 1.3920
Training Epoch: 12 [19328/50048]	Loss: 1.4999
Training Epoch: 12 [19456/50048]	Loss: 1.3927
Training Epoch: 12 [19584/50048]	Loss: 1.3489
Training Epoch: 12 [19712/50048]	Loss: 1.3004
Training Epoch: 12 [19840/50048]	Loss: 1.4986
Training Epoch: 12 [19968/50048]	Loss: 1.3168
Training Epoch: 12 [20096/50048]	Loss: 1.1764
Training Epoch: 12 [20224/50048]	Loss: 1.5596
Training Epoch: 12 [20352/50048]	Loss: 1.6065
Training Epoch: 12 [20480/50048]	Loss: 1.3864
Training Epoch: 12 [20608/50048]	Loss: 1.6747
Training Epoch: 12 [20736/50048]	Loss: 1.7100
Training Epoch: 12 [20864/50048]	Loss: 1.4625
Training Epoch: 12 [20992/50048]	Loss: 1.6161
Training Epoch: 12 [21120/50048]	Loss: 1.4537
Training Epoch: 12 [21248/50048]	Loss: 1.2875
Training Epoch: 12 [21376/50048]	Loss: 1.3269
Training Epoch: 12 [21504/50048]	Loss: 1.3767
Training Epoch: 12 [21632/50048]	Loss: 1.5498
Training Epoch: 12 [21760/50048]	Loss: 1.4868
Training Epoch: 12 [21888/50048]	Loss: 1.3278
Training Epoch: 12 [22016/50048]	Loss: 1.4784
Training Epoch: 12 [22144/50048]	Loss: 1.3002
Training Epoch: 12 [22272/50048]	Loss: 1.1952
Training Epoch: 12 [22400/50048]	Loss: 1.5716
Training Epoch: 12 [22528/50048]	Loss: 1.3660
Training Epoch: 12 [22656/50048]	Loss: 1.5146
Training Epoch: 12 [22784/50048]	Loss: 1.4444
Training Epoch: 12 [22912/50048]	Loss: 1.5795
Training Epoch: 12 [23040/50048]	Loss: 1.2480
Training Epoch: 12 [23168/50048]	Loss: 1.4879
Training Epoch: 12 [23296/50048]	Loss: 1.3054
Training Epoch: 12 [23424/50048]	Loss: 1.4217
Training Epoch: 12 [23552/50048]	Loss: 1.4936
Training Epoch: 12 [23680/50048]	Loss: 1.4547
Training Epoch: 12 [23808/50048]	Loss: 1.5303
Training Epoch: 12 [23936/50048]	Loss: 1.3531
Training Epoch: 12 [24064/50048]	Loss: 1.3338
Training Epoch: 12 [24192/50048]	Loss: 1.3009
Training Epoch: 12 [24320/50048]	Loss: 1.2283
Training Epoch: 12 [24448/50048]	Loss: 1.4074
Training Epoch: 12 [24576/50048]	Loss: 1.4932
Training Epoch: 12 [24704/50048]	Loss: 1.3331
Training Epoch: 12 [24832/50048]	Loss: 1.7988
Training Epoch: 12 [24960/50048]	Loss: 1.5370
Training Epoch: 12 [25088/50048]	Loss: 1.5772
Training Epoch: 12 [25216/50048]	Loss: 1.4359
Training Epoch: 12 [25344/50048]	Loss: 1.4643
Training Epoch: 12 [25472/50048]	Loss: 1.6287
Training Epoch: 12 [25600/50048]	Loss: 1.2269
Training Epoch: 12 [25728/50048]	Loss: 1.5015
Training Epoch: 12 [25856/50048]	Loss: 1.6464
Training Epoch: 12 [25984/50048]	Loss: 1.5090
Training Epoch: 12 [26112/50048]	Loss: 1.6237
Training Epoch: 12 [26240/50048]	Loss: 1.6016
Training Epoch: 12 [26368/50048]	Loss: 1.5597
Training Epoch: 12 [26496/50048]	Loss: 1.6414
Training Epoch: 12 [26624/50048]	Loss: 1.2638
Training Epoch: 12 [26752/50048]	Loss: 1.3205
Training Epoch: 12 [26880/50048]	Loss: 1.3995
Training Epoch: 12 [27008/50048]	Loss: 1.3536
Training Epoch: 12 [27136/50048]	Loss: 1.4037
Training Epoch: 12 [27264/50048]	Loss: 1.2383
Training Epoch: 12 [27392/50048]	Loss: 1.2786
Training Epoch: 12 [27520/50048]	Loss: 1.5203
Training Epoch: 12 [27648/50048]	Loss: 1.6957
Training Epoch: 12 [27776/50048]	Loss: 1.4370
Training Epoch: 12 [27904/50048]	Loss: 1.5323
Training Epoch: 12 [28032/50048]	Loss: 1.3741
Training Epoch: 12 [28160/50048]	Loss: 1.3977
Training Epoch: 12 [28288/50048]	Loss: 1.0668
Training Epoch: 12 [28416/50048]	Loss: 1.2914
Training Epoch: 12 [28544/50048]	Loss: 1.6667
Training Epoch: 12 [28672/50048]	Loss: 1.3837
Training Epoch: 12 [28800/50048]	Loss: 1.1633
Training Epoch: 12 [28928/50048]	Loss: 1.5340
Training Epoch: 12 [29056/50048]	Loss: 1.4170
Training Epoch: 12 [29184/50048]	Loss: 1.2616
Training Epoch: 12 [29312/50048]	Loss: 1.3700
Training Epoch: 12 [29440/50048]	Loss: 1.4683
Training Epoch: 12 [29568/50048]	Loss: 1.3868
Training Epoch: 12 [29696/50048]	Loss: 1.7032
Training Epoch: 12 [29824/50048]	Loss: 1.4479
Training Epoch: 12 [29952/50048]	Loss: 1.3622
Training Epoch: 12 [30080/50048]	Loss: 1.3398
Training Epoch: 12 [30208/50048]	Loss: 1.0070
Training Epoch: 12 [30336/50048]	Loss: 1.7938
Training Epoch: 12 [30464/50048]	Loss: 1.4146
Training Epoch: 12 [30592/50048]	Loss: 1.3580
Training Epoch: 12 [30720/50048]	Loss: 1.2641
Training Epoch: 12 [30848/50048]	Loss: 1.3192
Training Epoch: 12 [30976/50048]	Loss: 1.2597
Training Epoch: 12 [31104/50048]	Loss: 1.2834
Training Epoch: 12 [31232/50048]	Loss: 1.5671
Training Epoch: 12 [31360/50048]	Loss: 1.3894
Training Epoch: 12 [31488/50048]	Loss: 1.5758
Training Epoch: 12 [31616/50048]	Loss: 1.4369
Training Epoch: 12 [31744/50048]	Loss: 1.2504
Training Epoch: 12 [31872/50048]	Loss: 1.1400
Training Epoch: 12 [32000/50048]	Loss: 1.6394
Training Epoch: 12 [32128/50048]	Loss: 1.5301
Training Epoch: 12 [32256/50048]	Loss: 1.2748
Training Epoch: 12 [32384/50048]	Loss: 1.3591
Training Epoch: 12 [32512/50048]	Loss: 1.3970
Training Epoch: 12 [32640/50048]	Loss: 1.6065
Training Epoch: 12 [32768/50048]	Loss: 1.4429
Training Epoch: 12 [32896/50048]	Loss: 1.3873
Training Epoch: 12 [33024/50048]	Loss: 1.3452
Training Epoch: 12 [33152/50048]	Loss: 1.4299
Training Epoch: 12 [33280/50048]	Loss: 1.3805
Training Epoch: 12 [33408/50048]	Loss: 1.3448
Training Epoch: 12 [33536/50048]	Loss: 1.3976
Training Epoch: 12 [33664/50048]	Loss: 1.3300
Training Epoch: 12 [33792/50048]	Loss: 1.2655
Training Epoch: 12 [33920/50048]	Loss: 1.3567
Training Epoch: 12 [34048/50048]	Loss: 1.4192
Training Epoch: 12 [34176/50048]	Loss: 1.3405
Training Epoch: 12 [34304/50048]	Loss: 1.3024
Training Epoch: 12 [34432/50048]	Loss: 1.3985
Training Epoch: 12 [34560/50048]	Loss: 1.3433
Training Epoch: 12 [34688/50048]	Loss: 1.3298
Training Epoch: 12 [34816/50048]	Loss: 1.2300
Training Epoch: 12 [34944/50048]	Loss: 1.6164
Training Epoch: 12 [35072/50048]	Loss: 1.3608
Training Epoch: 12 [35200/50048]	Loss: 1.3571
Training Epoch: 12 [35328/50048]	Loss: 1.4156
Training Epoch: 12 [35456/50048]	Loss: 1.3474
Training Epoch: 12 [35584/50048]	Loss: 1.5600
Training Epoch: 12 [35712/50048]	Loss: 1.2835
Training Epoch: 12 [35840/50048]	Loss: 1.2866
Training Epoch: 12 [35968/50048]	Loss: 1.2394
Training Epoch: 12 [36096/50048]	Loss: 1.6332
Training Epoch: 12 [36224/50048]	Loss: 1.5208
Training Epoch: 12 [36352/50048]	Loss: 1.6830
Training Epoch: 12 [36480/50048]	Loss: 1.4261
Training Epoch: 12 [36608/50048]	Loss: 1.3306
Training Epoch: 12 [36736/50048]	Loss: 1.1851
Training Epoch: 12 [36864/50048]	Loss: 1.2896
Training Epoch: 12 [36992/50048]	Loss: 1.3540
Training Epoch: 12 [37120/50048]	Loss: 1.4400
Training Epoch: 12 [37248/50048]	Loss: 1.5399
Training Epoch: 12 [37376/50048]	Loss: 1.7813
Training Epoch: 12 [37504/50048]	Loss: 1.7258
Training Epoch: 12 [37632/50048]	Loss: 1.6392
Training Epoch: 12 [37760/50048]	Loss: 1.2136
Training Epoch: 12 [37888/50048]	Loss: 1.3826
Training Epoch: 12 [38016/50048]	Loss: 1.2903
Training Epoch: 12 [38144/50048]	Loss: 1.6402
Training Epoch: 12 [38272/50048]	Loss: 1.1811
Training Epoch: 12 [38400/50048]	Loss: 1.4209
Training Epoch: 12 [38528/50048]	Loss: 1.2061
Training Epoch: 12 [38656/50048]	Loss: 1.3610
Training Epoch: 12 [38784/50048]	Loss: 1.6110
Training Epoch: 12 [38912/50048]	Loss: 1.3577
Training Epoch: 12 [39040/50048]	Loss: 1.4236
Training Epoch: 12 [39168/50048]	Loss: 1.6542
Training Epoch: 12 [39296/50048]	Loss: 1.4366
Training Epoch: 12 [39424/50048]	Loss: 1.4439
Training Epoch: 12 [39552/50048]	Loss: 1.4095
Training Epoch: 12 [39680/50048]	Loss: 1.4542
Training Epoch: 12 [39808/50048]	Loss: 1.2359
Training Epoch: 12 [39936/50048]	Loss: 1.3104
Training Epoch: 12 [40064/50048]	Loss: 1.4261
Training Epoch: 12 [40192/50048]	Loss: 1.3182
Training Epoch: 12 [40320/50048]	Loss: 1.3518
Training Epoch: 12 [40448/50048]	Loss: 1.2673
Training Epoch: 12 [40576/50048]	Loss: 1.2231
Training Epoch: 12 [40704/50048]	Loss: 1.3842
Training Epoch: 12 [40832/50048]	Loss: 1.5052
Training Epoch: 12 [40960/50048]	Loss: 1.2981
Training Epoch: 12 [41088/50048]	Loss: 1.6171
Training Epoch: 12 [41216/50048]	Loss: 1.2585
Training Epoch: 12 [41344/50048]	Loss: 1.3768
Training Epoch: 12 [41472/50048]	Loss: 1.8167
Training Epoch: 12 [41600/50048]	Loss: 1.2061
Training Epoch: 12 [41728/50048]	Loss: 1.4915
Training Epoch: 12 [41856/50048]	Loss: 1.5607
Training Epoch: 12 [41984/50048]	Loss: 1.3321
Training Epoch: 12 [42112/50048]	Loss: 1.4376
Training Epoch: 12 [42240/50048]	Loss: 1.1798
Training Epoch: 12 [42368/50048]	Loss: 1.5405
Training Epoch: 12 [42496/50048]	Loss: 1.0995
Training Epoch: 12 [42624/50048]	Loss: 1.3104
Training Epoch: 12 [42752/50048]	Loss: 1.7746
Training Epoch: 12 [42880/50048]	Loss: 1.1867
Training Epoch: 12 [43008/50048]	Loss: 1.4909
Training Epoch: 12 [43136/50048]	Loss: 1.2699
Training Epoch: 12 [43264/50048]	Loss: 1.4194
Training Epoch: 12 [43392/50048]	Loss: 1.4408
Training Epoch: 12 [43520/50048]	Loss: 1.4096
Training Epoch: 12 [43648/50048]	Loss: 1.5927
Training Epoch: 12 [43776/50048]	Loss: 1.3209
Training Epoch: 12 [43904/50048]	Loss: 1.2775
Training Epoch: 12 [44032/50048]	Loss: 1.7165
Training Epoch: 12 [44160/50048]	Loss: 1.4427
Training Epoch: 12 [44288/50048]	Loss: 1.3832
Training Epoch: 12 [44416/50048]	Loss: 1.4436
Training Epoch: 12 [44544/50048]	Loss: 1.2840
Training Epoch: 12 [44672/50048]	Loss: 1.4489
Training Epoch: 12 [44800/50048]	Loss: 1.2863
Training Epoch: 12 [44928/50048]	Loss: 1.2440
Training Epoch: 12 [45056/50048]	Loss: 1.4347
Training Epoch: 12 [45184/50048]	Loss: 1.5069
Training Epoch: 12 [45312/50048]	Loss: 1.7147
Training Epoch: 12 [45440/50048]	Loss: 1.4980
Training Epoch: 12 [45568/50048]	Loss: 1.3552
Training Epoch: 12 [45696/50048]	Loss: 1.4415
2022-12-06 03:56:54,187 [ZeusDataLoader(train)] train epoch 13 done: time=86.54 energy=10514.93
2022-12-06 03:56:54,188 [ZeusDataLoader(eval)] Epoch 13 begin.
Training Epoch: 12 [45824/50048]	Loss: 1.3130
Training Epoch: 12 [45952/50048]	Loss: 1.4724
Training Epoch: 12 [46080/50048]	Loss: 1.4863
Training Epoch: 12 [46208/50048]	Loss: 1.5616
Training Epoch: 12 [46336/50048]	Loss: 1.2761
Training Epoch: 12 [46464/50048]	Loss: 1.3337
Training Epoch: 12 [46592/50048]	Loss: 1.3025
Training Epoch: 12 [46720/50048]	Loss: 1.1893
Training Epoch: 12 [46848/50048]	Loss: 1.5006
Training Epoch: 12 [46976/50048]	Loss: 1.5714
Training Epoch: 12 [47104/50048]	Loss: 1.4673
Training Epoch: 12 [47232/50048]	Loss: 1.3585
Training Epoch: 12 [47360/50048]	Loss: 1.5634
Training Epoch: 12 [47488/50048]	Loss: 1.3157
Training Epoch: 12 [47616/50048]	Loss: 1.3677
Training Epoch: 12 [47744/50048]	Loss: 1.3734
Training Epoch: 12 [47872/50048]	Loss: 1.3510
Training Epoch: 12 [48000/50048]	Loss: 1.5534
Training Epoch: 12 [48128/50048]	Loss: 1.3908
Training Epoch: 12 [48256/50048]	Loss: 1.3043
Training Epoch: 12 [48384/50048]	Loss: 1.4674
Training Epoch: 12 [48512/50048]	Loss: 1.3074
Training Epoch: 12 [48640/50048]	Loss: 1.3694
Training Epoch: 12 [48768/50048]	Loss: 1.5373
Training Epoch: 12 [48896/50048]	Loss: 1.3190
Training Epoch: 12 [49024/50048]	Loss: 1.4057
Training Epoch: 12 [49152/50048]	Loss: 1.2315
Training Epoch: 12 [49280/50048]	Loss: 1.1535
Training Epoch: 12 [49408/50048]	Loss: 1.5631
Training Epoch: 12 [49536/50048]	Loss: 1.2850
Training Epoch: 12 [49664/50048]	Loss: 1.6148
Training Epoch: 12 [49792/50048]	Loss: 1.3349
Training Epoch: 12 [49920/50048]	Loss: 1.5415
Training Epoch: 12 [50048/50048]	Loss: 1.1479
2022-12-06 08:56:57.846 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 03:56:57,860 [ZeusDataLoader(eval)] eval epoch 13 done: time=3.66 energy=442.80
2022-12-06 03:56:57,860 [ZeusDataLoader(train)] Up to epoch 13: time=1175.36, energy=142315.28, cost=174001.61
2022-12-06 03:56:57,860 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 03:56:57,860 [ZeusDataLoader(train)] Expected next epoch: time=1265.16, energy=153113.29, cost=187257.99
2022-12-06 03:56:57,861 [ZeusDataLoader(train)] Epoch 14 begin.
Validation Epoch: 12, Average loss: 0.0129, Accuracy: 0.5451
2022-12-06 03:56:58,047 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 03:56:58,047 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 08:56:58.049 [ZeusMonitor] Monitor started.
2022-12-06 08:56:58.049 [ZeusMonitor] Running indefinitely. 2022-12-06 08:56:58.049 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 08:56:58.049 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e14+gpu0.power.log
Training Epoch: 13 [128/50048]	Loss: 1.2545
Training Epoch: 13 [256/50048]	Loss: 1.1488
Training Epoch: 13 [384/50048]	Loss: 1.6468
Training Epoch: 13 [512/50048]	Loss: 0.9794
Training Epoch: 13 [640/50048]	Loss: 1.1676
Training Epoch: 13 [768/50048]	Loss: 1.2408
Training Epoch: 13 [896/50048]	Loss: 1.4561
Training Epoch: 13 [1024/50048]	Loss: 1.2434
Training Epoch: 13 [1152/50048]	Loss: 1.1911
Training Epoch: 13 [1280/50048]	Loss: 1.3886
Training Epoch: 13 [1408/50048]	Loss: 1.3702
Training Epoch: 13 [1536/50048]	Loss: 1.3768
Training Epoch: 13 [1664/50048]	Loss: 1.2847
Training Epoch: 13 [1792/50048]	Loss: 1.3126
Training Epoch: 13 [1920/50048]	Loss: 1.4177
Training Epoch: 13 [2048/50048]	Loss: 1.2874
Training Epoch: 13 [2176/50048]	Loss: 1.3899
Training Epoch: 13 [2304/50048]	Loss: 1.3081
Training Epoch: 13 [2432/50048]	Loss: 1.2663
Training Epoch: 13 [2560/50048]	Loss: 1.4693
Training Epoch: 13 [2688/50048]	Loss: 1.4542
Training Epoch: 13 [2816/50048]	Loss: 1.3387
Training Epoch: 13 [2944/50048]	Loss: 1.2041
Training Epoch: 13 [3072/50048]	Loss: 1.3201
Training Epoch: 13 [3200/50048]	Loss: 1.2642
Training Epoch: 13 [3328/50048]	Loss: 1.2560
Training Epoch: 13 [3456/50048]	Loss: 1.2049
Training Epoch: 13 [3584/50048]	Loss: 1.2129
Training Epoch: 13 [3712/50048]	Loss: 1.4829
Training Epoch: 13 [3840/50048]	Loss: 1.2147
Training Epoch: 13 [3968/50048]	Loss: 1.2308
Training Epoch: 13 [4096/50048]	Loss: 1.1854
Training Epoch: 13 [4224/50048]	Loss: 1.3506
Training Epoch: 13 [4352/50048]	Loss: 1.3849
Training Epoch: 13 [4480/50048]	Loss: 1.3238
Training Epoch: 13 [4608/50048]	Loss: 1.1676
Training Epoch: 13 [4736/50048]	Loss: 1.5185
Training Epoch: 13 [4864/50048]	Loss: 1.2449
Training Epoch: 13 [4992/50048]	Loss: 1.4553
Training Epoch: 13 [5120/50048]	Loss: 1.3088
Training Epoch: 13 [5248/50048]	Loss: 1.3918
Training Epoch: 13 [5376/50048]	Loss: 1.2028
Training Epoch: 13 [5504/50048]	Loss: 1.2942
Training Epoch: 13 [5632/50048]	Loss: 1.3537
Training Epoch: 13 [5760/50048]	Loss: 1.5037
Training Epoch: 13 [5888/50048]	Loss: 1.4386
Training Epoch: 13 [6016/50048]	Loss: 1.2785
Training Epoch: 13 [6144/50048]	Loss: 1.2929
Training Epoch: 13 [6272/50048]	Loss: 1.3336
Training Epoch: 13 [6400/50048]	Loss: 1.3873
Training Epoch: 13 [6528/50048]	Loss: 1.3675
Training Epoch: 13 [6656/50048]	Loss: 1.0110
Training Epoch: 13 [6784/50048]	Loss: 1.2665
Training Epoch: 13 [6912/50048]	Loss: 1.2465
Training Epoch: 13 [7040/50048]	Loss: 1.0124
Training Epoch: 13 [7168/50048]	Loss: 1.5592
Training Epoch: 13 [7296/50048]	Loss: 1.3580
Training Epoch: 13 [7424/50048]	Loss: 1.5248
Training Epoch: 13 [7552/50048]	Loss: 1.4382
Training Epoch: 13 [7680/50048]	Loss: 1.2391
Training Epoch: 13 [7808/50048]	Loss: 1.3996
Training Epoch: 13 [7936/50048]	Loss: 1.3487
Training Epoch: 13 [8064/50048]	Loss: 1.2235
Training Epoch: 13 [8192/50048]	Loss: 1.2161
Training Epoch: 13 [8320/50048]	Loss: 1.0548
Training Epoch: 13 [8448/50048]	Loss: 1.2719
Training Epoch: 13 [8576/50048]	Loss: 1.1079
Training Epoch: 13 [8704/50048]	Loss: 1.2313
Training Epoch: 13 [8832/50048]	Loss: 1.2450
Training Epoch: 13 [8960/50048]	Loss: 1.3510
Training Epoch: 13 [9088/50048]	Loss: 1.4928
Training Epoch: 13 [9216/50048]	Loss: 1.2386
Training Epoch: 13 [9344/50048]	Loss: 1.3590
Training Epoch: 13 [9472/50048]	Loss: 1.4399
Training Epoch: 13 [9600/50048]	Loss: 1.5228
Training Epoch: 13 [9728/50048]	Loss: 1.3044
Training Epoch: 13 [9856/50048]	Loss: 1.5641
Training Epoch: 13 [9984/50048]	Loss: 1.2318
Training Epoch: 13 [10112/50048]	Loss: 1.5049
Training Epoch: 13 [10240/50048]	Loss: 1.2397
Training Epoch: 13 [10368/50048]	Loss: 1.3460
Training Epoch: 13 [10496/50048]	Loss: 1.4685
Training Epoch: 13 [10624/50048]	Loss: 1.2054
Training Epoch: 13 [10752/50048]	Loss: 1.2282
Training Epoch: 13 [10880/50048]	Loss: 1.3734
Training Epoch: 13 [11008/50048]	Loss: 1.3583
Training Epoch: 13 [11136/50048]	Loss: 1.3499
Training Epoch: 13 [11264/50048]	Loss: 1.4847
Training Epoch: 13 [11392/50048]	Loss: 1.3744
Training Epoch: 13 [11520/50048]	Loss: 1.3202
Training Epoch: 13 [11648/50048]	Loss: 1.4069
Training Epoch: 13 [11776/50048]	Loss: 1.6093
Training Epoch: 13 [11904/50048]	Loss: 1.5465
Training Epoch: 13 [12032/50048]	Loss: 1.2573
Training Epoch: 13 [12160/50048]	Loss: 1.4286
Training Epoch: 13 [12288/50048]	Loss: 1.3050
Training Epoch: 13 [12416/50048]	Loss: 1.2993
Training Epoch: 13 [12544/50048]	Loss: 1.2708
Training Epoch: 13 [12672/50048]	Loss: 1.2236
Training Epoch: 13 [12800/50048]	Loss: 1.4170
Training Epoch: 13 [12928/50048]	Loss: 1.2762
Training Epoch: 13 [13056/50048]	Loss: 1.4747
Training Epoch: 13 [13184/50048]	Loss: 1.4847
Training Epoch: 13 [13312/50048]	Loss: 1.0807
Training Epoch: 13 [13440/50048]	Loss: 1.3676
Training Epoch: 13 [13568/50048]	Loss: 1.3452
Training Epoch: 13 [13696/50048]	Loss: 1.2648
Training Epoch: 13 [13824/50048]	Loss: 1.2713
Training Epoch: 13 [13952/50048]	Loss: 1.2755
Training Epoch: 13 [14080/50048]	Loss: 1.4703
Training Epoch: 13 [14208/50048]	Loss: 1.2146
Training Epoch: 13 [14336/50048]	Loss: 1.3921
Training Epoch: 13 [14464/50048]	Loss: 1.3676
Training Epoch: 13 [14592/50048]	Loss: 1.2036
Training Epoch: 13 [14720/50048]	Loss: 1.1729
Training Epoch: 13 [14848/50048]	Loss: 1.3642
Training Epoch: 13 [14976/50048]	Loss: 1.2780
Training Epoch: 13 [15104/50048]	Loss: 1.3157
Training Epoch: 13 [15232/50048]	Loss: 1.4817
Training Epoch: 13 [15360/50048]	Loss: 1.1435
Training Epoch: 13 [15488/50048]	Loss: 1.1909
Training Epoch: 13 [15616/50048]	Loss: 1.2447
Training Epoch: 13 [15744/50048]	Loss: 1.2699
Training Epoch: 13 [15872/50048]	Loss: 1.4208
Training Epoch: 13 [16000/50048]	Loss: 1.4478
Training Epoch: 13 [16128/50048]	Loss: 1.3907
Training Epoch: 13 [16256/50048]	Loss: 1.4964
Training Epoch: 13 [16384/50048]	Loss: 1.5498
Training Epoch: 13 [16512/50048]	Loss: 1.2942
Training Epoch: 13 [16640/50048]	Loss: 1.3404
Training Epoch: 13 [16768/50048]	Loss: 1.3791
Training Epoch: 13 [16896/50048]	Loss: 1.4577
Training Epoch: 13 [17024/50048]	Loss: 1.2188
Training Epoch: 13 [17152/50048]	Loss: 1.1977
Training Epoch: 13 [17280/50048]	Loss: 1.5062
Training Epoch: 13 [17408/50048]	Loss: 1.3269
Training Epoch: 13 [17536/50048]	Loss: 1.3706
Training Epoch: 13 [17664/50048]	Loss: 1.2325
Training Epoch: 13 [17792/50048]	Loss: 1.2099
Training Epoch: 13 [17920/50048]	Loss: 1.2582
Training Epoch: 13 [18048/50048]	Loss: 1.4041
Training Epoch: 13 [18176/50048]	Loss: 1.3335
Training Epoch: 13 [18304/50048]	Loss: 1.1662
Training Epoch: 13 [18432/50048]	Loss: 1.0842
Training Epoch: 13 [18560/50048]	Loss: 1.4734
Training Epoch: 13 [18688/50048]	Loss: 1.3178
Training Epoch: 13 [18816/50048]	Loss: 1.2171
Training Epoch: 13 [18944/50048]	Loss: 1.2933
Training Epoch: 13 [19072/50048]	Loss: 1.5612
Training Epoch: 13 [19200/50048]	Loss: 1.2349
Training Epoch: 13 [19328/50048]	Loss: 1.3352
Training Epoch: 13 [19456/50048]	Loss: 1.5147
Training Epoch: 13 [19584/50048]	Loss: 1.2371
Training Epoch: 13 [19712/50048]	Loss: 1.5108
Training Epoch: 13 [19840/50048]	Loss: 1.3278
Training Epoch: 13 [19968/50048]	Loss: 1.1836
Training Epoch: 13 [20096/50048]	Loss: 1.7417
Training Epoch: 13 [20224/50048]	Loss: 1.2850
Training Epoch: 13 [20352/50048]	Loss: 1.3149
Training Epoch: 13 [20480/50048]	Loss: 1.2038
Training Epoch: 13 [20608/50048]	Loss: 1.2417
Training Epoch: 13 [20736/50048]	Loss: 1.3535
Training Epoch: 13 [20864/50048]	Loss: 1.2994
Training Epoch: 13 [20992/50048]	Loss: 1.1384
Training Epoch: 13 [21120/50048]	Loss: 1.4414
Training Epoch: 13 [21248/50048]	Loss: 1.5392
Training Epoch: 13 [21376/50048]	Loss: 1.3005
Training Epoch: 13 [21504/50048]	Loss: 1.2828
Training Epoch: 13 [21632/50048]	Loss: 1.1353
Training Epoch: 13 [21760/50048]	Loss: 1.4819
Training Epoch: 13 [21888/50048]	Loss: 1.3231
Training Epoch: 13 [22016/50048]	Loss: 1.3841
Training Epoch: 13 [22144/50048]	Loss: 1.3611
Training Epoch: 13 [22272/50048]	Loss: 1.2220
Training Epoch: 13 [22400/50048]	Loss: 1.3720
Training Epoch: 13 [22528/50048]	Loss: 1.3386
Training Epoch: 13 [22656/50048]	Loss: 1.4399
Training Epoch: 13 [22784/50048]	Loss: 1.4196
Training Epoch: 13 [22912/50048]	Loss: 1.3488
Training Epoch: 13 [23040/50048]	Loss: 1.3249
Training Epoch: 13 [23168/50048]	Loss: 1.3076
Training Epoch: 13 [23296/50048]	Loss: 1.4171
Training Epoch: 13 [23424/50048]	Loss: 1.3755
Training Epoch: 13 [23552/50048]	Loss: 1.2818
Training Epoch: 13 [23680/50048]	Loss: 1.0025
Training Epoch: 13 [23808/50048]	Loss: 1.2702
Training Epoch: 13 [23936/50048]	Loss: 1.1663
Training Epoch: 13 [24064/50048]	Loss: 1.5515
Training Epoch: 13 [24192/50048]	Loss: 1.5279
Training Epoch: 13 [24320/50048]	Loss: 1.3974
Training Epoch: 13 [24448/50048]	Loss: 1.2388
Training Epoch: 13 [24576/50048]	Loss: 1.3738
Training Epoch: 13 [24704/50048]	Loss: 1.3218
Training Epoch: 13 [24832/50048]	Loss: 1.1478
Training Epoch: 13 [24960/50048]	Loss: 1.1809
Training Epoch: 13 [25088/50048]	Loss: 1.3176
Training Epoch: 13 [25216/50048]	Loss: 1.4063
Training Epoch: 13 [25344/50048]	Loss: 1.4484
Training Epoch: 13 [25472/50048]	Loss: 1.4350
Training Epoch: 13 [25600/50048]	Loss: 1.3270
Training Epoch: 13 [25728/50048]	Loss: 1.5072
Training Epoch: 13 [25856/50048]	Loss: 1.1404
Training Epoch: 13 [25984/50048]	Loss: 1.4397
Training Epoch: 13 [26112/50048]	Loss: 1.4625
Training Epoch: 13 [26240/50048]	Loss: 1.6808
Training Epoch: 13 [26368/50048]	Loss: 1.5195
Training Epoch: 13 [26496/50048]	Loss: 1.5218
Training Epoch: 13 [26624/50048]	Loss: 1.4878
Training Epoch: 13 [26752/50048]	Loss: 1.2561
Training Epoch: 13 [26880/50048]	Loss: 1.1073
Training Epoch: 13 [27008/50048]	Loss: 1.3531
Training Epoch: 13 [27136/50048]	Loss: 1.4449
Training Epoch: 13 [27264/50048]	Loss: 1.4048
Training Epoch: 13 [27392/50048]	Loss: 1.5292
Training Epoch: 13 [27520/50048]	Loss: 1.1933
Training Epoch: 13 [27648/50048]	Loss: 1.2413
Training Epoch: 13 [27776/50048]	Loss: 1.4939
Training Epoch: 13 [27904/50048]	Loss: 1.1209
Training Epoch: 13 [28032/50048]	Loss: 1.2514
Training Epoch: 13 [28160/50048]	Loss: 1.5709
Training Epoch: 13 [28288/50048]	Loss: 1.1726
Training Epoch: 13 [28416/50048]	Loss: 1.4034
Training Epoch: 13 [28544/50048]	Loss: 1.3439
Training Epoch: 13 [28672/50048]	Loss: 1.3605
Training Epoch: 13 [28800/50048]	Loss: 1.3982
Training Epoch: 13 [28928/50048]	Loss: 1.3350
Training Epoch: 13 [29056/50048]	Loss: 1.1235
Training Epoch: 13 [29184/50048]	Loss: 1.1953
Training Epoch: 13 [29312/50048]	Loss: 1.5504
Training Epoch: 13 [29440/50048]	Loss: 1.3564
Training Epoch: 13 [29568/50048]	Loss: 1.1125
Training Epoch: 13 [29696/50048]	Loss: 1.4292
Training Epoch: 13 [29824/50048]	Loss: 1.3018
Training Epoch: 13 [29952/50048]	Loss: 1.3419
Training Epoch: 13 [30080/50048]	Loss: 1.3074
Training Epoch: 13 [30208/50048]	Loss: 1.3895
Training Epoch: 13 [30336/50048]	Loss: 1.3309
Training Epoch: 13 [30464/50048]	Loss: 1.6013
Training Epoch: 13 [30592/50048]	Loss: 1.4365
Training Epoch: 13 [30720/50048]	Loss: 1.3272
Training Epoch: 13 [30848/50048]	Loss: 1.5153
Training Epoch: 13 [30976/50048]	Loss: 1.3119
Training Epoch: 13 [31104/50048]	Loss: 1.1801
Training Epoch: 13 [31232/50048]	Loss: 1.3982
Training Epoch: 13 [31360/50048]	Loss: 1.4533
Training Epoch: 13 [31488/50048]	Loss: 1.4138
Training Epoch: 13 [31616/50048]	Loss: 1.3477
Training Epoch: 13 [31744/50048]	Loss: 1.3075
Training Epoch: 13 [31872/50048]	Loss: 1.1990
Training Epoch: 13 [32000/50048]	Loss: 1.4093
Training Epoch: 13 [32128/50048]	Loss: 1.1747
Training Epoch: 13 [32256/50048]	Loss: 1.4117
Training Epoch: 13 [32384/50048]	Loss: 1.0327
Training Epoch: 13 [32512/50048]	Loss: 1.2596
Training Epoch: 13 [32640/50048]	Loss: 1.4001
Training Epoch: 13 [32768/50048]	Loss: 1.1575
Training Epoch: 13 [32896/50048]	Loss: 1.4436
Training Epoch: 13 [33024/50048]	Loss: 1.3958
Training Epoch: 13 [33152/50048]	Loss: 1.4604
Training Epoch: 13 [33280/50048]	Loss: 1.3087
Training Epoch: 13 [33408/50048]	Loss: 1.2134
Training Epoch: 13 [33536/50048]	Loss: 1.5148
Training Epoch: 13 [33664/50048]	Loss: 1.3486
Training Epoch: 13 [33792/50048]	Loss: 1.4855
Training Epoch: 13 [33920/50048]	Loss: 1.2551
Training Epoch: 13 [34048/50048]	Loss: 1.5172
Training Epoch: 13 [34176/50048]	Loss: 1.5461
Training Epoch: 13 [34304/50048]	Loss: 1.1935
Training Epoch: 13 [34432/50048]	Loss: 1.7445
Training Epoch: 13 [34560/50048]	Loss: 1.7125
Training Epoch: 13 [34688/50048]	Loss: 1.3844
Training Epoch: 13 [34816/50048]	Loss: 1.4478
Training Epoch: 13 [34944/50048]	Loss: 1.3157
Training Epoch: 13 [35072/50048]	Loss: 1.3306
Training Epoch: 13 [35200/50048]	Loss: 1.7654
Training Epoch: 13 [35328/50048]	Loss: 1.2548
Training Epoch: 13 [35456/50048]	Loss: 1.4973
Training Epoch: 13 [35584/50048]	Loss: 1.3871
Training Epoch: 13 [35712/50048]	Loss: 1.5199
Training Epoch: 13 [35840/50048]	Loss: 1.3984
Training Epoch: 13 [35968/50048]	Loss: 1.1992
Training Epoch: 13 [36096/50048]	Loss: 1.2015
Training Epoch: 13 [36224/50048]	Loss: 1.3398
Training Epoch: 13 [36352/50048]	Loss: 1.4782
Training Epoch: 13 [36480/50048]	Loss: 1.4919
Training Epoch: 13 [36608/50048]	Loss: 1.4504
Training Epoch: 13 [36736/50048]	Loss: 1.4200
Training Epoch: 13 [36864/50048]	Loss: 1.5047
Training Epoch: 13 [36992/50048]	Loss: 1.3298
Training Epoch: 13 [37120/50048]	Loss: 1.3206
Training Epoch: 13 [37248/50048]	Loss: 1.4909
Training Epoch: 13 [37376/50048]	Loss: 1.5171
Training Epoch: 13 [37504/50048]	Loss: 1.4257
Training Epoch: 13 [37632/50048]	Loss: 1.2309
Training Epoch: 13 [37760/50048]	Loss: 1.4949
Training Epoch: 13 [37888/50048]	Loss: 1.3888
Training Epoch: 13 [38016/50048]	Loss: 1.4082
Training Epoch: 13 [38144/50048]	Loss: 1.2271
Training Epoch: 13 [38272/50048]	Loss: 1.1748
Training Epoch: 13 [38400/50048]	Loss: 1.2560
Training Epoch: 13 [38528/50048]	Loss: 1.2963
Training Epoch: 13 [38656/50048]	Loss: 1.3337
Training Epoch: 13 [38784/50048]	Loss: 1.3675
Training Epoch: 13 [38912/50048]	Loss: 1.2700
Training Epoch: 13 [39040/50048]	Loss: 1.4396
Training Epoch: 13 [39168/50048]	Loss: 1.2132
Training Epoch: 13 [39296/50048]	Loss: 1.1327
Training Epoch: 13 [39424/50048]	Loss: 1.3151
Training Epoch: 13 [39552/50048]	Loss: 1.4147
Training Epoch: 13 [39680/50048]	Loss: 1.4569
Training Epoch: 13 [39808/50048]	Loss: 1.5422
Training Epoch: 13 [39936/50048]	Loss: 1.3438
Training Epoch: 13 [40064/50048]	Loss: 1.4946
Training Epoch: 13 [40192/50048]	Loss: 1.3123
Training Epoch: 13 [40320/50048]	Loss: 1.4963
Training Epoch: 13 [40448/50048]	Loss: 1.3540
Training Epoch: 13 [40576/50048]	Loss: 1.2692
Training Epoch: 13 [40704/50048]	Loss: 1.1610
Training Epoch: 13 [40832/50048]	Loss: 1.3204
Training Epoch: 13 [40960/50048]	Loss: 1.4508
Training Epoch: 13 [41088/50048]	Loss: 1.3075
Training Epoch: 13 [41216/50048]	Loss: 1.2538
Training Epoch: 13 [41344/50048]	Loss: 1.2423
Training Epoch: 13 [41472/50048]	Loss: 1.3093
Training Epoch: 13 [41600/50048]	Loss: 1.4130
Training Epoch: 13 [41728/50048]	Loss: 1.2998
Training Epoch: 13 [41856/50048]	Loss: 1.3359
Training Epoch: 13 [41984/50048]	Loss: 1.5689
Training Epoch: 13 [42112/50048]	Loss: 1.2007
Training Epoch: 13 [42240/50048]	Loss: 1.3297
Training Epoch: 13 [42368/50048]	Loss: 1.4057
Training Epoch: 13 [42496/50048]	Loss: 1.2300
Training Epoch: 13 [42624/50048]	Loss: 1.1808
Training Epoch: 13 [42752/50048]	Loss: 1.3706
Training Epoch: 13 [42880/50048]	Loss: 1.3807
Training Epoch: 13 [43008/50048]	Loss: 1.1708
Training Epoch: 13 [43136/50048]	Loss: 1.4347
Training Epoch: 13 [43264/50048]	Loss: 1.4971
Training Epoch: 13 [43392/50048]	Loss: 1.3394
Training Epoch: 13 [43520/50048]	Loss: 1.4503
Training Epoch: 13 [43648/50048]	Loss: 1.5208
Training Epoch: 13 [43776/50048]	Loss: 1.2728
Training Epoch: 13 [43904/50048]	Loss: 1.4752
Training Epoch: 13 [44032/50048]	Loss: 1.2095
Training Epoch: 13 [44160/50048]	Loss: 1.2371
Training Epoch: 13 [44288/50048]	Loss: 1.4380
Training Epoch: 13 [44416/50048]	Loss: 1.2059
Training Epoch: 13 [44544/50048]	Loss: 1.4011
Training Epoch: 13 [44672/50048]	Loss: 1.3031
Training Epoch: 13 [44800/50048]	Loss: 1.3155
Training Epoch: 13 [44928/50048]	Loss: 1.3091
Training Epoch: 13 [45056/50048]	Loss: 1.4384
Training Epoch: 13 [45184/50048]	Loss: 1.3759
Training Epoch: 13 [45312/50048]	Loss: 1.3428
Training Epoch: 13 [45440/50048]	Loss: 1.7484
Training Epoch: 13 [45568/50048]	Loss: 1.2762
Training Epoch: 13 [45696/50048]	Loss: 1.2854
2022-12-06 03:58:24,412 [ZeusDataLoader(train)] train epoch 14 done: time=86.54 energy=10511.61
2022-12-06 03:58:24,414 [ZeusDataLoader(eval)] Epoch 14 begin.
Training Epoch: 13 [45824/50048]	Loss: 1.1609
Training Epoch: 13 [45952/50048]	Loss: 1.4749
Training Epoch: 13 [46080/50048]	Loss: 1.1711
Training Epoch: 13 [46208/50048]	Loss: 1.1809
Training Epoch: 13 [46336/50048]	Loss: 1.3868
Training Epoch: 13 [46464/50048]	Loss: 1.3542
Training Epoch: 13 [46592/50048]	Loss: 1.2585
Training Epoch: 13 [46720/50048]	Loss: 1.1239
Training Epoch: 13 [46848/50048]	Loss: 1.3641
Training Epoch: 13 [46976/50048]	Loss: 1.6018
Training Epoch: 13 [47104/50048]	Loss: 1.3655
Training Epoch: 13 [47232/50048]	Loss: 1.2646
Training Epoch: 13 [47360/50048]	Loss: 1.6748
Training Epoch: 13 [47488/50048]	Loss: 1.4425
Training Epoch: 13 [47616/50048]	Loss: 1.3876
Training Epoch: 13 [47744/50048]	Loss: 1.2819
Training Epoch: 13 [47872/50048]	Loss: 1.2116
Training Epoch: 13 [48000/50048]	Loss: 1.5095
Training Epoch: 13 [48128/50048]	Loss: 1.3202
Training Epoch: 13 [48256/50048]	Loss: 1.4819
Training Epoch: 13 [48384/50048]	Loss: 1.0571
Training Epoch: 13 [48512/50048]	Loss: 1.5424
Training Epoch: 13 [48640/50048]	Loss: 1.3915
Training Epoch: 13 [48768/50048]	Loss: 1.4674
Training Epoch: 13 [48896/50048]	Loss: 1.2702
Training Epoch: 13 [49024/50048]	Loss: 1.2778
Training Epoch: 13 [49152/50048]	Loss: 1.3410
Training Epoch: 13 [49280/50048]	Loss: 1.3221
Training Epoch: 13 [49408/50048]	Loss: 1.4712
Training Epoch: 13 [49536/50048]	Loss: 1.6385
Training Epoch: 13 [49664/50048]	Loss: 1.6218
Training Epoch: 13 [49792/50048]	Loss: 1.6573
Training Epoch: 13 [49920/50048]	Loss: 1.4388
Training Epoch: 13 [50048/50048]	Loss: 1.3720
2022-12-06 08:58:28.131 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 03:58:28,168 [ZeusDataLoader(eval)] eval epoch 14 done: time=3.75 energy=454.31
2022-12-06 03:58:28,168 [ZeusDataLoader(train)] Up to epoch 14: time=1265.65, energy=153281.20, cost=187384.62
2022-12-06 03:58:28,168 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 03:58:28,168 [ZeusDataLoader(train)] Expected next epoch: time=1355.44, energy=164079.21, cost=200641.00
2022-12-06 03:58:28,169 [ZeusDataLoader(train)] Epoch 15 begin.
Validation Epoch: 13, Average loss: 0.0126, Accuracy: 0.5647
2022-12-06 03:58:28,339 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 03:58:28,340 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 08:58:28.342 [ZeusMonitor] Monitor started.
2022-12-06 08:58:28.342 [ZeusMonitor] Running indefinitely. 2022-12-06 08:58:28.342 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 08:58:28.342 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e15+gpu0.power.log
Training Epoch: 14 [128/50048]	Loss: 1.2138
Training Epoch: 14 [256/50048]	Loss: 1.2683
Training Epoch: 14 [384/50048]	Loss: 1.2709
Training Epoch: 14 [512/50048]	Loss: 1.1093
Training Epoch: 14 [640/50048]	Loss: 1.2644
Training Epoch: 14 [768/50048]	Loss: 1.3318
Training Epoch: 14 [896/50048]	Loss: 1.0557
Training Epoch: 14 [1024/50048]	Loss: 1.1717
Training Epoch: 14 [1152/50048]	Loss: 1.0722
Training Epoch: 14 [1280/50048]	Loss: 1.5574
Training Epoch: 14 [1408/50048]	Loss: 1.3294
Training Epoch: 14 [1536/50048]	Loss: 1.0291
Training Epoch: 14 [1664/50048]	Loss: 1.3361
Training Epoch: 14 [1792/50048]	Loss: 1.3678
Training Epoch: 14 [1920/50048]	Loss: 1.3346
Training Epoch: 14 [2048/50048]	Loss: 1.1212
Training Epoch: 14 [2176/50048]	Loss: 1.1616
Training Epoch: 14 [2304/50048]	Loss: 1.3052
Training Epoch: 14 [2432/50048]	Loss: 1.5129
Training Epoch: 14 [2560/50048]	Loss: 1.2741
Training Epoch: 14 [2688/50048]	Loss: 1.1043
Training Epoch: 14 [2816/50048]	Loss: 1.0740
Training Epoch: 14 [2944/50048]	Loss: 1.2171
Training Epoch: 14 [3072/50048]	Loss: 1.2177
Training Epoch: 14 [3200/50048]	Loss: 1.4096
Training Epoch: 14 [3328/50048]	Loss: 1.4427
Training Epoch: 14 [3456/50048]	Loss: 1.1861
Training Epoch: 14 [3584/50048]	Loss: 1.5776
Training Epoch: 14 [3712/50048]	Loss: 1.3334
Training Epoch: 14 [3840/50048]	Loss: 1.3080
Training Epoch: 14 [3968/50048]	Loss: 1.6670
Training Epoch: 14 [4096/50048]	Loss: 1.1339
Training Epoch: 14 [4224/50048]	Loss: 1.3787
Training Epoch: 14 [4352/50048]	Loss: 1.1240
Training Epoch: 14 [4480/50048]	Loss: 1.2744
Training Epoch: 14 [4608/50048]	Loss: 1.2153
Training Epoch: 14 [4736/50048]	Loss: 1.2873
Training Epoch: 14 [4864/50048]	Loss: 1.3027
Training Epoch: 14 [4992/50048]	Loss: 1.3433
Training Epoch: 14 [5120/50048]	Loss: 1.1536
Training Epoch: 14 [5248/50048]	Loss: 1.3920
Training Epoch: 14 [5376/50048]	Loss: 1.2803
Training Epoch: 14 [5504/50048]	Loss: 1.1816
Training Epoch: 14 [5632/50048]	Loss: 1.4966
Training Epoch: 14 [5760/50048]	Loss: 1.3412
Training Epoch: 14 [5888/50048]	Loss: 1.3606
Training Epoch: 14 [6016/50048]	Loss: 0.9998
Training Epoch: 14 [6144/50048]	Loss: 1.2938
Training Epoch: 14 [6272/50048]	Loss: 1.0969
Training Epoch: 14 [6400/50048]	Loss: 1.0803
Training Epoch: 14 [6528/50048]	Loss: 1.2457
Training Epoch: 14 [6656/50048]	Loss: 1.2038
Training Epoch: 14 [6784/50048]	Loss: 1.2116
Training Epoch: 14 [6912/50048]	Loss: 1.3811
Training Epoch: 14 [7040/50048]	Loss: 1.4964
Training Epoch: 14 [7168/50048]	Loss: 1.2832
Training Epoch: 14 [7296/50048]	Loss: 1.1710
Training Epoch: 14 [7424/50048]	Loss: 1.6527
Training Epoch: 14 [7552/50048]	Loss: 1.4832
Training Epoch: 14 [7680/50048]	Loss: 1.2886
Training Epoch: 14 [7808/50048]	Loss: 1.1622
Training Epoch: 14 [7936/50048]	Loss: 1.1264
Training Epoch: 14 [8064/50048]	Loss: 1.0942
Training Epoch: 14 [8192/50048]	Loss: 1.1567
Training Epoch: 14 [8320/50048]	Loss: 1.3188
Training Epoch: 14 [8448/50048]	Loss: 1.1870
Training Epoch: 14 [8576/50048]	Loss: 1.1397
Training Epoch: 14 [8704/50048]	Loss: 1.0935
Training Epoch: 14 [8832/50048]	Loss: 1.1087
Training Epoch: 14 [8960/50048]	Loss: 1.2056
Training Epoch: 14 [9088/50048]	Loss: 1.1828
Training Epoch: 14 [9216/50048]	Loss: 1.0839
Training Epoch: 14 [9344/50048]	Loss: 1.3534
Training Epoch: 14 [9472/50048]	Loss: 1.2229
Training Epoch: 14 [9600/50048]	Loss: 1.0921
Training Epoch: 14 [9728/50048]	Loss: 1.0995
Training Epoch: 14 [9856/50048]	Loss: 1.2397
Training Epoch: 14 [9984/50048]	Loss: 1.4397
Training Epoch: 14 [10112/50048]	Loss: 1.3805
Training Epoch: 14 [10240/50048]	Loss: 1.1811
Training Epoch: 14 [10368/50048]	Loss: 1.0390
Training Epoch: 14 [10496/50048]	Loss: 1.2952
Training Epoch: 14 [10624/50048]	Loss: 1.3692
Training Epoch: 14 [10752/50048]	Loss: 1.1150
Training Epoch: 14 [10880/50048]	Loss: 1.1057
Training Epoch: 14 [11008/50048]	Loss: 1.1270
Training Epoch: 14 [11136/50048]	Loss: 1.1892
Training Epoch: 14 [11264/50048]	Loss: 1.3044
Training Epoch: 14 [11392/50048]	Loss: 1.2463
Training Epoch: 14 [11520/50048]	Loss: 1.0926
Training Epoch: 14 [11648/50048]	Loss: 1.4651
Training Epoch: 14 [11776/50048]	Loss: 0.9957
Training Epoch: 14 [11904/50048]	Loss: 1.3399
Training Epoch: 14 [12032/50048]	Loss: 1.2393
Training Epoch: 14 [12160/50048]	Loss: 1.0845
Training Epoch: 14 [12288/50048]	Loss: 1.2141
Training Epoch: 14 [12416/50048]	Loss: 1.3361
Training Epoch: 14 [12544/50048]	Loss: 1.1608
Training Epoch: 14 [12672/50048]	Loss: 1.3840
Training Epoch: 14 [12800/50048]	Loss: 1.1554
Training Epoch: 14 [12928/50048]	Loss: 1.3371
Training Epoch: 14 [13056/50048]	Loss: 1.6227
Training Epoch: 14 [13184/50048]	Loss: 1.2206
Training Epoch: 14 [13312/50048]	Loss: 1.1193
Training Epoch: 14 [13440/50048]	Loss: 1.2785
Training Epoch: 14 [13568/50048]	Loss: 1.1428
Training Epoch: 14 [13696/50048]	Loss: 0.9361
Training Epoch: 14 [13824/50048]	Loss: 1.4192
Training Epoch: 14 [13952/50048]	Loss: 1.0673
Training Epoch: 14 [14080/50048]	Loss: 1.3506
Training Epoch: 14 [14208/50048]	Loss: 1.3626
Training Epoch: 14 [14336/50048]	Loss: 1.2112
Training Epoch: 14 [14464/50048]	Loss: 1.3314
Training Epoch: 14 [14592/50048]	Loss: 1.3096
Training Epoch: 14 [14720/50048]	Loss: 1.3384
Training Epoch: 14 [14848/50048]	Loss: 1.0201
Training Epoch: 14 [14976/50048]	Loss: 1.1732
Training Epoch: 14 [15104/50048]	Loss: 1.5975
Training Epoch: 14 [15232/50048]	Loss: 1.2693
Training Epoch: 14 [15360/50048]	Loss: 1.2143
Training Epoch: 14 [15488/50048]	Loss: 0.9291
Training Epoch: 14 [15616/50048]	Loss: 1.0449
Training Epoch: 14 [15744/50048]	Loss: 1.2607
Training Epoch: 14 [15872/50048]	Loss: 1.5534
Training Epoch: 14 [16000/50048]	Loss: 1.2173
Training Epoch: 14 [16128/50048]	Loss: 1.0610
Training Epoch: 14 [16256/50048]	Loss: 1.3058
Training Epoch: 14 [16384/50048]	Loss: 1.4157
Training Epoch: 14 [16512/50048]	Loss: 1.3011
Training Epoch: 14 [16640/50048]	Loss: 1.3581
Training Epoch: 14 [16768/50048]	Loss: 1.2638
Training Epoch: 14 [16896/50048]	Loss: 1.4373
Training Epoch: 14 [17024/50048]	Loss: 1.1323
Training Epoch: 14 [17152/50048]	Loss: 1.1989
Training Epoch: 14 [17280/50048]	Loss: 1.4579
Training Epoch: 14 [17408/50048]	Loss: 1.5765
Training Epoch: 14 [17536/50048]	Loss: 1.2968
Training Epoch: 14 [17664/50048]	Loss: 1.3734
Training Epoch: 14 [17792/50048]	Loss: 1.1233
Training Epoch: 14 [17920/50048]	Loss: 1.3455
Training Epoch: 14 [18048/50048]	Loss: 1.3938
Training Epoch: 14 [18176/50048]	Loss: 1.4273
Training Epoch: 14 [18304/50048]	Loss: 1.4711
Training Epoch: 14 [18432/50048]	Loss: 1.2644
Training Epoch: 14 [18560/50048]	Loss: 1.3828
Training Epoch: 14 [18688/50048]	Loss: 1.0893
Training Epoch: 14 [18816/50048]	Loss: 1.1361
Training Epoch: 14 [18944/50048]	Loss: 1.2618
Training Epoch: 14 [19072/50048]	Loss: 1.2816
Training Epoch: 14 [19200/50048]	Loss: 1.5320
Training Epoch: 14 [19328/50048]	Loss: 1.2957
Training Epoch: 14 [19456/50048]	Loss: 1.2068
Training Epoch: 14 [19584/50048]	Loss: 1.3960
Training Epoch: 14 [19712/50048]	Loss: 1.5340
Training Epoch: 14 [19840/50048]	Loss: 1.1975
Training Epoch: 14 [19968/50048]	Loss: 1.2537
Training Epoch: 14 [20096/50048]	Loss: 1.2609
Training Epoch: 14 [20224/50048]	Loss: 1.3032
Training Epoch: 14 [20352/50048]	Loss: 1.0985
Training Epoch: 14 [20480/50048]	Loss: 1.1183
Training Epoch: 14 [20608/50048]	Loss: 1.4568
Training Epoch: 14 [20736/50048]	Loss: 1.1775
Training Epoch: 14 [20864/50048]	Loss: 1.4396
Training Epoch: 14 [20992/50048]	Loss: 1.6462
Training Epoch: 14 [21120/50048]	Loss: 1.1910
Training Epoch: 14 [21248/50048]	Loss: 1.1104
Training Epoch: 14 [21376/50048]	Loss: 1.2281
Training Epoch: 14 [21504/50048]	Loss: 1.4003
Training Epoch: 14 [21632/50048]	Loss: 1.2587
Training Epoch: 14 [21760/50048]	Loss: 1.0381
Training Epoch: 14 [21888/50048]	Loss: 1.6103
Training Epoch: 14 [22016/50048]	Loss: 1.2296
Training Epoch: 14 [22144/50048]	Loss: 1.2712
Training Epoch: 14 [22272/50048]	Loss: 1.2414
Training Epoch: 14 [22400/50048]	Loss: 1.4815
Training Epoch: 14 [22528/50048]	Loss: 1.2026
Training Epoch: 14 [22656/50048]	Loss: 1.1729
Training Epoch: 14 [22784/50048]	Loss: 1.2089
Training Epoch: 14 [22912/50048]	Loss: 1.2709
Training Epoch: 14 [23040/50048]	Loss: 1.3361
Training Epoch: 14 [23168/50048]	Loss: 1.4408
Training Epoch: 14 [23296/50048]	Loss: 1.3265
Training Epoch: 14 [23424/50048]	Loss: 1.3027
Training Epoch: 14 [23552/50048]	Loss: 1.3207
Training Epoch: 14 [23680/50048]	Loss: 1.2838
Training Epoch: 14 [23808/50048]	Loss: 1.5382
Training Epoch: 14 [23936/50048]	Loss: 1.2278
Training Epoch: 14 [24064/50048]	Loss: 1.1333
Training Epoch: 14 [24192/50048]	Loss: 1.1058
Training Epoch: 14 [24320/50048]	Loss: 1.4934
Training Epoch: 14 [24448/50048]	Loss: 1.2409
Training Epoch: 14 [24576/50048]	Loss: 1.4462
Training Epoch: 14 [24704/50048]	Loss: 1.5480
Training Epoch: 14 [24832/50048]	Loss: 1.1953
Training Epoch: 14 [24960/50048]	Loss: 1.5269
Training Epoch: 14 [25088/50048]	Loss: 1.5125
Training Epoch: 14 [25216/50048]	Loss: 1.3407
Training Epoch: 14 [25344/50048]	Loss: 1.3306
Training Epoch: 14 [25472/50048]	Loss: 1.1612
Training Epoch: 14 [25600/50048]	Loss: 1.2737
Training Epoch: 14 [25728/50048]	Loss: 1.3419
Training Epoch: 14 [25856/50048]	Loss: 1.2601
Training Epoch: 14 [25984/50048]	Loss: 1.3607
Training Epoch: 14 [26112/50048]	Loss: 1.3038
Training Epoch: 14 [26240/50048]	Loss: 1.3379
Training Epoch: 14 [26368/50048]	Loss: 1.1388
Training Epoch: 14 [26496/50048]	Loss: 1.0039
Training Epoch: 14 [26624/50048]	Loss: 1.5237
Training Epoch: 14 [26752/50048]	Loss: 1.3592
Training Epoch: 14 [26880/50048]	Loss: 1.3174
Training Epoch: 14 [27008/50048]	Loss: 1.3501
Training Epoch: 14 [27136/50048]	Loss: 1.2253
Training Epoch: 14 [27264/50048]	Loss: 0.9704
Training Epoch: 14 [27392/50048]	Loss: 1.0023
Training Epoch: 14 [27520/50048]	Loss: 1.3181
Training Epoch: 14 [27648/50048]	Loss: 1.1512
Training Epoch: 14 [27776/50048]	Loss: 1.2488
Training Epoch: 14 [27904/50048]	Loss: 0.9922
Training Epoch: 14 [28032/50048]	Loss: 1.2810
Training Epoch: 14 [28160/50048]	Loss: 1.5059
Training Epoch: 14 [28288/50048]	Loss: 1.1993
Training Epoch: 14 [28416/50048]	Loss: 1.1232
Training Epoch: 14 [28544/50048]	Loss: 1.6596
Training Epoch: 14 [28672/50048]	Loss: 1.2201
Training Epoch: 14 [28800/50048]	Loss: 1.2315
Training Epoch: 14 [28928/50048]	Loss: 1.1728
Training Epoch: 14 [29056/50048]	Loss: 1.3289
Training Epoch: 14 [29184/50048]	Loss: 1.1308
Training Epoch: 14 [29312/50048]	Loss: 1.2236
Training Epoch: 14 [29440/50048]	Loss: 1.2282
Training Epoch: 14 [29568/50048]	Loss: 1.4028
Training Epoch: 14 [29696/50048]	Loss: 1.1169
Training Epoch: 14 [29824/50048]	Loss: 1.3763
Training Epoch: 14 [29952/50048]	Loss: 1.1411
Training Epoch: 14 [30080/50048]	Loss: 1.1468
Training Epoch: 14 [30208/50048]	Loss: 1.2015
Training Epoch: 14 [30336/50048]	Loss: 1.2478
Training Epoch: 14 [30464/50048]	Loss: 1.3315
Training Epoch: 14 [30592/50048]	Loss: 1.3210
Training Epoch: 14 [30720/50048]	Loss: 1.4591
Training Epoch: 14 [30848/50048]	Loss: 1.1970
Training Epoch: 14 [30976/50048]	Loss: 1.1812
Training Epoch: 14 [31104/50048]	Loss: 1.3579
Training Epoch: 14 [31232/50048]	Loss: 1.3161
Training Epoch: 14 [31360/50048]	Loss: 1.4846
Training Epoch: 14 [31488/50048]	Loss: 1.2171
Training Epoch: 14 [31616/50048]	Loss: 1.2914
Training Epoch: 14 [31744/50048]	Loss: 1.1632
Training Epoch: 14 [31872/50048]	Loss: 1.2728
Training Epoch: 14 [32000/50048]	Loss: 1.2882
Training Epoch: 14 [32128/50048]	Loss: 1.3766
Training Epoch: 14 [32256/50048]	Loss: 1.4573
Training Epoch: 14 [32384/50048]	Loss: 1.4323
Training Epoch: 14 [32512/50048]	Loss: 1.3525
Training Epoch: 14 [32640/50048]	Loss: 1.4033
Training Epoch: 14 [32768/50048]	Loss: 1.4670
Training Epoch: 14 [32896/50048]	Loss: 1.1955
Training Epoch: 14 [33024/50048]	Loss: 1.2548
Training Epoch: 14 [33152/50048]	Loss: 1.4781
Training Epoch: 14 [33280/50048]	Loss: 1.4333
Training Epoch: 14 [33408/50048]	Loss: 1.2844
Training Epoch: 14 [33536/50048]	Loss: 1.4451
Training Epoch: 14 [33664/50048]	Loss: 1.3311
Training Epoch: 14 [33792/50048]	Loss: 1.2765
Training Epoch: 14 [33920/50048]	Loss: 1.2954
Training Epoch: 14 [34048/50048]	Loss: 1.3306
Training Epoch: 14 [34176/50048]	Loss: 1.2121
Training Epoch: 14 [34304/50048]	Loss: 1.2810
Training Epoch: 14 [34432/50048]	Loss: 1.5127
Training Epoch: 14 [34560/50048]	Loss: 1.2246
Training Epoch: 14 [34688/50048]	Loss: 1.2815
Training Epoch: 14 [34816/50048]	Loss: 1.5065
Training Epoch: 14 [34944/50048]	Loss: 1.4970
Training Epoch: 14 [35072/50048]	Loss: 1.3127
Training Epoch: 14 [35200/50048]	Loss: 1.2623
Training Epoch: 14 [35328/50048]	Loss: 1.4687
Training Epoch: 14 [35456/50048]	Loss: 1.4595
Training Epoch: 14 [35584/50048]	Loss: 1.2171
Training Epoch: 14 [35712/50048]	Loss: 0.9899
Training Epoch: 14 [35840/50048]	Loss: 1.3923
Training Epoch: 14 [35968/50048]	Loss: 1.2542
Training Epoch: 14 [36096/50048]	Loss: 1.3647
Training Epoch: 14 [36224/50048]	Loss: 1.1988
Training Epoch: 14 [36352/50048]	Loss: 1.2911
Training Epoch: 14 [36480/50048]	Loss: 1.4734
Training Epoch: 14 [36608/50048]	Loss: 1.3864
Training Epoch: 14 [36736/50048]	Loss: 1.2263
Training Epoch: 14 [36864/50048]	Loss: 1.4621
Training Epoch: 14 [36992/50048]	Loss: 1.2808
Training Epoch: 14 [37120/50048]	Loss: 1.2822
Training Epoch: 14 [37248/50048]	Loss: 1.4137
Training Epoch: 14 [37376/50048]	Loss: 1.3265
Training Epoch: 14 [37504/50048]	Loss: 1.3154
Training Epoch: 14 [37632/50048]	Loss: 1.0423
Training Epoch: 14 [37760/50048]	Loss: 1.1373
Training Epoch: 14 [37888/50048]	Loss: 1.0854
Training Epoch: 14 [38016/50048]	Loss: 1.2862
Training Epoch: 14 [38144/50048]	Loss: 1.2252
Training Epoch: 14 [38272/50048]	Loss: 1.3240
Training Epoch: 14 [38400/50048]	Loss: 1.3330
Training Epoch: 14 [38528/50048]	Loss: 1.3454
Training Epoch: 14 [38656/50048]	Loss: 1.3004
Training Epoch: 14 [38784/50048]	Loss: 1.3065
Training Epoch: 14 [38912/50048]	Loss: 1.4460
Training Epoch: 14 [39040/50048]	Loss: 1.1866
Training Epoch: 14 [39168/50048]	Loss: 1.1887
Training Epoch: 14 [39296/50048]	Loss: 1.2667
Training Epoch: 14 [39424/50048]	Loss: 1.2741
Training Epoch: 14 [39552/50048]	Loss: 1.2882
Training Epoch: 14 [39680/50048]	Loss: 1.4009
Training Epoch: 14 [39808/50048]	Loss: 1.2197
Training Epoch: 14 [39936/50048]	Loss: 1.3092
Training Epoch: 14 [40064/50048]	Loss: 1.4178
Training Epoch: 14 [40192/50048]	Loss: 1.3564
Training Epoch: 14 [40320/50048]	Loss: 1.3517
Training Epoch: 14 [40448/50048]	Loss: 1.1688
Training Epoch: 14 [40576/50048]	Loss: 1.4208
Training Epoch: 14 [40704/50048]	Loss: 1.1933
Training Epoch: 14 [40832/50048]	Loss: 1.3370
Training Epoch: 14 [40960/50048]	Loss: 1.1520
Training Epoch: 14 [41088/50048]	Loss: 1.2370
Training Epoch: 14 [41216/50048]	Loss: 1.3014
Training Epoch: 14 [41344/50048]	Loss: 1.3209
Training Epoch: 14 [41472/50048]	Loss: 1.3695
Training Epoch: 14 [41600/50048]	Loss: 1.1855
Training Epoch: 14 [41728/50048]	Loss: 1.3398
Training Epoch: 14 [41856/50048]	Loss: 1.4047
Training Epoch: 14 [41984/50048]	Loss: 1.3838
Training Epoch: 14 [42112/50048]	Loss: 1.1136
Training Epoch: 14 [42240/50048]	Loss: 1.4306
Training Epoch: 14 [42368/50048]	Loss: 1.4625
Training Epoch: 14 [42496/50048]	Loss: 1.4704
Training Epoch: 14 [42624/50048]	Loss: 1.1899
Training Epoch: 14 [42752/50048]	Loss: 1.2234
Training Epoch: 14 [42880/50048]	Loss: 1.4585
Training Epoch: 14 [43008/50048]	Loss: 1.3211
Training Epoch: 14 [43136/50048]	Loss: 1.4299
Training Epoch: 14 [43264/50048]	Loss: 1.1504
Training Epoch: 14 [43392/50048]	Loss: 1.2814
Training Epoch: 14 [43520/50048]	Loss: 1.2842
Training Epoch: 14 [43648/50048]	Loss: 1.4127
Training Epoch: 14 [43776/50048]	Loss: 1.2248
Training Epoch: 14 [43904/50048]	Loss: 1.4673
Training Epoch: 14 [44032/50048]	Loss: 1.1399
Training Epoch: 14 [44160/50048]	Loss: 1.3692
Training Epoch: 14 [44288/50048]	Loss: 1.5280
Training Epoch: 14 [44416/50048]	Loss: 1.0912
Training Epoch: 14 [44544/50048]	Loss: 1.2653
Training Epoch: 14 [44672/50048]	Loss: 1.1390
Training Epoch: 14 [44800/50048]	Loss: 1.3756
Training Epoch: 14 [44928/50048]	Loss: 1.1534
Training Epoch: 14 [45056/50048]	Loss: 1.4723
Training Epoch: 14 [45184/50048]	Loss: 1.2869
Training Epoch: 14 [45312/50048]	Loss: 1.3229
Training Epoch: 14 [45440/50048]	Loss: 1.5744
Training Epoch: 14 [45568/50048]	Loss: 1.3705
Training Epoch: 14 [45696/50048]	Loss: 1.1526
2022-12-06 03:59:54,522 [ZeusDataLoader(train)] train epoch 15 done: time=86.34 energy=10501.19
2022-12-06 03:59:54,523 [ZeusDataLoader(eval)] Epoch 15 begin.
Training Epoch: 14 [45824/50048]	Loss: 1.3801
Training Epoch: 14 [45952/50048]	Loss: 1.4007
Training Epoch: 14 [46080/50048]	Loss: 1.2249
Training Epoch: 14 [46208/50048]	Loss: 1.5120
Training Epoch: 14 [46336/50048]	Loss: 1.2935
Training Epoch: 14 [46464/50048]	Loss: 1.2847
Training Epoch: 14 [46592/50048]	Loss: 1.1224
Training Epoch: 14 [46720/50048]	Loss: 1.3581
Training Epoch: 14 [46848/50048]	Loss: 1.4229
Training Epoch: 14 [46976/50048]	Loss: 1.2305
Training Epoch: 14 [47104/50048]	Loss: 1.0586
Training Epoch: 14 [47232/50048]	Loss: 1.3423
Training Epoch: 14 [47360/50048]	Loss: 1.1899
Training Epoch: 14 [47488/50048]	Loss: 1.2597
Training Epoch: 14 [47616/50048]	Loss: 1.4446
Training Epoch: 14 [47744/50048]	Loss: 1.2713
Training Epoch: 14 [47872/50048]	Loss: 1.4589
Training Epoch: 14 [48000/50048]	Loss: 1.1750
Training Epoch: 14 [48128/50048]	Loss: 1.2791
Training Epoch: 14 [48256/50048]	Loss: 1.4371
Training Epoch: 14 [48384/50048]	Loss: 1.5670
Training Epoch: 14 [48512/50048]	Loss: 1.1170
Training Epoch: 14 [48640/50048]	Loss: 1.3683
Training Epoch: 14 [48768/50048]	Loss: 1.3465
Training Epoch: 14 [48896/50048]	Loss: 1.1285
Training Epoch: 14 [49024/50048]	Loss: 1.1737
Training Epoch: 14 [49152/50048]	Loss: 1.4509
Training Epoch: 14 [49280/50048]	Loss: 1.0982
Training Epoch: 14 [49408/50048]	Loss: 1.4342
Training Epoch: 14 [49536/50048]	Loss: 1.2910
Training Epoch: 14 [49664/50048]	Loss: 1.5680
Training Epoch: 14 [49792/50048]	Loss: 1.1065
Training Epoch: 14 [49920/50048]	Loss: 1.3320
Training Epoch: 14 [50048/50048]	Loss: 1.4096
2022-12-06 08:59:58.279 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 03:59:58,305 [ZeusDataLoader(eval)] eval epoch 15 done: time=3.77 energy=452.96
2022-12-06 03:59:58,305 [ZeusDataLoader(train)] Up to epoch 15: time=1355.76, energy=164235.35, cost=200746.79
2022-12-06 03:59:58,305 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 03:59:58,306 [ZeusDataLoader(train)] Expected next epoch: time=1445.56, energy=175033.36, cost=214003.17
2022-12-06 03:59:58,306 [ZeusDataLoader(train)] Epoch 16 begin.
Validation Epoch: 14, Average loss: 0.0123, Accuracy: 0.5698
2022-12-06 03:59:58,480 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 03:59:58,481 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 08:59:58.482 [ZeusMonitor] Monitor started.
2022-12-06 08:59:58.482 [ZeusMonitor] Running indefinitely. 2022-12-06 08:59:58.482 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 08:59:58.483 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e16+gpu0.power.log
Training Epoch: 15 [128/50048]	Loss: 1.4049
Training Epoch: 15 [256/50048]	Loss: 1.4371
Training Epoch: 15 [384/50048]	Loss: 1.4139
Training Epoch: 15 [512/50048]	Loss: 1.3141
Training Epoch: 15 [640/50048]	Loss: 1.1295
Training Epoch: 15 [768/50048]	Loss: 1.2649
Training Epoch: 15 [896/50048]	Loss: 1.0286
Training Epoch: 15 [1024/50048]	Loss: 1.0678
Training Epoch: 15 [1152/50048]	Loss: 1.3770
Training Epoch: 15 [1280/50048]	Loss: 1.2031
Training Epoch: 15 [1408/50048]	Loss: 1.1112
Training Epoch: 15 [1536/50048]	Loss: 1.1233
Training Epoch: 15 [1664/50048]	Loss: 1.0904
Training Epoch: 15 [1792/50048]	Loss: 1.2722
Training Epoch: 15 [1920/50048]	Loss: 1.0374
Training Epoch: 15 [2048/50048]	Loss: 1.4427
Training Epoch: 15 [2176/50048]	Loss: 1.4068
Training Epoch: 15 [2304/50048]	Loss: 1.1294
Training Epoch: 15 [2432/50048]	Loss: 1.4190
Training Epoch: 15 [2560/50048]	Loss: 1.1069
Training Epoch: 15 [2688/50048]	Loss: 1.3406
Training Epoch: 15 [2816/50048]	Loss: 1.1374
Training Epoch: 15 [2944/50048]	Loss: 1.1932
Training Epoch: 15 [3072/50048]	Loss: 1.0360
Training Epoch: 15 [3200/50048]	Loss: 1.3445
Training Epoch: 15 [3328/50048]	Loss: 1.1921
Training Epoch: 15 [3456/50048]	Loss: 1.2824
Training Epoch: 15 [3584/50048]	Loss: 1.0625
Training Epoch: 15 [3712/50048]	Loss: 1.1093
Training Epoch: 15 [3840/50048]	Loss: 1.4237
Training Epoch: 15 [3968/50048]	Loss: 1.3098
Training Epoch: 15 [4096/50048]	Loss: 1.0723
Training Epoch: 15 [4224/50048]	Loss: 0.9604
Training Epoch: 15 [4352/50048]	Loss: 1.1451
Training Epoch: 15 [4480/50048]	Loss: 1.1885
Training Epoch: 15 [4608/50048]	Loss: 1.1895
Training Epoch: 15 [4736/50048]	Loss: 1.1295
Training Epoch: 15 [4864/50048]	Loss: 1.2097
Training Epoch: 15 [4992/50048]	Loss: 1.2998
Training Epoch: 15 [5120/50048]	Loss: 1.2881
Training Epoch: 15 [5248/50048]	Loss: 1.0735
Training Epoch: 15 [5376/50048]	Loss: 1.6183
Training Epoch: 15 [5504/50048]	Loss: 1.2788
Training Epoch: 15 [5632/50048]	Loss: 1.2195
Training Epoch: 15 [5760/50048]	Loss: 0.9899
Training Epoch: 15 [5888/50048]	Loss: 1.1107
Training Epoch: 15 [6016/50048]	Loss: 1.1991
Training Epoch: 15 [6144/50048]	Loss: 1.2093
Training Epoch: 15 [6272/50048]	Loss: 1.3284
Training Epoch: 15 [6400/50048]	Loss: 1.3009
Training Epoch: 15 [6528/50048]	Loss: 1.2442
Training Epoch: 15 [6656/50048]	Loss: 1.0881
Training Epoch: 15 [6784/50048]	Loss: 1.1201
Training Epoch: 15 [6912/50048]	Loss: 1.2629
Training Epoch: 15 [7040/50048]	Loss: 1.1967
Training Epoch: 15 [7168/50048]	Loss: 1.2094
Training Epoch: 15 [7296/50048]	Loss: 1.2821
Training Epoch: 15 [7424/50048]	Loss: 1.1745
Training Epoch: 15 [7552/50048]	Loss: 1.3744
Training Epoch: 15 [7680/50048]	Loss: 1.1132
Training Epoch: 15 [7808/50048]	Loss: 1.1282
Training Epoch: 15 [7936/50048]	Loss: 1.2734
Training Epoch: 15 [8064/50048]	Loss: 1.2649
Training Epoch: 15 [8192/50048]	Loss: 1.2340
Training Epoch: 15 [8320/50048]	Loss: 1.3912
Training Epoch: 15 [8448/50048]	Loss: 1.2261
Training Epoch: 15 [8576/50048]	Loss: 1.1616
Training Epoch: 15 [8704/50048]	Loss: 1.2049
Training Epoch: 15 [8832/50048]	Loss: 1.3143
Training Epoch: 15 [8960/50048]	Loss: 1.1769
Training Epoch: 15 [9088/50048]	Loss: 1.0886
Training Epoch: 15 [9216/50048]	Loss: 1.1028
Training Epoch: 15 [9344/50048]	Loss: 1.2749
Training Epoch: 15 [9472/50048]	Loss: 1.1900
Training Epoch: 15 [9600/50048]	Loss: 1.0643
Training Epoch: 15 [9728/50048]	Loss: 1.0782
Training Epoch: 15 [9856/50048]	Loss: 1.2172
Training Epoch: 15 [9984/50048]	Loss: 1.1708
Training Epoch: 15 [10112/50048]	Loss: 1.1744
Training Epoch: 15 [10240/50048]	Loss: 1.0886
Training Epoch: 15 [10368/50048]	Loss: 1.1014
Training Epoch: 15 [10496/50048]	Loss: 1.3413
Training Epoch: 15 [10624/50048]	Loss: 1.1542
Training Epoch: 15 [10752/50048]	Loss: 1.1869
Training Epoch: 15 [10880/50048]	Loss: 1.3089
Training Epoch: 15 [11008/50048]	Loss: 1.0911
Training Epoch: 15 [11136/50048]	Loss: 1.1015
Training Epoch: 15 [11264/50048]	Loss: 1.1994
Training Epoch: 15 [11392/50048]	Loss: 1.3703
Training Epoch: 15 [11520/50048]	Loss: 1.1243
Training Epoch: 15 [11648/50048]	Loss: 1.2393
Training Epoch: 15 [11776/50048]	Loss: 1.3401
Training Epoch: 15 [11904/50048]	Loss: 1.1141
Training Epoch: 15 [12032/50048]	Loss: 1.2071
Training Epoch: 15 [12160/50048]	Loss: 1.2682
Training Epoch: 15 [12288/50048]	Loss: 1.2707
Training Epoch: 15 [12416/50048]	Loss: 1.2943
Training Epoch: 15 [12544/50048]	Loss: 1.2118
Training Epoch: 15 [12672/50048]	Loss: 1.3150
Training Epoch: 15 [12800/50048]	Loss: 0.9767
Training Epoch: 15 [12928/50048]	Loss: 1.2404
Training Epoch: 15 [13056/50048]	Loss: 1.1320
Training Epoch: 15 [13184/50048]	Loss: 1.3042
Training Epoch: 15 [13312/50048]	Loss: 1.0022
Training Epoch: 15 [13440/50048]	Loss: 1.1996
Training Epoch: 15 [13568/50048]	Loss: 1.2127
Training Epoch: 15 [13696/50048]	Loss: 1.3797
Training Epoch: 15 [13824/50048]	Loss: 1.2975
Training Epoch: 15 [13952/50048]	Loss: 1.1928
Training Epoch: 15 [14080/50048]	Loss: 1.5031
Training Epoch: 15 [14208/50048]	Loss: 1.3055
Training Epoch: 15 [14336/50048]	Loss: 1.2160
Training Epoch: 15 [14464/50048]	Loss: 1.3574
Training Epoch: 15 [14592/50048]	Loss: 1.4987
Training Epoch: 15 [14720/50048]	Loss: 1.1066
Training Epoch: 15 [14848/50048]	Loss: 1.4064
Training Epoch: 15 [14976/50048]	Loss: 1.4197
Training Epoch: 15 [15104/50048]	Loss: 1.2278
Training Epoch: 15 [15232/50048]	Loss: 1.2527
Training Epoch: 15 [15360/50048]	Loss: 1.2253
Training Epoch: 15 [15488/50048]	Loss: 1.2451
Training Epoch: 15 [15616/50048]	Loss: 1.3386
Training Epoch: 15 [15744/50048]	Loss: 1.2512
Training Epoch: 15 [15872/50048]	Loss: 1.1107
Training Epoch: 15 [16000/50048]	Loss: 1.3096
Training Epoch: 15 [16128/50048]	Loss: 1.4404
Training Epoch: 15 [16256/50048]	Loss: 1.2443
Training Epoch: 15 [16384/50048]	Loss: 1.1859
Training Epoch: 15 [16512/50048]	Loss: 1.2805
Training Epoch: 15 [16640/50048]	Loss: 1.1726
Training Epoch: 15 [16768/50048]	Loss: 1.2071
Training Epoch: 15 [16896/50048]	Loss: 1.2500
Training Epoch: 15 [17024/50048]	Loss: 1.2198
Training Epoch: 15 [17152/50048]	Loss: 1.2293
Training Epoch: 15 [17280/50048]	Loss: 1.2979
Training Epoch: 15 [17408/50048]	Loss: 1.2215
Training Epoch: 15 [17536/50048]	Loss: 1.2827
Training Epoch: 15 [17664/50048]	Loss: 1.3414
Training Epoch: 15 [17792/50048]	Loss: 1.3378
Training Epoch: 15 [17920/50048]	Loss: 1.1799
Training Epoch: 15 [18048/50048]	Loss: 1.0732
Training Epoch: 15 [18176/50048]	Loss: 1.2082
Training Epoch: 15 [18304/50048]	Loss: 1.3227
Training Epoch: 15 [18432/50048]	Loss: 0.9886
Training Epoch: 15 [18560/50048]	Loss: 1.2128
Training Epoch: 15 [18688/50048]	Loss: 1.2598
Training Epoch: 15 [18816/50048]	Loss: 1.2576
Training Epoch: 15 [18944/50048]	Loss: 1.2389
Training Epoch: 15 [19072/50048]	Loss: 1.1973
Training Epoch: 15 [19200/50048]	Loss: 1.1974
Training Epoch: 15 [19328/50048]	Loss: 1.2147
Training Epoch: 15 [19456/50048]	Loss: 1.1574
Training Epoch: 15 [19584/50048]	Loss: 1.2335
Training Epoch: 15 [19712/50048]	Loss: 1.2527
Training Epoch: 15 [19840/50048]	Loss: 1.2299
Training Epoch: 15 [19968/50048]	Loss: 1.1576
Training Epoch: 15 [20096/50048]	Loss: 1.1816
Training Epoch: 15 [20224/50048]	Loss: 1.3777
Training Epoch: 15 [20352/50048]	Loss: 1.0442
Training Epoch: 15 [20480/50048]	Loss: 1.2314
Training Epoch: 15 [20608/50048]	Loss: 1.0948
Training Epoch: 15 [20736/50048]	Loss: 1.1720
Training Epoch: 15 [20864/50048]	Loss: 1.1613
Training Epoch: 15 [20992/50048]	Loss: 1.2305
Training Epoch: 15 [21120/50048]	Loss: 1.2188
Training Epoch: 15 [21248/50048]	Loss: 1.3818
Training Epoch: 15 [21376/50048]	Loss: 1.1180
Training Epoch: 15 [21504/50048]	Loss: 1.0829
Training Epoch: 15 [21632/50048]	Loss: 1.1833
Training Epoch: 15 [21760/50048]	Loss: 1.3328
Training Epoch: 15 [21888/50048]	Loss: 1.2990
Training Epoch: 15 [22016/50048]	Loss: 1.3781
Training Epoch: 15 [22144/50048]	Loss: 1.1906
Training Epoch: 15 [22272/50048]	Loss: 1.2644
Training Epoch: 15 [22400/50048]	Loss: 1.0415
Training Epoch: 15 [22528/50048]	Loss: 1.2602
Training Epoch: 15 [22656/50048]	Loss: 1.1844
Training Epoch: 15 [22784/50048]	Loss: 1.3478
Training Epoch: 15 [22912/50048]	Loss: 1.4175
Training Epoch: 15 [23040/50048]	Loss: 1.3403
Training Epoch: 15 [23168/50048]	Loss: 1.3529
Training Epoch: 15 [23296/50048]	Loss: 1.0908
Training Epoch: 15 [23424/50048]	Loss: 1.3732
Training Epoch: 15 [23552/50048]	Loss: 1.1573
Training Epoch: 15 [23680/50048]	Loss: 1.2315
Training Epoch: 15 [23808/50048]	Loss: 1.2063
Training Epoch: 15 [23936/50048]	Loss: 1.1631
Training Epoch: 15 [24064/50048]	Loss: 1.1802
Training Epoch: 15 [24192/50048]	Loss: 1.2251
Training Epoch: 15 [24320/50048]	Loss: 1.3837
Training Epoch: 15 [24448/50048]	Loss: 1.2142
Training Epoch: 15 [24576/50048]	Loss: 1.1726
Training Epoch: 15 [24704/50048]	Loss: 1.2509
Training Epoch: 15 [24832/50048]	Loss: 1.5493
Training Epoch: 15 [24960/50048]	Loss: 1.2067
Training Epoch: 15 [25088/50048]	Loss: 1.0428
Training Epoch: 15 [25216/50048]	Loss: 1.1845
Training Epoch: 15 [25344/50048]	Loss: 1.1232
Training Epoch: 15 [25472/50048]	Loss: 1.2223
Training Epoch: 15 [25600/50048]	Loss: 1.3857
Training Epoch: 15 [25728/50048]	Loss: 1.2432
Training Epoch: 15 [25856/50048]	Loss: 1.2080
Training Epoch: 15 [25984/50048]	Loss: 1.0669
Training Epoch: 15 [26112/50048]	Loss: 1.2131
Training Epoch: 15 [26240/50048]	Loss: 1.1250
Training Epoch: 15 [26368/50048]	Loss: 1.2665
Training Epoch: 15 [26496/50048]	Loss: 1.3573
Training Epoch: 15 [26624/50048]	Loss: 1.0666
Training Epoch: 15 [26752/50048]	Loss: 1.3384
Training Epoch: 15 [26880/50048]	Loss: 1.4127
Training Epoch: 15 [27008/50048]	Loss: 1.1591
Training Epoch: 15 [27136/50048]	Loss: 1.2710
Training Epoch: 15 [27264/50048]	Loss: 1.3205
Training Epoch: 15 [27392/50048]	Loss: 1.2729
Training Epoch: 15 [27520/50048]	Loss: 1.4015
Training Epoch: 15 [27648/50048]	Loss: 1.2100
Training Epoch: 15 [27776/50048]	Loss: 1.1274
Training Epoch: 15 [27904/50048]	Loss: 0.9670
Training Epoch: 15 [28032/50048]	Loss: 1.3481
Training Epoch: 15 [28160/50048]	Loss: 1.3313
Training Epoch: 15 [28288/50048]	Loss: 1.0965
Training Epoch: 15 [28416/50048]	Loss: 0.8984
Training Epoch: 15 [28544/50048]	Loss: 1.1755
Training Epoch: 15 [28672/50048]	Loss: 1.1548
Training Epoch: 15 [28800/50048]	Loss: 1.1416
Training Epoch: 15 [28928/50048]	Loss: 1.1825
Training Epoch: 15 [29056/50048]	Loss: 1.2087
Training Epoch: 15 [29184/50048]	Loss: 1.2227
Training Epoch: 15 [29312/50048]	Loss: 1.7395
Training Epoch: 15 [29440/50048]	Loss: 1.2811
Training Epoch: 15 [29568/50048]	Loss: 1.3208
Training Epoch: 15 [29696/50048]	Loss: 1.2819
Training Epoch: 15 [29824/50048]	Loss: 1.0744
Training Epoch: 15 [29952/50048]	Loss: 1.3296
Training Epoch: 15 [30080/50048]	Loss: 1.2062
Training Epoch: 15 [30208/50048]	Loss: 1.0326
Training Epoch: 15 [30336/50048]	Loss: 1.2557
Training Epoch: 15 [30464/50048]	Loss: 1.1018
Training Epoch: 15 [30592/50048]	Loss: 1.4012
Training Epoch: 15 [30720/50048]	Loss: 1.2054
Training Epoch: 15 [30848/50048]	Loss: 1.1814
Training Epoch: 15 [30976/50048]	Loss: 1.0915
Training Epoch: 15 [31104/50048]	Loss: 1.1494
Training Epoch: 15 [31232/50048]	Loss: 1.0762
Training Epoch: 15 [31360/50048]	Loss: 1.3475
Training Epoch: 15 [31488/50048]	Loss: 1.2785
Training Epoch: 15 [31616/50048]	Loss: 1.2398
Training Epoch: 15 [31744/50048]	Loss: 1.1442
Training Epoch: 15 [31872/50048]	Loss: 1.3334
Training Epoch: 15 [32000/50048]	Loss: 1.2954
Training Epoch: 15 [32128/50048]	Loss: 1.1178
Training Epoch: 15 [32256/50048]	Loss: 1.2923
Training Epoch: 15 [32384/50048]	Loss: 1.1124
Training Epoch: 15 [32512/50048]	Loss: 1.1554
Training Epoch: 15 [32640/50048]	Loss: 1.3197
Training Epoch: 15 [32768/50048]	Loss: 1.1788
Training Epoch: 15 [32896/50048]	Loss: 1.2288
Training Epoch: 15 [33024/50048]	Loss: 1.3003
Training Epoch: 15 [33152/50048]	Loss: 1.3146
Training Epoch: 15 [33280/50048]	Loss: 1.1000
Training Epoch: 15 [33408/50048]	Loss: 1.2283
Training Epoch: 15 [33536/50048]	Loss: 1.2041
Training Epoch: 15 [33664/50048]	Loss: 1.3038
Training Epoch: 15 [33792/50048]	Loss: 1.1295
Training Epoch: 15 [33920/50048]	Loss: 1.2464
Training Epoch: 15 [34048/50048]	Loss: 1.4016
Training Epoch: 15 [34176/50048]	Loss: 1.2695
Training Epoch: 15 [34304/50048]	Loss: 1.2007
Training Epoch: 15 [34432/50048]	Loss: 1.4407
Training Epoch: 15 [34560/50048]	Loss: 1.1218
Training Epoch: 15 [34688/50048]	Loss: 1.1983
Training Epoch: 15 [34816/50048]	Loss: 1.2215
Training Epoch: 15 [34944/50048]	Loss: 1.2277
Training Epoch: 15 [35072/50048]	Loss: 1.1737
Training Epoch: 15 [35200/50048]	Loss: 1.2412
Training Epoch: 15 [35328/50048]	Loss: 1.3548
Training Epoch: 15 [35456/50048]	Loss: 1.1834
Training Epoch: 15 [35584/50048]	Loss: 1.1403
Training Epoch: 15 [35712/50048]	Loss: 1.1786
Training Epoch: 15 [35840/50048]	Loss: 1.0864
Training Epoch: 15 [35968/50048]	Loss: 1.2461
Training Epoch: 15 [36096/50048]	Loss: 1.1512
Training Epoch: 15 [36224/50048]	Loss: 1.2702
Training Epoch: 15 [36352/50048]	Loss: 0.9232
Training Epoch: 15 [36480/50048]	Loss: 1.1482
Training Epoch: 15 [36608/50048]	Loss: 1.2597
Training Epoch: 15 [36736/50048]	Loss: 1.2819
Training Epoch: 15 [36864/50048]	Loss: 1.3851
Training Epoch: 15 [36992/50048]	Loss: 1.2754
Training Epoch: 15 [37120/50048]	Loss: 1.3624
Training Epoch: 15 [37248/50048]	Loss: 1.2170
Training Epoch: 15 [37376/50048]	Loss: 1.3313
Training Epoch: 15 [37504/50048]	Loss: 1.1186
Training Epoch: 15 [37632/50048]	Loss: 1.3925
Training Epoch: 15 [37760/50048]	Loss: 1.4053
Training Epoch: 15 [37888/50048]	Loss: 1.0457
Training Epoch: 15 [38016/50048]	Loss: 1.2407
Training Epoch: 15 [38144/50048]	Loss: 1.0984
Training Epoch: 15 [38272/50048]	Loss: 1.0786
Training Epoch: 15 [38400/50048]	Loss: 1.2478
Training Epoch: 15 [38528/50048]	Loss: 1.4288
Training Epoch: 15 [38656/50048]	Loss: 1.3498
Training Epoch: 15 [38784/50048]	Loss: 1.0217
Training Epoch: 15 [38912/50048]	Loss: 1.2503
Training Epoch: 15 [39040/50048]	Loss: 1.2267
Training Epoch: 15 [39168/50048]	Loss: 1.1476
Training Epoch: 15 [39296/50048]	Loss: 1.1885
Training Epoch: 15 [39424/50048]	Loss: 1.2129
Training Epoch: 15 [39552/50048]	Loss: 1.3367
Training Epoch: 15 [39680/50048]	Loss: 1.2617
Training Epoch: 15 [39808/50048]	Loss: 1.1258
Training Epoch: 15 [39936/50048]	Loss: 1.6245
Training Epoch: 15 [40064/50048]	Loss: 1.3874
Training Epoch: 15 [40192/50048]	Loss: 1.2290
Training Epoch: 15 [40320/50048]	Loss: 1.2876
Training Epoch: 15 [40448/50048]	Loss: 1.2081
Training Epoch: 15 [40576/50048]	Loss: 1.1262
Training Epoch: 15 [40704/50048]	Loss: 1.1880
Training Epoch: 15 [40832/50048]	Loss: 1.3404
Training Epoch: 15 [40960/50048]	Loss: 1.0497
Training Epoch: 15 [41088/50048]	Loss: 1.2844
Training Epoch: 15 [41216/50048]	Loss: 1.3259
Training Epoch: 15 [41344/50048]	Loss: 1.7019
Training Epoch: 15 [41472/50048]	Loss: 1.1575
Training Epoch: 15 [41600/50048]	Loss: 1.1757
Training Epoch: 15 [41728/50048]	Loss: 1.3648
Training Epoch: 15 [41856/50048]	Loss: 1.4532
Training Epoch: 15 [41984/50048]	Loss: 1.2141
Training Epoch: 15 [42112/50048]	Loss: 1.2591
Training Epoch: 15 [42240/50048]	Loss: 1.2659
Training Epoch: 15 [42368/50048]	Loss: 1.3512
Training Epoch: 15 [42496/50048]	Loss: 1.0664
Training Epoch: 15 [42624/50048]	Loss: 1.4119
Training Epoch: 15 [42752/50048]	Loss: 1.2440
Training Epoch: 15 [42880/50048]	Loss: 1.2362
Training Epoch: 15 [43008/50048]	Loss: 1.3477
Training Epoch: 15 [43136/50048]	Loss: 1.3100
Training Epoch: 15 [43264/50048]	Loss: 1.3055
Training Epoch: 15 [43392/50048]	Loss: 1.1937
Training Epoch: 15 [43520/50048]	Loss: 1.1702
Training Epoch: 15 [43648/50048]	Loss: 1.1432
Training Epoch: 15 [43776/50048]	Loss: 1.5261
Training Epoch: 15 [43904/50048]	Loss: 1.2741
Training Epoch: 15 [44032/50048]	Loss: 1.1518
Training Epoch: 15 [44160/50048]	Loss: 1.0934
Training Epoch: 15 [44288/50048]	Loss: 1.0973
Training Epoch: 15 [44416/50048]	Loss: 1.2694
Training Epoch: 15 [44544/50048]	Loss: 1.2228
Training Epoch: 15 [44672/50048]	Loss: 1.0892
Training Epoch: 15 [44800/50048]	Loss: 1.0401
Training Epoch: 15 [44928/50048]	Loss: 1.3199
Training Epoch: 15 [45056/50048]	Loss: 1.2400
Training Epoch: 15 [45184/50048]	Loss: 1.3011
Training Epoch: 15 [45312/50048]	Loss: 1.3436
Training Epoch: 15 [45440/50048]	Loss: 1.2333
Training Epoch: 15 [45568/50048]	Loss: 1.1672
Training Epoch: 15 [45696/50048]	Loss: 1.5019
2022-12-06 04:01:24,784 [ZeusDataLoader(train)] train epoch 16 done: time=86.47 energy=10508.75
2022-12-06 04:01:24,786 [ZeusDataLoader(eval)] Epoch 16 begin.
Training Epoch: 15 [45824/50048]	Loss: 1.1789
Training Epoch: 15 [45952/50048]	Loss: 1.3010
Training Epoch: 15 [46080/50048]	Loss: 1.2466
Training Epoch: 15 [46208/50048]	Loss: 1.1485
Training Epoch: 15 [46336/50048]	Loss: 1.1539
Training Epoch: 15 [46464/50048]	Loss: 1.0119
Training Epoch: 15 [46592/50048]	Loss: 1.3275
Training Epoch: 15 [46720/50048]	Loss: 1.2038
Training Epoch: 15 [46848/50048]	Loss: 1.1539
Training Epoch: 15 [46976/50048]	Loss: 1.0141
Training Epoch: 15 [47104/50048]	Loss: 1.3199
Training Epoch: 15 [47232/50048]	Loss: 1.2778
Training Epoch: 15 [47360/50048]	Loss: 1.3905
Training Epoch: 15 [47488/50048]	Loss: 1.1895
Training Epoch: 15 [47616/50048]	Loss: 1.5391
Training Epoch: 15 [47744/50048]	Loss: 1.1132
Training Epoch: 15 [47872/50048]	Loss: 1.2924
Training Epoch: 15 [48000/50048]	Loss: 1.1567
Training Epoch: 15 [48128/50048]	Loss: 1.3412
Training Epoch: 15 [48256/50048]	Loss: 1.0527
Training Epoch: 15 [48384/50048]	Loss: 1.0607
Training Epoch: 15 [48512/50048]	Loss: 1.3039
Training Epoch: 15 [48640/50048]	Loss: 1.3275
Training Epoch: 15 [48768/50048]	Loss: 0.9592
Training Epoch: 15 [48896/50048]	Loss: 1.3177
Training Epoch: 15 [49024/50048]	Loss: 1.0901
Training Epoch: 15 [49152/50048]	Loss: 1.0282
Training Epoch: 15 [49280/50048]	Loss: 1.2750
Training Epoch: 15 [49408/50048]	Loss: 1.1405
Training Epoch: 15 [49536/50048]	Loss: 1.2886
Training Epoch: 15 [49664/50048]	Loss: 1.2885
Training Epoch: 15 [49792/50048]	Loss: 1.1348
Training Epoch: 15 [49920/50048]	Loss: 1.0849
Training Epoch: 15 [50048/50048]	Loss: 1.2887
2022-12-06 09:01:28.432 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:01:28,456 [ZeusDataLoader(eval)] eval epoch 16 done: time=3.66 energy=445.37
2022-12-06 04:01:28,456 [ZeusDataLoader(train)] Up to epoch 16: time=1445.89, energy=175189.47, cost=214110.13
2022-12-06 04:01:28,456 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:01:28,456 [ZeusDataLoader(train)] Expected next epoch: time=1535.69, energy=185987.48, cost=227366.51
2022-12-06 04:01:28,457 [ZeusDataLoader(train)] Epoch 17 begin.
Validation Epoch: 15, Average loss: 0.0120, Accuracy: 0.5850
2022-12-06 04:01:28,593 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:01:28,594 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:01:28.597 [ZeusMonitor] Monitor started.
2022-12-06 09:01:28.598 [ZeusMonitor] Running indefinitely. 2022-12-06 09:01:28.598 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:01:28.598 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e17+gpu0.power.log
Training Epoch: 16 [128/50048]	Loss: 1.3799
Training Epoch: 16 [256/50048]	Loss: 1.1120
Training Epoch: 16 [384/50048]	Loss: 1.2275
Training Epoch: 16 [512/50048]	Loss: 1.1880
Training Epoch: 16 [640/50048]	Loss: 1.1273
Training Epoch: 16 [768/50048]	Loss: 1.1694
Training Epoch: 16 [896/50048]	Loss: 1.2031
Training Epoch: 16 [1024/50048]	Loss: 1.1852
Training Epoch: 16 [1152/50048]	Loss: 1.1199
Training Epoch: 16 [1280/50048]	Loss: 1.0322
Training Epoch: 16 [1408/50048]	Loss: 1.0531
Training Epoch: 16 [1536/50048]	Loss: 1.2333
Training Epoch: 16 [1664/50048]	Loss: 1.3042
Training Epoch: 16 [1792/50048]	Loss: 1.1100
Training Epoch: 16 [1920/50048]	Loss: 1.3234
Training Epoch: 16 [2048/50048]	Loss: 1.0604
Training Epoch: 16 [2176/50048]	Loss: 1.2679
Training Epoch: 16 [2304/50048]	Loss: 1.1299
Training Epoch: 16 [2432/50048]	Loss: 1.0451
Training Epoch: 16 [2560/50048]	Loss: 1.2403
Training Epoch: 16 [2688/50048]	Loss: 1.0598
Training Epoch: 16 [2816/50048]	Loss: 1.1834
Training Epoch: 16 [2944/50048]	Loss: 1.0337
Training Epoch: 16 [3072/50048]	Loss: 0.9360
Training Epoch: 16 [3200/50048]	Loss: 1.1017
Training Epoch: 16 [3328/50048]	Loss: 1.4424
Training Epoch: 16 [3456/50048]	Loss: 1.1010
Training Epoch: 16 [3584/50048]	Loss: 1.0235
Training Epoch: 16 [3712/50048]	Loss: 1.1944
Training Epoch: 16 [3840/50048]	Loss: 1.2030
Training Epoch: 16 [3968/50048]	Loss: 1.1238
Training Epoch: 16 [4096/50048]	Loss: 1.2587
Training Epoch: 16 [4224/50048]	Loss: 1.0745
Training Epoch: 16 [4352/50048]	Loss: 1.2936
Training Epoch: 16 [4480/50048]	Loss: 1.0339
Training Epoch: 16 [4608/50048]	Loss: 1.1933
Training Epoch: 16 [4736/50048]	Loss: 1.2272
Training Epoch: 16 [4864/50048]	Loss: 1.2857
Training Epoch: 16 [4992/50048]	Loss: 1.1447
Training Epoch: 16 [5120/50048]	Loss: 1.0503
Training Epoch: 16 [5248/50048]	Loss: 1.0948
Training Epoch: 16 [5376/50048]	Loss: 1.3283
Training Epoch: 16 [5504/50048]	Loss: 1.3428
Training Epoch: 16 [5632/50048]	Loss: 1.0487
Training Epoch: 16 [5760/50048]	Loss: 1.0270
Training Epoch: 16 [5888/50048]	Loss: 1.2049
Training Epoch: 16 [6016/50048]	Loss: 1.1431
Training Epoch: 16 [6144/50048]	Loss: 1.0489
Training Epoch: 16 [6272/50048]	Loss: 1.1391
Training Epoch: 16 [6400/50048]	Loss: 1.3428
Training Epoch: 16 [6528/50048]	Loss: 1.0948
Training Epoch: 16 [6656/50048]	Loss: 1.0525
Training Epoch: 16 [6784/50048]	Loss: 1.2426
Training Epoch: 16 [6912/50048]	Loss: 1.1470
Training Epoch: 16 [7040/50048]	Loss: 0.8456
Training Epoch: 16 [7168/50048]	Loss: 1.0367
Training Epoch: 16 [7296/50048]	Loss: 1.0489
Training Epoch: 16 [7424/50048]	Loss: 1.0876
Training Epoch: 16 [7552/50048]	Loss: 0.9131
Training Epoch: 16 [7680/50048]	Loss: 0.9699
Training Epoch: 16 [7808/50048]	Loss: 1.0973
Training Epoch: 16 [7936/50048]	Loss: 1.1027
Training Epoch: 16 [8064/50048]	Loss: 1.1461
Training Epoch: 16 [8192/50048]	Loss: 0.8488
Training Epoch: 16 [8320/50048]	Loss: 1.1687
Training Epoch: 16 [8448/50048]	Loss: 1.2029
Training Epoch: 16 [8576/50048]	Loss: 1.0651
Training Epoch: 16 [8704/50048]	Loss: 0.9035
Training Epoch: 16 [8832/50048]	Loss: 1.1275
Training Epoch: 16 [8960/50048]	Loss: 0.9872
Training Epoch: 16 [9088/50048]	Loss: 1.1155
Training Epoch: 16 [9216/50048]	Loss: 1.2492
Training Epoch: 16 [9344/50048]	Loss: 1.2385
Training Epoch: 16 [9472/50048]	Loss: 1.2151
Training Epoch: 16 [9600/50048]	Loss: 0.9234
Training Epoch: 16 [9728/50048]	Loss: 1.1289
Training Epoch: 16 [9856/50048]	Loss: 1.0531
Training Epoch: 16 [9984/50048]	Loss: 1.1370
Training Epoch: 16 [10112/50048]	Loss: 1.0623
Training Epoch: 16 [10240/50048]	Loss: 0.9955
Training Epoch: 16 [10368/50048]	Loss: 1.1630
Training Epoch: 16 [10496/50048]	Loss: 1.1170
Training Epoch: 16 [10624/50048]	Loss: 1.0992
Training Epoch: 16 [10752/50048]	Loss: 1.1110
Training Epoch: 16 [10880/50048]	Loss: 1.3449
Training Epoch: 16 [11008/50048]	Loss: 1.2285
Training Epoch: 16 [11136/50048]	Loss: 1.1027
Training Epoch: 16 [11264/50048]	Loss: 1.1660
Training Epoch: 16 [11392/50048]	Loss: 1.1184
Training Epoch: 16 [11520/50048]	Loss: 1.1369
Training Epoch: 16 [11648/50048]	Loss: 1.1271
Training Epoch: 16 [11776/50048]	Loss: 1.2247
Training Epoch: 16 [11904/50048]	Loss: 1.0242
Training Epoch: 16 [12032/50048]	Loss: 1.1677
Training Epoch: 16 [12160/50048]	Loss: 1.1710
Training Epoch: 16 [12288/50048]	Loss: 1.1270
Training Epoch: 16 [12416/50048]	Loss: 1.2274
Training Epoch: 16 [12544/50048]	Loss: 1.2351
Training Epoch: 16 [12672/50048]	Loss: 1.0629
Training Epoch: 16 [12800/50048]	Loss: 1.0746
Training Epoch: 16 [12928/50048]	Loss: 1.2818
Training Epoch: 16 [13056/50048]	Loss: 1.1366
Training Epoch: 16 [13184/50048]	Loss: 1.3570
Training Epoch: 16 [13312/50048]	Loss: 1.0354
Training Epoch: 16 [13440/50048]	Loss: 1.2106
Training Epoch: 16 [13568/50048]	Loss: 1.1619
Training Epoch: 16 [13696/50048]	Loss: 1.1116
Training Epoch: 16 [13824/50048]	Loss: 1.2130
Training Epoch: 16 [13952/50048]	Loss: 1.0255
Training Epoch: 16 [14080/50048]	Loss: 1.1673
Training Epoch: 16 [14208/50048]	Loss: 1.3394
Training Epoch: 16 [14336/50048]	Loss: 1.1850
Training Epoch: 16 [14464/50048]	Loss: 1.1761
Training Epoch: 16 [14592/50048]	Loss: 1.2015
Training Epoch: 16 [14720/50048]	Loss: 1.1609
Training Epoch: 16 [14848/50048]	Loss: 1.0965
Training Epoch: 16 [14976/50048]	Loss: 1.3557
Training Epoch: 16 [15104/50048]	Loss: 1.0848
Training Epoch: 16 [15232/50048]	Loss: 1.1603
Training Epoch: 16 [15360/50048]	Loss: 1.4058
Training Epoch: 16 [15488/50048]	Loss: 1.0902
Training Epoch: 16 [15616/50048]	Loss: 1.3291
Training Epoch: 16 [15744/50048]	Loss: 1.0053
Training Epoch: 16 [15872/50048]	Loss: 1.0018
Training Epoch: 16 [16000/50048]	Loss: 1.2881
Training Epoch: 16 [16128/50048]	Loss: 1.2290
Training Epoch: 16 [16256/50048]	Loss: 1.1938
Training Epoch: 16 [16384/50048]	Loss: 1.2142
Training Epoch: 16 [16512/50048]	Loss: 1.0777
Training Epoch: 16 [16640/50048]	Loss: 1.0675
Training Epoch: 16 [16768/50048]	Loss: 0.9548
Training Epoch: 16 [16896/50048]	Loss: 1.0579
Training Epoch: 16 [17024/50048]	Loss: 1.2425
Training Epoch: 16 [17152/50048]	Loss: 1.2864
Training Epoch: 16 [17280/50048]	Loss: 1.1285
Training Epoch: 16 [17408/50048]	Loss: 1.2711
Training Epoch: 16 [17536/50048]	Loss: 1.1665
Training Epoch: 16 [17664/50048]	Loss: 1.2310
Training Epoch: 16 [17792/50048]	Loss: 1.3109
Training Epoch: 16 [17920/50048]	Loss: 1.4195
Training Epoch: 16 [18048/50048]	Loss: 1.1058
Training Epoch: 16 [18176/50048]	Loss: 1.0417
Training Epoch: 16 [18304/50048]	Loss: 1.1367
Training Epoch: 16 [18432/50048]	Loss: 1.3371
Training Epoch: 16 [18560/50048]	Loss: 1.1905
Training Epoch: 16 [18688/50048]	Loss: 1.2403
Training Epoch: 16 [18816/50048]	Loss: 1.0975
Training Epoch: 16 [18944/50048]	Loss: 1.1079
Training Epoch: 16 [19072/50048]	Loss: 1.1639
Training Epoch: 16 [19200/50048]	Loss: 1.2911
Training Epoch: 16 [19328/50048]	Loss: 1.0092
Training Epoch: 16 [19456/50048]	Loss: 1.2457
Training Epoch: 16 [19584/50048]	Loss: 0.9986
Training Epoch: 16 [19712/50048]	Loss: 1.1754
Training Epoch: 16 [19840/50048]	Loss: 1.0978
Training Epoch: 16 [19968/50048]	Loss: 1.2233
Training Epoch: 16 [20096/50048]	Loss: 0.9937
Training Epoch: 16 [20224/50048]	Loss: 1.2040
Training Epoch: 16 [20352/50048]	Loss: 1.0433
Training Epoch: 16 [20480/50048]	Loss: 1.1372
Training Epoch: 16 [20608/50048]	Loss: 1.1001
Training Epoch: 16 [20736/50048]	Loss: 1.3025
Training Epoch: 16 [20864/50048]	Loss: 1.2153
Training Epoch: 16 [20992/50048]	Loss: 1.4461
Training Epoch: 16 [21120/50048]	Loss: 1.0750
Training Epoch: 16 [21248/50048]	Loss: 1.1389
Training Epoch: 16 [21376/50048]	Loss: 1.1093
Training Epoch: 16 [21504/50048]	Loss: 1.4124
Training Epoch: 16 [21632/50048]	Loss: 1.2001
Training Epoch: 16 [21760/50048]	Loss: 1.4393
Training Epoch: 16 [21888/50048]	Loss: 1.0756
Training Epoch: 16 [22016/50048]	Loss: 1.1030
Training Epoch: 16 [22144/50048]	Loss: 1.1257
Training Epoch: 16 [22272/50048]	Loss: 1.2628
Training Epoch: 16 [22400/50048]	Loss: 1.2481
Training Epoch: 16 [22528/50048]	Loss: 1.0695
Training Epoch: 16 [22656/50048]	Loss: 1.1598
Training Epoch: 16 [22784/50048]	Loss: 0.9271
Training Epoch: 16 [22912/50048]	Loss: 1.4022
Training Epoch: 16 [23040/50048]	Loss: 1.1088
Training Epoch: 16 [23168/50048]	Loss: 1.1412
Training Epoch: 16 [23296/50048]	Loss: 1.2084
Training Epoch: 16 [23424/50048]	Loss: 1.1619
Training Epoch: 16 [23552/50048]	Loss: 1.4330
Training Epoch: 16 [23680/50048]	Loss: 1.1759
Training Epoch: 16 [23808/50048]	Loss: 1.1028
Training Epoch: 16 [23936/50048]	Loss: 1.1706
Training Epoch: 16 [24064/50048]	Loss: 1.3339
Training Epoch: 16 [24192/50048]	Loss: 1.2459
Training Epoch: 16 [24320/50048]	Loss: 1.2483
Training Epoch: 16 [24448/50048]	Loss: 1.2703
Training Epoch: 16 [24576/50048]	Loss: 0.9542
Training Epoch: 16 [24704/50048]	Loss: 1.2108
Training Epoch: 16 [24832/50048]	Loss: 1.1218
Training Epoch: 16 [24960/50048]	Loss: 1.1345
Training Epoch: 16 [25088/50048]	Loss: 1.1480
Training Epoch: 16 [25216/50048]	Loss: 1.2865
Training Epoch: 16 [25344/50048]	Loss: 1.1330
Training Epoch: 16 [25472/50048]	Loss: 1.1672
Training Epoch: 16 [25600/50048]	Loss: 1.1201
Training Epoch: 16 [25728/50048]	Loss: 1.1673
Training Epoch: 16 [25856/50048]	Loss: 1.1249
Training Epoch: 16 [25984/50048]	Loss: 0.9785
Training Epoch: 16 [26112/50048]	Loss: 1.3203
Training Epoch: 16 [26240/50048]	Loss: 1.0905
Training Epoch: 16 [26368/50048]	Loss: 1.1349
Training Epoch: 16 [26496/50048]	Loss: 1.2298
Training Epoch: 16 [26624/50048]	Loss: 1.1167
Training Epoch: 16 [26752/50048]	Loss: 1.3530
Training Epoch: 16 [26880/50048]	Loss: 1.0710
Training Epoch: 16 [27008/50048]	Loss: 1.1020
Training Epoch: 16 [27136/50048]	Loss: 1.3232
Training Epoch: 16 [27264/50048]	Loss: 1.2008
Training Epoch: 16 [27392/50048]	Loss: 1.2846
Training Epoch: 16 [27520/50048]	Loss: 1.3396
Training Epoch: 16 [27648/50048]	Loss: 1.0947
Training Epoch: 16 [27776/50048]	Loss: 1.3079
Training Epoch: 16 [27904/50048]	Loss: 1.1943
Training Epoch: 16 [28032/50048]	Loss: 1.1216
Training Epoch: 16 [28160/50048]	Loss: 1.2033
Training Epoch: 16 [28288/50048]	Loss: 1.1919
Training Epoch: 16 [28416/50048]	Loss: 0.9787
Training Epoch: 16 [28544/50048]	Loss: 1.3349
Training Epoch: 16 [28672/50048]	Loss: 0.9857
Training Epoch: 16 [28800/50048]	Loss: 1.2383
Training Epoch: 16 [28928/50048]	Loss: 1.1041
Training Epoch: 16 [29056/50048]	Loss: 0.9820
Training Epoch: 16 [29184/50048]	Loss: 1.4413
Training Epoch: 16 [29312/50048]	Loss: 1.4594
Training Epoch: 16 [29440/50048]	Loss: 1.2482
Training Epoch: 16 [29568/50048]	Loss: 0.9497
Training Epoch: 16 [29696/50048]	Loss: 1.1544
Training Epoch: 16 [29824/50048]	Loss: 1.2070
Training Epoch: 16 [29952/50048]	Loss: 1.3962
Training Epoch: 16 [30080/50048]	Loss: 1.1206
Training Epoch: 16 [30208/50048]	Loss: 1.3248
Training Epoch: 16 [30336/50048]	Loss: 1.1116
Training Epoch: 16 [30464/50048]	Loss: 1.0694
Training Epoch: 16 [30592/50048]	Loss: 1.2427
Training Epoch: 16 [30720/50048]	Loss: 1.0880
Training Epoch: 16 [30848/50048]	Loss: 1.2544
Training Epoch: 16 [30976/50048]	Loss: 1.1412
Training Epoch: 16 [31104/50048]	Loss: 1.1756
Training Epoch: 16 [31232/50048]	Loss: 1.3249
Training Epoch: 16 [31360/50048]	Loss: 1.1709
Training Epoch: 16 [31488/50048]	Loss: 1.1433
Training Epoch: 16 [31616/50048]	Loss: 1.0576
Training Epoch: 16 [31744/50048]	Loss: 0.9831
Training Epoch: 16 [31872/50048]	Loss: 1.4553
Training Epoch: 16 [32000/50048]	Loss: 1.4631
Training Epoch: 16 [32128/50048]	Loss: 1.1709
Training Epoch: 16 [32256/50048]	Loss: 1.1384
Training Epoch: 16 [32384/50048]	Loss: 1.3362
Training Epoch: 16 [32512/50048]	Loss: 1.2193
Training Epoch: 16 [32640/50048]	Loss: 1.0925
Training Epoch: 16 [32768/50048]	Loss: 1.0728
Training Epoch: 16 [32896/50048]	Loss: 1.1207
Training Epoch: 16 [33024/50048]	Loss: 1.2509
Training Epoch: 16 [33152/50048]	Loss: 1.2864
Training Epoch: 16 [33280/50048]	Loss: 1.2718
Training Epoch: 16 [33408/50048]	Loss: 1.0924
Training Epoch: 16 [33536/50048]	Loss: 1.0418
Training Epoch: 16 [33664/50048]	Loss: 1.1149
Training Epoch: 16 [33792/50048]	Loss: 1.1975
Training Epoch: 16 [33920/50048]	Loss: 1.3762
Training Epoch: 16 [34048/50048]	Loss: 1.3492
Training Epoch: 16 [34176/50048]	Loss: 1.2079
Training Epoch: 16 [34304/50048]	Loss: 1.2959
Training Epoch: 16 [34432/50048]	Loss: 1.0511
Training Epoch: 16 [34560/50048]	Loss: 1.2177
Training Epoch: 16 [34688/50048]	Loss: 1.3260
Training Epoch: 16 [34816/50048]	Loss: 1.2583
Training Epoch: 16 [34944/50048]	Loss: 1.2312
Training Epoch: 16 [35072/50048]	Loss: 1.3639
Training Epoch: 16 [35200/50048]	Loss: 1.2426
Training Epoch: 16 [35328/50048]	Loss: 1.2638
Training Epoch: 16 [35456/50048]	Loss: 1.0710
Training Epoch: 16 [35584/50048]	Loss: 1.3637
Training Epoch: 16 [35712/50048]	Loss: 1.1825
Training Epoch: 16 [35840/50048]	Loss: 1.1587
Training Epoch: 16 [35968/50048]	Loss: 1.2215
Training Epoch: 16 [36096/50048]	Loss: 1.1032
Training Epoch: 16 [36224/50048]	Loss: 1.1760
Training Epoch: 16 [36352/50048]	Loss: 1.1111
Training Epoch: 16 [36480/50048]	Loss: 1.3834
Training Epoch: 16 [36608/50048]	Loss: 1.0566
Training Epoch: 16 [36736/50048]	Loss: 1.2936
Training Epoch: 16 [36864/50048]	Loss: 1.3664
Training Epoch: 16 [36992/50048]	Loss: 1.0806
Training Epoch: 16 [37120/50048]	Loss: 0.9683
Training Epoch: 16 [37248/50048]	Loss: 1.0745
Training Epoch: 16 [37376/50048]	Loss: 1.2131
Training Epoch: 16 [37504/50048]	Loss: 1.2152
Training Epoch: 16 [37632/50048]	Loss: 0.9907
Training Epoch: 16 [37760/50048]	Loss: 1.1999
Training Epoch: 16 [37888/50048]	Loss: 1.2571
Training Epoch: 16 [38016/50048]	Loss: 1.2648
Training Epoch: 16 [38144/50048]	Loss: 1.0954
Training Epoch: 16 [38272/50048]	Loss: 0.9535
Training Epoch: 16 [38400/50048]	Loss: 1.2876
Training Epoch: 16 [38528/50048]	Loss: 1.2270
Training Epoch: 16 [38656/50048]	Loss: 0.9571
Training Epoch: 16 [38784/50048]	Loss: 1.1386
Training Epoch: 16 [38912/50048]	Loss: 1.4176
Training Epoch: 16 [39040/50048]	Loss: 1.2315
Training Epoch: 16 [39168/50048]	Loss: 1.3511
Training Epoch: 16 [39296/50048]	Loss: 1.4771
Training Epoch: 16 [39424/50048]	Loss: 1.5314
Training Epoch: 16 [39552/50048]	Loss: 1.2530
Training Epoch: 16 [39680/50048]	Loss: 1.2202
Training Epoch: 16 [39808/50048]	Loss: 1.1777
Training Epoch: 16 [39936/50048]	Loss: 1.1949
Training Epoch: 16 [40064/50048]	Loss: 1.1372
Training Epoch: 16 [40192/50048]	Loss: 1.1841
Training Epoch: 16 [40320/50048]	Loss: 1.3472
Training Epoch: 16 [40448/50048]	Loss: 1.4785
Training Epoch: 16 [40576/50048]	Loss: 1.1507
Training Epoch: 16 [40704/50048]	Loss: 1.4108
Training Epoch: 16 [40832/50048]	Loss: 1.2695
Training Epoch: 16 [40960/50048]	Loss: 1.3662
Training Epoch: 16 [41088/50048]	Loss: 1.1077
Training Epoch: 16 [41216/50048]	Loss: 1.2317
Training Epoch: 16 [41344/50048]	Loss: 0.8966
Training Epoch: 16 [41472/50048]	Loss: 1.2795
Training Epoch: 16 [41600/50048]	Loss: 1.1379
Training Epoch: 16 [41728/50048]	Loss: 1.0301
Training Epoch: 16 [41856/50048]	Loss: 1.3937
Training Epoch: 16 [41984/50048]	Loss: 1.2076
Training Epoch: 16 [42112/50048]	Loss: 1.1494
Training Epoch: 16 [42240/50048]	Loss: 1.2813
Training Epoch: 16 [42368/50048]	Loss: 1.2566
Training Epoch: 16 [42496/50048]	Loss: 1.1389
Training Epoch: 16 [42624/50048]	Loss: 1.1481
Training Epoch: 16 [42752/50048]	Loss: 1.0907
Training Epoch: 16 [42880/50048]	Loss: 1.0474
Training Epoch: 16 [43008/50048]	Loss: 1.3048
Training Epoch: 16 [43136/50048]	Loss: 1.3080
Training Epoch: 16 [43264/50048]	Loss: 1.3702
Training Epoch: 16 [43392/50048]	Loss: 1.1457
Training Epoch: 16 [43520/50048]	Loss: 1.2416
Training Epoch: 16 [43648/50048]	Loss: 1.0387
Training Epoch: 16 [43776/50048]	Loss: 1.3515
Training Epoch: 16 [43904/50048]	Loss: 1.3096
Training Epoch: 16 [44032/50048]	Loss: 1.4722
Training Epoch: 16 [44160/50048]	Loss: 1.2533
Training Epoch: 16 [44288/50048]	Loss: 1.1240
Training Epoch: 16 [44416/50048]	Loss: 1.1707
Training Epoch: 16 [44544/50048]	Loss: 1.3078
Training Epoch: 16 [44672/50048]	Loss: 1.3655
Training Epoch: 16 [44800/50048]	Loss: 1.1254
Training Epoch: 16 [44928/50048]	Loss: 1.5526
Training Epoch: 16 [45056/50048]	Loss: 1.5263
Training Epoch: 16 [45184/50048]	Loss: 1.2618
Training Epoch: 16 [45312/50048]	Loss: 1.2125
Training Epoch: 16 [45440/50048]	Loss: 1.1094
Training Epoch: 16 [45568/50048]	Loss: 1.2385
Training Epoch: 16 [45696/50048]	Loss: 1.3658
2022-12-06 04:02:54,904 [ZeusDataLoader(train)] train epoch 17 done: time=86.44 energy=10508.51
2022-12-06 04:02:54,906 [ZeusDataLoader(eval)] Epoch 17 begin.
Training Epoch: 16 [45824/50048]	Loss: 1.3594
Training Epoch: 16 [45952/50048]	Loss: 1.4275
Training Epoch: 16 [46080/50048]	Loss: 1.2788
Training Epoch: 16 [46208/50048]	Loss: 1.2125
Training Epoch: 16 [46336/50048]	Loss: 1.4012
Training Epoch: 16 [46464/50048]	Loss: 1.1176
Training Epoch: 16 [46592/50048]	Loss: 1.3372
Training Epoch: 16 [46720/50048]	Loss: 1.1592
Training Epoch: 16 [46848/50048]	Loss: 1.2630
Training Epoch: 16 [46976/50048]	Loss: 1.0721
Training Epoch: 16 [47104/50048]	Loss: 1.2859
Training Epoch: 16 [47232/50048]	Loss: 1.3372
Training Epoch: 16 [47360/50048]	Loss: 1.1898
Training Epoch: 16 [47488/50048]	Loss: 1.1591
Training Epoch: 16 [47616/50048]	Loss: 1.0377
Training Epoch: 16 [47744/50048]	Loss: 1.2467
Training Epoch: 16 [47872/50048]	Loss: 1.1005
Training Epoch: 16 [48000/50048]	Loss: 1.2608
Training Epoch: 16 [48128/50048]	Loss: 1.4163
Training Epoch: 16 [48256/50048]	Loss: 1.1240
Training Epoch: 16 [48384/50048]	Loss: 1.1779
Training Epoch: 16 [48512/50048]	Loss: 1.1817
Training Epoch: 16 [48640/50048]	Loss: 1.0733
Training Epoch: 16 [48768/50048]	Loss: 1.2597
Training Epoch: 16 [48896/50048]	Loss: 1.2845
Training Epoch: 16 [49024/50048]	Loss: 1.3707
Training Epoch: 16 [49152/50048]	Loss: 1.1526
Training Epoch: 16 [49280/50048]	Loss: 1.1012
Training Epoch: 16 [49408/50048]	Loss: 0.9565
Training Epoch: 16 [49536/50048]	Loss: 1.2837
Training Epoch: 16 [49664/50048]	Loss: 1.3567
Training Epoch: 16 [49792/50048]	Loss: 1.0221
Training Epoch: 16 [49920/50048]	Loss: 1.3062
Training Epoch: 16 [50048/50048]	Loss: 1.2091
2022-12-06 09:02:58.618 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:02:58,668 [ZeusDataLoader(eval)] eval epoch 17 done: time=3.75 energy=454.37
2022-12-06 04:02:58,668 [ZeusDataLoader(train)] Up to epoch 17: time=1536.08, energy=186152.34, cost=227483.15
2022-12-06 04:02:58,668 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:02:58,668 [ZeusDataLoader(train)] Expected next epoch: time=1625.88, energy=196950.36, cost=240739.54
2022-12-06 04:02:58,669 [ZeusDataLoader(train)] Epoch 18 begin.
Validation Epoch: 16, Average loss: 0.0119, Accuracy: 0.5920
2022-12-06 04:02:58,843 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:02:58,844 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:02:58.846 [ZeusMonitor] Monitor started.
2022-12-06 09:02:58.846 [ZeusMonitor] Running indefinitely. 2022-12-06 09:02:58.846 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:02:58.846 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e18+gpu0.power.log
Training Epoch: 17 [128/50048]	Loss: 1.1362
Training Epoch: 17 [256/50048]	Loss: 1.1528
Training Epoch: 17 [384/50048]	Loss: 1.0129
Training Epoch: 17 [512/50048]	Loss: 1.1148
Training Epoch: 17 [640/50048]	Loss: 1.1563
Training Epoch: 17 [768/50048]	Loss: 0.8860
Training Epoch: 17 [896/50048]	Loss: 1.1740
Training Epoch: 17 [1024/50048]	Loss: 1.1604
Training Epoch: 17 [1152/50048]	Loss: 1.0328
Training Epoch: 17 [1280/50048]	Loss: 1.0614
Training Epoch: 17 [1408/50048]	Loss: 0.9930
Training Epoch: 17 [1536/50048]	Loss: 1.1661
Training Epoch: 17 [1664/50048]	Loss: 1.0681
Training Epoch: 17 [1792/50048]	Loss: 1.0892
Training Epoch: 17 [1920/50048]	Loss: 1.0766
Training Epoch: 17 [2048/50048]	Loss: 1.0205
Training Epoch: 17 [2176/50048]	Loss: 0.9041
Training Epoch: 17 [2304/50048]	Loss: 1.0004
Training Epoch: 17 [2432/50048]	Loss: 0.8029
Training Epoch: 17 [2560/50048]	Loss: 1.0697
Training Epoch: 17 [2688/50048]	Loss: 0.8814
Training Epoch: 17 [2816/50048]	Loss: 1.0729
Training Epoch: 17 [2944/50048]	Loss: 0.8291
Training Epoch: 17 [3072/50048]	Loss: 1.0930
Training Epoch: 17 [3200/50048]	Loss: 1.2038
Training Epoch: 17 [3328/50048]	Loss: 0.8988
Training Epoch: 17 [3456/50048]	Loss: 0.7898
Training Epoch: 17 [3584/50048]	Loss: 1.1759
Training Epoch: 17 [3712/50048]	Loss: 0.8018
Training Epoch: 17 [3840/50048]	Loss: 1.1117
Training Epoch: 17 [3968/50048]	Loss: 1.0709
Training Epoch: 17 [4096/50048]	Loss: 1.0636
Training Epoch: 17 [4224/50048]	Loss: 1.0507
Training Epoch: 17 [4352/50048]	Loss: 1.3336
Training Epoch: 17 [4480/50048]	Loss: 1.2408
Training Epoch: 17 [4608/50048]	Loss: 1.0637
Training Epoch: 17 [4736/50048]	Loss: 1.0298
Training Epoch: 17 [4864/50048]	Loss: 1.2178
Training Epoch: 17 [4992/50048]	Loss: 0.9394
Training Epoch: 17 [5120/50048]	Loss: 1.1289
Training Epoch: 17 [5248/50048]	Loss: 1.4283
Training Epoch: 17 [5376/50048]	Loss: 1.0535
Training Epoch: 17 [5504/50048]	Loss: 1.2270
Training Epoch: 17 [5632/50048]	Loss: 1.1062
Training Epoch: 17 [5760/50048]	Loss: 1.2048
Training Epoch: 17 [5888/50048]	Loss: 0.9319
Training Epoch: 17 [6016/50048]	Loss: 0.8998
Training Epoch: 17 [6144/50048]	Loss: 1.3272
Training Epoch: 17 [6272/50048]	Loss: 1.1487
Training Epoch: 17 [6400/50048]	Loss: 0.9951
Training Epoch: 17 [6528/50048]	Loss: 1.0108
Training Epoch: 17 [6656/50048]	Loss: 0.8962
Training Epoch: 17 [6784/50048]	Loss: 1.1594
Training Epoch: 17 [6912/50048]	Loss: 1.0767
Training Epoch: 17 [7040/50048]	Loss: 0.9882
Training Epoch: 17 [7168/50048]	Loss: 1.2397
Training Epoch: 17 [7296/50048]	Loss: 1.1054
Training Epoch: 17 [7424/50048]	Loss: 1.0174
Training Epoch: 17 [7552/50048]	Loss: 0.9517
Training Epoch: 17 [7680/50048]	Loss: 1.1788
Training Epoch: 17 [7808/50048]	Loss: 1.1730
Training Epoch: 17 [7936/50048]	Loss: 1.2999
Training Epoch: 17 [8064/50048]	Loss: 1.0000
Training Epoch: 17 [8192/50048]	Loss: 0.7690
Training Epoch: 17 [8320/50048]	Loss: 0.9161
Training Epoch: 17 [8448/50048]	Loss: 1.2117
Training Epoch: 17 [8576/50048]	Loss: 1.0042
Training Epoch: 17 [8704/50048]	Loss: 0.8977
Training Epoch: 17 [8832/50048]	Loss: 1.1527
Training Epoch: 17 [8960/50048]	Loss: 1.1212
Training Epoch: 17 [9088/50048]	Loss: 1.0394
Training Epoch: 17 [9216/50048]	Loss: 1.0679
Training Epoch: 17 [9344/50048]	Loss: 0.9202
Training Epoch: 17 [9472/50048]	Loss: 1.3580
Training Epoch: 17 [9600/50048]	Loss: 1.2754
Training Epoch: 17 [9728/50048]	Loss: 1.0104
Training Epoch: 17 [9856/50048]	Loss: 1.0651
Training Epoch: 17 [9984/50048]	Loss: 1.1947
Training Epoch: 17 [10112/50048]	Loss: 1.1672
Training Epoch: 17 [10240/50048]	Loss: 1.1085
Training Epoch: 17 [10368/50048]	Loss: 0.9654
Training Epoch: 17 [10496/50048]	Loss: 1.0936
Training Epoch: 17 [10624/50048]	Loss: 0.9859
Training Epoch: 17 [10752/50048]	Loss: 1.1288
Training Epoch: 17 [10880/50048]	Loss: 1.3152
Training Epoch: 17 [11008/50048]	Loss: 1.1848
Training Epoch: 17 [11136/50048]	Loss: 1.2437
Training Epoch: 17 [11264/50048]	Loss: 1.2311
Training Epoch: 17 [11392/50048]	Loss: 1.0293
Training Epoch: 17 [11520/50048]	Loss: 0.8861
Training Epoch: 17 [11648/50048]	Loss: 1.0389
Training Epoch: 17 [11776/50048]	Loss: 0.9974
Training Epoch: 17 [11904/50048]	Loss: 1.1104
Training Epoch: 17 [12032/50048]	Loss: 1.0629
Training Epoch: 17 [12160/50048]	Loss: 1.0004
Training Epoch: 17 [12288/50048]	Loss: 1.2660
Training Epoch: 17 [12416/50048]	Loss: 1.1799
Training Epoch: 17 [12544/50048]	Loss: 1.0817
Training Epoch: 17 [12672/50048]	Loss: 1.0288
Training Epoch: 17 [12800/50048]	Loss: 1.1245
Training Epoch: 17 [12928/50048]	Loss: 1.2881
Training Epoch: 17 [13056/50048]	Loss: 1.1109
Training Epoch: 17 [13184/50048]	Loss: 1.1457
Training Epoch: 17 [13312/50048]	Loss: 1.1844
Training Epoch: 17 [13440/50048]	Loss: 1.0690
Training Epoch: 17 [13568/50048]	Loss: 1.1459
Training Epoch: 17 [13696/50048]	Loss: 1.0362
Training Epoch: 17 [13824/50048]	Loss: 1.2576
Training Epoch: 17 [13952/50048]	Loss: 1.1009
Training Epoch: 17 [14080/50048]	Loss: 1.1003
Training Epoch: 17 [14208/50048]	Loss: 1.0600
Training Epoch: 17 [14336/50048]	Loss: 1.0105
Training Epoch: 17 [14464/50048]	Loss: 0.8869
Training Epoch: 17 [14592/50048]	Loss: 1.2241
Training Epoch: 17 [14720/50048]	Loss: 0.9760
Training Epoch: 17 [14848/50048]	Loss: 1.1364
Training Epoch: 17 [14976/50048]	Loss: 1.0484
Training Epoch: 17 [15104/50048]	Loss: 1.1607
Training Epoch: 17 [15232/50048]	Loss: 0.9908
Training Epoch: 17 [15360/50048]	Loss: 0.8953
Training Epoch: 17 [15488/50048]	Loss: 1.2488
Training Epoch: 17 [15616/50048]	Loss: 1.1543
Training Epoch: 17 [15744/50048]	Loss: 1.0986
Training Epoch: 17 [15872/50048]	Loss: 1.1853
Training Epoch: 17 [16000/50048]	Loss: 0.9641
Training Epoch: 17 [16128/50048]	Loss: 1.2096
Training Epoch: 17 [16256/50048]	Loss: 0.9632
Training Epoch: 17 [16384/50048]	Loss: 1.0396
Training Epoch: 17 [16512/50048]	Loss: 1.2073
Training Epoch: 17 [16640/50048]	Loss: 1.3550
Training Epoch: 17 [16768/50048]	Loss: 1.0222
Training Epoch: 17 [16896/50048]	Loss: 1.2005
Training Epoch: 17 [17024/50048]	Loss: 1.0536
Training Epoch: 17 [17152/50048]	Loss: 0.9420
Training Epoch: 17 [17280/50048]	Loss: 1.2704
Training Epoch: 17 [17408/50048]	Loss: 0.9599
Training Epoch: 17 [17536/50048]	Loss: 1.0774
Training Epoch: 17 [17664/50048]	Loss: 1.0406
Training Epoch: 17 [17792/50048]	Loss: 1.2179
Training Epoch: 17 [17920/50048]	Loss: 1.2832
Training Epoch: 17 [18048/50048]	Loss: 1.0974
Training Epoch: 17 [18176/50048]	Loss: 1.2735
Training Epoch: 17 [18304/50048]	Loss: 1.3700
Training Epoch: 17 [18432/50048]	Loss: 1.0061
Training Epoch: 17 [18560/50048]	Loss: 1.2755
Training Epoch: 17 [18688/50048]	Loss: 0.9675
Training Epoch: 17 [18816/50048]	Loss: 0.8686
Training Epoch: 17 [18944/50048]	Loss: 1.0562
Training Epoch: 17 [19072/50048]	Loss: 0.8090
Training Epoch: 17 [19200/50048]	Loss: 1.2814
Training Epoch: 17 [19328/50048]	Loss: 1.1807
Training Epoch: 17 [19456/50048]	Loss: 1.1974
Training Epoch: 17 [19584/50048]	Loss: 1.2167
Training Epoch: 17 [19712/50048]	Loss: 1.1128
Training Epoch: 17 [19840/50048]	Loss: 0.9724
Training Epoch: 17 [19968/50048]	Loss: 0.9137
Training Epoch: 17 [20096/50048]	Loss: 1.0216
Training Epoch: 17 [20224/50048]	Loss: 1.0551
Training Epoch: 17 [20352/50048]	Loss: 1.1713
Training Epoch: 17 [20480/50048]	Loss: 1.1453
Training Epoch: 17 [20608/50048]	Loss: 1.2522
Training Epoch: 17 [20736/50048]	Loss: 1.2038
Training Epoch: 17 [20864/50048]	Loss: 1.0152
Training Epoch: 17 [20992/50048]	Loss: 1.2022
Training Epoch: 17 [21120/50048]	Loss: 1.1706
Training Epoch: 17 [21248/50048]	Loss: 1.0300
Training Epoch: 17 [21376/50048]	Loss: 0.9859
Training Epoch: 17 [21504/50048]	Loss: 1.1568
Training Epoch: 17 [21632/50048]	Loss: 1.1910
Training Epoch: 17 [21760/50048]	Loss: 1.0742
Training Epoch: 17 [21888/50048]	Loss: 1.1250
Training Epoch: 17 [22016/50048]	Loss: 1.1755
Training Epoch: 17 [22144/50048]	Loss: 1.0918
Training Epoch: 17 [22272/50048]	Loss: 1.1651
Training Epoch: 17 [22400/50048]	Loss: 0.9802
Training Epoch: 17 [22528/50048]	Loss: 1.4003
Training Epoch: 17 [22656/50048]	Loss: 1.0616
Training Epoch: 17 [22784/50048]	Loss: 1.2063
Training Epoch: 17 [22912/50048]	Loss: 1.0977
Training Epoch: 17 [23040/50048]	Loss: 1.3235
Training Epoch: 17 [23168/50048]	Loss: 1.1308
Training Epoch: 17 [23296/50048]	Loss: 1.1241
Training Epoch: 17 [23424/50048]	Loss: 1.0991
Training Epoch: 17 [23552/50048]	Loss: 1.0845
Training Epoch: 17 [23680/50048]	Loss: 1.1185
Training Epoch: 17 [23808/50048]	Loss: 1.1262
Training Epoch: 17 [23936/50048]	Loss: 1.0473
Training Epoch: 17 [24064/50048]	Loss: 1.2553
Training Epoch: 17 [24192/50048]	Loss: 1.3280
Training Epoch: 17 [24320/50048]	Loss: 1.1836
Training Epoch: 17 [24448/50048]	Loss: 1.0691
Training Epoch: 17 [24576/50048]	Loss: 1.1524
Training Epoch: 17 [24704/50048]	Loss: 1.0624
Training Epoch: 17 [24832/50048]	Loss: 1.1579
Training Epoch: 17 [24960/50048]	Loss: 1.1718
Training Epoch: 17 [25088/50048]	Loss: 1.1283
Training Epoch: 17 [25216/50048]	Loss: 1.1586
Training Epoch: 17 [25344/50048]	Loss: 1.3731
Training Epoch: 17 [25472/50048]	Loss: 0.9862
Training Epoch: 17 [25600/50048]	Loss: 0.9892
Training Epoch: 17 [25728/50048]	Loss: 1.0719
Training Epoch: 17 [25856/50048]	Loss: 1.1459
Training Epoch: 17 [25984/50048]	Loss: 1.4214
Training Epoch: 17 [26112/50048]	Loss: 1.1184
Training Epoch: 17 [26240/50048]	Loss: 1.1881
Training Epoch: 17 [26368/50048]	Loss: 1.0910
Training Epoch: 17 [26496/50048]	Loss: 1.0368
Training Epoch: 17 [26624/50048]	Loss: 1.1054
Training Epoch: 17 [26752/50048]	Loss: 1.2063
Training Epoch: 17 [26880/50048]	Loss: 1.1711
Training Epoch: 17 [27008/50048]	Loss: 0.9486
Training Epoch: 17 [27136/50048]	Loss: 1.0254
Training Epoch: 17 [27264/50048]	Loss: 1.0693
Training Epoch: 17 [27392/50048]	Loss: 0.9227
Training Epoch: 17 [27520/50048]	Loss: 1.3130
Training Epoch: 17 [27648/50048]	Loss: 1.0388
Training Epoch: 17 [27776/50048]	Loss: 1.1798
Training Epoch: 17 [27904/50048]	Loss: 0.9951
Training Epoch: 17 [28032/50048]	Loss: 1.3405
Training Epoch: 17 [28160/50048]	Loss: 1.0968
Training Epoch: 17 [28288/50048]	Loss: 1.1304
Training Epoch: 17 [28416/50048]	Loss: 1.0525
Training Epoch: 17 [28544/50048]	Loss: 1.2568
Training Epoch: 17 [28672/50048]	Loss: 1.2650
Training Epoch: 17 [28800/50048]	Loss: 1.2468
Training Epoch: 17 [28928/50048]	Loss: 1.1489
Training Epoch: 17 [29056/50048]	Loss: 1.2771
Training Epoch: 17 [29184/50048]	Loss: 1.2342
Training Epoch: 17 [29312/50048]	Loss: 1.1488
Training Epoch: 17 [29440/50048]	Loss: 0.9630
Training Epoch: 17 [29568/50048]	Loss: 1.0042
Training Epoch: 17 [29696/50048]	Loss: 1.0886
Training Epoch: 17 [29824/50048]	Loss: 1.2404
Training Epoch: 17 [29952/50048]	Loss: 1.3822
Training Epoch: 17 [30080/50048]	Loss: 1.3715
Training Epoch: 17 [30208/50048]	Loss: 1.1778
Training Epoch: 17 [30336/50048]	Loss: 1.0336
Training Epoch: 17 [30464/50048]	Loss: 1.1350
Training Epoch: 17 [30592/50048]	Loss: 1.1649
Training Epoch: 17 [30720/50048]	Loss: 1.0024
Training Epoch: 17 [30848/50048]	Loss: 0.9921
Training Epoch: 17 [30976/50048]	Loss: 0.8681
Training Epoch: 17 [31104/50048]	Loss: 1.0238
Training Epoch: 17 [31232/50048]	Loss: 1.2923
Training Epoch: 17 [31360/50048]	Loss: 1.3382
Training Epoch: 17 [31488/50048]	Loss: 1.2396
Training Epoch: 17 [31616/50048]	Loss: 1.3144
Training Epoch: 17 [31744/50048]	Loss: 1.0507
Training Epoch: 17 [31872/50048]	Loss: 1.0963
Training Epoch: 17 [32000/50048]	Loss: 1.2654
Training Epoch: 17 [32128/50048]	Loss: 1.3628
Training Epoch: 17 [32256/50048]	Loss: 1.0898
Training Epoch: 17 [32384/50048]	Loss: 1.1547
Training Epoch: 17 [32512/50048]	Loss: 1.1466
Training Epoch: 17 [32640/50048]	Loss: 1.0718
Training Epoch: 17 [32768/50048]	Loss: 0.9131
Training Epoch: 17 [32896/50048]	Loss: 1.1842
Training Epoch: 17 [33024/50048]	Loss: 1.3587
Training Epoch: 17 [33152/50048]	Loss: 0.9929
Training Epoch: 17 [33280/50048]	Loss: 1.0748
Training Epoch: 17 [33408/50048]	Loss: 1.0743
Training Epoch: 17 [33536/50048]	Loss: 1.2736
Training Epoch: 17 [33664/50048]	Loss: 1.1341
Training Epoch: 17 [33792/50048]	Loss: 1.0028
Training Epoch: 17 [33920/50048]	Loss: 0.9107
Training Epoch: 17 [34048/50048]	Loss: 0.9280
Training Epoch: 17 [34176/50048]	Loss: 1.2955
Training Epoch: 17 [34304/50048]	Loss: 1.1998
Training Epoch: 17 [34432/50048]	Loss: 1.3161
Training Epoch: 17 [34560/50048]	Loss: 1.5003
Training Epoch: 17 [34688/50048]	Loss: 1.1284
Training Epoch: 17 [34816/50048]	Loss: 0.9864
Training Epoch: 17 [34944/50048]	Loss: 0.9782
Training Epoch: 17 [35072/50048]	Loss: 1.0017
Training Epoch: 17 [35200/50048]	Loss: 1.2955
Training Epoch: 17 [35328/50048]	Loss: 1.0691
Training Epoch: 17 [35456/50048]	Loss: 1.0596
Training Epoch: 17 [35584/50048]	Loss: 1.0848
Training Epoch: 17 [35712/50048]	Loss: 0.9444
Training Epoch: 17 [35840/50048]	Loss: 1.2268
Training Epoch: 17 [35968/50048]	Loss: 1.1843
Training Epoch: 17 [36096/50048]	Loss: 1.4186
Training Epoch: 17 [36224/50048]	Loss: 1.2901
Training Epoch: 17 [36352/50048]	Loss: 1.2272
Training Epoch: 17 [36480/50048]	Loss: 1.1007
Training Epoch: 17 [36608/50048]	Loss: 1.1037
Training Epoch: 17 [36736/50048]	Loss: 1.1014
Training Epoch: 17 [36864/50048]	Loss: 1.4087
Training Epoch: 17 [36992/50048]	Loss: 1.2466
Training Epoch: 17 [37120/50048]	Loss: 1.3824
Training Epoch: 17 [37248/50048]	Loss: 1.2171
Training Epoch: 17 [37376/50048]	Loss: 1.0828
Training Epoch: 17 [37504/50048]	Loss: 1.1340
Training Epoch: 17 [37632/50048]	Loss: 1.0201
Training Epoch: 17 [37760/50048]	Loss: 1.3037
Training Epoch: 17 [37888/50048]	Loss: 1.0047
Training Epoch: 17 [38016/50048]	Loss: 1.0897
Training Epoch: 17 [38144/50048]	Loss: 1.2689
Training Epoch: 17 [38272/50048]	Loss: 0.9812
Training Epoch: 17 [38400/50048]	Loss: 1.2964
Training Epoch: 17 [38528/50048]	Loss: 1.0708
Training Epoch: 17 [38656/50048]	Loss: 1.2722
Training Epoch: 17 [38784/50048]	Loss: 1.1798
Training Epoch: 17 [38912/50048]	Loss: 1.1158
Training Epoch: 17 [39040/50048]	Loss: 1.3505
Training Epoch: 17 [39168/50048]	Loss: 1.3805
Training Epoch: 17 [39296/50048]	Loss: 1.3099
Training Epoch: 17 [39424/50048]	Loss: 0.9302
Training Epoch: 17 [39552/50048]	Loss: 1.3219
Training Epoch: 17 [39680/50048]	Loss: 1.1143
Training Epoch: 17 [39808/50048]	Loss: 0.9578
Training Epoch: 17 [39936/50048]	Loss: 1.0179
Training Epoch: 17 [40064/50048]	Loss: 0.9803
Training Epoch: 17 [40192/50048]	Loss: 1.1232
Training Epoch: 17 [40320/50048]	Loss: 1.1537
Training Epoch: 17 [40448/50048]	Loss: 1.1000
Training Epoch: 17 [40576/50048]	Loss: 1.0963
Training Epoch: 17 [40704/50048]	Loss: 1.2637
Training Epoch: 17 [40832/50048]	Loss: 1.0543
Training Epoch: 17 [40960/50048]	Loss: 1.0584
Training Epoch: 17 [41088/50048]	Loss: 1.2993
Training Epoch: 17 [41216/50048]	Loss: 1.2861
Training Epoch: 17 [41344/50048]	Loss: 1.3296
Training Epoch: 17 [41472/50048]	Loss: 1.1729
Training Epoch: 17 [41600/50048]	Loss: 1.2228
Training Epoch: 17 [41728/50048]	Loss: 1.0366
Training Epoch: 17 [41856/50048]	Loss: 1.2210
Training Epoch: 17 [41984/50048]	Loss: 1.2017
Training Epoch: 17 [42112/50048]	Loss: 0.9127
Training Epoch: 17 [42240/50048]	Loss: 1.3689
Training Epoch: 17 [42368/50048]	Loss: 1.2854
Training Epoch: 17 [42496/50048]	Loss: 1.2510
Training Epoch: 17 [42624/50048]	Loss: 1.2095
Training Epoch: 17 [42752/50048]	Loss: 0.9729
Training Epoch: 17 [42880/50048]	Loss: 0.9791
Training Epoch: 17 [43008/50048]	Loss: 0.9274
Training Epoch: 17 [43136/50048]	Loss: 1.0561
Training Epoch: 17 [43264/50048]	Loss: 1.0090
Training Epoch: 17 [43392/50048]	Loss: 1.1746
Training Epoch: 17 [43520/50048]	Loss: 1.2100
Training Epoch: 17 [43648/50048]	Loss: 1.1213
Training Epoch: 17 [43776/50048]	Loss: 1.0936
Training Epoch: 17 [43904/50048]	Loss: 1.2821
Training Epoch: 17 [44032/50048]	Loss: 1.2710
Training Epoch: 17 [44160/50048]	Loss: 0.9733
Training Epoch: 17 [44288/50048]	Loss: 1.2197
Training Epoch: 17 [44416/50048]	Loss: 1.1862
Training Epoch: 17 [44544/50048]	Loss: 1.1656
Training Epoch: 17 [44672/50048]	Loss: 0.9458
Training Epoch: 17 [44800/50048]	Loss: 1.3308
Training Epoch: 17 [44928/50048]	Loss: 1.2475
Training Epoch: 17 [45056/50048]	Loss: 1.2751
Training Epoch: 17 [45184/50048]	Loss: 1.0050
Training Epoch: 17 [45312/50048]	Loss: 0.9584
Training Epoch: 17 [45440/50048]	Loss: 1.0492
Training Epoch: 17 [45568/50048]	Loss: 1.2414
Training Epoch: 17 [45696/50048]	Loss: 0.9837
2022-12-06 04:04:25,195 [ZeusDataLoader(train)] train epoch 18 done: time=86.52 energy=10504.10
2022-12-06 04:04:25,196 [ZeusDataLoader(eval)] Epoch 18 begin.
Training Epoch: 17 [45824/50048]	Loss: 1.1123
Training Epoch: 17 [45952/50048]	Loss: 1.1062
Training Epoch: 17 [46080/50048]	Loss: 1.0060
Training Epoch: 17 [46208/50048]	Loss: 1.1301
Training Epoch: 17 [46336/50048]	Loss: 0.9806
Training Epoch: 17 [46464/50048]	Loss: 1.3254
Training Epoch: 17 [46592/50048]	Loss: 1.0599
Training Epoch: 17 [46720/50048]	Loss: 1.0954
Training Epoch: 17 [46848/50048]	Loss: 1.0931
Training Epoch: 17 [46976/50048]	Loss: 1.3489
Training Epoch: 17 [47104/50048]	Loss: 1.1418
Training Epoch: 17 [47232/50048]	Loss: 0.8866
Training Epoch: 17 [47360/50048]	Loss: 1.2925
Training Epoch: 17 [47488/50048]	Loss: 1.4159
Training Epoch: 17 [47616/50048]	Loss: 1.2564
Training Epoch: 17 [47744/50048]	Loss: 1.2547
Training Epoch: 17 [47872/50048]	Loss: 1.1845
Training Epoch: 17 [48000/50048]	Loss: 1.0637
Training Epoch: 17 [48128/50048]	Loss: 1.0813
Training Epoch: 17 [48256/50048]	Loss: 1.1996
Training Epoch: 17 [48384/50048]	Loss: 0.9279
Training Epoch: 17 [48512/50048]	Loss: 1.1518
Training Epoch: 17 [48640/50048]	Loss: 1.1020
Training Epoch: 17 [48768/50048]	Loss: 1.0390
Training Epoch: 17 [48896/50048]	Loss: 1.1037
Training Epoch: 17 [49024/50048]	Loss: 1.2908
Training Epoch: 17 [49152/50048]	Loss: 1.0648
Training Epoch: 17 [49280/50048]	Loss: 1.1631
Training Epoch: 17 [49408/50048]	Loss: 1.2680
Training Epoch: 17 [49536/50048]	Loss: 1.1251
Training Epoch: 17 [49664/50048]	Loss: 1.1609
Training Epoch: 17 [49792/50048]	Loss: 1.3560
Training Epoch: 17 [49920/50048]	Loss: 1.1916
Training Epoch: 17 [50048/50048]	Loss: 1.1047
2022-12-06 09:04:28.871 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:04:28,895 [ZeusDataLoader(eval)] eval epoch 18 done: time=3.69 energy=453.20
2022-12-06 04:04:28,896 [ZeusDataLoader(train)] Up to epoch 18: time=1626.29, energy=197109.64, cost=240854.80
2022-12-06 04:04:28,896 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:04:28,896 [ZeusDataLoader(train)] Expected next epoch: time=1716.08, energy=207907.66, cost=254111.18
2022-12-06 04:04:28,897 [ZeusDataLoader(train)] Epoch 19 begin.
Validation Epoch: 17, Average loss: 0.0121, Accuracy: 0.5846
2022-12-06 04:04:29,068 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:04:29,069 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:04:29.080 [ZeusMonitor] Monitor started.
2022-12-06 09:04:29.080 [ZeusMonitor] Running indefinitely. 2022-12-06 09:04:29.080 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:04:29.080 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e19+gpu0.power.log
Training Epoch: 18 [128/50048]	Loss: 1.0265
Training Epoch: 18 [256/50048]	Loss: 0.9603
Training Epoch: 18 [384/50048]	Loss: 0.9689
Training Epoch: 18 [512/50048]	Loss: 0.7962
Training Epoch: 18 [640/50048]	Loss: 1.1902
Training Epoch: 18 [768/50048]	Loss: 1.1829
Training Epoch: 18 [896/50048]	Loss: 0.9626
Training Epoch: 18 [1024/50048]	Loss: 0.9920
Training Epoch: 18 [1152/50048]	Loss: 1.0019
Training Epoch: 18 [1280/50048]	Loss: 0.9734
Training Epoch: 18 [1408/50048]	Loss: 1.0349
Training Epoch: 18 [1536/50048]	Loss: 1.2541
Training Epoch: 18 [1664/50048]	Loss: 1.2150
Training Epoch: 18 [1792/50048]	Loss: 1.1951
Training Epoch: 18 [1920/50048]	Loss: 0.9607
Training Epoch: 18 [2048/50048]	Loss: 0.8990
Training Epoch: 18 [2176/50048]	Loss: 0.9921
Training Epoch: 18 [2304/50048]	Loss: 0.9043
Training Epoch: 18 [2432/50048]	Loss: 0.9579
Training Epoch: 18 [2560/50048]	Loss: 1.1532
Training Epoch: 18 [2688/50048]	Loss: 1.0627
Training Epoch: 18 [2816/50048]	Loss: 1.0779
Training Epoch: 18 [2944/50048]	Loss: 1.1392
Training Epoch: 18 [3072/50048]	Loss: 0.9689
Training Epoch: 18 [3200/50048]	Loss: 0.9778
Training Epoch: 18 [3328/50048]	Loss: 1.0209
Training Epoch: 18 [3456/50048]	Loss: 1.0048
Training Epoch: 18 [3584/50048]	Loss: 1.0511
Training Epoch: 18 [3712/50048]	Loss: 1.0977
Training Epoch: 18 [3840/50048]	Loss: 1.0199
Training Epoch: 18 [3968/50048]	Loss: 1.0576
Training Epoch: 18 [4096/50048]	Loss: 0.8791
Training Epoch: 18 [4224/50048]	Loss: 0.8793
Training Epoch: 18 [4352/50048]	Loss: 0.9942
Training Epoch: 18 [4480/50048]	Loss: 1.1026
Training Epoch: 18 [4608/50048]	Loss: 1.0810
Training Epoch: 18 [4736/50048]	Loss: 1.0737
Training Epoch: 18 [4864/50048]	Loss: 1.1225
Training Epoch: 18 [4992/50048]	Loss: 0.8526
Training Epoch: 18 [5120/50048]	Loss: 1.0406
Training Epoch: 18 [5248/50048]	Loss: 0.9973
Training Epoch: 18 [5376/50048]	Loss: 1.0229
Training Epoch: 18 [5504/50048]	Loss: 1.0690
Training Epoch: 18 [5632/50048]	Loss: 1.1350
Training Epoch: 18 [5760/50048]	Loss: 1.0801
Training Epoch: 18 [5888/50048]	Loss: 0.9246
Training Epoch: 18 [6016/50048]	Loss: 1.0118
Training Epoch: 18 [6144/50048]	Loss: 0.9748
Training Epoch: 18 [6272/50048]	Loss: 0.9689
Training Epoch: 18 [6400/50048]	Loss: 0.8816
Training Epoch: 18 [6528/50048]	Loss: 1.0905
Training Epoch: 18 [6656/50048]	Loss: 0.8556
Training Epoch: 18 [6784/50048]	Loss: 1.0239
Training Epoch: 18 [6912/50048]	Loss: 1.0414
Training Epoch: 18 [7040/50048]	Loss: 1.2526
Training Epoch: 18 [7168/50048]	Loss: 1.0240
Training Epoch: 18 [7296/50048]	Loss: 1.0624
Training Epoch: 18 [7424/50048]	Loss: 1.1481
Training Epoch: 18 [7552/50048]	Loss: 1.1094
Training Epoch: 18 [7680/50048]	Loss: 1.0478
Training Epoch: 18 [7808/50048]	Loss: 1.1828
Training Epoch: 18 [7936/50048]	Loss: 1.0025
Training Epoch: 18 [8064/50048]	Loss: 1.2620
Training Epoch: 18 [8192/50048]	Loss: 0.8877
Training Epoch: 18 [8320/50048]	Loss: 1.0616
Training Epoch: 18 [8448/50048]	Loss: 0.9554
Training Epoch: 18 [8576/50048]	Loss: 1.0197
Training Epoch: 18 [8704/50048]	Loss: 0.9471
Training Epoch: 18 [8832/50048]	Loss: 1.1187
Training Epoch: 18 [8960/50048]	Loss: 0.8578
Training Epoch: 18 [9088/50048]	Loss: 0.9729
Training Epoch: 18 [9216/50048]	Loss: 1.0546
Training Epoch: 18 [9344/50048]	Loss: 1.1338
Training Epoch: 18 [9472/50048]	Loss: 1.1438
Training Epoch: 18 [9600/50048]	Loss: 1.2355
Training Epoch: 18 [9728/50048]	Loss: 1.0058
Training Epoch: 18 [9856/50048]	Loss: 0.9402
Training Epoch: 18 [9984/50048]	Loss: 1.0002
Training Epoch: 18 [10112/50048]	Loss: 1.0432
Training Epoch: 18 [10240/50048]	Loss: 1.0942
Training Epoch: 18 [10368/50048]	Loss: 1.1637
Training Epoch: 18 [10496/50048]	Loss: 1.0019
Training Epoch: 18 [10624/50048]	Loss: 1.0933
Training Epoch: 18 [10752/50048]	Loss: 1.2342
Training Epoch: 18 [10880/50048]	Loss: 1.0170
Training Epoch: 18 [11008/50048]	Loss: 1.0432
Training Epoch: 18 [11136/50048]	Loss: 1.0527
Training Epoch: 18 [11264/50048]	Loss: 1.0194
Training Epoch: 18 [11392/50048]	Loss: 1.2052
Training Epoch: 18 [11520/50048]	Loss: 0.9451
Training Epoch: 18 [11648/50048]	Loss: 1.0586
Training Epoch: 18 [11776/50048]	Loss: 1.1126
Training Epoch: 18 [11904/50048]	Loss: 1.1794
Training Epoch: 18 [12032/50048]	Loss: 1.0622
Training Epoch: 18 [12160/50048]	Loss: 1.0123
Training Epoch: 18 [12288/50048]	Loss: 1.0792
Training Epoch: 18 [12416/50048]	Loss: 1.1510
Training Epoch: 18 [12544/50048]	Loss: 1.1857
Training Epoch: 18 [12672/50048]	Loss: 1.1141
Training Epoch: 18 [12800/50048]	Loss: 1.1579
Training Epoch: 18 [12928/50048]	Loss: 1.1050
Training Epoch: 18 [13056/50048]	Loss: 1.1038
Training Epoch: 18 [13184/50048]	Loss: 0.9688
Training Epoch: 18 [13312/50048]	Loss: 0.9894
Training Epoch: 18 [13440/50048]	Loss: 1.3342
Training Epoch: 18 [13568/50048]	Loss: 1.2208
Training Epoch: 18 [13696/50048]	Loss: 1.0399
Training Epoch: 18 [13824/50048]	Loss: 0.9686
Training Epoch: 18 [13952/50048]	Loss: 1.1374
Training Epoch: 18 [14080/50048]	Loss: 1.1436
Training Epoch: 18 [14208/50048]	Loss: 0.8321
Training Epoch: 18 [14336/50048]	Loss: 0.9418
Training Epoch: 18 [14464/50048]	Loss: 1.1539
Training Epoch: 18 [14592/50048]	Loss: 1.5062
Training Epoch: 18 [14720/50048]	Loss: 0.9789
Training Epoch: 18 [14848/50048]	Loss: 1.2106
Training Epoch: 18 [14976/50048]	Loss: 1.0199
Training Epoch: 18 [15104/50048]	Loss: 1.4051
Training Epoch: 18 [15232/50048]	Loss: 1.0390
Training Epoch: 18 [15360/50048]	Loss: 1.2117
Training Epoch: 18 [15488/50048]	Loss: 1.1494
Training Epoch: 18 [15616/50048]	Loss: 1.0344
Training Epoch: 18 [15744/50048]	Loss: 1.1344
Training Epoch: 18 [15872/50048]	Loss: 1.1250
Training Epoch: 18 [16000/50048]	Loss: 1.0674
Training Epoch: 18 [16128/50048]	Loss: 0.8128
Training Epoch: 18 [16256/50048]	Loss: 1.0544
Training Epoch: 18 [16384/50048]	Loss: 1.1655
Training Epoch: 18 [16512/50048]	Loss: 1.0685
Training Epoch: 18 [16640/50048]	Loss: 1.2174
Training Epoch: 18 [16768/50048]	Loss: 0.9405
Training Epoch: 18 [16896/50048]	Loss: 1.0971
Training Epoch: 18 [17024/50048]	Loss: 1.1036
Training Epoch: 18 [17152/50048]	Loss: 1.1060
Training Epoch: 18 [17280/50048]	Loss: 0.9460
Training Epoch: 18 [17408/50048]	Loss: 1.0733
Training Epoch: 18 [17536/50048]	Loss: 1.0102
Training Epoch: 18 [17664/50048]	Loss: 1.0817
Training Epoch: 18 [17792/50048]	Loss: 1.1913
Training Epoch: 18 [17920/50048]	Loss: 1.0455
Training Epoch: 18 [18048/50048]	Loss: 1.0794
Training Epoch: 18 [18176/50048]	Loss: 1.2202
Training Epoch: 18 [18304/50048]	Loss: 1.1617
Training Epoch: 18 [18432/50048]	Loss: 1.2567
Training Epoch: 18 [18560/50048]	Loss: 1.2122
Training Epoch: 18 [18688/50048]	Loss: 1.2979
Training Epoch: 18 [18816/50048]	Loss: 0.9555
Training Epoch: 18 [18944/50048]	Loss: 1.0152
Training Epoch: 18 [19072/50048]	Loss: 1.2110
Training Epoch: 18 [19200/50048]	Loss: 1.2415
Training Epoch: 18 [19328/50048]	Loss: 0.9629
Training Epoch: 18 [19456/50048]	Loss: 1.0443
Training Epoch: 18 [19584/50048]	Loss: 1.0492
Training Epoch: 18 [19712/50048]	Loss: 1.1660
Training Epoch: 18 [19840/50048]	Loss: 1.0890
Training Epoch: 18 [19968/50048]	Loss: 1.0871
Training Epoch: 18 [20096/50048]	Loss: 0.9360
Training Epoch: 18 [20224/50048]	Loss: 1.0717
Training Epoch: 18 [20352/50048]	Loss: 1.0038
Training Epoch: 18 [20480/50048]	Loss: 0.9871
Training Epoch: 18 [20608/50048]	Loss: 1.0173
Training Epoch: 18 [20736/50048]	Loss: 1.0768
Training Epoch: 18 [20864/50048]	Loss: 1.2109
Training Epoch: 18 [20992/50048]	Loss: 0.9269
Training Epoch: 18 [21120/50048]	Loss: 0.9750
Training Epoch: 18 [21248/50048]	Loss: 1.3060
Training Epoch: 18 [21376/50048]	Loss: 1.1407
Training Epoch: 18 [21504/50048]	Loss: 1.1371
Training Epoch: 18 [21632/50048]	Loss: 1.0994
Training Epoch: 18 [21760/50048]	Loss: 1.0830
Training Epoch: 18 [21888/50048]	Loss: 1.1526
Training Epoch: 18 [22016/50048]	Loss: 1.0399
Training Epoch: 18 [22144/50048]	Loss: 0.9112
Training Epoch: 18 [22272/50048]	Loss: 1.1227
Training Epoch: 18 [22400/50048]	Loss: 1.2313
Training Epoch: 18 [22528/50048]	Loss: 1.1172
Training Epoch: 18 [22656/50048]	Loss: 0.9831
Training Epoch: 18 [22784/50048]	Loss: 0.9008
Training Epoch: 18 [22912/50048]	Loss: 1.1370
Training Epoch: 18 [23040/50048]	Loss: 1.0122
Training Epoch: 18 [23168/50048]	Loss: 1.1813
Training Epoch: 18 [23296/50048]	Loss: 1.1529
Training Epoch: 18 [23424/50048]	Loss: 0.8028
Training Epoch: 18 [23552/50048]	Loss: 0.8578
Training Epoch: 18 [23680/50048]	Loss: 1.2078
Training Epoch: 18 [23808/50048]	Loss: 0.8898
Training Epoch: 18 [23936/50048]	Loss: 1.0180
Training Epoch: 18 [24064/50048]	Loss: 1.1953
Training Epoch: 18 [24192/50048]	Loss: 0.8374
Training Epoch: 18 [24320/50048]	Loss: 0.7658
Training Epoch: 18 [24448/50048]	Loss: 0.9108
Training Epoch: 18 [24576/50048]	Loss: 1.3639
Training Epoch: 18 [24704/50048]	Loss: 0.9881
Training Epoch: 18 [24832/50048]	Loss: 0.9956
Training Epoch: 18 [24960/50048]	Loss: 1.2466
Training Epoch: 18 [25088/50048]	Loss: 1.1850
Training Epoch: 18 [25216/50048]	Loss: 0.9090
Training Epoch: 18 [25344/50048]	Loss: 1.1565
Training Epoch: 18 [25472/50048]	Loss: 0.8974
Training Epoch: 18 [25600/50048]	Loss: 1.0004
Training Epoch: 18 [25728/50048]	Loss: 1.1063
Training Epoch: 18 [25856/50048]	Loss: 0.7690
Training Epoch: 18 [25984/50048]	Loss: 1.0359
Training Epoch: 18 [26112/50048]	Loss: 1.0716
Training Epoch: 18 [26240/50048]	Loss: 0.9997
Training Epoch: 18 [26368/50048]	Loss: 1.0495
Training Epoch: 18 [26496/50048]	Loss: 1.0357
Training Epoch: 18 [26624/50048]	Loss: 1.0528
Training Epoch: 18 [26752/50048]	Loss: 1.1639
Training Epoch: 18 [26880/50048]	Loss: 1.0880
Training Epoch: 18 [27008/50048]	Loss: 0.9246
Training Epoch: 18 [27136/50048]	Loss: 1.1931
Training Epoch: 18 [27264/50048]	Loss: 1.0009
Training Epoch: 18 [27392/50048]	Loss: 1.2223
Training Epoch: 18 [27520/50048]	Loss: 1.0188
Training Epoch: 18 [27648/50048]	Loss: 1.1133
Training Epoch: 18 [27776/50048]	Loss: 1.0922
Training Epoch: 18 [27904/50048]	Loss: 1.0291
Training Epoch: 18 [28032/50048]	Loss: 1.0924
Training Epoch: 18 [28160/50048]	Loss: 1.1202
Training Epoch: 18 [28288/50048]	Loss: 1.0195
Training Epoch: 18 [28416/50048]	Loss: 1.1269
Training Epoch: 18 [28544/50048]	Loss: 1.0157
Training Epoch: 18 [28672/50048]	Loss: 1.0782
Training Epoch: 18 [28800/50048]	Loss: 1.2517
Training Epoch: 18 [28928/50048]	Loss: 1.0163
Training Epoch: 18 [29056/50048]	Loss: 1.1336
Training Epoch: 18 [29184/50048]	Loss: 1.0434
Training Epoch: 18 [29312/50048]	Loss: 1.0729
Training Epoch: 18 [29440/50048]	Loss: 1.0615
Training Epoch: 18 [29568/50048]	Loss: 1.2011
Training Epoch: 18 [29696/50048]	Loss: 1.1556
Training Epoch: 18 [29824/50048]	Loss: 1.0310
Training Epoch: 18 [29952/50048]	Loss: 1.2568
Training Epoch: 18 [30080/50048]	Loss: 1.2044
Training Epoch: 18 [30208/50048]	Loss: 0.9394
Training Epoch: 18 [30336/50048]	Loss: 1.0948
Training Epoch: 18 [30464/50048]	Loss: 1.1324
Training Epoch: 18 [30592/50048]	Loss: 1.1088
Training Epoch: 18 [30720/50048]	Loss: 1.0228
Training Epoch: 18 [30848/50048]	Loss: 1.1509
Training Epoch: 18 [30976/50048]	Loss: 1.0590
Training Epoch: 18 [31104/50048]	Loss: 1.3131
Training Epoch: 18 [31232/50048]	Loss: 1.2374
Training Epoch: 18 [31360/50048]	Loss: 1.3266
Training Epoch: 18 [31488/50048]	Loss: 1.3398
Training Epoch: 18 [31616/50048]	Loss: 1.1896
Training Epoch: 18 [31744/50048]	Loss: 1.0544
Training Epoch: 18 [31872/50048]	Loss: 1.1492
Training Epoch: 18 [32000/50048]	Loss: 1.1303
Training Epoch: 18 [32128/50048]	Loss: 0.9882
Training Epoch: 18 [32256/50048]	Loss: 0.9866
Training Epoch: 18 [32384/50048]	Loss: 1.0924
Training Epoch: 18 [32512/50048]	Loss: 0.7317
Training Epoch: 18 [32640/50048]	Loss: 1.1457
Training Epoch: 18 [32768/50048]	Loss: 1.0757
Training Epoch: 18 [32896/50048]	Loss: 1.0458
Training Epoch: 18 [33024/50048]	Loss: 1.0962
Training Epoch: 18 [33152/50048]	Loss: 1.1365
Training Epoch: 18 [33280/50048]	Loss: 1.0781
Training Epoch: 18 [33408/50048]	Loss: 0.9152
Training Epoch: 18 [33536/50048]	Loss: 1.2637
Training Epoch: 18 [33664/50048]	Loss: 1.1179
Training Epoch: 18 [33792/50048]	Loss: 1.0620
Training Epoch: 18 [33920/50048]	Loss: 1.2175
Training Epoch: 18 [34048/50048]	Loss: 0.9448
Training Epoch: 18 [34176/50048]	Loss: 1.2422
Training Epoch: 18 [34304/50048]	Loss: 1.0152
Training Epoch: 18 [34432/50048]	Loss: 0.8977
Training Epoch: 18 [34560/50048]	Loss: 0.9903
Training Epoch: 18 [34688/50048]	Loss: 1.1611
Training Epoch: 18 [34816/50048]	Loss: 1.0789
Training Epoch: 18 [34944/50048]	Loss: 1.0911
Training Epoch: 18 [35072/50048]	Loss: 1.1686
Training Epoch: 18 [35200/50048]	Loss: 1.0317
Training Epoch: 18 [35328/50048]	Loss: 1.1068
Training Epoch: 18 [35456/50048]	Loss: 1.0916
Training Epoch: 18 [35584/50048]	Loss: 1.2801
Training Epoch: 18 [35712/50048]	Loss: 1.0202
Training Epoch: 18 [35840/50048]	Loss: 0.9753
Training Epoch: 18 [35968/50048]	Loss: 0.9571
Training Epoch: 18 [36096/50048]	Loss: 0.9786
Training Epoch: 18 [36224/50048]	Loss: 1.0645
Training Epoch: 18 [36352/50048]	Loss: 1.3575
Training Epoch: 18 [36480/50048]	Loss: 1.0953
Training Epoch: 18 [36608/50048]	Loss: 1.1896
Training Epoch: 18 [36736/50048]	Loss: 0.9589
Training Epoch: 18 [36864/50048]	Loss: 1.2363
Training Epoch: 18 [36992/50048]	Loss: 1.2144
Training Epoch: 18 [37120/50048]	Loss: 1.0741
Training Epoch: 18 [37248/50048]	Loss: 0.9989
Training Epoch: 18 [37376/50048]	Loss: 1.1605
Training Epoch: 18 [37504/50048]	Loss: 1.0749
Training Epoch: 18 [37632/50048]	Loss: 1.0871
Training Epoch: 18 [37760/50048]	Loss: 1.1616
Training Epoch: 18 [37888/50048]	Loss: 1.0349
Training Epoch: 18 [38016/50048]	Loss: 1.0837
Training Epoch: 18 [38144/50048]	Loss: 1.4057
Training Epoch: 18 [38272/50048]	Loss: 1.0626
Training Epoch: 18 [38400/50048]	Loss: 1.1544
Training Epoch: 18 [38528/50048]	Loss: 1.0863
Training Epoch: 18 [38656/50048]	Loss: 1.3571
Training Epoch: 18 [38784/50048]	Loss: 1.0581
Training Epoch: 18 [38912/50048]	Loss: 1.1816
Training Epoch: 18 [39040/50048]	Loss: 1.0277
Training Epoch: 18 [39168/50048]	Loss: 1.2027
Training Epoch: 18 [39296/50048]	Loss: 1.2069
Training Epoch: 18 [39424/50048]	Loss: 1.3324
Training Epoch: 18 [39552/50048]	Loss: 1.1961
Training Epoch: 18 [39680/50048]	Loss: 1.1210
Training Epoch: 18 [39808/50048]	Loss: 1.1954
Training Epoch: 18 [39936/50048]	Loss: 0.8871
Training Epoch: 18 [40064/50048]	Loss: 1.0258
Training Epoch: 18 [40192/50048]	Loss: 0.8559
Training Epoch: 18 [40320/50048]	Loss: 1.0055
Training Epoch: 18 [40448/50048]	Loss: 0.9318
Training Epoch: 18 [40576/50048]	Loss: 1.1538
Training Epoch: 18 [40704/50048]	Loss: 0.9158
Training Epoch: 18 [40832/50048]	Loss: 0.9661
Training Epoch: 18 [40960/50048]	Loss: 1.2745
Training Epoch: 18 [41088/50048]	Loss: 1.2920
Training Epoch: 18 [41216/50048]	Loss: 1.1051
Training Epoch: 18 [41344/50048]	Loss: 1.3226
Training Epoch: 18 [41472/50048]	Loss: 1.0783
Training Epoch: 18 [41600/50048]	Loss: 1.1098
Training Epoch: 18 [41728/50048]	Loss: 1.3604
Training Epoch: 18 [41856/50048]	Loss: 0.9618
Training Epoch: 18 [41984/50048]	Loss: 1.3388
Training Epoch: 18 [42112/50048]	Loss: 1.1469
Training Epoch: 18 [42240/50048]	Loss: 1.2024
Training Epoch: 18 [42368/50048]	Loss: 0.8303
Training Epoch: 18 [42496/50048]	Loss: 1.0550
Training Epoch: 18 [42624/50048]	Loss: 1.2685
Training Epoch: 18 [42752/50048]	Loss: 1.0357
Training Epoch: 18 [42880/50048]	Loss: 0.9404
Training Epoch: 18 [43008/50048]	Loss: 1.0947
Training Epoch: 18 [43136/50048]	Loss: 1.0934
Training Epoch: 18 [43264/50048]	Loss: 1.1185
Training Epoch: 18 [43392/50048]	Loss: 1.1394
Training Epoch: 18 [43520/50048]	Loss: 0.9935
Training Epoch: 18 [43648/50048]	Loss: 0.9431
Training Epoch: 18 [43776/50048]	Loss: 1.3654
Training Epoch: 18 [43904/50048]	Loss: 1.3368
Training Epoch: 18 [44032/50048]	Loss: 0.9529
Training Epoch: 18 [44160/50048]	Loss: 1.1044
Training Epoch: 18 [44288/50048]	Loss: 1.0228
Training Epoch: 18 [44416/50048]	Loss: 0.9241
Training Epoch: 18 [44544/50048]	Loss: 1.0773
Training Epoch: 18 [44672/50048]	Loss: 1.2055
Training Epoch: 18 [44800/50048]	Loss: 1.2702
Training Epoch: 18 [44928/50048]	Loss: 1.1487
Training Epoch: 18 [45056/50048]	Loss: 1.0244
Training Epoch: 18 [45184/50048]	Loss: 1.0488
Training Epoch: 18 [45312/50048]	Loss: 1.0722
Training Epoch: 18 [45440/50048]	Loss: 1.1044
Training Epoch: 18 [45568/50048]	Loss: 1.2350
Training Epoch: 18 [45696/50048]	Loss: 0.9492
2022-12-06 04:05:55,294 [ZeusDataLoader(train)] train epoch 19 done: time=86.39 energy=10501.17
2022-12-06 04:05:55,295 [ZeusDataLoader(eval)] Epoch 19 begin.
Training Epoch: 18 [45824/50048]	Loss: 1.3023
Training Epoch: 18 [45952/50048]	Loss: 1.1267
Training Epoch: 18 [46080/50048]	Loss: 1.2132
Training Epoch: 18 [46208/50048]	Loss: 0.9914
Training Epoch: 18 [46336/50048]	Loss: 1.0627
Training Epoch: 18 [46464/50048]	Loss: 0.9982
Training Epoch: 18 [46592/50048]	Loss: 1.0381
Training Epoch: 18 [46720/50048]	Loss: 1.0465
Training Epoch: 18 [46848/50048]	Loss: 1.1730
Training Epoch: 18 [46976/50048]	Loss: 1.2915
Training Epoch: 18 [47104/50048]	Loss: 0.9030
Training Epoch: 18 [47232/50048]	Loss: 1.1984
Training Epoch: 18 [47360/50048]	Loss: 0.9560
Training Epoch: 18 [47488/50048]	Loss: 1.1040
Training Epoch: 18 [47616/50048]	Loss: 0.8578
Training Epoch: 18 [47744/50048]	Loss: 1.2053
Training Epoch: 18 [47872/50048]	Loss: 0.9979
Training Epoch: 18 [48000/50048]	Loss: 1.1497
Training Epoch: 18 [48128/50048]	Loss: 1.0489
Training Epoch: 18 [48256/50048]	Loss: 1.1362
Training Epoch: 18 [48384/50048]	Loss: 1.3457
Training Epoch: 18 [48512/50048]	Loss: 1.1923
Training Epoch: 18 [48640/50048]	Loss: 1.0174
Training Epoch: 18 [48768/50048]	Loss: 1.0113
Training Epoch: 18 [48896/50048]	Loss: 1.1046
Training Epoch: 18 [49024/50048]	Loss: 0.9136
Training Epoch: 18 [49152/50048]	Loss: 1.4223
Training Epoch: 18 [49280/50048]	Loss: 1.2197
Training Epoch: 18 [49408/50048]	Loss: 1.1595
Training Epoch: 18 [49536/50048]	Loss: 1.0117
Training Epoch: 18 [49664/50048]	Loss: 1.1024
Training Epoch: 18 [49792/50048]	Loss: 1.1620
Training Epoch: 18 [49920/50048]	Loss: 1.0660
Training Epoch: 18 [50048/50048]	Loss: 1.5008
2022-12-06 09:05:58.983 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:05:58,995 [ZeusDataLoader(eval)] eval epoch 19 done: time=3.69 energy=454.61
2022-12-06 04:05:58,995 [ZeusDataLoader(train)] Up to epoch 19: time=1716.36, energy=208065.42, cost=254214.46
2022-12-06 04:05:58,995 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:05:58,995 [ZeusDataLoader(train)] Expected next epoch: time=1806.16, energy=218863.43, cost=267470.84
2022-12-06 04:05:58,996 [ZeusDataLoader(train)] Epoch 20 begin.
Validation Epoch: 18, Average loss: 0.0124, Accuracy: 0.5806
2022-12-06 04:05:59,178 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:05:59,178 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:05:59.180 [ZeusMonitor] Monitor started.
2022-12-06 09:05:59.180 [ZeusMonitor] Running indefinitely. 2022-12-06 09:05:59.180 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:05:59.180 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e20+gpu0.power.log
Training Epoch: 19 [128/50048]	Loss: 1.0123
Training Epoch: 19 [256/50048]	Loss: 0.9777
Training Epoch: 19 [384/50048]	Loss: 0.9522
Training Epoch: 19 [512/50048]	Loss: 0.7640
Training Epoch: 19 [640/50048]	Loss: 1.0855
Training Epoch: 19 [768/50048]	Loss: 0.9284
Training Epoch: 19 [896/50048]	Loss: 1.0998
Training Epoch: 19 [1024/50048]	Loss: 1.0118
Training Epoch: 19 [1152/50048]	Loss: 0.9965
Training Epoch: 19 [1280/50048]	Loss: 1.1078
Training Epoch: 19 [1408/50048]	Loss: 1.0079
Training Epoch: 19 [1536/50048]	Loss: 1.0638
Training Epoch: 19 [1664/50048]	Loss: 1.2041
Training Epoch: 19 [1792/50048]	Loss: 1.0872
Training Epoch: 19 [1920/50048]	Loss: 1.1045
Training Epoch: 19 [2048/50048]	Loss: 0.8251
Training Epoch: 19 [2176/50048]	Loss: 0.8769
Training Epoch: 19 [2304/50048]	Loss: 1.0022
Training Epoch: 19 [2432/50048]	Loss: 0.7859
Training Epoch: 19 [2560/50048]	Loss: 0.9919
Training Epoch: 19 [2688/50048]	Loss: 0.9339
Training Epoch: 19 [2816/50048]	Loss: 0.9796
Training Epoch: 19 [2944/50048]	Loss: 1.1341
Training Epoch: 19 [3072/50048]	Loss: 0.7771
Training Epoch: 19 [3200/50048]	Loss: 1.1417
Training Epoch: 19 [3328/50048]	Loss: 1.1935
Training Epoch: 19 [3456/50048]	Loss: 1.1679
Training Epoch: 19 [3584/50048]	Loss: 0.8306
Training Epoch: 19 [3712/50048]	Loss: 0.9679
Training Epoch: 19 [3840/50048]	Loss: 0.8304
Training Epoch: 19 [3968/50048]	Loss: 1.2034
Training Epoch: 19 [4096/50048]	Loss: 1.0086
Training Epoch: 19 [4224/50048]	Loss: 0.9782
Training Epoch: 19 [4352/50048]	Loss: 1.0293
Training Epoch: 19 [4480/50048]	Loss: 0.9382
Training Epoch: 19 [4608/50048]	Loss: 1.0178
Training Epoch: 19 [4736/50048]	Loss: 0.7765
Training Epoch: 19 [4864/50048]	Loss: 1.1520
Training Epoch: 19 [4992/50048]	Loss: 1.1550
Training Epoch: 19 [5120/50048]	Loss: 1.1534
Training Epoch: 19 [5248/50048]	Loss: 0.8483
Training Epoch: 19 [5376/50048]	Loss: 0.9823
Training Epoch: 19 [5504/50048]	Loss: 0.9540
Training Epoch: 19 [5632/50048]	Loss: 1.2947
Training Epoch: 19 [5760/50048]	Loss: 0.9492
Training Epoch: 19 [5888/50048]	Loss: 1.2916
Training Epoch: 19 [6016/50048]	Loss: 1.0307
Training Epoch: 19 [6144/50048]	Loss: 0.9845
Training Epoch: 19 [6272/50048]	Loss: 1.0409
Training Epoch: 19 [6400/50048]	Loss: 1.0039
Training Epoch: 19 [6528/50048]	Loss: 1.0664
Training Epoch: 19 [6656/50048]	Loss: 1.0084
Training Epoch: 19 [6784/50048]	Loss: 0.8792
Training Epoch: 19 [6912/50048]	Loss: 0.9903
Training Epoch: 19 [7040/50048]	Loss: 1.0173
Training Epoch: 19 [7168/50048]	Loss: 1.0246
Training Epoch: 19 [7296/50048]	Loss: 0.8236
Training Epoch: 19 [7424/50048]	Loss: 1.0017
Training Epoch: 19 [7552/50048]	Loss: 1.1434
Training Epoch: 19 [7680/50048]	Loss: 1.0527
Training Epoch: 19 [7808/50048]	Loss: 1.0127
Training Epoch: 19 [7936/50048]	Loss: 1.0536
Training Epoch: 19 [8064/50048]	Loss: 0.8912
Training Epoch: 19 [8192/50048]	Loss: 0.8947
Training Epoch: 19 [8320/50048]	Loss: 1.0447
Training Epoch: 19 [8448/50048]	Loss: 0.8013
Training Epoch: 19 [8576/50048]	Loss: 1.0608
Training Epoch: 19 [8704/50048]	Loss: 0.9390
Training Epoch: 19 [8832/50048]	Loss: 1.0347
Training Epoch: 19 [8960/50048]	Loss: 0.9542
Training Epoch: 19 [9088/50048]	Loss: 0.9632
Training Epoch: 19 [9216/50048]	Loss: 1.0038
Training Epoch: 19 [9344/50048]	Loss: 1.0105
Training Epoch: 19 [9472/50048]	Loss: 1.1338
Training Epoch: 19 [9600/50048]	Loss: 0.9554
Training Epoch: 19 [9728/50048]	Loss: 1.0063
Training Epoch: 19 [9856/50048]	Loss: 1.0348
Training Epoch: 19 [9984/50048]	Loss: 1.2030
Training Epoch: 19 [10112/50048]	Loss: 1.0382
Training Epoch: 19 [10240/50048]	Loss: 1.0457
Training Epoch: 19 [10368/50048]	Loss: 1.1239
Training Epoch: 19 [10496/50048]	Loss: 0.6553
Training Epoch: 19 [10624/50048]	Loss: 1.0933
Training Epoch: 19 [10752/50048]	Loss: 1.0990
Training Epoch: 19 [10880/50048]	Loss: 0.9030
Training Epoch: 19 [11008/50048]	Loss: 0.8997
Training Epoch: 19 [11136/50048]	Loss: 1.2261
Training Epoch: 19 [11264/50048]	Loss: 0.9786
Training Epoch: 19 [11392/50048]	Loss: 0.8809
Training Epoch: 19 [11520/50048]	Loss: 1.0948
Training Epoch: 19 [11648/50048]	Loss: 0.9681
Training Epoch: 19 [11776/50048]	Loss: 0.9925
Training Epoch: 19 [11904/50048]	Loss: 0.8903
Training Epoch: 19 [12032/50048]	Loss: 1.2000
Training Epoch: 19 [12160/50048]	Loss: 0.9701
Training Epoch: 19 [12288/50048]	Loss: 1.0041
Training Epoch: 19 [12416/50048]	Loss: 1.0327
Training Epoch: 19 [12544/50048]	Loss: 0.9485
Training Epoch: 19 [12672/50048]	Loss: 1.0084
Training Epoch: 19 [12800/50048]	Loss: 1.0325
Training Epoch: 19 [12928/50048]	Loss: 1.0383
Training Epoch: 19 [13056/50048]	Loss: 1.0269
Training Epoch: 19 [13184/50048]	Loss: 0.8815
Training Epoch: 19 [13312/50048]	Loss: 0.8692
Training Epoch: 19 [13440/50048]	Loss: 1.0039
Training Epoch: 19 [13568/50048]	Loss: 0.9110
Training Epoch: 19 [13696/50048]	Loss: 1.1454
Training Epoch: 19 [13824/50048]	Loss: 1.1135
Training Epoch: 19 [13952/50048]	Loss: 1.1645
Training Epoch: 19 [14080/50048]	Loss: 1.1541
Training Epoch: 19 [14208/50048]	Loss: 1.0187
Training Epoch: 19 [14336/50048]	Loss: 0.9047
Training Epoch: 19 [14464/50048]	Loss: 0.9273
Training Epoch: 19 [14592/50048]	Loss: 1.0776
Training Epoch: 19 [14720/50048]	Loss: 1.1125
Training Epoch: 19 [14848/50048]	Loss: 1.1409
Training Epoch: 19 [14976/50048]	Loss: 0.8576
Training Epoch: 19 [15104/50048]	Loss: 1.1489
Training Epoch: 19 [15232/50048]	Loss: 1.0253
Training Epoch: 19 [15360/50048]	Loss: 1.0323
Training Epoch: 19 [15488/50048]	Loss: 1.2092
Training Epoch: 19 [15616/50048]	Loss: 0.7723
Training Epoch: 19 [15744/50048]	Loss: 0.9301
Training Epoch: 19 [15872/50048]	Loss: 1.1200
Training Epoch: 19 [16000/50048]	Loss: 1.0663
Training Epoch: 19 [16128/50048]	Loss: 0.8512
Training Epoch: 19 [16256/50048]	Loss: 1.0026
Training Epoch: 19 [16384/50048]	Loss: 0.9499
Training Epoch: 19 [16512/50048]	Loss: 1.0530
Training Epoch: 19 [16640/50048]	Loss: 1.0207
Training Epoch: 19 [16768/50048]	Loss: 0.9789
Training Epoch: 19 [16896/50048]	Loss: 0.9908
Training Epoch: 19 [17024/50048]	Loss: 1.0212
Training Epoch: 19 [17152/50048]	Loss: 1.1101
Training Epoch: 19 [17280/50048]	Loss: 1.0707
Training Epoch: 19 [17408/50048]	Loss: 1.1437
Training Epoch: 19 [17536/50048]	Loss: 0.8152
Training Epoch: 19 [17664/50048]	Loss: 1.2060
Training Epoch: 19 [17792/50048]	Loss: 0.9400
Training Epoch: 19 [17920/50048]	Loss: 1.0162
Training Epoch: 19 [18048/50048]	Loss: 0.9351
Training Epoch: 19 [18176/50048]	Loss: 1.0767
Training Epoch: 19 [18304/50048]	Loss: 1.1039
Training Epoch: 19 [18432/50048]	Loss: 1.2322
Training Epoch: 19 [18560/50048]	Loss: 1.0211
Training Epoch: 19 [18688/50048]	Loss: 0.9860
Training Epoch: 19 [18816/50048]	Loss: 0.9397
Training Epoch: 19 [18944/50048]	Loss: 0.9438
Training Epoch: 19 [19072/50048]	Loss: 0.9485
Training Epoch: 19 [19200/50048]	Loss: 1.0688
Training Epoch: 19 [19328/50048]	Loss: 0.9266
Training Epoch: 19 [19456/50048]	Loss: 1.0210
Training Epoch: 19 [19584/50048]	Loss: 1.0919
Training Epoch: 19 [19712/50048]	Loss: 1.0077
Training Epoch: 19 [19840/50048]	Loss: 0.8948
Training Epoch: 19 [19968/50048]	Loss: 1.1853
Training Epoch: 19 [20096/50048]	Loss: 1.0709
Training Epoch: 19 [20224/50048]	Loss: 1.0661
Training Epoch: 19 [20352/50048]	Loss: 1.0202
Training Epoch: 19 [20480/50048]	Loss: 0.9503
Training Epoch: 19 [20608/50048]	Loss: 1.0512
Training Epoch: 19 [20736/50048]	Loss: 0.9940
Training Epoch: 19 [20864/50048]	Loss: 0.9798
Training Epoch: 19 [20992/50048]	Loss: 1.2395
Training Epoch: 19 [21120/50048]	Loss: 0.7927
Training Epoch: 19 [21248/50048]	Loss: 1.0405
Training Epoch: 19 [21376/50048]	Loss: 1.0273
Training Epoch: 19 [21504/50048]	Loss: 0.9982
Training Epoch: 19 [21632/50048]	Loss: 1.4535
Training Epoch: 19 [21760/50048]	Loss: 1.0794
Training Epoch: 19 [21888/50048]	Loss: 1.1994
Training Epoch: 19 [22016/50048]	Loss: 1.2715
Training Epoch: 19 [22144/50048]	Loss: 1.0200
Training Epoch: 19 [22272/50048]	Loss: 0.9402
Training Epoch: 19 [22400/50048]	Loss: 0.9063
Training Epoch: 19 [22528/50048]	Loss: 0.9416
Training Epoch: 19 [22656/50048]	Loss: 1.2514
Training Epoch: 19 [22784/50048]	Loss: 0.8808
Training Epoch: 19 [22912/50048]	Loss: 1.0473
Training Epoch: 19 [23040/50048]	Loss: 1.0005
Training Epoch: 19 [23168/50048]	Loss: 1.0891
Training Epoch: 19 [23296/50048]	Loss: 1.1467
Training Epoch: 19 [23424/50048]	Loss: 1.3044
Training Epoch: 19 [23552/50048]	Loss: 1.0798
Training Epoch: 19 [23680/50048]	Loss: 1.0888
Training Epoch: 19 [23808/50048]	Loss: 1.1359
Training Epoch: 19 [23936/50048]	Loss: 0.8432
Training Epoch: 19 [24064/50048]	Loss: 1.0163
Training Epoch: 19 [24192/50048]	Loss: 1.0249
Training Epoch: 19 [24320/50048]	Loss: 1.0775
Training Epoch: 19 [24448/50048]	Loss: 1.1439
Training Epoch: 19 [24576/50048]	Loss: 0.9297
Training Epoch: 19 [24704/50048]	Loss: 1.0252
Training Epoch: 19 [24832/50048]	Loss: 1.0811
Training Epoch: 19 [24960/50048]	Loss: 0.9789
Training Epoch: 19 [25088/50048]	Loss: 1.1310
Training Epoch: 19 [25216/50048]	Loss: 1.0711
Training Epoch: 19 [25344/50048]	Loss: 1.0805
Training Epoch: 19 [25472/50048]	Loss: 0.9628
Training Epoch: 19 [25600/50048]	Loss: 1.0120
Training Epoch: 19 [25728/50048]	Loss: 1.3259
Training Epoch: 19 [25856/50048]	Loss: 1.0509
Training Epoch: 19 [25984/50048]	Loss: 1.0858
Training Epoch: 19 [26112/50048]	Loss: 1.1114
Training Epoch: 19 [26240/50048]	Loss: 0.9867
Training Epoch: 19 [26368/50048]	Loss: 0.8356
Training Epoch: 19 [26496/50048]	Loss: 0.9943
Training Epoch: 19 [26624/50048]	Loss: 0.9934
Training Epoch: 19 [26752/50048]	Loss: 0.9400
Training Epoch: 19 [26880/50048]	Loss: 1.0781
Training Epoch: 19 [27008/50048]	Loss: 0.9748
Training Epoch: 19 [27136/50048]	Loss: 1.0222
Training Epoch: 19 [27264/50048]	Loss: 0.8893
Training Epoch: 19 [27392/50048]	Loss: 1.1086
Training Epoch: 19 [27520/50048]	Loss: 1.0800
Training Epoch: 19 [27648/50048]	Loss: 0.9667
Training Epoch: 19 [27776/50048]	Loss: 1.1259
Training Epoch: 19 [27904/50048]	Loss: 1.0311
Training Epoch: 19 [28032/50048]	Loss: 1.2143
Training Epoch: 19 [28160/50048]	Loss: 1.0451
Training Epoch: 19 [28288/50048]	Loss: 0.9758
Training Epoch: 19 [28416/50048]	Loss: 0.8042
Training Epoch: 19 [28544/50048]	Loss: 1.2777
Training Epoch: 19 [28672/50048]	Loss: 0.9799
Training Epoch: 19 [28800/50048]	Loss: 0.9551
Training Epoch: 19 [28928/50048]	Loss: 1.1163
Training Epoch: 19 [29056/50048]	Loss: 1.0919
Training Epoch: 19 [29184/50048]	Loss: 1.1070
Training Epoch: 19 [29312/50048]	Loss: 1.1220
Training Epoch: 19 [29440/50048]	Loss: 0.9891
Training Epoch: 19 [29568/50048]	Loss: 1.0371
Training Epoch: 19 [29696/50048]	Loss: 1.0856
Training Epoch: 19 [29824/50048]	Loss: 1.4077
Training Epoch: 19 [29952/50048]	Loss: 1.1204
Training Epoch: 19 [30080/50048]	Loss: 1.0572
Training Epoch: 19 [30208/50048]	Loss: 1.0969
Training Epoch: 19 [30336/50048]	Loss: 1.1073
Training Epoch: 19 [30464/50048]	Loss: 1.1615
Training Epoch: 19 [30592/50048]	Loss: 0.8947
Training Epoch: 19 [30720/50048]	Loss: 0.9038
Training Epoch: 19 [30848/50048]	Loss: 1.0030
Training Epoch: 19 [30976/50048]	Loss: 1.0023
Training Epoch: 19 [31104/50048]	Loss: 1.3037
Training Epoch: 19 [31232/50048]	Loss: 0.9006
Training Epoch: 19 [31360/50048]	Loss: 1.1173
Training Epoch: 19 [31488/50048]	Loss: 1.0309
Training Epoch: 19 [31616/50048]	Loss: 1.1418
Training Epoch: 19 [31744/50048]	Loss: 1.0115
Training Epoch: 19 [31872/50048]	Loss: 1.1533
Training Epoch: 19 [32000/50048]	Loss: 0.9993
Training Epoch: 19 [32128/50048]	Loss: 1.0463
Training Epoch: 19 [32256/50048]	Loss: 1.0791
Training Epoch: 19 [32384/50048]	Loss: 1.1566
Training Epoch: 19 [32512/50048]	Loss: 1.3007
Training Epoch: 19 [32640/50048]	Loss: 1.0861
Training Epoch: 19 [32768/50048]	Loss: 1.0711
Training Epoch: 19 [32896/50048]	Loss: 1.1163
Training Epoch: 19 [33024/50048]	Loss: 1.2474
Training Epoch: 19 [33152/50048]	Loss: 1.0473
Training Epoch: 19 [33280/50048]	Loss: 1.1492
Training Epoch: 19 [33408/50048]	Loss: 0.9354
Training Epoch: 19 [33536/50048]	Loss: 0.8047
Training Epoch: 19 [33664/50048]	Loss: 1.1526
Training Epoch: 19 [33792/50048]	Loss: 0.9042
Training Epoch: 19 [33920/50048]	Loss: 0.9307
Training Epoch: 19 [34048/50048]	Loss: 1.1445
Training Epoch: 19 [34176/50048]	Loss: 1.3434
Training Epoch: 19 [34304/50048]	Loss: 1.0977
Training Epoch: 19 [34432/50048]	Loss: 0.9420
Training Epoch: 19 [34560/50048]	Loss: 1.1219
Training Epoch: 19 [34688/50048]	Loss: 0.8502
Training Epoch: 19 [34816/50048]	Loss: 1.1164
Training Epoch: 19 [34944/50048]	Loss: 0.8786
Training Epoch: 19 [35072/50048]	Loss: 1.1066
Training Epoch: 19 [35200/50048]	Loss: 1.0390
Training Epoch: 19 [35328/50048]	Loss: 1.0666
Training Epoch: 19 [35456/50048]	Loss: 1.0886
Training Epoch: 19 [35584/50048]	Loss: 1.0946
Training Epoch: 19 [35712/50048]	Loss: 1.0056
Training Epoch: 19 [35840/50048]	Loss: 0.9814
Training Epoch: 19 [35968/50048]	Loss: 1.0799
Training Epoch: 19 [36096/50048]	Loss: 0.9107
Training Epoch: 19 [36224/50048]	Loss: 1.1413
Training Epoch: 19 [36352/50048]	Loss: 1.0943
Training Epoch: 19 [36480/50048]	Loss: 1.0749
Training Epoch: 19 [36608/50048]	Loss: 1.0187
Training Epoch: 19 [36736/50048]	Loss: 1.0072
Training Epoch: 19 [36864/50048]	Loss: 1.0355
Training Epoch: 19 [36992/50048]	Loss: 1.0118
Training Epoch: 19 [37120/50048]	Loss: 1.0518
Training Epoch: 19 [37248/50048]	Loss: 0.8218
Training Epoch: 19 [37376/50048]	Loss: 0.9550
Training Epoch: 19 [37504/50048]	Loss: 0.7889
Training Epoch: 19 [37632/50048]	Loss: 1.1306
Training Epoch: 19 [37760/50048]	Loss: 1.2664
Training Epoch: 19 [37888/50048]	Loss: 0.9976
Training Epoch: 19 [38016/50048]	Loss: 1.2561
Training Epoch: 19 [38144/50048]	Loss: 1.2350
Training Epoch: 19 [38272/50048]	Loss: 0.9495
Training Epoch: 19 [38400/50048]	Loss: 1.0954
Training Epoch: 19 [38528/50048]	Loss: 1.2522
Training Epoch: 19 [38656/50048]	Loss: 0.8655
Training Epoch: 19 [38784/50048]	Loss: 1.2709
Training Epoch: 19 [38912/50048]	Loss: 1.1775
Training Epoch: 19 [39040/50048]	Loss: 1.1093
Training Epoch: 19 [39168/50048]	Loss: 0.9182
Training Epoch: 19 [39296/50048]	Loss: 0.8313
Training Epoch: 19 [39424/50048]	Loss: 1.1453
Training Epoch: 19 [39552/50048]	Loss: 0.9918
Training Epoch: 19 [39680/50048]	Loss: 1.0509
Training Epoch: 19 [39808/50048]	Loss: 0.9578
Training Epoch: 19 [39936/50048]	Loss: 1.0887
Training Epoch: 19 [40064/50048]	Loss: 1.0547
Training Epoch: 19 [40192/50048]	Loss: 0.9131
Training Epoch: 19 [40320/50048]	Loss: 0.9042
Training Epoch: 19 [40448/50048]	Loss: 1.0502
Training Epoch: 19 [40576/50048]	Loss: 1.0146
Training Epoch: 19 [40704/50048]	Loss: 0.9244
Training Epoch: 19 [40832/50048]	Loss: 0.9066
Training Epoch: 19 [40960/50048]	Loss: 1.1151
Training Epoch: 19 [41088/50048]	Loss: 1.1607
Training Epoch: 19 [41216/50048]	Loss: 0.9750
Training Epoch: 19 [41344/50048]	Loss: 0.9364
Training Epoch: 19 [41472/50048]	Loss: 1.0388
Training Epoch: 19 [41600/50048]	Loss: 1.0094
Training Epoch: 19 [41728/50048]	Loss: 1.0152
Training Epoch: 19 [41856/50048]	Loss: 1.1195
Training Epoch: 19 [41984/50048]	Loss: 1.1004
Training Epoch: 19 [42112/50048]	Loss: 1.2891
Training Epoch: 19 [42240/50048]	Loss: 1.2498
Training Epoch: 19 [42368/50048]	Loss: 1.0861
Training Epoch: 19 [42496/50048]	Loss: 1.0864
Training Epoch: 19 [42624/50048]	Loss: 0.9741
Training Epoch: 19 [42752/50048]	Loss: 0.9643
Training Epoch: 19 [42880/50048]	Loss: 1.0488
Training Epoch: 19 [43008/50048]	Loss: 0.9429
Training Epoch: 19 [43136/50048]	Loss: 0.8107
Training Epoch: 19 [43264/50048]	Loss: 1.0684
Training Epoch: 19 [43392/50048]	Loss: 1.1970
Training Epoch: 19 [43520/50048]	Loss: 1.1199
Training Epoch: 19 [43648/50048]	Loss: 1.2937
Training Epoch: 19 [43776/50048]	Loss: 0.9180
Training Epoch: 19 [43904/50048]	Loss: 0.9374
Training Epoch: 19 [44032/50048]	Loss: 1.1868
Training Epoch: 19 [44160/50048]	Loss: 0.9606
Training Epoch: 19 [44288/50048]	Loss: 1.1637
Training Epoch: 19 [44416/50048]	Loss: 0.8915
Training Epoch: 19 [44544/50048]	Loss: 0.8548
Training Epoch: 19 [44672/50048]	Loss: 1.1604
Training Epoch: 19 [44800/50048]	Loss: 1.0660
Training Epoch: 19 [44928/50048]	Loss: 0.9031
Training Epoch: 19 [45056/50048]	Loss: 1.3102
Training Epoch: 19 [45184/50048]	Loss: 0.9546
Training Epoch: 19 [45312/50048]	Loss: 1.0561
Training Epoch: 19 [45440/50048]	Loss: 0.9866
Training Epoch: 19 [45568/50048]	Loss: 1.0696
Training Epoch: 19 [45696/50048]	Loss: 0.9876
2022-12-06 04:07:25,497 [ZeusDataLoader(train)] train epoch 20 done: time=86.49 energy=10517.59
2022-12-06 04:07:25,498 [ZeusDataLoader(eval)] Epoch 20 begin.
Training Epoch: 19 [45824/50048]	Loss: 1.1331
Training Epoch: 19 [45952/50048]	Loss: 0.9742
Training Epoch: 19 [46080/50048]	Loss: 1.1045
Training Epoch: 19 [46208/50048]	Loss: 1.1018
Training Epoch: 19 [46336/50048]	Loss: 1.0013
Training Epoch: 19 [46464/50048]	Loss: 1.1166
Training Epoch: 19 [46592/50048]	Loss: 1.1161
Training Epoch: 19 [46720/50048]	Loss: 0.9845
Training Epoch: 19 [46848/50048]	Loss: 1.0311
Training Epoch: 19 [46976/50048]	Loss: 1.1522
Training Epoch: 19 [47104/50048]	Loss: 1.1694
Training Epoch: 19 [47232/50048]	Loss: 0.8993
Training Epoch: 19 [47360/50048]	Loss: 0.9186
Training Epoch: 19 [47488/50048]	Loss: 1.0347
Training Epoch: 19 [47616/50048]	Loss: 1.0898
Training Epoch: 19 [47744/50048]	Loss: 0.9424
Training Epoch: 19 [47872/50048]	Loss: 1.0626
Training Epoch: 19 [48000/50048]	Loss: 0.9607
Training Epoch: 19 [48128/50048]	Loss: 1.2055
Training Epoch: 19 [48256/50048]	Loss: 0.9621
Training Epoch: 19 [48384/50048]	Loss: 1.0297
Training Epoch: 19 [48512/50048]	Loss: 0.9308
Training Epoch: 19 [48640/50048]	Loss: 1.0841
Training Epoch: 19 [48768/50048]	Loss: 1.3018
Training Epoch: 19 [48896/50048]	Loss: 1.2339
Training Epoch: 19 [49024/50048]	Loss: 1.2110
Training Epoch: 19 [49152/50048]	Loss: 1.1225
Training Epoch: 19 [49280/50048]	Loss: 1.1270
Training Epoch: 19 [49408/50048]	Loss: 1.0649
Training Epoch: 19 [49536/50048]	Loss: 0.9442
Training Epoch: 19 [49664/50048]	Loss: 1.0935
Training Epoch: 19 [49792/50048]	Loss: 1.0681
Training Epoch: 19 [49920/50048]	Loss: 0.9356
Training Epoch: 19 [50048/50048]	Loss: 1.0302
2022-12-06 09:07:29.159 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:07:29,170 [ZeusDataLoader(eval)] eval epoch 20 done: time=3.66 energy=441.36
2022-12-06 04:07:29,170 [ZeusDataLoader(train)] Up to epoch 20: time=1806.52, energy=219024.37, cost=267582.33
2022-12-06 04:07:29,170 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:07:29,170 [ZeusDataLoader(train)] Expected next epoch: time=1896.31, energy=229822.39, cost=280838.71
2022-12-06 04:07:29,171 [ZeusDataLoader(train)] Epoch 21 begin.
Validation Epoch: 19, Average loss: 0.0119, Accuracy: 0.6002
2022-12-06 04:07:29,343 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:07:29,343 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:07:29.345 [ZeusMonitor] Monitor started.
2022-12-06 09:07:29.345 [ZeusMonitor] Running indefinitely. 2022-12-06 09:07:29.345 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:07:29.345 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e21+gpu0.power.log
Training Epoch: 20 [128/50048]	Loss: 0.9853
Training Epoch: 20 [256/50048]	Loss: 0.8478
Training Epoch: 20 [384/50048]	Loss: 0.9217
Training Epoch: 20 [512/50048]	Loss: 0.8594
Training Epoch: 20 [640/50048]	Loss: 0.8427
Training Epoch: 20 [768/50048]	Loss: 0.9949
Training Epoch: 20 [896/50048]	Loss: 0.8814
Training Epoch: 20 [1024/50048]	Loss: 0.8633
Training Epoch: 20 [1152/50048]	Loss: 0.9286
Training Epoch: 20 [1280/50048]	Loss: 1.0693
Training Epoch: 20 [1408/50048]	Loss: 0.8809
Training Epoch: 20 [1536/50048]	Loss: 0.8962
Training Epoch: 20 [1664/50048]	Loss: 0.9765
Training Epoch: 20 [1792/50048]	Loss: 0.7500
Training Epoch: 20 [1920/50048]	Loss: 1.1210
Training Epoch: 20 [2048/50048]	Loss: 1.0219
Training Epoch: 20 [2176/50048]	Loss: 0.8961
Training Epoch: 20 [2304/50048]	Loss: 1.0268
Training Epoch: 20 [2432/50048]	Loss: 0.6995
Training Epoch: 20 [2560/50048]	Loss: 0.8610
Training Epoch: 20 [2688/50048]	Loss: 1.0615
Training Epoch: 20 [2816/50048]	Loss: 1.0646
Training Epoch: 20 [2944/50048]	Loss: 1.0050
Training Epoch: 20 [3072/50048]	Loss: 0.9725
Training Epoch: 20 [3200/50048]	Loss: 1.1218
Training Epoch: 20 [3328/50048]	Loss: 0.8816
Training Epoch: 20 [3456/50048]	Loss: 0.9485
Training Epoch: 20 [3584/50048]	Loss: 0.8714
Training Epoch: 20 [3712/50048]	Loss: 0.8523
Training Epoch: 20 [3840/50048]	Loss: 0.7713
Training Epoch: 20 [3968/50048]	Loss: 0.8453
Training Epoch: 20 [4096/50048]	Loss: 0.7697
Training Epoch: 20 [4224/50048]	Loss: 0.7953
Training Epoch: 20 [4352/50048]	Loss: 0.9731
Training Epoch: 20 [4480/50048]	Loss: 0.8808
Training Epoch: 20 [4608/50048]	Loss: 1.0099
Training Epoch: 20 [4736/50048]	Loss: 1.1896
Training Epoch: 20 [4864/50048]	Loss: 1.0142
Training Epoch: 20 [4992/50048]	Loss: 0.8901
Training Epoch: 20 [5120/50048]	Loss: 0.7657
Training Epoch: 20 [5248/50048]	Loss: 1.0511
Training Epoch: 20 [5376/50048]	Loss: 0.8157
Training Epoch: 20 [5504/50048]	Loss: 0.9148
Training Epoch: 20 [5632/50048]	Loss: 1.1950
Training Epoch: 20 [5760/50048]	Loss: 1.0034
Training Epoch: 20 [5888/50048]	Loss: 0.9496
Training Epoch: 20 [6016/50048]	Loss: 1.0000
Training Epoch: 20 [6144/50048]	Loss: 0.9871
Training Epoch: 20 [6272/50048]	Loss: 0.8268
Training Epoch: 20 [6400/50048]	Loss: 0.8616
Training Epoch: 20 [6528/50048]	Loss: 1.1104
Training Epoch: 20 [6656/50048]	Loss: 0.9642
Training Epoch: 20 [6784/50048]	Loss: 0.8639
Training Epoch: 20 [6912/50048]	Loss: 1.0124
Training Epoch: 20 [7040/50048]	Loss: 1.0094
Training Epoch: 20 [7168/50048]	Loss: 0.8753
Training Epoch: 20 [7296/50048]	Loss: 0.9473
Training Epoch: 20 [7424/50048]	Loss: 0.9932
Training Epoch: 20 [7552/50048]	Loss: 0.8882
Training Epoch: 20 [7680/50048]	Loss: 0.7431
Training Epoch: 20 [7808/50048]	Loss: 1.0077
Training Epoch: 20 [7936/50048]	Loss: 1.1510
Training Epoch: 20 [8064/50048]	Loss: 0.9162
Training Epoch: 20 [8192/50048]	Loss: 0.7924
Training Epoch: 20 [8320/50048]	Loss: 0.9881
Training Epoch: 20 [8448/50048]	Loss: 1.0492
Training Epoch: 20 [8576/50048]	Loss: 1.1117
Training Epoch: 20 [8704/50048]	Loss: 0.9424
Training Epoch: 20 [8832/50048]	Loss: 0.8925
Training Epoch: 20 [8960/50048]	Loss: 0.8385
Training Epoch: 20 [9088/50048]	Loss: 0.9843
Training Epoch: 20 [9216/50048]	Loss: 0.9551
Training Epoch: 20 [9344/50048]	Loss: 1.0243
Training Epoch: 20 [9472/50048]	Loss: 1.0220
Training Epoch: 20 [9600/50048]	Loss: 0.9874
Training Epoch: 20 [9728/50048]	Loss: 0.8149
Training Epoch: 20 [9856/50048]	Loss: 0.8729
Training Epoch: 20 [9984/50048]	Loss: 0.9862
Training Epoch: 20 [10112/50048]	Loss: 1.0587
Training Epoch: 20 [10240/50048]	Loss: 0.7395
Training Epoch: 20 [10368/50048]	Loss: 0.9097
Training Epoch: 20 [10496/50048]	Loss: 0.9406
Training Epoch: 20 [10624/50048]	Loss: 0.9371
Training Epoch: 20 [10752/50048]	Loss: 0.9577
Training Epoch: 20 [10880/50048]	Loss: 1.1803
Training Epoch: 20 [11008/50048]	Loss: 1.0093
Training Epoch: 20 [11136/50048]	Loss: 0.8183
Training Epoch: 20 [11264/50048]	Loss: 0.9088
Training Epoch: 20 [11392/50048]	Loss: 0.9308
Training Epoch: 20 [11520/50048]	Loss: 0.9065
Training Epoch: 20 [11648/50048]	Loss: 0.8213
Training Epoch: 20 [11776/50048]	Loss: 0.8654
Training Epoch: 20 [11904/50048]	Loss: 0.8359
Training Epoch: 20 [12032/50048]	Loss: 1.1080
Training Epoch: 20 [12160/50048]	Loss: 0.9847
Training Epoch: 20 [12288/50048]	Loss: 1.0060
Training Epoch: 20 [12416/50048]	Loss: 1.2298
Training Epoch: 20 [12544/50048]	Loss: 0.9270
Training Epoch: 20 [12672/50048]	Loss: 0.7988
Training Epoch: 20 [12800/50048]	Loss: 0.7862
Training Epoch: 20 [12928/50048]	Loss: 0.9951
Training Epoch: 20 [13056/50048]	Loss: 0.8756
Training Epoch: 20 [13184/50048]	Loss: 1.0119
Training Epoch: 20 [13312/50048]	Loss: 0.8134
Training Epoch: 20 [13440/50048]	Loss: 1.0348
Training Epoch: 20 [13568/50048]	Loss: 0.9944
Training Epoch: 20 [13696/50048]	Loss: 0.7562
Training Epoch: 20 [13824/50048]	Loss: 0.7673
Training Epoch: 20 [13952/50048]	Loss: 1.2244
Training Epoch: 20 [14080/50048]	Loss: 1.0180
Training Epoch: 20 [14208/50048]	Loss: 0.8969
Training Epoch: 20 [14336/50048]	Loss: 0.9142
Training Epoch: 20 [14464/50048]	Loss: 1.0909
Training Epoch: 20 [14592/50048]	Loss: 0.8429
Training Epoch: 20 [14720/50048]	Loss: 1.0108
Training Epoch: 20 [14848/50048]	Loss: 0.8826
Training Epoch: 20 [14976/50048]	Loss: 1.2114
Training Epoch: 20 [15104/50048]	Loss: 1.1134
Training Epoch: 20 [15232/50048]	Loss: 0.9975
Training Epoch: 20 [15360/50048]	Loss: 1.0839
Training Epoch: 20 [15488/50048]	Loss: 1.0497
Training Epoch: 20 [15616/50048]	Loss: 1.1827
Training Epoch: 20 [15744/50048]	Loss: 0.9866
Training Epoch: 20 [15872/50048]	Loss: 1.0168
Training Epoch: 20 [16000/50048]	Loss: 0.8377
Training Epoch: 20 [16128/50048]	Loss: 0.9462
Training Epoch: 20 [16256/50048]	Loss: 1.0724
Training Epoch: 20 [16384/50048]	Loss: 1.1318
Training Epoch: 20 [16512/50048]	Loss: 0.8524
Training Epoch: 20 [16640/50048]	Loss: 0.8788
Training Epoch: 20 [16768/50048]	Loss: 1.1855
Training Epoch: 20 [16896/50048]	Loss: 1.0921
Training Epoch: 20 [17024/50048]	Loss: 1.1346
Training Epoch: 20 [17152/50048]	Loss: 1.0747
Training Epoch: 20 [17280/50048]	Loss: 1.0067
Training Epoch: 20 [17408/50048]	Loss: 1.0321
Training Epoch: 20 [17536/50048]	Loss: 0.8462
Training Epoch: 20 [17664/50048]	Loss: 0.8955
Training Epoch: 20 [17792/50048]	Loss: 0.9256
Training Epoch: 20 [17920/50048]	Loss: 1.0182
Training Epoch: 20 [18048/50048]	Loss: 0.9582
Training Epoch: 20 [18176/50048]	Loss: 1.0118
Training Epoch: 20 [18304/50048]	Loss: 0.9271
Training Epoch: 20 [18432/50048]	Loss: 0.9756
Training Epoch: 20 [18560/50048]	Loss: 0.9869
Training Epoch: 20 [18688/50048]	Loss: 0.9115
Training Epoch: 20 [18816/50048]	Loss: 0.9677
Training Epoch: 20 [18944/50048]	Loss: 1.0586
Training Epoch: 20 [19072/50048]	Loss: 0.9757
Training Epoch: 20 [19200/50048]	Loss: 0.9884
Training Epoch: 20 [19328/50048]	Loss: 1.0264
Training Epoch: 20 [19456/50048]	Loss: 1.2454
Training Epoch: 20 [19584/50048]	Loss: 1.2441
Training Epoch: 20 [19712/50048]	Loss: 1.0747
Training Epoch: 20 [19840/50048]	Loss: 0.9090
Training Epoch: 20 [19968/50048]	Loss: 0.8976
Training Epoch: 20 [20096/50048]	Loss: 0.8416
Training Epoch: 20 [20224/50048]	Loss: 0.9871
Training Epoch: 20 [20352/50048]	Loss: 1.0394
Training Epoch: 20 [20480/50048]	Loss: 0.7998
Training Epoch: 20 [20608/50048]	Loss: 1.0468
Training Epoch: 20 [20736/50048]	Loss: 0.8534
Training Epoch: 20 [20864/50048]	Loss: 1.3155
Training Epoch: 20 [20992/50048]	Loss: 0.9828
Training Epoch: 20 [21120/50048]	Loss: 1.1172
Training Epoch: 20 [21248/50048]	Loss: 1.1793
Training Epoch: 20 [21376/50048]	Loss: 0.9279
Training Epoch: 20 [21504/50048]	Loss: 0.9452
Training Epoch: 20 [21632/50048]	Loss: 0.9492
Training Epoch: 20 [21760/50048]	Loss: 0.9519
Training Epoch: 20 [21888/50048]	Loss: 0.9154
Training Epoch: 20 [22016/50048]	Loss: 0.9781
Training Epoch: 20 [22144/50048]	Loss: 0.7598
Training Epoch: 20 [22272/50048]	Loss: 1.1789
Training Epoch: 20 [22400/50048]	Loss: 1.1734
Training Epoch: 20 [22528/50048]	Loss: 0.9745
Training Epoch: 20 [22656/50048]	Loss: 0.7924
Training Epoch: 20 [22784/50048]	Loss: 1.0092
Training Epoch: 20 [22912/50048]	Loss: 0.8765
Training Epoch: 20 [23040/50048]	Loss: 0.9789
Training Epoch: 20 [23168/50048]	Loss: 1.2996
Training Epoch: 20 [23296/50048]	Loss: 1.0562
Training Epoch: 20 [23424/50048]	Loss: 0.6553
Training Epoch: 20 [23552/50048]	Loss: 0.8427
Training Epoch: 20 [23680/50048]	Loss: 0.9642
Training Epoch: 20 [23808/50048]	Loss: 0.9896
Training Epoch: 20 [23936/50048]	Loss: 1.0810
Training Epoch: 20 [24064/50048]	Loss: 1.1588
Training Epoch: 20 [24192/50048]	Loss: 1.1509
Training Epoch: 20 [24320/50048]	Loss: 1.0581
Training Epoch: 20 [24448/50048]	Loss: 0.9220
Training Epoch: 20 [24576/50048]	Loss: 0.9946
Training Epoch: 20 [24704/50048]	Loss: 0.8617
Training Epoch: 20 [24832/50048]	Loss: 1.2200
Training Epoch: 20 [24960/50048]	Loss: 1.0341
Training Epoch: 20 [25088/50048]	Loss: 1.0434
Training Epoch: 20 [25216/50048]	Loss: 1.0388
Training Epoch: 20 [25344/50048]	Loss: 1.1798
Training Epoch: 20 [25472/50048]	Loss: 1.0065
Training Epoch: 20 [25600/50048]	Loss: 0.9890
Training Epoch: 20 [25728/50048]	Loss: 0.9949
Training Epoch: 20 [25856/50048]	Loss: 1.0233
Training Epoch: 20 [25984/50048]	Loss: 1.0281
Training Epoch: 20 [26112/50048]	Loss: 1.2541
Training Epoch: 20 [26240/50048]	Loss: 1.1223
Training Epoch: 20 [26368/50048]	Loss: 1.0807
Training Epoch: 20 [26496/50048]	Loss: 0.9832
Training Epoch: 20 [26624/50048]	Loss: 0.9439
Training Epoch: 20 [26752/50048]	Loss: 0.8122
Training Epoch: 20 [26880/50048]	Loss: 0.9863
Training Epoch: 20 [27008/50048]	Loss: 0.9972
Training Epoch: 20 [27136/50048]	Loss: 0.9555
Training Epoch: 20 [27264/50048]	Loss: 0.7922
Training Epoch: 20 [27392/50048]	Loss: 0.9252
Training Epoch: 20 [27520/50048]	Loss: 1.0002
Training Epoch: 20 [27648/50048]	Loss: 1.0538
Training Epoch: 20 [27776/50048]	Loss: 1.1382
Training Epoch: 20 [27904/50048]	Loss: 0.9587
Training Epoch: 20 [28032/50048]	Loss: 0.9525
Training Epoch: 20 [28160/50048]	Loss: 1.1098
Training Epoch: 20 [28288/50048]	Loss: 1.0405
Training Epoch: 20 [28416/50048]	Loss: 1.2512
Training Epoch: 20 [28544/50048]	Loss: 1.0819
Training Epoch: 20 [28672/50048]	Loss: 0.8698
Training Epoch: 20 [28800/50048]	Loss: 0.9824
Training Epoch: 20 [28928/50048]	Loss: 0.8904
Training Epoch: 20 [29056/50048]	Loss: 1.1432
Training Epoch: 20 [29184/50048]	Loss: 0.8442
Training Epoch: 20 [29312/50048]	Loss: 1.0743
Training Epoch: 20 [29440/50048]	Loss: 0.9580
Training Epoch: 20 [29568/50048]	Loss: 0.9780
Training Epoch: 20 [29696/50048]	Loss: 0.9336
Training Epoch: 20 [29824/50048]	Loss: 0.8657
Training Epoch: 20 [29952/50048]	Loss: 0.8590
Training Epoch: 20 [30080/50048]	Loss: 0.8086
Training Epoch: 20 [30208/50048]	Loss: 1.0503
Training Epoch: 20 [30336/50048]	Loss: 1.1988
Training Epoch: 20 [30464/50048]	Loss: 1.0153
Training Epoch: 20 [30592/50048]	Loss: 0.9201
Training Epoch: 20 [30720/50048]	Loss: 1.1244
Training Epoch: 20 [30848/50048]	Loss: 0.8877
Training Epoch: 20 [30976/50048]	Loss: 0.9636
Training Epoch: 20 [31104/50048]	Loss: 0.8453
Training Epoch: 20 [31232/50048]	Loss: 0.9561
Training Epoch: 20 [31360/50048]	Loss: 1.3006
Training Epoch: 20 [31488/50048]	Loss: 0.9303
Training Epoch: 20 [31616/50048]	Loss: 0.9499
Training Epoch: 20 [31744/50048]	Loss: 1.0479
Training Epoch: 20 [31872/50048]	Loss: 1.1136
Training Epoch: 20 [32000/50048]	Loss: 1.0283
Training Epoch: 20 [32128/50048]	Loss: 0.8899
Training Epoch: 20 [32256/50048]	Loss: 0.8524
Training Epoch: 20 [32384/50048]	Loss: 1.0177
Training Epoch: 20 [32512/50048]	Loss: 1.0126
Training Epoch: 20 [32640/50048]	Loss: 0.8745
Training Epoch: 20 [32768/50048]	Loss: 0.8618
Training Epoch: 20 [32896/50048]	Loss: 0.9105
Training Epoch: 20 [33024/50048]	Loss: 1.1787
Training Epoch: 20 [33152/50048]	Loss: 1.1341
Training Epoch: 20 [33280/50048]	Loss: 1.2014
Training Epoch: 20 [33408/50048]	Loss: 0.9927
Training Epoch: 20 [33536/50048]	Loss: 0.9552
Training Epoch: 20 [33664/50048]	Loss: 0.8239
Training Epoch: 20 [33792/50048]	Loss: 0.8931
Training Epoch: 20 [33920/50048]	Loss: 1.1304
Training Epoch: 20 [34048/50048]	Loss: 0.9261
Training Epoch: 20 [34176/50048]	Loss: 1.2416
Training Epoch: 20 [34304/50048]	Loss: 0.9213
Training Epoch: 20 [34432/50048]	Loss: 0.8698
Training Epoch: 20 [34560/50048]	Loss: 1.1285
Training Epoch: 20 [34688/50048]	Loss: 1.0563
Training Epoch: 20 [34816/50048]	Loss: 1.0718
Training Epoch: 20 [34944/50048]	Loss: 1.1148
Training Epoch: 20 [35072/50048]	Loss: 0.8494
Training Epoch: 20 [35200/50048]	Loss: 1.0634
Training Epoch: 20 [35328/50048]	Loss: 0.8447
Training Epoch: 20 [35456/50048]	Loss: 0.9724
Training Epoch: 20 [35584/50048]	Loss: 0.9567
Training Epoch: 20 [35712/50048]	Loss: 0.8527
Training Epoch: 20 [35840/50048]	Loss: 1.1294
Training Epoch: 20 [35968/50048]	Loss: 0.9750
Training Epoch: 20 [36096/50048]	Loss: 1.0183
Training Epoch: 20 [36224/50048]	Loss: 0.9854
Training Epoch: 20 [36352/50048]	Loss: 0.9568
Training Epoch: 20 [36480/50048]	Loss: 1.0159
Training Epoch: 20 [36608/50048]	Loss: 1.0641
Training Epoch: 20 [36736/50048]	Loss: 1.0501
Training Epoch: 20 [36864/50048]	Loss: 1.1800
Training Epoch: 20 [36992/50048]	Loss: 1.2323
Training Epoch: 20 [37120/50048]	Loss: 0.9781
Training Epoch: 20 [37248/50048]	Loss: 0.7810
Training Epoch: 20 [37376/50048]	Loss: 1.1626
Training Epoch: 20 [37504/50048]	Loss: 1.0307
Training Epoch: 20 [37632/50048]	Loss: 0.9031
Training Epoch: 20 [37760/50048]	Loss: 0.9117
Training Epoch: 20 [37888/50048]	Loss: 1.0632
Training Epoch: 20 [38016/50048]	Loss: 1.0640
Training Epoch: 20 [38144/50048]	Loss: 0.9865
Training Epoch: 20 [38272/50048]	Loss: 0.9532
Training Epoch: 20 [38400/50048]	Loss: 1.1014
Training Epoch: 20 [38528/50048]	Loss: 1.0936
Training Epoch: 20 [38656/50048]	Loss: 0.8952
Training Epoch: 20 [38784/50048]	Loss: 1.0024
Training Epoch: 20 [38912/50048]	Loss: 1.0942
Training Epoch: 20 [39040/50048]	Loss: 0.8693
Training Epoch: 20 [39168/50048]	Loss: 1.2020
Training Epoch: 20 [39296/50048]	Loss: 0.9596
Training Epoch: 20 [39424/50048]	Loss: 1.1503
Training Epoch: 20 [39552/50048]	Loss: 0.9750
Training Epoch: 20 [39680/50048]	Loss: 0.9199
Training Epoch: 20 [39808/50048]	Loss: 0.9488
Training Epoch: 20 [39936/50048]	Loss: 1.2025
Training Epoch: 20 [40064/50048]	Loss: 1.2216
Training Epoch: 20 [40192/50048]	Loss: 0.9668
Training Epoch: 20 [40320/50048]	Loss: 1.2952
Training Epoch: 20 [40448/50048]	Loss: 1.0450
Training Epoch: 20 [40576/50048]	Loss: 0.7594
Training Epoch: 20 [40704/50048]	Loss: 1.1844
Training Epoch: 20 [40832/50048]	Loss: 1.0918
Training Epoch: 20 [40960/50048]	Loss: 0.8734
Training Epoch: 20 [41088/50048]	Loss: 1.0938
Training Epoch: 20 [41216/50048]	Loss: 0.9142
Training Epoch: 20 [41344/50048]	Loss: 0.9139
Training Epoch: 20 [41472/50048]	Loss: 0.9751
Training Epoch: 20 [41600/50048]	Loss: 1.0524
Training Epoch: 20 [41728/50048]	Loss: 1.0252
Training Epoch: 20 [41856/50048]	Loss: 1.0325
Training Epoch: 20 [41984/50048]	Loss: 1.0312
Training Epoch: 20 [42112/50048]	Loss: 1.0826
Training Epoch: 20 [42240/50048]	Loss: 1.0646
Training Epoch: 20 [42368/50048]	Loss: 0.9181
Training Epoch: 20 [42496/50048]	Loss: 0.9376
Training Epoch: 20 [42624/50048]	Loss: 0.8879
Training Epoch: 20 [42752/50048]	Loss: 0.9695
Training Epoch: 20 [42880/50048]	Loss: 1.0876
Training Epoch: 20 [43008/50048]	Loss: 1.2080
Training Epoch: 20 [43136/50048]	Loss: 0.6381
Training Epoch: 20 [43264/50048]	Loss: 1.1442
Training Epoch: 20 [43392/50048]	Loss: 1.0261
Training Epoch: 20 [43520/50048]	Loss: 0.8293
Training Epoch: 20 [43648/50048]	Loss: 1.1840
Training Epoch: 20 [43776/50048]	Loss: 1.0823
Training Epoch: 20 [43904/50048]	Loss: 0.9304
Training Epoch: 20 [44032/50048]	Loss: 0.8735
Training Epoch: 20 [44160/50048]	Loss: 1.1202
Training Epoch: 20 [44288/50048]	Loss: 1.1684
Training Epoch: 20 [44416/50048]	Loss: 0.9092
Training Epoch: 20 [44544/50048]	Loss: 0.9484
Training Epoch: 20 [44672/50048]	Loss: 0.9576
Training Epoch: 20 [44800/50048]	Loss: 0.9993
Training Epoch: 20 [44928/50048]	Loss: 1.0836
Training Epoch: 20 [45056/50048]	Loss: 0.9593
Training Epoch: 20 [45184/50048]	Loss: 0.9202
Training Epoch: 20 [45312/50048]	Loss: 1.0180
Training Epoch: 20 [45440/50048]	Loss: 1.0702
Training Epoch: 20 [45568/50048]	Loss: 0.8609
Training Epoch: 20 [45696/50048]	Loss: 0.9046
2022-12-06 04:08:55,660 [ZeusDataLoader(train)] train epoch 21 done: time=86.48 energy=10507.89
2022-12-06 04:08:55,662 [ZeusDataLoader(eval)] Epoch 21 begin.
Training Epoch: 20 [45824/50048]	Loss: 0.9661
Training Epoch: 20 [45952/50048]	Loss: 0.9797
Training Epoch: 20 [46080/50048]	Loss: 1.0233
Training Epoch: 20 [46208/50048]	Loss: 0.9871
Training Epoch: 20 [46336/50048]	Loss: 1.1829
Training Epoch: 20 [46464/50048]	Loss: 0.9606
Training Epoch: 20 [46592/50048]	Loss: 0.9266
Training Epoch: 20 [46720/50048]	Loss: 1.1710
Training Epoch: 20 [46848/50048]	Loss: 0.9568
Training Epoch: 20 [46976/50048]	Loss: 0.8661
Training Epoch: 20 [47104/50048]	Loss: 0.9284
Training Epoch: 20 [47232/50048]	Loss: 1.1403
Training Epoch: 20 [47360/50048]	Loss: 0.7780
Training Epoch: 20 [47488/50048]	Loss: 1.0404
Training Epoch: 20 [47616/50048]	Loss: 0.8831
Training Epoch: 20 [47744/50048]	Loss: 1.0523
Training Epoch: 20 [47872/50048]	Loss: 1.1839
Training Epoch: 20 [48000/50048]	Loss: 1.1017
Training Epoch: 20 [48128/50048]	Loss: 0.9690
Training Epoch: 20 [48256/50048]	Loss: 1.0023
Training Epoch: 20 [48384/50048]	Loss: 1.0821
Training Epoch: 20 [48512/50048]	Loss: 0.9173
Training Epoch: 20 [48640/50048]	Loss: 0.9620
Training Epoch: 20 [48768/50048]	Loss: 1.0020
Training Epoch: 20 [48896/50048]	Loss: 1.3814
Training Epoch: 20 [49024/50048]	Loss: 1.0347
Training Epoch: 20 [49152/50048]	Loss: 1.0058
Training Epoch: 20 [49280/50048]	Loss: 0.9452
Training Epoch: 20 [49408/50048]	Loss: 0.8942
Training Epoch: 20 [49536/50048]	Loss: 1.0111
Training Epoch: 20 [49664/50048]	Loss: 1.0365
Training Epoch: 20 [49792/50048]	Loss: 1.0999
Training Epoch: 20 [49920/50048]	Loss: 0.9502
Training Epoch: 20 [50048/50048]	Loss: 1.2634
2022-12-06 09:08:59.304 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:08:59,318 [ZeusDataLoader(eval)] eval epoch 21 done: time=3.65 energy=440.41
2022-12-06 04:08:59,319 [ZeusDataLoader(train)] Up to epoch 21: time=1896.64, energy=229972.67, cost=280942.54
2022-12-06 04:08:59,319 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:08:59,319 [ZeusDataLoader(train)] Expected next epoch: time=1986.44, energy=240770.68, cost=294198.92
2022-12-06 04:08:59,320 [ZeusDataLoader(train)] Epoch 22 begin.
Validation Epoch: 20, Average loss: 0.0125, Accuracy: 0.5860
2022-12-06 04:08:59,495 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:08:59,496 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:08:59.498 [ZeusMonitor] Monitor started.
2022-12-06 09:08:59.498 [ZeusMonitor] Running indefinitely. 2022-12-06 09:08:59.498 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:08:59.498 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e22+gpu0.power.log
Training Epoch: 21 [128/50048]	Loss: 0.9323
Training Epoch: 21 [256/50048]	Loss: 0.9299
Training Epoch: 21 [384/50048]	Loss: 0.8750
Training Epoch: 21 [512/50048]	Loss: 1.0032
Training Epoch: 21 [640/50048]	Loss: 0.9330
Training Epoch: 21 [768/50048]	Loss: 0.9234
Training Epoch: 21 [896/50048]	Loss: 0.9183
Training Epoch: 21 [1024/50048]	Loss: 0.8952
Training Epoch: 21 [1152/50048]	Loss: 0.9129
Training Epoch: 21 [1280/50048]	Loss: 0.6062
Training Epoch: 21 [1408/50048]	Loss: 0.8904
Training Epoch: 21 [1536/50048]	Loss: 0.7887
Training Epoch: 21 [1664/50048]	Loss: 0.9151
Training Epoch: 21 [1792/50048]	Loss: 0.9638
Training Epoch: 21 [1920/50048]	Loss: 0.9660
Training Epoch: 21 [2048/50048]	Loss: 1.0663
Training Epoch: 21 [2176/50048]	Loss: 0.7389
Training Epoch: 21 [2304/50048]	Loss: 0.8476
Training Epoch: 21 [2432/50048]	Loss: 1.0055
Training Epoch: 21 [2560/50048]	Loss: 0.7794
Training Epoch: 21 [2688/50048]	Loss: 0.8225
Training Epoch: 21 [2816/50048]	Loss: 0.9217
Training Epoch: 21 [2944/50048]	Loss: 0.9916
Training Epoch: 21 [3072/50048]	Loss: 0.8475
Training Epoch: 21 [3200/50048]	Loss: 1.0058
Training Epoch: 21 [3328/50048]	Loss: 0.7452
Training Epoch: 21 [3456/50048]	Loss: 0.8270
Training Epoch: 21 [3584/50048]	Loss: 0.7863
Training Epoch: 21 [3712/50048]	Loss: 0.9344
Training Epoch: 21 [3840/50048]	Loss: 1.0088
Training Epoch: 21 [3968/50048]	Loss: 0.8272
Training Epoch: 21 [4096/50048]	Loss: 0.9827
Training Epoch: 21 [4224/50048]	Loss: 0.9172
Training Epoch: 21 [4352/50048]	Loss: 0.9187
Training Epoch: 21 [4480/50048]	Loss: 0.8252
Training Epoch: 21 [4608/50048]	Loss: 0.9375
Training Epoch: 21 [4736/50048]	Loss: 0.8712
Training Epoch: 21 [4864/50048]	Loss: 0.6667
Training Epoch: 21 [4992/50048]	Loss: 0.6874
Training Epoch: 21 [5120/50048]	Loss: 1.0315
Training Epoch: 21 [5248/50048]	Loss: 0.9315
Training Epoch: 21 [5376/50048]	Loss: 0.7309
Training Epoch: 21 [5504/50048]	Loss: 0.8597
Training Epoch: 21 [5632/50048]	Loss: 0.8915
Training Epoch: 21 [5760/50048]	Loss: 0.8710
Training Epoch: 21 [5888/50048]	Loss: 0.9232
Training Epoch: 21 [6016/50048]	Loss: 0.7931
Training Epoch: 21 [6144/50048]	Loss: 1.0575
Training Epoch: 21 [6272/50048]	Loss: 0.7245
Training Epoch: 21 [6400/50048]	Loss: 0.9162
Training Epoch: 21 [6528/50048]	Loss: 0.8811
Training Epoch: 21 [6656/50048]	Loss: 0.8519
Training Epoch: 21 [6784/50048]	Loss: 0.7652
Training Epoch: 21 [6912/50048]	Loss: 0.8205
Training Epoch: 21 [7040/50048]	Loss: 1.0040
Training Epoch: 21 [7168/50048]	Loss: 0.7696
Training Epoch: 21 [7296/50048]	Loss: 0.6927
Training Epoch: 21 [7424/50048]	Loss: 0.9451
Training Epoch: 21 [7552/50048]	Loss: 0.8659
Training Epoch: 21 [7680/50048]	Loss: 0.8832
Training Epoch: 21 [7808/50048]	Loss: 1.1635
Training Epoch: 21 [7936/50048]	Loss: 0.9825
Training Epoch: 21 [8064/50048]	Loss: 0.8012
Training Epoch: 21 [8192/50048]	Loss: 0.9961
Training Epoch: 21 [8320/50048]	Loss: 0.8854
Training Epoch: 21 [8448/50048]	Loss: 0.9554
Training Epoch: 21 [8576/50048]	Loss: 0.9084
Training Epoch: 21 [8704/50048]	Loss: 0.9148
Training Epoch: 21 [8832/50048]	Loss: 0.8112
Training Epoch: 21 [8960/50048]	Loss: 0.9550
Training Epoch: 21 [9088/50048]	Loss: 0.8057
Training Epoch: 21 [9216/50048]	Loss: 0.9573
Training Epoch: 21 [9344/50048]	Loss: 0.8315
Training Epoch: 21 [9472/50048]	Loss: 0.7446
Training Epoch: 21 [9600/50048]	Loss: 0.9048
Training Epoch: 21 [9728/50048]	Loss: 0.9462
Training Epoch: 21 [9856/50048]	Loss: 1.1057
Training Epoch: 21 [9984/50048]	Loss: 0.9271
Training Epoch: 21 [10112/50048]	Loss: 1.0606
Training Epoch: 21 [10240/50048]	Loss: 0.7913
Training Epoch: 21 [10368/50048]	Loss: 0.9112
Training Epoch: 21 [10496/50048]	Loss: 0.8613
Training Epoch: 21 [10624/50048]	Loss: 0.8922
Training Epoch: 21 [10752/50048]	Loss: 0.8590
Training Epoch: 21 [10880/50048]	Loss: 0.8729
Training Epoch: 21 [11008/50048]	Loss: 0.9154
Training Epoch: 21 [11136/50048]	Loss: 0.8867
Training Epoch: 21 [11264/50048]	Loss: 0.9894
Training Epoch: 21 [11392/50048]	Loss: 0.8699
Training Epoch: 21 [11520/50048]	Loss: 0.8804
Training Epoch: 21 [11648/50048]	Loss: 0.9762
Training Epoch: 21 [11776/50048]	Loss: 0.9319
Training Epoch: 21 [11904/50048]	Loss: 0.8067
Training Epoch: 21 [12032/50048]	Loss: 0.8978
Training Epoch: 21 [12160/50048]	Loss: 0.9385
Training Epoch: 21 [12288/50048]	Loss: 0.9011
Training Epoch: 21 [12416/50048]	Loss: 0.9625
Training Epoch: 21 [12544/50048]	Loss: 1.0237
Training Epoch: 21 [12672/50048]	Loss: 0.9079
Training Epoch: 21 [12800/50048]	Loss: 1.1457
Training Epoch: 21 [12928/50048]	Loss: 0.8680
Training Epoch: 21 [13056/50048]	Loss: 0.8761
Training Epoch: 21 [13184/50048]	Loss: 0.9260
Training Epoch: 21 [13312/50048]	Loss: 0.8076
Training Epoch: 21 [13440/50048]	Loss: 1.0474
Training Epoch: 21 [13568/50048]	Loss: 1.1106
Training Epoch: 21 [13696/50048]	Loss: 0.7242
Training Epoch: 21 [13824/50048]	Loss: 0.9765
Training Epoch: 21 [13952/50048]	Loss: 1.0034
Training Epoch: 21 [14080/50048]	Loss: 1.1033
Training Epoch: 21 [14208/50048]	Loss: 1.2457
Training Epoch: 21 [14336/50048]	Loss: 0.7781
Training Epoch: 21 [14464/50048]	Loss: 0.9240
Training Epoch: 21 [14592/50048]	Loss: 0.8010
Training Epoch: 21 [14720/50048]	Loss: 0.9596
Training Epoch: 21 [14848/50048]	Loss: 0.9034
Training Epoch: 21 [14976/50048]	Loss: 1.0058
Training Epoch: 21 [15104/50048]	Loss: 1.0902
Training Epoch: 21 [15232/50048]	Loss: 0.9472
Training Epoch: 21 [15360/50048]	Loss: 0.8215
Training Epoch: 21 [15488/50048]	Loss: 1.0104
Training Epoch: 21 [15616/50048]	Loss: 0.9690
Training Epoch: 21 [15744/50048]	Loss: 0.9913
Training Epoch: 21 [15872/50048]	Loss: 0.9741
Training Epoch: 21 [16000/50048]	Loss: 1.1487
Training Epoch: 21 [16128/50048]	Loss: 0.9530
Training Epoch: 21 [16256/50048]	Loss: 1.0125
Training Epoch: 21 [16384/50048]	Loss: 1.0582
Training Epoch: 21 [16512/50048]	Loss: 0.9735
Training Epoch: 21 [16640/50048]	Loss: 0.8619
Training Epoch: 21 [16768/50048]	Loss: 0.9485
Training Epoch: 21 [16896/50048]	Loss: 0.8678
Training Epoch: 21 [17024/50048]	Loss: 0.8999
Training Epoch: 21 [17152/50048]	Loss: 0.8929
Training Epoch: 21 [17280/50048]	Loss: 1.1361
Training Epoch: 21 [17408/50048]	Loss: 0.9140
Training Epoch: 21 [17536/50048]	Loss: 0.6900
Training Epoch: 21 [17664/50048]	Loss: 1.0244
Training Epoch: 21 [17792/50048]	Loss: 0.9581
Training Epoch: 21 [17920/50048]	Loss: 0.8743
Training Epoch: 21 [18048/50048]	Loss: 0.9550
Training Epoch: 21 [18176/50048]	Loss: 0.9549
Training Epoch: 21 [18304/50048]	Loss: 0.7501
Training Epoch: 21 [18432/50048]	Loss: 0.8599
Training Epoch: 21 [18560/50048]	Loss: 0.9909
Training Epoch: 21 [18688/50048]	Loss: 0.8028
Training Epoch: 21 [18816/50048]	Loss: 1.1851
Training Epoch: 21 [18944/50048]	Loss: 1.0294
Training Epoch: 21 [19072/50048]	Loss: 1.1099
Training Epoch: 21 [19200/50048]	Loss: 0.9954
Training Epoch: 21 [19328/50048]	Loss: 1.0487
Training Epoch: 21 [19456/50048]	Loss: 0.9688
Training Epoch: 21 [19584/50048]	Loss: 0.9320
Training Epoch: 21 [19712/50048]	Loss: 0.9882
Training Epoch: 21 [19840/50048]	Loss: 0.9517
Training Epoch: 21 [19968/50048]	Loss: 0.8299
Training Epoch: 21 [20096/50048]	Loss: 0.9243
Training Epoch: 21 [20224/50048]	Loss: 0.7716
Training Epoch: 21 [20352/50048]	Loss: 0.9932
Training Epoch: 21 [20480/50048]	Loss: 1.0553
Training Epoch: 21 [20608/50048]	Loss: 0.9853
Training Epoch: 21 [20736/50048]	Loss: 1.0082
Training Epoch: 21 [20864/50048]	Loss: 0.9231
Training Epoch: 21 [20992/50048]	Loss: 0.9735
Training Epoch: 21 [21120/50048]	Loss: 0.9449
Training Epoch: 21 [21248/50048]	Loss: 0.8792
Training Epoch: 21 [21376/50048]	Loss: 0.8672
Training Epoch: 21 [21504/50048]	Loss: 1.2956
Training Epoch: 21 [21632/50048]	Loss: 0.9909
Training Epoch: 21 [21760/50048]	Loss: 0.9999
Training Epoch: 21 [21888/50048]	Loss: 1.1201
Training Epoch: 21 [22016/50048]	Loss: 1.1230
Training Epoch: 21 [22144/50048]	Loss: 1.0889
Training Epoch: 21 [22272/50048]	Loss: 1.0201
Training Epoch: 21 [22400/50048]	Loss: 0.6550
Training Epoch: 21 [22528/50048]	Loss: 0.7867
Training Epoch: 21 [22656/50048]	Loss: 0.9385
Training Epoch: 21 [22784/50048]	Loss: 0.8237
Training Epoch: 21 [22912/50048]	Loss: 0.8170
Training Epoch: 21 [23040/50048]	Loss: 1.2001
Training Epoch: 21 [23168/50048]	Loss: 1.0095
Training Epoch: 21 [23296/50048]	Loss: 0.9479
Training Epoch: 21 [23424/50048]	Loss: 0.7951
Training Epoch: 21 [23552/50048]	Loss: 0.8928
Training Epoch: 21 [23680/50048]	Loss: 1.0328
Training Epoch: 21 [23808/50048]	Loss: 1.0209
Training Epoch: 21 [23936/50048]	Loss: 0.8715
Training Epoch: 21 [24064/50048]	Loss: 0.8254
Training Epoch: 21 [24192/50048]	Loss: 0.9362
Training Epoch: 21 [24320/50048]	Loss: 0.7837
Training Epoch: 21 [24448/50048]	Loss: 1.0026
Training Epoch: 21 [24576/50048]	Loss: 0.9924
Training Epoch: 21 [24704/50048]	Loss: 1.0703
Training Epoch: 21 [24832/50048]	Loss: 1.1036
Training Epoch: 21 [24960/50048]	Loss: 1.0494
Training Epoch: 21 [25088/50048]	Loss: 0.9395
Training Epoch: 21 [25216/50048]	Loss: 0.9750
Training Epoch: 21 [25344/50048]	Loss: 0.8408
Training Epoch: 21 [25472/50048]	Loss: 1.0104
Training Epoch: 21 [25600/50048]	Loss: 0.9278
Training Epoch: 21 [25728/50048]	Loss: 0.9825
Training Epoch: 21 [25856/50048]	Loss: 1.0499
Training Epoch: 21 [25984/50048]	Loss: 1.0707
Training Epoch: 21 [26112/50048]	Loss: 1.1388
Training Epoch: 21 [26240/50048]	Loss: 0.9587
Training Epoch: 21 [26368/50048]	Loss: 0.8440
Training Epoch: 21 [26496/50048]	Loss: 0.9716
Training Epoch: 21 [26624/50048]	Loss: 0.8697
Training Epoch: 21 [26752/50048]	Loss: 0.9793
Training Epoch: 21 [26880/50048]	Loss: 0.9890
Training Epoch: 21 [27008/50048]	Loss: 1.1572
Training Epoch: 21 [27136/50048]	Loss: 0.7040
Training Epoch: 21 [27264/50048]	Loss: 0.9881
Training Epoch: 21 [27392/50048]	Loss: 1.1803
Training Epoch: 21 [27520/50048]	Loss: 0.9953
Training Epoch: 21 [27648/50048]	Loss: 1.2370
Training Epoch: 21 [27776/50048]	Loss: 0.8202
Training Epoch: 21 [27904/50048]	Loss: 1.0098
Training Epoch: 21 [28032/50048]	Loss: 0.8191
Training Epoch: 21 [28160/50048]	Loss: 0.8798
Training Epoch: 21 [28288/50048]	Loss: 0.9085
Training Epoch: 21 [28416/50048]	Loss: 0.9339
Training Epoch: 21 [28544/50048]	Loss: 0.9113
Training Epoch: 21 [28672/50048]	Loss: 1.1283
Training Epoch: 21 [28800/50048]	Loss: 1.2505
Training Epoch: 21 [28928/50048]	Loss: 0.9469
Training Epoch: 21 [29056/50048]	Loss: 1.1665
Training Epoch: 21 [29184/50048]	Loss: 1.0244
Training Epoch: 21 [29312/50048]	Loss: 0.8472
Training Epoch: 21 [29440/50048]	Loss: 1.0351
Training Epoch: 21 [29568/50048]	Loss: 0.9374
Training Epoch: 21 [29696/50048]	Loss: 1.2621
Training Epoch: 21 [29824/50048]	Loss: 1.0251
Training Epoch: 21 [29952/50048]	Loss: 1.1150
Training Epoch: 21 [30080/50048]	Loss: 0.8497
Training Epoch: 21 [30208/50048]	Loss: 0.9466
Training Epoch: 21 [30336/50048]	Loss: 0.8803
Training Epoch: 21 [30464/50048]	Loss: 0.7321
Training Epoch: 21 [30592/50048]	Loss: 0.8459
Training Epoch: 21 [30720/50048]	Loss: 1.1189
Training Epoch: 21 [30848/50048]	Loss: 0.9037
Training Epoch: 21 [30976/50048]	Loss: 1.0559
Training Epoch: 21 [31104/50048]	Loss: 1.0374
Training Epoch: 21 [31232/50048]	Loss: 0.8896
Training Epoch: 21 [31360/50048]	Loss: 1.0012
Training Epoch: 21 [31488/50048]	Loss: 0.8404
Training Epoch: 21 [31616/50048]	Loss: 0.7862
Training Epoch: 21 [31744/50048]	Loss: 0.9731
Training Epoch: 21 [31872/50048]	Loss: 0.9280
Training Epoch: 21 [32000/50048]	Loss: 1.1445
Training Epoch: 21 [32128/50048]	Loss: 0.7907
Training Epoch: 21 [32256/50048]	Loss: 1.1354
Training Epoch: 21 [32384/50048]	Loss: 0.9100
Training Epoch: 21 [32512/50048]	Loss: 1.0670
Training Epoch: 21 [32640/50048]	Loss: 1.0314
Training Epoch: 21 [32768/50048]	Loss: 0.9096
Training Epoch: 21 [32896/50048]	Loss: 0.7871
Training Epoch: 21 [33024/50048]	Loss: 0.9308
Training Epoch: 21 [33152/50048]	Loss: 1.0598
Training Epoch: 21 [33280/50048]	Loss: 0.9591
Training Epoch: 21 [33408/50048]	Loss: 0.8705
Training Epoch: 21 [33536/50048]	Loss: 0.8378
Training Epoch: 21 [33664/50048]	Loss: 0.9973
Training Epoch: 21 [33792/50048]	Loss: 0.9159
Training Epoch: 21 [33920/50048]	Loss: 0.8138
Training Epoch: 21 [34048/50048]	Loss: 0.8631
Training Epoch: 21 [34176/50048]	Loss: 1.0454
Training Epoch: 21 [34304/50048]	Loss: 0.8484
Training Epoch: 21 [34432/50048]	Loss: 0.7698
Training Epoch: 21 [34560/50048]	Loss: 0.7754
Training Epoch: 21 [34688/50048]	Loss: 0.7661
Training Epoch: 21 [34816/50048]	Loss: 1.0589
Training Epoch: 21 [34944/50048]	Loss: 0.8742
Training Epoch: 21 [35072/50048]	Loss: 1.0186
Training Epoch: 21 [35200/50048]	Loss: 1.0993
Training Epoch: 21 [35328/50048]	Loss: 0.6554
Training Epoch: 21 [35456/50048]	Loss: 1.0328
Training Epoch: 21 [35584/50048]	Loss: 0.8652
Training Epoch: 21 [35712/50048]	Loss: 0.8245
Training Epoch: 21 [35840/50048]	Loss: 1.0218
Training Epoch: 21 [35968/50048]	Loss: 0.8711
Training Epoch: 21 [36096/50048]	Loss: 1.0471
Training Epoch: 21 [36224/50048]	Loss: 0.9013
Training Epoch: 21 [36352/50048]	Loss: 0.9193
Training Epoch: 21 [36480/50048]	Loss: 0.9911
Training Epoch: 21 [36608/50048]	Loss: 0.8779
Training Epoch: 21 [36736/50048]	Loss: 1.0560
Training Epoch: 21 [36864/50048]	Loss: 1.0212
Training Epoch: 21 [36992/50048]	Loss: 1.0469
Training Epoch: 21 [37120/50048]	Loss: 1.0505
Training Epoch: 21 [37248/50048]	Loss: 1.1289
Training Epoch: 21 [37376/50048]	Loss: 0.8894
Training Epoch: 21 [37504/50048]	Loss: 0.9312
Training Epoch: 21 [37632/50048]	Loss: 1.0675
Training Epoch: 21 [37760/50048]	Loss: 1.0346
Training Epoch: 21 [37888/50048]	Loss: 0.8213
Training Epoch: 21 [38016/50048]	Loss: 0.9116
Training Epoch: 21 [38144/50048]	Loss: 0.9263
Training Epoch: 21 [38272/50048]	Loss: 0.9499
Training Epoch: 21 [38400/50048]	Loss: 1.0374
Training Epoch: 21 [38528/50048]	Loss: 0.9606
Training Epoch: 21 [38656/50048]	Loss: 1.1303
Training Epoch: 21 [38784/50048]	Loss: 0.8090
Training Epoch: 21 [38912/50048]	Loss: 1.1857
Training Epoch: 21 [39040/50048]	Loss: 1.1028
Training Epoch: 21 [39168/50048]	Loss: 0.8466
Training Epoch: 21 [39296/50048]	Loss: 1.0230
Training Epoch: 21 [39424/50048]	Loss: 0.9276
Training Epoch: 21 [39552/50048]	Loss: 0.9246
Training Epoch: 21 [39680/50048]	Loss: 1.0680
Training Epoch: 21 [39808/50048]	Loss: 0.8564
Training Epoch: 21 [39936/50048]	Loss: 1.1231
Training Epoch: 21 [40064/50048]	Loss: 0.9853
Training Epoch: 21 [40192/50048]	Loss: 0.9780
Training Epoch: 21 [40320/50048]	Loss: 0.7694
Training Epoch: 21 [40448/50048]	Loss: 0.8441
Training Epoch: 21 [40576/50048]	Loss: 1.0918
Training Epoch: 21 [40704/50048]	Loss: 0.9121
Training Epoch: 21 [40832/50048]	Loss: 0.9888
Training Epoch: 21 [40960/50048]	Loss: 0.9664
Training Epoch: 21 [41088/50048]	Loss: 1.1056
Training Epoch: 21 [41216/50048]	Loss: 0.8221
Training Epoch: 21 [41344/50048]	Loss: 1.1553
Training Epoch: 21 [41472/50048]	Loss: 0.8007
Training Epoch: 21 [41600/50048]	Loss: 1.0106
Training Epoch: 21 [41728/50048]	Loss: 1.0569
Training Epoch: 21 [41856/50048]	Loss: 1.1381
Training Epoch: 21 [41984/50048]	Loss: 1.0349
Training Epoch: 21 [42112/50048]	Loss: 1.0200
Training Epoch: 21 [42240/50048]	Loss: 1.1719
Training Epoch: 21 [42368/50048]	Loss: 1.0564
Training Epoch: 21 [42496/50048]	Loss: 1.0413
Training Epoch: 21 [42624/50048]	Loss: 0.8653
Training Epoch: 21 [42752/50048]	Loss: 0.8601
Training Epoch: 21 [42880/50048]	Loss: 0.8358
Training Epoch: 21 [43008/50048]	Loss: 1.2507
Training Epoch: 21 [43136/50048]	Loss: 1.0711
Training Epoch: 21 [43264/50048]	Loss: 1.0878
Training Epoch: 21 [43392/50048]	Loss: 0.9393
Training Epoch: 21 [43520/50048]	Loss: 0.8760
Training Epoch: 21 [43648/50048]	Loss: 1.0528
Training Epoch: 21 [43776/50048]	Loss: 1.0782
Training Epoch: 21 [43904/50048]	Loss: 0.8312
Training Epoch: 21 [44032/50048]	Loss: 1.0705
Training Epoch: 21 [44160/50048]	Loss: 1.1459
Training Epoch: 21 [44288/50048]	Loss: 0.8142
Training Epoch: 21 [44416/50048]	Loss: 0.9536
Training Epoch: 21 [44544/50048]	Loss: 0.8839
Training Epoch: 21 [44672/50048]	Loss: 1.0158
Training Epoch: 21 [44800/50048]	Loss: 0.9528
Training Epoch: 21 [44928/50048]	Loss: 0.9025
Training Epoch: 21 [45056/50048]	Loss: 0.9980
Training Epoch: 21 [45184/50048]	Loss: 0.7517
Training Epoch: 21 [45312/50048]	Loss: 1.1219
Training Epoch: 21 [45440/50048]	Loss: 1.0008
Training Epoch: 21 [45568/50048]	Loss: 0.9410
Training Epoch: 21 [45696/50048]	Loss: 0.9561
2022-12-06 04:10:25,762 [ZeusDataLoader(train)] train epoch 22 done: time=86.43 energy=10509.68
2022-12-06 04:10:25,764 [ZeusDataLoader(eval)] Epoch 22 begin.
Training Epoch: 21 [45824/50048]	Loss: 1.0763
Training Epoch: 21 [45952/50048]	Loss: 0.8204
Training Epoch: 21 [46080/50048]	Loss: 1.1196
Training Epoch: 21 [46208/50048]	Loss: 1.1311
Training Epoch: 21 [46336/50048]	Loss: 0.8998
Training Epoch: 21 [46464/50048]	Loss: 0.7945
Training Epoch: 21 [46592/50048]	Loss: 1.1419
Training Epoch: 21 [46720/50048]	Loss: 1.0480
Training Epoch: 21 [46848/50048]	Loss: 1.0692
Training Epoch: 21 [46976/50048]	Loss: 1.0266
Training Epoch: 21 [47104/50048]	Loss: 1.0686
Training Epoch: 21 [47232/50048]	Loss: 1.1061
Training Epoch: 21 [47360/50048]	Loss: 1.0041
Training Epoch: 21 [47488/50048]	Loss: 0.7324
Training Epoch: 21 [47616/50048]	Loss: 0.8950
Training Epoch: 21 [47744/50048]	Loss: 1.0154
Training Epoch: 21 [47872/50048]	Loss: 1.1652
Training Epoch: 21 [48000/50048]	Loss: 0.7821
Training Epoch: 21 [48128/50048]	Loss: 0.8756
Training Epoch: 21 [48256/50048]	Loss: 1.1847
Training Epoch: 21 [48384/50048]	Loss: 0.7360
Training Epoch: 21 [48512/50048]	Loss: 1.0074
Training Epoch: 21 [48640/50048]	Loss: 1.1105
Training Epoch: 21 [48768/50048]	Loss: 1.0670
Training Epoch: 21 [48896/50048]	Loss: 1.0324
Training Epoch: 21 [49024/50048]	Loss: 1.0340
Training Epoch: 21 [49152/50048]	Loss: 1.0025
Training Epoch: 21 [49280/50048]	Loss: 1.0920
Training Epoch: 21 [49408/50048]	Loss: 0.9002
Training Epoch: 21 [49536/50048]	Loss: 0.9318
Training Epoch: 21 [49664/50048]	Loss: 1.1647
Training Epoch: 21 [49792/50048]	Loss: 0.9532
Training Epoch: 21 [49920/50048]	Loss: 0.9418
Training Epoch: 21 [50048/50048]	Loss: 0.7650
2022-12-06 09:10:29.505 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:10:29,530 [ZeusDataLoader(eval)] eval epoch 22 done: time=3.76 energy=452.19
2022-12-06 04:10:29,530 [ZeusDataLoader(train)] Up to epoch 22: time=1986.83, energy=240934.54, cost=294315.08
2022-12-06 04:10:29,530 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:10:29,531 [ZeusDataLoader(train)] Expected next epoch: time=2076.63, energy=251732.56, cost=307571.46
2022-12-06 04:10:29,531 [ZeusDataLoader(train)] Epoch 23 begin.
Validation Epoch: 21, Average loss: 0.0117, Accuracy: 0.6192
2022-12-06 04:10:29,710 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:10:29,711 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:10:29.713 [ZeusMonitor] Monitor started.
2022-12-06 09:10:29.713 [ZeusMonitor] Running indefinitely. 2022-12-06 09:10:29.713 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:10:29.713 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e23+gpu0.power.log
Training Epoch: 22 [128/50048]	Loss: 0.9514
Training Epoch: 22 [256/50048]	Loss: 0.9537
Training Epoch: 22 [384/50048]	Loss: 0.7202
Training Epoch: 22 [512/50048]	Loss: 0.9056
Training Epoch: 22 [640/50048]	Loss: 0.6901
Training Epoch: 22 [768/50048]	Loss: 0.9485
Training Epoch: 22 [896/50048]	Loss: 0.8987
Training Epoch: 22 [1024/50048]	Loss: 0.7156
Training Epoch: 22 [1152/50048]	Loss: 0.8770
Training Epoch: 22 [1280/50048]	Loss: 0.8474
Training Epoch: 22 [1408/50048]	Loss: 0.7854
Training Epoch: 22 [1536/50048]	Loss: 0.8091
Training Epoch: 22 [1664/50048]	Loss: 0.8875
Training Epoch: 22 [1792/50048]	Loss: 1.0469
Training Epoch: 22 [1920/50048]	Loss: 0.8371
Training Epoch: 22 [2048/50048]	Loss: 0.7191
Training Epoch: 22 [2176/50048]	Loss: 0.6711
Training Epoch: 22 [2304/50048]	Loss: 1.0628
Training Epoch: 22 [2432/50048]	Loss: 0.8462
Training Epoch: 22 [2560/50048]	Loss: 0.9614
Training Epoch: 22 [2688/50048]	Loss: 0.8515
Training Epoch: 22 [2816/50048]	Loss: 0.8227
Training Epoch: 22 [2944/50048]	Loss: 0.8719
Training Epoch: 22 [3072/50048]	Loss: 0.7424
Training Epoch: 22 [3200/50048]	Loss: 0.5094
Training Epoch: 22 [3328/50048]	Loss: 0.8591
Training Epoch: 22 [3456/50048]	Loss: 0.8805
Training Epoch: 22 [3584/50048]	Loss: 0.8423
Training Epoch: 22 [3712/50048]	Loss: 0.8410
Training Epoch: 22 [3840/50048]	Loss: 0.7214
Training Epoch: 22 [3968/50048]	Loss: 0.8439
Training Epoch: 22 [4096/50048]	Loss: 0.8753
Training Epoch: 22 [4224/50048]	Loss: 0.8967
Training Epoch: 22 [4352/50048]	Loss: 0.8211
Training Epoch: 22 [4480/50048]	Loss: 0.7594
Training Epoch: 22 [4608/50048]	Loss: 0.9411
Training Epoch: 22 [4736/50048]	Loss: 0.9957
Training Epoch: 22 [4864/50048]	Loss: 0.7033
Training Epoch: 22 [4992/50048]	Loss: 1.0965
Training Epoch: 22 [5120/50048]	Loss: 0.8898
Training Epoch: 22 [5248/50048]	Loss: 0.7895
Training Epoch: 22 [5376/50048]	Loss: 0.9457
Training Epoch: 22 [5504/50048]	Loss: 0.8845
Training Epoch: 22 [5632/50048]	Loss: 0.7804
Training Epoch: 22 [5760/50048]	Loss: 0.7490
Training Epoch: 22 [5888/50048]	Loss: 1.0495
Training Epoch: 22 [6016/50048]	Loss: 0.9220
Training Epoch: 22 [6144/50048]	Loss: 0.8200
Training Epoch: 22 [6272/50048]	Loss: 0.7545
Training Epoch: 22 [6400/50048]	Loss: 0.8750
Training Epoch: 22 [6528/50048]	Loss: 1.1075
Training Epoch: 22 [6656/50048]	Loss: 0.7354
Training Epoch: 22 [6784/50048]	Loss: 0.9527
Training Epoch: 22 [6912/50048]	Loss: 0.9666
Training Epoch: 22 [7040/50048]	Loss: 0.8656
Training Epoch: 22 [7168/50048]	Loss: 0.9737
Training Epoch: 22 [7296/50048]	Loss: 0.8692
Training Epoch: 22 [7424/50048]	Loss: 0.9025
Training Epoch: 22 [7552/50048]	Loss: 0.8504
Training Epoch: 22 [7680/50048]	Loss: 0.9913
Training Epoch: 22 [7808/50048]	Loss: 0.9745
Training Epoch: 22 [7936/50048]	Loss: 1.0153
Training Epoch: 22 [8064/50048]	Loss: 0.8046
Training Epoch: 22 [8192/50048]	Loss: 0.9310
Training Epoch: 22 [8320/50048]	Loss: 0.9463
Training Epoch: 22 [8448/50048]	Loss: 0.9630
Training Epoch: 22 [8576/50048]	Loss: 0.9378
Training Epoch: 22 [8704/50048]	Loss: 0.9364
Training Epoch: 22 [8832/50048]	Loss: 0.9599
Training Epoch: 22 [8960/50048]	Loss: 0.8051
Training Epoch: 22 [9088/50048]	Loss: 0.9802
Training Epoch: 22 [9216/50048]	Loss: 0.7653
Training Epoch: 22 [9344/50048]	Loss: 0.6531
Training Epoch: 22 [9472/50048]	Loss: 0.7872
Training Epoch: 22 [9600/50048]	Loss: 0.7565
Training Epoch: 22 [9728/50048]	Loss: 0.9856
Training Epoch: 22 [9856/50048]	Loss: 0.9215
Training Epoch: 22 [9984/50048]	Loss: 1.0306
Training Epoch: 22 [10112/50048]	Loss: 0.4971
Training Epoch: 22 [10240/50048]	Loss: 0.7194
Training Epoch: 22 [10368/50048]	Loss: 0.8776
Training Epoch: 22 [10496/50048]	Loss: 1.0388
Training Epoch: 22 [10624/50048]	Loss: 0.9295
Training Epoch: 22 [10752/50048]	Loss: 0.8529
Training Epoch: 22 [10880/50048]	Loss: 0.9339
Training Epoch: 22 [11008/50048]	Loss: 0.9260
Training Epoch: 22 [11136/50048]	Loss: 0.8708
Training Epoch: 22 [11264/50048]	Loss: 0.9037
Training Epoch: 22 [11392/50048]	Loss: 0.8565
Training Epoch: 22 [11520/50048]	Loss: 0.9245
Training Epoch: 22 [11648/50048]	Loss: 1.0609
Training Epoch: 22 [11776/50048]	Loss: 0.7352
Training Epoch: 22 [11904/50048]	Loss: 0.8663
Training Epoch: 22 [12032/50048]	Loss: 0.9046
Training Epoch: 22 [12160/50048]	Loss: 0.9208
Training Epoch: 22 [12288/50048]	Loss: 0.6734
Training Epoch: 22 [12416/50048]	Loss: 0.9005
Training Epoch: 22 [12544/50048]	Loss: 0.8668
Training Epoch: 22 [12672/50048]	Loss: 0.8306
Training Epoch: 22 [12800/50048]	Loss: 0.7016
Training Epoch: 22 [12928/50048]	Loss: 0.8563
Training Epoch: 22 [13056/50048]	Loss: 0.8699
Training Epoch: 22 [13184/50048]	Loss: 0.7871
Training Epoch: 22 [13312/50048]	Loss: 0.8186
Training Epoch: 22 [13440/50048]	Loss: 0.7283
Training Epoch: 22 [13568/50048]	Loss: 0.8272
Training Epoch: 22 [13696/50048]	Loss: 0.9261
Training Epoch: 22 [13824/50048]	Loss: 0.9779
Training Epoch: 22 [13952/50048]	Loss: 0.8419
Training Epoch: 22 [14080/50048]	Loss: 1.0090
Training Epoch: 22 [14208/50048]	Loss: 0.8274
Training Epoch: 22 [14336/50048]	Loss: 0.7372
Training Epoch: 22 [14464/50048]	Loss: 0.8814
Training Epoch: 22 [14592/50048]	Loss: 0.8874
Training Epoch: 22 [14720/50048]	Loss: 0.9307
Training Epoch: 22 [14848/50048]	Loss: 0.9239
Training Epoch: 22 [14976/50048]	Loss: 0.8544
Training Epoch: 22 [15104/50048]	Loss: 1.2434
Training Epoch: 22 [15232/50048]	Loss: 0.9215
Training Epoch: 22 [15360/50048]	Loss: 0.6922
Training Epoch: 22 [15488/50048]	Loss: 0.9321
Training Epoch: 22 [15616/50048]	Loss: 0.9585
Training Epoch: 22 [15744/50048]	Loss: 0.8363
Training Epoch: 22 [15872/50048]	Loss: 0.8626
Training Epoch: 22 [16000/50048]	Loss: 0.9570
Training Epoch: 22 [16128/50048]	Loss: 0.8961
Training Epoch: 22 [16256/50048]	Loss: 0.9507
Training Epoch: 22 [16384/50048]	Loss: 0.8640
Training Epoch: 22 [16512/50048]	Loss: 0.7397
Training Epoch: 22 [16640/50048]	Loss: 0.7355
Training Epoch: 22 [16768/50048]	Loss: 1.2265
Training Epoch: 22 [16896/50048]	Loss: 0.8295
Training Epoch: 22 [17024/50048]	Loss: 0.9354
Training Epoch: 22 [17152/50048]	Loss: 0.8715
Training Epoch: 22 [17280/50048]	Loss: 0.8822
Training Epoch: 22 [17408/50048]	Loss: 0.8831
Training Epoch: 22 [17536/50048]	Loss: 0.9360
Training Epoch: 22 [17664/50048]	Loss: 1.0413
Training Epoch: 22 [17792/50048]	Loss: 0.9213
Training Epoch: 22 [17920/50048]	Loss: 1.0150
Training Epoch: 22 [18048/50048]	Loss: 0.8301
Training Epoch: 22 [18176/50048]	Loss: 0.8473
Training Epoch: 22 [18304/50048]	Loss: 1.0431
Training Epoch: 22 [18432/50048]	Loss: 0.7626
Training Epoch: 22 [18560/50048]	Loss: 0.9161
Training Epoch: 22 [18688/50048]	Loss: 0.9881
Training Epoch: 22 [18816/50048]	Loss: 1.0408
Training Epoch: 22 [18944/50048]	Loss: 0.8196
Training Epoch: 22 [19072/50048]	Loss: 0.9451
Training Epoch: 22 [19200/50048]	Loss: 0.8106
Training Epoch: 22 [19328/50048]	Loss: 0.9149
Training Epoch: 22 [19456/50048]	Loss: 0.8196
Training Epoch: 22 [19584/50048]	Loss: 1.0763
Training Epoch: 22 [19712/50048]	Loss: 0.8487
Training Epoch: 22 [19840/50048]	Loss: 1.0162
Training Epoch: 22 [19968/50048]	Loss: 0.9288
Training Epoch: 22 [20096/50048]	Loss: 0.9283
Training Epoch: 22 [20224/50048]	Loss: 0.8592
Training Epoch: 22 [20352/50048]	Loss: 0.6971
Training Epoch: 22 [20480/50048]	Loss: 1.0937
Training Epoch: 22 [20608/50048]	Loss: 0.9322
Training Epoch: 22 [20736/50048]	Loss: 0.7148
Training Epoch: 22 [20864/50048]	Loss: 0.8774
Training Epoch: 22 [20992/50048]	Loss: 1.0984
Training Epoch: 22 [21120/50048]	Loss: 0.7060
Training Epoch: 22 [21248/50048]	Loss: 0.9611
Training Epoch: 22 [21376/50048]	Loss: 0.9735
Training Epoch: 22 [21504/50048]	Loss: 0.8514
Training Epoch: 22 [21632/50048]	Loss: 0.9992
Training Epoch: 22 [21760/50048]	Loss: 1.0768
Training Epoch: 22 [21888/50048]	Loss: 0.9325
Training Epoch: 22 [22016/50048]	Loss: 0.7322
Training Epoch: 22 [22144/50048]	Loss: 1.0986
Training Epoch: 22 [22272/50048]	Loss: 0.8351
Training Epoch: 22 [22400/50048]	Loss: 0.9187
Training Epoch: 22 [22528/50048]	Loss: 1.1931
Training Epoch: 22 [22656/50048]	Loss: 0.9603
Training Epoch: 22 [22784/50048]	Loss: 0.9380
Training Epoch: 22 [22912/50048]	Loss: 0.8126
Training Epoch: 22 [23040/50048]	Loss: 0.8591
Training Epoch: 22 [23168/50048]	Loss: 0.9212
Training Epoch: 22 [23296/50048]	Loss: 0.9624
Training Epoch: 22 [23424/50048]	Loss: 0.8173
Training Epoch: 22 [23552/50048]	Loss: 0.9450
Training Epoch: 22 [23680/50048]	Loss: 0.7481
Training Epoch: 22 [23808/50048]	Loss: 1.0848
Training Epoch: 22 [23936/50048]	Loss: 0.7894
Training Epoch: 22 [24064/50048]	Loss: 0.8135
Training Epoch: 22 [24192/50048]	Loss: 1.0947
Training Epoch: 22 [24320/50048]	Loss: 1.0681
Training Epoch: 22 [24448/50048]	Loss: 0.9573
Training Epoch: 22 [24576/50048]	Loss: 0.9471
Training Epoch: 22 [24704/50048]	Loss: 1.1156
Training Epoch: 22 [24832/50048]	Loss: 0.8929
Training Epoch: 22 [24960/50048]	Loss: 1.0650
Training Epoch: 22 [25088/50048]	Loss: 0.8782
Training Epoch: 22 [25216/50048]	Loss: 0.7851
Training Epoch: 22 [25344/50048]	Loss: 1.0649
Training Epoch: 22 [25472/50048]	Loss: 1.0033
Training Epoch: 22 [25600/50048]	Loss: 0.9117
Training Epoch: 22 [25728/50048]	Loss: 1.0688
Training Epoch: 22 [25856/50048]	Loss: 0.8670
Training Epoch: 22 [25984/50048]	Loss: 0.8500
Training Epoch: 22 [26112/50048]	Loss: 0.9891
Training Epoch: 22 [26240/50048]	Loss: 1.3988
Training Epoch: 22 [26368/50048]	Loss: 0.9730
Training Epoch: 22 [26496/50048]	Loss: 0.7051
Training Epoch: 22 [26624/50048]	Loss: 1.0257
Training Epoch: 22 [26752/50048]	Loss: 0.8923
Training Epoch: 22 [26880/50048]	Loss: 0.9410
Training Epoch: 22 [27008/50048]	Loss: 0.9397
Training Epoch: 22 [27136/50048]	Loss: 0.8878
Training Epoch: 22 [27264/50048]	Loss: 1.1395
Training Epoch: 22 [27392/50048]	Loss: 1.0412
Training Epoch: 22 [27520/50048]	Loss: 0.8099
Training Epoch: 22 [27648/50048]	Loss: 0.7716
Training Epoch: 22 [27776/50048]	Loss: 0.8661
Training Epoch: 22 [27904/50048]	Loss: 0.7299
Training Epoch: 22 [28032/50048]	Loss: 1.0476
Training Epoch: 22 [28160/50048]	Loss: 0.9742
Training Epoch: 22 [28288/50048]	Loss: 0.7580
Training Epoch: 22 [28416/50048]	Loss: 1.0042
Training Epoch: 22 [28544/50048]	Loss: 0.8682
Training Epoch: 22 [28672/50048]	Loss: 0.9002
Training Epoch: 22 [28800/50048]	Loss: 1.2682
Training Epoch: 22 [28928/50048]	Loss: 0.9989
Training Epoch: 22 [29056/50048]	Loss: 0.8484
Training Epoch: 22 [29184/50048]	Loss: 1.0687
Training Epoch: 22 [29312/50048]	Loss: 0.7458
Training Epoch: 22 [29440/50048]	Loss: 0.7821
Training Epoch: 22 [29568/50048]	Loss: 0.9500
Training Epoch: 22 [29696/50048]	Loss: 1.0991
Training Epoch: 22 [29824/50048]	Loss: 0.9006
Training Epoch: 22 [29952/50048]	Loss: 0.8827
Training Epoch: 22 [30080/50048]	Loss: 0.7809
Training Epoch: 22 [30208/50048]	Loss: 0.9010
Training Epoch: 22 [30336/50048]	Loss: 0.7531
Training Epoch: 22 [30464/50048]	Loss: 0.8254
Training Epoch: 22 [30592/50048]	Loss: 0.9336
Training Epoch: 22 [30720/50048]	Loss: 0.9739
Training Epoch: 22 [30848/50048]	Loss: 1.0528
Training Epoch: 22 [30976/50048]	Loss: 0.9999
Training Epoch: 22 [31104/50048]	Loss: 1.1901
Training Epoch: 22 [31232/50048]	Loss: 0.9621
Training Epoch: 22 [31360/50048]	Loss: 0.7613
Training Epoch: 22 [31488/50048]	Loss: 1.0002
Training Epoch: 22 [31616/50048]	Loss: 1.0216
Training Epoch: 22 [31744/50048]	Loss: 0.9557
Training Epoch: 22 [31872/50048]	Loss: 0.8039
Training Epoch: 22 [32000/50048]	Loss: 0.7360
Training Epoch: 22 [32128/50048]	Loss: 0.8975
Training Epoch: 22 [32256/50048]	Loss: 1.0011
Training Epoch: 22 [32384/50048]	Loss: 0.8003
Training Epoch: 22 [32512/50048]	Loss: 0.7486
Training Epoch: 22 [32640/50048]	Loss: 0.9502
Training Epoch: 22 [32768/50048]	Loss: 0.9447
Training Epoch: 22 [32896/50048]	Loss: 0.7763
Training Epoch: 22 [33024/50048]	Loss: 0.9285
Training Epoch: 22 [33152/50048]	Loss: 0.8857
Training Epoch: 22 [33280/50048]	Loss: 1.0035
Training Epoch: 22 [33408/50048]	Loss: 1.0448
Training Epoch: 22 [33536/50048]	Loss: 0.7770
Training Epoch: 22 [33664/50048]	Loss: 1.0686
Training Epoch: 22 [33792/50048]	Loss: 1.0809
Training Epoch: 22 [33920/50048]	Loss: 0.9366
Training Epoch: 22 [34048/50048]	Loss: 0.8112
Training Epoch: 22 [34176/50048]	Loss: 0.9517
Training Epoch: 22 [34304/50048]	Loss: 0.9125
Training Epoch: 22 [34432/50048]	Loss: 0.9867
Training Epoch: 22 [34560/50048]	Loss: 1.0079
Training Epoch: 22 [34688/50048]	Loss: 0.9473
Training Epoch: 22 [34816/50048]	Loss: 0.9213
Training Epoch: 22 [34944/50048]	Loss: 0.8145
Training Epoch: 22 [35072/50048]	Loss: 0.8310
Training Epoch: 22 [35200/50048]	Loss: 0.7557
Training Epoch: 22 [35328/50048]	Loss: 0.7437
Training Epoch: 22 [35456/50048]	Loss: 1.0982
Training Epoch: 22 [35584/50048]	Loss: 1.1631
Training Epoch: 22 [35712/50048]	Loss: 0.9994
Training Epoch: 22 [35840/50048]	Loss: 0.9528
Training Epoch: 22 [35968/50048]	Loss: 0.9131
Training Epoch: 22 [36096/50048]	Loss: 1.1030
Training Epoch: 22 [36224/50048]	Loss: 1.3324
Training Epoch: 22 [36352/50048]	Loss: 0.9343
Training Epoch: 22 [36480/50048]	Loss: 0.7689
Training Epoch: 22 [36608/50048]	Loss: 0.7299
Training Epoch: 22 [36736/50048]	Loss: 0.9645
Training Epoch: 22 [36864/50048]	Loss: 0.9854
Training Epoch: 22 [36992/50048]	Loss: 0.7815
Training Epoch: 22 [37120/50048]	Loss: 1.0727
Training Epoch: 22 [37248/50048]	Loss: 1.1219
Training Epoch: 22 [37376/50048]	Loss: 0.6733
Training Epoch: 22 [37504/50048]	Loss: 0.7702
Training Epoch: 22 [37632/50048]	Loss: 1.1102
Training Epoch: 22 [37760/50048]	Loss: 0.8362
Training Epoch: 22 [37888/50048]	Loss: 1.0831
Training Epoch: 22 [38016/50048]	Loss: 0.9751
Training Epoch: 22 [38144/50048]	Loss: 1.0049
Training Epoch: 22 [38272/50048]	Loss: 0.9594
Training Epoch: 22 [38400/50048]	Loss: 0.9789
Training Epoch: 22 [38528/50048]	Loss: 0.9090
Training Epoch: 22 [38656/50048]	Loss: 0.9996
Training Epoch: 22 [38784/50048]	Loss: 0.7057
Training Epoch: 22 [38912/50048]	Loss: 0.8166
Training Epoch: 22 [39040/50048]	Loss: 0.9868
Training Epoch: 22 [39168/50048]	Loss: 0.8391
Training Epoch: 22 [39296/50048]	Loss: 0.7466
Training Epoch: 22 [39424/50048]	Loss: 0.8897
Training Epoch: 22 [39552/50048]	Loss: 0.9180
Training Epoch: 22 [39680/50048]	Loss: 1.1495
Training Epoch: 22 [39808/50048]	Loss: 0.9034
Training Epoch: 22 [39936/50048]	Loss: 0.8689
Training Epoch: 22 [40064/50048]	Loss: 0.8593
Training Epoch: 22 [40192/50048]	Loss: 0.9462
Training Epoch: 22 [40320/50048]	Loss: 0.7705
Training Epoch: 22 [40448/50048]	Loss: 0.7876
Training Epoch: 22 [40576/50048]	Loss: 0.8640
Training Epoch: 22 [40704/50048]	Loss: 0.8815
Training Epoch: 22 [40832/50048]	Loss: 0.8922
Training Epoch: 22 [40960/50048]	Loss: 1.1067
Training Epoch: 22 [41088/50048]	Loss: 0.8649
Training Epoch: 22 [41216/50048]	Loss: 0.9922
Training Epoch: 22 [41344/50048]	Loss: 0.8635
Training Epoch: 22 [41472/50048]	Loss: 0.7623
Training Epoch: 22 [41600/50048]	Loss: 1.0194
Training Epoch: 22 [41728/50048]	Loss: 0.9288
Training Epoch: 22 [41856/50048]	Loss: 0.9747
Training Epoch: 22 [41984/50048]	Loss: 0.9528
Training Epoch: 22 [42112/50048]	Loss: 0.9169
Training Epoch: 22 [42240/50048]	Loss: 0.9740
Training Epoch: 22 [42368/50048]	Loss: 0.7319
Training Epoch: 22 [42496/50048]	Loss: 0.8998
Training Epoch: 22 [42624/50048]	Loss: 1.1877
Training Epoch: 22 [42752/50048]	Loss: 0.6468
Training Epoch: 22 [42880/50048]	Loss: 1.0094
Training Epoch: 22 [43008/50048]	Loss: 0.8173
Training Epoch: 22 [43136/50048]	Loss: 1.0905
Training Epoch: 22 [43264/50048]	Loss: 0.8465
Training Epoch: 22 [43392/50048]	Loss: 0.8600
Training Epoch: 22 [43520/50048]	Loss: 0.7881
Training Epoch: 22 [43648/50048]	Loss: 0.8682
Training Epoch: 22 [43776/50048]	Loss: 0.9323
Training Epoch: 22 [43904/50048]	Loss: 0.7397
Training Epoch: 22 [44032/50048]	Loss: 0.9684
Training Epoch: 22 [44160/50048]	Loss: 1.1137
Training Epoch: 22 [44288/50048]	Loss: 0.8863
Training Epoch: 22 [44416/50048]	Loss: 1.0132
Training Epoch: 22 [44544/50048]	Loss: 0.9840
Training Epoch: 22 [44672/50048]	Loss: 0.8975
Training Epoch: 22 [44800/50048]	Loss: 1.0976
Training Epoch: 22 [44928/50048]	Loss: 0.8672
Training Epoch: 22 [45056/50048]	Loss: 0.8065
Training Epoch: 22 [45184/50048]	Loss: 1.0427
Training Epoch: 22 [45312/50048]	Loss: 0.7851
Training Epoch: 22 [45440/50048]	Loss: 1.0229
Training Epoch: 22 [45568/50048]	Loss: 0.9379
Training Epoch: 22 [45696/50048]	Loss: 0.9561
2022-12-06 04:11:56,029 [ZeusDataLoader(train)] train epoch 23 done: time=86.49 energy=10509.20
2022-12-06 04:11:56,030 [ZeusDataLoader(eval)] Epoch 23 begin.
Training Epoch: 22 [45824/50048]	Loss: 0.8894
Training Epoch: 22 [45952/50048]	Loss: 0.9075
Training Epoch: 22 [46080/50048]	Loss: 0.9660
Training Epoch: 22 [46208/50048]	Loss: 0.9313
Training Epoch: 22 [46336/50048]	Loss: 0.9719
Training Epoch: 22 [46464/50048]	Loss: 0.9419
Training Epoch: 22 [46592/50048]	Loss: 0.7981
Training Epoch: 22 [46720/50048]	Loss: 0.7857
Training Epoch: 22 [46848/50048]	Loss: 0.9328
Training Epoch: 22 [46976/50048]	Loss: 0.9231
Training Epoch: 22 [47104/50048]	Loss: 0.9676
Training Epoch: 22 [47232/50048]	Loss: 1.3664
Training Epoch: 22 [47360/50048]	Loss: 1.0904
Training Epoch: 22 [47488/50048]	Loss: 0.9227
Training Epoch: 22 [47616/50048]	Loss: 1.4783
Training Epoch: 22 [47744/50048]	Loss: 1.2874
Training Epoch: 22 [47872/50048]	Loss: 0.9979
Training Epoch: 22 [48000/50048]	Loss: 1.0380
Training Epoch: 22 [48128/50048]	Loss: 0.8017
Training Epoch: 22 [48256/50048]	Loss: 0.7634
Training Epoch: 22 [48384/50048]	Loss: 0.9586
Training Epoch: 22 [48512/50048]	Loss: 0.8429
Training Epoch: 22 [48640/50048]	Loss: 1.0688
Training Epoch: 22 [48768/50048]	Loss: 0.9695
Training Epoch: 22 [48896/50048]	Loss: 0.8930
Training Epoch: 22 [49024/50048]	Loss: 1.0791
Training Epoch: 22 [49152/50048]	Loss: 0.9276
Training Epoch: 22 [49280/50048]	Loss: 0.9682
Training Epoch: 22 [49408/50048]	Loss: 0.9054
Training Epoch: 22 [49536/50048]	Loss: 0.8884
Training Epoch: 22 [49664/50048]	Loss: 1.0722
Training Epoch: 22 [49792/50048]	Loss: 0.8609
Training Epoch: 22 [49920/50048]	Loss: 0.7938
Training Epoch: 22 [50048/50048]	Loss: 0.9327
2022-12-06 09:11:59.741 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:11:59,789 [ZeusDataLoader(eval)] eval epoch 23 done: time=3.75 energy=453.86
2022-12-06 04:11:59,789 [ZeusDataLoader(train)] Up to epoch 23: time=2077.07, energy=251897.60, cost=307692.34
2022-12-06 04:11:59,789 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:11:59,790 [ZeusDataLoader(train)] Expected next epoch: time=2166.87, energy=262695.62, cost=320948.72
2022-12-06 04:11:59,790 [ZeusDataLoader(train)] Epoch 24 begin.
Validation Epoch: 22, Average loss: 0.0119, Accuracy: 0.6091
2022-12-06 04:11:59,961 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:11:59,962 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:11:59.976 [ZeusMonitor] Monitor started.
2022-12-06 09:11:59.976 [ZeusMonitor] Running indefinitely. 2022-12-06 09:11:59.976 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:11:59.976 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e24+gpu0.power.log
Training Epoch: 23 [128/50048]	Loss: 0.9493
Training Epoch: 23 [256/50048]	Loss: 0.7679
Training Epoch: 23 [384/50048]	Loss: 0.8998
Training Epoch: 23 [512/50048]	Loss: 1.0438
Training Epoch: 23 [640/50048]	Loss: 0.9187
Training Epoch: 23 [768/50048]	Loss: 0.7257
Training Epoch: 23 [896/50048]	Loss: 0.8844
Training Epoch: 23 [1024/50048]	Loss: 0.6361
Training Epoch: 23 [1152/50048]	Loss: 0.8029
Training Epoch: 23 [1280/50048]	Loss: 0.9038
Training Epoch: 23 [1408/50048]	Loss: 0.8217
Training Epoch: 23 [1536/50048]	Loss: 0.9738
Training Epoch: 23 [1664/50048]	Loss: 0.7518
Training Epoch: 23 [1792/50048]	Loss: 0.8545
Training Epoch: 23 [1920/50048]	Loss: 0.7121
Training Epoch: 23 [2048/50048]	Loss: 0.7875
Training Epoch: 23 [2176/50048]	Loss: 0.8647
Training Epoch: 23 [2304/50048]	Loss: 0.7704
Training Epoch: 23 [2432/50048]	Loss: 0.6623
Training Epoch: 23 [2560/50048]	Loss: 0.8324
Training Epoch: 23 [2688/50048]	Loss: 0.7451
Training Epoch: 23 [2816/50048]	Loss: 0.8903
Training Epoch: 23 [2944/50048]	Loss: 0.9458
Training Epoch: 23 [3072/50048]	Loss: 0.9566
Training Epoch: 23 [3200/50048]	Loss: 0.8181
Training Epoch: 23 [3328/50048]	Loss: 0.9169
Training Epoch: 23 [3456/50048]	Loss: 0.9652
Training Epoch: 23 [3584/50048]	Loss: 0.8243
Training Epoch: 23 [3712/50048]	Loss: 0.8026
Training Epoch: 23 [3840/50048]	Loss: 0.7593
Training Epoch: 23 [3968/50048]	Loss: 0.6955
Training Epoch: 23 [4096/50048]	Loss: 0.7968
Training Epoch: 23 [4224/50048]	Loss: 1.0363
Training Epoch: 23 [4352/50048]	Loss: 0.9217
Training Epoch: 23 [4480/50048]	Loss: 0.8791
Training Epoch: 23 [4608/50048]	Loss: 0.8396
Training Epoch: 23 [4736/50048]	Loss: 0.9244
Training Epoch: 23 [4864/50048]	Loss: 0.9118
Training Epoch: 23 [4992/50048]	Loss: 0.6393
Training Epoch: 23 [5120/50048]	Loss: 0.7497
Training Epoch: 23 [5248/50048]	Loss: 0.8020
Training Epoch: 23 [5376/50048]	Loss: 0.8415
Training Epoch: 23 [5504/50048]	Loss: 0.8556
Training Epoch: 23 [5632/50048]	Loss: 0.8962
Training Epoch: 23 [5760/50048]	Loss: 0.8160
Training Epoch: 23 [5888/50048]	Loss: 0.7706
Training Epoch: 23 [6016/50048]	Loss: 0.7673
Training Epoch: 23 [6144/50048]	Loss: 0.7454
Training Epoch: 23 [6272/50048]	Loss: 0.7498
Training Epoch: 23 [6400/50048]	Loss: 0.8242
Training Epoch: 23 [6528/50048]	Loss: 0.9021
Training Epoch: 23 [6656/50048]	Loss: 0.6555
Training Epoch: 23 [6784/50048]	Loss: 0.7966
Training Epoch: 23 [6912/50048]	Loss: 0.9225
Training Epoch: 23 [7040/50048]	Loss: 0.7673
Training Epoch: 23 [7168/50048]	Loss: 1.1346
Training Epoch: 23 [7296/50048]	Loss: 0.8709
Training Epoch: 23 [7424/50048]	Loss: 0.8168
Training Epoch: 23 [7552/50048]	Loss: 0.6572
Training Epoch: 23 [7680/50048]	Loss: 0.7236
Training Epoch: 23 [7808/50048]	Loss: 0.7777
Training Epoch: 23 [7936/50048]	Loss: 0.7898
Training Epoch: 23 [8064/50048]	Loss: 0.8317
Training Epoch: 23 [8192/50048]	Loss: 0.7494
Training Epoch: 23 [8320/50048]	Loss: 0.9684
Training Epoch: 23 [8448/50048]	Loss: 0.8264
Training Epoch: 23 [8576/50048]	Loss: 0.7031
Training Epoch: 23 [8704/50048]	Loss: 0.7406
Training Epoch: 23 [8832/50048]	Loss: 0.8306
Training Epoch: 23 [8960/50048]	Loss: 0.7883
Training Epoch: 23 [9088/50048]	Loss: 0.8009
Training Epoch: 23 [9216/50048]	Loss: 0.6938
Training Epoch: 23 [9344/50048]	Loss: 0.7424
Training Epoch: 23 [9472/50048]	Loss: 0.9634
Training Epoch: 23 [9600/50048]	Loss: 0.6721
Training Epoch: 23 [9728/50048]	Loss: 0.8461
Training Epoch: 23 [9856/50048]	Loss: 0.8102
Training Epoch: 23 [9984/50048]	Loss: 0.9554
Training Epoch: 23 [10112/50048]	Loss: 0.8160
Training Epoch: 23 [10240/50048]	Loss: 0.8473
Training Epoch: 23 [10368/50048]	Loss: 0.7086
Training Epoch: 23 [10496/50048]	Loss: 0.9049
Training Epoch: 23 [10624/50048]	Loss: 1.0436
Training Epoch: 23 [10752/50048]	Loss: 1.0615
Training Epoch: 23 [10880/50048]	Loss: 0.8300
Training Epoch: 23 [11008/50048]	Loss: 0.9067
Training Epoch: 23 [11136/50048]	Loss: 0.9780
Training Epoch: 23 [11264/50048]	Loss: 0.9201
Training Epoch: 23 [11392/50048]	Loss: 0.6830
Training Epoch: 23 [11520/50048]	Loss: 0.8960
Training Epoch: 23 [11648/50048]	Loss: 0.6258
Training Epoch: 23 [11776/50048]	Loss: 0.9274
Training Epoch: 23 [11904/50048]	Loss: 0.8596
Training Epoch: 23 [12032/50048]	Loss: 0.7849
Training Epoch: 23 [12160/50048]	Loss: 0.7454
Training Epoch: 23 [12288/50048]	Loss: 0.7032
Training Epoch: 23 [12416/50048]	Loss: 0.7570
Training Epoch: 23 [12544/50048]	Loss: 0.8117
Training Epoch: 23 [12672/50048]	Loss: 0.8191
Training Epoch: 23 [12800/50048]	Loss: 1.0360
Training Epoch: 23 [12928/50048]	Loss: 0.6788
Training Epoch: 23 [13056/50048]	Loss: 0.8476
Training Epoch: 23 [13184/50048]	Loss: 0.8831
Training Epoch: 23 [13312/50048]	Loss: 0.9392
Training Epoch: 23 [13440/50048]	Loss: 0.9683
Training Epoch: 23 [13568/50048]	Loss: 0.7844
Training Epoch: 23 [13696/50048]	Loss: 0.7055
Training Epoch: 23 [13824/50048]	Loss: 0.8619
Training Epoch: 23 [13952/50048]	Loss: 0.9442
Training Epoch: 23 [14080/50048]	Loss: 0.6443
Training Epoch: 23 [14208/50048]	Loss: 0.8578
Training Epoch: 23 [14336/50048]	Loss: 0.7818
Training Epoch: 23 [14464/50048]	Loss: 0.9007
Training Epoch: 23 [14592/50048]	Loss: 1.0502
Training Epoch: 23 [14720/50048]	Loss: 0.8167
Training Epoch: 23 [14848/50048]	Loss: 0.8514
Training Epoch: 23 [14976/50048]	Loss: 0.8013
Training Epoch: 23 [15104/50048]	Loss: 0.8935
Training Epoch: 23 [15232/50048]	Loss: 0.7982
Training Epoch: 23 [15360/50048]	Loss: 0.8056
Training Epoch: 23 [15488/50048]	Loss: 0.9844
Training Epoch: 23 [15616/50048]	Loss: 0.8898
Training Epoch: 23 [15744/50048]	Loss: 1.0058
Training Epoch: 23 [15872/50048]	Loss: 0.9513
Training Epoch: 23 [16000/50048]	Loss: 0.8575
Training Epoch: 23 [16128/50048]	Loss: 0.8252
Training Epoch: 23 [16256/50048]	Loss: 0.8174
Training Epoch: 23 [16384/50048]	Loss: 0.8645
Training Epoch: 23 [16512/50048]	Loss: 0.9518
Training Epoch: 23 [16640/50048]	Loss: 0.7598
Training Epoch: 23 [16768/50048]	Loss: 0.8070
Training Epoch: 23 [16896/50048]	Loss: 0.8462
Training Epoch: 23 [17024/50048]	Loss: 0.8433
Training Epoch: 23 [17152/50048]	Loss: 0.6869
Training Epoch: 23 [17280/50048]	Loss: 1.0019
Training Epoch: 23 [17408/50048]	Loss: 0.9178
Training Epoch: 23 [17536/50048]	Loss: 0.7852
Training Epoch: 23 [17664/50048]	Loss: 0.9352
Training Epoch: 23 [17792/50048]	Loss: 0.7595
Training Epoch: 23 [17920/50048]	Loss: 0.7169
Training Epoch: 23 [18048/50048]	Loss: 1.0490
Training Epoch: 23 [18176/50048]	Loss: 0.7860
Training Epoch: 23 [18304/50048]	Loss: 0.7574
Training Epoch: 23 [18432/50048]	Loss: 1.0837
Training Epoch: 23 [18560/50048]	Loss: 0.7214
Training Epoch: 23 [18688/50048]	Loss: 0.9038
Training Epoch: 23 [18816/50048]	Loss: 0.6930
Training Epoch: 23 [18944/50048]	Loss: 1.0645
Training Epoch: 23 [19072/50048]	Loss: 0.7550
Training Epoch: 23 [19200/50048]	Loss: 1.0857
Training Epoch: 23 [19328/50048]	Loss: 0.7351
Training Epoch: 23 [19456/50048]	Loss: 0.8303
Training Epoch: 23 [19584/50048]	Loss: 0.9647
Training Epoch: 23 [19712/50048]	Loss: 0.8117
Training Epoch: 23 [19840/50048]	Loss: 0.6857
Training Epoch: 23 [19968/50048]	Loss: 0.5648
Training Epoch: 23 [20096/50048]	Loss: 0.8033
Training Epoch: 23 [20224/50048]	Loss: 0.8390
Training Epoch: 23 [20352/50048]	Loss: 0.8042
Training Epoch: 23 [20480/50048]	Loss: 1.2354
Training Epoch: 23 [20608/50048]	Loss: 0.8108
Training Epoch: 23 [20736/50048]	Loss: 1.0692
Training Epoch: 23 [20864/50048]	Loss: 0.8587
Training Epoch: 23 [20992/50048]	Loss: 0.7686
Training Epoch: 23 [21120/50048]	Loss: 0.9464
Training Epoch: 23 [21248/50048]	Loss: 0.8585
Training Epoch: 23 [21376/50048]	Loss: 0.7562
Training Epoch: 23 [21504/50048]	Loss: 0.8517
Training Epoch: 23 [21632/50048]	Loss: 0.7801
Training Epoch: 23 [21760/50048]	Loss: 0.7496
Training Epoch: 23 [21888/50048]	Loss: 0.7001
Training Epoch: 23 [22016/50048]	Loss: 0.7765
Training Epoch: 23 [22144/50048]	Loss: 0.8251
Training Epoch: 23 [22272/50048]	Loss: 1.0055
Training Epoch: 23 [22400/50048]	Loss: 0.8316
Training Epoch: 23 [22528/50048]	Loss: 0.9928
Training Epoch: 23 [22656/50048]	Loss: 0.8738
Training Epoch: 23 [22784/50048]	Loss: 0.6668
Training Epoch: 23 [22912/50048]	Loss: 0.8039
Training Epoch: 23 [23040/50048]	Loss: 0.8572
Training Epoch: 23 [23168/50048]	Loss: 0.7361
Training Epoch: 23 [23296/50048]	Loss: 1.0187
Training Epoch: 23 [23424/50048]	Loss: 0.8010
Training Epoch: 23 [23552/50048]	Loss: 0.7523
Training Epoch: 23 [23680/50048]	Loss: 0.8679
Training Epoch: 23 [23808/50048]	Loss: 0.7587
Training Epoch: 23 [23936/50048]	Loss: 0.7826
Training Epoch: 23 [24064/50048]	Loss: 0.8090
Training Epoch: 23 [24192/50048]	Loss: 0.9539
Training Epoch: 23 [24320/50048]	Loss: 0.9950
Training Epoch: 23 [24448/50048]	Loss: 0.8980
Training Epoch: 23 [24576/50048]	Loss: 1.0120
Training Epoch: 23 [24704/50048]	Loss: 0.7463
Training Epoch: 23 [24832/50048]	Loss: 0.8330
Training Epoch: 23 [24960/50048]	Loss: 0.8152
Training Epoch: 23 [25088/50048]	Loss: 0.9857
Training Epoch: 23 [25216/50048]	Loss: 0.7338
Training Epoch: 23 [25344/50048]	Loss: 0.8748
Training Epoch: 23 [25472/50048]	Loss: 1.0394
Training Epoch: 23 [25600/50048]	Loss: 1.0852
Training Epoch: 23 [25728/50048]	Loss: 0.9818
Training Epoch: 23 [25856/50048]	Loss: 0.8846
Training Epoch: 23 [25984/50048]	Loss: 0.8675
Training Epoch: 23 [26112/50048]	Loss: 0.9839
Training Epoch: 23 [26240/50048]	Loss: 0.9556
Training Epoch: 23 [26368/50048]	Loss: 0.8603
Training Epoch: 23 [26496/50048]	Loss: 1.0344
Training Epoch: 23 [26624/50048]	Loss: 0.8854
Training Epoch: 23 [26752/50048]	Loss: 0.8391
Training Epoch: 23 [26880/50048]	Loss: 0.8604
Training Epoch: 23 [27008/50048]	Loss: 0.8688
Training Epoch: 23 [27136/50048]	Loss: 0.8561
Training Epoch: 23 [27264/50048]	Loss: 0.8497
Training Epoch: 23 [27392/50048]	Loss: 0.7738
Training Epoch: 23 [27520/50048]	Loss: 0.7127
Training Epoch: 23 [27648/50048]	Loss: 1.0244
Training Epoch: 23 [27776/50048]	Loss: 1.0727
Training Epoch: 23 [27904/50048]	Loss: 0.8003
Training Epoch: 23 [28032/50048]	Loss: 0.6996
Training Epoch: 23 [28160/50048]	Loss: 0.9450
Training Epoch: 23 [28288/50048]	Loss: 1.0149
Training Epoch: 23 [28416/50048]	Loss: 0.9196
Training Epoch: 23 [28544/50048]	Loss: 0.8497
Training Epoch: 23 [28672/50048]	Loss: 0.7716
Training Epoch: 23 [28800/50048]	Loss: 1.0948
Training Epoch: 23 [28928/50048]	Loss: 0.9582
Training Epoch: 23 [29056/50048]	Loss: 0.9880
Training Epoch: 23 [29184/50048]	Loss: 0.7589
Training Epoch: 23 [29312/50048]	Loss: 0.6517
Training Epoch: 23 [29440/50048]	Loss: 1.2049
Training Epoch: 23 [29568/50048]	Loss: 0.7773
Training Epoch: 23 [29696/50048]	Loss: 0.8902
Training Epoch: 23 [29824/50048]	Loss: 0.8512
Training Epoch: 23 [29952/50048]	Loss: 1.1231
Training Epoch: 23 [30080/50048]	Loss: 0.8945
Training Epoch: 23 [30208/50048]	Loss: 0.7225
Training Epoch: 23 [30336/50048]	Loss: 0.9297
Training Epoch: 23 [30464/50048]	Loss: 0.9873
Training Epoch: 23 [30592/50048]	Loss: 1.0206
Training Epoch: 23 [30720/50048]	Loss: 0.9050
Training Epoch: 23 [30848/50048]	Loss: 0.8772
Training Epoch: 23 [30976/50048]	Loss: 0.9300
Training Epoch: 23 [31104/50048]	Loss: 1.1256
Training Epoch: 23 [31232/50048]	Loss: 0.9391
Training Epoch: 23 [31360/50048]	Loss: 0.7199
Training Epoch: 23 [31488/50048]	Loss: 1.2793
Training Epoch: 23 [31616/50048]	Loss: 0.9729
Training Epoch: 23 [31744/50048]	Loss: 0.8963
Training Epoch: 23 [31872/50048]	Loss: 1.0199
Training Epoch: 23 [32000/50048]	Loss: 0.8496
Training Epoch: 23 [32128/50048]	Loss: 0.8408
Training Epoch: 23 [32256/50048]	Loss: 0.8080
Training Epoch: 23 [32384/50048]	Loss: 0.9408
Training Epoch: 23 [32512/50048]	Loss: 0.8516
Training Epoch: 23 [32640/50048]	Loss: 1.0245
Training Epoch: 23 [32768/50048]	Loss: 0.8242
Training Epoch: 23 [32896/50048]	Loss: 0.8638
Training Epoch: 23 [33024/50048]	Loss: 0.8668
Training Epoch: 23 [33152/50048]	Loss: 0.9034
Training Epoch: 23 [33280/50048]	Loss: 0.9763
Training Epoch: 23 [33408/50048]	Loss: 0.8667
Training Epoch: 23 [33536/50048]	Loss: 0.9442
Training Epoch: 23 [33664/50048]	Loss: 0.7472
Training Epoch: 23 [33792/50048]	Loss: 1.0187
Training Epoch: 23 [33920/50048]	Loss: 0.8887
Training Epoch: 23 [34048/50048]	Loss: 0.9237
Training Epoch: 23 [34176/50048]	Loss: 0.9926
Training Epoch: 23 [34304/50048]	Loss: 0.8577
Training Epoch: 23 [34432/50048]	Loss: 0.8937
Training Epoch: 23 [34560/50048]	Loss: 0.7509
Training Epoch: 23 [34688/50048]	Loss: 0.8545
Training Epoch: 23 [34816/50048]	Loss: 1.0795
Training Epoch: 23 [34944/50048]	Loss: 0.7889
Training Epoch: 23 [35072/50048]	Loss: 0.8653
Training Epoch: 23 [35200/50048]	Loss: 0.7732
Training Epoch: 23 [35328/50048]	Loss: 1.0657
Training Epoch: 23 [35456/50048]	Loss: 0.9278
Training Epoch: 23 [35584/50048]	Loss: 0.6883
Training Epoch: 23 [35712/50048]	Loss: 0.8925
Training Epoch: 23 [35840/50048]	Loss: 0.8650
Training Epoch: 23 [35968/50048]	Loss: 0.9822
Training Epoch: 23 [36096/50048]	Loss: 0.8962
Training Epoch: 23 [36224/50048]	Loss: 0.9126
Training Epoch: 23 [36352/50048]	Loss: 0.9025
Training Epoch: 23 [36480/50048]	Loss: 0.7074
Training Epoch: 23 [36608/50048]	Loss: 0.8887
Training Epoch: 23 [36736/50048]	Loss: 0.7375
Training Epoch: 23 [36864/50048]	Loss: 0.8499
Training Epoch: 23 [36992/50048]	Loss: 0.7856
Training Epoch: 23 [37120/50048]	Loss: 0.8728
Training Epoch: 23 [37248/50048]	Loss: 0.8601
Training Epoch: 23 [37376/50048]	Loss: 0.9198
Training Epoch: 23 [37504/50048]	Loss: 0.7825
Training Epoch: 23 [37632/50048]	Loss: 0.9097
Training Epoch: 23 [37760/50048]	Loss: 1.0473
Training Epoch: 23 [37888/50048]	Loss: 0.8802
Training Epoch: 23 [38016/50048]	Loss: 1.1082
Training Epoch: 23 [38144/50048]	Loss: 0.6153
Training Epoch: 23 [38272/50048]	Loss: 0.9648
Training Epoch: 23 [38400/50048]	Loss: 0.9584
Training Epoch: 23 [38528/50048]	Loss: 1.1104
Training Epoch: 23 [38656/50048]	Loss: 0.9556
Training Epoch: 23 [38784/50048]	Loss: 0.8515
Training Epoch: 23 [38912/50048]	Loss: 0.8697
Training Epoch: 23 [39040/50048]	Loss: 1.0918
Training Epoch: 23 [39168/50048]	Loss: 0.8953
Training Epoch: 23 [39296/50048]	Loss: 0.8731
Training Epoch: 23 [39424/50048]	Loss: 0.8062
Training Epoch: 23 [39552/50048]	Loss: 1.0476
Training Epoch: 23 [39680/50048]	Loss: 0.9247
Training Epoch: 23 [39808/50048]	Loss: 0.9046
Training Epoch: 23 [39936/50048]	Loss: 1.1488
Training Epoch: 23 [40064/50048]	Loss: 1.0400
Training Epoch: 23 [40192/50048]	Loss: 0.7357
Training Epoch: 23 [40320/50048]	Loss: 0.7839
Training Epoch: 23 [40448/50048]	Loss: 0.9266
Training Epoch: 23 [40576/50048]	Loss: 1.0075
Training Epoch: 23 [40704/50048]	Loss: 0.9066
Training Epoch: 23 [40832/50048]	Loss: 1.2321
Training Epoch: 23 [40960/50048]	Loss: 1.0218
Training Epoch: 23 [41088/50048]	Loss: 1.0005
Training Epoch: 23 [41216/50048]	Loss: 0.8206
Training Epoch: 23 [41344/50048]	Loss: 0.9384
Training Epoch: 23 [41472/50048]	Loss: 0.8766
Training Epoch: 23 [41600/50048]	Loss: 0.8818
Training Epoch: 23 [41728/50048]	Loss: 0.8109
Training Epoch: 23 [41856/50048]	Loss: 1.0315
Training Epoch: 23 [41984/50048]	Loss: 0.9539
Training Epoch: 23 [42112/50048]	Loss: 0.8322
Training Epoch: 23 [42240/50048]	Loss: 0.9534
Training Epoch: 23 [42368/50048]	Loss: 1.1104
Training Epoch: 23 [42496/50048]	Loss: 1.0284
Training Epoch: 23 [42624/50048]	Loss: 0.9489
Training Epoch: 23 [42752/50048]	Loss: 0.8603
Training Epoch: 23 [42880/50048]	Loss: 0.8729
Training Epoch: 23 [43008/50048]	Loss: 0.8252
Training Epoch: 23 [43136/50048]	Loss: 1.0117
Training Epoch: 23 [43264/50048]	Loss: 0.7010
Training Epoch: 23 [43392/50048]	Loss: 0.9607
Training Epoch: 23 [43520/50048]	Loss: 0.8425
Training Epoch: 23 [43648/50048]	Loss: 0.9917
Training Epoch: 23 [43776/50048]	Loss: 0.9509
Training Epoch: 23 [43904/50048]	Loss: 1.1865
Training Epoch: 23 [44032/50048]	Loss: 0.7042
Training Epoch: 23 [44160/50048]	Loss: 0.8302
Training Epoch: 23 [44288/50048]	Loss: 0.7542
Training Epoch: 23 [44416/50048]	Loss: 0.9949
Training Epoch: 23 [44544/50048]	Loss: 0.9080
Training Epoch: 23 [44672/50048]	Loss: 0.8381
Training Epoch: 23 [44800/50048]	Loss: 0.8891
Training Epoch: 23 [44928/50048]	Loss: 0.9179
Training Epoch: 23 [45056/50048]	Loss: 0.9833
Training Epoch: 23 [45184/50048]	Loss: 0.9269
Training Epoch: 23 [45312/50048]	Loss: 0.9059
Training Epoch: 23 [45440/50048]	Loss: 0.8313
Training Epoch: 23 [45568/50048]	Loss: 0.7070
Training Epoch: 23 [45696/50048]	Loss: 0.9039
2022-12-06 04:13:26,317 [ZeusDataLoader(train)] train epoch 24 done: time=86.52 energy=10514.82
2022-12-06 04:13:26,318 [ZeusDataLoader(eval)] Epoch 24 begin.
Training Epoch: 23 [45824/50048]	Loss: 0.8657
Training Epoch: 23 [45952/50048]	Loss: 1.1561
Training Epoch: 23 [46080/50048]	Loss: 1.0174
Training Epoch: 23 [46208/50048]	Loss: 0.7874
Training Epoch: 23 [46336/50048]	Loss: 1.0524
Training Epoch: 23 [46464/50048]	Loss: 0.9542
Training Epoch: 23 [46592/50048]	Loss: 0.7466
Training Epoch: 23 [46720/50048]	Loss: 1.1377
Training Epoch: 23 [46848/50048]	Loss: 0.9334
Training Epoch: 23 [46976/50048]	Loss: 0.8351
Training Epoch: 23 [47104/50048]	Loss: 0.8021
Training Epoch: 23 [47232/50048]	Loss: 0.7223
Training Epoch: 23 [47360/50048]	Loss: 1.0265
Training Epoch: 23 [47488/50048]	Loss: 0.7852
Training Epoch: 23 [47616/50048]	Loss: 1.0731
Training Epoch: 23 [47744/50048]	Loss: 0.8671
Training Epoch: 23 [47872/50048]	Loss: 1.0764
Training Epoch: 23 [48000/50048]	Loss: 0.9883
Training Epoch: 23 [48128/50048]	Loss: 0.7723
Training Epoch: 23 [48256/50048]	Loss: 1.2378
Training Epoch: 23 [48384/50048]	Loss: 0.8210
Training Epoch: 23 [48512/50048]	Loss: 1.0448
Training Epoch: 23 [48640/50048]	Loss: 0.8763
Training Epoch: 23 [48768/50048]	Loss: 1.0146
Training Epoch: 23 [48896/50048]	Loss: 0.8530
Training Epoch: 23 [49024/50048]	Loss: 1.0532
Training Epoch: 23 [49152/50048]	Loss: 0.7565
Training Epoch: 23 [49280/50048]	Loss: 0.8835
Training Epoch: 23 [49408/50048]	Loss: 0.8605
Training Epoch: 23 [49536/50048]	Loss: 0.8431
Training Epoch: 23 [49664/50048]	Loss: 0.9215
Training Epoch: 23 [49792/50048]	Loss: 0.7757
Training Epoch: 23 [49920/50048]	Loss: 1.0190
Training Epoch: 23 [50048/50048]	Loss: 0.9972
2022-12-06 09:13:30.016 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:13:30,067 [ZeusDataLoader(eval)] eval epoch 24 done: time=3.74 energy=454.86
2022-12-06 04:13:30,067 [ZeusDataLoader(train)] Up to epoch 24: time=2167.32, energy=262867.28, cost=321074.58
2022-12-06 04:13:30,067 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:13:30,067 [ZeusDataLoader(train)] Expected next epoch: time=2257.12, energy=273665.30, cost=334330.96
2022-12-06 04:13:30,068 [ZeusDataLoader(train)] Epoch 25 begin.
Validation Epoch: 23, Average loss: 0.0116, Accuracy: 0.6163
2022-12-06 04:13:30,249 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:13:30,250 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:13:30.252 [ZeusMonitor] Monitor started.
2022-12-06 09:13:30.252 [ZeusMonitor] Running indefinitely. 2022-12-06 09:13:30.252 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:13:30.252 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e25+gpu0.power.log
Training Epoch: 24 [128/50048]	Loss: 0.5767
Training Epoch: 24 [256/50048]	Loss: 0.8383
Training Epoch: 24 [384/50048]	Loss: 0.5952
Training Epoch: 24 [512/50048]	Loss: 0.8489
Training Epoch: 24 [640/50048]	Loss: 0.6752
Training Epoch: 24 [768/50048]	Loss: 0.6634
Training Epoch: 24 [896/50048]	Loss: 0.6733
Training Epoch: 24 [1024/50048]	Loss: 0.7584
Training Epoch: 24 [1152/50048]	Loss: 0.8223
Training Epoch: 24 [1280/50048]	Loss: 0.7904
Training Epoch: 24 [1408/50048]	Loss: 0.8088
Training Epoch: 24 [1536/50048]	Loss: 0.5549
Training Epoch: 24 [1664/50048]	Loss: 0.5117
Training Epoch: 24 [1792/50048]	Loss: 0.8629
Training Epoch: 24 [1920/50048]	Loss: 0.8499
Training Epoch: 24 [2048/50048]	Loss: 0.7545
Training Epoch: 24 [2176/50048]	Loss: 0.8193
Training Epoch: 24 [2304/50048]	Loss: 0.7700
Training Epoch: 24 [2432/50048]	Loss: 0.8193
Training Epoch: 24 [2560/50048]	Loss: 0.8511
Training Epoch: 24 [2688/50048]	Loss: 0.8022
Training Epoch: 24 [2816/50048]	Loss: 0.9731
Training Epoch: 24 [2944/50048]	Loss: 0.7500
Training Epoch: 24 [3072/50048]	Loss: 0.7966
Training Epoch: 24 [3200/50048]	Loss: 0.9151
Training Epoch: 24 [3328/50048]	Loss: 0.8220
Training Epoch: 24 [3456/50048]	Loss: 0.6678
Training Epoch: 24 [3584/50048]	Loss: 0.7294
Training Epoch: 24 [3712/50048]	Loss: 0.7804
Training Epoch: 24 [3840/50048]	Loss: 0.6678
Training Epoch: 24 [3968/50048]	Loss: 0.6776
Training Epoch: 24 [4096/50048]	Loss: 0.7496
Training Epoch: 24 [4224/50048]	Loss: 0.9329
Training Epoch: 24 [4352/50048]	Loss: 0.8649
Training Epoch: 24 [4480/50048]	Loss: 0.6891
Training Epoch: 24 [4608/50048]	Loss: 0.8359
Training Epoch: 24 [4736/50048]	Loss: 0.6979
Training Epoch: 24 [4864/50048]	Loss: 0.8238
Training Epoch: 24 [4992/50048]	Loss: 0.8357
Training Epoch: 24 [5120/50048]	Loss: 0.7121
Training Epoch: 24 [5248/50048]	Loss: 0.5813
Training Epoch: 24 [5376/50048]	Loss: 0.8937
Training Epoch: 24 [5504/50048]	Loss: 0.7520
Training Epoch: 24 [5632/50048]	Loss: 0.8278
Training Epoch: 24 [5760/50048]	Loss: 0.8004
Training Epoch: 24 [5888/50048]	Loss: 0.7900
Training Epoch: 24 [6016/50048]	Loss: 0.7490
Training Epoch: 24 [6144/50048]	Loss: 0.9142
Training Epoch: 24 [6272/50048]	Loss: 0.8978
Training Epoch: 24 [6400/50048]	Loss: 0.6654
Training Epoch: 24 [6528/50048]	Loss: 0.9147
Training Epoch: 24 [6656/50048]	Loss: 1.0356
Training Epoch: 24 [6784/50048]	Loss: 0.8692
Training Epoch: 24 [6912/50048]	Loss: 0.8270
Training Epoch: 24 [7040/50048]	Loss: 0.9584
Training Epoch: 24 [7168/50048]	Loss: 0.7619
Training Epoch: 24 [7296/50048]	Loss: 0.6962
Training Epoch: 24 [7424/50048]	Loss: 0.8975
Training Epoch: 24 [7552/50048]	Loss: 0.8655
Training Epoch: 24 [7680/50048]	Loss: 0.8868
Training Epoch: 24 [7808/50048]	Loss: 0.7549
Training Epoch: 24 [7936/50048]	Loss: 0.8070
Training Epoch: 24 [8064/50048]	Loss: 1.0062
Training Epoch: 24 [8192/50048]	Loss: 0.8762
Training Epoch: 24 [8320/50048]	Loss: 0.9166
Training Epoch: 24 [8448/50048]	Loss: 0.8186
Training Epoch: 24 [8576/50048]	Loss: 0.8447
Training Epoch: 24 [8704/50048]	Loss: 0.9508
Training Epoch: 24 [8832/50048]	Loss: 0.7377
Training Epoch: 24 [8960/50048]	Loss: 0.6966
Training Epoch: 24 [9088/50048]	Loss: 0.6979
Training Epoch: 24 [9216/50048]	Loss: 0.9176
Training Epoch: 24 [9344/50048]	Loss: 0.7196
Training Epoch: 24 [9472/50048]	Loss: 0.8825
Training Epoch: 24 [9600/50048]	Loss: 0.6974
Training Epoch: 24 [9728/50048]	Loss: 0.9508
Training Epoch: 24 [9856/50048]	Loss: 0.7545
Training Epoch: 24 [9984/50048]	Loss: 0.6666
Training Epoch: 24 [10112/50048]	Loss: 0.6335
Training Epoch: 24 [10240/50048]	Loss: 1.0130
Training Epoch: 24 [10368/50048]	Loss: 0.7420
Training Epoch: 24 [10496/50048]	Loss: 0.8293
Training Epoch: 24 [10624/50048]	Loss: 0.8652
Training Epoch: 24 [10752/50048]	Loss: 0.6248
Training Epoch: 24 [10880/50048]	Loss: 0.7209
Training Epoch: 24 [11008/50048]	Loss: 0.8452
Training Epoch: 24 [11136/50048]	Loss: 0.8060
Training Epoch: 24 [11264/50048]	Loss: 0.9041
Training Epoch: 24 [11392/50048]	Loss: 0.8320
Training Epoch: 24 [11520/50048]	Loss: 0.6598
Training Epoch: 24 [11648/50048]	Loss: 0.8618
Training Epoch: 24 [11776/50048]	Loss: 1.0767
Training Epoch: 24 [11904/50048]	Loss: 0.7091
Training Epoch: 24 [12032/50048]	Loss: 0.7627
Training Epoch: 24 [12160/50048]	Loss: 0.7359
Training Epoch: 24 [12288/50048]	Loss: 0.7402
Training Epoch: 24 [12416/50048]	Loss: 0.8537
Training Epoch: 24 [12544/50048]	Loss: 0.6959
Training Epoch: 24 [12672/50048]	Loss: 0.7911
Training Epoch: 24 [12800/50048]	Loss: 0.9164
Training Epoch: 24 [12928/50048]	Loss: 0.9238
Training Epoch: 24 [13056/50048]	Loss: 0.7315
Training Epoch: 24 [13184/50048]	Loss: 0.7446
Training Epoch: 24 [13312/50048]	Loss: 0.7234
Training Epoch: 24 [13440/50048]	Loss: 0.8228
Training Epoch: 24 [13568/50048]	Loss: 0.9318
Training Epoch: 24 [13696/50048]	Loss: 0.8604
Training Epoch: 24 [13824/50048]	Loss: 0.8831
Training Epoch: 24 [13952/50048]	Loss: 0.7322
Training Epoch: 24 [14080/50048]	Loss: 0.8472
Training Epoch: 24 [14208/50048]	Loss: 0.7458
Training Epoch: 24 [14336/50048]	Loss: 0.9793
Training Epoch: 24 [14464/50048]	Loss: 0.8156
Training Epoch: 24 [14592/50048]	Loss: 0.7925
Training Epoch: 24 [14720/50048]	Loss: 0.8427
Training Epoch: 24 [14848/50048]	Loss: 0.7071
Training Epoch: 24 [14976/50048]	Loss: 0.9311
Training Epoch: 24 [15104/50048]	Loss: 0.7408
Training Epoch: 24 [15232/50048]	Loss: 0.8245
Training Epoch: 24 [15360/50048]	Loss: 0.7851
Training Epoch: 24 [15488/50048]	Loss: 0.7965
Training Epoch: 24 [15616/50048]	Loss: 0.5744
Training Epoch: 24 [15744/50048]	Loss: 0.7781
Training Epoch: 24 [15872/50048]	Loss: 0.6307
Training Epoch: 24 [16000/50048]	Loss: 0.9701
Training Epoch: 24 [16128/50048]	Loss: 0.7705
Training Epoch: 24 [16256/50048]	Loss: 0.8248
Training Epoch: 24 [16384/50048]	Loss: 0.8869
Training Epoch: 24 [16512/50048]	Loss: 0.8367
Training Epoch: 24 [16640/50048]	Loss: 0.8679
Training Epoch: 24 [16768/50048]	Loss: 0.8109
Training Epoch: 24 [16896/50048]	Loss: 0.8757
Training Epoch: 24 [17024/50048]	Loss: 1.0279
Training Epoch: 24 [17152/50048]	Loss: 0.9481
Training Epoch: 24 [17280/50048]	Loss: 0.8768
Training Epoch: 24 [17408/50048]	Loss: 0.7591
Training Epoch: 24 [17536/50048]	Loss: 1.0413
Training Epoch: 24 [17664/50048]	Loss: 0.8771
Training Epoch: 24 [17792/50048]	Loss: 0.7022
Training Epoch: 24 [17920/50048]	Loss: 0.9013
Training Epoch: 24 [18048/50048]	Loss: 0.7915
Training Epoch: 24 [18176/50048]	Loss: 1.0564
Training Epoch: 24 [18304/50048]	Loss: 0.9229
Training Epoch: 24 [18432/50048]	Loss: 0.7920
Training Epoch: 24 [18560/50048]	Loss: 0.7498
Training Epoch: 24 [18688/50048]	Loss: 0.7166
Training Epoch: 24 [18816/50048]	Loss: 0.7466
Training Epoch: 24 [18944/50048]	Loss: 0.9699
Training Epoch: 24 [19072/50048]	Loss: 0.7714
Training Epoch: 24 [19200/50048]	Loss: 0.8454
Training Epoch: 24 [19328/50048]	Loss: 0.7974
Training Epoch: 24 [19456/50048]	Loss: 0.8226
Training Epoch: 24 [19584/50048]	Loss: 0.9433
Training Epoch: 24 [19712/50048]	Loss: 0.7598
Training Epoch: 24 [19840/50048]	Loss: 0.7704
Training Epoch: 24 [19968/50048]	Loss: 0.7427
Training Epoch: 24 [20096/50048]	Loss: 0.7516
Training Epoch: 24 [20224/50048]	Loss: 0.7795
Training Epoch: 24 [20352/50048]	Loss: 0.7206
Training Epoch: 24 [20480/50048]	Loss: 0.7115
Training Epoch: 24 [20608/50048]	Loss: 0.7545
Training Epoch: 24 [20736/50048]	Loss: 0.5863
Training Epoch: 24 [20864/50048]	Loss: 0.8947
Training Epoch: 24 [20992/50048]	Loss: 0.8359
Training Epoch: 24 [21120/50048]	Loss: 0.8740
Training Epoch: 24 [21248/50048]	Loss: 0.9338
Training Epoch: 24 [21376/50048]	Loss: 1.1738
Training Epoch: 24 [21504/50048]	Loss: 0.9073
Training Epoch: 24 [21632/50048]	Loss: 0.7483
Training Epoch: 24 [21760/50048]	Loss: 0.9566
Training Epoch: 24 [21888/50048]	Loss: 0.7993
Training Epoch: 24 [22016/50048]	Loss: 0.7436
Training Epoch: 24 [22144/50048]	Loss: 0.7260
Training Epoch: 24 [22272/50048]	Loss: 0.9133
Training Epoch: 24 [22400/50048]	Loss: 0.7269
Training Epoch: 24 [22528/50048]	Loss: 0.8876
Training Epoch: 24 [22656/50048]	Loss: 0.7253
Training Epoch: 24 [22784/50048]	Loss: 1.0497
Training Epoch: 24 [22912/50048]	Loss: 0.8451
Training Epoch: 24 [23040/50048]	Loss: 0.6999
Training Epoch: 24 [23168/50048]	Loss: 0.7799
Training Epoch: 24 [23296/50048]	Loss: 0.9215
Training Epoch: 24 [23424/50048]	Loss: 0.8058
Training Epoch: 24 [23552/50048]	Loss: 0.8577
Training Epoch: 24 [23680/50048]	Loss: 0.6190
Training Epoch: 24 [23808/50048]	Loss: 1.0444
Training Epoch: 24 [23936/50048]	Loss: 0.7865
Training Epoch: 24 [24064/50048]	Loss: 0.8400
Training Epoch: 24 [24192/50048]	Loss: 0.9361
Training Epoch: 24 [24320/50048]	Loss: 0.6762
Training Epoch: 24 [24448/50048]	Loss: 0.7262
Training Epoch: 24 [24576/50048]	Loss: 0.6305
Training Epoch: 24 [24704/50048]	Loss: 0.6533
Training Epoch: 24 [24832/50048]	Loss: 0.8529
Training Epoch: 24 [24960/50048]	Loss: 1.0745
Training Epoch: 24 [25088/50048]	Loss: 0.7422
Training Epoch: 24 [25216/50048]	Loss: 0.8308
Training Epoch: 24 [25344/50048]	Loss: 0.9650
Training Epoch: 24 [25472/50048]	Loss: 1.0133
Training Epoch: 24 [25600/50048]	Loss: 0.6168
Training Epoch: 24 [25728/50048]	Loss: 0.7068
Training Epoch: 24 [25856/50048]	Loss: 0.8287
Training Epoch: 24 [25984/50048]	Loss: 0.9661
Training Epoch: 24 [26112/50048]	Loss: 0.8369
Training Epoch: 24 [26240/50048]	Loss: 1.0203
Training Epoch: 24 [26368/50048]	Loss: 0.7186
Training Epoch: 24 [26496/50048]	Loss: 0.8333
Training Epoch: 24 [26624/50048]	Loss: 1.0133
Training Epoch: 24 [26752/50048]	Loss: 1.0729
Training Epoch: 24 [26880/50048]	Loss: 0.7241
Training Epoch: 24 [27008/50048]	Loss: 0.8781
Training Epoch: 24 [27136/50048]	Loss: 0.7766
Training Epoch: 24 [27264/50048]	Loss: 0.8649
Training Epoch: 24 [27392/50048]	Loss: 0.8289
Training Epoch: 24 [27520/50048]	Loss: 0.9412
Training Epoch: 24 [27648/50048]	Loss: 0.9072
Training Epoch: 24 [27776/50048]	Loss: 0.9992
Training Epoch: 24 [27904/50048]	Loss: 0.7929
Training Epoch: 24 [28032/50048]	Loss: 0.7963
Training Epoch: 24 [28160/50048]	Loss: 1.0246
Training Epoch: 24 [28288/50048]	Loss: 0.9651
Training Epoch: 24 [28416/50048]	Loss: 0.8120
Training Epoch: 24 [28544/50048]	Loss: 0.6916
Training Epoch: 24 [28672/50048]	Loss: 0.9473
Training Epoch: 24 [28800/50048]	Loss: 0.7104
Training Epoch: 24 [28928/50048]	Loss: 0.8574
Training Epoch: 24 [29056/50048]	Loss: 0.8774
Training Epoch: 24 [29184/50048]	Loss: 0.8315
Training Epoch: 24 [29312/50048]	Loss: 0.7795
Training Epoch: 24 [29440/50048]	Loss: 0.9023
Training Epoch: 24 [29568/50048]	Loss: 0.6945
Training Epoch: 24 [29696/50048]	Loss: 0.9592
Training Epoch: 24 [29824/50048]	Loss: 0.8360
Training Epoch: 24 [29952/50048]	Loss: 0.9605
Training Epoch: 24 [30080/50048]	Loss: 0.9222
Training Epoch: 24 [30208/50048]	Loss: 0.7196
Training Epoch: 24 [30336/50048]	Loss: 0.7125
Training Epoch: 24 [30464/50048]	Loss: 1.1720
Training Epoch: 24 [30592/50048]	Loss: 0.9254
Training Epoch: 24 [30720/50048]	Loss: 0.8689
Training Epoch: 24 [30848/50048]	Loss: 0.8382
Training Epoch: 24 [30976/50048]	Loss: 0.8864
Training Epoch: 24 [31104/50048]	Loss: 0.8584
Training Epoch: 24 [31232/50048]	Loss: 0.8099
Training Epoch: 24 [31360/50048]	Loss: 0.8697
Training Epoch: 24 [31488/50048]	Loss: 1.0474
Training Epoch: 24 [31616/50048]	Loss: 0.8692
Training Epoch: 24 [31744/50048]	Loss: 0.9050
Training Epoch: 24 [31872/50048]	Loss: 0.9205
Training Epoch: 24 [32000/50048]	Loss: 0.8949
Training Epoch: 24 [32128/50048]	Loss: 0.8446
Training Epoch: 24 [32256/50048]	Loss: 0.7499
Training Epoch: 24 [32384/50048]	Loss: 0.9835
Training Epoch: 24 [32512/50048]	Loss: 0.9416
Training Epoch: 24 [32640/50048]	Loss: 1.0328
Training Epoch: 24 [32768/50048]	Loss: 0.8053
Training Epoch: 24 [32896/50048]	Loss: 0.7958
Training Epoch: 24 [33024/50048]	Loss: 0.7690
Training Epoch: 24 [33152/50048]	Loss: 0.9910
Training Epoch: 24 [33280/50048]	Loss: 0.9067
Training Epoch: 24 [33408/50048]	Loss: 0.9397
Training Epoch: 24 [33536/50048]	Loss: 0.9298
Training Epoch: 24 [33664/50048]	Loss: 0.7008
Training Epoch: 24 [33792/50048]	Loss: 0.8089
Training Epoch: 24 [33920/50048]	Loss: 0.9672
Training Epoch: 24 [34048/50048]	Loss: 0.7872
Training Epoch: 24 [34176/50048]	Loss: 0.7597
Training Epoch: 24 [34304/50048]	Loss: 0.9792
Training Epoch: 24 [34432/50048]	Loss: 1.0105
Training Epoch: 24 [34560/50048]	Loss: 0.8755
Training Epoch: 24 [34688/50048]	Loss: 0.6581
Training Epoch: 24 [34816/50048]	Loss: 0.9769
Training Epoch: 24 [34944/50048]	Loss: 0.8078
Training Epoch: 24 [35072/50048]	Loss: 0.9895
Training Epoch: 24 [35200/50048]	Loss: 0.9495
Training Epoch: 24 [35328/50048]	Loss: 1.0813
Training Epoch: 24 [35456/50048]	Loss: 0.8584
Training Epoch: 24 [35584/50048]	Loss: 0.9437
Training Epoch: 24 [35712/50048]	Loss: 0.8732
Training Epoch: 24 [35840/50048]	Loss: 0.7344
Training Epoch: 24 [35968/50048]	Loss: 0.8538
Training Epoch: 24 [36096/50048]	Loss: 0.9937
Training Epoch: 24 [36224/50048]	Loss: 0.7292
Training Epoch: 24 [36352/50048]	Loss: 0.8899
Training Epoch: 24 [36480/50048]	Loss: 0.8970
Training Epoch: 24 [36608/50048]	Loss: 0.8665
Training Epoch: 24 [36736/50048]	Loss: 0.8058
Training Epoch: 24 [36864/50048]	Loss: 0.9253
Training Epoch: 24 [36992/50048]	Loss: 0.9701
Training Epoch: 24 [37120/50048]	Loss: 0.8626
Training Epoch: 24 [37248/50048]	Loss: 0.9626
Training Epoch: 24 [37376/50048]	Loss: 0.9001
Training Epoch: 24 [37504/50048]	Loss: 0.8223
Training Epoch: 24 [37632/50048]	Loss: 0.8010
Training Epoch: 24 [37760/50048]	Loss: 0.8723
Training Epoch: 24 [37888/50048]	Loss: 0.8887
Training Epoch: 24 [38016/50048]	Loss: 1.0671
Training Epoch: 24 [38144/50048]	Loss: 0.9320
Training Epoch: 24 [38272/50048]	Loss: 0.6688
Training Epoch: 24 [38400/50048]	Loss: 0.9035
Training Epoch: 24 [38528/50048]	Loss: 0.7392
Training Epoch: 24 [38656/50048]	Loss: 0.7581
Training Epoch: 24 [38784/50048]	Loss: 0.9387
Training Epoch: 24 [38912/50048]	Loss: 0.9199
Training Epoch: 24 [39040/50048]	Loss: 0.7328
Training Epoch: 24 [39168/50048]	Loss: 0.8136
Training Epoch: 24 [39296/50048]	Loss: 0.7831
Training Epoch: 24 [39424/50048]	Loss: 0.8251
Training Epoch: 24 [39552/50048]	Loss: 0.8767
Training Epoch: 24 [39680/50048]	Loss: 1.0210
Training Epoch: 24 [39808/50048]	Loss: 0.7866
Training Epoch: 24 [39936/50048]	Loss: 0.7335
Training Epoch: 24 [40064/50048]	Loss: 0.7578
Training Epoch: 24 [40192/50048]	Loss: 0.9107
Training Epoch: 24 [40320/50048]	Loss: 0.9796
Training Epoch: 24 [40448/50048]	Loss: 0.7825
Training Epoch: 24 [40576/50048]	Loss: 0.6956
Training Epoch: 24 [40704/50048]	Loss: 0.7845
Training Epoch: 24 [40832/50048]	Loss: 0.7033
Training Epoch: 24 [40960/50048]	Loss: 0.8778
Training Epoch: 24 [41088/50048]	Loss: 0.9648
Training Epoch: 24 [41216/50048]	Loss: 0.7889
Training Epoch: 24 [41344/50048]	Loss: 0.6616
Training Epoch: 24 [41472/50048]	Loss: 0.8488
Training Epoch: 24 [41600/50048]	Loss: 1.0544
Training Epoch: 24 [41728/50048]	Loss: 0.8521
Training Epoch: 24 [41856/50048]	Loss: 1.0379
Training Epoch: 24 [41984/50048]	Loss: 0.6440
Training Epoch: 24 [42112/50048]	Loss: 0.9339
Training Epoch: 24 [42240/50048]	Loss: 0.9569
Training Epoch: 24 [42368/50048]	Loss: 0.8844
Training Epoch: 24 [42496/50048]	Loss: 1.0771
Training Epoch: 24 [42624/50048]	Loss: 1.0360
Training Epoch: 24 [42752/50048]	Loss: 0.9149
Training Epoch: 24 [42880/50048]	Loss: 0.9213
Training Epoch: 24 [43008/50048]	Loss: 0.8442
Training Epoch: 24 [43136/50048]	Loss: 1.0024
Training Epoch: 24 [43264/50048]	Loss: 1.0440
Training Epoch: 24 [43392/50048]	Loss: 0.6955
Training Epoch: 24 [43520/50048]	Loss: 0.7542
Training Epoch: 24 [43648/50048]	Loss: 0.9588
Training Epoch: 24 [43776/50048]	Loss: 0.5318
Training Epoch: 24 [43904/50048]	Loss: 0.8339
Training Epoch: 24 [44032/50048]	Loss: 0.8085
Training Epoch: 24 [44160/50048]	Loss: 0.9787
Training Epoch: 24 [44288/50048]	Loss: 0.6945
Training Epoch: 24 [44416/50048]	Loss: 0.7329
Training Epoch: 24 [44544/50048]	Loss: 0.7934
Training Epoch: 24 [44672/50048]	Loss: 0.9423
Training Epoch: 24 [44800/50048]	Loss: 0.9197
Training Epoch: 24 [44928/50048]	Loss: 1.0759
Training Epoch: 24 [45056/50048]	Loss: 0.7969
Training Epoch: 24 [45184/50048]	Loss: 0.7299
Training Epoch: 24 [45312/50048]	Loss: 0.5430
Training Epoch: 24 [45440/50048]	Loss: 0.9593
Training Epoch: 24 [45568/50048]	Loss: 0.9335
Training Epoch: 24 [45696/50048]	Loss: 1.1443
2022-12-06 04:14:56,582 [ZeusDataLoader(train)] train epoch 25 done: time=86.50 energy=10510.79
2022-12-06 04:14:56,584 [ZeusDataLoader(eval)] Epoch 25 begin.
Training Epoch: 24 [45824/50048]	Loss: 0.8652
Training Epoch: 24 [45952/50048]	Loss: 0.8823
Training Epoch: 24 [46080/50048]	Loss: 0.9114
Training Epoch: 24 [46208/50048]	Loss: 0.8914
Training Epoch: 24 [46336/50048]	Loss: 1.1148
Training Epoch: 24 [46464/50048]	Loss: 1.0362
Training Epoch: 24 [46592/50048]	Loss: 0.8081
Training Epoch: 24 [46720/50048]	Loss: 0.8280
Training Epoch: 24 [46848/50048]	Loss: 1.1237
Training Epoch: 24 [46976/50048]	Loss: 0.9555
Training Epoch: 24 [47104/50048]	Loss: 0.7543
Training Epoch: 24 [47232/50048]	Loss: 0.9479
Training Epoch: 24 [47360/50048]	Loss: 0.9976
Training Epoch: 24 [47488/50048]	Loss: 1.1197
Training Epoch: 24 [47616/50048]	Loss: 0.9514
Training Epoch: 24 [47744/50048]	Loss: 1.0778
Training Epoch: 24 [47872/50048]	Loss: 1.0603
Training Epoch: 24 [48000/50048]	Loss: 0.8717
Training Epoch: 24 [48128/50048]	Loss: 0.9427
Training Epoch: 24 [48256/50048]	Loss: 0.7172
Training Epoch: 24 [48384/50048]	Loss: 0.7475
Training Epoch: 24 [48512/50048]	Loss: 0.7197
Training Epoch: 24 [48640/50048]	Loss: 1.0291
Training Epoch: 24 [48768/50048]	Loss: 0.7380
Training Epoch: 24 [48896/50048]	Loss: 0.9865
Training Epoch: 24 [49024/50048]	Loss: 0.9880
Training Epoch: 24 [49152/50048]	Loss: 0.9092
Training Epoch: 24 [49280/50048]	Loss: 0.7760
Training Epoch: 24 [49408/50048]	Loss: 0.9088
Training Epoch: 24 [49536/50048]	Loss: 0.7774
Training Epoch: 24 [49664/50048]	Loss: 1.0539
Training Epoch: 24 [49792/50048]	Loss: 0.8411
Training Epoch: 24 [49920/50048]	Loss: 0.8954
Training Epoch: 24 [50048/50048]	Loss: 0.8940
2022-12-06 09:15:00.272 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:15:00,292 [ZeusDataLoader(eval)] eval epoch 25 done: time=3.70 energy=455.08
2022-12-06 04:15:00,292 [ZeusDataLoader(train)] Up to epoch 25: time=2257.53, energy=273833.15, cost=334450.22
2022-12-06 04:15:00,292 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:15:00,292 [ZeusDataLoader(train)] Expected next epoch: time=2347.33, energy=284631.16, cost=347706.60
2022-12-06 04:15:00,293 [ZeusDataLoader(train)] Epoch 26 begin.
Validation Epoch: 24, Average loss: 0.0117, Accuracy: 0.6185
2022-12-06 04:15:00,440 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:15:00,441 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:15:00.444 [ZeusMonitor] Monitor started.
2022-12-06 09:15:00.444 [ZeusMonitor] Running indefinitely. 2022-12-06 09:15:00.444 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:15:00.444 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e26+gpu0.power.log
Training Epoch: 25 [128/50048]	Loss: 0.7719
Training Epoch: 25 [256/50048]	Loss: 0.8781
Training Epoch: 25 [384/50048]	Loss: 0.7643
Training Epoch: 25 [512/50048]	Loss: 0.7682
Training Epoch: 25 [640/50048]	Loss: 0.6959
Training Epoch: 25 [768/50048]	Loss: 0.6923
Training Epoch: 25 [896/50048]	Loss: 0.6073
Training Epoch: 25 [1024/50048]	Loss: 0.9521
Training Epoch: 25 [1152/50048]	Loss: 0.7287
Training Epoch: 25 [1280/50048]	Loss: 0.7790
Training Epoch: 25 [1408/50048]	Loss: 0.6619
Training Epoch: 25 [1536/50048]	Loss: 0.6685
Training Epoch: 25 [1664/50048]	Loss: 0.6888
Training Epoch: 25 [1792/50048]	Loss: 0.6669
Training Epoch: 25 [1920/50048]	Loss: 0.7212
Training Epoch: 25 [2048/50048]	Loss: 0.6600
Training Epoch: 25 [2176/50048]	Loss: 0.9474
Training Epoch: 25 [2304/50048]	Loss: 0.9963
Training Epoch: 25 [2432/50048]	Loss: 0.7076
Training Epoch: 25 [2560/50048]	Loss: 0.7014
Training Epoch: 25 [2688/50048]	Loss: 0.7542
Training Epoch: 25 [2816/50048]	Loss: 0.7670
Training Epoch: 25 [2944/50048]	Loss: 0.6425
Training Epoch: 25 [3072/50048]	Loss: 1.0532
Training Epoch: 25 [3200/50048]	Loss: 0.7137
Training Epoch: 25 [3328/50048]	Loss: 0.7595
Training Epoch: 25 [3456/50048]	Loss: 0.8440
Training Epoch: 25 [3584/50048]	Loss: 0.6465
Training Epoch: 25 [3712/50048]	Loss: 0.6909
Training Epoch: 25 [3840/50048]	Loss: 0.7907
Training Epoch: 25 [3968/50048]	Loss: 0.6433
Training Epoch: 25 [4096/50048]	Loss: 0.7740
Training Epoch: 25 [4224/50048]	Loss: 0.6094
Training Epoch: 25 [4352/50048]	Loss: 0.7431
Training Epoch: 25 [4480/50048]	Loss: 0.6253
Training Epoch: 25 [4608/50048]	Loss: 0.8526
Training Epoch: 25 [4736/50048]	Loss: 0.6776
Training Epoch: 25 [4864/50048]	Loss: 0.5117
Training Epoch: 25 [4992/50048]	Loss: 0.7411
Training Epoch: 25 [5120/50048]	Loss: 0.8675
Training Epoch: 25 [5248/50048]	Loss: 0.9273
Training Epoch: 25 [5376/50048]	Loss: 0.8033
Training Epoch: 25 [5504/50048]	Loss: 0.8668
Training Epoch: 25 [5632/50048]	Loss: 0.7349
Training Epoch: 25 [5760/50048]	Loss: 0.7753
Training Epoch: 25 [5888/50048]	Loss: 0.8463
Training Epoch: 25 [6016/50048]	Loss: 0.8168
Training Epoch: 25 [6144/50048]	Loss: 0.7435
Training Epoch: 25 [6272/50048]	Loss: 0.7270
Training Epoch: 25 [6400/50048]	Loss: 0.8243
Training Epoch: 25 [6528/50048]	Loss: 0.6808
Training Epoch: 25 [6656/50048]	Loss: 0.6591
Training Epoch: 25 [6784/50048]	Loss: 0.8708
Training Epoch: 25 [6912/50048]	Loss: 0.7150
Training Epoch: 25 [7040/50048]	Loss: 0.8347
Training Epoch: 25 [7168/50048]	Loss: 0.6795
Training Epoch: 25 [7296/50048]	Loss: 0.6696
Training Epoch: 25 [7424/50048]	Loss: 0.7339
Training Epoch: 25 [7552/50048]	Loss: 0.7379
Training Epoch: 25 [7680/50048]	Loss: 0.7588
Training Epoch: 25 [7808/50048]	Loss: 0.8183
Training Epoch: 25 [7936/50048]	Loss: 0.7573
Training Epoch: 25 [8064/50048]	Loss: 0.7096
Training Epoch: 25 [8192/50048]	Loss: 0.8700
Training Epoch: 25 [8320/50048]	Loss: 1.0565
Training Epoch: 25 [8448/50048]	Loss: 0.9241
Training Epoch: 25 [8576/50048]	Loss: 0.7240
Training Epoch: 25 [8704/50048]	Loss: 0.8725
Training Epoch: 25 [8832/50048]	Loss: 0.7564
Training Epoch: 25 [8960/50048]	Loss: 0.7468
Training Epoch: 25 [9088/50048]	Loss: 0.7105
Training Epoch: 25 [9216/50048]	Loss: 0.6118
Training Epoch: 25 [9344/50048]	Loss: 0.8109
Training Epoch: 25 [9472/50048]	Loss: 0.7359
Training Epoch: 25 [9600/50048]	Loss: 0.8847
Training Epoch: 25 [9728/50048]	Loss: 0.7900
Training Epoch: 25 [9856/50048]	Loss: 0.6360
Training Epoch: 25 [9984/50048]	Loss: 0.8012
Training Epoch: 25 [10112/50048]	Loss: 0.6936
Training Epoch: 25 [10240/50048]	Loss: 0.7424
Training Epoch: 25 [10368/50048]	Loss: 0.7999
Training Epoch: 25 [10496/50048]	Loss: 0.7476
Training Epoch: 25 [10624/50048]	Loss: 0.8467
Training Epoch: 25 [10752/50048]	Loss: 0.6879
Training Epoch: 25 [10880/50048]	Loss: 0.7365
Training Epoch: 25 [11008/50048]	Loss: 0.9831
Training Epoch: 25 [11136/50048]	Loss: 0.7053
Training Epoch: 25 [11264/50048]	Loss: 0.7831
Training Epoch: 25 [11392/50048]	Loss: 0.5224
Training Epoch: 25 [11520/50048]	Loss: 0.6816
Training Epoch: 25 [11648/50048]	Loss: 0.7773
Training Epoch: 25 [11776/50048]	Loss: 0.6747
Training Epoch: 25 [11904/50048]	Loss: 0.7809
Training Epoch: 25 [12032/50048]	Loss: 0.9848
Training Epoch: 25 [12160/50048]	Loss: 0.7302
Training Epoch: 25 [12288/50048]	Loss: 0.7480
Training Epoch: 25 [12416/50048]	Loss: 0.8386
Training Epoch: 25 [12544/50048]	Loss: 0.7953
Training Epoch: 25 [12672/50048]	Loss: 0.5883
Training Epoch: 25 [12800/50048]	Loss: 0.8686
Training Epoch: 25 [12928/50048]	Loss: 0.8111
Training Epoch: 25 [13056/50048]	Loss: 0.7901
Training Epoch: 25 [13184/50048]	Loss: 0.7564
Training Epoch: 25 [13312/50048]	Loss: 0.8020
Training Epoch: 25 [13440/50048]	Loss: 0.5383
Training Epoch: 25 [13568/50048]	Loss: 0.9046
Training Epoch: 25 [13696/50048]	Loss: 0.8314
Training Epoch: 25 [13824/50048]	Loss: 0.7846
Training Epoch: 25 [13952/50048]	Loss: 0.6500
Training Epoch: 25 [14080/50048]	Loss: 0.9619
Training Epoch: 25 [14208/50048]	Loss: 0.6513
Training Epoch: 25 [14336/50048]	Loss: 0.7521
Training Epoch: 25 [14464/50048]	Loss: 0.7059
Training Epoch: 25 [14592/50048]	Loss: 0.9223
Training Epoch: 25 [14720/50048]	Loss: 0.8042
Training Epoch: 25 [14848/50048]	Loss: 0.7157
Training Epoch: 25 [14976/50048]	Loss: 0.6342
Training Epoch: 25 [15104/50048]	Loss: 0.7503
Training Epoch: 25 [15232/50048]	Loss: 0.8927
Training Epoch: 25 [15360/50048]	Loss: 0.6515
Training Epoch: 25 [15488/50048]	Loss: 0.7867
Training Epoch: 25 [15616/50048]	Loss: 0.8400
Training Epoch: 25 [15744/50048]	Loss: 1.0086
Training Epoch: 25 [15872/50048]	Loss: 0.9151
Training Epoch: 25 [16000/50048]	Loss: 0.7708
Training Epoch: 25 [16128/50048]	Loss: 0.8472
Training Epoch: 25 [16256/50048]	Loss: 0.7765
Training Epoch: 25 [16384/50048]	Loss: 0.7045
Training Epoch: 25 [16512/50048]	Loss: 0.6979
Training Epoch: 25 [16640/50048]	Loss: 0.7602
Training Epoch: 25 [16768/50048]	Loss: 0.7294
Training Epoch: 25 [16896/50048]	Loss: 0.8378
Training Epoch: 25 [17024/50048]	Loss: 0.8409
Training Epoch: 25 [17152/50048]	Loss: 0.6189
Training Epoch: 25 [17280/50048]	Loss: 0.9517
Training Epoch: 25 [17408/50048]	Loss: 0.7939
Training Epoch: 25 [17536/50048]	Loss: 1.0024
Training Epoch: 25 [17664/50048]	Loss: 0.9062
Training Epoch: 25 [17792/50048]	Loss: 0.9003
Training Epoch: 25 [17920/50048]	Loss: 0.8308
Training Epoch: 25 [18048/50048]	Loss: 0.7506
Training Epoch: 25 [18176/50048]	Loss: 0.5359
Training Epoch: 25 [18304/50048]	Loss: 0.6562
Training Epoch: 25 [18432/50048]	Loss: 0.8612
Training Epoch: 25 [18560/50048]	Loss: 0.7265
Training Epoch: 25 [18688/50048]	Loss: 0.6222
Training Epoch: 25 [18816/50048]	Loss: 0.7836
Training Epoch: 25 [18944/50048]	Loss: 0.7546
Training Epoch: 25 [19072/50048]	Loss: 0.6012
Training Epoch: 25 [19200/50048]	Loss: 0.9000
Training Epoch: 25 [19328/50048]	Loss: 0.8835
Training Epoch: 25 [19456/50048]	Loss: 0.7271
Training Epoch: 25 [19584/50048]	Loss: 0.8444
Training Epoch: 25 [19712/50048]	Loss: 0.8391
Training Epoch: 25 [19840/50048]	Loss: 0.7819
Training Epoch: 25 [19968/50048]	Loss: 0.6728
Training Epoch: 25 [20096/50048]	Loss: 0.7872
Training Epoch: 25 [20224/50048]	Loss: 0.9047
Training Epoch: 25 [20352/50048]	Loss: 0.7019
Training Epoch: 25 [20480/50048]	Loss: 0.7539
Training Epoch: 25 [20608/50048]	Loss: 0.7295
Training Epoch: 25 [20736/50048]	Loss: 0.8187
Training Epoch: 25 [20864/50048]	Loss: 0.6912
Training Epoch: 25 [20992/50048]	Loss: 0.7368
Training Epoch: 25 [21120/50048]	Loss: 0.7038
Training Epoch: 25 [21248/50048]	Loss: 0.6007
Training Epoch: 25 [21376/50048]	Loss: 0.7876
Training Epoch: 25 [21504/50048]	Loss: 0.7847
Training Epoch: 25 [21632/50048]	Loss: 0.9391
Training Epoch: 25 [21760/50048]	Loss: 1.1060
Training Epoch: 25 [21888/50048]	Loss: 0.9303
Training Epoch: 25 [22016/50048]	Loss: 0.8501
Training Epoch: 25 [22144/50048]	Loss: 0.7206
Training Epoch: 25 [22272/50048]	Loss: 0.9767
Training Epoch: 25 [22400/50048]	Loss: 0.7914
Training Epoch: 25 [22528/50048]	Loss: 0.9683
Training Epoch: 25 [22656/50048]	Loss: 0.9242
Training Epoch: 25 [22784/50048]	Loss: 0.7131
Training Epoch: 25 [22912/50048]	Loss: 0.6957
Training Epoch: 25 [23040/50048]	Loss: 0.8533
Training Epoch: 25 [23168/50048]	Loss: 0.6641
Training Epoch: 25 [23296/50048]	Loss: 0.7754
Training Epoch: 25 [23424/50048]	Loss: 0.7238
Training Epoch: 25 [23552/50048]	Loss: 1.0557
Training Epoch: 25 [23680/50048]	Loss: 0.7338
Training Epoch: 25 [23808/50048]	Loss: 0.9323
Training Epoch: 25 [23936/50048]	Loss: 0.7244
Training Epoch: 25 [24064/50048]	Loss: 0.7523
Training Epoch: 25 [24192/50048]	Loss: 0.7741
Training Epoch: 25 [24320/50048]	Loss: 0.7731
Training Epoch: 25 [24448/50048]	Loss: 0.7237
Training Epoch: 25 [24576/50048]	Loss: 0.7442
Training Epoch: 25 [24704/50048]	Loss: 0.7688
Training Epoch: 25 [24832/50048]	Loss: 0.7575
Training Epoch: 25 [24960/50048]	Loss: 0.7525
Training Epoch: 25 [25088/50048]	Loss: 0.8278
Training Epoch: 25 [25216/50048]	Loss: 0.9929
Training Epoch: 25 [25344/50048]	Loss: 0.8706
Training Epoch: 25 [25472/50048]	Loss: 0.8588
Training Epoch: 25 [25600/50048]	Loss: 0.5857
Training Epoch: 25 [25728/50048]	Loss: 0.7904
Training Epoch: 25 [25856/50048]	Loss: 0.9274
Training Epoch: 25 [25984/50048]	Loss: 0.8095
Training Epoch: 25 [26112/50048]	Loss: 0.9147
Training Epoch: 25 [26240/50048]	Loss: 0.9567
Training Epoch: 25 [26368/50048]	Loss: 0.7310
Training Epoch: 25 [26496/50048]	Loss: 0.8543
Training Epoch: 25 [26624/50048]	Loss: 0.8594
Training Epoch: 25 [26752/50048]	Loss: 0.8339
Training Epoch: 25 [26880/50048]	Loss: 0.7143
Training Epoch: 25 [27008/50048]	Loss: 0.9484
Training Epoch: 25 [27136/50048]	Loss: 0.8079
Training Epoch: 25 [27264/50048]	Loss: 0.8721
Training Epoch: 25 [27392/50048]	Loss: 0.8373
Training Epoch: 25 [27520/50048]	Loss: 0.8032
Training Epoch: 25 [27648/50048]	Loss: 0.8924
Training Epoch: 25 [27776/50048]	Loss: 0.9468
Training Epoch: 25 [27904/50048]	Loss: 1.0064
Training Epoch: 25 [28032/50048]	Loss: 0.7086
Training Epoch: 25 [28160/50048]	Loss: 1.0157
Training Epoch: 25 [28288/50048]	Loss: 0.9868
Training Epoch: 25 [28416/50048]	Loss: 0.8448
Training Epoch: 25 [28544/50048]	Loss: 1.0524
Training Epoch: 25 [28672/50048]	Loss: 0.7577
Training Epoch: 25 [28800/50048]	Loss: 0.9184
Training Epoch: 25 [28928/50048]	Loss: 0.6258
Training Epoch: 25 [29056/50048]	Loss: 0.9705
Training Epoch: 25 [29184/50048]	Loss: 0.8676
Training Epoch: 25 [29312/50048]	Loss: 0.7350
Training Epoch: 25 [29440/50048]	Loss: 0.6619
Training Epoch: 25 [29568/50048]	Loss: 0.8395
Training Epoch: 25 [29696/50048]	Loss: 0.9214
Training Epoch: 25 [29824/50048]	Loss: 0.8118
Training Epoch: 25 [29952/50048]	Loss: 1.0449
Training Epoch: 25 [30080/50048]	Loss: 0.8663
Training Epoch: 25 [30208/50048]	Loss: 0.7104
Training Epoch: 25 [30336/50048]	Loss: 1.0123
Training Epoch: 25 [30464/50048]	Loss: 0.8138
Training Epoch: 25 [30592/50048]	Loss: 0.9028
Training Epoch: 25 [30720/50048]	Loss: 0.8010
Training Epoch: 25 [30848/50048]	Loss: 0.6551
Training Epoch: 25 [30976/50048]	Loss: 0.8978
Training Epoch: 25 [31104/50048]	Loss: 0.7740
Training Epoch: 25 [31232/50048]	Loss: 0.8552
Training Epoch: 25 [31360/50048]	Loss: 0.8872
Training Epoch: 25 [31488/50048]	Loss: 0.8952
Training Epoch: 25 [31616/50048]	Loss: 0.9058
Training Epoch: 25 [31744/50048]	Loss: 0.8583
Training Epoch: 25 [31872/50048]	Loss: 0.8812
Training Epoch: 25 [32000/50048]	Loss: 0.8234
Training Epoch: 25 [32128/50048]	Loss: 0.9003
Training Epoch: 25 [32256/50048]	Loss: 0.7924
Training Epoch: 25 [32384/50048]	Loss: 0.7062
Training Epoch: 25 [32512/50048]	Loss: 0.9602
Training Epoch: 25 [32640/50048]	Loss: 0.8292
Training Epoch: 25 [32768/50048]	Loss: 0.7684
Training Epoch: 25 [32896/50048]	Loss: 0.7686
Training Epoch: 25 [33024/50048]	Loss: 0.7206
Training Epoch: 25 [33152/50048]	Loss: 0.7475
Training Epoch: 25 [33280/50048]	Loss: 0.9495
Training Epoch: 25 [33408/50048]	Loss: 0.8793
Training Epoch: 25 [33536/50048]	Loss: 0.9111
Training Epoch: 25 [33664/50048]	Loss: 0.8335
Training Epoch: 25 [33792/50048]	Loss: 0.8771
Training Epoch: 25 [33920/50048]	Loss: 0.9299
Training Epoch: 25 [34048/50048]	Loss: 0.7205
Training Epoch: 25 [34176/50048]	Loss: 1.1358
Training Epoch: 25 [34304/50048]	Loss: 0.8547
Training Epoch: 25 [34432/50048]	Loss: 0.8461
Training Epoch: 25 [34560/50048]	Loss: 0.8207
Training Epoch: 25 [34688/50048]	Loss: 1.0031
Training Epoch: 25 [34816/50048]	Loss: 0.7200
Training Epoch: 25 [34944/50048]	Loss: 0.9547
Training Epoch: 25 [35072/50048]	Loss: 0.8802
Training Epoch: 25 [35200/50048]	Loss: 0.6632
Training Epoch: 25 [35328/50048]	Loss: 0.7187
Training Epoch: 25 [35456/50048]	Loss: 0.8569
Training Epoch: 25 [35584/50048]	Loss: 0.8359
Training Epoch: 25 [35712/50048]	Loss: 0.8989
Training Epoch: 25 [35840/50048]	Loss: 0.9519
Training Epoch: 25 [35968/50048]	Loss: 0.9385
Training Epoch: 25 [36096/50048]	Loss: 0.8901
Training Epoch: 25 [36224/50048]	Loss: 0.6842
Training Epoch: 25 [36352/50048]	Loss: 1.1179
Training Epoch: 25 [36480/50048]	Loss: 0.8288
Training Epoch: 25 [36608/50048]	Loss: 0.9002
Training Epoch: 25 [36736/50048]	Loss: 1.0721
Training Epoch: 25 [36864/50048]	Loss: 0.7119
Training Epoch: 25 [36992/50048]	Loss: 0.8315
Training Epoch: 25 [37120/50048]	Loss: 0.9535
Training Epoch: 25 [37248/50048]	Loss: 0.6630
Training Epoch: 25 [37376/50048]	Loss: 0.8484
Training Epoch: 25 [37504/50048]	Loss: 0.9604
Training Epoch: 25 [37632/50048]	Loss: 0.6572
Training Epoch: 25 [37760/50048]	Loss: 0.9529
Training Epoch: 25 [37888/50048]	Loss: 1.0447
Training Epoch: 25 [38016/50048]	Loss: 0.9800
Training Epoch: 25 [38144/50048]	Loss: 0.6608
Training Epoch: 25 [38272/50048]	Loss: 0.9086
Training Epoch: 25 [38400/50048]	Loss: 0.8164
Training Epoch: 25 [38528/50048]	Loss: 0.7267
Training Epoch: 25 [38656/50048]	Loss: 0.8777
Training Epoch: 25 [38784/50048]	Loss: 0.6207
Training Epoch: 25 [38912/50048]	Loss: 0.6754
Training Epoch: 25 [39040/50048]	Loss: 0.7845
Training Epoch: 25 [39168/50048]	Loss: 0.8938
Training Epoch: 25 [39296/50048]	Loss: 0.6922
Training Epoch: 25 [39424/50048]	Loss: 0.8009
Training Epoch: 25 [39552/50048]	Loss: 0.8992
Training Epoch: 25 [39680/50048]	Loss: 0.8965
Training Epoch: 25 [39808/50048]	Loss: 0.9097
Training Epoch: 25 [39936/50048]	Loss: 0.7592
Training Epoch: 25 [40064/50048]	Loss: 0.9374
Training Epoch: 25 [40192/50048]	Loss: 0.7993
Training Epoch: 25 [40320/50048]	Loss: 0.6790
Training Epoch: 25 [40448/50048]	Loss: 0.7738
Training Epoch: 25 [40576/50048]	Loss: 0.8591
Training Epoch: 25 [40704/50048]	Loss: 0.9313
Training Epoch: 25 [40832/50048]	Loss: 0.7747
Training Epoch: 25 [40960/50048]	Loss: 0.9538
Training Epoch: 25 [41088/50048]	Loss: 0.7334
Training Epoch: 25 [41216/50048]	Loss: 0.9159
Training Epoch: 25 [41344/50048]	Loss: 0.9086
Training Epoch: 25 [41472/50048]	Loss: 0.7856
Training Epoch: 25 [41600/50048]	Loss: 0.7970
Training Epoch: 25 [41728/50048]	Loss: 0.7272
Training Epoch: 25 [41856/50048]	Loss: 0.9364
Training Epoch: 25 [41984/50048]	Loss: 1.0208
Training Epoch: 25 [42112/50048]	Loss: 0.8791
Training Epoch: 25 [42240/50048]	Loss: 1.1966
Training Epoch: 25 [42368/50048]	Loss: 1.0042
Training Epoch: 25 [42496/50048]	Loss: 0.8604
Training Epoch: 25 [42624/50048]	Loss: 1.0168
Training Epoch: 25 [42752/50048]	Loss: 0.8409
Training Epoch: 25 [42880/50048]	Loss: 1.0306
Training Epoch: 25 [43008/50048]	Loss: 1.1106
Training Epoch: 25 [43136/50048]	Loss: 0.7581
Training Epoch: 25 [43264/50048]	Loss: 0.8163
Training Epoch: 25 [43392/50048]	Loss: 0.9035
Training Epoch: 25 [43520/50048]	Loss: 0.7811
Training Epoch: 25 [43648/50048]	Loss: 0.8965
Training Epoch: 25 [43776/50048]	Loss: 0.7805
Training Epoch: 25 [43904/50048]	Loss: 1.0271
Training Epoch: 25 [44032/50048]	Loss: 1.0481
Training Epoch: 25 [44160/50048]	Loss: 0.8309
Training Epoch: 25 [44288/50048]	Loss: 0.8890
Training Epoch: 25 [44416/50048]	Loss: 0.5853
Training Epoch: 25 [44544/50048]	Loss: 0.7751
Training Epoch: 25 [44672/50048]	Loss: 0.8408
Training Epoch: 25 [44800/50048]	Loss: 0.9779
Training Epoch: 25 [44928/50048]	Loss: 0.9011
Training Epoch: 25 [45056/50048]	Loss: 1.0550
Training Epoch: 25 [45184/50048]	Loss: 0.6868
Training Epoch: 25 [45312/50048]	Loss: 0.9754
Training Epoch: 25 [45440/50048]	Loss: 0.9611
Training Epoch: 25 [45568/50048]	Loss: 0.6788
Training Epoch: 25 [45696/50048]	Loss: 0.8375
2022-12-06 04:16:26,820 [ZeusDataLoader(train)] train epoch 26 done: time=86.52 energy=10510.41
2022-12-06 04:16:26,822 [ZeusDataLoader(eval)] Epoch 26 begin.
Training Epoch: 25 [45824/50048]	Loss: 0.8399
Training Epoch: 25 [45952/50048]	Loss: 0.7891
Training Epoch: 25 [46080/50048]	Loss: 0.8714
Training Epoch: 25 [46208/50048]	Loss: 0.8767
Training Epoch: 25 [46336/50048]	Loss: 0.8485
Training Epoch: 25 [46464/50048]	Loss: 0.8753
Training Epoch: 25 [46592/50048]	Loss: 0.8440
Training Epoch: 25 [46720/50048]	Loss: 0.8350
Training Epoch: 25 [46848/50048]	Loss: 0.9479
Training Epoch: 25 [46976/50048]	Loss: 0.9000
Training Epoch: 25 [47104/50048]	Loss: 0.7750
Training Epoch: 25 [47232/50048]	Loss: 0.8348
Training Epoch: 25 [47360/50048]	Loss: 0.9371
Training Epoch: 25 [47488/50048]	Loss: 0.7675
Training Epoch: 25 [47616/50048]	Loss: 1.0033
Training Epoch: 25 [47744/50048]	Loss: 1.0234
Training Epoch: 25 [47872/50048]	Loss: 1.0699
Training Epoch: 25 [48000/50048]	Loss: 0.6608
Training Epoch: 25 [48128/50048]	Loss: 0.8817
Training Epoch: 25 [48256/50048]	Loss: 0.6690
Training Epoch: 25 [48384/50048]	Loss: 0.9220
Training Epoch: 25 [48512/50048]	Loss: 0.7492
Training Epoch: 25 [48640/50048]	Loss: 0.7914
Training Epoch: 25 [48768/50048]	Loss: 0.8970
Training Epoch: 25 [48896/50048]	Loss: 1.0290
Training Epoch: 25 [49024/50048]	Loss: 0.8086
Training Epoch: 25 [49152/50048]	Loss: 0.8830
Training Epoch: 25 [49280/50048]	Loss: 0.7263
Training Epoch: 25 [49408/50048]	Loss: 0.8333
Training Epoch: 25 [49536/50048]	Loss: 0.9355
Training Epoch: 25 [49664/50048]	Loss: 0.7093
Training Epoch: 25 [49792/50048]	Loss: 0.8930
Training Epoch: 25 [49920/50048]	Loss: 1.0135
Training Epoch: 25 [50048/50048]	Loss: 0.8874
2022-12-06 09:16:30.508 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:16:30,524 [ZeusDataLoader(eval)] eval epoch 26 done: time=3.69 energy=454.22
2022-12-06 04:16:30,525 [ZeusDataLoader(train)] Up to epoch 26: time=2347.74, energy=284797.77, cost=347825.96
2022-12-06 04:16:30,525 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:16:30,525 [ZeusDataLoader(train)] Expected next epoch: time=2437.54, energy=295595.79, cost=361082.34
2022-12-06 04:16:30,526 [ZeusDataLoader(train)] Epoch 27 begin.
Validation Epoch: 25, Average loss: 0.0119, Accuracy: 0.6137
2022-12-06 04:16:30,707 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:16:30,708 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:16:30.710 [ZeusMonitor] Monitor started.
2022-12-06 09:16:30.710 [ZeusMonitor] Running indefinitely. 2022-12-06 09:16:30.710 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:16:30.710 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e27+gpu0.power.log
Training Epoch: 26 [128/50048]	Loss: 0.6777
Training Epoch: 26 [256/50048]	Loss: 0.6640
Training Epoch: 26 [384/50048]	Loss: 0.7142
Training Epoch: 26 [512/50048]	Loss: 0.7929
Training Epoch: 26 [640/50048]	Loss: 0.7282
Training Epoch: 26 [768/50048]	Loss: 0.5528
Training Epoch: 26 [896/50048]	Loss: 0.6626
Training Epoch: 26 [1024/50048]	Loss: 0.7337
Training Epoch: 26 [1152/50048]	Loss: 0.8565
Training Epoch: 26 [1280/50048]	Loss: 0.6460
Training Epoch: 26 [1408/50048]	Loss: 0.6679
Training Epoch: 26 [1536/50048]	Loss: 0.7186
Training Epoch: 26 [1664/50048]	Loss: 0.6905
Training Epoch: 26 [1792/50048]	Loss: 0.7374
Training Epoch: 26 [1920/50048]	Loss: 0.7379
Training Epoch: 26 [2048/50048]	Loss: 0.7687
Training Epoch: 26 [2176/50048]	Loss: 0.7199
Training Epoch: 26 [2304/50048]	Loss: 0.7694
Training Epoch: 26 [2432/50048]	Loss: 0.6603
Training Epoch: 26 [2560/50048]	Loss: 0.9618
Training Epoch: 26 [2688/50048]	Loss: 0.4999
Training Epoch: 26 [2816/50048]	Loss: 0.7222
Training Epoch: 26 [2944/50048]	Loss: 0.6942
Training Epoch: 26 [3072/50048]	Loss: 0.9507
Training Epoch: 26 [3200/50048]	Loss: 0.8129
Training Epoch: 26 [3328/50048]	Loss: 0.5931
Training Epoch: 26 [3456/50048]	Loss: 0.6078
Training Epoch: 26 [3584/50048]	Loss: 0.6395
Training Epoch: 26 [3712/50048]	Loss: 0.7018
Training Epoch: 26 [3840/50048]	Loss: 0.7002
Training Epoch: 26 [3968/50048]	Loss: 0.6973
Training Epoch: 26 [4096/50048]	Loss: 0.7094
Training Epoch: 26 [4224/50048]	Loss: 0.7875
Training Epoch: 26 [4352/50048]	Loss: 0.8312
Training Epoch: 26 [4480/50048]	Loss: 0.6337
Training Epoch: 26 [4608/50048]	Loss: 0.9307
Training Epoch: 26 [4736/50048]	Loss: 0.7612
Training Epoch: 26 [4864/50048]	Loss: 0.4896
Training Epoch: 26 [4992/50048]	Loss: 0.8318
Training Epoch: 26 [5120/50048]	Loss: 0.6511
Training Epoch: 26 [5248/50048]	Loss: 0.6393
Training Epoch: 26 [5376/50048]	Loss: 0.8930
Training Epoch: 26 [5504/50048]	Loss: 0.7574
Training Epoch: 26 [5632/50048]	Loss: 0.5614
Training Epoch: 26 [5760/50048]	Loss: 0.7002
Training Epoch: 26 [5888/50048]	Loss: 0.8604
Training Epoch: 26 [6016/50048]	Loss: 0.7486
Training Epoch: 26 [6144/50048]	Loss: 0.7875
Training Epoch: 26 [6272/50048]	Loss: 0.6828
Training Epoch: 26 [6400/50048]	Loss: 0.6725
Training Epoch: 26 [6528/50048]	Loss: 0.6010
Training Epoch: 26 [6656/50048]	Loss: 0.6828
Training Epoch: 26 [6784/50048]	Loss: 0.6262
Training Epoch: 26 [6912/50048]	Loss: 0.7055
Training Epoch: 26 [7040/50048]	Loss: 0.6892
Training Epoch: 26 [7168/50048]	Loss: 0.7182
Training Epoch: 26 [7296/50048]	Loss: 0.6527
Training Epoch: 26 [7424/50048]	Loss: 0.9181
Training Epoch: 26 [7552/50048]	Loss: 0.9541
Training Epoch: 26 [7680/50048]	Loss: 0.7359
Training Epoch: 26 [7808/50048]	Loss: 0.8162
Training Epoch: 26 [7936/50048]	Loss: 0.6938
Training Epoch: 26 [8064/50048]	Loss: 0.5967
Training Epoch: 26 [8192/50048]	Loss: 0.5768
Training Epoch: 26 [8320/50048]	Loss: 0.6890
Training Epoch: 26 [8448/50048]	Loss: 0.8109
Training Epoch: 26 [8576/50048]	Loss: 0.8168
Training Epoch: 26 [8704/50048]	Loss: 0.6752
Training Epoch: 26 [8832/50048]	Loss: 0.7512
Training Epoch: 26 [8960/50048]	Loss: 0.8631
Training Epoch: 26 [9088/50048]	Loss: 0.6735
Training Epoch: 26 [9216/50048]	Loss: 0.7443
Training Epoch: 26 [9344/50048]	Loss: 0.6265
Training Epoch: 26 [9472/50048]	Loss: 0.8134
Training Epoch: 26 [9600/50048]	Loss: 0.6777
Training Epoch: 26 [9728/50048]	Loss: 0.8387
Training Epoch: 26 [9856/50048]	Loss: 0.7153
Training Epoch: 26 [9984/50048]	Loss: 0.7598
Training Epoch: 26 [10112/50048]	Loss: 0.8299
Training Epoch: 26 [10240/50048]	Loss: 0.7880
Training Epoch: 26 [10368/50048]	Loss: 0.5702
Training Epoch: 26 [10496/50048]	Loss: 0.7574
Training Epoch: 26 [10624/50048]	Loss: 1.0484
Training Epoch: 26 [10752/50048]	Loss: 0.8012
Training Epoch: 26 [10880/50048]	Loss: 0.7163
Training Epoch: 26 [11008/50048]	Loss: 0.6226
Training Epoch: 26 [11136/50048]	Loss: 0.6095
Training Epoch: 26 [11264/50048]	Loss: 0.7131
Training Epoch: 26 [11392/50048]	Loss: 0.6949
Training Epoch: 26 [11520/50048]	Loss: 0.7904
Training Epoch: 26 [11648/50048]	Loss: 0.7612
Training Epoch: 26 [11776/50048]	Loss: 0.5941
Training Epoch: 26 [11904/50048]	Loss: 0.6649
Training Epoch: 26 [12032/50048]	Loss: 0.6674
Training Epoch: 26 [12160/50048]	Loss: 0.6905
Training Epoch: 26 [12288/50048]	Loss: 0.7640
Training Epoch: 26 [12416/50048]	Loss: 0.5958
Training Epoch: 26 [12544/50048]	Loss: 0.6894
Training Epoch: 26 [12672/50048]	Loss: 0.7516
Training Epoch: 26 [12800/50048]	Loss: 0.6567
Training Epoch: 26 [12928/50048]	Loss: 0.7929
Training Epoch: 26 [13056/50048]	Loss: 0.6222
Training Epoch: 26 [13184/50048]	Loss: 0.7843
Training Epoch: 26 [13312/50048]	Loss: 0.5957
Training Epoch: 26 [13440/50048]	Loss: 0.6714
Training Epoch: 26 [13568/50048]	Loss: 0.5961
Training Epoch: 26 [13696/50048]	Loss: 0.8414
Training Epoch: 26 [13824/50048]	Loss: 0.8961
Training Epoch: 26 [13952/50048]	Loss: 0.6828
Training Epoch: 26 [14080/50048]	Loss: 0.7682
Training Epoch: 26 [14208/50048]	Loss: 0.7325
Training Epoch: 26 [14336/50048]	Loss: 0.6730
Training Epoch: 26 [14464/50048]	Loss: 0.7394
Training Epoch: 26 [14592/50048]	Loss: 0.6829
Training Epoch: 26 [14720/50048]	Loss: 0.7335
Training Epoch: 26 [14848/50048]	Loss: 0.6676
Training Epoch: 26 [14976/50048]	Loss: 0.6370
Training Epoch: 26 [15104/50048]	Loss: 0.7876
Training Epoch: 26 [15232/50048]	Loss: 0.8581
Training Epoch: 26 [15360/50048]	Loss: 0.7538
Training Epoch: 26 [15488/50048]	Loss: 0.8101
Training Epoch: 26 [15616/50048]	Loss: 0.7572
Training Epoch: 26 [15744/50048]	Loss: 0.6862
Training Epoch: 26 [15872/50048]	Loss: 0.9539
Training Epoch: 26 [16000/50048]	Loss: 0.6256
Training Epoch: 26 [16128/50048]	Loss: 0.7776
Training Epoch: 26 [16256/50048]	Loss: 0.6359
Training Epoch: 26 [16384/50048]	Loss: 0.7542
Training Epoch: 26 [16512/50048]	Loss: 0.6961
Training Epoch: 26 [16640/50048]	Loss: 0.8771
Training Epoch: 26 [16768/50048]	Loss: 0.6871
Training Epoch: 26 [16896/50048]	Loss: 0.7815
Training Epoch: 26 [17024/50048]	Loss: 0.7316
Training Epoch: 26 [17152/50048]	Loss: 0.9461
Training Epoch: 26 [17280/50048]	Loss: 0.6905
Training Epoch: 26 [17408/50048]	Loss: 0.9289
Training Epoch: 26 [17536/50048]	Loss: 0.7334
Training Epoch: 26 [17664/50048]	Loss: 0.9682
Training Epoch: 26 [17792/50048]	Loss: 0.7065
Training Epoch: 26 [17920/50048]	Loss: 0.8768
Training Epoch: 26 [18048/50048]	Loss: 0.7519
Training Epoch: 26 [18176/50048]	Loss: 0.8138
Training Epoch: 26 [18304/50048]	Loss: 0.6340
Training Epoch: 26 [18432/50048]	Loss: 0.7015
Training Epoch: 26 [18560/50048]	Loss: 0.7065
Training Epoch: 26 [18688/50048]	Loss: 0.9979
Training Epoch: 26 [18816/50048]	Loss: 0.8747
Training Epoch: 26 [18944/50048]	Loss: 0.7801
Training Epoch: 26 [19072/50048]	Loss: 0.8269
Training Epoch: 26 [19200/50048]	Loss: 0.6091
Training Epoch: 26 [19328/50048]	Loss: 0.7943
Training Epoch: 26 [19456/50048]	Loss: 0.7069
Training Epoch: 26 [19584/50048]	Loss: 0.7741
Training Epoch: 26 [19712/50048]	Loss: 0.7260
Training Epoch: 26 [19840/50048]	Loss: 0.8185
Training Epoch: 26 [19968/50048]	Loss: 0.7337
Training Epoch: 26 [20096/50048]	Loss: 0.7106
Training Epoch: 26 [20224/50048]	Loss: 0.8274
Training Epoch: 26 [20352/50048]	Loss: 0.5980
Training Epoch: 26 [20480/50048]	Loss: 0.7138
Training Epoch: 26 [20608/50048]	Loss: 0.7045
Training Epoch: 26 [20736/50048]	Loss: 0.6481
Training Epoch: 26 [20864/50048]	Loss: 0.7736
Training Epoch: 26 [20992/50048]	Loss: 0.8719
Training Epoch: 26 [21120/50048]	Loss: 0.8315
Training Epoch: 26 [21248/50048]	Loss: 0.7327
Training Epoch: 26 [21376/50048]	Loss: 0.5864
Training Epoch: 26 [21504/50048]	Loss: 0.7065
Training Epoch: 26 [21632/50048]	Loss: 0.5346
Training Epoch: 26 [21760/50048]	Loss: 0.7943
Training Epoch: 26 [21888/50048]	Loss: 0.8296
Training Epoch: 26 [22016/50048]	Loss: 0.8451
Training Epoch: 26 [22144/50048]	Loss: 0.8192
Training Epoch: 26 [22272/50048]	Loss: 0.6274
Training Epoch: 26 [22400/50048]	Loss: 0.6923
Training Epoch: 26 [22528/50048]	Loss: 0.8693
Training Epoch: 26 [22656/50048]	Loss: 0.6508
Training Epoch: 26 [22784/50048]	Loss: 0.7642
Training Epoch: 26 [22912/50048]	Loss: 0.9946
Training Epoch: 26 [23040/50048]	Loss: 0.7469
Training Epoch: 26 [23168/50048]	Loss: 0.6506
Training Epoch: 26 [23296/50048]	Loss: 0.8217
Training Epoch: 26 [23424/50048]	Loss: 0.8646
Training Epoch: 26 [23552/50048]	Loss: 0.7389
Training Epoch: 26 [23680/50048]	Loss: 0.6971
Training Epoch: 26 [23808/50048]	Loss: 0.8380
Training Epoch: 26 [23936/50048]	Loss: 0.6763
Training Epoch: 26 [24064/50048]	Loss: 0.6818
Training Epoch: 26 [24192/50048]	Loss: 0.8414
Training Epoch: 26 [24320/50048]	Loss: 0.6602
Training Epoch: 26 [24448/50048]	Loss: 0.8035
Training Epoch: 26 [24576/50048]	Loss: 0.7971
Training Epoch: 26 [24704/50048]	Loss: 0.7114
Training Epoch: 26 [24832/50048]	Loss: 0.9189
Training Epoch: 26 [24960/50048]	Loss: 0.9651
Training Epoch: 26 [25088/50048]	Loss: 0.7564
Training Epoch: 26 [25216/50048]	Loss: 0.5948
Training Epoch: 26 [25344/50048]	Loss: 0.8766
Training Epoch: 26 [25472/50048]	Loss: 0.6464
Training Epoch: 26 [25600/50048]	Loss: 0.8183
Training Epoch: 26 [25728/50048]	Loss: 0.9721
Training Epoch: 26 [25856/50048]	Loss: 0.6930
Training Epoch: 26 [25984/50048]	Loss: 0.7728
Training Epoch: 26 [26112/50048]	Loss: 0.7783
Training Epoch: 26 [26240/50048]	Loss: 0.7153
Training Epoch: 26 [26368/50048]	Loss: 0.7288
Training Epoch: 26 [26496/50048]	Loss: 0.8876
Training Epoch: 26 [26624/50048]	Loss: 0.7491
Training Epoch: 26 [26752/50048]	Loss: 0.7417
Training Epoch: 26 [26880/50048]	Loss: 0.6899
Training Epoch: 26 [27008/50048]	Loss: 0.8163
Training Epoch: 26 [27136/50048]	Loss: 0.9605
Training Epoch: 26 [27264/50048]	Loss: 0.8585
Training Epoch: 26 [27392/50048]	Loss: 0.9213
Training Epoch: 26 [27520/50048]	Loss: 0.6206
Training Epoch: 26 [27648/50048]	Loss: 0.8571
Training Epoch: 26 [27776/50048]	Loss: 0.6548
Training Epoch: 26 [27904/50048]	Loss: 0.6405
Training Epoch: 26 [28032/50048]	Loss: 0.5881
Training Epoch: 26 [28160/50048]	Loss: 0.7513
Training Epoch: 26 [28288/50048]	Loss: 0.7200
Training Epoch: 26 [28416/50048]	Loss: 0.7119
Training Epoch: 26 [28544/50048]	Loss: 0.8110
Training Epoch: 26 [28672/50048]	Loss: 0.6668
Training Epoch: 26 [28800/50048]	Loss: 0.7100
Training Epoch: 26 [28928/50048]	Loss: 0.6562
Training Epoch: 26 [29056/50048]	Loss: 0.7868
Training Epoch: 26 [29184/50048]	Loss: 0.7254
Training Epoch: 26 [29312/50048]	Loss: 0.7702
Training Epoch: 26 [29440/50048]	Loss: 0.8451
Training Epoch: 26 [29568/50048]	Loss: 0.7000
Training Epoch: 26 [29696/50048]	Loss: 0.7441
Training Epoch: 26 [29824/50048]	Loss: 0.6591
Training Epoch: 26 [29952/50048]	Loss: 0.7672
Training Epoch: 26 [30080/50048]	Loss: 0.8170
Training Epoch: 26 [30208/50048]	Loss: 0.7557
Training Epoch: 26 [30336/50048]	Loss: 1.0179
Training Epoch: 26 [30464/50048]	Loss: 0.7099
Training Epoch: 26 [30592/50048]	Loss: 1.0592
Training Epoch: 26 [30720/50048]	Loss: 1.1610
Training Epoch: 26 [30848/50048]	Loss: 0.7261
Training Epoch: 26 [30976/50048]	Loss: 0.6969
Training Epoch: 26 [31104/50048]	Loss: 0.9387
Training Epoch: 26 [31232/50048]	Loss: 0.6659
Training Epoch: 26 [31360/50048]	Loss: 0.7444
Training Epoch: 26 [31488/50048]	Loss: 0.6943
Training Epoch: 26 [31616/50048]	Loss: 0.8233
Training Epoch: 26 [31744/50048]	Loss: 0.8260
Training Epoch: 26 [31872/50048]	Loss: 0.6714
Training Epoch: 26 [32000/50048]	Loss: 0.7377
Training Epoch: 26 [32128/50048]	Loss: 0.7528
Training Epoch: 26 [32256/50048]	Loss: 0.8340
Training Epoch: 26 [32384/50048]	Loss: 0.7645
Training Epoch: 26 [32512/50048]	Loss: 0.7366
Training Epoch: 26 [32640/50048]	Loss: 0.7589
Training Epoch: 26 [32768/50048]	Loss: 0.9095
Training Epoch: 26 [32896/50048]	Loss: 0.7986
Training Epoch: 26 [33024/50048]	Loss: 0.6797
Training Epoch: 26 [33152/50048]	Loss: 0.7987
Training Epoch: 26 [33280/50048]	Loss: 0.9415
Training Epoch: 26 [33408/50048]	Loss: 0.7720
Training Epoch: 26 [33536/50048]	Loss: 0.7494
Training Epoch: 26 [33664/50048]	Loss: 0.6648
Training Epoch: 26 [33792/50048]	Loss: 0.6806
Training Epoch: 26 [33920/50048]	Loss: 0.7756
Training Epoch: 26 [34048/50048]	Loss: 0.7838
Training Epoch: 26 [34176/50048]	Loss: 0.8561
Training Epoch: 26 [34304/50048]	Loss: 0.8606
Training Epoch: 26 [34432/50048]	Loss: 1.0279
Training Epoch: 26 [34560/50048]	Loss: 0.7341
Training Epoch: 26 [34688/50048]	Loss: 0.8771
Training Epoch: 26 [34816/50048]	Loss: 0.9982
Training Epoch: 26 [34944/50048]	Loss: 0.7017
Training Epoch: 26 [35072/50048]	Loss: 0.8761
Training Epoch: 26 [35200/50048]	Loss: 0.7293
Training Epoch: 26 [35328/50048]	Loss: 0.8328
Training Epoch: 26 [35456/50048]	Loss: 0.5978
Training Epoch: 26 [35584/50048]	Loss: 0.8844
Training Epoch: 26 [35712/50048]	Loss: 0.8646
Training Epoch: 26 [35840/50048]	Loss: 0.8839
Training Epoch: 26 [35968/50048]	Loss: 0.9725
Training Epoch: 26 [36096/50048]	Loss: 0.7449
Training Epoch: 26 [36224/50048]	Loss: 0.5567
Training Epoch: 26 [36352/50048]	Loss: 0.7637
Training Epoch: 26 [36480/50048]	Loss: 0.6824
Training Epoch: 26 [36608/50048]	Loss: 0.6786
Training Epoch: 26 [36736/50048]	Loss: 0.9582
Training Epoch: 26 [36864/50048]	Loss: 0.8105
Training Epoch: 26 [36992/50048]	Loss: 0.8681
Training Epoch: 26 [37120/50048]	Loss: 0.7847
Training Epoch: 26 [37248/50048]	Loss: 0.6448
Training Epoch: 26 [37376/50048]	Loss: 0.9254
Training Epoch: 26 [37504/50048]	Loss: 0.7400
Training Epoch: 26 [37632/50048]	Loss: 0.7669
Training Epoch: 26 [37760/50048]	Loss: 0.8717
Training Epoch: 26 [37888/50048]	Loss: 0.6488
Training Epoch: 26 [38016/50048]	Loss: 0.8710
Training Epoch: 26 [38144/50048]	Loss: 0.8428
Training Epoch: 26 [38272/50048]	Loss: 0.9077
Training Epoch: 26 [38400/50048]	Loss: 0.8426
Training Epoch: 26 [38528/50048]	Loss: 0.9246
Training Epoch: 26 [38656/50048]	Loss: 0.7619
Training Epoch: 26 [38784/50048]	Loss: 0.7721
Training Epoch: 26 [38912/50048]	Loss: 0.8579
Training Epoch: 26 [39040/50048]	Loss: 0.7703
Training Epoch: 26 [39168/50048]	Loss: 0.8684
Training Epoch: 26 [39296/50048]	Loss: 0.6999
Training Epoch: 26 [39424/50048]	Loss: 0.8810
Training Epoch: 26 [39552/50048]	Loss: 0.8878
Training Epoch: 26 [39680/50048]	Loss: 0.8163
Training Epoch: 26 [39808/50048]	Loss: 0.8234
Training Epoch: 26 [39936/50048]	Loss: 0.7630
Training Epoch: 26 [40064/50048]	Loss: 0.6946
Training Epoch: 26 [40192/50048]	Loss: 0.8687
Training Epoch: 26 [40320/50048]	Loss: 0.7964
Training Epoch: 26 [40448/50048]	Loss: 0.7559
Training Epoch: 26 [40576/50048]	Loss: 0.6222
Training Epoch: 26 [40704/50048]	Loss: 0.7071
Training Epoch: 26 [40832/50048]	Loss: 0.9591
Training Epoch: 26 [40960/50048]	Loss: 0.9401
Training Epoch: 26 [41088/50048]	Loss: 1.0387
Training Epoch: 26 [41216/50048]	Loss: 0.9732
Training Epoch: 26 [41344/50048]	Loss: 0.7345
Training Epoch: 26 [41472/50048]	Loss: 0.9083
Training Epoch: 26 [41600/50048]	Loss: 0.8766
Training Epoch: 26 [41728/50048]	Loss: 0.8284
Training Epoch: 26 [41856/50048]	Loss: 0.9783
Training Epoch: 26 [41984/50048]	Loss: 0.8901
Training Epoch: 26 [42112/50048]	Loss: 0.7051
Training Epoch: 26 [42240/50048]	Loss: 0.7317
Training Epoch: 26 [42368/50048]	Loss: 0.8216
Training Epoch: 26 [42496/50048]	Loss: 0.8049
Training Epoch: 26 [42624/50048]	Loss: 0.8141
Training Epoch: 26 [42752/50048]	Loss: 0.8688
Training Epoch: 26 [42880/50048]	Loss: 0.7795
Training Epoch: 26 [43008/50048]	Loss: 0.9272
Training Epoch: 26 [43136/50048]	Loss: 0.8058
Training Epoch: 26 [43264/50048]	Loss: 0.7165
Training Epoch: 26 [43392/50048]	Loss: 0.4549
Training Epoch: 26 [43520/50048]	Loss: 0.9916
Training Epoch: 26 [43648/50048]	Loss: 0.8470
Training Epoch: 26 [43776/50048]	Loss: 0.7310
Training Epoch: 26 [43904/50048]	Loss: 0.8230
Training Epoch: 26 [44032/50048]	Loss: 0.8663
Training Epoch: 26 [44160/50048]	Loss: 0.8583
Training Epoch: 26 [44288/50048]	Loss: 0.9424
Training Epoch: 26 [44416/50048]	Loss: 0.8510
Training Epoch: 26 [44544/50048]	Loss: 0.7168
Training Epoch: 26 [44672/50048]	Loss: 0.7568
Training Epoch: 26 [44800/50048]	Loss: 0.9560
Training Epoch: 26 [44928/50048]	Loss: 0.6930
Training Epoch: 26 [45056/50048]	Loss: 0.8080
Training Epoch: 26 [45184/50048]	Loss: 0.9113
Training Epoch: 26 [45312/50048]	Loss: 0.8168
Training Epoch: 26 [45440/50048]	Loss: 0.8461
Training Epoch: 26 [45568/50048]	Loss: 0.6591
Training Epoch: 26 [45696/50048]	Loss: 0.7307
2022-12-06 04:17:56,997 [ZeusDataLoader(train)] train epoch 27 done: time=86.46 energy=10502.13
2022-12-06 04:17:56,999 [ZeusDataLoader(eval)] Epoch 27 begin.
Training Epoch: 26 [45824/50048]	Loss: 1.0523
Training Epoch: 26 [45952/50048]	Loss: 0.8495
Training Epoch: 26 [46080/50048]	Loss: 0.7959
Training Epoch: 26 [46208/50048]	Loss: 0.9254
Training Epoch: 26 [46336/50048]	Loss: 0.6515
Training Epoch: 26 [46464/50048]	Loss: 0.7122
Training Epoch: 26 [46592/50048]	Loss: 0.8599
Training Epoch: 26 [46720/50048]	Loss: 0.8661
Training Epoch: 26 [46848/50048]	Loss: 0.7094
Training Epoch: 26 [46976/50048]	Loss: 0.9819
Training Epoch: 26 [47104/50048]	Loss: 0.7495
Training Epoch: 26 [47232/50048]	Loss: 0.9222
Training Epoch: 26 [47360/50048]	Loss: 1.1534
Training Epoch: 26 [47488/50048]	Loss: 0.7407
Training Epoch: 26 [47616/50048]	Loss: 0.8635
Training Epoch: 26 [47744/50048]	Loss: 0.8590
Training Epoch: 26 [47872/50048]	Loss: 0.6271
Training Epoch: 26 [48000/50048]	Loss: 0.8080
Training Epoch: 26 [48128/50048]	Loss: 0.6976
Training Epoch: 26 [48256/50048]	Loss: 0.7443
Training Epoch: 26 [48384/50048]	Loss: 0.6446
Training Epoch: 26 [48512/50048]	Loss: 0.6054
Training Epoch: 26 [48640/50048]	Loss: 0.6809
Training Epoch: 26 [48768/50048]	Loss: 0.8487
Training Epoch: 26 [48896/50048]	Loss: 0.8462
Training Epoch: 26 [49024/50048]	Loss: 1.0350
Training Epoch: 26 [49152/50048]	Loss: 0.7257
Training Epoch: 26 [49280/50048]	Loss: 0.7480
Training Epoch: 26 [49408/50048]	Loss: 0.9670
Training Epoch: 26 [49536/50048]	Loss: 0.9256
Training Epoch: 26 [49664/50048]	Loss: 0.8456
Training Epoch: 26 [49792/50048]	Loss: 0.8622
Training Epoch: 26 [49920/50048]	Loss: 0.8439
Training Epoch: 26 [50048/50048]	Loss: 0.7983
2022-12-06 09:18:00.711 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:18:00,721 [ZeusDataLoader(eval)] eval epoch 27 done: time=3.71 energy=452.68
2022-12-06 04:18:00,722 [ZeusDataLoader(train)] Up to epoch 27: time=2437.91, energy=295752.58, cost=361193.67
2022-12-06 04:18:00,722 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:18:00,722 [ZeusDataLoader(train)] Expected next epoch: time=2527.71, energy=306550.60, cost=374450.05
2022-12-06 04:18:00,723 [ZeusDataLoader(train)] Epoch 28 begin.
Validation Epoch: 26, Average loss: 0.0126, Accuracy: 0.6027
2022-12-06 09:18:00.910 [ZeusMonitor] Monitor started.
2022-12-06 09:18:00.911 [ZeusMonitor] Running indefinitely. 2022-12-06 09:18:00.911 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:18:00.911 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e28+gpu0.power.log
2022-12-06 04:18:00,915 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:18:00,916 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
Training Epoch: 27 [128/50048]	Loss: 0.6386
Training Epoch: 27 [256/50048]	Loss: 0.6410
Training Epoch: 27 [384/50048]	Loss: 0.6569
Training Epoch: 27 [512/50048]	Loss: 0.7384
Training Epoch: 27 [640/50048]	Loss: 0.6059
Training Epoch: 27 [768/50048]	Loss: 0.7272
Training Epoch: 27 [896/50048]	Loss: 0.7818
Training Epoch: 27 [1024/50048]	Loss: 0.6951
Training Epoch: 27 [1152/50048]	Loss: 0.6494
Training Epoch: 27 [1280/50048]	Loss: 0.7880
Training Epoch: 27 [1408/50048]	Loss: 0.8372
Training Epoch: 27 [1536/50048]	Loss: 0.5672
Training Epoch: 27 [1664/50048]	Loss: 0.7137
Training Epoch: 27 [1792/50048]	Loss: 0.7102
Training Epoch: 27 [1920/50048]	Loss: 0.6090
Training Epoch: 27 [2048/50048]	Loss: 0.6931
Training Epoch: 27 [2176/50048]	Loss: 0.6527
Training Epoch: 27 [2304/50048]	Loss: 0.7618
Training Epoch: 27 [2432/50048]	Loss: 0.6821
Training Epoch: 27 [2560/50048]	Loss: 0.6011
Training Epoch: 27 [2688/50048]	Loss: 0.8848
Training Epoch: 27 [2816/50048]	Loss: 0.5633
Training Epoch: 27 [2944/50048]	Loss: 0.8186
Training Epoch: 27 [3072/50048]	Loss: 0.5882
Training Epoch: 27 [3200/50048]	Loss: 0.7913
Training Epoch: 27 [3328/50048]	Loss: 0.7127
Training Epoch: 27 [3456/50048]	Loss: 0.6864
Training Epoch: 27 [3584/50048]	Loss: 0.6621
Training Epoch: 27 [3712/50048]	Loss: 0.6389
Training Epoch: 27 [3840/50048]	Loss: 0.7057
Training Epoch: 27 [3968/50048]	Loss: 0.8492
Training Epoch: 27 [4096/50048]	Loss: 0.6199
Training Epoch: 27 [4224/50048]	Loss: 0.6916
Training Epoch: 27 [4352/50048]	Loss: 0.8417
Training Epoch: 27 [4480/50048]	Loss: 0.5591
Training Epoch: 27 [4608/50048]	Loss: 0.8913
Training Epoch: 27 [4736/50048]	Loss: 0.7963
Training Epoch: 27 [4864/50048]	Loss: 0.6720
Training Epoch: 27 [4992/50048]	Loss: 0.5435
Training Epoch: 27 [5120/50048]	Loss: 0.5525
Training Epoch: 27 [5248/50048]	Loss: 0.5744
Training Epoch: 27 [5376/50048]	Loss: 0.6369
Training Epoch: 27 [5504/50048]	Loss: 0.5957
Training Epoch: 27 [5632/50048]	Loss: 0.6434
Training Epoch: 27 [5760/50048]	Loss: 0.6669
Training Epoch: 27 [5888/50048]	Loss: 0.6411
Training Epoch: 27 [6016/50048]	Loss: 0.5707
Training Epoch: 27 [6144/50048]	Loss: 0.6664
Training Epoch: 27 [6272/50048]	Loss: 0.7899
Training Epoch: 27 [6400/50048]	Loss: 0.6676
Training Epoch: 27 [6528/50048]	Loss: 0.6293
Training Epoch: 27 [6656/50048]	Loss: 0.6818
Training Epoch: 27 [6784/50048]	Loss: 0.7015
Training Epoch: 27 [6912/50048]	Loss: 0.6880
Training Epoch: 27 [7040/50048]	Loss: 0.7369
Training Epoch: 27 [7168/50048]	Loss: 0.5835
Training Epoch: 27 [7296/50048]	Loss: 0.7088
Training Epoch: 27 [7424/50048]	Loss: 0.7947
Training Epoch: 27 [7552/50048]	Loss: 0.5591
Training Epoch: 27 [7680/50048]	Loss: 0.5948
Training Epoch: 27 [7808/50048]	Loss: 0.6967
Training Epoch: 27 [7936/50048]	Loss: 0.6752
Training Epoch: 27 [8064/50048]	Loss: 0.6515
Training Epoch: 27 [8192/50048]	Loss: 0.7877
Training Epoch: 27 [8320/50048]	Loss: 0.7424
Training Epoch: 27 [8448/50048]	Loss: 0.5767
Training Epoch: 27 [8576/50048]	Loss: 0.6282
Training Epoch: 27 [8704/50048]	Loss: 0.5550
Training Epoch: 27 [8832/50048]	Loss: 0.7115
Training Epoch: 27 [8960/50048]	Loss: 0.6507
Training Epoch: 27 [9088/50048]	Loss: 0.7472
Training Epoch: 27 [9216/50048]	Loss: 0.7821
Training Epoch: 27 [9344/50048]	Loss: 0.6460
Training Epoch: 27 [9472/50048]	Loss: 0.4752
Training Epoch: 27 [9600/50048]	Loss: 0.9731
Training Epoch: 27 [9728/50048]	Loss: 0.8850
Training Epoch: 27 [9856/50048]	Loss: 0.4561
Training Epoch: 27 [9984/50048]	Loss: 0.7603
Training Epoch: 27 [10112/50048]	Loss: 0.7279
Training Epoch: 27 [10240/50048]	Loss: 0.7418
Training Epoch: 27 [10368/50048]	Loss: 0.6772
Training Epoch: 27 [10496/50048]	Loss: 0.6936
Training Epoch: 27 [10624/50048]	Loss: 0.5569
Training Epoch: 27 [10752/50048]	Loss: 0.6666
Training Epoch: 27 [10880/50048]	Loss: 0.6122
Training Epoch: 27 [11008/50048]	Loss: 0.8236
Training Epoch: 27 [11136/50048]	Loss: 0.6880
Training Epoch: 27 [11264/50048]	Loss: 0.8186
Training Epoch: 27 [11392/50048]	Loss: 0.6889
Training Epoch: 27 [11520/50048]	Loss: 0.5851
Training Epoch: 27 [11648/50048]	Loss: 0.7578
Training Epoch: 27 [11776/50048]	Loss: 0.7330
Training Epoch: 27 [11904/50048]	Loss: 0.7611
Training Epoch: 27 [12032/50048]	Loss: 0.7703
Training Epoch: 27 [12160/50048]	Loss: 0.6660
Training Epoch: 27 [12288/50048]	Loss: 0.5901
Training Epoch: 27 [12416/50048]	Loss: 0.9299
Training Epoch: 27 [12544/50048]	Loss: 0.7376
Training Epoch: 27 [12672/50048]	Loss: 0.7326
Training Epoch: 27 [12800/50048]	Loss: 0.6766
Training Epoch: 27 [12928/50048]	Loss: 0.7332
Training Epoch: 27 [13056/50048]	Loss: 0.6941
Training Epoch: 27 [13184/50048]	Loss: 0.5966
Training Epoch: 27 [13312/50048]	Loss: 0.5087
Training Epoch: 27 [13440/50048]	Loss: 0.5833
Training Epoch: 27 [13568/50048]	Loss: 0.6438
Training Epoch: 27 [13696/50048]	Loss: 0.7177
Training Epoch: 27 [13824/50048]	Loss: 0.6031
Training Epoch: 27 [13952/50048]	Loss: 0.7594
Training Epoch: 27 [14080/50048]	Loss: 1.0691
Training Epoch: 27 [14208/50048]	Loss: 0.6951
Training Epoch: 27 [14336/50048]	Loss: 0.6766
Training Epoch: 27 [14464/50048]	Loss: 0.6370
Training Epoch: 27 [14592/50048]	Loss: 0.7647
Training Epoch: 27 [14720/50048]	Loss: 0.5514
Training Epoch: 27 [14848/50048]	Loss: 0.7715
Training Epoch: 27 [14976/50048]	Loss: 0.6048
Training Epoch: 27 [15104/50048]	Loss: 0.7594
Training Epoch: 27 [15232/50048]	Loss: 0.7797
Training Epoch: 27 [15360/50048]	Loss: 0.8613
Training Epoch: 27 [15488/50048]	Loss: 0.8156
Training Epoch: 27 [15616/50048]	Loss: 0.7625
Training Epoch: 27 [15744/50048]	Loss: 0.7247
Training Epoch: 27 [15872/50048]	Loss: 0.6764
Training Epoch: 27 [16000/50048]	Loss: 0.7023
Training Epoch: 27 [16128/50048]	Loss: 0.7125
Training Epoch: 27 [16256/50048]	Loss: 0.9330
Training Epoch: 27 [16384/50048]	Loss: 0.5844
Training Epoch: 27 [16512/50048]	Loss: 0.7424
Training Epoch: 27 [16640/50048]	Loss: 0.9122
Training Epoch: 27 [16768/50048]	Loss: 0.6649
Training Epoch: 27 [16896/50048]	Loss: 0.9642
Training Epoch: 27 [17024/50048]	Loss: 0.6331
Training Epoch: 27 [17152/50048]	Loss: 0.5003
Training Epoch: 27 [17280/50048]	Loss: 0.8260
Training Epoch: 27 [17408/50048]	Loss: 0.7741
Training Epoch: 27 [17536/50048]	Loss: 0.8469
Training Epoch: 27 [17664/50048]	Loss: 0.6737
Training Epoch: 27 [17792/50048]	Loss: 0.7364
Training Epoch: 27 [17920/50048]	Loss: 0.8090
Training Epoch: 27 [18048/50048]	Loss: 0.6644
Training Epoch: 27 [18176/50048]	Loss: 0.7230
Training Epoch: 27 [18304/50048]	Loss: 0.6133
Training Epoch: 27 [18432/50048]	Loss: 0.7638
Training Epoch: 27 [18560/50048]	Loss: 0.6512
Training Epoch: 27 [18688/50048]	Loss: 0.8139
Training Epoch: 27 [18816/50048]	Loss: 0.7664
Training Epoch: 27 [18944/50048]	Loss: 0.8210
Training Epoch: 27 [19072/50048]	Loss: 0.7755
Training Epoch: 27 [19200/50048]	Loss: 0.9106
Training Epoch: 27 [19328/50048]	Loss: 0.6762
Training Epoch: 27 [19456/50048]	Loss: 0.6591
Training Epoch: 27 [19584/50048]	Loss: 0.6120
Training Epoch: 27 [19712/50048]	Loss: 0.7850
Training Epoch: 27 [19840/50048]	Loss: 0.8503
Training Epoch: 27 [19968/50048]	Loss: 0.6261
Training Epoch: 27 [20096/50048]	Loss: 0.5018
Training Epoch: 27 [20224/50048]	Loss: 0.8441
Training Epoch: 27 [20352/50048]	Loss: 0.7751
Training Epoch: 27 [20480/50048]	Loss: 0.7912
Training Epoch: 27 [20608/50048]	Loss: 0.6689
Training Epoch: 27 [20736/50048]	Loss: 0.6996
Training Epoch: 27 [20864/50048]	Loss: 0.8203
Training Epoch: 27 [20992/50048]	Loss: 0.7345
Training Epoch: 27 [21120/50048]	Loss: 0.8220
Training Epoch: 27 [21248/50048]	Loss: 0.7819
Training Epoch: 27 [21376/50048]	Loss: 0.7938
Training Epoch: 27 [21504/50048]	Loss: 0.5810
Training Epoch: 27 [21632/50048]	Loss: 0.6937
Training Epoch: 27 [21760/50048]	Loss: 0.7851
Training Epoch: 27 [21888/50048]	Loss: 0.7959
Training Epoch: 27 [22016/50048]	Loss: 0.6648
Training Epoch: 27 [22144/50048]	Loss: 0.8946
Training Epoch: 27 [22272/50048]	Loss: 0.7564
Training Epoch: 27 [22400/50048]	Loss: 0.8135
Training Epoch: 27 [22528/50048]	Loss: 0.5843
Training Epoch: 27 [22656/50048]	Loss: 0.6998
Training Epoch: 27 [22784/50048]	Loss: 0.6970
Training Epoch: 27 [22912/50048]	Loss: 0.6619
Training Epoch: 27 [23040/50048]	Loss: 0.7896
Training Epoch: 27 [23168/50048]	Loss: 0.8650
Training Epoch: 27 [23296/50048]	Loss: 0.6643
Training Epoch: 27 [23424/50048]	Loss: 0.7888
Training Epoch: 27 [23552/50048]	Loss: 0.6173
Training Epoch: 27 [23680/50048]	Loss: 0.6508
Training Epoch: 27 [23808/50048]	Loss: 0.6355
Training Epoch: 27 [23936/50048]	Loss: 0.6826
Training Epoch: 27 [24064/50048]	Loss: 0.7733
Training Epoch: 27 [24192/50048]	Loss: 0.5818
Training Epoch: 27 [24320/50048]	Loss: 0.7342
Training Epoch: 27 [24448/50048]	Loss: 0.7170
Training Epoch: 27 [24576/50048]	Loss: 0.8385
Training Epoch: 27 [24704/50048]	Loss: 0.6523
Training Epoch: 27 [24832/50048]	Loss: 0.6780
Training Epoch: 27 [24960/50048]	Loss: 0.8202
Training Epoch: 27 [25088/50048]	Loss: 0.7180
Training Epoch: 27 [25216/50048]	Loss: 0.8368
Training Epoch: 27 [25344/50048]	Loss: 0.6282
Training Epoch: 27 [25472/50048]	Loss: 0.8446
Training Epoch: 27 [25600/50048]	Loss: 0.7142
Training Epoch: 27 [25728/50048]	Loss: 0.6584
Training Epoch: 27 [25856/50048]	Loss: 0.8251
Training Epoch: 27 [25984/50048]	Loss: 0.8799
Training Epoch: 27 [26112/50048]	Loss: 0.7314
Training Epoch: 27 [26240/50048]	Loss: 0.6112
Training Epoch: 27 [26368/50048]	Loss: 0.8253
Training Epoch: 27 [26496/50048]	Loss: 0.6792
Training Epoch: 27 [26624/50048]	Loss: 0.7608
Training Epoch: 27 [26752/50048]	Loss: 0.8806
Training Epoch: 27 [26880/50048]	Loss: 0.7177
Training Epoch: 27 [27008/50048]	Loss: 0.7380
Training Epoch: 27 [27136/50048]	Loss: 0.6923
Training Epoch: 27 [27264/50048]	Loss: 0.8220
Training Epoch: 27 [27392/50048]	Loss: 0.6643
Training Epoch: 27 [27520/50048]	Loss: 0.6929
Training Epoch: 27 [27648/50048]	Loss: 0.7935
Training Epoch: 27 [27776/50048]	Loss: 0.7065
Training Epoch: 27 [27904/50048]	Loss: 0.8511
Training Epoch: 27 [28032/50048]	Loss: 0.8853
Training Epoch: 27 [28160/50048]	Loss: 0.7335
Training Epoch: 27 [28288/50048]	Loss: 0.5693
Training Epoch: 27 [28416/50048]	Loss: 0.8371
Training Epoch: 27 [28544/50048]	Loss: 0.8317
Training Epoch: 27 [28672/50048]	Loss: 0.6251
Training Epoch: 27 [28800/50048]	Loss: 0.6071
Training Epoch: 27 [28928/50048]	Loss: 0.6279
Training Epoch: 27 [29056/50048]	Loss: 0.6898
Training Epoch: 27 [29184/50048]	Loss: 0.7960
Training Epoch: 27 [29312/50048]	Loss: 0.8803
Training Epoch: 27 [29440/50048]	Loss: 0.7707
Training Epoch: 27 [29568/50048]	Loss: 0.9976
Training Epoch: 27 [29696/50048]	Loss: 0.6739
Training Epoch: 27 [29824/50048]	Loss: 0.7227
Training Epoch: 27 [29952/50048]	Loss: 0.8682
Training Epoch: 27 [30080/50048]	Loss: 0.5091
Training Epoch: 27 [30208/50048]	Loss: 0.8832
Training Epoch: 27 [30336/50048]	Loss: 0.7568
Training Epoch: 27 [30464/50048]	Loss: 0.5486
Training Epoch: 27 [30592/50048]	Loss: 0.6614
Training Epoch: 27 [30720/50048]	Loss: 0.5092
Training Epoch: 27 [30848/50048]	Loss: 0.8233
Training Epoch: 27 [30976/50048]	Loss: 1.0421
Training Epoch: 27 [31104/50048]	Loss: 0.7435
Training Epoch: 27 [31232/50048]	Loss: 0.7120
Training Epoch: 27 [31360/50048]	Loss: 0.7996
Training Epoch: 27 [31488/50048]	Loss: 0.7197
Training Epoch: 27 [31616/50048]	Loss: 0.7995
Training Epoch: 27 [31744/50048]	Loss: 0.6902
Training Epoch: 27 [31872/50048]	Loss: 0.6477
Training Epoch: 27 [32000/50048]	Loss: 0.7459
Training Epoch: 27 [32128/50048]	Loss: 0.8133
Training Epoch: 27 [32256/50048]	Loss: 0.7881
Training Epoch: 27 [32384/50048]	Loss: 0.7298
Training Epoch: 27 [32512/50048]	Loss: 1.0068
Training Epoch: 27 [32640/50048]	Loss: 0.5402
Training Epoch: 27 [32768/50048]	Loss: 0.7758
Training Epoch: 27 [32896/50048]	Loss: 0.6145
Training Epoch: 27 [33024/50048]	Loss: 0.7510
Training Epoch: 27 [33152/50048]	Loss: 0.7325
Training Epoch: 27 [33280/50048]	Loss: 0.8629
Training Epoch: 27 [33408/50048]	Loss: 0.6043
Training Epoch: 27 [33536/50048]	Loss: 0.6051
Training Epoch: 27 [33664/50048]	Loss: 0.6768
Training Epoch: 27 [33792/50048]	Loss: 0.8517
Training Epoch: 27 [33920/50048]	Loss: 0.7466
Training Epoch: 27 [34048/50048]	Loss: 0.8559
Training Epoch: 27 [34176/50048]	Loss: 0.8841
Training Epoch: 27 [34304/50048]	Loss: 0.6966
Training Epoch: 27 [34432/50048]	Loss: 0.8081
Training Epoch: 27 [34560/50048]	Loss: 0.6310
Training Epoch: 27 [34688/50048]	Loss: 0.9415
Training Epoch: 27 [34816/50048]	Loss: 0.9257
Training Epoch: 27 [34944/50048]	Loss: 0.8383
Training Epoch: 27 [35072/50048]	Loss: 0.7005
Training Epoch: 27 [35200/50048]	Loss: 0.8529
Training Epoch: 27 [35328/50048]	Loss: 0.7918
Training Epoch: 27 [35456/50048]	Loss: 0.7472
Training Epoch: 27 [35584/50048]	Loss: 1.1028
Training Epoch: 27 [35712/50048]	Loss: 0.7942
Training Epoch: 27 [35840/50048]	Loss: 0.6813
Training Epoch: 27 [35968/50048]	Loss: 0.5867
Training Epoch: 27 [36096/50048]	Loss: 0.5510
Training Epoch: 27 [36224/50048]	Loss: 0.7410
Training Epoch: 27 [36352/50048]	Loss: 0.7430
Training Epoch: 27 [36480/50048]	Loss: 0.8029
Training Epoch: 27 [36608/50048]	Loss: 0.7676
Training Epoch: 27 [36736/50048]	Loss: 0.8637
Training Epoch: 27 [36864/50048]	Loss: 0.7368
Training Epoch: 27 [36992/50048]	Loss: 0.8554
Training Epoch: 27 [37120/50048]	Loss: 0.8310
Training Epoch: 27 [37248/50048]	Loss: 0.5852
Training Epoch: 27 [37376/50048]	Loss: 0.7544
Training Epoch: 27 [37504/50048]	Loss: 0.5776
Training Epoch: 27 [37632/50048]	Loss: 0.8360
Training Epoch: 27 [37760/50048]	Loss: 0.8634
Training Epoch: 27 [37888/50048]	Loss: 0.6861
Training Epoch: 27 [38016/50048]	Loss: 0.7408
Training Epoch: 27 [38144/50048]	Loss: 0.7528
Training Epoch: 27 [38272/50048]	Loss: 0.6905
Training Epoch: 27 [38400/50048]	Loss: 0.8472
Training Epoch: 27 [38528/50048]	Loss: 0.6725
Training Epoch: 27 [38656/50048]	Loss: 0.7449
Training Epoch: 27 [38784/50048]	Loss: 0.7157
Training Epoch: 27 [38912/50048]	Loss: 0.8165
Training Epoch: 27 [39040/50048]	Loss: 0.8078
Training Epoch: 27 [39168/50048]	Loss: 0.6643
Training Epoch: 27 [39296/50048]	Loss: 0.8035
Training Epoch: 27 [39424/50048]	Loss: 0.7824
Training Epoch: 27 [39552/50048]	Loss: 0.8071
Training Epoch: 27 [39680/50048]	Loss: 0.9649
Training Epoch: 27 [39808/50048]	Loss: 0.6617
Training Epoch: 27 [39936/50048]	Loss: 0.6989
Training Epoch: 27 [40064/50048]	Loss: 0.7289
Training Epoch: 27 [40192/50048]	Loss: 0.6868
Training Epoch: 27 [40320/50048]	Loss: 0.8628
Training Epoch: 27 [40448/50048]	Loss: 0.7555
Training Epoch: 27 [40576/50048]	Loss: 0.7466
Training Epoch: 27 [40704/50048]	Loss: 0.8887
Training Epoch: 27 [40832/50048]	Loss: 0.8231
Training Epoch: 27 [40960/50048]	Loss: 0.6224
Training Epoch: 27 [41088/50048]	Loss: 0.8016
Training Epoch: 27 [41216/50048]	Loss: 0.6754
Training Epoch: 27 [41344/50048]	Loss: 0.7873
Training Epoch: 27 [41472/50048]	Loss: 0.6705
Training Epoch: 27 [41600/50048]	Loss: 0.5904
Training Epoch: 27 [41728/50048]	Loss: 0.6456
Training Epoch: 27 [41856/50048]	Loss: 0.8773
Training Epoch: 27 [41984/50048]	Loss: 0.6830
Training Epoch: 27 [42112/50048]	Loss: 0.7388
Training Epoch: 27 [42240/50048]	Loss: 0.7787
Training Epoch: 27 [42368/50048]	Loss: 0.9355
Training Epoch: 27 [42496/50048]	Loss: 0.6250
Training Epoch: 27 [42624/50048]	Loss: 0.7176
Training Epoch: 27 [42752/50048]	Loss: 0.5903
Training Epoch: 27 [42880/50048]	Loss: 0.7099
Training Epoch: 27 [43008/50048]	Loss: 0.6652
Training Epoch: 27 [43136/50048]	Loss: 0.6619
Training Epoch: 27 [43264/50048]	Loss: 0.6857
Training Epoch: 27 [43392/50048]	Loss: 0.7501
Training Epoch: 27 [43520/50048]	Loss: 0.8864
Training Epoch: 27 [43648/50048]	Loss: 0.7715
Training Epoch: 27 [43776/50048]	Loss: 0.8622
Training Epoch: 27 [43904/50048]	Loss: 0.7978
Training Epoch: 27 [44032/50048]	Loss: 0.6648
Training Epoch: 27 [44160/50048]	Loss: 0.7395
Training Epoch: 27 [44288/50048]	Loss: 0.8554
Training Epoch: 27 [44416/50048]	Loss: 0.5582
Training Epoch: 27 [44544/50048]	Loss: 0.8269
Training Epoch: 27 [44672/50048]	Loss: 0.8334
Training Epoch: 27 [44800/50048]	Loss: 0.6046
Training Epoch: 27 [44928/50048]	Loss: 0.8055
Training Epoch: 27 [45056/50048]	Loss: 0.6675
Training Epoch: 27 [45184/50048]	Loss: 0.7281
Training Epoch: 27 [45312/50048]	Loss: 0.7163
Training Epoch: 27 [45440/50048]	Loss: 0.5729
Training Epoch: 27 [45568/50048]	Loss: 0.7223
Training Epoch: 27 [45696/50048]	Loss: 0.8401
2022-12-06 04:19:27,267 [ZeusDataLoader(train)] train epoch 28 done: time=86.53 energy=10521.63
2022-12-06 04:19:27,269 [ZeusDataLoader(eval)] Epoch 28 begin.
Training Epoch: 27 [45824/50048]	Loss: 0.9639
Training Epoch: 27 [45952/50048]	Loss: 0.8857
Training Epoch: 27 [46080/50048]	Loss: 0.8880
Training Epoch: 27 [46208/50048]	Loss: 1.0529
Training Epoch: 27 [46336/50048]	Loss: 0.7579
Training Epoch: 27 [46464/50048]	Loss: 0.9730
Training Epoch: 27 [46592/50048]	Loss: 0.9150
Training Epoch: 27 [46720/50048]	Loss: 0.6162
Training Epoch: 27 [46848/50048]	Loss: 0.8414
Training Epoch: 27 [46976/50048]	Loss: 0.8125
Training Epoch: 27 [47104/50048]	Loss: 0.8274
Training Epoch: 27 [47232/50048]	Loss: 0.8412
Training Epoch: 27 [47360/50048]	Loss: 0.6516
Training Epoch: 27 [47488/50048]	Loss: 0.9519
Training Epoch: 27 [47616/50048]	Loss: 0.6562
Training Epoch: 27 [47744/50048]	Loss: 0.6878
Training Epoch: 27 [47872/50048]	Loss: 0.8957
Training Epoch: 27 [48000/50048]	Loss: 0.7373
Training Epoch: 27 [48128/50048]	Loss: 0.6024
Training Epoch: 27 [48256/50048]	Loss: 0.7486
Training Epoch: 27 [48384/50048]	Loss: 0.9573
Training Epoch: 27 [48512/50048]	Loss: 0.8995
Training Epoch: 27 [48640/50048]	Loss: 0.7099
Training Epoch: 27 [48768/50048]	Loss: 0.6947
Training Epoch: 27 [48896/50048]	Loss: 0.9716
Training Epoch: 27 [49024/50048]	Loss: 0.6924
Training Epoch: 27 [49152/50048]	Loss: 0.8135
Training Epoch: 27 [49280/50048]	Loss: 0.7622
Training Epoch: 27 [49408/50048]	Loss: 0.8874
Training Epoch: 27 [49536/50048]	Loss: 0.6794
Training Epoch: 27 [49664/50048]	Loss: 0.7106
Training Epoch: 27 [49792/50048]	Loss: 0.7588
Training Epoch: 27 [49920/50048]	Loss: 0.8001
Training Epoch: 27 [50048/50048]	Loss: 1.0148
2022-12-06 09:19:30.939 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:19:30,978 [ZeusDataLoader(eval)] eval epoch 28 done: time=3.70 energy=452.80
2022-12-06 04:19:30,979 [ZeusDataLoader(train)] Up to epoch 28: time=2528.15, energy=306727.01, cost=374576.45
2022-12-06 04:19:30,979 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:19:30,979 [ZeusDataLoader(train)] Expected next epoch: time=2617.95, energy=317525.02, cost=387832.84
2022-12-06 04:19:30,980 [ZeusDataLoader(train)] Epoch 29 begin.
Validation Epoch: 27, Average loss: 0.0118, Accuracy: 0.6266
2022-12-06 04:19:31,159 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:19:31,160 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:19:31.162 [ZeusMonitor] Monitor started.
2022-12-06 09:19:31.162 [ZeusMonitor] Running indefinitely. 2022-12-06 09:19:31.162 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:19:31.162 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e29+gpu0.power.log
Training Epoch: 28 [128/50048]	Loss: 0.6478
Training Epoch: 28 [256/50048]	Loss: 0.5722
Training Epoch: 28 [384/50048]	Loss: 0.5566
Training Epoch: 28 [512/50048]	Loss: 0.7402
Training Epoch: 28 [640/50048]	Loss: 0.5295
Training Epoch: 28 [768/50048]	Loss: 0.6435
Training Epoch: 28 [896/50048]	Loss: 0.4271
Training Epoch: 28 [1024/50048]	Loss: 0.6199
Training Epoch: 28 [1152/50048]	Loss: 0.7068
Training Epoch: 28 [1280/50048]	Loss: 0.7439
Training Epoch: 28 [1408/50048]	Loss: 0.5877
Training Epoch: 28 [1536/50048]	Loss: 0.6336
Training Epoch: 28 [1664/50048]	Loss: 0.6499
Training Epoch: 28 [1792/50048]	Loss: 0.6408
Training Epoch: 28 [1920/50048]	Loss: 0.6307
Training Epoch: 28 [2048/50048]	Loss: 0.8763
Training Epoch: 28 [2176/50048]	Loss: 0.6463
Training Epoch: 28 [2304/50048]	Loss: 0.6850
Training Epoch: 28 [2432/50048]	Loss: 0.6159
Training Epoch: 28 [2560/50048]	Loss: 0.7123
Training Epoch: 28 [2688/50048]	Loss: 0.7044
Training Epoch: 28 [2816/50048]	Loss: 0.6098
Training Epoch: 28 [2944/50048]	Loss: 0.7851
Training Epoch: 28 [3072/50048]	Loss: 0.5897
Training Epoch: 28 [3200/50048]	Loss: 0.5508
Training Epoch: 28 [3328/50048]	Loss: 0.5077
Training Epoch: 28 [3456/50048]	Loss: 0.7334
Training Epoch: 28 [3584/50048]	Loss: 0.5557
Training Epoch: 28 [3712/50048]	Loss: 0.5839
Training Epoch: 28 [3840/50048]	Loss: 0.9132
Training Epoch: 28 [3968/50048]	Loss: 0.6351
Training Epoch: 28 [4096/50048]	Loss: 0.9286
Training Epoch: 28 [4224/50048]	Loss: 0.7608
Training Epoch: 28 [4352/50048]	Loss: 0.7183
Training Epoch: 28 [4480/50048]	Loss: 0.5376
Training Epoch: 28 [4608/50048]	Loss: 0.6980
Training Epoch: 28 [4736/50048]	Loss: 0.5086
Training Epoch: 28 [4864/50048]	Loss: 0.6308
Training Epoch: 28 [4992/50048]	Loss: 0.6582
Training Epoch: 28 [5120/50048]	Loss: 0.6479
Training Epoch: 28 [5248/50048]	Loss: 0.5205
Training Epoch: 28 [5376/50048]	Loss: 0.5497
Training Epoch: 28 [5504/50048]	Loss: 0.7150
Training Epoch: 28 [5632/50048]	Loss: 0.7334
Training Epoch: 28 [5760/50048]	Loss: 0.6798
Training Epoch: 28 [5888/50048]	Loss: 0.8743
Training Epoch: 28 [6016/50048]	Loss: 0.5971
Training Epoch: 28 [6144/50048]	Loss: 0.6774
Training Epoch: 28 [6272/50048]	Loss: 0.9298
Training Epoch: 28 [6400/50048]	Loss: 0.6536
Training Epoch: 28 [6528/50048]	Loss: 0.5377
Training Epoch: 28 [6656/50048]	Loss: 0.6742
Training Epoch: 28 [6784/50048]	Loss: 0.5633
Training Epoch: 28 [6912/50048]	Loss: 0.5589
Training Epoch: 28 [7040/50048]	Loss: 0.6743
Training Epoch: 28 [7168/50048]	Loss: 0.6593
Training Epoch: 28 [7296/50048]	Loss: 0.8113
Training Epoch: 28 [7424/50048]	Loss: 0.6477
Training Epoch: 28 [7552/50048]	Loss: 0.7253
Training Epoch: 28 [7680/50048]	Loss: 0.5748
Training Epoch: 28 [7808/50048]	Loss: 0.5307
Training Epoch: 28 [7936/50048]	Loss: 0.7945
Training Epoch: 28 [8064/50048]	Loss: 0.8665
Training Epoch: 28 [8192/50048]	Loss: 0.8248
Training Epoch: 28 [8320/50048]	Loss: 0.4879
Training Epoch: 28 [8448/50048]	Loss: 0.5732
Training Epoch: 28 [8576/50048]	Loss: 0.5507
Training Epoch: 28 [8704/50048]	Loss: 0.7562
Training Epoch: 28 [8832/50048]	Loss: 0.6362
Training Epoch: 28 [8960/50048]	Loss: 0.6601
Training Epoch: 28 [9088/50048]	Loss: 0.8076
Training Epoch: 28 [9216/50048]	Loss: 0.7733
Training Epoch: 28 [9344/50048]	Loss: 0.7031
Training Epoch: 28 [9472/50048]	Loss: 0.8822
Training Epoch: 28 [9600/50048]	Loss: 0.5099
Training Epoch: 28 [9728/50048]	Loss: 0.5825
Training Epoch: 28 [9856/50048]	Loss: 0.8584
Training Epoch: 28 [9984/50048]	Loss: 0.5566
Training Epoch: 28 [10112/50048]	Loss: 0.6225
Training Epoch: 28 [10240/50048]	Loss: 0.5358
Training Epoch: 28 [10368/50048]	Loss: 0.4663
Training Epoch: 28 [10496/50048]	Loss: 0.6491
Training Epoch: 28 [10624/50048]	Loss: 0.5502
Training Epoch: 28 [10752/50048]	Loss: 0.5789
Training Epoch: 28 [10880/50048]	Loss: 0.7898
Training Epoch: 28 [11008/50048]	Loss: 0.6277
Training Epoch: 28 [11136/50048]	Loss: 0.7856
Training Epoch: 28 [11264/50048]	Loss: 0.6436
Training Epoch: 28 [11392/50048]	Loss: 0.6650
Training Epoch: 28 [11520/50048]	Loss: 0.6466
Training Epoch: 28 [11648/50048]	Loss: 0.8327
Training Epoch: 28 [11776/50048]	Loss: 0.6934
Training Epoch: 28 [11904/50048]	Loss: 0.8002
Training Epoch: 28 [12032/50048]	Loss: 0.7380
Training Epoch: 28 [12160/50048]	Loss: 0.6071
Training Epoch: 28 [12288/50048]	Loss: 0.6566
Training Epoch: 28 [12416/50048]	Loss: 0.7320
Training Epoch: 28 [12544/50048]	Loss: 0.7266
Training Epoch: 28 [12672/50048]	Loss: 0.6713
Training Epoch: 28 [12800/50048]	Loss: 0.8748
Training Epoch: 28 [12928/50048]	Loss: 0.6006
Training Epoch: 28 [13056/50048]	Loss: 0.5031
Training Epoch: 28 [13184/50048]	Loss: 0.5759
Training Epoch: 28 [13312/50048]	Loss: 0.7391
Training Epoch: 28 [13440/50048]	Loss: 0.7157
Training Epoch: 28 [13568/50048]	Loss: 0.7429
Training Epoch: 28 [13696/50048]	Loss: 0.8227
Training Epoch: 28 [13824/50048]	Loss: 0.7149
Training Epoch: 28 [13952/50048]	Loss: 0.7415
Training Epoch: 28 [14080/50048]	Loss: 0.5688
Training Epoch: 28 [14208/50048]	Loss: 0.6781
Training Epoch: 28 [14336/50048]	Loss: 0.7727
Training Epoch: 28 [14464/50048]	Loss: 0.6589
Training Epoch: 28 [14592/50048]	Loss: 0.6495
Training Epoch: 28 [14720/50048]	Loss: 0.6659
Training Epoch: 28 [14848/50048]	Loss: 0.6467
Training Epoch: 28 [14976/50048]	Loss: 0.6373
Training Epoch: 28 [15104/50048]	Loss: 0.8747
Training Epoch: 28 [15232/50048]	Loss: 0.7619
Training Epoch: 28 [15360/50048]	Loss: 0.5517
Training Epoch: 28 [15488/50048]	Loss: 0.5581
Training Epoch: 28 [15616/50048]	Loss: 0.7986
Training Epoch: 28 [15744/50048]	Loss: 0.5562
Training Epoch: 28 [15872/50048]	Loss: 0.8910
Training Epoch: 28 [16000/50048]	Loss: 0.6956
Training Epoch: 28 [16128/50048]	Loss: 0.6084
Training Epoch: 28 [16256/50048]	Loss: 0.6316
Training Epoch: 28 [16384/50048]	Loss: 0.7901
Training Epoch: 28 [16512/50048]	Loss: 0.7730
Training Epoch: 28 [16640/50048]	Loss: 0.8217
Training Epoch: 28 [16768/50048]	Loss: 0.4386
Training Epoch: 28 [16896/50048]	Loss: 0.8398
Training Epoch: 28 [17024/50048]	Loss: 0.6524
Training Epoch: 28 [17152/50048]	Loss: 0.9302
Training Epoch: 28 [17280/50048]	Loss: 0.6460
Training Epoch: 28 [17408/50048]	Loss: 0.6942
Training Epoch: 28 [17536/50048]	Loss: 0.7085
Training Epoch: 28 [17664/50048]	Loss: 0.9196
Training Epoch: 28 [17792/50048]	Loss: 0.4423
Training Epoch: 28 [17920/50048]	Loss: 0.6807
Training Epoch: 28 [18048/50048]	Loss: 0.8247
Training Epoch: 28 [18176/50048]	Loss: 0.6463
Training Epoch: 28 [18304/50048]	Loss: 0.7130
Training Epoch: 28 [18432/50048]	Loss: 0.7666
Training Epoch: 28 [18560/50048]	Loss: 0.8086
Training Epoch: 28 [18688/50048]	Loss: 0.7558
Training Epoch: 28 [18816/50048]	Loss: 0.7028
Training Epoch: 28 [18944/50048]	Loss: 0.8989
Training Epoch: 28 [19072/50048]	Loss: 0.6445
Training Epoch: 28 [19200/50048]	Loss: 0.5859
Training Epoch: 28 [19328/50048]	Loss: 0.8179
Training Epoch: 28 [19456/50048]	Loss: 0.7699
Training Epoch: 28 [19584/50048]	Loss: 0.7769
Training Epoch: 28 [19712/50048]	Loss: 0.6718
Training Epoch: 28 [19840/50048]	Loss: 0.5676
Training Epoch: 28 [19968/50048]	Loss: 0.7250
Training Epoch: 28 [20096/50048]	Loss: 0.5638
Training Epoch: 28 [20224/50048]	Loss: 0.7393
Training Epoch: 28 [20352/50048]	Loss: 0.7422
Training Epoch: 28 [20480/50048]	Loss: 0.6985
Training Epoch: 28 [20608/50048]	Loss: 0.6724
Training Epoch: 28 [20736/50048]	Loss: 0.8585
Training Epoch: 28 [20864/50048]	Loss: 0.7595
Training Epoch: 28 [20992/50048]	Loss: 0.7336
Training Epoch: 28 [21120/50048]	Loss: 0.8678
Training Epoch: 28 [21248/50048]	Loss: 0.7467
Training Epoch: 28 [21376/50048]	Loss: 0.5132
Training Epoch: 28 [21504/50048]	Loss: 0.6273
Training Epoch: 28 [21632/50048]	Loss: 0.8959
Training Epoch: 28 [21760/50048]	Loss: 0.7485
Training Epoch: 28 [21888/50048]	Loss: 0.4933
Training Epoch: 28 [22016/50048]	Loss: 0.6979
Training Epoch: 28 [22144/50048]	Loss: 0.7696
Training Epoch: 28 [22272/50048]	Loss: 0.6653
Training Epoch: 28 [22400/50048]	Loss: 0.6397
Training Epoch: 28 [22528/50048]	Loss: 0.6771
Training Epoch: 28 [22656/50048]	Loss: 0.6166
Training Epoch: 28 [22784/50048]	Loss: 0.7745
Training Epoch: 28 [22912/50048]	Loss: 1.0865
Training Epoch: 28 [23040/50048]	Loss: 0.6561
Training Epoch: 28 [23168/50048]	Loss: 0.7355
Training Epoch: 28 [23296/50048]	Loss: 0.7540
Training Epoch: 28 [23424/50048]	Loss: 0.6964
Training Epoch: 28 [23552/50048]	Loss: 0.8166
Training Epoch: 28 [23680/50048]	Loss: 0.5569
Training Epoch: 28 [23808/50048]	Loss: 0.5483
Training Epoch: 28 [23936/50048]	Loss: 0.6522
Training Epoch: 28 [24064/50048]	Loss: 0.7575
Training Epoch: 28 [24192/50048]	Loss: 0.5403
Training Epoch: 28 [24320/50048]	Loss: 0.6613
Training Epoch: 28 [24448/50048]	Loss: 0.7694
Training Epoch: 28 [24576/50048]	Loss: 0.6321
Training Epoch: 28 [24704/50048]	Loss: 0.7796
Training Epoch: 28 [24832/50048]	Loss: 0.8442
Training Epoch: 28 [24960/50048]	Loss: 0.7670
Training Epoch: 28 [25088/50048]	Loss: 0.7901
Training Epoch: 28 [25216/50048]	Loss: 0.7898
Training Epoch: 28 [25344/50048]	Loss: 0.6713
Training Epoch: 28 [25472/50048]	Loss: 0.5031
Training Epoch: 28 [25600/50048]	Loss: 0.7462
Training Epoch: 28 [25728/50048]	Loss: 0.7998
Training Epoch: 28 [25856/50048]	Loss: 0.5810
Training Epoch: 28 [25984/50048]	Loss: 0.6479
Training Epoch: 28 [26112/50048]	Loss: 0.7020
Training Epoch: 28 [26240/50048]	Loss: 0.6563
Training Epoch: 28 [26368/50048]	Loss: 0.6685
Training Epoch: 28 [26496/50048]	Loss: 0.6818
Training Epoch: 28 [26624/50048]	Loss: 0.7545
Training Epoch: 28 [26752/50048]	Loss: 0.7426
Training Epoch: 28 [26880/50048]	Loss: 0.6963
Training Epoch: 28 [27008/50048]	Loss: 0.7188
Training Epoch: 28 [27136/50048]	Loss: 0.6370
Training Epoch: 28 [27264/50048]	Loss: 0.6990
Training Epoch: 28 [27392/50048]	Loss: 0.9271
Training Epoch: 28 [27520/50048]	Loss: 0.6525
Training Epoch: 28 [27648/50048]	Loss: 0.6499
Training Epoch: 28 [27776/50048]	Loss: 0.9143
Training Epoch: 28 [27904/50048]	Loss: 0.7247
Training Epoch: 28 [28032/50048]	Loss: 0.5762
Training Epoch: 28 [28160/50048]	Loss: 0.7016
Training Epoch: 28 [28288/50048]	Loss: 0.7172
Training Epoch: 28 [28416/50048]	Loss: 0.6507
Training Epoch: 28 [28544/50048]	Loss: 0.7632
Training Epoch: 28 [28672/50048]	Loss: 0.6748
Training Epoch: 28 [28800/50048]	Loss: 0.6308
Training Epoch: 28 [28928/50048]	Loss: 0.6725
Training Epoch: 28 [29056/50048]	Loss: 0.6341
Training Epoch: 28 [29184/50048]	Loss: 0.7274
Training Epoch: 28 [29312/50048]	Loss: 0.5050
Training Epoch: 28 [29440/50048]	Loss: 0.8085
Training Epoch: 28 [29568/50048]	Loss: 0.5677
Training Epoch: 28 [29696/50048]	Loss: 0.8204
Training Epoch: 28 [29824/50048]	Loss: 0.6046
Training Epoch: 28 [29952/50048]	Loss: 0.8509
Training Epoch: 28 [30080/50048]	Loss: 0.7530
Training Epoch: 28 [30208/50048]	Loss: 0.7143
Training Epoch: 28 [30336/50048]	Loss: 0.6714
Training Epoch: 28 [30464/50048]	Loss: 0.7087
Training Epoch: 28 [30592/50048]	Loss: 0.8264
Training Epoch: 28 [30720/50048]	Loss: 0.5914
Training Epoch: 28 [30848/50048]	Loss: 0.8310
Training Epoch: 28 [30976/50048]	Loss: 0.6178
Training Epoch: 28 [31104/50048]	Loss: 0.7288
Training Epoch: 28 [31232/50048]	Loss: 0.6442
Training Epoch: 28 [31360/50048]	Loss: 0.7318
Training Epoch: 28 [31488/50048]	Loss: 0.7433
Training Epoch: 28 [31616/50048]	Loss: 0.6360
Training Epoch: 28 [31744/50048]	Loss: 0.7589
Training Epoch: 28 [31872/50048]	Loss: 0.7115
Training Epoch: 28 [32000/50048]	Loss: 0.7565
Training Epoch: 28 [32128/50048]	Loss: 0.8006
Training Epoch: 28 [32256/50048]	Loss: 0.6576
Training Epoch: 28 [32384/50048]	Loss: 0.7487
Training Epoch: 28 [32512/50048]	Loss: 0.5532
Training Epoch: 28 [32640/50048]	Loss: 0.8384
Training Epoch: 28 [32768/50048]	Loss: 0.5442
Training Epoch: 28 [32896/50048]	Loss: 0.5329
Training Epoch: 28 [33024/50048]	Loss: 0.7493
Training Epoch: 28 [33152/50048]	Loss: 0.7584
Training Epoch: 28 [33280/50048]	Loss: 0.7165
Training Epoch: 28 [33408/50048]	Loss: 0.6725
Training Epoch: 28 [33536/50048]	Loss: 0.7362
Training Epoch: 28 [33664/50048]	Loss: 0.6526
Training Epoch: 28 [33792/50048]	Loss: 0.5453
Training Epoch: 28 [33920/50048]	Loss: 0.8335
Training Epoch: 28 [34048/50048]	Loss: 0.5546
Training Epoch: 28 [34176/50048]	Loss: 0.7794
Training Epoch: 28 [34304/50048]	Loss: 0.8832
Training Epoch: 28 [34432/50048]	Loss: 0.6047
Training Epoch: 28 [34560/50048]	Loss: 0.7729
Training Epoch: 28 [34688/50048]	Loss: 0.7200
Training Epoch: 28 [34816/50048]	Loss: 0.7114
Training Epoch: 28 [34944/50048]	Loss: 0.9224
Training Epoch: 28 [35072/50048]	Loss: 0.8260
Training Epoch: 28 [35200/50048]	Loss: 0.4981
Training Epoch: 28 [35328/50048]	Loss: 0.5225
Training Epoch: 28 [35456/50048]	Loss: 0.7183
Training Epoch: 28 [35584/50048]	Loss: 0.9040
Training Epoch: 28 [35712/50048]	Loss: 0.7475
Training Epoch: 28 [35840/50048]	Loss: 0.6464
Training Epoch: 28 [35968/50048]	Loss: 0.7238
Training Epoch: 28 [36096/50048]	Loss: 0.6349
Training Epoch: 28 [36224/50048]	Loss: 0.9072
Training Epoch: 28 [36352/50048]	Loss: 0.8411
Training Epoch: 28 [36480/50048]	Loss: 0.7607
Training Epoch: 28 [36608/50048]	Loss: 0.7838
Training Epoch: 28 [36736/50048]	Loss: 0.8379
Training Epoch: 28 [36864/50048]	Loss: 0.8148
Training Epoch: 28 [36992/50048]	Loss: 0.6816
Training Epoch: 28 [37120/50048]	Loss: 0.8014
Training Epoch: 28 [37248/50048]	Loss: 0.7416
Training Epoch: 28 [37376/50048]	Loss: 0.8252
Training Epoch: 28 [37504/50048]	Loss: 0.6362
Training Epoch: 28 [37632/50048]	Loss: 0.6456
Training Epoch: 28 [37760/50048]	Loss: 0.5853
Training Epoch: 28 [37888/50048]	Loss: 1.0716
Training Epoch: 28 [38016/50048]	Loss: 0.7209
Training Epoch: 28 [38144/50048]	Loss: 0.8417
Training Epoch: 28 [38272/50048]	Loss: 0.7549
Training Epoch: 28 [38400/50048]	Loss: 0.5831
Training Epoch: 28 [38528/50048]	Loss: 0.8134
Training Epoch: 28 [38656/50048]	Loss: 0.7131
Training Epoch: 28 [38784/50048]	Loss: 0.7552
Training Epoch: 28 [38912/50048]	Loss: 0.8854
Training Epoch: 28 [39040/50048]	Loss: 0.6765
Training Epoch: 28 [39168/50048]	Loss: 0.7188
Training Epoch: 28 [39296/50048]	Loss: 0.8118
Training Epoch: 28 [39424/50048]	Loss: 0.8103
Training Epoch: 28 [39552/50048]	Loss: 0.7498
Training Epoch: 28 [39680/50048]	Loss: 0.7155
Training Epoch: 28 [39808/50048]	Loss: 0.6898
Training Epoch: 28 [39936/50048]	Loss: 1.0117
Training Epoch: 28 [40064/50048]	Loss: 0.9020
Training Epoch: 28 [40192/50048]	Loss: 0.6269
Training Epoch: 28 [40320/50048]	Loss: 0.8716
Training Epoch: 28 [40448/50048]	Loss: 0.6791
Training Epoch: 28 [40576/50048]	Loss: 0.9009
Training Epoch: 28 [40704/50048]	Loss: 0.8219
Training Epoch: 28 [40832/50048]	Loss: 1.0400
Training Epoch: 28 [40960/50048]	Loss: 0.5833
Training Epoch: 28 [41088/50048]	Loss: 0.7648
Training Epoch: 28 [41216/50048]	Loss: 0.9738
Training Epoch: 28 [41344/50048]	Loss: 0.7890
Training Epoch: 28 [41472/50048]	Loss: 0.6435
Training Epoch: 28 [41600/50048]	Loss: 0.7313
Training Epoch: 28 [41728/50048]	Loss: 0.7693
Training Epoch: 28 [41856/50048]	Loss: 0.5892
Training Epoch: 28 [41984/50048]	Loss: 0.6144
Training Epoch: 28 [42112/50048]	Loss: 0.6357
Training Epoch: 28 [42240/50048]	Loss: 0.6122
Training Epoch: 28 [42368/50048]	Loss: 0.8394
Training Epoch: 28 [42496/50048]	Loss: 0.5858
Training Epoch: 28 [42624/50048]	Loss: 0.8072
Training Epoch: 28 [42752/50048]	Loss: 0.6551
Training Epoch: 28 [42880/50048]	Loss: 0.7697
Training Epoch: 28 [43008/50048]	Loss: 0.6426
Training Epoch: 28 [43136/50048]	Loss: 0.6489
Training Epoch: 28 [43264/50048]	Loss: 0.4899
Training Epoch: 28 [43392/50048]	Loss: 0.7339
Training Epoch: 28 [43520/50048]	Loss: 0.7386
Training Epoch: 28 [43648/50048]	Loss: 0.8435
Training Epoch: 28 [43776/50048]	Loss: 0.7667
Training Epoch: 28 [43904/50048]	Loss: 0.7185
Training Epoch: 28 [44032/50048]	Loss: 0.8133
Training Epoch: 28 [44160/50048]	Loss: 0.7135
Training Epoch: 28 [44288/50048]	Loss: 0.9496
Training Epoch: 28 [44416/50048]	Loss: 0.7771
Training Epoch: 28 [44544/50048]	Loss: 0.8719
Training Epoch: 28 [44672/50048]	Loss: 0.8346
Training Epoch: 28 [44800/50048]	Loss: 0.7307
Training Epoch: 28 [44928/50048]	Loss: 0.9311
Training Epoch: 28 [45056/50048]	Loss: 0.9287
Training Epoch: 28 [45184/50048]	Loss: 0.7727
Training Epoch: 28 [45312/50048]	Loss: 0.7133
Training Epoch: 28 [45440/50048]	Loss: 0.7810
Training Epoch: 28 [45568/50048]	Loss: 0.7334
Training Epoch: 28 [45696/50048]	Loss: 0.5835
2022-12-06 04:20:57,499 [ZeusDataLoader(train)] train epoch 29 done: time=86.51 energy=10518.10
2022-12-06 04:20:57,500 [ZeusDataLoader(eval)] Epoch 29 begin.
Training Epoch: 28 [45824/50048]	Loss: 0.7080
Training Epoch: 28 [45952/50048]	Loss: 0.6791
Training Epoch: 28 [46080/50048]	Loss: 0.6784
Training Epoch: 28 [46208/50048]	Loss: 0.5781
Training Epoch: 28 [46336/50048]	Loss: 0.7443
Training Epoch: 28 [46464/50048]	Loss: 0.6222
Training Epoch: 28 [46592/50048]	Loss: 0.8521
Training Epoch: 28 [46720/50048]	Loss: 0.7225
Training Epoch: 28 [46848/50048]	Loss: 0.6599
Training Epoch: 28 [46976/50048]	Loss: 0.7408
Training Epoch: 28 [47104/50048]	Loss: 0.6958
Training Epoch: 28 [47232/50048]	Loss: 0.8033
Training Epoch: 28 [47360/50048]	Loss: 0.8177
Training Epoch: 28 [47488/50048]	Loss: 0.7684
Training Epoch: 28 [47616/50048]	Loss: 0.6738
Training Epoch: 28 [47744/50048]	Loss: 0.6389
Training Epoch: 28 [47872/50048]	Loss: 0.7961
Training Epoch: 28 [48000/50048]	Loss: 0.6501
Training Epoch: 28 [48128/50048]	Loss: 0.6237
Training Epoch: 28 [48256/50048]	Loss: 0.7934
Training Epoch: 28 [48384/50048]	Loss: 0.8132
Training Epoch: 28 [48512/50048]	Loss: 0.6340
Training Epoch: 28 [48640/50048]	Loss: 0.7411
Training Epoch: 28 [48768/50048]	Loss: 0.7587
Training Epoch: 28 [48896/50048]	Loss: 0.5591
Training Epoch: 28 [49024/50048]	Loss: 0.6602
Training Epoch: 28 [49152/50048]	Loss: 0.6338
Training Epoch: 28 [49280/50048]	Loss: 0.6705
Training Epoch: 28 [49408/50048]	Loss: 0.8096
Training Epoch: 28 [49536/50048]	Loss: 0.6084
Training Epoch: 28 [49664/50048]	Loss: 0.8527
Training Epoch: 28 [49792/50048]	Loss: 0.6957
Training Epoch: 28 [49920/50048]	Loss: 0.6065
Training Epoch: 28 [50048/50048]	Loss: 0.8822
2022-12-06 09:21:01.169 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:21:01,200 [ZeusDataLoader(eval)] eval epoch 29 done: time=3.69 energy=453.36
2022-12-06 04:21:01,200 [ZeusDataLoader(train)] Up to epoch 29: time=2618.35, energy=317698.48, cost=387954.60
2022-12-06 04:21:01,200 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:21:01,200 [ZeusDataLoader(train)] Expected next epoch: time=2708.15, energy=328496.49, cost=401210.99
2022-12-06 04:21:01,201 [ZeusDataLoader(train)] Epoch 30 begin.
Validation Epoch: 28, Average loss: 0.0120, Accuracy: 0.6247
2022-12-06 04:21:01,389 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:21:01,390 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:21:01.392 [ZeusMonitor] Monitor started.
2022-12-06 09:21:01.392 [ZeusMonitor] Running indefinitely. 2022-12-06 09:21:01.392 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:21:01.392 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e30+gpu0.power.log
Training Epoch: 29 [128/50048]	Loss: 0.4755
Training Epoch: 29 [256/50048]	Loss: 0.6111
Training Epoch: 29 [384/50048]	Loss: 0.5434
Training Epoch: 29 [512/50048]	Loss: 0.6866
Training Epoch: 29 [640/50048]	Loss: 0.5413
Training Epoch: 29 [768/50048]	Loss: 0.5821
Training Epoch: 29 [896/50048]	Loss: 0.5751
Training Epoch: 29 [1024/50048]	Loss: 0.5386
Training Epoch: 29 [1152/50048]	Loss: 0.5056
Training Epoch: 29 [1280/50048]	Loss: 0.4277
Training Epoch: 29 [1408/50048]	Loss: 0.7258
Training Epoch: 29 [1536/50048]	Loss: 0.6420
Training Epoch: 29 [1664/50048]	Loss: 0.6949
Training Epoch: 29 [1792/50048]	Loss: 0.5725
Training Epoch: 29 [1920/50048]	Loss: 0.8216
Training Epoch: 29 [2048/50048]	Loss: 0.5244
Training Epoch: 29 [2176/50048]	Loss: 0.6091
Training Epoch: 29 [2304/50048]	Loss: 0.5397
Training Epoch: 29 [2432/50048]	Loss: 0.6372
Training Epoch: 29 [2560/50048]	Loss: 0.5574
Training Epoch: 29 [2688/50048]	Loss: 0.5920
Training Epoch: 29 [2816/50048]	Loss: 0.5477
Training Epoch: 29 [2944/50048]	Loss: 0.7348
Training Epoch: 29 [3072/50048]	Loss: 0.7696
Training Epoch: 29 [3200/50048]	Loss: 0.4644
Training Epoch: 29 [3328/50048]	Loss: 0.6389
Training Epoch: 29 [3456/50048]	Loss: 0.6870
Training Epoch: 29 [3584/50048]	Loss: 0.6616
Training Epoch: 29 [3712/50048]	Loss: 0.6786
Training Epoch: 29 [3840/50048]	Loss: 0.4222
Training Epoch: 29 [3968/50048]	Loss: 0.5203
Training Epoch: 29 [4096/50048]	Loss: 0.6635
Training Epoch: 29 [4224/50048]	Loss: 0.5995
Training Epoch: 29 [4352/50048]	Loss: 0.7031
Training Epoch: 29 [4480/50048]	Loss: 0.6723
Training Epoch: 29 [4608/50048]	Loss: 0.7511
Training Epoch: 29 [4736/50048]	Loss: 0.7823
Training Epoch: 29 [4864/50048]	Loss: 0.5868
Training Epoch: 29 [4992/50048]	Loss: 0.5365
Training Epoch: 29 [5120/50048]	Loss: 0.6071
Training Epoch: 29 [5248/50048]	Loss: 0.6556
Training Epoch: 29 [5376/50048]	Loss: 0.5708
Training Epoch: 29 [5504/50048]	Loss: 0.6316
Training Epoch: 29 [5632/50048]	Loss: 0.6388
Training Epoch: 29 [5760/50048]	Loss: 0.6675
Training Epoch: 29 [5888/50048]	Loss: 0.6565
Training Epoch: 29 [6016/50048]	Loss: 0.5874
Training Epoch: 29 [6144/50048]	Loss: 0.5504
Training Epoch: 29 [6272/50048]	Loss: 0.4672
Training Epoch: 29 [6400/50048]	Loss: 0.6739
Training Epoch: 29 [6528/50048]	Loss: 0.6618
Training Epoch: 29 [6656/50048]	Loss: 0.6895
Training Epoch: 29 [6784/50048]	Loss: 0.6855
Training Epoch: 29 [6912/50048]	Loss: 0.5757
Training Epoch: 29 [7040/50048]	Loss: 0.7732
Training Epoch: 29 [7168/50048]	Loss: 0.6445
Training Epoch: 29 [7296/50048]	Loss: 0.5526
Training Epoch: 29 [7424/50048]	Loss: 0.6740
Training Epoch: 29 [7552/50048]	Loss: 0.6335
Training Epoch: 29 [7680/50048]	Loss: 0.6753
Training Epoch: 29 [7808/50048]	Loss: 0.7272
Training Epoch: 29 [7936/50048]	Loss: 0.7142
Training Epoch: 29 [8064/50048]	Loss: 0.6723
Training Epoch: 29 [8192/50048]	Loss: 0.5451
Training Epoch: 29 [8320/50048]	Loss: 0.5724
Training Epoch: 29 [8448/50048]	Loss: 0.5744
Training Epoch: 29 [8576/50048]	Loss: 0.6325
Training Epoch: 29 [8704/50048]	Loss: 0.6845
Training Epoch: 29 [8832/50048]	Loss: 0.6616
Training Epoch: 29 [8960/50048]	Loss: 0.8039
Training Epoch: 29 [9088/50048]	Loss: 0.7993
Training Epoch: 29 [9216/50048]	Loss: 0.7311
Training Epoch: 29 [9344/50048]	Loss: 0.6404
Training Epoch: 29 [9472/50048]	Loss: 0.6919
Training Epoch: 29 [9600/50048]	Loss: 0.7527
Training Epoch: 29 [9728/50048]	Loss: 0.6795
Training Epoch: 29 [9856/50048]	Loss: 0.4961
Training Epoch: 29 [9984/50048]	Loss: 0.6467
Training Epoch: 29 [10112/50048]	Loss: 0.7290
Training Epoch: 29 [10240/50048]	Loss: 0.7787
Training Epoch: 29 [10368/50048]	Loss: 0.6102
Training Epoch: 29 [10496/50048]	Loss: 0.5839
Training Epoch: 29 [10624/50048]	Loss: 0.5614
Training Epoch: 29 [10752/50048]	Loss: 0.6089
Training Epoch: 29 [10880/50048]	Loss: 0.5843
Training Epoch: 29 [11008/50048]	Loss: 0.6341
Training Epoch: 29 [11136/50048]	Loss: 0.6265
Training Epoch: 29 [11264/50048]	Loss: 0.5937
Training Epoch: 29 [11392/50048]	Loss: 0.7812
Training Epoch: 29 [11520/50048]	Loss: 0.6519
Training Epoch: 29 [11648/50048]	Loss: 0.6109
Training Epoch: 29 [11776/50048]	Loss: 0.7301
Training Epoch: 29 [11904/50048]	Loss: 0.9037
Training Epoch: 29 [12032/50048]	Loss: 0.9083
Training Epoch: 29 [12160/50048]	Loss: 0.7878
Training Epoch: 29 [12288/50048]	Loss: 0.7328
Training Epoch: 29 [12416/50048]	Loss: 0.6217
Training Epoch: 29 [12544/50048]	Loss: 0.5784
Training Epoch: 29 [12672/50048]	Loss: 0.7059
Training Epoch: 29 [12800/50048]	Loss: 0.8769
Training Epoch: 29 [12928/50048]	Loss: 0.7824
Training Epoch: 29 [13056/50048]	Loss: 0.7928
Training Epoch: 29 [13184/50048]	Loss: 0.4660
Training Epoch: 29 [13312/50048]	Loss: 0.6363
Training Epoch: 29 [13440/50048]	Loss: 0.5733
Training Epoch: 29 [13568/50048]	Loss: 0.6882
Training Epoch: 29 [13696/50048]	Loss: 0.6960
Training Epoch: 29 [13824/50048]	Loss: 0.7139
Training Epoch: 29 [13952/50048]	Loss: 0.6683
Training Epoch: 29 [14080/50048]	Loss: 0.5661
Training Epoch: 29 [14208/50048]	Loss: 0.6468
Training Epoch: 29 [14336/50048]	Loss: 0.6554
Training Epoch: 29 [14464/50048]	Loss: 0.6360
Training Epoch: 29 [14592/50048]	Loss: 0.7339
Training Epoch: 29 [14720/50048]	Loss: 0.6748
Training Epoch: 29 [14848/50048]	Loss: 0.6342
Training Epoch: 29 [14976/50048]	Loss: 0.7082
Training Epoch: 29 [15104/50048]	Loss: 0.7691
Training Epoch: 29 [15232/50048]	Loss: 0.6593
Training Epoch: 29 [15360/50048]	Loss: 0.6537
Training Epoch: 29 [15488/50048]	Loss: 0.6450
Training Epoch: 29 [15616/50048]	Loss: 0.4526
Training Epoch: 29 [15744/50048]	Loss: 0.6614
Training Epoch: 29 [15872/50048]	Loss: 0.5558
Training Epoch: 29 [16000/50048]	Loss: 0.7757
Training Epoch: 29 [16128/50048]	Loss: 0.6669
Training Epoch: 29 [16256/50048]	Loss: 0.6197
Training Epoch: 29 [16384/50048]	Loss: 0.6990
Training Epoch: 29 [16512/50048]	Loss: 0.6278
Training Epoch: 29 [16640/50048]	Loss: 0.7122
Training Epoch: 29 [16768/50048]	Loss: 0.7924
Training Epoch: 29 [16896/50048]	Loss: 0.6596
Training Epoch: 29 [17024/50048]	Loss: 0.5120
Training Epoch: 29 [17152/50048]	Loss: 0.6702
Training Epoch: 29 [17280/50048]	Loss: 0.6468
Training Epoch: 29 [17408/50048]	Loss: 0.6399
Training Epoch: 29 [17536/50048]	Loss: 0.5989
Training Epoch: 29 [17664/50048]	Loss: 0.6368
Training Epoch: 29 [17792/50048]	Loss: 0.7020
Training Epoch: 29 [17920/50048]	Loss: 0.7039
Training Epoch: 29 [18048/50048]	Loss: 0.6292
Training Epoch: 29 [18176/50048]	Loss: 0.6468
Training Epoch: 29 [18304/50048]	Loss: 0.7534
Training Epoch: 29 [18432/50048]	Loss: 0.6374
Training Epoch: 29 [18560/50048]	Loss: 0.6954
Training Epoch: 29 [18688/50048]	Loss: 0.5925
Training Epoch: 29 [18816/50048]	Loss: 0.8278
Training Epoch: 29 [18944/50048]	Loss: 0.8091
Training Epoch: 29 [19072/50048]	Loss: 0.8094
Training Epoch: 29 [19200/50048]	Loss: 0.6561
Training Epoch: 29 [19328/50048]	Loss: 0.6888
Training Epoch: 29 [19456/50048]	Loss: 0.7225
Training Epoch: 29 [19584/50048]	Loss: 0.4393
Training Epoch: 29 [19712/50048]	Loss: 0.6219
Training Epoch: 29 [19840/50048]	Loss: 0.4706
Training Epoch: 29 [19968/50048]	Loss: 0.6272
Training Epoch: 29 [20096/50048]	Loss: 0.6430
Training Epoch: 29 [20224/50048]	Loss: 0.6201
Training Epoch: 29 [20352/50048]	Loss: 1.0142
Training Epoch: 29 [20480/50048]	Loss: 0.7495
Training Epoch: 29 [20608/50048]	Loss: 0.6056
Training Epoch: 29 [20736/50048]	Loss: 0.6202
Training Epoch: 29 [20864/50048]	Loss: 0.8301
Training Epoch: 29 [20992/50048]	Loss: 0.8147
Training Epoch: 29 [21120/50048]	Loss: 0.5106
Training Epoch: 29 [21248/50048]	Loss: 0.6892
Training Epoch: 29 [21376/50048]	Loss: 0.7668
Training Epoch: 29 [21504/50048]	Loss: 0.7126
Training Epoch: 29 [21632/50048]	Loss: 0.7249
Training Epoch: 29 [21760/50048]	Loss: 0.5370
Training Epoch: 29 [21888/50048]	Loss: 0.7005
Training Epoch: 29 [22016/50048]	Loss: 0.6336
Training Epoch: 29 [22144/50048]	Loss: 0.5948
Training Epoch: 29 [22272/50048]	Loss: 0.5032
Training Epoch: 29 [22400/50048]	Loss: 0.6972
Training Epoch: 29 [22528/50048]	Loss: 0.5526
Training Epoch: 29 [22656/50048]	Loss: 0.7373
Training Epoch: 29 [22784/50048]	Loss: 0.6542
Training Epoch: 29 [22912/50048]	Loss: 0.6694
Training Epoch: 29 [23040/50048]	Loss: 0.5186
Training Epoch: 29 [23168/50048]	Loss: 0.5264
Training Epoch: 29 [23296/50048]	Loss: 0.7047
Training Epoch: 29 [23424/50048]	Loss: 0.6418
Training Epoch: 29 [23552/50048]	Loss: 0.7081
Training Epoch: 29 [23680/50048]	Loss: 0.6339
Training Epoch: 29 [23808/50048]	Loss: 0.6650
Training Epoch: 29 [23936/50048]	Loss: 0.6480
Training Epoch: 29 [24064/50048]	Loss: 0.6850
Training Epoch: 29 [24192/50048]	Loss: 0.5293
Training Epoch: 29 [24320/50048]	Loss: 0.6360
Training Epoch: 29 [24448/50048]	Loss: 0.8166
Training Epoch: 29 [24576/50048]	Loss: 0.7694
Training Epoch: 29 [24704/50048]	Loss: 0.6389
Training Epoch: 29 [24832/50048]	Loss: 0.7180
Training Epoch: 29 [24960/50048]	Loss: 0.5915
Training Epoch: 29 [25088/50048]	Loss: 0.6900
Training Epoch: 29 [25216/50048]	Loss: 0.6688
Training Epoch: 29 [25344/50048]	Loss: 0.6351
Training Epoch: 29 [25472/50048]	Loss: 0.7989
Training Epoch: 29 [25600/50048]	Loss: 0.6677
Training Epoch: 29 [25728/50048]	Loss: 0.6905
Training Epoch: 29 [25856/50048]	Loss: 0.6084
Training Epoch: 29 [25984/50048]	Loss: 0.5230
Training Epoch: 29 [26112/50048]	Loss: 0.5911
Training Epoch: 29 [26240/50048]	Loss: 0.7619
Training Epoch: 29 [26368/50048]	Loss: 0.5430
Training Epoch: 29 [26496/50048]	Loss: 0.7068
Training Epoch: 29 [26624/50048]	Loss: 0.5701
Training Epoch: 29 [26752/50048]	Loss: 0.5472
Training Epoch: 29 [26880/50048]	Loss: 0.7742
Training Epoch: 29 [27008/50048]	Loss: 0.7836
Training Epoch: 29 [27136/50048]	Loss: 0.7812
Training Epoch: 29 [27264/50048]	Loss: 0.6933
Training Epoch: 29 [27392/50048]	Loss: 0.6801
Training Epoch: 29 [27520/50048]	Loss: 0.6525
Training Epoch: 29 [27648/50048]	Loss: 0.6900
Training Epoch: 29 [27776/50048]	Loss: 0.6319
Training Epoch: 29 [27904/50048]	Loss: 0.6090
Training Epoch: 29 [28032/50048]	Loss: 0.7131
Training Epoch: 29 [28160/50048]	Loss: 0.6033
Training Epoch: 29 [28288/50048]	Loss: 0.5674
Training Epoch: 29 [28416/50048]	Loss: 0.5990
Training Epoch: 29 [28544/50048]	Loss: 0.6117
Training Epoch: 29 [28672/50048]	Loss: 0.6330
Training Epoch: 29 [28800/50048]	Loss: 0.6577
Training Epoch: 29 [28928/50048]	Loss: 0.7233
Training Epoch: 29 [29056/50048]	Loss: 0.7076
Training Epoch: 29 [29184/50048]	Loss: 0.5891
Training Epoch: 29 [29312/50048]	Loss: 0.6694
Training Epoch: 29 [29440/50048]	Loss: 0.6150
Training Epoch: 29 [29568/50048]	Loss: 0.5843
Training Epoch: 29 [29696/50048]	Loss: 0.8658
Training Epoch: 29 [29824/50048]	Loss: 0.6796
Training Epoch: 29 [29952/50048]	Loss: 0.6464
Training Epoch: 29 [30080/50048]	Loss: 0.6421
Training Epoch: 29 [30208/50048]	Loss: 0.5544
Training Epoch: 29 [30336/50048]	Loss: 0.6212
Training Epoch: 29 [30464/50048]	Loss: 0.6470
Training Epoch: 29 [30592/50048]	Loss: 0.6515
Training Epoch: 29 [30720/50048]	Loss: 0.7697
Training Epoch: 29 [30848/50048]	Loss: 0.7521
Training Epoch: 29 [30976/50048]	Loss: 0.6038
Training Epoch: 29 [31104/50048]	Loss: 0.6036
Training Epoch: 29 [31232/50048]	Loss: 0.8075
Training Epoch: 29 [31360/50048]	Loss: 0.7210
Training Epoch: 29 [31488/50048]	Loss: 0.5181
Training Epoch: 29 [31616/50048]	Loss: 0.7921
Training Epoch: 29 [31744/50048]	Loss: 0.8169
Training Epoch: 29 [31872/50048]	Loss: 0.5973
Training Epoch: 29 [32000/50048]	Loss: 0.8067
Training Epoch: 29 [32128/50048]	Loss: 0.8700
Training Epoch: 29 [32256/50048]	Loss: 0.8623
Training Epoch: 29 [32384/50048]	Loss: 0.9669
Training Epoch: 29 [32512/50048]	Loss: 0.6233
Training Epoch: 29 [32640/50048]	Loss: 0.5729
Training Epoch: 29 [32768/50048]	Loss: 0.8218
Training Epoch: 29 [32896/50048]	Loss: 0.6325
Training Epoch: 29 [33024/50048]	Loss: 0.6603
Training Epoch: 29 [33152/50048]	Loss: 0.6890
Training Epoch: 29 [33280/50048]	Loss: 0.5833
Training Epoch: 29 [33408/50048]	Loss: 0.6300
Training Epoch: 29 [33536/50048]	Loss: 0.6727
Training Epoch: 29 [33664/50048]	Loss: 0.5626
Training Epoch: 29 [33792/50048]	Loss: 0.7055
Training Epoch: 29 [33920/50048]	Loss: 0.8211
Training Epoch: 29 [34048/50048]	Loss: 0.6405
Training Epoch: 29 [34176/50048]	Loss: 0.7529
Training Epoch: 29 [34304/50048]	Loss: 0.8227
Training Epoch: 29 [34432/50048]	Loss: 0.8459
Training Epoch: 29 [34560/50048]	Loss: 0.8043
Training Epoch: 29 [34688/50048]	Loss: 0.7908
Training Epoch: 29 [34816/50048]	Loss: 0.6336
Training Epoch: 29 [34944/50048]	Loss: 0.5198
Training Epoch: 29 [35072/50048]	Loss: 0.5628
Training Epoch: 29 [35200/50048]	Loss: 0.4165
Training Epoch: 29 [35328/50048]	Loss: 0.7099
Training Epoch: 29 [35456/50048]	Loss: 0.5190
Training Epoch: 29 [35584/50048]	Loss: 0.5742
Training Epoch: 29 [35712/50048]	Loss: 0.7226
Training Epoch: 29 [35840/50048]	Loss: 0.6359
Training Epoch: 29 [35968/50048]	Loss: 0.4929
Training Epoch: 29 [36096/50048]	Loss: 0.8278
Training Epoch: 29 [36224/50048]	Loss: 0.5921
Training Epoch: 29 [36352/50048]	Loss: 0.7113
Training Epoch: 29 [36480/50048]	Loss: 0.6247
Training Epoch: 29 [36608/50048]	Loss: 0.7133
Training Epoch: 29 [36736/50048]	Loss: 0.5738
Training Epoch: 29 [36864/50048]	Loss: 0.8476
Training Epoch: 29 [36992/50048]	Loss: 0.7354
Training Epoch: 29 [37120/50048]	Loss: 0.7093
Training Epoch: 29 [37248/50048]	Loss: 0.5838
Training Epoch: 29 [37376/50048]	Loss: 0.5627
Training Epoch: 29 [37504/50048]	Loss: 0.6278
Training Epoch: 29 [37632/50048]	Loss: 0.6542
Training Epoch: 29 [37760/50048]	Loss: 0.6218
Training Epoch: 29 [37888/50048]	Loss: 0.7702
Training Epoch: 29 [38016/50048]	Loss: 0.6834
Training Epoch: 29 [38144/50048]	Loss: 0.6892
Training Epoch: 29 [38272/50048]	Loss: 0.6345
Training Epoch: 29 [38400/50048]	Loss: 0.6063
Training Epoch: 29 [38528/50048]	Loss: 0.5767
Training Epoch: 29 [38656/50048]	Loss: 0.7129
Training Epoch: 29 [38784/50048]	Loss: 0.6954
Training Epoch: 29 [38912/50048]	Loss: 0.6620
Training Epoch: 29 [39040/50048]	Loss: 0.6275
Training Epoch: 29 [39168/50048]	Loss: 0.7510
Training Epoch: 29 [39296/50048]	Loss: 0.6070
Training Epoch: 29 [39424/50048]	Loss: 0.7098
Training Epoch: 29 [39552/50048]	Loss: 0.8348
Training Epoch: 29 [39680/50048]	Loss: 0.8952
Training Epoch: 29 [39808/50048]	Loss: 0.7448
Training Epoch: 29 [39936/50048]	Loss: 0.6759
Training Epoch: 29 [40064/50048]	Loss: 0.6476
Training Epoch: 29 [40192/50048]	Loss: 0.6282
Training Epoch: 29 [40320/50048]	Loss: 0.7293
Training Epoch: 29 [40448/50048]	Loss: 0.7331
Training Epoch: 29 [40576/50048]	Loss: 0.7522
Training Epoch: 29 [40704/50048]	Loss: 0.7905
Training Epoch: 29 [40832/50048]	Loss: 0.9972
Training Epoch: 29 [40960/50048]	Loss: 0.5505
Training Epoch: 29 [41088/50048]	Loss: 0.6915
Training Epoch: 29 [41216/50048]	Loss: 0.6527
Training Epoch: 29 [41344/50048]	Loss: 0.5660
Training Epoch: 29 [41472/50048]	Loss: 0.6562
Training Epoch: 29 [41600/50048]	Loss: 0.8187
Training Epoch: 29 [41728/50048]	Loss: 0.6725
Training Epoch: 29 [41856/50048]	Loss: 0.8126
Training Epoch: 29 [41984/50048]	Loss: 0.8015
Training Epoch: 29 [42112/50048]	Loss: 0.8549
Training Epoch: 29 [42240/50048]	Loss: 0.7308
Training Epoch: 29 [42368/50048]	Loss: 0.7381
Training Epoch: 29 [42496/50048]	Loss: 0.8296
Training Epoch: 29 [42624/50048]	Loss: 0.6749
Training Epoch: 29 [42752/50048]	Loss: 0.8058
Training Epoch: 29 [42880/50048]	Loss: 0.5907
Training Epoch: 29 [43008/50048]	Loss: 0.9456
Training Epoch: 29 [43136/50048]	Loss: 0.7730
Training Epoch: 29 [43264/50048]	Loss: 0.8183
Training Epoch: 29 [43392/50048]	Loss: 0.6946
Training Epoch: 29 [43520/50048]	Loss: 0.7261
Training Epoch: 29 [43648/50048]	Loss: 0.6042
Training Epoch: 29 [43776/50048]	Loss: 0.5889
Training Epoch: 29 [43904/50048]	Loss: 0.6771
Training Epoch: 29 [44032/50048]	Loss: 0.6259
Training Epoch: 29 [44160/50048]	Loss: 0.5968
Training Epoch: 29 [44288/50048]	Loss: 0.5777
Training Epoch: 29 [44416/50048]	Loss: 0.8951
Training Epoch: 29 [44544/50048]	Loss: 0.7640
Training Epoch: 29 [44672/50048]	Loss: 0.5859
Training Epoch: 29 [44800/50048]	Loss: 0.8129
Training Epoch: 29 [44928/50048]	Loss: 0.6621
Training Epoch: 29 [45056/50048]	Loss: 0.9168
Training Epoch: 29 [45184/50048]	Loss: 0.6968
Training Epoch: 29 [45312/50048]	Loss: 0.7665
Training Epoch: 29 [45440/50048]	Loss: 0.8862
Training Epoch: 29 [45568/50048]	Loss: 0.6401
Training Epoch: 29 [45696/50048]	Loss: 0.7876
2022-12-06 04:22:27,626 [ZeusDataLoader(train)] train epoch 30 done: time=86.42 energy=10504.05
2022-12-06 04:22:27,627 [ZeusDataLoader(eval)] Epoch 30 begin.
Training Epoch: 29 [45824/50048]	Loss: 0.6665
Training Epoch: 29 [45952/50048]	Loss: 0.7486
Training Epoch: 29 [46080/50048]	Loss: 0.6529
Training Epoch: 29 [46208/50048]	Loss: 0.6763
Training Epoch: 29 [46336/50048]	Loss: 0.6471
Training Epoch: 29 [46464/50048]	Loss: 0.7850
Training Epoch: 29 [46592/50048]	Loss: 0.6095
Training Epoch: 29 [46720/50048]	Loss: 0.7558
Training Epoch: 29 [46848/50048]	Loss: 0.7976
Training Epoch: 29 [46976/50048]	Loss: 0.7076
Training Epoch: 29 [47104/50048]	Loss: 0.7663
Training Epoch: 29 [47232/50048]	Loss: 0.8366
Training Epoch: 29 [47360/50048]	Loss: 0.6603
Training Epoch: 29 [47488/50048]	Loss: 0.6547
Training Epoch: 29 [47616/50048]	Loss: 0.5997
Training Epoch: 29 [47744/50048]	Loss: 0.6347
Training Epoch: 29 [47872/50048]	Loss: 0.7904
Training Epoch: 29 [48000/50048]	Loss: 0.6756
Training Epoch: 29 [48128/50048]	Loss: 0.6499
Training Epoch: 29 [48256/50048]	Loss: 0.7241
Training Epoch: 29 [48384/50048]	Loss: 0.5710
Training Epoch: 29 [48512/50048]	Loss: 0.7740
Training Epoch: 29 [48640/50048]	Loss: 0.8787
Training Epoch: 29 [48768/50048]	Loss: 0.8404
Training Epoch: 29 [48896/50048]	Loss: 0.7540
Training Epoch: 29 [49024/50048]	Loss: 0.7183
Training Epoch: 29 [49152/50048]	Loss: 0.6030
Training Epoch: 29 [49280/50048]	Loss: 0.7802
Training Epoch: 29 [49408/50048]	Loss: 0.7941
Training Epoch: 29 [49536/50048]	Loss: 0.7283
Training Epoch: 29 [49664/50048]	Loss: 0.7450
Training Epoch: 29 [49792/50048]	Loss: 0.8107
Training Epoch: 29 [49920/50048]	Loss: 0.7691
Training Epoch: 29 [50048/50048]	Loss: 0.6521
2022-12-06 09:22:31.363 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:22:31,386 [ZeusDataLoader(eval)] eval epoch 30 done: time=3.75 energy=453.05
2022-12-06 04:22:31,386 [ZeusDataLoader(train)] Up to epoch 30: time=2708.51, energy=328655.58, cost=401322.53
2022-12-06 04:22:31,386 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:22:31,386 [ZeusDataLoader(train)] Expected next epoch: time=2798.31, energy=339453.59, cost=414578.91
2022-12-06 04:22:31,387 [ZeusDataLoader(train)] Epoch 31 begin.
Validation Epoch: 29, Average loss: 0.0127, Accuracy: 0.6087
2022-12-06 04:22:31,559 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:22:31,559 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:22:31.570 [ZeusMonitor] Monitor started.
2022-12-06 09:22:31.570 [ZeusMonitor] Running indefinitely. 2022-12-06 09:22:31.570 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:22:31.570 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e31+gpu0.power.log
Training Epoch: 30 [128/50048]	Loss: 0.6496
Training Epoch: 30 [256/50048]	Loss: 0.8012
Training Epoch: 30 [384/50048]	Loss: 0.6654
Training Epoch: 30 [512/50048]	Loss: 0.7285
Training Epoch: 30 [640/50048]	Loss: 0.8466
Training Epoch: 30 [768/50048]	Loss: 0.5501
Training Epoch: 30 [896/50048]	Loss: 0.6092
Training Epoch: 30 [1024/50048]	Loss: 0.6350
Training Epoch: 30 [1152/50048]	Loss: 0.6837
Training Epoch: 30 [1280/50048]	Loss: 0.6489
Training Epoch: 30 [1408/50048]	Loss: 0.5749
Training Epoch: 30 [1536/50048]	Loss: 0.7397
Training Epoch: 30 [1664/50048]	Loss: 0.5480
Training Epoch: 30 [1792/50048]	Loss: 0.5686
Training Epoch: 30 [1920/50048]	Loss: 0.5189
Training Epoch: 30 [2048/50048]	Loss: 0.6189
Training Epoch: 30 [2176/50048]	Loss: 0.7096
Training Epoch: 30 [2304/50048]	Loss: 0.5935
Training Epoch: 30 [2432/50048]	Loss: 0.6022
Training Epoch: 30 [2560/50048]	Loss: 0.4996
Training Epoch: 30 [2688/50048]	Loss: 0.4995
Training Epoch: 30 [2816/50048]	Loss: 0.7796
Training Epoch: 30 [2944/50048]	Loss: 0.6369
Training Epoch: 30 [3072/50048]	Loss: 0.4762
Training Epoch: 30 [3200/50048]	Loss: 0.6240
Training Epoch: 30 [3328/50048]	Loss: 0.5590
Training Epoch: 30 [3456/50048]	Loss: 0.4609
Training Epoch: 30 [3584/50048]	Loss: 0.5355
Training Epoch: 30 [3712/50048]	Loss: 0.6564
Training Epoch: 30 [3840/50048]	Loss: 0.4949
Training Epoch: 30 [3968/50048]	Loss: 0.5622
Training Epoch: 30 [4096/50048]	Loss: 0.4744
Training Epoch: 30 [4224/50048]	Loss: 0.6501
Training Epoch: 30 [4352/50048]	Loss: 0.5845
Training Epoch: 30 [4480/50048]	Loss: 0.4855
Training Epoch: 30 [4608/50048]	Loss: 0.6375
Training Epoch: 30 [4736/50048]	Loss: 0.5452
Training Epoch: 30 [4864/50048]	Loss: 0.8014
Training Epoch: 30 [4992/50048]	Loss: 0.6056
Training Epoch: 30 [5120/50048]	Loss: 0.5827
Training Epoch: 30 [5248/50048]	Loss: 0.5583
Training Epoch: 30 [5376/50048]	Loss: 0.5373
Training Epoch: 30 [5504/50048]	Loss: 0.6640
Training Epoch: 30 [5632/50048]	Loss: 0.7379
Training Epoch: 30 [5760/50048]	Loss: 0.6955
Training Epoch: 30 [5888/50048]	Loss: 0.7290
Training Epoch: 30 [6016/50048]	Loss: 0.5132
Training Epoch: 30 [6144/50048]	Loss: 0.4933
Training Epoch: 30 [6272/50048]	Loss: 0.5305
Training Epoch: 30 [6400/50048]	Loss: 0.6966
Training Epoch: 30 [6528/50048]	Loss: 0.5265
Training Epoch: 30 [6656/50048]	Loss: 0.4806
Training Epoch: 30 [6784/50048]	Loss: 0.6685
Training Epoch: 30 [6912/50048]	Loss: 0.7050
Training Epoch: 30 [7040/50048]	Loss: 0.5794
Training Epoch: 30 [7168/50048]	Loss: 0.7674
Training Epoch: 30 [7296/50048]	Loss: 0.6572
Training Epoch: 30 [7424/50048]	Loss: 0.8493
Training Epoch: 30 [7552/50048]	Loss: 0.4005
Training Epoch: 30 [7680/50048]	Loss: 0.6744
Training Epoch: 30 [7808/50048]	Loss: 0.7519
Training Epoch: 30 [7936/50048]	Loss: 0.6688
Training Epoch: 30 [8064/50048]	Loss: 0.5498
Training Epoch: 30 [8192/50048]	Loss: 0.5105
Training Epoch: 30 [8320/50048]	Loss: 0.5670
Training Epoch: 30 [8448/50048]	Loss: 0.8525
Training Epoch: 30 [8576/50048]	Loss: 0.6567
Training Epoch: 30 [8704/50048]	Loss: 0.6075
Training Epoch: 30 [8832/50048]	Loss: 0.6116
Training Epoch: 30 [8960/50048]	Loss: 0.5615
Training Epoch: 30 [9088/50048]	Loss: 0.6395
Training Epoch: 30 [9216/50048]	Loss: 0.4540
Training Epoch: 30 [9344/50048]	Loss: 0.4917
Training Epoch: 30 [9472/50048]	Loss: 0.5264
Training Epoch: 30 [9600/50048]	Loss: 0.6090
Training Epoch: 30 [9728/50048]	Loss: 0.5351
Training Epoch: 30 [9856/50048]	Loss: 0.6701
Training Epoch: 30 [9984/50048]	Loss: 0.6220
Training Epoch: 30 [10112/50048]	Loss: 0.5493
Training Epoch: 30 [10240/50048]	Loss: 0.4966
Training Epoch: 30 [10368/50048]	Loss: 0.5336
Training Epoch: 30 [10496/50048]	Loss: 0.7489
Training Epoch: 30 [10624/50048]	Loss: 0.6143
Training Epoch: 30 [10752/50048]	Loss: 0.7006
Training Epoch: 30 [10880/50048]	Loss: 0.4648
Training Epoch: 30 [11008/50048]	Loss: 0.6584
Training Epoch: 30 [11136/50048]	Loss: 0.7708
Training Epoch: 30 [11264/50048]	Loss: 0.6274
Training Epoch: 30 [11392/50048]	Loss: 0.5901
Training Epoch: 30 [11520/50048]	Loss: 0.7753
Training Epoch: 30 [11648/50048]	Loss: 0.5339
Training Epoch: 30 [11776/50048]	Loss: 0.4644
Training Epoch: 30 [11904/50048]	Loss: 0.6233
Training Epoch: 30 [12032/50048]	Loss: 0.5798
Training Epoch: 30 [12160/50048]	Loss: 0.7376
Training Epoch: 30 [12288/50048]	Loss: 0.5978
Training Epoch: 30 [12416/50048]	Loss: 0.6510
Training Epoch: 30 [12544/50048]	Loss: 0.5497
Training Epoch: 30 [12672/50048]	Loss: 0.5684
Training Epoch: 30 [12800/50048]	Loss: 0.5261
Training Epoch: 30 [12928/50048]	Loss: 0.7082
Training Epoch: 30 [13056/50048]	Loss: 0.6535
Training Epoch: 30 [13184/50048]	Loss: 0.4946
Training Epoch: 30 [13312/50048]	Loss: 0.7146
Training Epoch: 30 [13440/50048]	Loss: 0.7432
Training Epoch: 30 [13568/50048]	Loss: 0.5589
Training Epoch: 30 [13696/50048]	Loss: 0.6926
Training Epoch: 30 [13824/50048]	Loss: 0.5226
Training Epoch: 30 [13952/50048]	Loss: 0.6089
Training Epoch: 30 [14080/50048]	Loss: 0.4319
Training Epoch: 30 [14208/50048]	Loss: 0.5271
Training Epoch: 30 [14336/50048]	Loss: 0.6163
Training Epoch: 30 [14464/50048]	Loss: 0.6636
Training Epoch: 30 [14592/50048]	Loss: 0.5825
Training Epoch: 30 [14720/50048]	Loss: 0.6527
Training Epoch: 30 [14848/50048]	Loss: 0.7003
Training Epoch: 30 [14976/50048]	Loss: 0.6489
Training Epoch: 30 [15104/50048]	Loss: 0.5496
Training Epoch: 30 [15232/50048]	Loss: 0.6291
Training Epoch: 30 [15360/50048]	Loss: 0.7672
Training Epoch: 30 [15488/50048]	Loss: 0.5896
Training Epoch: 30 [15616/50048]	Loss: 0.6444
Training Epoch: 30 [15744/50048]	Loss: 0.5509
Training Epoch: 30 [15872/50048]	Loss: 0.5320
Training Epoch: 30 [16000/50048]	Loss: 0.6032
Training Epoch: 30 [16128/50048]	Loss: 0.7647
Training Epoch: 30 [16256/50048]	Loss: 0.6978
Training Epoch: 30 [16384/50048]	Loss: 0.6196
Training Epoch: 30 [16512/50048]	Loss: 0.5857
Training Epoch: 30 [16640/50048]	Loss: 0.6027
Training Epoch: 30 [16768/50048]	Loss: 0.9891
Training Epoch: 30 [16896/50048]	Loss: 0.6105
Training Epoch: 30 [17024/50048]	Loss: 0.6906
Training Epoch: 30 [17152/50048]	Loss: 0.6005
Training Epoch: 30 [17280/50048]	Loss: 0.5679
Training Epoch: 30 [17408/50048]	Loss: 0.5899
Training Epoch: 30 [17536/50048]	Loss: 0.5532
Training Epoch: 30 [17664/50048]	Loss: 0.5931
Training Epoch: 30 [17792/50048]	Loss: 0.8000
Training Epoch: 30 [17920/50048]	Loss: 0.6720
Training Epoch: 30 [18048/50048]	Loss: 0.5054
Training Epoch: 30 [18176/50048]	Loss: 0.6038
Training Epoch: 30 [18304/50048]	Loss: 0.6908
Training Epoch: 30 [18432/50048]	Loss: 0.5969
Training Epoch: 30 [18560/50048]	Loss: 0.5499
Training Epoch: 30 [18688/50048]	Loss: 0.5906
Training Epoch: 30 [18816/50048]	Loss: 0.6558
Training Epoch: 30 [18944/50048]	Loss: 0.4427
Training Epoch: 30 [19072/50048]	Loss: 0.7220
Training Epoch: 30 [19200/50048]	Loss: 0.6329
Training Epoch: 30 [19328/50048]	Loss: 0.6688
Training Epoch: 30 [19456/50048]	Loss: 0.5082
Training Epoch: 30 [19584/50048]	Loss: 0.6723
Training Epoch: 30 [19712/50048]	Loss: 0.5383
Training Epoch: 30 [19840/50048]	Loss: 0.7253
Training Epoch: 30 [19968/50048]	Loss: 0.5217
Training Epoch: 30 [20096/50048]	Loss: 0.6516
Training Epoch: 30 [20224/50048]	Loss: 0.4145
Training Epoch: 30 [20352/50048]	Loss: 0.6600
Training Epoch: 30 [20480/50048]	Loss: 0.6332
Training Epoch: 30 [20608/50048]	Loss: 0.6259
Training Epoch: 30 [20736/50048]	Loss: 0.6541
Training Epoch: 30 [20864/50048]	Loss: 0.6436
Training Epoch: 30 [20992/50048]	Loss: 0.6290
Training Epoch: 30 [21120/50048]	Loss: 0.5548
Training Epoch: 30 [21248/50048]	Loss: 0.5144
Training Epoch: 30 [21376/50048]	Loss: 0.5295
Training Epoch: 30 [21504/50048]	Loss: 0.6347
Training Epoch: 30 [21632/50048]	Loss: 0.6613
Training Epoch: 30 [21760/50048]	Loss: 0.5680
Training Epoch: 30 [21888/50048]	Loss: 0.6361
Training Epoch: 30 [22016/50048]	Loss: 0.7185
Training Epoch: 30 [22144/50048]	Loss: 0.5863
Training Epoch: 30 [22272/50048]	Loss: 0.5559
Training Epoch: 30 [22400/50048]	Loss: 0.5692
Training Epoch: 30 [22528/50048]	Loss: 0.5215
Training Epoch: 30 [22656/50048]	Loss: 0.7812
Training Epoch: 30 [22784/50048]	Loss: 0.7500
Training Epoch: 30 [22912/50048]	Loss: 0.6411
Training Epoch: 30 [23040/50048]	Loss: 0.6669
Training Epoch: 30 [23168/50048]	Loss: 0.5291
Training Epoch: 30 [23296/50048]	Loss: 0.5388
Training Epoch: 30 [23424/50048]	Loss: 0.6225
Training Epoch: 30 [23552/50048]	Loss: 0.5973
Training Epoch: 30 [23680/50048]	Loss: 0.6140
Training Epoch: 30 [23808/50048]	Loss: 0.7934
Training Epoch: 30 [23936/50048]	Loss: 0.6212
Training Epoch: 30 [24064/50048]	Loss: 0.8154
Training Epoch: 30 [24192/50048]	Loss: 0.7524
Training Epoch: 30 [24320/50048]	Loss: 0.4580
Training Epoch: 30 [24448/50048]	Loss: 0.5567
Training Epoch: 30 [24576/50048]	Loss: 0.5063
Training Epoch: 30 [24704/50048]	Loss: 0.6710
Training Epoch: 30 [24832/50048]	Loss: 0.6980
Training Epoch: 30 [24960/50048]	Loss: 0.6073
Training Epoch: 30 [25088/50048]	Loss: 0.6150
Training Epoch: 30 [25216/50048]	Loss: 0.6538
Training Epoch: 30 [25344/50048]	Loss: 0.8781
Training Epoch: 30 [25472/50048]	Loss: 0.6169
Training Epoch: 30 [25600/50048]	Loss: 0.7292
Training Epoch: 30 [25728/50048]	Loss: 0.6672
Training Epoch: 30 [25856/50048]	Loss: 0.4685
Training Epoch: 30 [25984/50048]	Loss: 0.5889
Training Epoch: 30 [26112/50048]	Loss: 0.4215
Training Epoch: 30 [26240/50048]	Loss: 0.7773
Training Epoch: 30 [26368/50048]	Loss: 0.7620
Training Epoch: 30 [26496/50048]	Loss: 0.6325
Training Epoch: 30 [26624/50048]	Loss: 0.5154
Training Epoch: 30 [26752/50048]	Loss: 1.0033
Training Epoch: 30 [26880/50048]	Loss: 0.5813
Training Epoch: 30 [27008/50048]	Loss: 0.8510
Training Epoch: 30 [27136/50048]	Loss: 0.4365
Training Epoch: 30 [27264/50048]	Loss: 0.6138
Training Epoch: 30 [27392/50048]	Loss: 0.6053
Training Epoch: 30 [27520/50048]	Loss: 0.7480
Training Epoch: 30 [27648/50048]	Loss: 0.5362
Training Epoch: 30 [27776/50048]	Loss: 0.7251
Training Epoch: 30 [27904/50048]	Loss: 0.7138
Training Epoch: 30 [28032/50048]	Loss: 0.7415
Training Epoch: 30 [28160/50048]	Loss: 0.5638
Training Epoch: 30 [28288/50048]	Loss: 0.6493
Training Epoch: 30 [28416/50048]	Loss: 0.7489
Training Epoch: 30 [28544/50048]	Loss: 0.6225
Training Epoch: 30 [28672/50048]	Loss: 0.7994
Training Epoch: 30 [28800/50048]	Loss: 0.5920
Training Epoch: 30 [28928/50048]	Loss: 0.7230
Training Epoch: 30 [29056/50048]	Loss: 0.6710
Training Epoch: 30 [29184/50048]	Loss: 0.5914
Training Epoch: 30 [29312/50048]	Loss: 0.8282
Training Epoch: 30 [29440/50048]	Loss: 0.6868
Training Epoch: 30 [29568/50048]	Loss: 0.6875
Training Epoch: 30 [29696/50048]	Loss: 0.5747
Training Epoch: 30 [29824/50048]	Loss: 0.5682
Training Epoch: 30 [29952/50048]	Loss: 0.7495
Training Epoch: 30 [30080/50048]	Loss: 0.5440
Training Epoch: 30 [30208/50048]	Loss: 0.7807
Training Epoch: 30 [30336/50048]	Loss: 0.7366
Training Epoch: 30 [30464/50048]	Loss: 0.6794
Training Epoch: 30 [30592/50048]	Loss: 0.6603
Training Epoch: 30 [30720/50048]	Loss: 0.7632
Training Epoch: 30 [30848/50048]	Loss: 0.6121
Training Epoch: 30 [30976/50048]	Loss: 0.6193
Training Epoch: 30 [31104/50048]	Loss: 0.6547
Training Epoch: 30 [31232/50048]	Loss: 0.6530
Training Epoch: 30 [31360/50048]	Loss: 0.4322
Training Epoch: 30 [31488/50048]	Loss: 0.6336
Training Epoch: 30 [31616/50048]	Loss: 0.5947
Training Epoch: 30 [31744/50048]	Loss: 0.7379
Training Epoch: 30 [31872/50048]	Loss: 0.5799
Training Epoch: 30 [32000/50048]	Loss: 0.7569
Training Epoch: 30 [32128/50048]	Loss: 0.5360
Training Epoch: 30 [32256/50048]	Loss: 0.4800
Training Epoch: 30 [32384/50048]	Loss: 0.6119
Training Epoch: 30 [32512/50048]	Loss: 0.6112
Training Epoch: 30 [32640/50048]	Loss: 0.6032
Training Epoch: 30 [32768/50048]	Loss: 0.6658
Training Epoch: 30 [32896/50048]	Loss: 0.6025
Training Epoch: 30 [33024/50048]	Loss: 0.6364
Training Epoch: 30 [33152/50048]	Loss: 0.7795
Training Epoch: 30 [33280/50048]	Loss: 0.6711
Training Epoch: 30 [33408/50048]	Loss: 0.6385
Training Epoch: 30 [33536/50048]	Loss: 0.7648
Training Epoch: 30 [33664/50048]	Loss: 0.5566
Training Epoch: 30 [33792/50048]	Loss: 0.8307
Training Epoch: 30 [33920/50048]	Loss: 0.6684
Training Epoch: 30 [34048/50048]	Loss: 0.6431
Training Epoch: 30 [34176/50048]	Loss: 0.6740
Training Epoch: 30 [34304/50048]	Loss: 0.6532
Training Epoch: 30 [34432/50048]	Loss: 0.5845
Training Epoch: 30 [34560/50048]	Loss: 0.7842
Training Epoch: 30 [34688/50048]	Loss: 0.6367
Training Epoch: 30 [34816/50048]	Loss: 0.6300
Training Epoch: 30 [34944/50048]	Loss: 0.6045
Training Epoch: 30 [35072/50048]	Loss: 0.7067
Training Epoch: 30 [35200/50048]	Loss: 0.6231
Training Epoch: 30 [35328/50048]	Loss: 0.6443
Training Epoch: 30 [35456/50048]	Loss: 0.6827
Training Epoch: 30 [35584/50048]	Loss: 0.6750
Training Epoch: 30 [35712/50048]	Loss: 0.7328
Training Epoch: 30 [35840/50048]	Loss: 0.6208
Training Epoch: 30 [35968/50048]	Loss: 0.6385
Training Epoch: 30 [36096/50048]	Loss: 0.6175
Training Epoch: 30 [36224/50048]	Loss: 0.6973
Training Epoch: 30 [36352/50048]	Loss: 0.6806
Training Epoch: 30 [36480/50048]	Loss: 0.6722
Training Epoch: 30 [36608/50048]	Loss: 0.7454
Training Epoch: 30 [36736/50048]	Loss: 0.6210
Training Epoch: 30 [36864/50048]	Loss: 0.9151
Training Epoch: 30 [36992/50048]	Loss: 0.6910
Training Epoch: 30 [37120/50048]	Loss: 0.6261
Training Epoch: 30 [37248/50048]	Loss: 0.5525
Training Epoch: 30 [37376/50048]	Loss: 0.7490
Training Epoch: 30 [37504/50048]	Loss: 0.6632
Training Epoch: 30 [37632/50048]	Loss: 0.6315
Training Epoch: 30 [37760/50048]	Loss: 0.6299
Training Epoch: 30 [37888/50048]	Loss: 0.8008
Training Epoch: 30 [38016/50048]	Loss: 0.6087
Training Epoch: 30 [38144/50048]	Loss: 0.7691
Training Epoch: 30 [38272/50048]	Loss: 0.6985
Training Epoch: 30 [38400/50048]	Loss: 0.7064
Training Epoch: 30 [38528/50048]	Loss: 0.7313
Training Epoch: 30 [38656/50048]	Loss: 0.5920
Training Epoch: 30 [38784/50048]	Loss: 0.8104
Training Epoch: 30 [38912/50048]	Loss: 0.6746
Training Epoch: 30 [39040/50048]	Loss: 0.5966
Training Epoch: 30 [39168/50048]	Loss: 0.6411
Training Epoch: 30 [39296/50048]	Loss: 0.7320
Training Epoch: 30 [39424/50048]	Loss: 0.7490
Training Epoch: 30 [39552/50048]	Loss: 0.5187
Training Epoch: 30 [39680/50048]	Loss: 0.7478
Training Epoch: 30 [39808/50048]	Loss: 0.7528
Training Epoch: 30 [39936/50048]	Loss: 0.7072
Training Epoch: 30 [40064/50048]	Loss: 0.7084
Training Epoch: 30 [40192/50048]	Loss: 0.6609
Training Epoch: 30 [40320/50048]	Loss: 0.6898
Training Epoch: 30 [40448/50048]	Loss: 0.7178
Training Epoch: 30 [40576/50048]	Loss: 0.5866
Training Epoch: 30 [40704/50048]	Loss: 0.7960
Training Epoch: 30 [40832/50048]	Loss: 0.7035
Training Epoch: 30 [40960/50048]	Loss: 0.6077
Training Epoch: 30 [41088/50048]	Loss: 0.5923
Training Epoch: 30 [41216/50048]	Loss: 0.6192
Training Epoch: 30 [41344/50048]	Loss: 0.7769
Training Epoch: 30 [41472/50048]	Loss: 0.7750
Training Epoch: 30 [41600/50048]	Loss: 0.5614
Training Epoch: 30 [41728/50048]	Loss: 0.8018
Training Epoch: 30 [41856/50048]	Loss: 0.6416
Training Epoch: 30 [41984/50048]	Loss: 0.6013
Training Epoch: 30 [42112/50048]	Loss: 0.6400
Training Epoch: 30 [42240/50048]	Loss: 0.7481
Training Epoch: 30 [42368/50048]	Loss: 0.7201
Training Epoch: 30 [42496/50048]	Loss: 0.6053
Training Epoch: 30 [42624/50048]	Loss: 0.6597
Training Epoch: 30 [42752/50048]	Loss: 0.8383
Training Epoch: 30 [42880/50048]	Loss: 0.6153
Training Epoch: 30 [43008/50048]	Loss: 0.6622
Training Epoch: 30 [43136/50048]	Loss: 0.7655
Training Epoch: 30 [43264/50048]	Loss: 0.6733
Training Epoch: 30 [43392/50048]	Loss: 0.7910
Training Epoch: 30 [43520/50048]	Loss: 0.8379
Training Epoch: 30 [43648/50048]	Loss: 0.4053
Training Epoch: 30 [43776/50048]	Loss: 0.7110
Training Epoch: 30 [43904/50048]	Loss: 0.5389
Training Epoch: 30 [44032/50048]	Loss: 0.4203
Training Epoch: 30 [44160/50048]	Loss: 0.5873
Training Epoch: 30 [44288/50048]	Loss: 0.5986
Training Epoch: 30 [44416/50048]	Loss: 0.5992
Training Epoch: 30 [44544/50048]	Loss: 0.5438
Training Epoch: 30 [44672/50048]	Loss: 0.7803
Training Epoch: 30 [44800/50048]	Loss: 0.6446
Training Epoch: 30 [44928/50048]	Loss: 0.6897
Training Epoch: 30 [45056/50048]	Loss: 0.7148
Training Epoch: 30 [45184/50048]	Loss: 0.5915
Training Epoch: 30 [45312/50048]	Loss: 0.5780
Training Epoch: 30 [45440/50048]	Loss: 0.6959
Training Epoch: 30 [45568/50048]	Loss: 0.6984
Training Epoch: 30 [45696/50048]	Loss: 0.6612
2022-12-06 04:23:57,737 [ZeusDataLoader(train)] train epoch 31 done: time=86.34 energy=10496.52
2022-12-06 04:23:57,739 [ZeusDataLoader(eval)] Epoch 31 begin.
Training Epoch: 30 [45824/50048]	Loss: 0.7423
Training Epoch: 30 [45952/50048]	Loss: 0.7432
Training Epoch: 30 [46080/50048]	Loss: 0.7540
Training Epoch: 30 [46208/50048]	Loss: 0.7484
Training Epoch: 30 [46336/50048]	Loss: 0.6638
Training Epoch: 30 [46464/50048]	Loss: 0.7112
Training Epoch: 30 [46592/50048]	Loss: 0.7059
Training Epoch: 30 [46720/50048]	Loss: 0.7928
Training Epoch: 30 [46848/50048]	Loss: 0.8398
Training Epoch: 30 [46976/50048]	Loss: 0.7388
Training Epoch: 30 [47104/50048]	Loss: 0.6860
Training Epoch: 30 [47232/50048]	Loss: 0.7637
Training Epoch: 30 [47360/50048]	Loss: 0.7413
Training Epoch: 30 [47488/50048]	Loss: 0.6425
Training Epoch: 30 [47616/50048]	Loss: 0.6786
Training Epoch: 30 [47744/50048]	Loss: 0.7830
Training Epoch: 30 [47872/50048]	Loss: 0.5735
Training Epoch: 30 [48000/50048]	Loss: 0.7251
Training Epoch: 30 [48128/50048]	Loss: 0.7522
Training Epoch: 30 [48256/50048]	Loss: 0.9529
Training Epoch: 30 [48384/50048]	Loss: 0.8636
Training Epoch: 30 [48512/50048]	Loss: 0.7399
Training Epoch: 30 [48640/50048]	Loss: 0.6268
Training Epoch: 30 [48768/50048]	Loss: 0.7172
Training Epoch: 30 [48896/50048]	Loss: 0.6925
Training Epoch: 30 [49024/50048]	Loss: 0.7095
Training Epoch: 30 [49152/50048]	Loss: 0.6539
Training Epoch: 30 [49280/50048]	Loss: 0.7510
Training Epoch: 30 [49408/50048]	Loss: 0.6258
Training Epoch: 30 [49536/50048]	Loss: 0.7205
Training Epoch: 30 [49664/50048]	Loss: 0.5656
Training Epoch: 30 [49792/50048]	Loss: 0.6678
Training Epoch: 30 [49920/50048]	Loss: 0.9743
Training Epoch: 30 [50048/50048]	Loss: 0.8935
2022-12-06 09:24:01.413 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:24:01,454 [ZeusDataLoader(eval)] eval epoch 31 done: time=3.71 energy=452.41
2022-12-06 04:24:01,455 [ZeusDataLoader(train)] Up to epoch 31: time=2798.56, energy=339604.50, cost=414676.03
2022-12-06 04:24:01,455 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:24:01,455 [ZeusDataLoader(train)] Expected next epoch: time=2888.36, energy=350402.52, cost=427932.42
2022-12-06 04:24:01,456 [ZeusDataLoader(train)] Epoch 32 begin.
Validation Epoch: 30, Average loss: 0.0126, Accuracy: 0.6112
2022-12-06 04:24:01,633 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:24:01,633 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:24:01.637 [ZeusMonitor] Monitor started.
2022-12-06 09:24:01.637 [ZeusMonitor] Running indefinitely. 2022-12-06 09:24:01.637 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:24:01.637 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e32+gpu0.power.log
Training Epoch: 31 [128/50048]	Loss: 0.7072
Training Epoch: 31 [256/50048]	Loss: 0.6202
Training Epoch: 31 [384/50048]	Loss: 0.5495
Training Epoch: 31 [512/50048]	Loss: 0.4387
Training Epoch: 31 [640/50048]	Loss: 0.3444
Training Epoch: 31 [768/50048]	Loss: 0.6209
Training Epoch: 31 [896/50048]	Loss: 0.5633
Training Epoch: 31 [1024/50048]	Loss: 0.7513
Training Epoch: 31 [1152/50048]	Loss: 0.4787
Training Epoch: 31 [1280/50048]	Loss: 0.4656
Training Epoch: 31 [1408/50048]	Loss: 0.5121
Training Epoch: 31 [1536/50048]	Loss: 0.4333
Training Epoch: 31 [1664/50048]	Loss: 0.6252
Training Epoch: 31 [1792/50048]	Loss: 0.5232
Training Epoch: 31 [1920/50048]	Loss: 0.4672
Training Epoch: 31 [2048/50048]	Loss: 0.6685
Training Epoch: 31 [2176/50048]	Loss: 0.6342
Training Epoch: 31 [2304/50048]	Loss: 0.5228
Training Epoch: 31 [2432/50048]	Loss: 0.5256
Training Epoch: 31 [2560/50048]	Loss: 0.6107
Training Epoch: 31 [2688/50048]	Loss: 0.5814
Training Epoch: 31 [2816/50048]	Loss: 0.6641
Training Epoch: 31 [2944/50048]	Loss: 0.6385
Training Epoch: 31 [3072/50048]	Loss: 0.5030
Training Epoch: 31 [3200/50048]	Loss: 0.4235
Training Epoch: 31 [3328/50048]	Loss: 0.5231
Training Epoch: 31 [3456/50048]	Loss: 0.6617
Training Epoch: 31 [3584/50048]	Loss: 0.5348
Training Epoch: 31 [3712/50048]	Loss: 0.4663
Training Epoch: 31 [3840/50048]	Loss: 0.4391
Training Epoch: 31 [3968/50048]	Loss: 0.4607
Training Epoch: 31 [4096/50048]	Loss: 0.6010
Training Epoch: 31 [4224/50048]	Loss: 0.4410
Training Epoch: 31 [4352/50048]	Loss: 0.4922
Training Epoch: 31 [4480/50048]	Loss: 0.6128
Training Epoch: 31 [4608/50048]	Loss: 0.4901
Training Epoch: 31 [4736/50048]	Loss: 0.5888
Training Epoch: 31 [4864/50048]	Loss: 0.5456
Training Epoch: 31 [4992/50048]	Loss: 0.5171
Training Epoch: 31 [5120/50048]	Loss: 0.5755
Training Epoch: 31 [5248/50048]	Loss: 0.6333
Training Epoch: 31 [5376/50048]	Loss: 0.7052
Training Epoch: 31 [5504/50048]	Loss: 0.6241
Training Epoch: 31 [5632/50048]	Loss: 0.4930
Training Epoch: 31 [5760/50048]	Loss: 0.6749
Training Epoch: 31 [5888/50048]	Loss: 0.5321
Training Epoch: 31 [6016/50048]	Loss: 0.6268
Training Epoch: 31 [6144/50048]	Loss: 0.5920
Training Epoch: 31 [6272/50048]	Loss: 0.6570
Training Epoch: 31 [6400/50048]	Loss: 0.7492
Training Epoch: 31 [6528/50048]	Loss: 0.6958
Training Epoch: 31 [6656/50048]	Loss: 0.7220
Training Epoch: 31 [6784/50048]	Loss: 0.6027
Training Epoch: 31 [6912/50048]	Loss: 0.5491
Training Epoch: 31 [7040/50048]	Loss: 0.5889
Training Epoch: 31 [7168/50048]	Loss: 0.5181
Training Epoch: 31 [7296/50048]	Loss: 0.6211
Training Epoch: 31 [7424/50048]	Loss: 0.4942
Training Epoch: 31 [7552/50048]	Loss: 0.3975
Training Epoch: 31 [7680/50048]	Loss: 0.6666
Training Epoch: 31 [7808/50048]	Loss: 0.5456
Training Epoch: 31 [7936/50048]	Loss: 0.6517
Training Epoch: 31 [8064/50048]	Loss: 0.6493
Training Epoch: 31 [8192/50048]	Loss: 0.5202
Training Epoch: 31 [8320/50048]	Loss: 0.5917
Training Epoch: 31 [8448/50048]	Loss: 0.4929
Training Epoch: 31 [8576/50048]	Loss: 0.6988
Training Epoch: 31 [8704/50048]	Loss: 0.5640
Training Epoch: 31 [8832/50048]	Loss: 0.6194
Training Epoch: 31 [8960/50048]	Loss: 0.5309
Training Epoch: 31 [9088/50048]	Loss: 0.5050
Training Epoch: 31 [9216/50048]	Loss: 0.5371
Training Epoch: 31 [9344/50048]	Loss: 0.5327
Training Epoch: 31 [9472/50048]	Loss: 0.6921
Training Epoch: 31 [9600/50048]	Loss: 0.4278
Training Epoch: 31 [9728/50048]	Loss: 0.7168
Training Epoch: 31 [9856/50048]	Loss: 0.5918
Training Epoch: 31 [9984/50048]	Loss: 0.7088
Training Epoch: 31 [10112/50048]	Loss: 0.4477
Training Epoch: 31 [10240/50048]	Loss: 0.6847
Training Epoch: 31 [10368/50048]	Loss: 0.4652
Training Epoch: 31 [10496/50048]	Loss: 0.4479
Training Epoch: 31 [10624/50048]	Loss: 0.5953
Training Epoch: 31 [10752/50048]	Loss: 0.7121
Training Epoch: 31 [10880/50048]	Loss: 0.5063
Training Epoch: 31 [11008/50048]	Loss: 0.5182
Training Epoch: 31 [11136/50048]	Loss: 0.7156
Training Epoch: 31 [11264/50048]	Loss: 0.6452
Training Epoch: 31 [11392/50048]	Loss: 0.4577
Training Epoch: 31 [11520/50048]	Loss: 0.5406
Training Epoch: 31 [11648/50048]	Loss: 0.6085
Training Epoch: 31 [11776/50048]	Loss: 0.5750
Training Epoch: 31 [11904/50048]	Loss: 0.5728
Training Epoch: 31 [12032/50048]	Loss: 0.6585
Training Epoch: 31 [12160/50048]	Loss: 0.5067
Training Epoch: 31 [12288/50048]	Loss: 0.6873
Training Epoch: 31 [12416/50048]	Loss: 0.6286
Training Epoch: 31 [12544/50048]	Loss: 0.5774
Training Epoch: 31 [12672/50048]	Loss: 0.7282
Training Epoch: 31 [12800/50048]	Loss: 0.6299
Training Epoch: 31 [12928/50048]	Loss: 0.5934
Training Epoch: 31 [13056/50048]	Loss: 0.6682
Training Epoch: 31 [13184/50048]	Loss: 0.6102
Training Epoch: 31 [13312/50048]	Loss: 0.6599
Training Epoch: 31 [13440/50048]	Loss: 0.5976
Training Epoch: 31 [13568/50048]	Loss: 0.5859
Training Epoch: 31 [13696/50048]	Loss: 0.6720
Training Epoch: 31 [13824/50048]	Loss: 0.5033
Training Epoch: 31 [13952/50048]	Loss: 0.5021
Training Epoch: 31 [14080/50048]	Loss: 0.5171
Training Epoch: 31 [14208/50048]	Loss: 0.5347
Training Epoch: 31 [14336/50048]	Loss: 0.5461
Training Epoch: 31 [14464/50048]	Loss: 0.7741
Training Epoch: 31 [14592/50048]	Loss: 0.5960
Training Epoch: 31 [14720/50048]	Loss: 0.5918
Training Epoch: 31 [14848/50048]	Loss: 0.4007
Training Epoch: 31 [14976/50048]	Loss: 0.5784
Training Epoch: 31 [15104/50048]	Loss: 0.5463
Training Epoch: 31 [15232/50048]	Loss: 0.5008
Training Epoch: 31 [15360/50048]	Loss: 0.4952
Training Epoch: 31 [15488/50048]	Loss: 0.5061
Training Epoch: 31 [15616/50048]	Loss: 0.6919
Training Epoch: 31 [15744/50048]	Loss: 0.5956
Training Epoch: 31 [15872/50048]	Loss: 0.6229
Training Epoch: 31 [16000/50048]	Loss: 0.6421
Training Epoch: 31 [16128/50048]	Loss: 0.4775
Training Epoch: 31 [16256/50048]	Loss: 0.7152
Training Epoch: 31 [16384/50048]	Loss: 0.6046
Training Epoch: 31 [16512/50048]	Loss: 0.7434
Training Epoch: 31 [16640/50048]	Loss: 0.5588
Training Epoch: 31 [16768/50048]	Loss: 0.6446
Training Epoch: 31 [16896/50048]	Loss: 0.5142
Training Epoch: 31 [17024/50048]	Loss: 0.7914
Training Epoch: 31 [17152/50048]	Loss: 0.5009
Training Epoch: 31 [17280/50048]	Loss: 0.6477
Training Epoch: 31 [17408/50048]	Loss: 0.6951
Training Epoch: 31 [17536/50048]	Loss: 0.6108
Training Epoch: 31 [17664/50048]	Loss: 0.5367
Training Epoch: 31 [17792/50048]	Loss: 0.5907
Training Epoch: 31 [17920/50048]	Loss: 0.6272
Training Epoch: 31 [18048/50048]	Loss: 0.6452
Training Epoch: 31 [18176/50048]	Loss: 0.6816
Training Epoch: 31 [18304/50048]	Loss: 0.5847
Training Epoch: 31 [18432/50048]	Loss: 0.5877
Training Epoch: 31 [18560/50048]	Loss: 0.7505
Training Epoch: 31 [18688/50048]	Loss: 0.4796
Training Epoch: 31 [18816/50048]	Loss: 0.4442
Training Epoch: 31 [18944/50048]	Loss: 0.6405
Training Epoch: 31 [19072/50048]	Loss: 0.7778
Training Epoch: 31 [19200/50048]	Loss: 0.6708
Training Epoch: 31 [19328/50048]	Loss: 0.7240
Training Epoch: 31 [19456/50048]	Loss: 0.6188
Training Epoch: 31 [19584/50048]	Loss: 0.6545
Training Epoch: 31 [19712/50048]	Loss: 0.8300
Training Epoch: 31 [19840/50048]	Loss: 0.7779
Training Epoch: 31 [19968/50048]	Loss: 0.7364
Training Epoch: 31 [20096/50048]	Loss: 0.6309
Training Epoch: 31 [20224/50048]	Loss: 0.6508
Training Epoch: 31 [20352/50048]	Loss: 0.5158
Training Epoch: 31 [20480/50048]	Loss: 0.6042
Training Epoch: 31 [20608/50048]	Loss: 0.7851
Training Epoch: 31 [20736/50048]	Loss: 0.6085
Training Epoch: 31 [20864/50048]	Loss: 0.6221
Training Epoch: 31 [20992/50048]	Loss: 0.6274
Training Epoch: 31 [21120/50048]	Loss: 0.7401
Training Epoch: 31 [21248/50048]	Loss: 0.6390
Training Epoch: 31 [21376/50048]	Loss: 0.7177
Training Epoch: 31 [21504/50048]	Loss: 0.6277
Training Epoch: 31 [21632/50048]	Loss: 0.5202
Training Epoch: 31 [21760/50048]	Loss: 0.5444
Training Epoch: 31 [21888/50048]	Loss: 0.5539
Training Epoch: 31 [22016/50048]	Loss: 0.4369
Training Epoch: 31 [22144/50048]	Loss: 0.5725
Training Epoch: 31 [22272/50048]	Loss: 0.6199
Training Epoch: 31 [22400/50048]	Loss: 0.6995
Training Epoch: 31 [22528/50048]	Loss: 0.6093
Training Epoch: 31 [22656/50048]	Loss: 0.7176
Training Epoch: 31 [22784/50048]	Loss: 0.6433
Training Epoch: 31 [22912/50048]	Loss: 0.6741
Training Epoch: 31 [23040/50048]	Loss: 0.5532
Training Epoch: 31 [23168/50048]	Loss: 0.6548
Training Epoch: 31 [23296/50048]	Loss: 0.6742
Training Epoch: 31 [23424/50048]	Loss: 0.5869
Training Epoch: 31 [23552/50048]	Loss: 0.6142
Training Epoch: 31 [23680/50048]	Loss: 0.6538
Training Epoch: 31 [23808/50048]	Loss: 0.6631
Training Epoch: 31 [23936/50048]	Loss: 0.5377
Training Epoch: 31 [24064/50048]	Loss: 0.6244
Training Epoch: 31 [24192/50048]	Loss: 0.5194
Training Epoch: 31 [24320/50048]	Loss: 0.7826
Training Epoch: 31 [24448/50048]	Loss: 0.6142
Training Epoch: 31 [24576/50048]	Loss: 0.5010
Training Epoch: 31 [24704/50048]	Loss: 0.6582
Training Epoch: 31 [24832/50048]	Loss: 0.5519
Training Epoch: 31 [24960/50048]	Loss: 0.6248
Training Epoch: 31 [25088/50048]	Loss: 0.5302
Training Epoch: 31 [25216/50048]	Loss: 0.6131
Training Epoch: 31 [25344/50048]	Loss: 0.5302
Training Epoch: 31 [25472/50048]	Loss: 0.7962
Training Epoch: 31 [25600/50048]	Loss: 0.5008
Training Epoch: 31 [25728/50048]	Loss: 0.5743
Training Epoch: 31 [25856/50048]	Loss: 0.6864
Training Epoch: 31 [25984/50048]	Loss: 0.7337
Training Epoch: 31 [26112/50048]	Loss: 0.6251
Training Epoch: 31 [26240/50048]	Loss: 0.6330
Training Epoch: 31 [26368/50048]	Loss: 0.5453
Training Epoch: 31 [26496/50048]	Loss: 0.6421
Training Epoch: 31 [26624/50048]	Loss: 0.6193
Training Epoch: 31 [26752/50048]	Loss: 0.6131
Training Epoch: 31 [26880/50048]	Loss: 0.6802
Training Epoch: 31 [27008/50048]	Loss: 0.5762
Training Epoch: 31 [27136/50048]	Loss: 0.5116
Training Epoch: 31 [27264/50048]	Loss: 0.7667
Training Epoch: 31 [27392/50048]	Loss: 0.4780
Training Epoch: 31 [27520/50048]	Loss: 0.5563
Training Epoch: 31 [27648/50048]	Loss: 0.6368
Training Epoch: 31 [27776/50048]	Loss: 0.5389
Training Epoch: 31 [27904/50048]	Loss: 0.6867
Training Epoch: 31 [28032/50048]	Loss: 0.6538
Training Epoch: 31 [28160/50048]	Loss: 0.4593
Training Epoch: 31 [28288/50048]	Loss: 0.6103
Training Epoch: 31 [28416/50048]	Loss: 0.5632
Training Epoch: 31 [28544/50048]	Loss: 0.5745
Training Epoch: 31 [28672/50048]	Loss: 0.6196
Training Epoch: 31 [28800/50048]	Loss: 0.7064
Training Epoch: 31 [28928/50048]	Loss: 0.6108
Training Epoch: 31 [29056/50048]	Loss: 0.5703
Training Epoch: 31 [29184/50048]	Loss: 0.6342
Training Epoch: 31 [29312/50048]	Loss: 0.5757
Training Epoch: 31 [29440/50048]	Loss: 0.6800
Training Epoch: 31 [29568/50048]	Loss: 0.7140
Training Epoch: 31 [29696/50048]	Loss: 0.6364
Training Epoch: 31 [29824/50048]	Loss: 0.6304
Training Epoch: 31 [29952/50048]	Loss: 0.5129
Training Epoch: 31 [30080/50048]	Loss: 0.7229
Training Epoch: 31 [30208/50048]	Loss: 0.5228
Training Epoch: 31 [30336/50048]	Loss: 0.5357
Training Epoch: 31 [30464/50048]	Loss: 0.5846
Training Epoch: 31 [30592/50048]	Loss: 0.6847
Training Epoch: 31 [30720/50048]	Loss: 0.6843
Training Epoch: 31 [30848/50048]	Loss: 0.5482
Training Epoch: 31 [30976/50048]	Loss: 0.5601
Training Epoch: 31 [31104/50048]	Loss: 0.7626
Training Epoch: 31 [31232/50048]	Loss: 0.7792
Training Epoch: 31 [31360/50048]	Loss: 0.7522
Training Epoch: 31 [31488/50048]	Loss: 0.6326
Training Epoch: 31 [31616/50048]	Loss: 0.5333
Training Epoch: 31 [31744/50048]	Loss: 0.6514
Training Epoch: 31 [31872/50048]	Loss: 0.6546
Training Epoch: 31 [32000/50048]	Loss: 0.5911
Training Epoch: 31 [32128/50048]	Loss: 0.7762
Training Epoch: 31 [32256/50048]	Loss: 0.8618
Training Epoch: 31 [32384/50048]	Loss: 0.8227
Training Epoch: 31 [32512/50048]	Loss: 0.7087
Training Epoch: 31 [32640/50048]	Loss: 0.5911
Training Epoch: 31 [32768/50048]	Loss: 0.8276
Training Epoch: 31 [32896/50048]	Loss: 0.5565
Training Epoch: 31 [33024/50048]	Loss: 0.5590
Training Epoch: 31 [33152/50048]	Loss: 0.5786
Training Epoch: 31 [33280/50048]	Loss: 0.6455
Training Epoch: 31 [33408/50048]	Loss: 0.7475
Training Epoch: 31 [33536/50048]	Loss: 0.5795
Training Epoch: 31 [33664/50048]	Loss: 0.7582
Training Epoch: 31 [33792/50048]	Loss: 0.5750
Training Epoch: 31 [33920/50048]	Loss: 0.7196
Training Epoch: 31 [34048/50048]	Loss: 0.6981
Training Epoch: 31 [34176/50048]	Loss: 0.6455
Training Epoch: 31 [34304/50048]	Loss: 0.7672
Training Epoch: 31 [34432/50048]	Loss: 0.9368
Training Epoch: 31 [34560/50048]	Loss: 0.6436
Training Epoch: 31 [34688/50048]	Loss: 0.6527
Training Epoch: 31 [34816/50048]	Loss: 0.6595
Training Epoch: 31 [34944/50048]	Loss: 0.7122
Training Epoch: 31 [35072/50048]	Loss: 0.7271
Training Epoch: 31 [35200/50048]	Loss: 0.7595
Training Epoch: 31 [35328/50048]	Loss: 0.4921
Training Epoch: 31 [35456/50048]	Loss: 0.4995
Training Epoch: 31 [35584/50048]	Loss: 0.7032
Training Epoch: 31 [35712/50048]	Loss: 0.5097
Training Epoch: 31 [35840/50048]	Loss: 0.4904
Training Epoch: 31 [35968/50048]	Loss: 0.6729
Training Epoch: 31 [36096/50048]	Loss: 0.5804
Training Epoch: 31 [36224/50048]	Loss: 0.6591
Training Epoch: 31 [36352/50048]	Loss: 0.5339
Training Epoch: 31 [36480/50048]	Loss: 0.6984
Training Epoch: 31 [36608/50048]	Loss: 0.5487
Training Epoch: 31 [36736/50048]	Loss: 1.0671
Training Epoch: 31 [36864/50048]	Loss: 0.7562
Training Epoch: 31 [36992/50048]	Loss: 0.6078
Training Epoch: 31 [37120/50048]	Loss: 0.7284
Training Epoch: 31 [37248/50048]	Loss: 0.7784
Training Epoch: 31 [37376/50048]	Loss: 0.5225
Training Epoch: 31 [37504/50048]	Loss: 0.5904
Training Epoch: 31 [37632/50048]	Loss: 0.7674
Training Epoch: 31 [37760/50048]	Loss: 0.7619
Training Epoch: 31 [37888/50048]	Loss: 0.6173
Training Epoch: 31 [38016/50048]	Loss: 0.6865
Training Epoch: 31 [38144/50048]	Loss: 0.4737
Training Epoch: 31 [38272/50048]	Loss: 0.6465
Training Epoch: 31 [38400/50048]	Loss: 0.7453
Training Epoch: 31 [38528/50048]	Loss: 0.6230
Training Epoch: 31 [38656/50048]	Loss: 0.5114
Training Epoch: 31 [38784/50048]	Loss: 0.6538
Training Epoch: 31 [38912/50048]	Loss: 0.7132
Training Epoch: 31 [39040/50048]	Loss: 0.6339
Training Epoch: 31 [39168/50048]	Loss: 0.7619
Training Epoch: 31 [39296/50048]	Loss: 0.5598
Training Epoch: 31 [39424/50048]	Loss: 0.7657
Training Epoch: 31 [39552/50048]	Loss: 0.5473
Training Epoch: 31 [39680/50048]	Loss: 0.6225
Training Epoch: 31 [39808/50048]	Loss: 0.9034
Training Epoch: 31 [39936/50048]	Loss: 0.6802
Training Epoch: 31 [40064/50048]	Loss: 0.7324
Training Epoch: 31 [40192/50048]	Loss: 0.5498
Training Epoch: 31 [40320/50048]	Loss: 0.6279
Training Epoch: 31 [40448/50048]	Loss: 0.6815
Training Epoch: 31 [40576/50048]	Loss: 0.6243
Training Epoch: 31 [40704/50048]	Loss: 0.6111
Training Epoch: 31 [40832/50048]	Loss: 0.6727
Training Epoch: 31 [40960/50048]	Loss: 0.4558
Training Epoch: 31 [41088/50048]	Loss: 0.6305
Training Epoch: 31 [41216/50048]	Loss: 0.4269
Training Epoch: 31 [41344/50048]	Loss: 0.6848
Training Epoch: 31 [41472/50048]	Loss: 0.6821
Training Epoch: 31 [41600/50048]	Loss: 0.7841
Training Epoch: 31 [41728/50048]	Loss: 0.6479
Training Epoch: 31 [41856/50048]	Loss: 0.6634
Training Epoch: 31 [41984/50048]	Loss: 0.5882
Training Epoch: 31 [42112/50048]	Loss: 0.6288
Training Epoch: 31 [42240/50048]	Loss: 0.4329
Training Epoch: 31 [42368/50048]	Loss: 0.5283
Training Epoch: 31 [42496/50048]	Loss: 0.6114
Training Epoch: 31 [42624/50048]	Loss: 0.6443
Training Epoch: 31 [42752/50048]	Loss: 0.6979
Training Epoch: 31 [42880/50048]	Loss: 0.4284
Training Epoch: 31 [43008/50048]	Loss: 0.5646
Training Epoch: 31 [43136/50048]	Loss: 0.6481
Training Epoch: 31 [43264/50048]	Loss: 0.5771
Training Epoch: 31 [43392/50048]	Loss: 0.6641
Training Epoch: 31 [43520/50048]	Loss: 0.6456
Training Epoch: 31 [43648/50048]	Loss: 0.5754
Training Epoch: 31 [43776/50048]	Loss: 0.6430
Training Epoch: 31 [43904/50048]	Loss: 0.5773
Training Epoch: 31 [44032/50048]	Loss: 0.6408
Training Epoch: 31 [44160/50048]	Loss: 0.6109
Training Epoch: 31 [44288/50048]	Loss: 1.0194
Training Epoch: 31 [44416/50048]	Loss: 0.7702
Training Epoch: 31 [44544/50048]	Loss: 0.8032
Training Epoch: 31 [44672/50048]	Loss: 0.4724
Training Epoch: 31 [44800/50048]	Loss: 0.7818
Training Epoch: 31 [44928/50048]	Loss: 0.7861
Training Epoch: 31 [45056/50048]	Loss: 0.7960
Training Epoch: 31 [45184/50048]	Loss: 0.6012
Training Epoch: 31 [45312/50048]	Loss: 0.7016
Training Epoch: 31 [45440/50048]	Loss: 0.8194
Training Epoch: 31 [45568/50048]	Loss: 0.6914
Training Epoch: 31 [45696/50048]	Loss: 0.5101
2022-12-06 04:25:28,334 [ZeusDataLoader(train)] train epoch 32 done: time=86.87 energy=10528.65
2022-12-06 04:25:28,335 [ZeusDataLoader(eval)] Epoch 32 begin.
Training Epoch: 31 [45824/50048]	Loss: 0.6565
Training Epoch: 31 [45952/50048]	Loss: 0.6349
Training Epoch: 31 [46080/50048]	Loss: 0.6874
Training Epoch: 31 [46208/50048]	Loss: 0.7178
Training Epoch: 31 [46336/50048]	Loss: 0.5145
Training Epoch: 31 [46464/50048]	Loss: 0.5217
Training Epoch: 31 [46592/50048]	Loss: 0.5197
Training Epoch: 31 [46720/50048]	Loss: 0.7644
Training Epoch: 31 [46848/50048]	Loss: 0.7017
Training Epoch: 31 [46976/50048]	Loss: 0.5713
Training Epoch: 31 [47104/50048]	Loss: 0.5441
Training Epoch: 31 [47232/50048]	Loss: 0.8232
Training Epoch: 31 [47360/50048]	Loss: 0.9258
Training Epoch: 31 [47488/50048]	Loss: 0.7773
Training Epoch: 31 [47616/50048]	Loss: 0.4223
Training Epoch: 31 [47744/50048]	Loss: 0.7835
Training Epoch: 31 [47872/50048]	Loss: 0.6264
Training Epoch: 31 [48000/50048]	Loss: 0.6821
Training Epoch: 31 [48128/50048]	Loss: 0.6402
Training Epoch: 31 [48256/50048]	Loss: 0.4992
Training Epoch: 31 [48384/50048]	Loss: 0.6942
Training Epoch: 31 [48512/50048]	Loss: 0.6636
Training Epoch: 31 [48640/50048]	Loss: 0.5460
Training Epoch: 31 [48768/50048]	Loss: 0.5522
Training Epoch: 31 [48896/50048]	Loss: 0.6314
Training Epoch: 31 [49024/50048]	Loss: 0.6145
Training Epoch: 31 [49152/50048]	Loss: 0.5415
Training Epoch: 31 [49280/50048]	Loss: 0.5115
Training Epoch: 31 [49408/50048]	Loss: 0.5707
Training Epoch: 31 [49536/50048]	Loss: 0.6254
Training Epoch: 31 [49664/50048]	Loss: 0.5478
Training Epoch: 31 [49792/50048]	Loss: 0.7019
Training Epoch: 31 [49920/50048]	Loss: 0.7108
Training Epoch: 31 [50048/50048]	Loss: 0.6955
2022-12-06 09:25:32.046 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:25:32,084 [ZeusDataLoader(eval)] eval epoch 32 done: time=3.74 energy=455.82
2022-12-06 04:25:32,084 [ZeusDataLoader(train)] Up to epoch 32: time=2889.16, energy=350588.97, cost=428096.42
2022-12-06 04:25:32,084 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:25:32,084 [ZeusDataLoader(train)] Expected next epoch: time=2978.96, energy=361386.99, cost=441352.80
2022-12-06 04:25:32,085 [ZeusDataLoader(train)] Epoch 33 begin.
Validation Epoch: 31, Average loss: 0.0124, Accuracy: 0.6203
2022-12-06 04:25:32,266 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:25:32,266 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:25:32.268 [ZeusMonitor] Monitor started.
2022-12-06 09:25:32.269 [ZeusMonitor] Running indefinitely. 2022-12-06 09:25:32.269 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:25:32.269 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e33+gpu0.power.log
Training Epoch: 32 [128/50048]	Loss: 0.5660
Training Epoch: 32 [256/50048]	Loss: 0.5203
Training Epoch: 32 [384/50048]	Loss: 0.5253
Training Epoch: 32 [512/50048]	Loss: 0.6595
Training Epoch: 32 [640/50048]	Loss: 0.4485
Training Epoch: 32 [768/50048]	Loss: 0.5104
Training Epoch: 32 [896/50048]	Loss: 0.4446
Training Epoch: 32 [1024/50048]	Loss: 0.5629
Training Epoch: 32 [1152/50048]	Loss: 0.4117
Training Epoch: 32 [1280/50048]	Loss: 0.4498
Training Epoch: 32 [1408/50048]	Loss: 0.4345
Training Epoch: 32 [1536/50048]	Loss: 0.4888
Training Epoch: 32 [1664/50048]	Loss: 0.4729
Training Epoch: 32 [1792/50048]	Loss: 0.6370
Training Epoch: 32 [1920/50048]	Loss: 0.6375
Training Epoch: 32 [2048/50048]	Loss: 0.5457
Training Epoch: 32 [2176/50048]	Loss: 0.6357
Training Epoch: 32 [2304/50048]	Loss: 0.6194
Training Epoch: 32 [2432/50048]	Loss: 0.6180
Training Epoch: 32 [2560/50048]	Loss: 0.4400
Training Epoch: 32 [2688/50048]	Loss: 0.6863
Training Epoch: 32 [2816/50048]	Loss: 0.5806
Training Epoch: 32 [2944/50048]	Loss: 0.4734
Training Epoch: 32 [3072/50048]	Loss: 0.6038
Training Epoch: 32 [3200/50048]	Loss: 0.7318
Training Epoch: 32 [3328/50048]	Loss: 0.7119
Training Epoch: 32 [3456/50048]	Loss: 0.4881
Training Epoch: 32 [3584/50048]	Loss: 0.6226
Training Epoch: 32 [3712/50048]	Loss: 0.4344
Training Epoch: 32 [3840/50048]	Loss: 0.5368
Training Epoch: 32 [3968/50048]	Loss: 0.5604
Training Epoch: 32 [4096/50048]	Loss: 0.4458
Training Epoch: 32 [4224/50048]	Loss: 0.4395
Training Epoch: 32 [4352/50048]	Loss: 0.5812
Training Epoch: 32 [4480/50048]	Loss: 0.4214
Training Epoch: 32 [4608/50048]	Loss: 0.6881
Training Epoch: 32 [4736/50048]	Loss: 0.4934
Training Epoch: 32 [4864/50048]	Loss: 0.5495
Training Epoch: 32 [4992/50048]	Loss: 0.9045
Training Epoch: 32 [5120/50048]	Loss: 0.5169
Training Epoch: 32 [5248/50048]	Loss: 0.7010
Training Epoch: 32 [5376/50048]	Loss: 0.5010
Training Epoch: 32 [5504/50048]	Loss: 0.6352
Training Epoch: 32 [5632/50048]	Loss: 0.5299
Training Epoch: 32 [5760/50048]	Loss: 0.6661
Training Epoch: 32 [5888/50048]	Loss: 0.4297
Training Epoch: 32 [6016/50048]	Loss: 0.4135
Training Epoch: 32 [6144/50048]	Loss: 0.6018
Training Epoch: 32 [6272/50048]	Loss: 0.7313
Training Epoch: 32 [6400/50048]	Loss: 0.5798
Training Epoch: 32 [6528/50048]	Loss: 0.5055
Training Epoch: 32 [6656/50048]	Loss: 0.5254
Training Epoch: 32 [6784/50048]	Loss: 0.5748
Training Epoch: 32 [6912/50048]	Loss: 0.5734
Training Epoch: 32 [7040/50048]	Loss: 0.6443
Training Epoch: 32 [7168/50048]	Loss: 0.4648
Training Epoch: 32 [7296/50048]	Loss: 0.5403
Training Epoch: 32 [7424/50048]	Loss: 0.4458
Training Epoch: 32 [7552/50048]	Loss: 0.6678
Training Epoch: 32 [7680/50048]	Loss: 0.5045
Training Epoch: 32 [7808/50048]	Loss: 0.5488
Training Epoch: 32 [7936/50048]	Loss: 0.6549
Training Epoch: 32 [8064/50048]	Loss: 0.4629
Training Epoch: 32 [8192/50048]	Loss: 0.5663
Training Epoch: 32 [8320/50048]	Loss: 0.4859
Training Epoch: 32 [8448/50048]	Loss: 0.6095
Training Epoch: 32 [8576/50048]	Loss: 0.6325
Training Epoch: 32 [8704/50048]	Loss: 0.4104
Training Epoch: 32 [8832/50048]	Loss: 0.4865
Training Epoch: 32 [8960/50048]	Loss: 0.6540
Training Epoch: 32 [9088/50048]	Loss: 0.6611
Training Epoch: 32 [9216/50048]	Loss: 0.7814
Training Epoch: 32 [9344/50048]	Loss: 0.4665
Training Epoch: 32 [9472/50048]	Loss: 0.3632
Training Epoch: 32 [9600/50048]	Loss: 0.6354
Training Epoch: 32 [9728/50048]	Loss: 0.5467
Training Epoch: 32 [9856/50048]	Loss: 0.5019
Training Epoch: 32 [9984/50048]	Loss: 0.6031
Training Epoch: 32 [10112/50048]	Loss: 0.4719
Training Epoch: 32 [10240/50048]	Loss: 0.6459
Training Epoch: 32 [10368/50048]	Loss: 0.5182
Training Epoch: 32 [10496/50048]	Loss: 0.5818
Training Epoch: 32 [10624/50048]	Loss: 0.3983
Training Epoch: 32 [10752/50048]	Loss: 0.5264
Training Epoch: 32 [10880/50048]	Loss: 0.6242
Training Epoch: 32 [11008/50048]	Loss: 0.5418
Training Epoch: 32 [11136/50048]	Loss: 0.6444
Training Epoch: 32 [11264/50048]	Loss: 0.4534
Training Epoch: 32 [11392/50048]	Loss: 0.6078
Training Epoch: 32 [11520/50048]	Loss: 0.7795
Training Epoch: 32 [11648/50048]	Loss: 0.5771
Training Epoch: 32 [11776/50048]	Loss: 0.6416
Training Epoch: 32 [11904/50048]	Loss: 0.5225
Training Epoch: 32 [12032/50048]	Loss: 0.5903
Training Epoch: 32 [12160/50048]	Loss: 0.5871
Training Epoch: 32 [12288/50048]	Loss: 0.5979
Training Epoch: 32 [12416/50048]	Loss: 0.6967
Training Epoch: 32 [12544/50048]	Loss: 0.5378
Training Epoch: 32 [12672/50048]	Loss: 0.4457
Training Epoch: 32 [12800/50048]	Loss: 0.5854
Training Epoch: 32 [12928/50048]	Loss: 0.5858
Training Epoch: 32 [13056/50048]	Loss: 0.5852
Training Epoch: 32 [13184/50048]	Loss: 0.4422
Training Epoch: 32 [13312/50048]	Loss: 0.5092
Training Epoch: 32 [13440/50048]	Loss: 0.6439
Training Epoch: 32 [13568/50048]	Loss: 0.5496
Training Epoch: 32 [13696/50048]	Loss: 0.5112
Training Epoch: 32 [13824/50048]	Loss: 0.5102
Training Epoch: 32 [13952/50048]	Loss: 0.5455
Training Epoch: 32 [14080/50048]	Loss: 0.5144
Training Epoch: 32 [14208/50048]	Loss: 0.4215
Training Epoch: 32 [14336/50048]	Loss: 0.4260
Training Epoch: 32 [14464/50048]	Loss: 0.6468
Training Epoch: 32 [14592/50048]	Loss: 0.4450
Training Epoch: 32 [14720/50048]	Loss: 0.5388
Training Epoch: 32 [14848/50048]	Loss: 0.5455
Training Epoch: 32 [14976/50048]	Loss: 0.4702
Training Epoch: 32 [15104/50048]	Loss: 0.5304
Training Epoch: 32 [15232/50048]	Loss: 0.4868
Training Epoch: 32 [15360/50048]	Loss: 0.6130
Training Epoch: 32 [15488/50048]	Loss: 0.5620
Training Epoch: 32 [15616/50048]	Loss: 0.4603
Training Epoch: 32 [15744/50048]	Loss: 0.6673
Training Epoch: 32 [15872/50048]	Loss: 0.4853
Training Epoch: 32 [16000/50048]	Loss: 0.5026
Training Epoch: 32 [16128/50048]	Loss: 0.4955
Training Epoch: 32 [16256/50048]	Loss: 0.4945
Training Epoch: 32 [16384/50048]	Loss: 0.5266
Training Epoch: 32 [16512/50048]	Loss: 0.7399
Training Epoch: 32 [16640/50048]	Loss: 0.5617
Training Epoch: 32 [16768/50048]	Loss: 0.4690
Training Epoch: 32 [16896/50048]	Loss: 0.5804
Training Epoch: 32 [17024/50048]	Loss: 0.7366
Training Epoch: 32 [17152/50048]	Loss: 0.6140
Training Epoch: 32 [17280/50048]	Loss: 0.4037
Training Epoch: 32 [17408/50048]	Loss: 0.6722
Training Epoch: 32 [17536/50048]	Loss: 0.6362
Training Epoch: 32 [17664/50048]	Loss: 0.5002
Training Epoch: 32 [17792/50048]	Loss: 0.4376
Training Epoch: 32 [17920/50048]	Loss: 0.4177
Training Epoch: 32 [18048/50048]	Loss: 0.7129
Training Epoch: 32 [18176/50048]	Loss: 0.7295
Training Epoch: 32 [18304/50048]	Loss: 0.6658
Training Epoch: 32 [18432/50048]	Loss: 0.4711
Training Epoch: 32 [18560/50048]	Loss: 0.5857
Training Epoch: 32 [18688/50048]	Loss: 0.5702
Training Epoch: 32 [18816/50048]	Loss: 0.6292
Training Epoch: 32 [18944/50048]	Loss: 0.7276
Training Epoch: 32 [19072/50048]	Loss: 0.7688
Training Epoch: 32 [19200/50048]	Loss: 0.5848
Training Epoch: 32 [19328/50048]	Loss: 0.6837
Training Epoch: 32 [19456/50048]	Loss: 0.5178
Training Epoch: 32 [19584/50048]	Loss: 0.6664
Training Epoch: 32 [19712/50048]	Loss: 0.5611
Training Epoch: 32 [19840/50048]	Loss: 0.5546
Training Epoch: 32 [19968/50048]	Loss: 0.8172
Training Epoch: 32 [20096/50048]	Loss: 0.6994
Training Epoch: 32 [20224/50048]	Loss: 0.4707
Training Epoch: 32 [20352/50048]	Loss: 0.6096
Training Epoch: 32 [20480/50048]	Loss: 0.4343
Training Epoch: 32 [20608/50048]	Loss: 0.6603
Training Epoch: 32 [20736/50048]	Loss: 0.6470
Training Epoch: 32 [20864/50048]	Loss: 0.6248
Training Epoch: 32 [20992/50048]	Loss: 0.5745
Training Epoch: 32 [21120/50048]	Loss: 0.5810
Training Epoch: 32 [21248/50048]	Loss: 0.7568
Training Epoch: 32 [21376/50048]	Loss: 0.4164
Training Epoch: 32 [21504/50048]	Loss: 0.7739
Training Epoch: 32 [21632/50048]	Loss: 0.6986
Training Epoch: 32 [21760/50048]	Loss: 0.5663
Training Epoch: 32 [21888/50048]	Loss: 0.6675
Training Epoch: 32 [22016/50048]	Loss: 0.6933
Training Epoch: 32 [22144/50048]	Loss: 0.6371
Training Epoch: 32 [22272/50048]	Loss: 0.7088
Training Epoch: 32 [22400/50048]	Loss: 0.6732
Training Epoch: 32 [22528/50048]	Loss: 0.5191
Training Epoch: 32 [22656/50048]	Loss: 0.5420
Training Epoch: 32 [22784/50048]	Loss: 0.6815
Training Epoch: 32 [22912/50048]	Loss: 0.8134
Training Epoch: 32 [23040/50048]	Loss: 0.4908
Training Epoch: 32 [23168/50048]	Loss: 0.5283
Training Epoch: 32 [23296/50048]	Loss: 0.6017
Training Epoch: 32 [23424/50048]	Loss: 0.5478
Training Epoch: 32 [23552/50048]	Loss: 0.6053
Training Epoch: 32 [23680/50048]	Loss: 0.4949
Training Epoch: 32 [23808/50048]	Loss: 0.7182
Training Epoch: 32 [23936/50048]	Loss: 0.6248
Training Epoch: 32 [24064/50048]	Loss: 0.5561
Training Epoch: 32 [24192/50048]	Loss: 0.7948
Training Epoch: 32 [24320/50048]	Loss: 0.6651
Training Epoch: 32 [24448/50048]	Loss: 0.4270
Training Epoch: 32 [24576/50048]	Loss: 0.4638
Training Epoch: 32 [24704/50048]	Loss: 0.5661
Training Epoch: 32 [24832/50048]	Loss: 0.5160
Training Epoch: 32 [24960/50048]	Loss: 0.8005
Training Epoch: 32 [25088/50048]	Loss: 0.7744
Training Epoch: 32 [25216/50048]	Loss: 0.6097
Training Epoch: 32 [25344/50048]	Loss: 0.6621
Training Epoch: 32 [25472/50048]	Loss: 0.6827
Training Epoch: 32 [25600/50048]	Loss: 0.4780
Training Epoch: 32 [25728/50048]	Loss: 0.4617
Training Epoch: 32 [25856/50048]	Loss: 0.6075
Training Epoch: 32 [25984/50048]	Loss: 0.6709
Training Epoch: 32 [26112/50048]	Loss: 0.5519
Training Epoch: 32 [26240/50048]	Loss: 0.5973
Training Epoch: 32 [26368/50048]	Loss: 0.3999
Training Epoch: 32 [26496/50048]	Loss: 0.5593
Training Epoch: 32 [26624/50048]	Loss: 0.3975
Training Epoch: 32 [26752/50048]	Loss: 0.5822
Training Epoch: 32 [26880/50048]	Loss: 0.5676
Training Epoch: 32 [27008/50048]	Loss: 0.4718
Training Epoch: 32 [27136/50048]	Loss: 0.4203
Training Epoch: 32 [27264/50048]	Loss: 0.6039
Training Epoch: 32 [27392/50048]	Loss: 0.5551
Training Epoch: 32 [27520/50048]	Loss: 0.5366
Training Epoch: 32 [27648/50048]	Loss: 0.6797
Training Epoch: 32 [27776/50048]	Loss: 0.7441
Training Epoch: 32 [27904/50048]	Loss: 0.7598
Training Epoch: 32 [28032/50048]	Loss: 0.4089
Training Epoch: 32 [28160/50048]	Loss: 0.5829
Training Epoch: 32 [28288/50048]	Loss: 0.6725
Training Epoch: 32 [28416/50048]	Loss: 0.7522
Training Epoch: 32 [28544/50048]	Loss: 0.5546
Training Epoch: 32 [28672/50048]	Loss: 0.5412
Training Epoch: 32 [28800/50048]	Loss: 0.5076
Training Epoch: 32 [28928/50048]	Loss: 0.4542
Training Epoch: 32 [29056/50048]	Loss: 0.5189
Training Epoch: 32 [29184/50048]	Loss: 0.5686
Training Epoch: 32 [29312/50048]	Loss: 0.5207
Training Epoch: 32 [29440/50048]	Loss: 0.5522
Training Epoch: 32 [29568/50048]	Loss: 0.6529
Training Epoch: 32 [29696/50048]	Loss: 0.7093
Training Epoch: 32 [29824/50048]	Loss: 0.7195
Training Epoch: 32 [29952/50048]	Loss: 0.4775
Training Epoch: 32 [30080/50048]	Loss: 0.6416
Training Epoch: 32 [30208/50048]	Loss: 0.5427
Training Epoch: 32 [30336/50048]	Loss: 0.6354
Training Epoch: 32 [30464/50048]	Loss: 0.5065
Training Epoch: 32 [30592/50048]	Loss: 0.6111
Training Epoch: 32 [30720/50048]	Loss: 0.6102
Training Epoch: 32 [30848/50048]	Loss: 0.4547
Training Epoch: 32 [30976/50048]	Loss: 0.7435
Training Epoch: 32 [31104/50048]	Loss: 0.6926
Training Epoch: 32 [31232/50048]	Loss: 0.5249
Training Epoch: 32 [31360/50048]	Loss: 0.6112
Training Epoch: 32 [31488/50048]	Loss: 0.5146
Training Epoch: 32 [31616/50048]	Loss: 0.6868
Training Epoch: 32 [31744/50048]	Loss: 0.7004
Training Epoch: 32 [31872/50048]	Loss: 0.5722
Training Epoch: 32 [32000/50048]	Loss: 0.5163
Training Epoch: 32 [32128/50048]	Loss: 0.7061
Training Epoch: 32 [32256/50048]	Loss: 0.7332
Training Epoch: 32 [32384/50048]	Loss: 0.4175
Training Epoch: 32 [32512/50048]	Loss: 0.5916
Training Epoch: 32 [32640/50048]	Loss: 0.6112
Training Epoch: 32 [32768/50048]	Loss: 0.6739
Training Epoch: 32 [32896/50048]	Loss: 0.6068
Training Epoch: 32 [33024/50048]	Loss: 0.6474
Training Epoch: 32 [33152/50048]	Loss: 0.6371
Training Epoch: 32 [33280/50048]	Loss: 0.6993
Training Epoch: 32 [33408/50048]	Loss: 0.4345
Training Epoch: 32 [33536/50048]	Loss: 0.6699
Training Epoch: 32 [33664/50048]	Loss: 0.7232
Training Epoch: 32 [33792/50048]	Loss: 0.6705
Training Epoch: 32 [33920/50048]	Loss: 0.6446
Training Epoch: 32 [34048/50048]	Loss: 0.7536
Training Epoch: 32 [34176/50048]	Loss: 0.7066
Training Epoch: 32 [34304/50048]	Loss: 0.5051
Training Epoch: 32 [34432/50048]	Loss: 0.5911
Training Epoch: 32 [34560/50048]	Loss: 0.6739
Training Epoch: 32 [34688/50048]	Loss: 0.6789
Training Epoch: 32 [34816/50048]	Loss: 0.7165
Training Epoch: 32 [34944/50048]	Loss: 0.7933
Training Epoch: 32 [35072/50048]	Loss: 0.5189
Training Epoch: 32 [35200/50048]	Loss: 0.6166
Training Epoch: 32 [35328/50048]	Loss: 0.5625
Training Epoch: 32 [35456/50048]	Loss: 0.5685
Training Epoch: 32 [35584/50048]	Loss: 0.7799
Training Epoch: 32 [35712/50048]	Loss: 0.6183
Training Epoch: 32 [35840/50048]	Loss: 0.4286
Training Epoch: 32 [35968/50048]	Loss: 0.6061
Training Epoch: 32 [36096/50048]	Loss: 0.7828
Training Epoch: 32 [36224/50048]	Loss: 0.6545
Training Epoch: 32 [36352/50048]	Loss: 0.7051
Training Epoch: 32 [36480/50048]	Loss: 0.6727
Training Epoch: 32 [36608/50048]	Loss: 0.5259
Training Epoch: 32 [36736/50048]	Loss: 0.5246
Training Epoch: 32 [36864/50048]	Loss: 0.5307
Training Epoch: 32 [36992/50048]	Loss: 0.6471
Training Epoch: 32 [37120/50048]	Loss: 0.5361
Training Epoch: 32 [37248/50048]	Loss: 0.7359
Training Epoch: 32 [37376/50048]	Loss: 0.5140
Training Epoch: 32 [37504/50048]	Loss: 0.6620
Training Epoch: 32 [37632/50048]	Loss: 0.5776
Training Epoch: 32 [37760/50048]	Loss: 0.6181
Training Epoch: 32 [37888/50048]	Loss: 0.6043
Training Epoch: 32 [38016/50048]	Loss: 0.6345
Training Epoch: 32 [38144/50048]	Loss: 0.5333
Training Epoch: 32 [38272/50048]	Loss: 0.4298
Training Epoch: 32 [38400/50048]	Loss: 0.5623
Training Epoch: 32 [38528/50048]	Loss: 0.5916
Training Epoch: 32 [38656/50048]	Loss: 0.6835
Training Epoch: 32 [38784/50048]	Loss: 0.6090
Training Epoch: 32 [38912/50048]	Loss: 0.5893
Training Epoch: 32 [39040/50048]	Loss: 0.7314
Training Epoch: 32 [39168/50048]	Loss: 0.5150
Training Epoch: 32 [39296/50048]	Loss: 0.4739
Training Epoch: 32 [39424/50048]	Loss: 0.5484
Training Epoch: 32 [39552/50048]	Loss: 0.5877
Training Epoch: 32 [39680/50048]	Loss: 0.6881
Training Epoch: 32 [39808/50048]	Loss: 0.7659
Training Epoch: 32 [39936/50048]	Loss: 0.6424
Training Epoch: 32 [40064/50048]	Loss: 0.5258
Training Epoch: 32 [40192/50048]	Loss: 0.4548
Training Epoch: 32 [40320/50048]	Loss: 0.5748
Training Epoch: 32 [40448/50048]	Loss: 0.5481
Training Epoch: 32 [40576/50048]	Loss: 0.4492
Training Epoch: 32 [40704/50048]	Loss: 0.7303
Training Epoch: 32 [40832/50048]	Loss: 0.6548
Training Epoch: 32 [40960/50048]	Loss: 0.6509
Training Epoch: 32 [41088/50048]	Loss: 0.5394
Training Epoch: 32 [41216/50048]	Loss: 0.7305
Training Epoch: 32 [41344/50048]	Loss: 0.4757
Training Epoch: 32 [41472/50048]	Loss: 0.4871
Training Epoch: 32 [41600/50048]	Loss: 0.4908
Training Epoch: 32 [41728/50048]	Loss: 0.6759
Training Epoch: 32 [41856/50048]	Loss: 0.6407
Training Epoch: 32 [41984/50048]	Loss: 0.6073
Training Epoch: 32 [42112/50048]	Loss: 0.5311
Training Epoch: 32 [42240/50048]	Loss: 0.7658
Training Epoch: 32 [42368/50048]	Loss: 0.5475
Training Epoch: 32 [42496/50048]	Loss: 0.5883
Training Epoch: 32 [42624/50048]	Loss: 0.6441
Training Epoch: 32 [42752/50048]	Loss: 0.5405
Training Epoch: 32 [42880/50048]	Loss: 0.5416
Training Epoch: 32 [43008/50048]	Loss: 0.6592
Training Epoch: 32 [43136/50048]	Loss: 0.7144
Training Epoch: 32 [43264/50048]	Loss: 0.6813
Training Epoch: 32 [43392/50048]	Loss: 0.4879
Training Epoch: 32 [43520/50048]	Loss: 0.6315
Training Epoch: 32 [43648/50048]	Loss: 0.6048
Training Epoch: 32 [43776/50048]	Loss: 0.7570
Training Epoch: 32 [43904/50048]	Loss: 0.7205
Training Epoch: 32 [44032/50048]	Loss: 0.5654
Training Epoch: 32 [44160/50048]	Loss: 0.5553
Training Epoch: 32 [44288/50048]	Loss: 0.6340
Training Epoch: 32 [44416/50048]	Loss: 0.5932
Training Epoch: 32 [44544/50048]	Loss: 0.6150
Training Epoch: 32 [44672/50048]	Loss: 0.5326
Training Epoch: 32 [44800/50048]	Loss: 0.5575
Training Epoch: 32 [44928/50048]	Loss: 0.5101
Training Epoch: 32 [45056/50048]	Loss: 0.6195
Training Epoch: 32 [45184/50048]	Loss: 0.5227
Training Epoch: 32 [45312/50048]	Loss: 0.5913
Training Epoch: 32 [45440/50048]	Loss: 0.6494
Training Epoch: 32 [45568/50048]	Loss: 0.7502
Training Epoch: 32 [45696/50048]	Loss: 0.6632
2022-12-06 04:26:58,558 [ZeusDataLoader(train)] train epoch 33 done: time=86.46 energy=10492.22
2022-12-06 04:26:58,560 [ZeusDataLoader(eval)] Epoch 33 begin.
Training Epoch: 32 [45824/50048]	Loss: 0.7419
Training Epoch: 32 [45952/50048]	Loss: 0.7685
Training Epoch: 32 [46080/50048]	Loss: 0.6380
Training Epoch: 32 [46208/50048]	Loss: 0.7174
Training Epoch: 32 [46336/50048]	Loss: 0.4360
Training Epoch: 32 [46464/50048]	Loss: 0.7001
Training Epoch: 32 [46592/50048]	Loss: 0.5591
Training Epoch: 32 [46720/50048]	Loss: 0.6025
Training Epoch: 32 [46848/50048]	Loss: 0.4133
Training Epoch: 32 [46976/50048]	Loss: 0.7294
Training Epoch: 32 [47104/50048]	Loss: 0.5943
Training Epoch: 32 [47232/50048]	Loss: 0.5827
Training Epoch: 32 [47360/50048]	Loss: 0.6507
Training Epoch: 32 [47488/50048]	Loss: 0.6860
Training Epoch: 32 [47616/50048]	Loss: 0.6008
Training Epoch: 32 [47744/50048]	Loss: 0.6444
Training Epoch: 32 [47872/50048]	Loss: 0.6504
Training Epoch: 32 [48000/50048]	Loss: 0.5705
Training Epoch: 32 [48128/50048]	Loss: 0.6668
Training Epoch: 32 [48256/50048]	Loss: 0.5698
Training Epoch: 32 [48384/50048]	Loss: 0.5689
Training Epoch: 32 [48512/50048]	Loss: 0.4606
Training Epoch: 32 [48640/50048]	Loss: 0.6498
Training Epoch: 32 [48768/50048]	Loss: 0.8428
Training Epoch: 32 [48896/50048]	Loss: 0.5774
Training Epoch: 32 [49024/50048]	Loss: 0.6610
Training Epoch: 32 [49152/50048]	Loss: 0.5308
Training Epoch: 32 [49280/50048]	Loss: 0.5525
Training Epoch: 32 [49408/50048]	Loss: 0.9060
Training Epoch: 32 [49536/50048]	Loss: 0.6824
Training Epoch: 32 [49664/50048]	Loss: 0.6525
Training Epoch: 32 [49792/50048]	Loss: 0.6858
Training Epoch: 32 [49920/50048]	Loss: 0.6006
Training Epoch: 32 [50048/50048]	Loss: 0.7849
2022-12-06 09:27:02.286 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:27:02,330 [ZeusDataLoader(eval)] eval epoch 33 done: time=3.76 energy=454.10
2022-12-06 04:27:02,330 [ZeusDataLoader(train)] Up to epoch 33: time=2979.39, energy=361535.29, cost=441464.17
2022-12-06 04:27:02,330 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:27:02,330 [ZeusDataLoader(train)] Expected next epoch: time=3069.19, energy=372333.31, cost=454720.55
2022-12-06 04:27:02,331 [ZeusDataLoader(train)] Epoch 34 begin.
Validation Epoch: 32, Average loss: 0.0129, Accuracy: 0.6167
2022-12-06 04:27:02,520 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:27:02,521 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:27:02.531 [ZeusMonitor] Monitor started.
2022-12-06 09:27:02.531 [ZeusMonitor] Running indefinitely. 2022-12-06 09:27:02.531 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:27:02.531 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e34+gpu0.power.log
Training Epoch: 33 [128/50048]	Loss: 0.7237
Training Epoch: 33 [256/50048]	Loss: 0.6668
Training Epoch: 33 [384/50048]	Loss: 0.7295
Training Epoch: 33 [512/50048]	Loss: 0.3898
Training Epoch: 33 [640/50048]	Loss: 0.6659
Training Epoch: 33 [768/50048]	Loss: 0.4667
Training Epoch: 33 [896/50048]	Loss: 0.4951
Training Epoch: 33 [1024/50048]	Loss: 0.5100
Training Epoch: 33 [1152/50048]	Loss: 0.4869
Training Epoch: 33 [1280/50048]	Loss: 0.4570
Training Epoch: 33 [1408/50048]	Loss: 0.4777
Training Epoch: 33 [1536/50048]	Loss: 0.4175
Training Epoch: 33 [1664/50048]	Loss: 0.6013
Training Epoch: 33 [1792/50048]	Loss: 0.5984
Training Epoch: 33 [1920/50048]	Loss: 0.5028
Training Epoch: 33 [2048/50048]	Loss: 0.5210
Training Epoch: 33 [2176/50048]	Loss: 0.4243
Training Epoch: 33 [2304/50048]	Loss: 0.5255
Training Epoch: 33 [2432/50048]	Loss: 0.5287
Training Epoch: 33 [2560/50048]	Loss: 0.5531
Training Epoch: 33 [2688/50048]	Loss: 0.6176
Training Epoch: 33 [2816/50048]	Loss: 0.5432
Training Epoch: 33 [2944/50048]	Loss: 0.5704
Training Epoch: 33 [3072/50048]	Loss: 0.3799
Training Epoch: 33 [3200/50048]	Loss: 0.4263
Training Epoch: 33 [3328/50048]	Loss: 0.3864
Training Epoch: 33 [3456/50048]	Loss: 0.4747
Training Epoch: 33 [3584/50048]	Loss: 0.7520
Training Epoch: 33 [3712/50048]	Loss: 0.4911
Training Epoch: 33 [3840/50048]	Loss: 0.5556
Training Epoch: 33 [3968/50048]	Loss: 0.5729
Training Epoch: 33 [4096/50048]	Loss: 0.7297
Training Epoch: 33 [4224/50048]	Loss: 0.5514
Training Epoch: 33 [4352/50048]	Loss: 0.6247
Training Epoch: 33 [4480/50048]	Loss: 0.4096
Training Epoch: 33 [4608/50048]	Loss: 0.5863
Training Epoch: 33 [4736/50048]	Loss: 0.7118
Training Epoch: 33 [4864/50048]	Loss: 0.4099
Training Epoch: 33 [4992/50048]	Loss: 0.5004
Training Epoch: 33 [5120/50048]	Loss: 0.5973
Training Epoch: 33 [5248/50048]	Loss: 0.4734
Training Epoch: 33 [5376/50048]	Loss: 0.5207
Training Epoch: 33 [5504/50048]	Loss: 0.4259
Training Epoch: 33 [5632/50048]	Loss: 0.4163
Training Epoch: 33 [5760/50048]	Loss: 0.4155
Training Epoch: 33 [5888/50048]	Loss: 0.4839
Training Epoch: 33 [6016/50048]	Loss: 0.4957
Training Epoch: 33 [6144/50048]	Loss: 0.4553
Training Epoch: 33 [6272/50048]	Loss: 0.7833
Training Epoch: 33 [6400/50048]	Loss: 0.6699
Training Epoch: 33 [6528/50048]	Loss: 0.6820
Training Epoch: 33 [6656/50048]	Loss: 0.4463
Training Epoch: 33 [6784/50048]	Loss: 0.8019
Training Epoch: 33 [6912/50048]	Loss: 0.4630
Training Epoch: 33 [7040/50048]	Loss: 0.6558
Training Epoch: 33 [7168/50048]	Loss: 0.5609
Training Epoch: 33 [7296/50048]	Loss: 0.6484
Training Epoch: 33 [7424/50048]	Loss: 0.5018
Training Epoch: 33 [7552/50048]	Loss: 0.5892
Training Epoch: 33 [7680/50048]	Loss: 0.3678
Training Epoch: 33 [7808/50048]	Loss: 0.3421
Training Epoch: 33 [7936/50048]	Loss: 0.5757
Training Epoch: 33 [8064/50048]	Loss: 0.4241
Training Epoch: 33 [8192/50048]	Loss: 0.5965
Training Epoch: 33 [8320/50048]	Loss: 0.3697
Training Epoch: 33 [8448/50048]	Loss: 0.4232
Training Epoch: 33 [8576/50048]	Loss: 0.5868
Training Epoch: 33 [8704/50048]	Loss: 0.6494
Training Epoch: 33 [8832/50048]	Loss: 0.3972
Training Epoch: 33 [8960/50048]	Loss: 0.5752
Training Epoch: 33 [9088/50048]	Loss: 0.6082
Training Epoch: 33 [9216/50048]	Loss: 0.6137
Training Epoch: 33 [9344/50048]	Loss: 0.5288
Training Epoch: 33 [9472/50048]	Loss: 0.5704
Training Epoch: 33 [9600/50048]	Loss: 0.6468
Training Epoch: 33 [9728/50048]	Loss: 0.6513
Training Epoch: 33 [9856/50048]	Loss: 0.5300
Training Epoch: 33 [9984/50048]	Loss: 0.4620
Training Epoch: 33 [10112/50048]	Loss: 0.4618
Training Epoch: 33 [10240/50048]	Loss: 0.5522
Training Epoch: 33 [10368/50048]	Loss: 0.5845
Training Epoch: 33 [10496/50048]	Loss: 0.6790
Training Epoch: 33 [10624/50048]	Loss: 0.4817
Training Epoch: 33 [10752/50048]	Loss: 0.5949
Training Epoch: 33 [10880/50048]	Loss: 0.5405
Training Epoch: 33 [11008/50048]	Loss: 0.5260
Training Epoch: 33 [11136/50048]	Loss: 0.5438
Training Epoch: 33 [11264/50048]	Loss: 0.5113
Training Epoch: 33 [11392/50048]	Loss: 0.7462
Training Epoch: 33 [11520/50048]	Loss: 0.6296
Training Epoch: 33 [11648/50048]	Loss: 0.6701
Training Epoch: 33 [11776/50048]	Loss: 0.4006
Training Epoch: 33 [11904/50048]	Loss: 0.5453
Training Epoch: 33 [12032/50048]	Loss: 0.4903
Training Epoch: 33 [12160/50048]	Loss: 0.7782
Training Epoch: 33 [12288/50048]	Loss: 0.5671
Training Epoch: 33 [12416/50048]	Loss: 0.5108
Training Epoch: 33 [12544/50048]	Loss: 0.5319
Training Epoch: 33 [12672/50048]	Loss: 0.4147
Training Epoch: 33 [12800/50048]	Loss: 0.4423
Training Epoch: 33 [12928/50048]	Loss: 0.5049
Training Epoch: 33 [13056/50048]	Loss: 0.6243
Training Epoch: 33 [13184/50048]	Loss: 0.5636
Training Epoch: 33 [13312/50048]	Loss: 0.7230
Training Epoch: 33 [13440/50048]	Loss: 0.5208
Training Epoch: 33 [13568/50048]	Loss: 0.4648
Training Epoch: 33 [13696/50048]	Loss: 0.4981
Training Epoch: 33 [13824/50048]	Loss: 0.5476
Training Epoch: 33 [13952/50048]	Loss: 0.5191
Training Epoch: 33 [14080/50048]	Loss: 0.4622
Training Epoch: 33 [14208/50048]	Loss: 0.6083
Training Epoch: 33 [14336/50048]	Loss: 0.6172
Training Epoch: 33 [14464/50048]	Loss: 0.6485
Training Epoch: 33 [14592/50048]	Loss: 0.3886
Training Epoch: 33 [14720/50048]	Loss: 0.4625
Training Epoch: 33 [14848/50048]	Loss: 0.6283
Training Epoch: 33 [14976/50048]	Loss: 0.7502
Training Epoch: 33 [15104/50048]	Loss: 0.4417
Training Epoch: 33 [15232/50048]	Loss: 0.5001
Training Epoch: 33 [15360/50048]	Loss: 0.5286
Training Epoch: 33 [15488/50048]	Loss: 0.5014
Training Epoch: 33 [15616/50048]	Loss: 0.6245
Training Epoch: 33 [15744/50048]	Loss: 0.5452
Training Epoch: 33 [15872/50048]	Loss: 0.6901
Training Epoch: 33 [16000/50048]	Loss: 0.4720
Training Epoch: 33 [16128/50048]	Loss: 0.6112
Training Epoch: 33 [16256/50048]	Loss: 0.6474
Training Epoch: 33 [16384/50048]	Loss: 0.6326
Training Epoch: 33 [16512/50048]	Loss: 0.6276
Training Epoch: 33 [16640/50048]	Loss: 0.4511
Training Epoch: 33 [16768/50048]	Loss: 0.5264
Training Epoch: 33 [16896/50048]	Loss: 0.5203
Training Epoch: 33 [17024/50048]	Loss: 0.3920
Training Epoch: 33 [17152/50048]	Loss: 0.4270
Training Epoch: 33 [17280/50048]	Loss: 0.6431
Training Epoch: 33 [17408/50048]	Loss: 0.4702
Training Epoch: 33 [17536/50048]	Loss: 0.4315
Training Epoch: 33 [17664/50048]	Loss: 0.5741
Training Epoch: 33 [17792/50048]	Loss: 0.5588
Training Epoch: 33 [17920/50048]	Loss: 0.5237
Training Epoch: 33 [18048/50048]	Loss: 0.5392
Training Epoch: 33 [18176/50048]	Loss: 0.5641
Training Epoch: 33 [18304/50048]	Loss: 0.5283
Training Epoch: 33 [18432/50048]	Loss: 0.6469
Training Epoch: 33 [18560/50048]	Loss: 0.6888
Training Epoch: 33 [18688/50048]	Loss: 0.5815
Training Epoch: 33 [18816/50048]	Loss: 0.4983
Training Epoch: 33 [18944/50048]	Loss: 0.4371
Training Epoch: 33 [19072/50048]	Loss: 0.6056
Training Epoch: 33 [19200/50048]	Loss: 0.7123
Training Epoch: 33 [19328/50048]	Loss: 0.7274
Training Epoch: 33 [19456/50048]	Loss: 0.6066
Training Epoch: 33 [19584/50048]	Loss: 0.4671
Training Epoch: 33 [19712/50048]	Loss: 0.5199
Training Epoch: 33 [19840/50048]	Loss: 0.6666
Training Epoch: 33 [19968/50048]	Loss: 0.5581
Training Epoch: 33 [20096/50048]	Loss: 0.4890
Training Epoch: 33 [20224/50048]	Loss: 0.5844
Training Epoch: 33 [20352/50048]	Loss: 0.4249
Training Epoch: 33 [20480/50048]	Loss: 0.4191
Training Epoch: 33 [20608/50048]	Loss: 0.4696
Training Epoch: 33 [20736/50048]	Loss: 0.8424
Training Epoch: 33 [20864/50048]	Loss: 0.6204
Training Epoch: 33 [20992/50048]	Loss: 0.6190
Training Epoch: 33 [21120/50048]	Loss: 0.6651
Training Epoch: 33 [21248/50048]	Loss: 0.5049
Training Epoch: 33 [21376/50048]	Loss: 0.6872
Training Epoch: 33 [21504/50048]	Loss: 0.5830
Training Epoch: 33 [21632/50048]	Loss: 0.5393
Training Epoch: 33 [21760/50048]	Loss: 0.6210
Training Epoch: 33 [21888/50048]	Loss: 0.6378
Training Epoch: 33 [22016/50048]	Loss: 0.6004
Training Epoch: 33 [22144/50048]	Loss: 0.6163
Training Epoch: 33 [22272/50048]	Loss: 0.6007
Training Epoch: 33 [22400/50048]	Loss: 0.4602
Training Epoch: 33 [22528/50048]	Loss: 0.4376
Training Epoch: 33 [22656/50048]	Loss: 0.4584
Training Epoch: 33 [22784/50048]	Loss: 0.6186
Training Epoch: 33 [22912/50048]	Loss: 0.5323
Training Epoch: 33 [23040/50048]	Loss: 0.7024
Training Epoch: 33 [23168/50048]	Loss: 0.5272
Training Epoch: 33 [23296/50048]	Loss: 0.6555
Training Epoch: 33 [23424/50048]	Loss: 0.7441
Training Epoch: 33 [23552/50048]	Loss: 0.5983
Training Epoch: 33 [23680/50048]	Loss: 0.5502
Training Epoch: 33 [23808/50048]	Loss: 0.5680
Training Epoch: 33 [23936/50048]	Loss: 0.5909
Training Epoch: 33 [24064/50048]	Loss: 0.6931
Training Epoch: 33 [24192/50048]	Loss: 0.7357
Training Epoch: 33 [24320/50048]	Loss: 0.6031
Training Epoch: 33 [24448/50048]	Loss: 0.5362
Training Epoch: 33 [24576/50048]	Loss: 0.5743
Training Epoch: 33 [24704/50048]	Loss: 0.5118
Training Epoch: 33 [24832/50048]	Loss: 0.4281
Training Epoch: 33 [24960/50048]	Loss: 0.6441
Training Epoch: 33 [25088/50048]	Loss: 0.5082
Training Epoch: 33 [25216/50048]	Loss: 0.5376
Training Epoch: 33 [25344/50048]	Loss: 0.5717
Training Epoch: 33 [25472/50048]	Loss: 0.6910
Training Epoch: 33 [25600/50048]	Loss: 0.6593
Training Epoch: 33 [25728/50048]	Loss: 0.5656
Training Epoch: 33 [25856/50048]	Loss: 0.6181
Training Epoch: 33 [25984/50048]	Loss: 0.4506
Training Epoch: 33 [26112/50048]	Loss: 0.5476
Training Epoch: 33 [26240/50048]	Loss: 0.4751
Training Epoch: 33 [26368/50048]	Loss: 0.4552
Training Epoch: 33 [26496/50048]	Loss: 0.5378
Training Epoch: 33 [26624/50048]	Loss: 0.5974
Training Epoch: 33 [26752/50048]	Loss: 0.5807
Training Epoch: 33 [26880/50048]	Loss: 0.3753
Training Epoch: 33 [27008/50048]	Loss: 0.6634
Training Epoch: 33 [27136/50048]	Loss: 0.4960
Training Epoch: 33 [27264/50048]	Loss: 0.4521
Training Epoch: 33 [27392/50048]	Loss: 0.4842
Training Epoch: 33 [27520/50048]	Loss: 0.5286
Training Epoch: 33 [27648/50048]	Loss: 0.4993
Training Epoch: 33 [27776/50048]	Loss: 0.5836
Training Epoch: 33 [27904/50048]	Loss: 0.6699
Training Epoch: 33 [28032/50048]	Loss: 0.5967
Training Epoch: 33 [28160/50048]	Loss: 0.5388
Training Epoch: 33 [28288/50048]	Loss: 0.4564
Training Epoch: 33 [28416/50048]	Loss: 0.6856
Training Epoch: 33 [28544/50048]	Loss: 0.5765
Training Epoch: 33 [28672/50048]	Loss: 0.4665
Training Epoch: 33 [28800/50048]	Loss: 0.3964
Training Epoch: 33 [28928/50048]	Loss: 0.5702
Training Epoch: 33 [29056/50048]	Loss: 0.8345
Training Epoch: 33 [29184/50048]	Loss: 0.5296
Training Epoch: 33 [29312/50048]	Loss: 0.5324
Training Epoch: 33 [29440/50048]	Loss: 0.6280
Training Epoch: 33 [29568/50048]	Loss: 0.6664
Training Epoch: 33 [29696/50048]	Loss: 0.5546
Training Epoch: 33 [29824/50048]	Loss: 0.6969
Training Epoch: 33 [29952/50048]	Loss: 0.5998
Training Epoch: 33 [30080/50048]	Loss: 0.4773
Training Epoch: 33 [30208/50048]	Loss: 0.6024
Training Epoch: 33 [30336/50048]	Loss: 0.6549
Training Epoch: 33 [30464/50048]	Loss: 0.6397
Training Epoch: 33 [30592/50048]	Loss: 0.6778
Training Epoch: 33 [30720/50048]	Loss: 0.5826
Training Epoch: 33 [30848/50048]	Loss: 0.6726
Training Epoch: 33 [30976/50048]	Loss: 0.5367
Training Epoch: 33 [31104/50048]	Loss: 0.5592
Training Epoch: 33 [31232/50048]	Loss: 0.6564
Training Epoch: 33 [31360/50048]	Loss: 0.6393
Training Epoch: 33 [31488/50048]	Loss: 0.6184
Training Epoch: 33 [31616/50048]	Loss: 0.5896
Training Epoch: 33 [31744/50048]	Loss: 0.6084
Training Epoch: 33 [31872/50048]	Loss: 0.6677
Training Epoch: 33 [32000/50048]	Loss: 0.5896
Training Epoch: 33 [32128/50048]	Loss: 0.6925
Training Epoch: 33 [32256/50048]	Loss: 0.4420
Training Epoch: 33 [32384/50048]	Loss: 0.5053
Training Epoch: 33 [32512/50048]	Loss: 0.6223
Training Epoch: 33 [32640/50048]	Loss: 0.6664
Training Epoch: 33 [32768/50048]	Loss: 0.5714
Training Epoch: 33 [32896/50048]	Loss: 0.6318
Training Epoch: 33 [33024/50048]	Loss: 0.6682
Training Epoch: 33 [33152/50048]	Loss: 0.6919
Training Epoch: 33 [33280/50048]	Loss: 0.5631
Training Epoch: 33 [33408/50048]	Loss: 0.6610
Training Epoch: 33 [33536/50048]	Loss: 0.7225
Training Epoch: 33 [33664/50048]	Loss: 0.5161
Training Epoch: 33 [33792/50048]	Loss: 0.5902
Training Epoch: 33 [33920/50048]	Loss: 0.7126
Training Epoch: 33 [34048/50048]	Loss: 0.5080
Training Epoch: 33 [34176/50048]	Loss: 0.7393
Training Epoch: 33 [34304/50048]	Loss: 0.5680
Training Epoch: 33 [34432/50048]	Loss: 0.5981
Training Epoch: 33 [34560/50048]	Loss: 0.5035
Training Epoch: 33 [34688/50048]	Loss: 0.5518
Training Epoch: 33 [34816/50048]	Loss: 0.5459
Training Epoch: 33 [34944/50048]	Loss: 0.5733
Training Epoch: 33 [35072/50048]	Loss: 0.7000
Training Epoch: 33 [35200/50048]	Loss: 0.5816
Training Epoch: 33 [35328/50048]	Loss: 0.6482
Training Epoch: 33 [35456/50048]	Loss: 0.4025
Training Epoch: 33 [35584/50048]	Loss: 0.5436
Training Epoch: 33 [35712/50048]	Loss: 0.5253
Training Epoch: 33 [35840/50048]	Loss: 0.5275
Training Epoch: 33 [35968/50048]	Loss: 0.5749
Training Epoch: 33 [36096/50048]	Loss: 0.6590
Training Epoch: 33 [36224/50048]	Loss: 0.4838
Training Epoch: 33 [36352/50048]	Loss: 0.5028
Training Epoch: 33 [36480/50048]	Loss: 0.5925
Training Epoch: 33 [36608/50048]	Loss: 0.6806
Training Epoch: 33 [36736/50048]	Loss: 0.6326
Training Epoch: 33 [36864/50048]	Loss: 0.5409
Training Epoch: 33 [36992/50048]	Loss: 0.5930
Training Epoch: 33 [37120/50048]	Loss: 0.7101
Training Epoch: 33 [37248/50048]	Loss: 0.6071
Training Epoch: 33 [37376/50048]	Loss: 0.6837
Training Epoch: 33 [37504/50048]	Loss: 0.5714
Training Epoch: 33 [37632/50048]	Loss: 0.4726
Training Epoch: 33 [37760/50048]	Loss: 0.8951
Training Epoch: 33 [37888/50048]	Loss: 0.6162
Training Epoch: 33 [38016/50048]	Loss: 0.4953
Training Epoch: 33 [38144/50048]	Loss: 0.6749
Training Epoch: 33 [38272/50048]	Loss: 0.5275
Training Epoch: 33 [38400/50048]	Loss: 0.5320
Training Epoch: 33 [38528/50048]	Loss: 0.4979
Training Epoch: 33 [38656/50048]	Loss: 0.5868
Training Epoch: 33 [38784/50048]	Loss: 0.5906
Training Epoch: 33 [38912/50048]	Loss: 0.5561
Training Epoch: 33 [39040/50048]	Loss: 0.4296
Training Epoch: 33 [39168/50048]	Loss: 0.5925
Training Epoch: 33 [39296/50048]	Loss: 0.4886
Training Epoch: 33 [39424/50048]	Loss: 0.7000
Training Epoch: 33 [39552/50048]	Loss: 0.6211
Training Epoch: 33 [39680/50048]	Loss: 0.5920
Training Epoch: 33 [39808/50048]	Loss: 0.5796
Training Epoch: 33 [39936/50048]	Loss: 0.6025
Training Epoch: 33 [40064/50048]	Loss: 0.5444
Training Epoch: 33 [40192/50048]	Loss: 0.5602
Training Epoch: 33 [40320/50048]	Loss: 0.4948
Training Epoch: 33 [40448/50048]	Loss: 0.5971
Training Epoch: 33 [40576/50048]	Loss: 0.7007
Training Epoch: 33 [40704/50048]	Loss: 0.5987
Training Epoch: 33 [40832/50048]	Loss: 0.5638
Training Epoch: 33 [40960/50048]	Loss: 0.5439
Training Epoch: 33 [41088/50048]	Loss: 0.4994
Training Epoch: 33 [41216/50048]	Loss: 0.5985
Training Epoch: 33 [41344/50048]	Loss: 0.4573
Training Epoch: 33 [41472/50048]	Loss: 0.7371
Training Epoch: 33 [41600/50048]	Loss: 0.5896
Training Epoch: 33 [41728/50048]	Loss: 0.6987
Training Epoch: 33 [41856/50048]	Loss: 0.4443
Training Epoch: 33 [41984/50048]	Loss: 0.6553
Training Epoch: 33 [42112/50048]	Loss: 0.7429
Training Epoch: 33 [42240/50048]	Loss: 0.6950
Training Epoch: 33 [42368/50048]	Loss: 0.5413
Training Epoch: 33 [42496/50048]	Loss: 0.5419
Training Epoch: 33 [42624/50048]	Loss: 0.4775
Training Epoch: 33 [42752/50048]	Loss: 0.6814
Training Epoch: 33 [42880/50048]	Loss: 0.5855
Training Epoch: 33 [43008/50048]	Loss: 0.5484
Training Epoch: 33 [43136/50048]	Loss: 0.5478
Training Epoch: 33 [43264/50048]	Loss: 0.6878
Training Epoch: 33 [43392/50048]	Loss: 0.5006
Training Epoch: 33 [43520/50048]	Loss: 0.5883
Training Epoch: 33 [43648/50048]	Loss: 0.5976
Training Epoch: 33 [43776/50048]	Loss: 0.7475
Training Epoch: 33 [43904/50048]	Loss: 0.6619
Training Epoch: 33 [44032/50048]	Loss: 0.6225
Training Epoch: 33 [44160/50048]	Loss: 0.6149
Training Epoch: 33 [44288/50048]	Loss: 0.5515
Training Epoch: 33 [44416/50048]	Loss: 0.7861
Training Epoch: 33 [44544/50048]	Loss: 0.5314
Training Epoch: 33 [44672/50048]	Loss: 0.7358
Training Epoch: 33 [44800/50048]	Loss: 0.4321
Training Epoch: 33 [44928/50048]	Loss: 0.6569
Training Epoch: 33 [45056/50048]	Loss: 0.6223
Training Epoch: 33 [45184/50048]	Loss: 0.5330
Training Epoch: 33 [45312/50048]	Loss: 0.7302
Training Epoch: 33 [45440/50048]	Loss: 0.5871
Training Epoch: 33 [45568/50048]	Loss: 0.4774
Training Epoch: 33 [45696/50048]	Loss: 0.5226
2022-12-06 04:28:28,765 [ZeusDataLoader(train)] train epoch 34 done: time=86.42 energy=10486.30
2022-12-06 04:28:28,766 [ZeusDataLoader(eval)] Epoch 34 begin.
Training Epoch: 33 [45824/50048]	Loss: 0.4675
Training Epoch: 33 [45952/50048]	Loss: 0.7064
Training Epoch: 33 [46080/50048]	Loss: 0.5987
Training Epoch: 33 [46208/50048]	Loss: 0.5320
Training Epoch: 33 [46336/50048]	Loss: 0.6433
Training Epoch: 33 [46464/50048]	Loss: 0.5611
Training Epoch: 33 [46592/50048]	Loss: 0.4807
Training Epoch: 33 [46720/50048]	Loss: 0.5577
Training Epoch: 33 [46848/50048]	Loss: 0.6083
Training Epoch: 33 [46976/50048]	Loss: 0.6644
Training Epoch: 33 [47104/50048]	Loss: 0.8611
Training Epoch: 33 [47232/50048]	Loss: 0.5197
Training Epoch: 33 [47360/50048]	Loss: 0.5296
Training Epoch: 33 [47488/50048]	Loss: 0.7009
Training Epoch: 33 [47616/50048]	Loss: 0.5484
Training Epoch: 33 [47744/50048]	Loss: 0.6368
Training Epoch: 33 [47872/50048]	Loss: 0.4938
Training Epoch: 33 [48000/50048]	Loss: 0.4560
Training Epoch: 33 [48128/50048]	Loss: 0.5569
Training Epoch: 33 [48256/50048]	Loss: 0.4804
Training Epoch: 33 [48384/50048]	Loss: 0.5029
Training Epoch: 33 [48512/50048]	Loss: 0.7125
Training Epoch: 33 [48640/50048]	Loss: 0.5859
Training Epoch: 33 [48768/50048]	Loss: 0.6047
Training Epoch: 33 [48896/50048]	Loss: 0.4734
Training Epoch: 33 [49024/50048]	Loss: 0.7203
Training Epoch: 33 [49152/50048]	Loss: 0.5809
Training Epoch: 33 [49280/50048]	Loss: 0.7630
Training Epoch: 33 [49408/50048]	Loss: 0.4297
Training Epoch: 33 [49536/50048]	Loss: 0.6134
Training Epoch: 33 [49664/50048]	Loss: 0.5982
Training Epoch: 33 [49792/50048]	Loss: 0.5769
Training Epoch: 33 [49920/50048]	Loss: 0.6977
Training Epoch: 33 [50048/50048]	Loss: 0.7321
2022-12-06 09:28:32.485 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:28:32,499 [ZeusDataLoader(eval)] eval epoch 34 done: time=3.72 energy=451.54
2022-12-06 04:28:32,500 [ZeusDataLoader(train)] Up to epoch 34: time=3069.54, energy=372473.13, cost=454821.00
2022-12-06 04:28:32,500 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:28:32,500 [ZeusDataLoader(train)] Expected next epoch: time=3159.33, energy=383271.14, cost=468077.38
2022-12-06 04:28:32,501 [ZeusDataLoader(train)] Epoch 35 begin.
Validation Epoch: 33, Average loss: 0.0126, Accuracy: 0.6219
2022-12-06 04:28:32,643 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:28:32,644 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:28:32.646 [ZeusMonitor] Monitor started.
2022-12-06 09:28:32.646 [ZeusMonitor] Running indefinitely. 2022-12-06 09:28:32.646 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:28:32.646 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e35+gpu0.power.log
Training Epoch: 34 [128/50048]	Loss: 0.4178
Training Epoch: 34 [256/50048]	Loss: 0.4979
Training Epoch: 34 [384/50048]	Loss: 0.4019
Training Epoch: 34 [512/50048]	Loss: 0.4719
Training Epoch: 34 [640/50048]	Loss: 0.4011
Training Epoch: 34 [768/50048]	Loss: 0.5772
Training Epoch: 34 [896/50048]	Loss: 0.4896
Training Epoch: 34 [1024/50048]	Loss: 0.5727
Training Epoch: 34 [1152/50048]	Loss: 0.3934
Training Epoch: 34 [1280/50048]	Loss: 0.7327
Training Epoch: 34 [1408/50048]	Loss: 0.4849
Training Epoch: 34 [1536/50048]	Loss: 0.4636
Training Epoch: 34 [1664/50048]	Loss: 0.3565
Training Epoch: 34 [1792/50048]	Loss: 0.6106
Training Epoch: 34 [1920/50048]	Loss: 0.4849
Training Epoch: 34 [2048/50048]	Loss: 0.4625
Training Epoch: 34 [2176/50048]	Loss: 0.4914
Training Epoch: 34 [2304/50048]	Loss: 0.4003
Training Epoch: 34 [2432/50048]	Loss: 0.3781
Training Epoch: 34 [2560/50048]	Loss: 0.5525
Training Epoch: 34 [2688/50048]	Loss: 0.5359
Training Epoch: 34 [2816/50048]	Loss: 0.3968
Training Epoch: 34 [2944/50048]	Loss: 0.5286
Training Epoch: 34 [3072/50048]	Loss: 0.5872
Training Epoch: 34 [3200/50048]	Loss: 0.4520
Training Epoch: 34 [3328/50048]	Loss: 0.4207
Training Epoch: 34 [3456/50048]	Loss: 0.3511
Training Epoch: 34 [3584/50048]	Loss: 0.4571
Training Epoch: 34 [3712/50048]	Loss: 0.4005
Training Epoch: 34 [3840/50048]	Loss: 0.2894
Training Epoch: 34 [3968/50048]	Loss: 0.4016
Training Epoch: 34 [4096/50048]	Loss: 0.5134
Training Epoch: 34 [4224/50048]	Loss: 0.4190
Training Epoch: 34 [4352/50048]	Loss: 0.4908
Training Epoch: 34 [4480/50048]	Loss: 0.5968
Training Epoch: 34 [4608/50048]	Loss: 0.4939
Training Epoch: 34 [4736/50048]	Loss: 0.5927
Training Epoch: 34 [4864/50048]	Loss: 0.5684
Training Epoch: 34 [4992/50048]	Loss: 0.3963
Training Epoch: 34 [5120/50048]	Loss: 0.4841
Training Epoch: 34 [5248/50048]	Loss: 0.3703
Training Epoch: 34 [5376/50048]	Loss: 0.5159
Training Epoch: 34 [5504/50048]	Loss: 0.4777
Training Epoch: 34 [5632/50048]	Loss: 0.4734
Training Epoch: 34 [5760/50048]	Loss: 0.5507
Training Epoch: 34 [5888/50048]	Loss: 0.5284
Training Epoch: 34 [6016/50048]	Loss: 0.3965
Training Epoch: 34 [6144/50048]	Loss: 0.6280
Training Epoch: 34 [6272/50048]	Loss: 0.4770
Training Epoch: 34 [6400/50048]	Loss: 0.5002
Training Epoch: 34 [6528/50048]	Loss: 0.4899
Training Epoch: 34 [6656/50048]	Loss: 0.4882
Training Epoch: 34 [6784/50048]	Loss: 0.3497
Training Epoch: 34 [6912/50048]	Loss: 0.5663
Training Epoch: 34 [7040/50048]	Loss: 0.5637
Training Epoch: 34 [7168/50048]	Loss: 0.5121
Training Epoch: 34 [7296/50048]	Loss: 0.6551
Training Epoch: 34 [7424/50048]	Loss: 0.5477
Training Epoch: 34 [7552/50048]	Loss: 0.4999
Training Epoch: 34 [7680/50048]	Loss: 0.5434
Training Epoch: 34 [7808/50048]	Loss: 0.4659
Training Epoch: 34 [7936/50048]	Loss: 0.5362
Training Epoch: 34 [8064/50048]	Loss: 0.5511
Training Epoch: 34 [8192/50048]	Loss: 0.5019
Training Epoch: 34 [8320/50048]	Loss: 0.4006
Training Epoch: 34 [8448/50048]	Loss: 0.3489
Training Epoch: 34 [8576/50048]	Loss: 0.4506
Training Epoch: 34 [8704/50048]	Loss: 0.5219
Training Epoch: 34 [8832/50048]	Loss: 0.5404
Training Epoch: 34 [8960/50048]	Loss: 0.4688
Training Epoch: 34 [9088/50048]	Loss: 0.4258
Training Epoch: 34 [9216/50048]	Loss: 0.4501
Training Epoch: 34 [9344/50048]	Loss: 0.6103
Training Epoch: 34 [9472/50048]	Loss: 0.4543
Training Epoch: 34 [9600/50048]	Loss: 0.4540
Training Epoch: 34 [9728/50048]	Loss: 0.5347
Training Epoch: 34 [9856/50048]	Loss: 0.5466
Training Epoch: 34 [9984/50048]	Loss: 0.6452
Training Epoch: 34 [10112/50048]	Loss: 0.5014
Training Epoch: 34 [10240/50048]	Loss: 0.4116
Training Epoch: 34 [10368/50048]	Loss: 0.5410
Training Epoch: 34 [10496/50048]	Loss: 0.4455
Training Epoch: 34 [10624/50048]	Loss: 0.4149
Training Epoch: 34 [10752/50048]	Loss: 0.3633
Training Epoch: 34 [10880/50048]	Loss: 0.6543
Training Epoch: 34 [11008/50048]	Loss: 0.6479
Training Epoch: 34 [11136/50048]	Loss: 0.3800
Training Epoch: 34 [11264/50048]	Loss: 0.3936
Training Epoch: 34 [11392/50048]	Loss: 0.4736
Training Epoch: 34 [11520/50048]	Loss: 0.4359
Training Epoch: 34 [11648/50048]	Loss: 0.4287
Training Epoch: 34 [11776/50048]	Loss: 0.4773
Training Epoch: 34 [11904/50048]	Loss: 0.5083
Training Epoch: 34 [12032/50048]	Loss: 0.5469
Training Epoch: 34 [12160/50048]	Loss: 0.4272
Training Epoch: 34 [12288/50048]	Loss: 0.4388
Training Epoch: 34 [12416/50048]	Loss: 0.4961
Training Epoch: 34 [12544/50048]	Loss: 0.5220
Training Epoch: 34 [12672/50048]	Loss: 0.6600
Training Epoch: 34 [12800/50048]	Loss: 0.4040
Training Epoch: 34 [12928/50048]	Loss: 0.6114
Training Epoch: 34 [13056/50048]	Loss: 0.5848
Training Epoch: 34 [13184/50048]	Loss: 0.6036
Training Epoch: 34 [13312/50048]	Loss: 0.5029
Training Epoch: 34 [13440/50048]	Loss: 0.4417
Training Epoch: 34 [13568/50048]	Loss: 0.6165
Training Epoch: 34 [13696/50048]	Loss: 0.4417
Training Epoch: 34 [13824/50048]	Loss: 0.5245
Training Epoch: 34 [13952/50048]	Loss: 0.3985
Training Epoch: 34 [14080/50048]	Loss: 0.4868
Training Epoch: 34 [14208/50048]	Loss: 0.5804
Training Epoch: 34 [14336/50048]	Loss: 0.5525
Training Epoch: 34 [14464/50048]	Loss: 0.5422
Training Epoch: 34 [14592/50048]	Loss: 0.4546
Training Epoch: 34 [14720/50048]	Loss: 0.4628
Training Epoch: 34 [14848/50048]	Loss: 0.5527
Training Epoch: 34 [14976/50048]	Loss: 0.4558
Training Epoch: 34 [15104/50048]	Loss: 0.6056
Training Epoch: 34 [15232/50048]	Loss: 0.5047
Training Epoch: 34 [15360/50048]	Loss: 0.4311
Training Epoch: 34 [15488/50048]	Loss: 0.6553
Training Epoch: 34 [15616/50048]	Loss: 0.5778
Training Epoch: 34 [15744/50048]	Loss: 0.5463
Training Epoch: 34 [15872/50048]	Loss: 0.4256
Training Epoch: 34 [16000/50048]	Loss: 0.5610
Training Epoch: 34 [16128/50048]	Loss: 0.6325
Training Epoch: 34 [16256/50048]	Loss: 0.6989
Training Epoch: 34 [16384/50048]	Loss: 0.5855
Training Epoch: 34 [16512/50048]	Loss: 0.4698
Training Epoch: 34 [16640/50048]	Loss: 0.5174
Training Epoch: 34 [16768/50048]	Loss: 0.3631
Training Epoch: 34 [16896/50048]	Loss: 0.5251
Training Epoch: 34 [17024/50048]	Loss: 0.4538
Training Epoch: 34 [17152/50048]	Loss: 0.4833
Training Epoch: 34 [17280/50048]	Loss: 0.4646
Training Epoch: 34 [17408/50048]	Loss: 0.4638
Training Epoch: 34 [17536/50048]	Loss: 0.5450
Training Epoch: 34 [17664/50048]	Loss: 0.4874
Training Epoch: 34 [17792/50048]	Loss: 0.5200
Training Epoch: 34 [17920/50048]	Loss: 0.4824
Training Epoch: 34 [18048/50048]	Loss: 0.6429
Training Epoch: 34 [18176/50048]	Loss: 0.5325
Training Epoch: 34 [18304/50048]	Loss: 0.4842
Training Epoch: 34 [18432/50048]	Loss: 0.5766
Training Epoch: 34 [18560/50048]	Loss: 0.5610
Training Epoch: 34 [18688/50048]	Loss: 0.5734
Training Epoch: 34 [18816/50048]	Loss: 0.5807
Training Epoch: 34 [18944/50048]	Loss: 0.3607
Training Epoch: 34 [19072/50048]	Loss: 0.4558
Training Epoch: 34 [19200/50048]	Loss: 0.5267
Training Epoch: 34 [19328/50048]	Loss: 0.4308
Training Epoch: 34 [19456/50048]	Loss: 0.4788
Training Epoch: 34 [19584/50048]	Loss: 0.6531
Training Epoch: 34 [19712/50048]	Loss: 0.4966
Training Epoch: 34 [19840/50048]	Loss: 0.5489
Training Epoch: 34 [19968/50048]	Loss: 0.4946
Training Epoch: 34 [20096/50048]	Loss: 0.7053
Training Epoch: 34 [20224/50048]	Loss: 0.5519
Training Epoch: 34 [20352/50048]	Loss: 0.4750
Training Epoch: 34 [20480/50048]	Loss: 0.6504
Training Epoch: 34 [20608/50048]	Loss: 0.5646
Training Epoch: 34 [20736/50048]	Loss: 0.6027
Training Epoch: 34 [20864/50048]	Loss: 0.6608
Training Epoch: 34 [20992/50048]	Loss: 0.6714
Training Epoch: 34 [21120/50048]	Loss: 0.5292
Training Epoch: 34 [21248/50048]	Loss: 0.4889
Training Epoch: 34 [21376/50048]	Loss: 0.6595
Training Epoch: 34 [21504/50048]	Loss: 0.6391
Training Epoch: 34 [21632/50048]	Loss: 0.5213
Training Epoch: 34 [21760/50048]	Loss: 0.6083
Training Epoch: 34 [21888/50048]	Loss: 0.3892
Training Epoch: 34 [22016/50048]	Loss: 0.5066
Training Epoch: 34 [22144/50048]	Loss: 0.5386
Training Epoch: 34 [22272/50048]	Loss: 0.4991
Training Epoch: 34 [22400/50048]	Loss: 0.5031
Training Epoch: 34 [22528/50048]	Loss: 0.5381
Training Epoch: 34 [22656/50048]	Loss: 0.5969
Training Epoch: 34 [22784/50048]	Loss: 0.5270
Training Epoch: 34 [22912/50048]	Loss: 0.5404
Training Epoch: 34 [23040/50048]	Loss: 0.5483
Training Epoch: 34 [23168/50048]	Loss: 0.4956
Training Epoch: 34 [23296/50048]	Loss: 0.5753
Training Epoch: 34 [23424/50048]	Loss: 0.4732
Training Epoch: 34 [23552/50048]	Loss: 0.4373
Training Epoch: 34 [23680/50048]	Loss: 0.6687
Training Epoch: 34 [23808/50048]	Loss: 0.6155
Training Epoch: 34 [23936/50048]	Loss: 0.3973
Training Epoch: 34 [24064/50048]	Loss: 0.5097
Training Epoch: 34 [24192/50048]	Loss: 0.5430
Training Epoch: 34 [24320/50048]	Loss: 0.5254
Training Epoch: 34 [24448/50048]	Loss: 0.4233
Training Epoch: 34 [24576/50048]	Loss: 0.5598
Training Epoch: 34 [24704/50048]	Loss: 0.5149
Training Epoch: 34 [24832/50048]	Loss: 0.6125
Training Epoch: 34 [24960/50048]	Loss: 0.6073
Training Epoch: 34 [25088/50048]	Loss: 0.6323
Training Epoch: 34 [25216/50048]	Loss: 0.5575
Training Epoch: 34 [25344/50048]	Loss: 0.4928
Training Epoch: 34 [25472/50048]	Loss: 0.6305
Training Epoch: 34 [25600/50048]	Loss: 0.4476
Training Epoch: 34 [25728/50048]	Loss: 0.5892
Training Epoch: 34 [25856/50048]	Loss: 0.5217
Training Epoch: 34 [25984/50048]	Loss: 0.6274
Training Epoch: 34 [26112/50048]	Loss: 0.4819
Training Epoch: 34 [26240/50048]	Loss: 0.4358
Training Epoch: 34 [26368/50048]	Loss: 0.6387
Training Epoch: 34 [26496/50048]	Loss: 0.4998
Training Epoch: 34 [26624/50048]	Loss: 0.7077
Training Epoch: 34 [26752/50048]	Loss: 0.6287
Training Epoch: 34 [26880/50048]	Loss: 0.6123
Training Epoch: 34 [27008/50048]	Loss: 0.5778
Training Epoch: 34 [27136/50048]	Loss: 0.7031
Training Epoch: 34 [27264/50048]	Loss: 0.4675
Training Epoch: 34 [27392/50048]	Loss: 0.5309
Training Epoch: 34 [27520/50048]	Loss: 0.3694
Training Epoch: 34 [27648/50048]	Loss: 0.6373
Training Epoch: 34 [27776/50048]	Loss: 0.4989
Training Epoch: 34 [27904/50048]	Loss: 0.6194
Training Epoch: 34 [28032/50048]	Loss: 0.5002
Training Epoch: 34 [28160/50048]	Loss: 0.5922
Training Epoch: 34 [28288/50048]	Loss: 0.6850
Training Epoch: 34 [28416/50048]	Loss: 0.5772
Training Epoch: 34 [28544/50048]	Loss: 0.4875
Training Epoch: 34 [28672/50048]	Loss: 0.4865
Training Epoch: 34 [28800/50048]	Loss: 0.3744
Training Epoch: 34 [28928/50048]	Loss: 0.6370
Training Epoch: 34 [29056/50048]	Loss: 0.5685
Training Epoch: 34 [29184/50048]	Loss: 0.3898
Training Epoch: 34 [29312/50048]	Loss: 0.4403
Training Epoch: 34 [29440/50048]	Loss: 0.6555
Training Epoch: 34 [29568/50048]	Loss: 0.6077
Training Epoch: 34 [29696/50048]	Loss: 0.6333
Training Epoch: 34 [29824/50048]	Loss: 0.6640
Training Epoch: 34 [29952/50048]	Loss: 0.6488
Training Epoch: 34 [30080/50048]	Loss: 0.5076
Training Epoch: 34 [30208/50048]	Loss: 0.4766
Training Epoch: 34 [30336/50048]	Loss: 0.7449
Training Epoch: 34 [30464/50048]	Loss: 0.4842
Training Epoch: 34 [30592/50048]	Loss: 0.5235
Training Epoch: 34 [30720/50048]	Loss: 0.6713
Training Epoch: 34 [30848/50048]	Loss: 0.6262
Training Epoch: 34 [30976/50048]	Loss: 0.4368
Training Epoch: 34 [31104/50048]	Loss: 0.6315
Training Epoch: 34 [31232/50048]	Loss: 0.6245
Training Epoch: 34 [31360/50048]	Loss: 0.5826
Training Epoch: 34 [31488/50048]	Loss: 0.7565
Training Epoch: 34 [31616/50048]	Loss: 0.5222
Training Epoch: 34 [31744/50048]	Loss: 0.5660
Training Epoch: 34 [31872/50048]	Loss: 0.6256
Training Epoch: 34 [32000/50048]	Loss: 0.4986
Training Epoch: 34 [32128/50048]	Loss: 0.5935
Training Epoch: 34 [32256/50048]	Loss: 0.5943
Training Epoch: 34 [32384/50048]	Loss: 0.7180
Training Epoch: 34 [32512/50048]	Loss: 0.5630
Training Epoch: 34 [32640/50048]	Loss: 0.6595
Training Epoch: 34 [32768/50048]	Loss: 0.7236
Training Epoch: 34 [32896/50048]	Loss: 0.5954
Training Epoch: 34 [33024/50048]	Loss: 0.6545
Training Epoch: 34 [33152/50048]	Loss: 0.4530
Training Epoch: 34 [33280/50048]	Loss: 0.8222
Training Epoch: 34 [33408/50048]	Loss: 0.5274
Training Epoch: 34 [33536/50048]	Loss: 0.5594
Training Epoch: 34 [33664/50048]	Loss: 0.4280
Training Epoch: 34 [33792/50048]	Loss: 0.5208
Training Epoch: 34 [33920/50048]	Loss: 0.5859
Training Epoch: 34 [34048/50048]	Loss: 0.6366
Training Epoch: 34 [34176/50048]	Loss: 0.4730
Training Epoch: 34 [34304/50048]	Loss: 0.5144
Training Epoch: 34 [34432/50048]	Loss: 0.6358
Training Epoch: 34 [34560/50048]	Loss: 0.4871
Training Epoch: 34 [34688/50048]	Loss: 0.7381
Training Epoch: 34 [34816/50048]	Loss: 0.6960
Training Epoch: 34 [34944/50048]	Loss: 0.5607
Training Epoch: 34 [35072/50048]	Loss: 0.5308
Training Epoch: 34 [35200/50048]	Loss: 0.6480
Training Epoch: 34 [35328/50048]	Loss: 0.7413
Training Epoch: 34 [35456/50048]	Loss: 0.5107
Training Epoch: 34 [35584/50048]	Loss: 0.5866
Training Epoch: 34 [35712/50048]	Loss: 0.6394
Training Epoch: 34 [35840/50048]	Loss: 0.5701
Training Epoch: 34 [35968/50048]	Loss: 0.7145
Training Epoch: 34 [36096/50048]	Loss: 0.4945
Training Epoch: 34 [36224/50048]	Loss: 0.4919
Training Epoch: 34 [36352/50048]	Loss: 0.4762
Training Epoch: 34 [36480/50048]	Loss: 0.7289
Training Epoch: 34 [36608/50048]	Loss: 0.4609
Training Epoch: 34 [36736/50048]	Loss: 0.6628
Training Epoch: 34 [36864/50048]	Loss: 0.5210
Training Epoch: 34 [36992/50048]	Loss: 0.4501
Training Epoch: 34 [37120/50048]	Loss: 0.7004
Training Epoch: 34 [37248/50048]	Loss: 0.4798
Training Epoch: 34 [37376/50048]	Loss: 0.5098
Training Epoch: 34 [37504/50048]	Loss: 0.4456
Training Epoch: 34 [37632/50048]	Loss: 0.6289
Training Epoch: 34 [37760/50048]	Loss: 0.6750
Training Epoch: 34 [37888/50048]	Loss: 0.4824
Training Epoch: 34 [38016/50048]	Loss: 0.4915
Training Epoch: 34 [38144/50048]	Loss: 0.5461
Training Epoch: 34 [38272/50048]	Loss: 0.5579
Training Epoch: 34 [38400/50048]	Loss: 0.5713
Training Epoch: 34 [38528/50048]	Loss: 0.4406
Training Epoch: 34 [38656/50048]	Loss: 0.6283
Training Epoch: 34 [38784/50048]	Loss: 0.7838
Training Epoch: 34 [38912/50048]	Loss: 0.5642
Training Epoch: 34 [39040/50048]	Loss: 0.6066
Training Epoch: 34 [39168/50048]	Loss: 0.7032
Training Epoch: 34 [39296/50048]	Loss: 0.7102
Training Epoch: 34 [39424/50048]	Loss: 0.5377
Training Epoch: 34 [39552/50048]	Loss: 0.5417
Training Epoch: 34 [39680/50048]	Loss: 0.6025
Training Epoch: 34 [39808/50048]	Loss: 0.6162
Training Epoch: 34 [39936/50048]	Loss: 0.4616
Training Epoch: 34 [40064/50048]	Loss: 0.4519
Training Epoch: 34 [40192/50048]	Loss: 0.5282
Training Epoch: 34 [40320/50048]	Loss: 0.6552
Training Epoch: 34 [40448/50048]	Loss: 0.7441
Training Epoch: 34 [40576/50048]	Loss: 0.4747
Training Epoch: 34 [40704/50048]	Loss: 0.6423
Training Epoch: 34 [40832/50048]	Loss: 0.6106
Training Epoch: 34 [40960/50048]	Loss: 0.5666
Training Epoch: 34 [41088/50048]	Loss: 0.4534
Training Epoch: 34 [41216/50048]	Loss: 0.3755
Training Epoch: 34 [41344/50048]	Loss: 0.4886
Training Epoch: 34 [41472/50048]	Loss: 0.5650
Training Epoch: 34 [41600/50048]	Loss: 0.5788
Training Epoch: 34 [41728/50048]	Loss: 0.5882
Training Epoch: 34 [41856/50048]	Loss: 0.6549
Training Epoch: 34 [41984/50048]	Loss: 0.6561
Training Epoch: 34 [42112/50048]	Loss: 0.5079
Training Epoch: 34 [42240/50048]	Loss: 0.5510
Training Epoch: 34 [42368/50048]	Loss: 0.5204
Training Epoch: 34 [42496/50048]	Loss: 0.4729
Training Epoch: 34 [42624/50048]	Loss: 0.5011
Training Epoch: 34 [42752/50048]	Loss: 0.6115
Training Epoch: 34 [42880/50048]	Loss: 0.4512
Training Epoch: 34 [43008/50048]	Loss: 0.6933
Training Epoch: 34 [43136/50048]	Loss: 0.4535
Training Epoch: 34 [43264/50048]	Loss: 0.4464
Training Epoch: 34 [43392/50048]	Loss: 0.5384
Training Epoch: 34 [43520/50048]	Loss: 0.6775
Training Epoch: 34 [43648/50048]	Loss: 0.6899
Training Epoch: 34 [43776/50048]	Loss: 0.5231
Training Epoch: 34 [43904/50048]	Loss: 0.5108
Training Epoch: 34 [44032/50048]	Loss: 0.4269
Training Epoch: 34 [44160/50048]	Loss: 0.6487
Training Epoch: 34 [44288/50048]	Loss: 0.6013
Training Epoch: 34 [44416/50048]	Loss: 0.6313
Training Epoch: 34 [44544/50048]	Loss: 0.5852
Training Epoch: 34 [44672/50048]	Loss: 0.5121
Training Epoch: 34 [44800/50048]	Loss: 0.6990
Training Epoch: 34 [44928/50048]	Loss: 0.4849
Training Epoch: 34 [45056/50048]	Loss: 0.5876
Training Epoch: 34 [45184/50048]	Loss: 0.6733
Training Epoch: 34 [45312/50048]	Loss: 0.4712
Training Epoch: 34 [45440/50048]	Loss: 0.4982
Training Epoch: 34 [45568/50048]	Loss: 0.5513
Training Epoch: 34 [45696/50048]	Loss: 0.6266
2022-12-06 04:29:58,894 [ZeusDataLoader(train)] train epoch 35 done: time=86.38 energy=10497.43
2022-12-06 04:29:58,896 [ZeusDataLoader(eval)] Epoch 35 begin.
Training Epoch: 34 [45824/50048]	Loss: 0.4956
Training Epoch: 34 [45952/50048]	Loss: 0.6041
Training Epoch: 34 [46080/50048]	Loss: 0.4580
Training Epoch: 34 [46208/50048]	Loss: 0.5651
Training Epoch: 34 [46336/50048]	Loss: 0.7275
Training Epoch: 34 [46464/50048]	Loss: 0.6191
Training Epoch: 34 [46592/50048]	Loss: 0.7917
Training Epoch: 34 [46720/50048]	Loss: 0.6017
Training Epoch: 34 [46848/50048]	Loss: 0.5437
Training Epoch: 34 [46976/50048]	Loss: 0.6526
Training Epoch: 34 [47104/50048]	Loss: 0.5200
Training Epoch: 34 [47232/50048]	Loss: 0.6433
Training Epoch: 34 [47360/50048]	Loss: 0.5828
Training Epoch: 34 [47488/50048]	Loss: 0.5773
Training Epoch: 34 [47616/50048]	Loss: 0.4464
Training Epoch: 34 [47744/50048]	Loss: 0.5807
Training Epoch: 34 [47872/50048]	Loss: 0.5156
Training Epoch: 34 [48000/50048]	Loss: 0.4848
Training Epoch: 34 [48128/50048]	Loss: 0.6672
Training Epoch: 34 [48256/50048]	Loss: 0.6238
Training Epoch: 34 [48384/50048]	Loss: 0.4662
Training Epoch: 34 [48512/50048]	Loss: 0.5539
Training Epoch: 34 [48640/50048]	Loss: 0.8552
Training Epoch: 34 [48768/50048]	Loss: 0.5629
Training Epoch: 34 [48896/50048]	Loss: 0.4602
Training Epoch: 34 [49024/50048]	Loss: 0.6265
Training Epoch: 34 [49152/50048]	Loss: 0.5588
Training Epoch: 34 [49280/50048]	Loss: 0.6010
Training Epoch: 34 [49408/50048]	Loss: 0.5268
Training Epoch: 34 [49536/50048]	Loss: 0.6011
Training Epoch: 34 [49664/50048]	Loss: 0.7576
Training Epoch: 34 [49792/50048]	Loss: 0.7305
Training Epoch: 34 [49920/50048]	Loss: 0.5400
Training Epoch: 34 [50048/50048]	Loss: 0.3927
2022-12-06 09:30:02.601 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:30:02,615 [ZeusDataLoader(eval)] eval epoch 35 done: time=3.71 energy=453.28
2022-12-06 04:30:02,615 [ZeusDataLoader(train)] Up to epoch 35: time=3159.63, energy=383423.83, cost=468179.53
2022-12-06 04:30:02,615 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:30:02,615 [ZeusDataLoader(train)] Expected next epoch: time=3249.43, energy=394221.85, cost=481435.91
2022-12-06 04:30:02,616 [ZeusDataLoader(train)] Epoch 36 begin.
Validation Epoch: 34, Average loss: 0.0130, Accuracy: 0.6206
2022-12-06 04:30:02,801 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:30:02,801 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:30:02.803 [ZeusMonitor] Monitor started.
2022-12-06 09:30:02.803 [ZeusMonitor] Running indefinitely. 2022-12-06 09:30:02.803 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:30:02.803 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e36+gpu0.power.log
Training Epoch: 35 [128/50048]	Loss: 0.4761
Training Epoch: 35 [256/50048]	Loss: 0.4897
Training Epoch: 35 [384/50048]	Loss: 0.4418
Training Epoch: 35 [512/50048]	Loss: 0.3923
Training Epoch: 35 [640/50048]	Loss: 0.5740
Training Epoch: 35 [768/50048]	Loss: 0.4898
Training Epoch: 35 [896/50048]	Loss: 0.3601
Training Epoch: 35 [1024/50048]	Loss: 0.4997
Training Epoch: 35 [1152/50048]	Loss: 0.5538
Training Epoch: 35 [1280/50048]	Loss: 0.5478
Training Epoch: 35 [1408/50048]	Loss: 0.5328
Training Epoch: 35 [1536/50048]	Loss: 0.5004
Training Epoch: 35 [1664/50048]	Loss: 0.5394
Training Epoch: 35 [1792/50048]	Loss: 0.4808
Training Epoch: 35 [1920/50048]	Loss: 0.5288
Training Epoch: 35 [2048/50048]	Loss: 0.4815
Training Epoch: 35 [2176/50048]	Loss: 0.4219
Training Epoch: 35 [2304/50048]	Loss: 0.4197
Training Epoch: 35 [2432/50048]	Loss: 0.5318
Training Epoch: 35 [2560/50048]	Loss: 0.4447
Training Epoch: 35 [2688/50048]	Loss: 0.4530
Training Epoch: 35 [2816/50048]	Loss: 0.5060
Training Epoch: 35 [2944/50048]	Loss: 0.3622
Training Epoch: 35 [3072/50048]	Loss: 0.4794
Training Epoch: 35 [3200/50048]	Loss: 0.4072
Training Epoch: 35 [3328/50048]	Loss: 0.4188
Training Epoch: 35 [3456/50048]	Loss: 0.4902
Training Epoch: 35 [3584/50048]	Loss: 0.4622
Training Epoch: 35 [3712/50048]	Loss: 0.4914
Training Epoch: 35 [3840/50048]	Loss: 0.4640
Training Epoch: 35 [3968/50048]	Loss: 0.6191
Training Epoch: 35 [4096/50048]	Loss: 0.5074
Training Epoch: 35 [4224/50048]	Loss: 0.5980
Training Epoch: 35 [4352/50048]	Loss: 0.3285
Training Epoch: 35 [4480/50048]	Loss: 0.3426
Training Epoch: 35 [4608/50048]	Loss: 0.4624
Training Epoch: 35 [4736/50048]	Loss: 0.5628
Training Epoch: 35 [4864/50048]	Loss: 0.2944
Training Epoch: 35 [4992/50048]	Loss: 0.5465
Training Epoch: 35 [5120/50048]	Loss: 0.4391
Training Epoch: 35 [5248/50048]	Loss: 0.3467
Training Epoch: 35 [5376/50048]	Loss: 0.4991
Training Epoch: 35 [5504/50048]	Loss: 0.4781
Training Epoch: 35 [5632/50048]	Loss: 0.4522
Training Epoch: 35 [5760/50048]	Loss: 0.5466
Training Epoch: 35 [5888/50048]	Loss: 0.3677
Training Epoch: 35 [6016/50048]	Loss: 0.4949
Training Epoch: 35 [6144/50048]	Loss: 0.5194
Training Epoch: 35 [6272/50048]	Loss: 0.4452
Training Epoch: 35 [6400/50048]	Loss: 0.4430
Training Epoch: 35 [6528/50048]	Loss: 0.3651
Training Epoch: 35 [6656/50048]	Loss: 0.4202
Training Epoch: 35 [6784/50048]	Loss: 0.4136
Training Epoch: 35 [6912/50048]	Loss: 0.4268
Training Epoch: 35 [7040/50048]	Loss: 0.5429
Training Epoch: 35 [7168/50048]	Loss: 0.4590
Training Epoch: 35 [7296/50048]	Loss: 0.3523
Training Epoch: 35 [7424/50048]	Loss: 0.4550
Training Epoch: 35 [7552/50048]	Loss: 0.3476
Training Epoch: 35 [7680/50048]	Loss: 0.3625
Training Epoch: 35 [7808/50048]	Loss: 0.4249
Training Epoch: 35 [7936/50048]	Loss: 0.3748
Training Epoch: 35 [8064/50048]	Loss: 0.4622
Training Epoch: 35 [8192/50048]	Loss: 0.5929
Training Epoch: 35 [8320/50048]	Loss: 0.4318
Training Epoch: 35 [8448/50048]	Loss: 0.3888
Training Epoch: 35 [8576/50048]	Loss: 0.5405
Training Epoch: 35 [8704/50048]	Loss: 0.6152
Training Epoch: 35 [8832/50048]	Loss: 0.4477
Training Epoch: 35 [8960/50048]	Loss: 0.5319
Training Epoch: 35 [9088/50048]	Loss: 0.5013
Training Epoch: 35 [9216/50048]	Loss: 0.3645
Training Epoch: 35 [9344/50048]	Loss: 0.5795
Training Epoch: 35 [9472/50048]	Loss: 0.4964
Training Epoch: 35 [9600/50048]	Loss: 0.4579
Training Epoch: 35 [9728/50048]	Loss: 0.5206
Training Epoch: 35 [9856/50048]	Loss: 0.4773
Training Epoch: 35 [9984/50048]	Loss: 0.5849
Training Epoch: 35 [10112/50048]	Loss: 0.5121
Training Epoch: 35 [10240/50048]	Loss: 0.4060
Training Epoch: 35 [10368/50048]	Loss: 0.4803
Training Epoch: 35 [10496/50048]	Loss: 0.5226
Training Epoch: 35 [10624/50048]	Loss: 0.4065
Training Epoch: 35 [10752/50048]	Loss: 0.4923
Training Epoch: 35 [10880/50048]	Loss: 0.5219
Training Epoch: 35 [11008/50048]	Loss: 0.4634
Training Epoch: 35 [11136/50048]	Loss: 0.4906
Training Epoch: 35 [11264/50048]	Loss: 0.4648
Training Epoch: 35 [11392/50048]	Loss: 0.5209
Training Epoch: 35 [11520/50048]	Loss: 0.5321
Training Epoch: 35 [11648/50048]	Loss: 0.3828
Training Epoch: 35 [11776/50048]	Loss: 0.4615
Training Epoch: 35 [11904/50048]	Loss: 0.6171
Training Epoch: 35 [12032/50048]	Loss: 0.4721
Training Epoch: 35 [12160/50048]	Loss: 0.4649
Training Epoch: 35 [12288/50048]	Loss: 0.5967
Training Epoch: 35 [12416/50048]	Loss: 0.5825
Training Epoch: 35 [12544/50048]	Loss: 0.5006
Training Epoch: 35 [12672/50048]	Loss: 0.4596
Training Epoch: 35 [12800/50048]	Loss: 0.4166
Training Epoch: 35 [12928/50048]	Loss: 0.3810
Training Epoch: 35 [13056/50048]	Loss: 0.4760
Training Epoch: 35 [13184/50048]	Loss: 0.6142
Training Epoch: 35 [13312/50048]	Loss: 0.5420
Training Epoch: 35 [13440/50048]	Loss: 0.5361
Training Epoch: 35 [13568/50048]	Loss: 0.5853
Training Epoch: 35 [13696/50048]	Loss: 0.6167
Training Epoch: 35 [13824/50048]	Loss: 0.3686
Training Epoch: 35 [13952/50048]	Loss: 0.5689
Training Epoch: 35 [14080/50048]	Loss: 0.5350
Training Epoch: 35 [14208/50048]	Loss: 0.5339
Training Epoch: 35 [14336/50048]	Loss: 0.4714
Training Epoch: 35 [14464/50048]	Loss: 0.5343
Training Epoch: 35 [14592/50048]	Loss: 0.4334
Training Epoch: 35 [14720/50048]	Loss: 0.4924
Training Epoch: 35 [14848/50048]	Loss: 0.5140
Training Epoch: 35 [14976/50048]	Loss: 0.4406
Training Epoch: 35 [15104/50048]	Loss: 0.3690
Training Epoch: 35 [15232/50048]	Loss: 0.5417
Training Epoch: 35 [15360/50048]	Loss: 0.4066
Training Epoch: 35 [15488/50048]	Loss: 0.4753
Training Epoch: 35 [15616/50048]	Loss: 0.4586
Training Epoch: 35 [15744/50048]	Loss: 0.4743
Training Epoch: 35 [15872/50048]	Loss: 0.5182
Training Epoch: 35 [16000/50048]	Loss: 0.4214
Training Epoch: 35 [16128/50048]	Loss: 0.4748
Training Epoch: 35 [16256/50048]	Loss: 0.4744
Training Epoch: 35 [16384/50048]	Loss: 0.4319
Training Epoch: 35 [16512/50048]	Loss: 0.4367
Training Epoch: 35 [16640/50048]	Loss: 0.5563
Training Epoch: 35 [16768/50048]	Loss: 0.4539
Training Epoch: 35 [16896/50048]	Loss: 0.4623
Training Epoch: 35 [17024/50048]	Loss: 0.3764
Training Epoch: 35 [17152/50048]	Loss: 0.4614
Training Epoch: 35 [17280/50048]	Loss: 0.4794
Training Epoch: 35 [17408/50048]	Loss: 0.6285
Training Epoch: 35 [17536/50048]	Loss: 0.6614
Training Epoch: 35 [17664/50048]	Loss: 0.4006
Training Epoch: 35 [17792/50048]	Loss: 0.5598
Training Epoch: 35 [17920/50048]	Loss: 0.5716
Training Epoch: 35 [18048/50048]	Loss: 0.6006
Training Epoch: 35 [18176/50048]	Loss: 0.4001
Training Epoch: 35 [18304/50048]	Loss: 0.4403
Training Epoch: 35 [18432/50048]	Loss: 0.5380
Training Epoch: 35 [18560/50048]	Loss: 0.4844
Training Epoch: 35 [18688/50048]	Loss: 0.4813
Training Epoch: 35 [18816/50048]	Loss: 0.5493
Training Epoch: 35 [18944/50048]	Loss: 0.4801
Training Epoch: 35 [19072/50048]	Loss: 0.5664
Training Epoch: 35 [19200/50048]	Loss: 0.5061
Training Epoch: 35 [19328/50048]	Loss: 0.4080
Training Epoch: 35 [19456/50048]	Loss: 0.4287
Training Epoch: 35 [19584/50048]	Loss: 0.4501
Training Epoch: 35 [19712/50048]	Loss: 0.6279
Training Epoch: 35 [19840/50048]	Loss: 0.5811
Training Epoch: 35 [19968/50048]	Loss: 0.4934
Training Epoch: 35 [20096/50048]	Loss: 0.4717
Training Epoch: 35 [20224/50048]	Loss: 0.4962
Training Epoch: 35 [20352/50048]	Loss: 0.5182
Training Epoch: 35 [20480/50048]	Loss: 0.4019
Training Epoch: 35 [20608/50048]	Loss: 0.5977
Training Epoch: 35 [20736/50048]	Loss: 0.3793
Training Epoch: 35 [20864/50048]	Loss: 0.4455
Training Epoch: 35 [20992/50048]	Loss: 0.4428
Training Epoch: 35 [21120/50048]	Loss: 0.5859
Training Epoch: 35 [21248/50048]	Loss: 0.4950
Training Epoch: 35 [21376/50048]	Loss: 0.5255
Training Epoch: 35 [21504/50048]	Loss: 0.6206
Training Epoch: 35 [21632/50048]	Loss: 0.5429
Training Epoch: 35 [21760/50048]	Loss: 0.4276
Training Epoch: 35 [21888/50048]	Loss: 0.5885
Training Epoch: 35 [22016/50048]	Loss: 0.5761
Training Epoch: 35 [22144/50048]	Loss: 0.4294
Training Epoch: 35 [22272/50048]	Loss: 0.5761
Training Epoch: 35 [22400/50048]	Loss: 0.7173
Training Epoch: 35 [22528/50048]	Loss: 0.5056
Training Epoch: 35 [22656/50048]	Loss: 0.5845
Training Epoch: 35 [22784/50048]	Loss: 0.5213
Training Epoch: 35 [22912/50048]	Loss: 0.5508
Training Epoch: 35 [23040/50048]	Loss: 0.4628
Training Epoch: 35 [23168/50048]	Loss: 0.5638
Training Epoch: 35 [23296/50048]	Loss: 0.5905
Training Epoch: 35 [23424/50048]	Loss: 0.4188
Training Epoch: 35 [23552/50048]	Loss: 0.6406
Training Epoch: 35 [23680/50048]	Loss: 0.6062
Training Epoch: 35 [23808/50048]	Loss: 0.4925
Training Epoch: 35 [23936/50048]	Loss: 0.6283
Training Epoch: 35 [24064/50048]	Loss: 0.5919
Training Epoch: 35 [24192/50048]	Loss: 0.4053
Training Epoch: 35 [24320/50048]	Loss: 0.5101
Training Epoch: 35 [24448/50048]	Loss: 0.5173
Training Epoch: 35 [24576/50048]	Loss: 0.6343
Training Epoch: 35 [24704/50048]	Loss: 0.5474
Training Epoch: 35 [24832/50048]	Loss: 0.5165
Training Epoch: 35 [24960/50048]	Loss: 0.4027
Training Epoch: 35 [25088/50048]	Loss: 0.4450
Training Epoch: 35 [25216/50048]	Loss: 0.5868
Training Epoch: 35 [25344/50048]	Loss: 0.4934
Training Epoch: 35 [25472/50048]	Loss: 0.5232
Training Epoch: 35 [25600/50048]	Loss: 0.3889
Training Epoch: 35 [25728/50048]	Loss: 0.4652
Training Epoch: 35 [25856/50048]	Loss: 0.7083
Training Epoch: 35 [25984/50048]	Loss: 0.5102
Training Epoch: 35 [26112/50048]	Loss: 0.5194
Training Epoch: 35 [26240/50048]	Loss: 0.5797
Training Epoch: 35 [26368/50048]	Loss: 0.6561
Training Epoch: 35 [26496/50048]	Loss: 0.4599
Training Epoch: 35 [26624/50048]	Loss: 0.5001
Training Epoch: 35 [26752/50048]	Loss: 0.4429
Training Epoch: 35 [26880/50048]	Loss: 0.6050
Training Epoch: 35 [27008/50048]	Loss: 0.3724
Training Epoch: 35 [27136/50048]	Loss: 0.5413
Training Epoch: 35 [27264/50048]	Loss: 0.4351
Training Epoch: 35 [27392/50048]	Loss: 0.4039
Training Epoch: 35 [27520/50048]	Loss: 0.4194
Training Epoch: 35 [27648/50048]	Loss: 0.6290
Training Epoch: 35 [27776/50048]	Loss: 0.4489
Training Epoch: 35 [27904/50048]	Loss: 0.5066
Training Epoch: 35 [28032/50048]	Loss: 0.5603
Training Epoch: 35 [28160/50048]	Loss: 0.4692
Training Epoch: 35 [28288/50048]	Loss: 0.6213
Training Epoch: 35 [28416/50048]	Loss: 0.6964
Training Epoch: 35 [28544/50048]	Loss: 0.5517
Training Epoch: 35 [28672/50048]	Loss: 0.4997
Training Epoch: 35 [28800/50048]	Loss: 0.4189
Training Epoch: 35 [28928/50048]	Loss: 0.5009
Training Epoch: 35 [29056/50048]	Loss: 0.4801
Training Epoch: 35 [29184/50048]	Loss: 0.6099
Training Epoch: 35 [29312/50048]	Loss: 0.4631
Training Epoch: 35 [29440/50048]	Loss: 0.3747
Training Epoch: 35 [29568/50048]	Loss: 0.5381
Training Epoch: 35 [29696/50048]	Loss: 0.6025
Training Epoch: 35 [29824/50048]	Loss: 0.5295
Training Epoch: 35 [29952/50048]	Loss: 0.4511
Training Epoch: 35 [30080/50048]	Loss: 0.4503
Training Epoch: 35 [30208/50048]	Loss: 0.5509
Training Epoch: 35 [30336/50048]	Loss: 0.4936
Training Epoch: 35 [30464/50048]	Loss: 0.6488
Training Epoch: 35 [30592/50048]	Loss: 0.4047
Training Epoch: 35 [30720/50048]	Loss: 0.5433
Training Epoch: 35 [30848/50048]	Loss: 0.4402
Training Epoch: 35 [30976/50048]	Loss: 0.4318
Training Epoch: 35 [31104/50048]	Loss: 0.4830
Training Epoch: 35 [31232/50048]	Loss: 0.4723
Training Epoch: 35 [31360/50048]	Loss: 0.4232
Training Epoch: 35 [31488/50048]	Loss: 0.5239
Training Epoch: 35 [31616/50048]	Loss: 0.6094
Training Epoch: 35 [31744/50048]	Loss: 0.4190
Training Epoch: 35 [31872/50048]	Loss: 0.3442
Training Epoch: 35 [32000/50048]	Loss: 0.4265
Training Epoch: 35 [32128/50048]	Loss: 0.4113
Training Epoch: 35 [32256/50048]	Loss: 0.6711
Training Epoch: 35 [32384/50048]	Loss: 0.6079
Training Epoch: 35 [32512/50048]	Loss: 0.5546
Training Epoch: 35 [32640/50048]	Loss: 0.4485
Training Epoch: 35 [32768/50048]	Loss: 0.4648
Training Epoch: 35 [32896/50048]	Loss: 0.5344
Training Epoch: 35 [33024/50048]	Loss: 0.5144
Training Epoch: 35 [33152/50048]	Loss: 0.7617
Training Epoch: 35 [33280/50048]	Loss: 0.3795
Training Epoch: 35 [33408/50048]	Loss: 0.6201
Training Epoch: 35 [33536/50048]	Loss: 0.4175
Training Epoch: 35 [33664/50048]	Loss: 0.5242
Training Epoch: 35 [33792/50048]	Loss: 0.5314
Training Epoch: 35 [33920/50048]	Loss: 0.4975
Training Epoch: 35 [34048/50048]	Loss: 0.4503
Training Epoch: 35 [34176/50048]	Loss: 0.5006
Training Epoch: 35 [34304/50048]	Loss: 0.5714
Training Epoch: 35 [34432/50048]	Loss: 0.5382
Training Epoch: 35 [34560/50048]	Loss: 0.4243
Training Epoch: 35 [34688/50048]	Loss: 0.5801
Training Epoch: 35 [34816/50048]	Loss: 0.4444
Training Epoch: 35 [34944/50048]	Loss: 0.4201
Training Epoch: 35 [35072/50048]	Loss: 0.4984
Training Epoch: 35 [35200/50048]	Loss: 0.4465
Training Epoch: 35 [35328/50048]	Loss: 0.5600
Training Epoch: 35 [35456/50048]	Loss: 0.6950
Training Epoch: 35 [35584/50048]	Loss: 0.4679
Training Epoch: 35 [35712/50048]	Loss: 0.3841
Training Epoch: 35 [35840/50048]	Loss: 0.6424
Training Epoch: 35 [35968/50048]	Loss: 0.5558
Training Epoch: 35 [36096/50048]	Loss: 0.5747
Training Epoch: 35 [36224/50048]	Loss: 0.6285
Training Epoch: 35 [36352/50048]	Loss: 0.3675
Training Epoch: 35 [36480/50048]	Loss: 0.6104
Training Epoch: 35 [36608/50048]	Loss: 0.3746
Training Epoch: 35 [36736/50048]	Loss: 0.4953
Training Epoch: 35 [36864/50048]	Loss: 0.6480
Training Epoch: 35 [36992/50048]	Loss: 0.5998
Training Epoch: 35 [37120/50048]	Loss: 0.4202
Training Epoch: 35 [37248/50048]	Loss: 0.5952
Training Epoch: 35 [37376/50048]	Loss: 0.5140
Training Epoch: 35 [37504/50048]	Loss: 0.7467
Training Epoch: 35 [37632/50048]	Loss: 0.5776
Training Epoch: 35 [37760/50048]	Loss: 0.4776
Training Epoch: 35 [37888/50048]	Loss: 0.4245
Training Epoch: 35 [38016/50048]	Loss: 0.5008
Training Epoch: 35 [38144/50048]	Loss: 0.4794
Training Epoch: 35 [38272/50048]	Loss: 0.4984
Training Epoch: 35 [38400/50048]	Loss: 0.6851
Training Epoch: 35 [38528/50048]	Loss: 0.5621
Training Epoch: 35 [38656/50048]	Loss: 0.5751
Training Epoch: 35 [38784/50048]	Loss: 0.5949
Training Epoch: 35 [38912/50048]	Loss: 0.4003
Training Epoch: 35 [39040/50048]	Loss: 0.5263
Training Epoch: 35 [39168/50048]	Loss: 0.5355
Training Epoch: 35 [39296/50048]	Loss: 0.4459
Training Epoch: 35 [39424/50048]	Loss: 0.6023
Training Epoch: 35 [39552/50048]	Loss: 0.5820
Training Epoch: 35 [39680/50048]	Loss: 0.4828
Training Epoch: 35 [39808/50048]	Loss: 0.5208
Training Epoch: 35 [39936/50048]	Loss: 0.5262
Training Epoch: 35 [40064/50048]	Loss: 0.4478
Training Epoch: 35 [40192/50048]	Loss: 0.5560
Training Epoch: 35 [40320/50048]	Loss: 0.6000
Training Epoch: 35 [40448/50048]	Loss: 0.4628
Training Epoch: 35 [40576/50048]	Loss: 0.5054
Training Epoch: 35 [40704/50048]	Loss: 0.5422
Training Epoch: 35 [40832/50048]	Loss: 0.6090
Training Epoch: 35 [40960/50048]	Loss: 0.5184
Training Epoch: 35 [41088/50048]	Loss: 0.4716
Training Epoch: 35 [41216/50048]	Loss: 0.5758
Training Epoch: 35 [41344/50048]	Loss: 0.5820
Training Epoch: 35 [41472/50048]	Loss: 0.6570
Training Epoch: 35 [41600/50048]	Loss: 0.4509
Training Epoch: 35 [41728/50048]	Loss: 0.6158
Training Epoch: 35 [41856/50048]	Loss: 0.4098
Training Epoch: 35 [41984/50048]	Loss: 0.5631
Training Epoch: 35 [42112/50048]	Loss: 0.4574
Training Epoch: 35 [42240/50048]	Loss: 0.6396
Training Epoch: 35 [42368/50048]	Loss: 0.5288
Training Epoch: 35 [42496/50048]	Loss: 0.5862
Training Epoch: 35 [42624/50048]	Loss: 0.5424
Training Epoch: 35 [42752/50048]	Loss: 0.4857
Training Epoch: 35 [42880/50048]	Loss: 0.5821
Training Epoch: 35 [43008/50048]	Loss: 0.5468
Training Epoch: 35 [43136/50048]	Loss: 0.6112
Training Epoch: 35 [43264/50048]	Loss: 0.4618
Training Epoch: 35 [43392/50048]	Loss: 0.5996
Training Epoch: 35 [43520/50048]	Loss: 0.6425
Training Epoch: 35 [43648/50048]	Loss: 0.5303
Training Epoch: 35 [43776/50048]	Loss: 0.5282
Training Epoch: 35 [43904/50048]	Loss: 0.7523
Training Epoch: 35 [44032/50048]	Loss: 0.5004
Training Epoch: 35 [44160/50048]	Loss: 0.6953
Training Epoch: 35 [44288/50048]	Loss: 0.5421
Training Epoch: 35 [44416/50048]	Loss: 0.5570
Training Epoch: 35 [44544/50048]	Loss: 0.4657
Training Epoch: 35 [44672/50048]	Loss: 0.5146
Training Epoch: 35 [44800/50048]	Loss: 0.6178
Training Epoch: 35 [44928/50048]	Loss: 0.5534
Training Epoch: 35 [45056/50048]	Loss: 0.5165
Training Epoch: 35 [45184/50048]	Loss: 0.5028
Training Epoch: 35 [45312/50048]	Loss: 0.5885
Training Epoch: 35 [45440/50048]	Loss: 0.5196
Training Epoch: 35 [45568/50048]	Loss: 0.5872
Training Epoch: 35 [45696/50048]	Loss: 0.6146
2022-12-06 04:31:29,127 [ZeusDataLoader(train)] train epoch 36 done: time=86.50 energy=10509.30
2022-12-06 04:31:29,129 [ZeusDataLoader(eval)] Epoch 36 begin.
Training Epoch: 35 [45824/50048]	Loss: 0.6229
Training Epoch: 35 [45952/50048]	Loss: 0.7358
Training Epoch: 35 [46080/50048]	Loss: 0.5287
Training Epoch: 35 [46208/50048]	Loss: 0.5827
Training Epoch: 35 [46336/50048]	Loss: 0.4990
Training Epoch: 35 [46464/50048]	Loss: 0.5721
Training Epoch: 35 [46592/50048]	Loss: 0.6641
Training Epoch: 35 [46720/50048]	Loss: 0.4553
Training Epoch: 35 [46848/50048]	Loss: 0.6643
Training Epoch: 35 [46976/50048]	Loss: 0.3291
Training Epoch: 35 [47104/50048]	Loss: 0.5963
Training Epoch: 35 [47232/50048]	Loss: 0.3144
Training Epoch: 35 [47360/50048]	Loss: 0.6697
Training Epoch: 35 [47488/50048]	Loss: 0.5255
Training Epoch: 35 [47616/50048]	Loss: 0.7432
Training Epoch: 35 [47744/50048]	Loss: 0.4907
Training Epoch: 35 [47872/50048]	Loss: 0.4817
Training Epoch: 35 [48000/50048]	Loss: 0.5479
Training Epoch: 35 [48128/50048]	Loss: 0.5536
Training Epoch: 35 [48256/50048]	Loss: 0.6105
Training Epoch: 35 [48384/50048]	Loss: 0.6529
Training Epoch: 35 [48512/50048]	Loss: 0.4273
Training Epoch: 35 [48640/50048]	Loss: 0.8713
Training Epoch: 35 [48768/50048]	Loss: 0.5458
Training Epoch: 35 [48896/50048]	Loss: 0.6499
Training Epoch: 35 [49024/50048]	Loss: 0.6158
Training Epoch: 35 [49152/50048]	Loss: 0.4904
Training Epoch: 35 [49280/50048]	Loss: 0.6785
Training Epoch: 35 [49408/50048]	Loss: 0.6143
Training Epoch: 35 [49536/50048]	Loss: 0.6103
Training Epoch: 35 [49664/50048]	Loss: 0.5160
Training Epoch: 35 [49792/50048]	Loss: 0.4538
Training Epoch: 35 [49920/50048]	Loss: 0.5305
Training Epoch: 35 [50048/50048]	Loss: 0.5158
2022-12-06 09:31:32.822 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:31:32,839 [ZeusDataLoader(eval)] eval epoch 36 done: time=3.70 energy=453.54
2022-12-06 04:31:32,839 [ZeusDataLoader(train)] Up to epoch 36: time=3249.83, energy=394386.68, cost=481553.63
2022-12-06 04:31:32,839 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:31:32,839 [ZeusDataLoader(train)] Expected next epoch: time=3339.63, energy=405184.69, cost=494810.01
2022-12-06 04:31:32,840 [ZeusDataLoader(train)] Epoch 37 begin.
Validation Epoch: 35, Average loss: 0.0129, Accuracy: 0.6260
2022-12-06 04:31:33,016 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:31:33,016 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:31:33.018 [ZeusMonitor] Monitor started.
2022-12-06 09:31:33.018 [ZeusMonitor] Running indefinitely. 2022-12-06 09:31:33.018 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:31:33.018 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e37+gpu0.power.log
Training Epoch: 36 [128/50048]	Loss: 0.5935
Training Epoch: 36 [256/50048]	Loss: 0.4689
Training Epoch: 36 [384/50048]	Loss: 0.3415
Training Epoch: 36 [512/50048]	Loss: 0.3431
Training Epoch: 36 [640/50048]	Loss: 0.3666
Training Epoch: 36 [768/50048]	Loss: 0.4413
Training Epoch: 36 [896/50048]	Loss: 0.4207
Training Epoch: 36 [1024/50048]	Loss: 0.2471
Training Epoch: 36 [1152/50048]	Loss: 0.4496
Training Epoch: 36 [1280/50048]	Loss: 0.4460
Training Epoch: 36 [1408/50048]	Loss: 0.4263
Training Epoch: 36 [1536/50048]	Loss: 0.3948
Training Epoch: 36 [1664/50048]	Loss: 0.4806
Training Epoch: 36 [1792/50048]	Loss: 0.3601
Training Epoch: 36 [1920/50048]	Loss: 0.3630
Training Epoch: 36 [2048/50048]	Loss: 0.4484
Training Epoch: 36 [2176/50048]	Loss: 0.4091
Training Epoch: 36 [2304/50048]	Loss: 0.5557
Training Epoch: 36 [2432/50048]	Loss: 0.3016
Training Epoch: 36 [2560/50048]	Loss: 0.3273
Training Epoch: 36 [2688/50048]	Loss: 0.4572
Training Epoch: 36 [2816/50048]	Loss: 0.4461
Training Epoch: 36 [2944/50048]	Loss: 0.4214
Training Epoch: 36 [3072/50048]	Loss: 0.4265
Training Epoch: 36 [3200/50048]	Loss: 0.4395
Training Epoch: 36 [3328/50048]	Loss: 0.4200
Training Epoch: 36 [3456/50048]	Loss: 0.3559
Training Epoch: 36 [3584/50048]	Loss: 0.3430
Training Epoch: 36 [3712/50048]	Loss: 0.5115
Training Epoch: 36 [3840/50048]	Loss: 0.4845
Training Epoch: 36 [3968/50048]	Loss: 0.5116
Training Epoch: 36 [4096/50048]	Loss: 0.5489
Training Epoch: 36 [4224/50048]	Loss: 0.5114
Training Epoch: 36 [4352/50048]	Loss: 0.5079
Training Epoch: 36 [4480/50048]	Loss: 0.4714
Training Epoch: 36 [4608/50048]	Loss: 0.6122
Training Epoch: 36 [4736/50048]	Loss: 0.4228
Training Epoch: 36 [4864/50048]	Loss: 0.3878
Training Epoch: 36 [4992/50048]	Loss: 0.5916
Training Epoch: 36 [5120/50048]	Loss: 0.5675
Training Epoch: 36 [5248/50048]	Loss: 0.5624
Training Epoch: 36 [5376/50048]	Loss: 0.5123
Training Epoch: 36 [5504/50048]	Loss: 0.5231
Training Epoch: 36 [5632/50048]	Loss: 0.5177
Training Epoch: 36 [5760/50048]	Loss: 0.4638
Training Epoch: 36 [5888/50048]	Loss: 0.3444
Training Epoch: 36 [6016/50048]	Loss: 0.5926
Training Epoch: 36 [6144/50048]	Loss: 0.3751
Training Epoch: 36 [6272/50048]	Loss: 0.5682
Training Epoch: 36 [6400/50048]	Loss: 0.4483
Training Epoch: 36 [6528/50048]	Loss: 0.5135
Training Epoch: 36 [6656/50048]	Loss: 0.4616
Training Epoch: 36 [6784/50048]	Loss: 0.3982
Training Epoch: 36 [6912/50048]	Loss: 0.5012
Training Epoch: 36 [7040/50048]	Loss: 0.4205
Training Epoch: 36 [7168/50048]	Loss: 0.4415
Training Epoch: 36 [7296/50048]	Loss: 0.5304
Training Epoch: 36 [7424/50048]	Loss: 0.5008
Training Epoch: 36 [7552/50048]	Loss: 0.3460
Training Epoch: 36 [7680/50048]	Loss: 0.4683
Training Epoch: 36 [7808/50048]	Loss: 0.4977
Training Epoch: 36 [7936/50048]	Loss: 0.4889
Training Epoch: 36 [8064/50048]	Loss: 0.4905
Training Epoch: 36 [8192/50048]	Loss: 0.4422
Training Epoch: 36 [8320/50048]	Loss: 0.4957
Training Epoch: 36 [8448/50048]	Loss: 0.4349
Training Epoch: 36 [8576/50048]	Loss: 0.4609
Training Epoch: 36 [8704/50048]	Loss: 0.6744
Training Epoch: 36 [8832/50048]	Loss: 0.5750
Training Epoch: 36 [8960/50048]	Loss: 0.3899
Training Epoch: 36 [9088/50048]	Loss: 0.5282
Training Epoch: 36 [9216/50048]	Loss: 0.4719
Training Epoch: 36 [9344/50048]	Loss: 0.4234
Training Epoch: 36 [9472/50048]	Loss: 0.5048
Training Epoch: 36 [9600/50048]	Loss: 0.4354
Training Epoch: 36 [9728/50048]	Loss: 0.4462
Training Epoch: 36 [9856/50048]	Loss: 0.6923
Training Epoch: 36 [9984/50048]	Loss: 0.4227
Training Epoch: 36 [10112/50048]	Loss: 0.5489
Training Epoch: 36 [10240/50048]	Loss: 0.4709
Training Epoch: 36 [10368/50048]	Loss: 0.3999
Training Epoch: 36 [10496/50048]	Loss: 0.4474
Training Epoch: 36 [10624/50048]	Loss: 0.4704
Training Epoch: 36 [10752/50048]	Loss: 0.6527
Training Epoch: 36 [10880/50048]	Loss: 0.4732
Training Epoch: 36 [11008/50048]	Loss: 0.5114
Training Epoch: 36 [11136/50048]	Loss: 0.3770
Training Epoch: 36 [11264/50048]	Loss: 0.4146
Training Epoch: 36 [11392/50048]	Loss: 0.5235
Training Epoch: 36 [11520/50048]	Loss: 0.4968
Training Epoch: 36 [11648/50048]	Loss: 0.4564
Training Epoch: 36 [11776/50048]	Loss: 0.4010
Training Epoch: 36 [11904/50048]	Loss: 0.3957
Training Epoch: 36 [12032/50048]	Loss: 0.3680
Training Epoch: 36 [12160/50048]	Loss: 0.5507
Training Epoch: 36 [12288/50048]	Loss: 0.3641
Training Epoch: 36 [12416/50048]	Loss: 0.2858
Training Epoch: 36 [12544/50048]	Loss: 0.4438
Training Epoch: 36 [12672/50048]	Loss: 0.6095
Training Epoch: 36 [12800/50048]	Loss: 0.4634
Training Epoch: 36 [12928/50048]	Loss: 0.4893
Training Epoch: 36 [13056/50048]	Loss: 0.5665
Training Epoch: 36 [13184/50048]	Loss: 0.6534
Training Epoch: 36 [13312/50048]	Loss: 0.5859
Training Epoch: 36 [13440/50048]	Loss: 0.5313
Training Epoch: 36 [13568/50048]	Loss: 0.3839
Training Epoch: 36 [13696/50048]	Loss: 0.4537
Training Epoch: 36 [13824/50048]	Loss: 0.4776
Training Epoch: 36 [13952/50048]	Loss: 0.5449
Training Epoch: 36 [14080/50048]	Loss: 0.4425
Training Epoch: 36 [14208/50048]	Loss: 0.4335
Training Epoch: 36 [14336/50048]	Loss: 0.5212
Training Epoch: 36 [14464/50048]	Loss: 0.4764
Training Epoch: 36 [14592/50048]	Loss: 0.4610
Training Epoch: 36 [14720/50048]	Loss: 0.5411
Training Epoch: 36 [14848/50048]	Loss: 0.4479
Training Epoch: 36 [14976/50048]	Loss: 0.4129
Training Epoch: 36 [15104/50048]	Loss: 0.4685
Training Epoch: 36 [15232/50048]	Loss: 0.4475
Training Epoch: 36 [15360/50048]	Loss: 0.4189
Training Epoch: 36 [15488/50048]	Loss: 0.4516
Training Epoch: 36 [15616/50048]	Loss: 0.4097
Training Epoch: 36 [15744/50048]	Loss: 0.4061
Training Epoch: 36 [15872/50048]	Loss: 0.5082
Training Epoch: 36 [16000/50048]	Loss: 0.3372
Training Epoch: 36 [16128/50048]	Loss: 0.3697
Training Epoch: 36 [16256/50048]	Loss: 0.6048
Training Epoch: 36 [16384/50048]	Loss: 0.6317
Training Epoch: 36 [16512/50048]	Loss: 0.4172
Training Epoch: 36 [16640/50048]	Loss: 0.6120
Training Epoch: 36 [16768/50048]	Loss: 0.4697
Training Epoch: 36 [16896/50048]	Loss: 0.5500
Training Epoch: 36 [17024/50048]	Loss: 0.4612
Training Epoch: 36 [17152/50048]	Loss: 0.4305
Training Epoch: 36 [17280/50048]	Loss: 0.5620
Training Epoch: 36 [17408/50048]	Loss: 0.5157
Training Epoch: 36 [17536/50048]	Loss: 0.4969
Training Epoch: 36 [17664/50048]	Loss: 0.3736
Training Epoch: 36 [17792/50048]	Loss: 0.4557
Training Epoch: 36 [17920/50048]	Loss: 0.3246
Training Epoch: 36 [18048/50048]	Loss: 0.4584
Training Epoch: 36 [18176/50048]	Loss: 0.5717
Training Epoch: 36 [18304/50048]	Loss: 0.5381
Training Epoch: 36 [18432/50048]	Loss: 0.5028
Training Epoch: 36 [18560/50048]	Loss: 0.5276
Training Epoch: 36 [18688/50048]	Loss: 0.4191
Training Epoch: 36 [18816/50048]	Loss: 0.4568
Training Epoch: 36 [18944/50048]	Loss: 0.4349
Training Epoch: 36 [19072/50048]	Loss: 0.6482
Training Epoch: 36 [19200/50048]	Loss: 0.5624
Training Epoch: 36 [19328/50048]	Loss: 0.5191
Training Epoch: 36 [19456/50048]	Loss: 0.4752
Training Epoch: 36 [19584/50048]	Loss: 0.5433
Training Epoch: 36 [19712/50048]	Loss: 0.4990
Training Epoch: 36 [19840/50048]	Loss: 0.5001
Training Epoch: 36 [19968/50048]	Loss: 0.3930
Training Epoch: 36 [20096/50048]	Loss: 0.4061
Training Epoch: 36 [20224/50048]	Loss: 0.4933
Training Epoch: 36 [20352/50048]	Loss: 0.4354
Training Epoch: 36 [20480/50048]	Loss: 0.4933
Training Epoch: 36 [20608/50048]	Loss: 0.4781
Training Epoch: 36 [20736/50048]	Loss: 0.4857
Training Epoch: 36 [20864/50048]	Loss: 0.5383
Training Epoch: 36 [20992/50048]	Loss: 0.5161
Training Epoch: 36 [21120/50048]	Loss: 0.3179
Training Epoch: 36 [21248/50048]	Loss: 0.3604
Training Epoch: 36 [21376/50048]	Loss: 0.4906
Training Epoch: 36 [21504/50048]	Loss: 0.5794
Training Epoch: 36 [21632/50048]	Loss: 0.5694
Training Epoch: 36 [21760/50048]	Loss: 0.6181
Training Epoch: 36 [21888/50048]	Loss: 0.4712
Training Epoch: 36 [22016/50048]	Loss: 0.6744
Training Epoch: 36 [22144/50048]	Loss: 0.5449
Training Epoch: 36 [22272/50048]	Loss: 0.5254
Training Epoch: 36 [22400/50048]	Loss: 0.3991
Training Epoch: 36 [22528/50048]	Loss: 0.5199
Training Epoch: 36 [22656/50048]	Loss: 0.4706
Training Epoch: 36 [22784/50048]	Loss: 0.3885
Training Epoch: 36 [22912/50048]	Loss: 0.4481
Training Epoch: 36 [23040/50048]	Loss: 0.4661
Training Epoch: 36 [23168/50048]	Loss: 0.3454
Training Epoch: 36 [23296/50048]	Loss: 0.3365
Training Epoch: 36 [23424/50048]	Loss: 0.4734
Training Epoch: 36 [23552/50048]	Loss: 0.4523
Training Epoch: 36 [23680/50048]	Loss: 0.4456
Training Epoch: 36 [23808/50048]	Loss: 0.3257
Training Epoch: 36 [23936/50048]	Loss: 0.4554
Training Epoch: 36 [24064/50048]	Loss: 0.6018
Training Epoch: 36 [24192/50048]	Loss: 0.5332
Training Epoch: 36 [24320/50048]	Loss: 0.4044
Training Epoch: 36 [24448/50048]	Loss: 0.4653
Training Epoch: 36 [24576/50048]	Loss: 0.4071
Training Epoch: 36 [24704/50048]	Loss: 0.4894
Training Epoch: 36 [24832/50048]	Loss: 0.6167
Training Epoch: 36 [24960/50048]	Loss: 0.4613
Training Epoch: 36 [25088/50048]	Loss: 0.5678
Training Epoch: 36 [25216/50048]	Loss: 0.4527
Training Epoch: 36 [25344/50048]	Loss: 0.4506
Training Epoch: 36 [25472/50048]	Loss: 0.5651
Training Epoch: 36 [25600/50048]	Loss: 0.5676
Training Epoch: 36 [25728/50048]	Loss: 0.4044
Training Epoch: 36 [25856/50048]	Loss: 0.4180
Training Epoch: 36 [25984/50048]	Loss: 0.5368
Training Epoch: 36 [26112/50048]	Loss: 0.5699
Training Epoch: 36 [26240/50048]	Loss: 0.5638
Training Epoch: 36 [26368/50048]	Loss: 0.5211
Training Epoch: 36 [26496/50048]	Loss: 0.5136
Training Epoch: 36 [26624/50048]	Loss: 0.4229
Training Epoch: 36 [26752/50048]	Loss: 0.5576
Training Epoch: 36 [26880/50048]	Loss: 0.6225
Training Epoch: 36 [27008/50048]	Loss: 0.5914
Training Epoch: 36 [27136/50048]	Loss: 0.5321
Training Epoch: 36 [27264/50048]	Loss: 0.3586
Training Epoch: 36 [27392/50048]	Loss: 0.4851
Training Epoch: 36 [27520/50048]	Loss: 0.4493
Training Epoch: 36 [27648/50048]	Loss: 0.6325
Training Epoch: 36 [27776/50048]	Loss: 0.6106
Training Epoch: 36 [27904/50048]	Loss: 0.5520
Training Epoch: 36 [28032/50048]	Loss: 0.5286
Training Epoch: 36 [28160/50048]	Loss: 0.4067
Training Epoch: 36 [28288/50048]	Loss: 0.5244
Training Epoch: 36 [28416/50048]	Loss: 0.5070
Training Epoch: 36 [28544/50048]	Loss: 0.6103
Training Epoch: 36 [28672/50048]	Loss: 0.5347
Training Epoch: 36 [28800/50048]	Loss: 0.5085
Training Epoch: 36 [28928/50048]	Loss: 0.4179
Training Epoch: 36 [29056/50048]	Loss: 0.4643
Training Epoch: 36 [29184/50048]	Loss: 0.4784
Training Epoch: 36 [29312/50048]	Loss: 0.3745
Training Epoch: 36 [29440/50048]	Loss: 0.5606
Training Epoch: 36 [29568/50048]	Loss: 0.6807
Training Epoch: 36 [29696/50048]	Loss: 0.3417
Training Epoch: 36 [29824/50048]	Loss: 0.5541
Training Epoch: 36 [29952/50048]	Loss: 0.5040
Training Epoch: 36 [30080/50048]	Loss: 0.5314
Training Epoch: 36 [30208/50048]	Loss: 0.5697
Training Epoch: 36 [30336/50048]	Loss: 0.6239
Training Epoch: 36 [30464/50048]	Loss: 0.5671
Training Epoch: 36 [30592/50048]	Loss: 0.6658
Training Epoch: 36 [30720/50048]	Loss: 0.5148
Training Epoch: 36 [30848/50048]	Loss: 0.5542
Training Epoch: 36 [30976/50048]	Loss: 0.3815
Training Epoch: 36 [31104/50048]	Loss: 0.5699
Training Epoch: 36 [31232/50048]	Loss: 0.5040
Training Epoch: 36 [31360/50048]	Loss: 0.4996
Training Epoch: 36 [31488/50048]	Loss: 0.5718
Training Epoch: 36 [31616/50048]	Loss: 0.6107
Training Epoch: 36 [31744/50048]	Loss: 0.6927
Training Epoch: 36 [31872/50048]	Loss: 0.4662
Training Epoch: 36 [32000/50048]	Loss: 0.5916
Training Epoch: 36 [32128/50048]	Loss: 0.4855
Training Epoch: 36 [32256/50048]	Loss: 0.6487
Training Epoch: 36 [32384/50048]	Loss: 0.5256
Training Epoch: 36 [32512/50048]	Loss: 0.4716
Training Epoch: 36 [32640/50048]	Loss: 0.5998
Training Epoch: 36 [32768/50048]	Loss: 0.4482
Training Epoch: 36 [32896/50048]	Loss: 0.5770
Training Epoch: 36 [33024/50048]	Loss: 0.5191
Training Epoch: 36 [33152/50048]	Loss: 0.5278
Training Epoch: 36 [33280/50048]	Loss: 0.7230
Training Epoch: 36 [33408/50048]	Loss: 0.4927
Training Epoch: 36 [33536/50048]	Loss: 0.5159
Training Epoch: 36 [33664/50048]	Loss: 0.3442
Training Epoch: 36 [33792/50048]	Loss: 0.3837
Training Epoch: 36 [33920/50048]	Loss: 0.3902
Training Epoch: 36 [34048/50048]	Loss: 0.3453
Training Epoch: 36 [34176/50048]	Loss: 0.5389
Training Epoch: 36 [34304/50048]	Loss: 0.4604
Training Epoch: 36 [34432/50048]	Loss: 0.5298
Training Epoch: 36 [34560/50048]	Loss: 0.6654
Training Epoch: 36 [34688/50048]	Loss: 0.6176
Training Epoch: 36 [34816/50048]	Loss: 0.3462
Training Epoch: 36 [34944/50048]	Loss: 0.4155
Training Epoch: 36 [35072/50048]	Loss: 0.4868
Training Epoch: 36 [35200/50048]	Loss: 0.5036
Training Epoch: 36 [35328/50048]	Loss: 0.5418
Training Epoch: 36 [35456/50048]	Loss: 0.5547
Training Epoch: 36 [35584/50048]	Loss: 0.5718
Training Epoch: 36 [35712/50048]	Loss: 0.4604
Training Epoch: 36 [35840/50048]	Loss: 0.5801
Training Epoch: 36 [35968/50048]	Loss: 0.4212
Training Epoch: 36 [36096/50048]	Loss: 0.4749
Training Epoch: 36 [36224/50048]	Loss: 0.5687
Training Epoch: 36 [36352/50048]	Loss: 0.5818
Training Epoch: 36 [36480/50048]	Loss: 0.4823
Training Epoch: 36 [36608/50048]	Loss: 0.5153
Training Epoch: 36 [36736/50048]	Loss: 0.3826
Training Epoch: 36 [36864/50048]	Loss: 0.4939
Training Epoch: 36 [36992/50048]	Loss: 0.5206
Training Epoch: 36 [37120/50048]	Loss: 0.4539
Training Epoch: 36 [37248/50048]	Loss: 0.4909
Training Epoch: 36 [37376/50048]	Loss: 0.4145
Training Epoch: 36 [37504/50048]	Loss: 0.4130
Training Epoch: 36 [37632/50048]	Loss: 0.5475
Training Epoch: 36 [37760/50048]	Loss: 0.6324
Training Epoch: 36 [37888/50048]	Loss: 0.4672
Training Epoch: 36 [38016/50048]	Loss: 0.6592
Training Epoch: 36 [38144/50048]	Loss: 0.4560
Training Epoch: 36 [38272/50048]	Loss: 0.4085
Training Epoch: 36 [38400/50048]	Loss: 0.4217
Training Epoch: 36 [38528/50048]	Loss: 0.6695
Training Epoch: 36 [38656/50048]	Loss: 0.3882
Training Epoch: 36 [38784/50048]	Loss: 0.4605
Training Epoch: 36 [38912/50048]	Loss: 0.4275
Training Epoch: 36 [39040/50048]	Loss: 0.6235
Training Epoch: 36 [39168/50048]	Loss: 0.4995
Training Epoch: 36 [39296/50048]	Loss: 0.5754
Training Epoch: 36 [39424/50048]	Loss: 0.4965
Training Epoch: 36 [39552/50048]	Loss: 0.5546
Training Epoch: 36 [39680/50048]	Loss: 0.3507
Training Epoch: 36 [39808/50048]	Loss: 0.4691
Training Epoch: 36 [39936/50048]	Loss: 0.5387
Training Epoch: 36 [40064/50048]	Loss: 0.5770
Training Epoch: 36 [40192/50048]	Loss: 0.4665
Training Epoch: 36 [40320/50048]	Loss: 0.6050
Training Epoch: 36 [40448/50048]	Loss: 0.4626
Training Epoch: 36 [40576/50048]	Loss: 0.5054
Training Epoch: 36 [40704/50048]	Loss: 0.5868
Training Epoch: 36 [40832/50048]	Loss: 0.4703
Training Epoch: 36 [40960/50048]	Loss: 0.4591
Training Epoch: 36 [41088/50048]	Loss: 0.6048
Training Epoch: 36 [41216/50048]	Loss: 0.3252
Training Epoch: 36 [41344/50048]	Loss: 0.4272
Training Epoch: 36 [41472/50048]	Loss: 0.4899
Training Epoch: 36 [41600/50048]	Loss: 0.6183
Training Epoch: 36 [41728/50048]	Loss: 0.6270
Training Epoch: 36 [41856/50048]	Loss: 0.5316
Training Epoch: 36 [41984/50048]	Loss: 0.6886
Training Epoch: 36 [42112/50048]	Loss: 0.3956
Training Epoch: 36 [42240/50048]	Loss: 0.5637
Training Epoch: 36 [42368/50048]	Loss: 0.4845
Training Epoch: 36 [42496/50048]	Loss: 0.4047
Training Epoch: 36 [42624/50048]	Loss: 0.5304
Training Epoch: 36 [42752/50048]	Loss: 0.6180
Training Epoch: 36 [42880/50048]	Loss: 0.6564
Training Epoch: 36 [43008/50048]	Loss: 0.5836
Training Epoch: 36 [43136/50048]	Loss: 0.4542
Training Epoch: 36 [43264/50048]	Loss: 0.4576
Training Epoch: 36 [43392/50048]	Loss: 0.6521
Training Epoch: 36 [43520/50048]	Loss: 0.7372
Training Epoch: 36 [43648/50048]	Loss: 0.4596
Training Epoch: 36 [43776/50048]	Loss: 0.5452
Training Epoch: 36 [43904/50048]	Loss: 0.6557
Training Epoch: 36 [44032/50048]	Loss: 0.7446
Training Epoch: 36 [44160/50048]	Loss: 0.4439
Training Epoch: 36 [44288/50048]	Loss: 0.4728
Training Epoch: 36 [44416/50048]	Loss: 0.5920
Training Epoch: 36 [44544/50048]	Loss: 0.6374
Training Epoch: 36 [44672/50048]	Loss: 0.4988
Training Epoch: 36 [44800/50048]	Loss: 0.6825
Training Epoch: 36 [44928/50048]	Loss: 0.6765
Training Epoch: 36 [45056/50048]	Loss: 0.5149
Training Epoch: 36 [45184/50048]	Loss: 0.4055
Training Epoch: 36 [45312/50048]	Loss: 0.4304
Training Epoch: 36 [45440/50048]	Loss: 0.6570
Training Epoch: 36 [45568/50048]	Loss: 0.5428
Training Epoch: 36 [45696/50048]	Loss: 0.6340
2022-12-06 04:32:59,218 [ZeusDataLoader(train)] train epoch 37 done: time=86.37 energy=10491.27
2022-12-06 04:32:59,220 [ZeusDataLoader(eval)] Epoch 37 begin.
Training Epoch: 36 [45824/50048]	Loss: 0.5818
Training Epoch: 36 [45952/50048]	Loss: 0.4116
Training Epoch: 36 [46080/50048]	Loss: 0.4249
Training Epoch: 36 [46208/50048]	Loss: 0.4944
Training Epoch: 36 [46336/50048]	Loss: 0.6256
Training Epoch: 36 [46464/50048]	Loss: 0.4361
Training Epoch: 36 [46592/50048]	Loss: 0.5892
Training Epoch: 36 [46720/50048]	Loss: 0.6020
Training Epoch: 36 [46848/50048]	Loss: 0.4999
Training Epoch: 36 [46976/50048]	Loss: 0.5146
Training Epoch: 36 [47104/50048]	Loss: 0.4041
Training Epoch: 36 [47232/50048]	Loss: 0.4498
Training Epoch: 36 [47360/50048]	Loss: 0.5944
Training Epoch: 36 [47488/50048]	Loss: 0.5005
Training Epoch: 36 [47616/50048]	Loss: 0.5436
Training Epoch: 36 [47744/50048]	Loss: 0.5056
Training Epoch: 36 [47872/50048]	Loss: 0.4233
Training Epoch: 36 [48000/50048]	Loss: 0.5573
Training Epoch: 36 [48128/50048]	Loss: 0.7525
Training Epoch: 36 [48256/50048]	Loss: 0.3058
Training Epoch: 36 [48384/50048]	Loss: 0.6229
Training Epoch: 36 [48512/50048]	Loss: 0.5424
Training Epoch: 36 [48640/50048]	Loss: 0.6217
Training Epoch: 36 [48768/50048]	Loss: 0.5649
Training Epoch: 36 [48896/50048]	Loss: 0.4787
Training Epoch: 36 [49024/50048]	Loss: 0.5476
Training Epoch: 36 [49152/50048]	Loss: 0.5544
Training Epoch: 36 [49280/50048]	Loss: 0.4949
Training Epoch: 36 [49408/50048]	Loss: 0.6128
Training Epoch: 36 [49536/50048]	Loss: 0.4524
Training Epoch: 36 [49664/50048]	Loss: 0.5144
Training Epoch: 36 [49792/50048]	Loss: 0.4694
Training Epoch: 36 [49920/50048]	Loss: 0.5178
Training Epoch: 36 [50048/50048]	Loss: 0.5747
2022-12-06 09:33:02.913 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:33:02,929 [ZeusDataLoader(eval)] eval epoch 37 done: time=3.70 energy=455.25
2022-12-06 04:33:02,930 [ZeusDataLoader(train)] Up to epoch 37: time=3339.90, energy=405333.19, cost=494907.88
2022-12-06 04:33:02,930 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:33:02,930 [ZeusDataLoader(train)] Expected next epoch: time=3429.70, energy=416131.20, cost=508164.26
2022-12-06 04:33:02,931 [ZeusDataLoader(train)] Epoch 38 begin.
Validation Epoch: 36, Average loss: 0.0130, Accuracy: 0.6304
2022-12-06 04:33:03,113 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:33:03,114 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:33:03.116 [ZeusMonitor] Monitor started.
2022-12-06 09:33:03.116 [ZeusMonitor] Running indefinitely. 2022-12-06 09:33:03.116 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:33:03.116 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e38+gpu0.power.log
Training Epoch: 37 [128/50048]	Loss: 0.5158
Training Epoch: 37 [256/50048]	Loss: 0.3380
Training Epoch: 37 [384/50048]	Loss: 0.3891
Training Epoch: 37 [512/50048]	Loss: 0.4345
Training Epoch: 37 [640/50048]	Loss: 0.3559
Training Epoch: 37 [768/50048]	Loss: 0.4504
Training Epoch: 37 [896/50048]	Loss: 0.5406
Training Epoch: 37 [1024/50048]	Loss: 0.4783
Training Epoch: 37 [1152/50048]	Loss: 0.4111
Training Epoch: 37 [1280/50048]	Loss: 0.4382
Training Epoch: 37 [1408/50048]	Loss: 0.4704
Training Epoch: 37 [1536/50048]	Loss: 0.4841
Training Epoch: 37 [1664/50048]	Loss: 0.4106
Training Epoch: 37 [1792/50048]	Loss: 0.4804
Training Epoch: 37 [1920/50048]	Loss: 0.4041
Training Epoch: 37 [2048/50048]	Loss: 0.3604
Training Epoch: 37 [2176/50048]	Loss: 0.2490
Training Epoch: 37 [2304/50048]	Loss: 0.3693
Training Epoch: 37 [2432/50048]	Loss: 0.3151
Training Epoch: 37 [2560/50048]	Loss: 0.5112
Training Epoch: 37 [2688/50048]	Loss: 0.4378
Training Epoch: 37 [2816/50048]	Loss: 0.3195
Training Epoch: 37 [2944/50048]	Loss: 0.4792
Training Epoch: 37 [3072/50048]	Loss: 0.5207
Training Epoch: 37 [3200/50048]	Loss: 0.3247
Training Epoch: 37 [3328/50048]	Loss: 0.4627
Training Epoch: 37 [3456/50048]	Loss: 0.3421
Training Epoch: 37 [3584/50048]	Loss: 0.4654
Training Epoch: 37 [3712/50048]	Loss: 0.5243
Training Epoch: 37 [3840/50048]	Loss: 0.4020
Training Epoch: 37 [3968/50048]	Loss: 0.4035
Training Epoch: 37 [4096/50048]	Loss: 0.2891
Training Epoch: 37 [4224/50048]	Loss: 0.4518
Training Epoch: 37 [4352/50048]	Loss: 0.3732
Training Epoch: 37 [4480/50048]	Loss: 0.4089
Training Epoch: 37 [4608/50048]	Loss: 0.3115
Training Epoch: 37 [4736/50048]	Loss: 0.4836
Training Epoch: 37 [4864/50048]	Loss: 0.4000
Training Epoch: 37 [4992/50048]	Loss: 0.4436
Training Epoch: 37 [5120/50048]	Loss: 0.3282
Training Epoch: 37 [5248/50048]	Loss: 0.3472
Training Epoch: 37 [5376/50048]	Loss: 0.4366
Training Epoch: 37 [5504/50048]	Loss: 0.4276
Training Epoch: 37 [5632/50048]	Loss: 0.4769
Training Epoch: 37 [5760/50048]	Loss: 0.4526
Training Epoch: 37 [5888/50048]	Loss: 0.3917
Training Epoch: 37 [6016/50048]	Loss: 0.3915
Training Epoch: 37 [6144/50048]	Loss: 0.3776
Training Epoch: 37 [6272/50048]	Loss: 0.4084
Training Epoch: 37 [6400/50048]	Loss: 0.4771
Training Epoch: 37 [6528/50048]	Loss: 0.3560
Training Epoch: 37 [6656/50048]	Loss: 0.3735
Training Epoch: 37 [6784/50048]	Loss: 0.4041
Training Epoch: 37 [6912/50048]	Loss: 0.3425
Training Epoch: 37 [7040/50048]	Loss: 0.4131
Training Epoch: 37 [7168/50048]	Loss: 0.3278
Training Epoch: 37 [7296/50048]	Loss: 0.3899
Training Epoch: 37 [7424/50048]	Loss: 0.4618
Training Epoch: 37 [7552/50048]	Loss: 0.4362
Training Epoch: 37 [7680/50048]	Loss: 0.4810
Training Epoch: 37 [7808/50048]	Loss: 0.4464
Training Epoch: 37 [7936/50048]	Loss: 0.3362
Training Epoch: 37 [8064/50048]	Loss: 0.4097
Training Epoch: 37 [8192/50048]	Loss: 0.5767
Training Epoch: 37 [8320/50048]	Loss: 0.4141
Training Epoch: 37 [8448/50048]	Loss: 0.4564
Training Epoch: 37 [8576/50048]	Loss: 0.4084
Training Epoch: 37 [8704/50048]	Loss: 0.3131
Training Epoch: 37 [8832/50048]	Loss: 0.4242
Training Epoch: 37 [8960/50048]	Loss: 0.4254
Training Epoch: 37 [9088/50048]	Loss: 0.3739
Training Epoch: 37 [9216/50048]	Loss: 0.3363
Training Epoch: 37 [9344/50048]	Loss: 0.4087
Training Epoch: 37 [9472/50048]	Loss: 0.4037
Training Epoch: 37 [9600/50048]	Loss: 0.3381
Training Epoch: 37 [9728/50048]	Loss: 0.4041
Training Epoch: 37 [9856/50048]	Loss: 0.4616
Training Epoch: 37 [9984/50048]	Loss: 0.5993
Training Epoch: 37 [10112/50048]	Loss: 0.6006
Training Epoch: 37 [10240/50048]	Loss: 0.6098
Training Epoch: 37 [10368/50048]	Loss: 0.4913
Training Epoch: 37 [10496/50048]	Loss: 0.3598
Training Epoch: 37 [10624/50048]	Loss: 0.4892
Training Epoch: 37 [10752/50048]	Loss: 0.3655
Training Epoch: 37 [10880/50048]	Loss: 0.3920
Training Epoch: 37 [11008/50048]	Loss: 0.6546
Training Epoch: 37 [11136/50048]	Loss: 0.5287
Training Epoch: 37 [11264/50048]	Loss: 0.4438
Training Epoch: 37 [11392/50048]	Loss: 0.5626
Training Epoch: 37 [11520/50048]	Loss: 0.5487
Training Epoch: 37 [11648/50048]	Loss: 0.5124
Training Epoch: 37 [11776/50048]	Loss: 0.3649
Training Epoch: 37 [11904/50048]	Loss: 0.3345
Training Epoch: 37 [12032/50048]	Loss: 0.4953
Training Epoch: 37 [12160/50048]	Loss: 0.4438
Training Epoch: 37 [12288/50048]	Loss: 0.5265
Training Epoch: 37 [12416/50048]	Loss: 0.4888
Training Epoch: 37 [12544/50048]	Loss: 0.3700
Training Epoch: 37 [12672/50048]	Loss: 0.3994
Training Epoch: 37 [12800/50048]	Loss: 0.3612
Training Epoch: 37 [12928/50048]	Loss: 0.3895
Training Epoch: 37 [13056/50048]	Loss: 0.3732
Training Epoch: 37 [13184/50048]	Loss: 0.4509
Training Epoch: 37 [13312/50048]	Loss: 0.4093
Training Epoch: 37 [13440/50048]	Loss: 0.5481
Training Epoch: 37 [13568/50048]	Loss: 0.4930
Training Epoch: 37 [13696/50048]	Loss: 0.4191
Training Epoch: 37 [13824/50048]	Loss: 0.3626
Training Epoch: 37 [13952/50048]	Loss: 0.5303
Training Epoch: 37 [14080/50048]	Loss: 0.3564
Training Epoch: 37 [14208/50048]	Loss: 0.4142
Training Epoch: 37 [14336/50048]	Loss: 0.4353
Training Epoch: 37 [14464/50048]	Loss: 0.3883
Training Epoch: 37 [14592/50048]	Loss: 0.4760
Training Epoch: 37 [14720/50048]	Loss: 0.4948
Training Epoch: 37 [14848/50048]	Loss: 0.5637
Training Epoch: 37 [14976/50048]	Loss: 0.5664
Training Epoch: 37 [15104/50048]	Loss: 0.3962
Training Epoch: 37 [15232/50048]	Loss: 0.4786
Training Epoch: 37 [15360/50048]	Loss: 0.4831
Training Epoch: 37 [15488/50048]	Loss: 0.5127
Training Epoch: 37 [15616/50048]	Loss: 0.4194
Training Epoch: 37 [15744/50048]	Loss: 0.4140
Training Epoch: 37 [15872/50048]	Loss: 0.4056
Training Epoch: 37 [16000/50048]	Loss: 0.4089
Training Epoch: 37 [16128/50048]	Loss: 0.3995
Training Epoch: 37 [16256/50048]	Loss: 0.4873
Training Epoch: 37 [16384/50048]	Loss: 0.5960
Training Epoch: 37 [16512/50048]	Loss: 0.4880
Training Epoch: 37 [16640/50048]	Loss: 0.4713
Training Epoch: 37 [16768/50048]	Loss: 0.4604
Training Epoch: 37 [16896/50048]	Loss: 0.3582
Training Epoch: 37 [17024/50048]	Loss: 0.5471
Training Epoch: 37 [17152/50048]	Loss: 0.5090
Training Epoch: 37 [17280/50048]	Loss: 0.3778
Training Epoch: 37 [17408/50048]	Loss: 0.4623
Training Epoch: 37 [17536/50048]	Loss: 0.5545
Training Epoch: 37 [17664/50048]	Loss: 0.4406
Training Epoch: 37 [17792/50048]	Loss: 0.4526
Training Epoch: 37 [17920/50048]	Loss: 0.5349
Training Epoch: 37 [18048/50048]	Loss: 0.3947
Training Epoch: 37 [18176/50048]	Loss: 0.4360
Training Epoch: 37 [18304/50048]	Loss: 0.3786
Training Epoch: 37 [18432/50048]	Loss: 0.4479
Training Epoch: 37 [18560/50048]	Loss: 0.5220
Training Epoch: 37 [18688/50048]	Loss: 0.5139
Training Epoch: 37 [18816/50048]	Loss: 0.4886
Training Epoch: 37 [18944/50048]	Loss: 0.3129
Training Epoch: 37 [19072/50048]	Loss: 0.3948
Training Epoch: 37 [19200/50048]	Loss: 0.5785
Training Epoch: 37 [19328/50048]	Loss: 0.5118
Training Epoch: 37 [19456/50048]	Loss: 0.5797
Training Epoch: 37 [19584/50048]	Loss: 0.4603
Training Epoch: 37 [19712/50048]	Loss: 0.6121
Training Epoch: 37 [19840/50048]	Loss: 0.5715
Training Epoch: 37 [19968/50048]	Loss: 0.3031
Training Epoch: 37 [20096/50048]	Loss: 0.3863
Training Epoch: 37 [20224/50048]	Loss: 0.4424
Training Epoch: 37 [20352/50048]	Loss: 0.4334
Training Epoch: 37 [20480/50048]	Loss: 0.5106
Training Epoch: 37 [20608/50048]	Loss: 0.5017
Training Epoch: 37 [20736/50048]	Loss: 0.6004
Training Epoch: 37 [20864/50048]	Loss: 0.5174
Training Epoch: 37 [20992/50048]	Loss: 0.6147
Training Epoch: 37 [21120/50048]	Loss: 0.3967
Training Epoch: 37 [21248/50048]	Loss: 0.5870
Training Epoch: 37 [21376/50048]	Loss: 0.4784
Training Epoch: 37 [21504/50048]	Loss: 0.4382
Training Epoch: 37 [21632/50048]	Loss: 0.5770
Training Epoch: 37 [21760/50048]	Loss: 0.4854
Training Epoch: 37 [21888/50048]	Loss: 0.5189
Training Epoch: 37 [22016/50048]	Loss: 0.4655
Training Epoch: 37 [22144/50048]	Loss: 0.4000
Training Epoch: 37 [22272/50048]	Loss: 0.5184
Training Epoch: 37 [22400/50048]	Loss: 0.4111
Training Epoch: 37 [22528/50048]	Loss: 0.5084
Training Epoch: 37 [22656/50048]	Loss: 0.4359
Training Epoch: 37 [22784/50048]	Loss: 0.4816
Training Epoch: 37 [22912/50048]	Loss: 0.5530
Training Epoch: 37 [23040/50048]	Loss: 0.4356
Training Epoch: 37 [23168/50048]	Loss: 0.4385
Training Epoch: 37 [23296/50048]	Loss: 0.5754
Training Epoch: 37 [23424/50048]	Loss: 0.4425
Training Epoch: 37 [23552/50048]	Loss: 0.5712
Training Epoch: 37 [23680/50048]	Loss: 0.5011
Training Epoch: 37 [23808/50048]	Loss: 0.4028
Training Epoch: 37 [23936/50048]	Loss: 0.3792
Training Epoch: 37 [24064/50048]	Loss: 0.3754
Training Epoch: 37 [24192/50048]	Loss: 0.5137
Training Epoch: 37 [24320/50048]	Loss: 0.4074
Training Epoch: 37 [24448/50048]	Loss: 0.5111
Training Epoch: 37 [24576/50048]	Loss: 0.4269
Training Epoch: 37 [24704/50048]	Loss: 0.5083
Training Epoch: 37 [24832/50048]	Loss: 0.5481
Training Epoch: 37 [24960/50048]	Loss: 0.4131
Training Epoch: 37 [25088/50048]	Loss: 0.5871
Training Epoch: 37 [25216/50048]	Loss: 0.4737
Training Epoch: 37 [25344/50048]	Loss: 0.6289
Training Epoch: 37 [25472/50048]	Loss: 0.4556
Training Epoch: 37 [25600/50048]	Loss: 0.5311
Training Epoch: 37 [25728/50048]	Loss: 0.5743
Training Epoch: 37 [25856/50048]	Loss: 0.5097
Training Epoch: 37 [25984/50048]	Loss: 0.6055
Training Epoch: 37 [26112/50048]	Loss: 0.3805
Training Epoch: 37 [26240/50048]	Loss: 0.5055
Training Epoch: 37 [26368/50048]	Loss: 0.4812
Training Epoch: 37 [26496/50048]	Loss: 0.3393
Training Epoch: 37 [26624/50048]	Loss: 0.4827
Training Epoch: 37 [26752/50048]	Loss: 0.4793
Training Epoch: 37 [26880/50048]	Loss: 0.4854
Training Epoch: 37 [27008/50048]	Loss: 0.5422
Training Epoch: 37 [27136/50048]	Loss: 0.7064
Training Epoch: 37 [27264/50048]	Loss: 0.5272
Training Epoch: 37 [27392/50048]	Loss: 0.4195
Training Epoch: 37 [27520/50048]	Loss: 0.4453
Training Epoch: 37 [27648/50048]	Loss: 0.4381
Training Epoch: 37 [27776/50048]	Loss: 0.5617
Training Epoch: 37 [27904/50048]	Loss: 0.5170
Training Epoch: 37 [28032/50048]	Loss: 0.5014
Training Epoch: 37 [28160/50048]	Loss: 0.4035
Training Epoch: 37 [28288/50048]	Loss: 0.5315
Training Epoch: 37 [28416/50048]	Loss: 0.4845
Training Epoch: 37 [28544/50048]	Loss: 0.5141
Training Epoch: 37 [28672/50048]	Loss: 0.4641
Training Epoch: 37 [28800/50048]	Loss: 0.5339
Training Epoch: 37 [28928/50048]	Loss: 0.4602
Training Epoch: 37 [29056/50048]	Loss: 0.5587
Training Epoch: 37 [29184/50048]	Loss: 0.5706
Training Epoch: 37 [29312/50048]	Loss: 0.3740
Training Epoch: 37 [29440/50048]	Loss: 0.5201
Training Epoch: 37 [29568/50048]	Loss: 0.4788
Training Epoch: 37 [29696/50048]	Loss: 0.3988
Training Epoch: 37 [29824/50048]	Loss: 0.5072
Training Epoch: 37 [29952/50048]	Loss: 0.4504
Training Epoch: 37 [30080/50048]	Loss: 0.3447
Training Epoch: 37 [30208/50048]	Loss: 0.3880
Training Epoch: 37 [30336/50048]	Loss: 0.4503
Training Epoch: 37 [30464/50048]	Loss: 0.5768
Training Epoch: 37 [30592/50048]	Loss: 0.5063
Training Epoch: 37 [30720/50048]	Loss: 0.5123
Training Epoch: 37 [30848/50048]	Loss: 0.3901
Training Epoch: 37 [30976/50048]	Loss: 0.4344
Training Epoch: 37 [31104/50048]	Loss: 0.4778
Training Epoch: 37 [31232/50048]	Loss: 0.4779
Training Epoch: 37 [31360/50048]	Loss: 0.5262
Training Epoch: 37 [31488/50048]	Loss: 0.3479
Training Epoch: 37 [31616/50048]	Loss: 0.3986
Training Epoch: 37 [31744/50048]	Loss: 0.5125
Training Epoch: 37 [31872/50048]	Loss: 0.4952
Training Epoch: 37 [32000/50048]	Loss: 0.3344
Training Epoch: 37 [32128/50048]	Loss: 0.7168
Training Epoch: 37 [32256/50048]	Loss: 0.5005
Training Epoch: 37 [32384/50048]	Loss: 0.4620
Training Epoch: 37 [32512/50048]	Loss: 0.6246
Training Epoch: 37 [32640/50048]	Loss: 0.4347
Training Epoch: 37 [32768/50048]	Loss: 0.6170
Training Epoch: 37 [32896/50048]	Loss: 0.5213
Training Epoch: 37 [33024/50048]	Loss: 0.4555
Training Epoch: 37 [33152/50048]	Loss: 0.4871
Training Epoch: 37 [33280/50048]	Loss: 0.5087
Training Epoch: 37 [33408/50048]	Loss: 0.5451
Training Epoch: 37 [33536/50048]	Loss: 0.4100
Training Epoch: 37 [33664/50048]	Loss: 0.6923
Training Epoch: 37 [33792/50048]	Loss: 0.4694
Training Epoch: 37 [33920/50048]	Loss: 0.5890
Training Epoch: 37 [34048/50048]	Loss: 0.4798
Training Epoch: 37 [34176/50048]	Loss: 0.4635
Training Epoch: 37 [34304/50048]	Loss: 0.4429
Training Epoch: 37 [34432/50048]	Loss: 0.3817
Training Epoch: 37 [34560/50048]	Loss: 0.6705
Training Epoch: 37 [34688/50048]	Loss: 0.4992
Training Epoch: 37 [34816/50048]	Loss: 0.5512
Training Epoch: 37 [34944/50048]	Loss: 0.4293
Training Epoch: 37 [35072/50048]	Loss: 0.4732
Training Epoch: 37 [35200/50048]	Loss: 0.4978
Training Epoch: 37 [35328/50048]	Loss: 0.6304
Training Epoch: 37 [35456/50048]	Loss: 0.5797
Training Epoch: 37 [35584/50048]	Loss: 0.5503
Training Epoch: 37 [35712/50048]	Loss: 0.4222
Training Epoch: 37 [35840/50048]	Loss: 0.5477
Training Epoch: 37 [35968/50048]	Loss: 0.5641
Training Epoch: 37 [36096/50048]	Loss: 0.4622
Training Epoch: 37 [36224/50048]	Loss: 0.5433
Training Epoch: 37 [36352/50048]	Loss: 0.5150
Training Epoch: 37 [36480/50048]	Loss: 0.5537
Training Epoch: 37 [36608/50048]	Loss: 0.4546
Training Epoch: 37 [36736/50048]	Loss: 0.3642
Training Epoch: 37 [36864/50048]	Loss: 0.5623
Training Epoch: 37 [36992/50048]	Loss: 0.4921
Training Epoch: 37 [37120/50048]	Loss: 0.5727
Training Epoch: 37 [37248/50048]	Loss: 0.4051
Training Epoch: 37 [37376/50048]	Loss: 0.3973
Training Epoch: 37 [37504/50048]	Loss: 0.6475
Training Epoch: 37 [37632/50048]	Loss: 0.4939
Training Epoch: 37 [37760/50048]	Loss: 0.4984
Training Epoch: 37 [37888/50048]	Loss: 0.5745
Training Epoch: 37 [38016/50048]	Loss: 0.4552
Training Epoch: 37 [38144/50048]	Loss: 0.5214
Training Epoch: 37 [38272/50048]	Loss: 0.4620
Training Epoch: 37 [38400/50048]	Loss: 0.5145
Training Epoch: 37 [38528/50048]	Loss: 0.4715
Training Epoch: 37 [38656/50048]	Loss: 0.4597
Training Epoch: 37 [38784/50048]	Loss: 0.4428
Training Epoch: 37 [38912/50048]	Loss: 0.5208
Training Epoch: 37 [39040/50048]	Loss: 0.3361
Training Epoch: 37 [39168/50048]	Loss: 0.3768
Training Epoch: 37 [39296/50048]	Loss: 0.5343
Training Epoch: 37 [39424/50048]	Loss: 0.4497
Training Epoch: 37 [39552/50048]	Loss: 0.4599
Training Epoch: 37 [39680/50048]	Loss: 0.3829
Training Epoch: 37 [39808/50048]	Loss: 0.5148
Training Epoch: 37 [39936/50048]	Loss: 0.5830
Training Epoch: 37 [40064/50048]	Loss: 0.5234
Training Epoch: 37 [40192/50048]	Loss: 0.5118
Training Epoch: 37 [40320/50048]	Loss: 0.5549
Training Epoch: 37 [40448/50048]	Loss: 0.3328
Training Epoch: 37 [40576/50048]	Loss: 0.5261
Training Epoch: 37 [40704/50048]	Loss: 0.6052
Training Epoch: 37 [40832/50048]	Loss: 0.4490
Training Epoch: 37 [40960/50048]	Loss: 0.6063
Training Epoch: 37 [41088/50048]	Loss: 0.4375
Training Epoch: 37 [41216/50048]	Loss: 0.3318
Training Epoch: 37 [41344/50048]	Loss: 0.6380
Training Epoch: 37 [41472/50048]	Loss: 0.5983
Training Epoch: 37 [41600/50048]	Loss: 0.4459
Training Epoch: 37 [41728/50048]	Loss: 0.5325
Training Epoch: 37 [41856/50048]	Loss: 0.3830
Training Epoch: 37 [41984/50048]	Loss: 0.4775
Training Epoch: 37 [42112/50048]	Loss: 0.5074
Training Epoch: 37 [42240/50048]	Loss: 0.6461
Training Epoch: 37 [42368/50048]	Loss: 0.5318
Training Epoch: 37 [42496/50048]	Loss: 0.6391
Training Epoch: 37 [42624/50048]	Loss: 0.4366
Training Epoch: 37 [42752/50048]	Loss: 0.4321
Training Epoch: 37 [42880/50048]	Loss: 0.3636
Training Epoch: 37 [43008/50048]	Loss: 0.4088
Training Epoch: 37 [43136/50048]	Loss: 0.6119
Training Epoch: 37 [43264/50048]	Loss: 0.5039
Training Epoch: 37 [43392/50048]	Loss: 0.3553
Training Epoch: 37 [43520/50048]	Loss: 0.5326
Training Epoch: 37 [43648/50048]	Loss: 0.6238
Training Epoch: 37 [43776/50048]	Loss: 0.4346
Training Epoch: 37 [43904/50048]	Loss: 0.3976
Training Epoch: 37 [44032/50048]	Loss: 0.6689
Training Epoch: 37 [44160/50048]	Loss: 0.4982
Training Epoch: 37 [44288/50048]	Loss: 0.5151
Training Epoch: 37 [44416/50048]	Loss: 0.6566
Training Epoch: 37 [44544/50048]	Loss: 0.4806
Training Epoch: 37 [44672/50048]	Loss: 0.3382
Training Epoch: 37 [44800/50048]	Loss: 0.6415
Training Epoch: 37 [44928/50048]	Loss: 0.6429
Training Epoch: 37 [45056/50048]	Loss: 0.4691
Training Epoch: 37 [45184/50048]	Loss: 0.5986
Training Epoch: 37 [45312/50048]	Loss: 0.4728
Training Epoch: 37 [45440/50048]	Loss: 0.4952
Training Epoch: 37 [45568/50048]	Loss: 0.4980
Training Epoch: 37 [45696/50048]	Loss: 0.4733
2022-12-06 04:34:29,470 [ZeusDataLoader(train)] train epoch 38 done: time=86.53 energy=10518.23
2022-12-06 04:34:29,472 [ZeusDataLoader(eval)] Epoch 38 begin.
Training Epoch: 37 [45824/50048]	Loss: 0.6444
Training Epoch: 37 [45952/50048]	Loss: 0.4297
Training Epoch: 37 [46080/50048]	Loss: 0.6380
Training Epoch: 37 [46208/50048]	Loss: 0.4723
Training Epoch: 37 [46336/50048]	Loss: 0.5521
Training Epoch: 37 [46464/50048]	Loss: 0.6080
Training Epoch: 37 [46592/50048]	Loss: 0.4755
Training Epoch: 37 [46720/50048]	Loss: 0.4582
Training Epoch: 37 [46848/50048]	Loss: 0.4745
Training Epoch: 37 [46976/50048]	Loss: 0.6269
Training Epoch: 37 [47104/50048]	Loss: 0.4284
Training Epoch: 37 [47232/50048]	Loss: 0.6059
Training Epoch: 37 [47360/50048]	Loss: 0.5183
Training Epoch: 37 [47488/50048]	Loss: 0.4563
Training Epoch: 37 [47616/50048]	Loss: 0.6435
Training Epoch: 37 [47744/50048]	Loss: 0.4289
Training Epoch: 37 [47872/50048]	Loss: 0.5338
Training Epoch: 37 [48000/50048]	Loss: 0.5728
Training Epoch: 37 [48128/50048]	Loss: 0.4569
Training Epoch: 37 [48256/50048]	Loss: 0.4598
Training Epoch: 37 [48384/50048]	Loss: 0.6014
Training Epoch: 37 [48512/50048]	Loss: 0.5419
Training Epoch: 37 [48640/50048]	Loss: 0.4420
Training Epoch: 37 [48768/50048]	Loss: 0.5633
Training Epoch: 37 [48896/50048]	Loss: 0.2854
Training Epoch: 37 [49024/50048]	Loss: 0.4559
Training Epoch: 37 [49152/50048]	Loss: 0.5405
Training Epoch: 37 [49280/50048]	Loss: 0.4629
Training Epoch: 37 [49408/50048]	Loss: 0.4744
Training Epoch: 37 [49536/50048]	Loss: 0.5494
Training Epoch: 37 [49664/50048]	Loss: 0.5811
Training Epoch: 37 [49792/50048]	Loss: 0.5310
Training Epoch: 37 [49920/50048]	Loss: 0.4884
Training Epoch: 37 [50048/50048]	Loss: 0.6157
2022-12-06 09:34:33.146 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:34:33,186 [ZeusDataLoader(eval)] eval epoch 38 done: time=3.71 energy=455.72
2022-12-06 04:34:33,186 [ZeusDataLoader(train)] Up to epoch 38: time=3430.13, energy=416307.14, cost=508290.36
2022-12-06 04:34:33,186 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:34:33,187 [ZeusDataLoader(train)] Expected next epoch: time=3519.93, energy=427105.15, cost=521546.74
2022-12-06 04:34:33,188 [ZeusDataLoader(train)] Epoch 39 begin.
Validation Epoch: 37, Average loss: 0.0135, Accuracy: 0.6224
2022-12-06 04:34:33,377 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:34:33,377 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:34:33.379 [ZeusMonitor] Monitor started.
2022-12-06 09:34:33.379 [ZeusMonitor] Running indefinitely. 2022-12-06 09:34:33.379 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:34:33.379 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e39+gpu0.power.log
Training Epoch: 38 [128/50048]	Loss: 0.5447
Training Epoch: 38 [256/50048]	Loss: 0.5543
Training Epoch: 38 [384/50048]	Loss: 0.4301
Training Epoch: 38 [512/50048]	Loss: 0.5601
Training Epoch: 38 [640/50048]	Loss: 0.3810
Training Epoch: 38 [768/50048]	Loss: 0.4274
Training Epoch: 38 [896/50048]	Loss: 0.3817
Training Epoch: 38 [1024/50048]	Loss: 0.4428
Training Epoch: 38 [1152/50048]	Loss: 0.3326
Training Epoch: 38 [1280/50048]	Loss: 0.3890
Training Epoch: 38 [1408/50048]	Loss: 0.5168
Training Epoch: 38 [1536/50048]	Loss: 0.3193
Training Epoch: 38 [1664/50048]	Loss: 0.4104
Training Epoch: 38 [1792/50048]	Loss: 0.2863
Training Epoch: 38 [1920/50048]	Loss: 0.3460
Training Epoch: 38 [2048/50048]	Loss: 0.3494
Training Epoch: 38 [2176/50048]	Loss: 0.3690
Training Epoch: 38 [2304/50048]	Loss: 0.3543
Training Epoch: 38 [2432/50048]	Loss: 0.5687
Training Epoch: 38 [2560/50048]	Loss: 0.4579
Training Epoch: 38 [2688/50048]	Loss: 0.4567
Training Epoch: 38 [2816/50048]	Loss: 0.3145
Training Epoch: 38 [2944/50048]	Loss: 0.3711
Training Epoch: 38 [3072/50048]	Loss: 0.3126
Training Epoch: 38 [3200/50048]	Loss: 0.3677
Training Epoch: 38 [3328/50048]	Loss: 0.3496
Training Epoch: 38 [3456/50048]	Loss: 0.4303
Training Epoch: 38 [3584/50048]	Loss: 0.4304
Training Epoch: 38 [3712/50048]	Loss: 0.3914
Training Epoch: 38 [3840/50048]	Loss: 0.4343
Training Epoch: 38 [3968/50048]	Loss: 0.3642
Training Epoch: 38 [4096/50048]	Loss: 0.4200
Training Epoch: 38 [4224/50048]	Loss: 0.4036
Training Epoch: 38 [4352/50048]	Loss: 0.5196
Training Epoch: 38 [4480/50048]	Loss: 0.4612
Training Epoch: 38 [4608/50048]	Loss: 0.3801
Training Epoch: 38 [4736/50048]	Loss: 0.4036
Training Epoch: 38 [4864/50048]	Loss: 0.3831
Training Epoch: 38 [4992/50048]	Loss: 0.3571
Training Epoch: 38 [5120/50048]	Loss: 0.3763
Training Epoch: 38 [5248/50048]	Loss: 0.3561
Training Epoch: 38 [5376/50048]	Loss: 0.2759
Training Epoch: 38 [5504/50048]	Loss: 0.4855
Training Epoch: 38 [5632/50048]	Loss: 0.3868
Training Epoch: 38 [5760/50048]	Loss: 0.4782
Training Epoch: 38 [5888/50048]	Loss: 0.4414
Training Epoch: 38 [6016/50048]	Loss: 0.5310
Training Epoch: 38 [6144/50048]	Loss: 0.4058
Training Epoch: 38 [6272/50048]	Loss: 0.4664
Training Epoch: 38 [6400/50048]	Loss: 0.4424
Training Epoch: 38 [6528/50048]	Loss: 0.4104
Training Epoch: 38 [6656/50048]	Loss: 0.3657
Training Epoch: 38 [6784/50048]	Loss: 0.4334
Training Epoch: 38 [6912/50048]	Loss: 0.4423
Training Epoch: 38 [7040/50048]	Loss: 0.5034
Training Epoch: 38 [7168/50048]	Loss: 0.3722
Training Epoch: 38 [7296/50048]	Loss: 0.3196
Training Epoch: 38 [7424/50048]	Loss: 0.4138
Training Epoch: 38 [7552/50048]	Loss: 0.3813
Training Epoch: 38 [7680/50048]	Loss: 0.3769
Training Epoch: 38 [7808/50048]	Loss: 0.3904
Training Epoch: 38 [7936/50048]	Loss: 0.3938
Training Epoch: 38 [8064/50048]	Loss: 0.3056
Training Epoch: 38 [8192/50048]	Loss: 0.4214
Training Epoch: 38 [8320/50048]	Loss: 0.5149
Training Epoch: 38 [8448/50048]	Loss: 0.3915
Training Epoch: 38 [8576/50048]	Loss: 0.2868
Training Epoch: 38 [8704/50048]	Loss: 0.3139
Training Epoch: 38 [8832/50048]	Loss: 0.3756
Training Epoch: 38 [8960/50048]	Loss: 0.3306
Training Epoch: 38 [9088/50048]	Loss: 0.3917
Training Epoch: 38 [9216/50048]	Loss: 0.4424
Training Epoch: 38 [9344/50048]	Loss: 0.3785
Training Epoch: 38 [9472/50048]	Loss: 0.5630
Training Epoch: 38 [9600/50048]	Loss: 0.3939
Training Epoch: 38 [9728/50048]	Loss: 0.3482
Training Epoch: 38 [9856/50048]	Loss: 0.4292
Training Epoch: 38 [9984/50048]	Loss: 0.3943
Training Epoch: 38 [10112/50048]	Loss: 0.4518
Training Epoch: 38 [10240/50048]	Loss: 0.2978
Training Epoch: 38 [10368/50048]	Loss: 0.4899
Training Epoch: 38 [10496/50048]	Loss: 0.4104
Training Epoch: 38 [10624/50048]	Loss: 0.3702
Training Epoch: 38 [10752/50048]	Loss: 0.4703
Training Epoch: 38 [10880/50048]	Loss: 0.4206
Training Epoch: 38 [11008/50048]	Loss: 0.3756
Training Epoch: 38 [11136/50048]	Loss: 0.4462
Training Epoch: 38 [11264/50048]	Loss: 0.5427
Training Epoch: 38 [11392/50048]	Loss: 0.4272
Training Epoch: 38 [11520/50048]	Loss: 0.3819
Training Epoch: 38 [11648/50048]	Loss: 0.2934
Training Epoch: 38 [11776/50048]	Loss: 0.4808
Training Epoch: 38 [11904/50048]	Loss: 0.4507
Training Epoch: 38 [12032/50048]	Loss: 0.5392
Training Epoch: 38 [12160/50048]	Loss: 0.4254
Training Epoch: 38 [12288/50048]	Loss: 0.4412
Training Epoch: 38 [12416/50048]	Loss: 0.4553
Training Epoch: 38 [12544/50048]	Loss: 0.3570
Training Epoch: 38 [12672/50048]	Loss: 0.4439
Training Epoch: 38 [12800/50048]	Loss: 0.4660
Training Epoch: 38 [12928/50048]	Loss: 0.4417
Training Epoch: 38 [13056/50048]	Loss: 0.5203
Training Epoch: 38 [13184/50048]	Loss: 0.2754
Training Epoch: 38 [13312/50048]	Loss: 0.6132
Training Epoch: 38 [13440/50048]	Loss: 0.3565
Training Epoch: 38 [13568/50048]	Loss: 0.4392
Training Epoch: 38 [13696/50048]	Loss: 0.5045
Training Epoch: 38 [13824/50048]	Loss: 0.4425
Training Epoch: 38 [13952/50048]	Loss: 0.5402
Training Epoch: 38 [14080/50048]	Loss: 0.5776
Training Epoch: 38 [14208/50048]	Loss: 0.4238
Training Epoch: 38 [14336/50048]	Loss: 0.4101
Training Epoch: 38 [14464/50048]	Loss: 0.4346
Training Epoch: 38 [14592/50048]	Loss: 0.3301
Training Epoch: 38 [14720/50048]	Loss: 0.4226
Training Epoch: 38 [14848/50048]	Loss: 0.3566
Training Epoch: 38 [14976/50048]	Loss: 0.4846
Training Epoch: 38 [15104/50048]	Loss: 0.4457
Training Epoch: 38 [15232/50048]	Loss: 0.5078
Training Epoch: 38 [15360/50048]	Loss: 0.4277
Training Epoch: 38 [15488/50048]	Loss: 0.4314
Training Epoch: 38 [15616/50048]	Loss: 0.4296
Training Epoch: 38 [15744/50048]	Loss: 0.4258
Training Epoch: 38 [15872/50048]	Loss: 0.4262
Training Epoch: 38 [16000/50048]	Loss: 0.4058
Training Epoch: 38 [16128/50048]	Loss: 0.2756
Training Epoch: 38 [16256/50048]	Loss: 0.4526
Training Epoch: 38 [16384/50048]	Loss: 0.4018
Training Epoch: 38 [16512/50048]	Loss: 0.4733
Training Epoch: 38 [16640/50048]	Loss: 0.4479
Training Epoch: 38 [16768/50048]	Loss: 0.4795
Training Epoch: 38 [16896/50048]	Loss: 0.4789
Training Epoch: 38 [17024/50048]	Loss: 0.4066
Training Epoch: 38 [17152/50048]	Loss: 0.3996
Training Epoch: 38 [17280/50048]	Loss: 0.4892
Training Epoch: 38 [17408/50048]	Loss: 0.4588
Training Epoch: 38 [17536/50048]	Loss: 0.4853
Training Epoch: 38 [17664/50048]	Loss: 0.4871
Training Epoch: 38 [17792/50048]	Loss: 0.6140
Training Epoch: 38 [17920/50048]	Loss: 0.4022
Training Epoch: 38 [18048/50048]	Loss: 0.4203
Training Epoch: 38 [18176/50048]	Loss: 0.3638
Training Epoch: 38 [18304/50048]	Loss: 0.3933
Training Epoch: 38 [18432/50048]	Loss: 0.4329
Training Epoch: 38 [18560/50048]	Loss: 0.3740
Training Epoch: 38 [18688/50048]	Loss: 0.5785
Training Epoch: 38 [18816/50048]	Loss: 0.4946
Training Epoch: 38 [18944/50048]	Loss: 0.4220
Training Epoch: 38 [19072/50048]	Loss: 0.4903
Training Epoch: 38 [19200/50048]	Loss: 0.4865
Training Epoch: 38 [19328/50048]	Loss: 0.4811
Training Epoch: 38 [19456/50048]	Loss: 0.3704
Training Epoch: 38 [19584/50048]	Loss: 0.4301
Training Epoch: 38 [19712/50048]	Loss: 0.5333
Training Epoch: 38 [19840/50048]	Loss: 0.4048
Training Epoch: 38 [19968/50048]	Loss: 0.3821
Training Epoch: 38 [20096/50048]	Loss: 0.5546
Training Epoch: 38 [20224/50048]	Loss: 0.7500
Training Epoch: 38 [20352/50048]	Loss: 0.6117
Training Epoch: 38 [20480/50048]	Loss: 0.4276
Training Epoch: 38 [20608/50048]	Loss: 0.4562
Training Epoch: 38 [20736/50048]	Loss: 0.3534
Training Epoch: 38 [20864/50048]	Loss: 0.4997
Training Epoch: 38 [20992/50048]	Loss: 0.4838
Training Epoch: 38 [21120/50048]	Loss: 0.3307
Training Epoch: 38 [21248/50048]	Loss: 0.3724
Training Epoch: 38 [21376/50048]	Loss: 0.3776
Training Epoch: 38 [21504/50048]	Loss: 0.3987
Training Epoch: 38 [21632/50048]	Loss: 0.4551
Training Epoch: 38 [21760/50048]	Loss: 0.5393
Training Epoch: 38 [21888/50048]	Loss: 0.5670
Training Epoch: 38 [22016/50048]	Loss: 0.2954
Training Epoch: 38 [22144/50048]	Loss: 0.5452
Training Epoch: 38 [22272/50048]	Loss: 0.3603
Training Epoch: 38 [22400/50048]	Loss: 0.4141
Training Epoch: 38 [22528/50048]	Loss: 0.5145
Training Epoch: 38 [22656/50048]	Loss: 0.4216
Training Epoch: 38 [22784/50048]	Loss: 0.3806
Training Epoch: 38 [22912/50048]	Loss: 0.4297
Training Epoch: 38 [23040/50048]	Loss: 0.4888
Training Epoch: 38 [23168/50048]	Loss: 0.6062
Training Epoch: 38 [23296/50048]	Loss: 0.6267
Training Epoch: 38 [23424/50048]	Loss: 0.5865
Training Epoch: 38 [23552/50048]	Loss: 0.6907
Training Epoch: 38 [23680/50048]	Loss: 0.4163
Training Epoch: 38 [23808/50048]	Loss: 0.4398
Training Epoch: 38 [23936/50048]	Loss: 0.6390
Training Epoch: 38 [24064/50048]	Loss: 0.5198
Training Epoch: 38 [24192/50048]	Loss: 0.5915
Training Epoch: 38 [24320/50048]	Loss: 0.4335
Training Epoch: 38 [24448/50048]	Loss: 0.5416
Training Epoch: 38 [24576/50048]	Loss: 0.3927
Training Epoch: 38 [24704/50048]	Loss: 0.5294
Training Epoch: 38 [24832/50048]	Loss: 0.5388
Training Epoch: 38 [24960/50048]	Loss: 0.4894
Training Epoch: 38 [25088/50048]	Loss: 0.3286
Training Epoch: 38 [25216/50048]	Loss: 0.4235
Training Epoch: 38 [25344/50048]	Loss: 0.2444
Training Epoch: 38 [25472/50048]	Loss: 0.4628
Training Epoch: 38 [25600/50048]	Loss: 0.4789
Training Epoch: 38 [25728/50048]	Loss: 0.4511
Training Epoch: 38 [25856/50048]	Loss: 0.4051
Training Epoch: 38 [25984/50048]	Loss: 0.4856
Training Epoch: 38 [26112/50048]	Loss: 0.3669
Training Epoch: 38 [26240/50048]	Loss: 0.4408
Training Epoch: 38 [26368/50048]	Loss: 0.4773
Training Epoch: 38 [26496/50048]	Loss: 0.4563
Training Epoch: 38 [26624/50048]	Loss: 0.4044
Training Epoch: 38 [26752/50048]	Loss: 0.4133
Training Epoch: 38 [26880/50048]	Loss: 0.3643
Training Epoch: 38 [27008/50048]	Loss: 0.5489
Training Epoch: 38 [27136/50048]	Loss: 0.3995
Training Epoch: 38 [27264/50048]	Loss: 0.5021
Training Epoch: 38 [27392/50048]	Loss: 0.4100
Training Epoch: 38 [27520/50048]	Loss: 0.4104
Training Epoch: 38 [27648/50048]	Loss: 0.5237
Training Epoch: 38 [27776/50048]	Loss: 0.4726
Training Epoch: 38 [27904/50048]	Loss: 0.5868
Training Epoch: 38 [28032/50048]	Loss: 0.4517
Training Epoch: 38 [28160/50048]	Loss: 0.6036
Training Epoch: 38 [28288/50048]	Loss: 0.5400
Training Epoch: 38 [28416/50048]	Loss: 0.5025
Training Epoch: 38 [28544/50048]	Loss: 0.4464
Training Epoch: 38 [28672/50048]	Loss: 0.4365
Training Epoch: 38 [28800/50048]	Loss: 0.4592
Training Epoch: 38 [28928/50048]	Loss: 0.3290
Training Epoch: 38 [29056/50048]	Loss: 0.4760
Training Epoch: 38 [29184/50048]	Loss: 0.4505
Training Epoch: 38 [29312/50048]	Loss: 0.4606
Training Epoch: 38 [29440/50048]	Loss: 0.3872
Training Epoch: 38 [29568/50048]	Loss: 0.3452
Training Epoch: 38 [29696/50048]	Loss: 0.3584
Training Epoch: 38 [29824/50048]	Loss: 0.4726
Training Epoch: 38 [29952/50048]	Loss: 0.4107
Training Epoch: 38 [30080/50048]	Loss: 0.4634
Training Epoch: 38 [30208/50048]	Loss: 0.4092
Training Epoch: 38 [30336/50048]	Loss: 0.4265
Training Epoch: 38 [30464/50048]	Loss: 0.3443
Training Epoch: 38 [30592/50048]	Loss: 0.4362
Training Epoch: 38 [30720/50048]	Loss: 0.4835
Training Epoch: 38 [30848/50048]	Loss: 0.4867
Training Epoch: 38 [30976/50048]	Loss: 0.4064
Training Epoch: 38 [31104/50048]	Loss: 0.4409
Training Epoch: 38 [31232/50048]	Loss: 0.4186
Training Epoch: 38 [31360/50048]	Loss: 0.4083
Training Epoch: 38 [31488/50048]	Loss: 0.5313
Training Epoch: 38 [31616/50048]	Loss: 0.4138
Training Epoch: 38 [31744/50048]	Loss: 0.4347
Training Epoch: 38 [31872/50048]	Loss: 0.3597
Training Epoch: 38 [32000/50048]	Loss: 0.5338
Training Epoch: 38 [32128/50048]	Loss: 0.4880
Training Epoch: 38 [32256/50048]	Loss: 0.4247
Training Epoch: 38 [32384/50048]	Loss: 0.4866
Training Epoch: 38 [32512/50048]	Loss: 0.3716
Training Epoch: 38 [32640/50048]	Loss: 0.5125
Training Epoch: 38 [32768/50048]	Loss: 0.4086
Training Epoch: 38 [32896/50048]	Loss: 0.5405
Training Epoch: 38 [33024/50048]	Loss: 0.5019
Training Epoch: 38 [33152/50048]	Loss: 0.4656
Training Epoch: 38 [33280/50048]	Loss: 0.4613
Training Epoch: 38 [33408/50048]	Loss: 0.4437
Training Epoch: 38 [33536/50048]	Loss: 0.3844
Training Epoch: 38 [33664/50048]	Loss: 0.5376
Training Epoch: 38 [33792/50048]	Loss: 0.6046
Training Epoch: 38 [33920/50048]	Loss: 0.4898
Training Epoch: 38 [34048/50048]	Loss: 0.5036
Training Epoch: 38 [34176/50048]	Loss: 0.4463
Training Epoch: 38 [34304/50048]	Loss: 0.4501
Training Epoch: 38 [34432/50048]	Loss: 0.4257
Training Epoch: 38 [34560/50048]	Loss: 0.4976
Training Epoch: 38 [34688/50048]	Loss: 0.5033
Training Epoch: 38 [34816/50048]	Loss: 0.7492
Training Epoch: 38 [34944/50048]	Loss: 0.3952
Training Epoch: 38 [35072/50048]	Loss: 0.5089
Training Epoch: 38 [35200/50048]	Loss: 0.3260
Training Epoch: 38 [35328/50048]	Loss: 0.4381
Training Epoch: 38 [35456/50048]	Loss: 0.4815
Training Epoch: 38 [35584/50048]	Loss: 0.4316
Training Epoch: 38 [35712/50048]	Loss: 0.3747
Training Epoch: 38 [35840/50048]	Loss: 0.4703
Training Epoch: 38 [35968/50048]	Loss: 0.6230
Training Epoch: 38 [36096/50048]	Loss: 0.4230
Training Epoch: 38 [36224/50048]	Loss: 0.3472
Training Epoch: 38 [36352/50048]	Loss: 0.4328
Training Epoch: 38 [36480/50048]	Loss: 0.4228
Training Epoch: 38 [36608/50048]	Loss: 0.3796
Training Epoch: 38 [36736/50048]	Loss: 0.5556
Training Epoch: 38 [36864/50048]	Loss: 0.6374
Training Epoch: 38 [36992/50048]	Loss: 0.5636
Training Epoch: 38 [37120/50048]	Loss: 0.5025
Training Epoch: 38 [37248/50048]	Loss: 0.5402
Training Epoch: 38 [37376/50048]	Loss: 0.4700
Training Epoch: 38 [37504/50048]	Loss: 0.5070
Training Epoch: 38 [37632/50048]	Loss: 0.6425
Training Epoch: 38 [37760/50048]	Loss: 0.5765
Training Epoch: 38 [37888/50048]	Loss: 0.4038
Training Epoch: 38 [38016/50048]	Loss: 0.3852
Training Epoch: 38 [38144/50048]	Loss: 0.6111
Training Epoch: 38 [38272/50048]	Loss: 0.5571
Training Epoch: 38 [38400/50048]	Loss: 0.4790
Training Epoch: 38 [38528/50048]	Loss: 0.3923
Training Epoch: 38 [38656/50048]	Loss: 0.4795
Training Epoch: 38 [38784/50048]	Loss: 0.5121
Training Epoch: 38 [38912/50048]	Loss: 0.3562
Training Epoch: 38 [39040/50048]	Loss: 0.4580
Training Epoch: 38 [39168/50048]	Loss: 0.4280
Training Epoch: 38 [39296/50048]	Loss: 0.4092
Training Epoch: 38 [39424/50048]	Loss: 0.4257
Training Epoch: 38 [39552/50048]	Loss: 0.5587
Training Epoch: 38 [39680/50048]	Loss: 0.5119
Training Epoch: 38 [39808/50048]	Loss: 0.4387
Training Epoch: 38 [39936/50048]	Loss: 0.5013
Training Epoch: 38 [40064/50048]	Loss: 0.6627
Training Epoch: 38 [40192/50048]	Loss: 0.6074
Training Epoch: 38 [40320/50048]	Loss: 0.5030
Training Epoch: 38 [40448/50048]	Loss: 0.5491
Training Epoch: 38 [40576/50048]	Loss: 0.4000
Training Epoch: 38 [40704/50048]	Loss: 0.4846
Training Epoch: 38 [40832/50048]	Loss: 0.3993
Training Epoch: 38 [40960/50048]	Loss: 0.3777
Training Epoch: 38 [41088/50048]	Loss: 0.3712
Training Epoch: 38 [41216/50048]	Loss: 0.5667
Training Epoch: 38 [41344/50048]	Loss: 0.3699
Training Epoch: 38 [41472/50048]	Loss: 0.5768
Training Epoch: 38 [41600/50048]	Loss: 0.4271
Training Epoch: 38 [41728/50048]	Loss: 0.4842
Training Epoch: 38 [41856/50048]	Loss: 0.4959
Training Epoch: 38 [41984/50048]	Loss: 0.3618
Training Epoch: 38 [42112/50048]	Loss: 0.4462
Training Epoch: 38 [42240/50048]	Loss: 0.4131
Training Epoch: 38 [42368/50048]	Loss: 0.3309
Training Epoch: 38 [42496/50048]	Loss: 0.4866
Training Epoch: 38 [42624/50048]	Loss: 0.5807
Training Epoch: 38 [42752/50048]	Loss: 0.4899
Training Epoch: 38 [42880/50048]	Loss: 0.3753
Training Epoch: 38 [43008/50048]	Loss: 0.4402
Training Epoch: 38 [43136/50048]	Loss: 0.5486
Training Epoch: 38 [43264/50048]	Loss: 0.3601
Training Epoch: 38 [43392/50048]	Loss: 0.3996
Training Epoch: 38 [43520/50048]	Loss: 0.5682
Training Epoch: 38 [43648/50048]	Loss: 0.3793
Training Epoch: 38 [43776/50048]	Loss: 0.5063
Training Epoch: 38 [43904/50048]	Loss: 0.5638
Training Epoch: 38 [44032/50048]	Loss: 0.5433
Training Epoch: 38 [44160/50048]	Loss: 0.4002
Training Epoch: 38 [44288/50048]	Loss: 0.5980
Training Epoch: 38 [44416/50048]	Loss: 0.4916
Training Epoch: 38 [44544/50048]	Loss: 0.5190
Training Epoch: 38 [44672/50048]	Loss: 0.5180
Training Epoch: 38 [44800/50048]	Loss: 0.4427
Training Epoch: 38 [44928/50048]	Loss: 0.4584
Training Epoch: 38 [45056/50048]	Loss: 0.4350
Training Epoch: 38 [45184/50048]	Loss: 0.3445
Training Epoch: 38 [45312/50048]	Loss: 0.3430
Training Epoch: 38 [45440/50048]	Loss: 0.6571
Training Epoch: 38 [45568/50048]	Loss: 0.5618
Training Epoch: 38 [45696/50048]	Loss: 0.4195
2022-12-06 04:35:59,623 [ZeusDataLoader(train)] train epoch 39 done: time=86.42 energy=10495.04
2022-12-06 04:35:59,625 [ZeusDataLoader(eval)] Epoch 39 begin.
Training Epoch: 38 [45824/50048]	Loss: 0.3738
Training Epoch: 38 [45952/50048]	Loss: 0.5147
Training Epoch: 38 [46080/50048]	Loss: 0.4267
Training Epoch: 38 [46208/50048]	Loss: 0.4214
Training Epoch: 38 [46336/50048]	Loss: 0.6647
Training Epoch: 38 [46464/50048]	Loss: 0.4179
Training Epoch: 38 [46592/50048]	Loss: 0.4863
Training Epoch: 38 [46720/50048]	Loss: 0.4668
Training Epoch: 38 [46848/50048]	Loss: 0.5109
Training Epoch: 38 [46976/50048]	Loss: 0.5474
Training Epoch: 38 [47104/50048]	Loss: 0.5877
Training Epoch: 38 [47232/50048]	Loss: 0.6382
Training Epoch: 38 [47360/50048]	Loss: 0.3683
Training Epoch: 38 [47488/50048]	Loss: 0.4284
Training Epoch: 38 [47616/50048]	Loss: 0.4514
Training Epoch: 38 [47744/50048]	Loss: 0.8090
Training Epoch: 38 [47872/50048]	Loss: 0.3990
Training Epoch: 38 [48000/50048]	Loss: 0.4673
Training Epoch: 38 [48128/50048]	Loss: 0.4645
Training Epoch: 38 [48256/50048]	Loss: 0.4826
Training Epoch: 38 [48384/50048]	Loss: 0.4120
Training Epoch: 38 [48512/50048]	Loss: 0.5554
Training Epoch: 38 [48640/50048]	Loss: 0.6329
Training Epoch: 38 [48768/50048]	Loss: 0.5450
Training Epoch: 38 [48896/50048]	Loss: 0.3828
Training Epoch: 38 [49024/50048]	Loss: 0.4455
Training Epoch: 38 [49152/50048]	Loss: 0.7330
Training Epoch: 38 [49280/50048]	Loss: 0.3951
Training Epoch: 38 [49408/50048]	Loss: 0.5390
Training Epoch: 38 [49536/50048]	Loss: 0.5845
Training Epoch: 38 [49664/50048]	Loss: 0.4850
Training Epoch: 38 [49792/50048]	Loss: 0.5749
Training Epoch: 38 [49920/50048]	Loss: 0.4171
Training Epoch: 38 [50048/50048]	Loss: 0.4733
2022-12-06 09:36:03.294 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:36:03,308 [ZeusDataLoader(eval)] eval epoch 39 done: time=3.67 energy=439.11
2022-12-06 04:36:03,308 [ZeusDataLoader(train)] Up to epoch 39: time=3520.23, energy=427241.28, cost=521641.07
2022-12-06 04:36:03,308 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:36:03,308 [ZeusDataLoader(train)] Expected next epoch: time=3610.03, energy=438039.30, cost=534897.45
2022-12-06 04:36:03,309 [ZeusDataLoader(train)] Epoch 40 begin.
Validation Epoch: 38, Average loss: 0.0139, Accuracy: 0.6207
2022-12-06 04:36:03,450 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:36:03,451 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:36:03.453 [ZeusMonitor] Monitor started.
2022-12-06 09:36:03.453 [ZeusMonitor] Running indefinitely. 2022-12-06 09:36:03.453 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:36:03.453 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e40+gpu0.power.log
Training Epoch: 39 [128/50048]	Loss: 0.4828
Training Epoch: 39 [256/50048]	Loss: 0.3853
Training Epoch: 39 [384/50048]	Loss: 0.3954
Training Epoch: 39 [512/50048]	Loss: 0.3370
Training Epoch: 39 [640/50048]	Loss: 0.3316
Training Epoch: 39 [768/50048]	Loss: 0.2649
Training Epoch: 39 [896/50048]	Loss: 0.4880
Training Epoch: 39 [1024/50048]	Loss: 0.3326
Training Epoch: 39 [1152/50048]	Loss: 0.4079
Training Epoch: 39 [1280/50048]	Loss: 0.2643
Training Epoch: 39 [1408/50048]	Loss: 0.4881
Training Epoch: 39 [1536/50048]	Loss: 0.3961
Training Epoch: 39 [1664/50048]	Loss: 0.2709
Training Epoch: 39 [1792/50048]	Loss: 0.3776
Training Epoch: 39 [1920/50048]	Loss: 0.4271
Training Epoch: 39 [2048/50048]	Loss: 0.3691
Training Epoch: 39 [2176/50048]	Loss: 0.3439
Training Epoch: 39 [2304/50048]	Loss: 0.4028
Training Epoch: 39 [2432/50048]	Loss: 0.3949
Training Epoch: 39 [2560/50048]	Loss: 0.3869
Training Epoch: 39 [2688/50048]	Loss: 0.5611
Training Epoch: 39 [2816/50048]	Loss: 0.3892
Training Epoch: 39 [2944/50048]	Loss: 0.3954
Training Epoch: 39 [3072/50048]	Loss: 0.3370
Training Epoch: 39 [3200/50048]	Loss: 0.5145
Training Epoch: 39 [3328/50048]	Loss: 0.3054
Training Epoch: 39 [3456/50048]	Loss: 0.3655
Training Epoch: 39 [3584/50048]	Loss: 0.4141
Training Epoch: 39 [3712/50048]	Loss: 0.4415
Training Epoch: 39 [3840/50048]	Loss: 0.4905
Training Epoch: 39 [3968/50048]	Loss: 0.4177
Training Epoch: 39 [4096/50048]	Loss: 0.4381
Training Epoch: 39 [4224/50048]	Loss: 0.3305
Training Epoch: 39 [4352/50048]	Loss: 0.4841
Training Epoch: 39 [4480/50048]	Loss: 0.3839
Training Epoch: 39 [4608/50048]	Loss: 0.5198
Training Epoch: 39 [4736/50048]	Loss: 0.3606
Training Epoch: 39 [4864/50048]	Loss: 0.3986
Training Epoch: 39 [4992/50048]	Loss: 0.5640
Training Epoch: 39 [5120/50048]	Loss: 0.4291
Training Epoch: 39 [5248/50048]	Loss: 0.4154
Training Epoch: 39 [5376/50048]	Loss: 0.3510
Training Epoch: 39 [5504/50048]	Loss: 0.3582
Training Epoch: 39 [5632/50048]	Loss: 0.3291
Training Epoch: 39 [5760/50048]	Loss: 0.6003
Training Epoch: 39 [5888/50048]	Loss: 0.4554
Training Epoch: 39 [6016/50048]	Loss: 0.3426
Training Epoch: 39 [6144/50048]	Loss: 0.4898
Training Epoch: 39 [6272/50048]	Loss: 0.3305
Training Epoch: 39 [6400/50048]	Loss: 0.3890
Training Epoch: 39 [6528/50048]	Loss: 0.3909
Training Epoch: 39 [6656/50048]	Loss: 0.3335
Training Epoch: 39 [6784/50048]	Loss: 0.4597
Training Epoch: 39 [6912/50048]	Loss: 0.3269
Training Epoch: 39 [7040/50048]	Loss: 0.3394
Training Epoch: 39 [7168/50048]	Loss: 0.3754
Training Epoch: 39 [7296/50048]	Loss: 0.4802
Training Epoch: 39 [7424/50048]	Loss: 0.4873
Training Epoch: 39 [7552/50048]	Loss: 0.3768
Training Epoch: 39 [7680/50048]	Loss: 0.5624
Training Epoch: 39 [7808/50048]	Loss: 0.3512
Training Epoch: 39 [7936/50048]	Loss: 0.6377
Training Epoch: 39 [8064/50048]	Loss: 0.4463
Training Epoch: 39 [8192/50048]	Loss: 0.4435
Training Epoch: 39 [8320/50048]	Loss: 0.4654
Training Epoch: 39 [8448/50048]	Loss: 0.3522
Training Epoch: 39 [8576/50048]	Loss: 0.3542
Training Epoch: 39 [8704/50048]	Loss: 0.4708
Training Epoch: 39 [8832/50048]	Loss: 0.4003
Training Epoch: 39 [8960/50048]	Loss: 0.3093
Training Epoch: 39 [9088/50048]	Loss: 0.4597
Training Epoch: 39 [9216/50048]	Loss: 0.4119
Training Epoch: 39 [9344/50048]	Loss: 0.4195
Training Epoch: 39 [9472/50048]	Loss: 0.4286
Training Epoch: 39 [9600/50048]	Loss: 0.2073
Training Epoch: 39 [9728/50048]	Loss: 0.4247
Training Epoch: 39 [9856/50048]	Loss: 0.4753
Training Epoch: 39 [9984/50048]	Loss: 0.4134
Training Epoch: 39 [10112/50048]	Loss: 0.3283
Training Epoch: 39 [10240/50048]	Loss: 0.4710
Training Epoch: 39 [10368/50048]	Loss: 0.4576
Training Epoch: 39 [10496/50048]	Loss: 0.4932
Training Epoch: 39 [10624/50048]	Loss: 0.2839
Training Epoch: 39 [10752/50048]	Loss: 0.4686
Training Epoch: 39 [10880/50048]	Loss: 0.3559
Training Epoch: 39 [11008/50048]	Loss: 0.4913
Training Epoch: 39 [11136/50048]	Loss: 0.3748
Training Epoch: 39 [11264/50048]	Loss: 0.4288
Training Epoch: 39 [11392/50048]	Loss: 0.3894
Training Epoch: 39 [11520/50048]	Loss: 0.3434
Training Epoch: 39 [11648/50048]	Loss: 0.3316
Training Epoch: 39 [11776/50048]	Loss: 0.3408
Training Epoch: 39 [11904/50048]	Loss: 0.4379
Training Epoch: 39 [12032/50048]	Loss: 0.3731
Training Epoch: 39 [12160/50048]	Loss: 0.3352
Training Epoch: 39 [12288/50048]	Loss: 0.4197
Training Epoch: 39 [12416/50048]	Loss: 0.4907
Training Epoch: 39 [12544/50048]	Loss: 0.2873
Training Epoch: 39 [12672/50048]	Loss: 0.3882
Training Epoch: 39 [12800/50048]	Loss: 0.4315
Training Epoch: 39 [12928/50048]	Loss: 0.4811
Training Epoch: 39 [13056/50048]	Loss: 0.4703
Training Epoch: 39 [13184/50048]	Loss: 0.3898
Training Epoch: 39 [13312/50048]	Loss: 0.4033
Training Epoch: 39 [13440/50048]	Loss: 0.3786
Training Epoch: 39 [13568/50048]	Loss: 0.2811
Training Epoch: 39 [13696/50048]	Loss: 0.5626
Training Epoch: 39 [13824/50048]	Loss: 0.4561
Training Epoch: 39 [13952/50048]	Loss: 0.3803
Training Epoch: 39 [14080/50048]	Loss: 0.3546
Training Epoch: 39 [14208/50048]	Loss: 0.3522
Training Epoch: 39 [14336/50048]	Loss: 0.3762
Training Epoch: 39 [14464/50048]	Loss: 0.5555
Training Epoch: 39 [14592/50048]	Loss: 0.4866
Training Epoch: 39 [14720/50048]	Loss: 0.4172
Training Epoch: 39 [14848/50048]	Loss: 0.4805
Training Epoch: 39 [14976/50048]	Loss: 0.3202
Training Epoch: 39 [15104/50048]	Loss: 0.4556
Training Epoch: 39 [15232/50048]	Loss: 0.5474
Training Epoch: 39 [15360/50048]	Loss: 0.3576
Training Epoch: 39 [15488/50048]	Loss: 0.4665
Training Epoch: 39 [15616/50048]	Loss: 0.3263
Training Epoch: 39 [15744/50048]	Loss: 0.2985
Training Epoch: 39 [15872/50048]	Loss: 0.4430
Training Epoch: 39 [16000/50048]	Loss: 0.5595
Training Epoch: 39 [16128/50048]	Loss: 0.4837
Training Epoch: 39 [16256/50048]	Loss: 0.4136
Training Epoch: 39 [16384/50048]	Loss: 0.3261
Training Epoch: 39 [16512/50048]	Loss: 0.4334
Training Epoch: 39 [16640/50048]	Loss: 0.4804
Training Epoch: 39 [16768/50048]	Loss: 0.4754
Training Epoch: 39 [16896/50048]	Loss: 0.3959
Training Epoch: 39 [17024/50048]	Loss: 0.4116
Training Epoch: 39 [17152/50048]	Loss: 0.3256
Training Epoch: 39 [17280/50048]	Loss: 0.2796
Training Epoch: 39 [17408/50048]	Loss: 0.3217
Training Epoch: 39 [17536/50048]	Loss: 0.3371
Training Epoch: 39 [17664/50048]	Loss: 0.3356
Training Epoch: 39 [17792/50048]	Loss: 0.3918
Training Epoch: 39 [17920/50048]	Loss: 0.3488
Training Epoch: 39 [18048/50048]	Loss: 0.5385
Training Epoch: 39 [18176/50048]	Loss: 0.3064
Training Epoch: 39 [18304/50048]	Loss: 0.5076
Training Epoch: 39 [18432/50048]	Loss: 0.4256
Training Epoch: 39 [18560/50048]	Loss: 0.3179
Training Epoch: 39 [18688/50048]	Loss: 0.4137
Training Epoch: 39 [18816/50048]	Loss: 0.3616
Training Epoch: 39 [18944/50048]	Loss: 0.4000
Training Epoch: 39 [19072/50048]	Loss: 0.5250
Training Epoch: 39 [19200/50048]	Loss: 0.4157
Training Epoch: 39 [19328/50048]	Loss: 0.4351
Training Epoch: 39 [19456/50048]	Loss: 0.5253
Training Epoch: 39 [19584/50048]	Loss: 0.4632
Training Epoch: 39 [19712/50048]	Loss: 0.4696
Training Epoch: 39 [19840/50048]	Loss: 0.4813
Training Epoch: 39 [19968/50048]	Loss: 0.3936
Training Epoch: 39 [20096/50048]	Loss: 0.3630
Training Epoch: 39 [20224/50048]	Loss: 0.5723
Training Epoch: 39 [20352/50048]	Loss: 0.4720
Training Epoch: 39 [20480/50048]	Loss: 0.5473
Training Epoch: 39 [20608/50048]	Loss: 0.3673
Training Epoch: 39 [20736/50048]	Loss: 0.3317
Training Epoch: 39 [20864/50048]	Loss: 0.3659
Training Epoch: 39 [20992/50048]	Loss: 0.4857
Training Epoch: 39 [21120/50048]	Loss: 0.3599
Training Epoch: 39 [21248/50048]	Loss: 0.5832
Training Epoch: 39 [21376/50048]	Loss: 0.4292
Training Epoch: 39 [21504/50048]	Loss: 0.4053
Training Epoch: 39 [21632/50048]	Loss: 0.3199
Training Epoch: 39 [21760/50048]	Loss: 0.3931
Training Epoch: 39 [21888/50048]	Loss: 0.3501
Training Epoch: 39 [22016/50048]	Loss: 0.4692
Training Epoch: 39 [22144/50048]	Loss: 0.5588
Training Epoch: 39 [22272/50048]	Loss: 0.5323
Training Epoch: 39 [22400/50048]	Loss: 0.4299
Training Epoch: 39 [22528/50048]	Loss: 0.3597
Training Epoch: 39 [22656/50048]	Loss: 0.4807
Training Epoch: 39 [22784/50048]	Loss: 0.4527
Training Epoch: 39 [22912/50048]	Loss: 0.4802
Training Epoch: 39 [23040/50048]	Loss: 0.5623
Training Epoch: 39 [23168/50048]	Loss: 0.3495
Training Epoch: 39 [23296/50048]	Loss: 0.3844
Training Epoch: 39 [23424/50048]	Loss: 0.3664
Training Epoch: 39 [23552/50048]	Loss: 0.3748
Training Epoch: 39 [23680/50048]	Loss: 0.3925
Training Epoch: 39 [23808/50048]	Loss: 0.2779
Training Epoch: 39 [23936/50048]	Loss: 0.3678
Training Epoch: 39 [24064/50048]	Loss: 0.4350
Training Epoch: 39 [24192/50048]	Loss: 0.3689
Training Epoch: 39 [24320/50048]	Loss: 0.4068
Training Epoch: 39 [24448/50048]	Loss: 0.4999
Training Epoch: 39 [24576/50048]	Loss: 0.4656
Training Epoch: 39 [24704/50048]	Loss: 0.4835
Training Epoch: 39 [24832/50048]	Loss: 0.5700
Training Epoch: 39 [24960/50048]	Loss: 0.5605
Training Epoch: 39 [25088/50048]	Loss: 0.4664
Training Epoch: 39 [25216/50048]	Loss: 0.4086
Training Epoch: 39 [25344/50048]	Loss: 0.4635
Training Epoch: 39 [25472/50048]	Loss: 0.4626
Training Epoch: 39 [25600/50048]	Loss: 0.3974
Training Epoch: 39 [25728/50048]	Loss: 0.4588
Training Epoch: 39 [25856/50048]	Loss: 0.5271
Training Epoch: 39 [25984/50048]	Loss: 0.4236
Training Epoch: 39 [26112/50048]	Loss: 0.4952
Training Epoch: 39 [26240/50048]	Loss: 0.4132
Training Epoch: 39 [26368/50048]	Loss: 0.3896
Training Epoch: 39 [26496/50048]	Loss: 0.5324
Training Epoch: 39 [26624/50048]	Loss: 0.4979
Training Epoch: 39 [26752/50048]	Loss: 0.6170
Training Epoch: 39 [26880/50048]	Loss: 0.3600
Training Epoch: 39 [27008/50048]	Loss: 0.4030
Training Epoch: 39 [27136/50048]	Loss: 0.3094
Training Epoch: 39 [27264/50048]	Loss: 0.3096
Training Epoch: 39 [27392/50048]	Loss: 0.5118
Training Epoch: 39 [27520/50048]	Loss: 0.3848
Training Epoch: 39 [27648/50048]	Loss: 0.4857
Training Epoch: 39 [27776/50048]	Loss: 0.3641
Training Epoch: 39 [27904/50048]	Loss: 0.3738
Training Epoch: 39 [28032/50048]	Loss: 0.3608
Training Epoch: 39 [28160/50048]	Loss: 0.4497
Training Epoch: 39 [28288/50048]	Loss: 0.4705
Training Epoch: 39 [28416/50048]	Loss: 0.3839
Training Epoch: 39 [28544/50048]	Loss: 0.3427
Training Epoch: 39 [28672/50048]	Loss: 0.3464
Training Epoch: 39 [28800/50048]	Loss: 0.5080
Training Epoch: 39 [28928/50048]	Loss: 0.3867
Training Epoch: 39 [29056/50048]	Loss: 0.4510
Training Epoch: 39 [29184/50048]	Loss: 0.5391
Training Epoch: 39 [29312/50048]	Loss: 0.3829
Training Epoch: 39 [29440/50048]	Loss: 0.5233
Training Epoch: 39 [29568/50048]	Loss: 0.3124
Training Epoch: 39 [29696/50048]	Loss: 0.4144
Training Epoch: 39 [29824/50048]	Loss: 0.4268
Training Epoch: 39 [29952/50048]	Loss: 0.4953
Training Epoch: 39 [30080/50048]	Loss: 0.4040
Training Epoch: 39 [30208/50048]	Loss: 0.3155
Training Epoch: 39 [30336/50048]	Loss: 0.3143
Training Epoch: 39 [30464/50048]	Loss: 0.3605
Training Epoch: 39 [30592/50048]	Loss: 0.3862
Training Epoch: 39 [30720/50048]	Loss: 0.4875
Training Epoch: 39 [30848/50048]	Loss: 0.5301
Training Epoch: 39 [30976/50048]	Loss: 0.5224
Training Epoch: 39 [31104/50048]	Loss: 0.5102
Training Epoch: 39 [31232/50048]	Loss: 0.4818
Training Epoch: 39 [31360/50048]	Loss: 0.4525
Training Epoch: 39 [31488/50048]	Loss: 0.6305
Training Epoch: 39 [31616/50048]	Loss: 0.5095
Training Epoch: 39 [31744/50048]	Loss: 0.4538
Training Epoch: 39 [31872/50048]	Loss: 0.5962
Training Epoch: 39 [32000/50048]	Loss: 0.4497
Training Epoch: 39 [32128/50048]	Loss: 0.2981
Training Epoch: 39 [32256/50048]	Loss: 0.4570
Training Epoch: 39 [32384/50048]	Loss: 0.3175
Training Epoch: 39 [32512/50048]	Loss: 0.3462
Training Epoch: 39 [32640/50048]	Loss: 0.3837
Training Epoch: 39 [32768/50048]	Loss: 0.4648
Training Epoch: 39 [32896/50048]	Loss: 0.5402
Training Epoch: 39 [33024/50048]	Loss: 0.5170
Training Epoch: 39 [33152/50048]	Loss: 0.5738
Training Epoch: 39 [33280/50048]	Loss: 0.5046
Training Epoch: 39 [33408/50048]	Loss: 0.4738
Training Epoch: 39 [33536/50048]	Loss: 0.3886
Training Epoch: 39 [33664/50048]	Loss: 0.4854
Training Epoch: 39 [33792/50048]	Loss: 0.4533
Training Epoch: 39 [33920/50048]	Loss: 0.4204
Training Epoch: 39 [34048/50048]	Loss: 0.4519
Training Epoch: 39 [34176/50048]	Loss: 0.3718
Training Epoch: 39 [34304/50048]	Loss: 0.4328
Training Epoch: 39 [34432/50048]	Loss: 0.4713
Training Epoch: 39 [34560/50048]	Loss: 0.3862
Training Epoch: 39 [34688/50048]	Loss: 0.4932
Training Epoch: 39 [34816/50048]	Loss: 0.4644
Training Epoch: 39 [34944/50048]	Loss: 0.4748
Training Epoch: 39 [35072/50048]	Loss: 0.3664
Training Epoch: 39 [35200/50048]	Loss: 0.4518
Training Epoch: 39 [35328/50048]	Loss: 0.3144
Training Epoch: 39 [35456/50048]	Loss: 0.6273
Training Epoch: 39 [35584/50048]	Loss: 0.3891
Training Epoch: 39 [35712/50048]	Loss: 0.3909
Training Epoch: 39 [35840/50048]	Loss: 0.4174
Training Epoch: 39 [35968/50048]	Loss: 0.3805
Training Epoch: 39 [36096/50048]	Loss: 0.4973
Training Epoch: 39 [36224/50048]	Loss: 0.3995
Training Epoch: 39 [36352/50048]	Loss: 0.4551
Training Epoch: 39 [36480/50048]	Loss: 0.4824
Training Epoch: 39 [36608/50048]	Loss: 0.4668
Training Epoch: 39 [36736/50048]	Loss: 0.3155
Training Epoch: 39 [36864/50048]	Loss: 0.3991
Training Epoch: 39 [36992/50048]	Loss: 0.3604
Training Epoch: 39 [37120/50048]	Loss: 0.4410
Training Epoch: 39 [37248/50048]	Loss: 0.5183
Training Epoch: 39 [37376/50048]	Loss: 0.4838
Training Epoch: 39 [37504/50048]	Loss: 0.3647
Training Epoch: 39 [37632/50048]	Loss: 0.4377
Training Epoch: 39 [37760/50048]	Loss: 0.2562
Training Epoch: 39 [37888/50048]	Loss: 0.5367
Training Epoch: 39 [38016/50048]	Loss: 0.5070
Training Epoch: 39 [38144/50048]	Loss: 0.5690
Training Epoch: 39 [38272/50048]	Loss: 0.4223
Training Epoch: 39 [38400/50048]	Loss: 0.4892
Training Epoch: 39 [38528/50048]	Loss: 0.4329
Training Epoch: 39 [38656/50048]	Loss: 0.3969
Training Epoch: 39 [38784/50048]	Loss: 0.4727
Training Epoch: 39 [38912/50048]	Loss: 0.3949
Training Epoch: 39 [39040/50048]	Loss: 0.6385
Training Epoch: 39 [39168/50048]	Loss: 0.4389
Training Epoch: 39 [39296/50048]	Loss: 0.4718
Training Epoch: 39 [39424/50048]	Loss: 0.4704
Training Epoch: 39 [39552/50048]	Loss: 0.4464
Training Epoch: 39 [39680/50048]	Loss: 0.5052
Training Epoch: 39 [39808/50048]	Loss: 0.4256
Training Epoch: 39 [39936/50048]	Loss: 0.4203
Training Epoch: 39 [40064/50048]	Loss: 0.3921
Training Epoch: 39 [40192/50048]	Loss: 0.5219
Training Epoch: 39 [40320/50048]	Loss: 0.3969
Training Epoch: 39 [40448/50048]	Loss: 0.4718
Training Epoch: 39 [40576/50048]	Loss: 0.3726
Training Epoch: 39 [40704/50048]	Loss: 0.4789
Training Epoch: 39 [40832/50048]	Loss: 0.4484
Training Epoch: 39 [40960/50048]	Loss: 0.4707
Training Epoch: 39 [41088/50048]	Loss: 0.5632
Training Epoch: 39 [41216/50048]	Loss: 0.4039
Training Epoch: 39 [41344/50048]	Loss: 0.5476
Training Epoch: 39 [41472/50048]	Loss: 0.4887
Training Epoch: 39 [41600/50048]	Loss: 0.4694
Training Epoch: 39 [41728/50048]	Loss: 0.4161
Training Epoch: 39 [41856/50048]	Loss: 0.3808
Training Epoch: 39 [41984/50048]	Loss: 0.4654
Training Epoch: 39 [42112/50048]	Loss: 0.5551
Training Epoch: 39 [42240/50048]	Loss: 0.5392
Training Epoch: 39 [42368/50048]	Loss: 0.4055
Training Epoch: 39 [42496/50048]	Loss: 0.3851
Training Epoch: 39 [42624/50048]	Loss: 0.3531
Training Epoch: 39 [42752/50048]	Loss: 0.5190
Training Epoch: 39 [42880/50048]	Loss: 0.3247
Training Epoch: 39 [43008/50048]	Loss: 0.4956
Training Epoch: 39 [43136/50048]	Loss: 0.3680
Training Epoch: 39 [43264/50048]	Loss: 0.4933
Training Epoch: 39 [43392/50048]	Loss: 0.4587
Training Epoch: 39 [43520/50048]	Loss: 0.6301
Training Epoch: 39 [43648/50048]	Loss: 0.4389
Training Epoch: 39 [43776/50048]	Loss: 0.4072
Training Epoch: 39 [43904/50048]	Loss: 0.5668
Training Epoch: 39 [44032/50048]	Loss: 0.5284
Training Epoch: 39 [44160/50048]	Loss: 0.4350
Training Epoch: 39 [44288/50048]	Loss: 0.7120
Training Epoch: 39 [44416/50048]	Loss: 0.4164
Training Epoch: 39 [44544/50048]	Loss: 0.5791
Training Epoch: 39 [44672/50048]	Loss: 0.5591
Training Epoch: 39 [44800/50048]	Loss: 0.4378
Training Epoch: 39 [44928/50048]	Loss: 0.5476
Training Epoch: 39 [45056/50048]	Loss: 0.5121
Training Epoch: 39 [45184/50048]	Loss: 0.5519
Training Epoch: 39 [45312/50048]	Loss: 0.5893
Training Epoch: 39 [45440/50048]	Loss: 0.4772
Training Epoch: 39 [45568/50048]	Loss: 0.3555
Training Epoch: 39 [45696/50048]	Loss: 0.3549
2022-12-06 04:37:29,708 [ZeusDataLoader(train)] train epoch 40 done: time=86.39 energy=10497.10
2022-12-06 04:37:29,710 [ZeusDataLoader(eval)] Epoch 40 begin.
Training Epoch: 39 [45824/50048]	Loss: 0.5033
Training Epoch: 39 [45952/50048]	Loss: 0.4447
Training Epoch: 39 [46080/50048]	Loss: 0.3347
Training Epoch: 39 [46208/50048]	Loss: 0.4149
Training Epoch: 39 [46336/50048]	Loss: 0.4157
Training Epoch: 39 [46464/50048]	Loss: 0.4629
Training Epoch: 39 [46592/50048]	Loss: 0.6543
Training Epoch: 39 [46720/50048]	Loss: 0.4744
Training Epoch: 39 [46848/50048]	Loss: 0.4792
Training Epoch: 39 [46976/50048]	Loss: 0.5061
Training Epoch: 39 [47104/50048]	Loss: 0.5465
Training Epoch: 39 [47232/50048]	Loss: 0.5514
Training Epoch: 39 [47360/50048]	Loss: 0.5570
Training Epoch: 39 [47488/50048]	Loss: 0.4372
Training Epoch: 39 [47616/50048]	Loss: 0.4559
Training Epoch: 39 [47744/50048]	Loss: 0.4985
Training Epoch: 39 [47872/50048]	Loss: 0.4299
Training Epoch: 39 [48000/50048]	Loss: 0.4920
Training Epoch: 39 [48128/50048]	Loss: 0.4836
Training Epoch: 39 [48256/50048]	Loss: 0.5929
Training Epoch: 39 [48384/50048]	Loss: 0.4674
Training Epoch: 39 [48512/50048]	Loss: 0.4370
Training Epoch: 39 [48640/50048]	Loss: 0.4129
Training Epoch: 39 [48768/50048]	Loss: 0.4420
Training Epoch: 39 [48896/50048]	Loss: 0.4402
Training Epoch: 39 [49024/50048]	Loss: 0.4043
Training Epoch: 39 [49152/50048]	Loss: 0.3195
Training Epoch: 39 [49280/50048]	Loss: 0.4807
Training Epoch: 39 [49408/50048]	Loss: 0.5818
Training Epoch: 39 [49536/50048]	Loss: 0.4449
Training Epoch: 39 [49664/50048]	Loss: 0.4292
Training Epoch: 39 [49792/50048]	Loss: 0.6551
Training Epoch: 39 [49920/50048]	Loss: 0.6227
Training Epoch: 39 [50048/50048]	Loss: 0.5260
2022-12-06 09:37:33.374 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:37:33,391 [ZeusDataLoader(eval)] eval epoch 40 done: time=3.67 energy=441.11
2022-12-06 04:37:33,391 [ZeusDataLoader(train)] Up to epoch 40: time=3610.29, energy=438179.50, cost=534990.53
2022-12-06 04:37:33,391 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:37:33,391 [ZeusDataLoader(train)] Expected next epoch: time=3700.09, energy=448977.51, cost=548246.92
2022-12-06 04:37:33,392 [ZeusDataLoader(train)] Epoch 41 begin.
Validation Epoch: 39, Average loss: 0.0136, Accuracy: 0.6281
2022-12-06 04:37:33,529 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:37:33,529 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:37:33.533 [ZeusMonitor] Monitor started.
2022-12-06 09:37:33.533 [ZeusMonitor] Running indefinitely. 2022-12-06 09:37:33.533 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:37:33.533 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e41+gpu0.power.log
Training Epoch: 40 [128/50048]	Loss: 0.4535
Training Epoch: 40 [256/50048]	Loss: 0.3075
Training Epoch: 40 [384/50048]	Loss: 0.3326
Training Epoch: 40 [512/50048]	Loss: 0.3826
Training Epoch: 40 [640/50048]	Loss: 0.3932
Training Epoch: 40 [768/50048]	Loss: 0.3359
Training Epoch: 40 [896/50048]	Loss: 0.2766
Training Epoch: 40 [1024/50048]	Loss: 0.4123
Training Epoch: 40 [1152/50048]	Loss: 0.4639
Training Epoch: 40 [1280/50048]	Loss: 0.3844
Training Epoch: 40 [1408/50048]	Loss: 0.3130
Training Epoch: 40 [1536/50048]	Loss: 0.4178
Training Epoch: 40 [1664/50048]	Loss: 0.4272
Training Epoch: 40 [1792/50048]	Loss: 0.3417
Training Epoch: 40 [1920/50048]	Loss: 0.4716
Training Epoch: 40 [2048/50048]	Loss: 0.3564
Training Epoch: 40 [2176/50048]	Loss: 0.3684
Training Epoch: 40 [2304/50048]	Loss: 0.5096
Training Epoch: 40 [2432/50048]	Loss: 0.3399
Training Epoch: 40 [2560/50048]	Loss: 0.4206
Training Epoch: 40 [2688/50048]	Loss: 0.3527
Training Epoch: 40 [2816/50048]	Loss: 0.3819
Training Epoch: 40 [2944/50048]	Loss: 0.2963
Training Epoch: 40 [3072/50048]	Loss: 0.3838
Training Epoch: 40 [3200/50048]	Loss: 0.3135
Training Epoch: 40 [3328/50048]	Loss: 0.2558
Training Epoch: 40 [3456/50048]	Loss: 0.4443
Training Epoch: 40 [3584/50048]	Loss: 0.4422
Training Epoch: 40 [3712/50048]	Loss: 0.5327
Training Epoch: 40 [3840/50048]	Loss: 0.4039
Training Epoch: 40 [3968/50048]	Loss: 0.3705
Training Epoch: 40 [4096/50048]	Loss: 0.3842
Training Epoch: 40 [4224/50048]	Loss: 0.3160
Training Epoch: 40 [4352/50048]	Loss: 0.4265
Training Epoch: 40 [4480/50048]	Loss: 0.2668
Training Epoch: 40 [4608/50048]	Loss: 0.4013
Training Epoch: 40 [4736/50048]	Loss: 0.3636
Training Epoch: 40 [4864/50048]	Loss: 0.5147
Training Epoch: 40 [4992/50048]	Loss: 0.3578
Training Epoch: 40 [5120/50048]	Loss: 0.3570
Training Epoch: 40 [5248/50048]	Loss: 0.3499
Training Epoch: 40 [5376/50048]	Loss: 0.3715
Training Epoch: 40 [5504/50048]	Loss: 0.4345
Training Epoch: 40 [5632/50048]	Loss: 0.2927
Training Epoch: 40 [5760/50048]	Loss: 0.3780
Training Epoch: 40 [5888/50048]	Loss: 0.3351
Training Epoch: 40 [6016/50048]	Loss: 0.2495
Training Epoch: 40 [6144/50048]	Loss: 0.4230
Training Epoch: 40 [6272/50048]	Loss: 0.3952
Training Epoch: 40 [6400/50048]	Loss: 0.4994
Training Epoch: 40 [6528/50048]	Loss: 0.3325
Training Epoch: 40 [6656/50048]	Loss: 0.5331
Training Epoch: 40 [6784/50048]	Loss: 0.3485
Training Epoch: 40 [6912/50048]	Loss: 0.3140
Training Epoch: 40 [7040/50048]	Loss: 0.3152
Training Epoch: 40 [7168/50048]	Loss: 0.4137
Training Epoch: 40 [7296/50048]	Loss: 0.3223
Training Epoch: 40 [7424/50048]	Loss: 0.3340
Training Epoch: 40 [7552/50048]	Loss: 0.2805
Training Epoch: 40 [7680/50048]	Loss: 0.3488
Training Epoch: 40 [7808/50048]	Loss: 0.4837
Training Epoch: 40 [7936/50048]	Loss: 0.3620
Training Epoch: 40 [8064/50048]	Loss: 0.3779
Training Epoch: 40 [8192/50048]	Loss: 0.4072
Training Epoch: 40 [8320/50048]	Loss: 0.3545
Training Epoch: 40 [8448/50048]	Loss: 0.3316
Training Epoch: 40 [8576/50048]	Loss: 0.3831
Training Epoch: 40 [8704/50048]	Loss: 0.3836
Training Epoch: 40 [8832/50048]	Loss: 0.3492
Training Epoch: 40 [8960/50048]	Loss: 0.3592
Training Epoch: 40 [9088/50048]	Loss: 0.3250
Training Epoch: 40 [9216/50048]	Loss: 0.3917
Training Epoch: 40 [9344/50048]	Loss: 0.4685
Training Epoch: 40 [9472/50048]	Loss: 0.3971
Training Epoch: 40 [9600/50048]	Loss: 0.3172
Training Epoch: 40 [9728/50048]	Loss: 0.4380
Training Epoch: 40 [9856/50048]	Loss: 0.3950
Training Epoch: 40 [9984/50048]	Loss: 0.3965
Training Epoch: 40 [10112/50048]	Loss: 0.4061
Training Epoch: 40 [10240/50048]	Loss: 0.4590
Training Epoch: 40 [10368/50048]	Loss: 0.4041
Training Epoch: 40 [10496/50048]	Loss: 0.4250
Training Epoch: 40 [10624/50048]	Loss: 0.3221
Training Epoch: 40 [10752/50048]	Loss: 0.6045
Training Epoch: 40 [10880/50048]	Loss: 0.2943
Training Epoch: 40 [11008/50048]	Loss: 0.4749
Training Epoch: 40 [11136/50048]	Loss: 0.3047
Training Epoch: 40 [11264/50048]	Loss: 0.4785
Training Epoch: 40 [11392/50048]	Loss: 0.5090
Training Epoch: 40 [11520/50048]	Loss: 0.4020
Training Epoch: 40 [11648/50048]	Loss: 0.4224
Training Epoch: 40 [11776/50048]	Loss: 0.3321
Training Epoch: 40 [11904/50048]	Loss: 0.3951
Training Epoch: 40 [12032/50048]	Loss: 0.3984
Training Epoch: 40 [12160/50048]	Loss: 0.3527
Training Epoch: 40 [12288/50048]	Loss: 0.3349
Training Epoch: 40 [12416/50048]	Loss: 0.5344
Training Epoch: 40 [12544/50048]	Loss: 0.4348
Training Epoch: 40 [12672/50048]	Loss: 0.3766
Training Epoch: 40 [12800/50048]	Loss: 0.4380
Training Epoch: 40 [12928/50048]	Loss: 0.2773
Training Epoch: 40 [13056/50048]	Loss: 0.5383
Training Epoch: 40 [13184/50048]	Loss: 0.2585
Training Epoch: 40 [13312/50048]	Loss: 0.5326
Training Epoch: 40 [13440/50048]	Loss: 0.3563
Training Epoch: 40 [13568/50048]	Loss: 0.4074
Training Epoch: 40 [13696/50048]	Loss: 0.3684
Training Epoch: 40 [13824/50048]	Loss: 0.3462
Training Epoch: 40 [13952/50048]	Loss: 0.2649
Training Epoch: 40 [14080/50048]	Loss: 0.4358
Training Epoch: 40 [14208/50048]	Loss: 0.5682
Training Epoch: 40 [14336/50048]	Loss: 0.3422
Training Epoch: 40 [14464/50048]	Loss: 0.2467
Training Epoch: 40 [14592/50048]	Loss: 0.5100
Training Epoch: 40 [14720/50048]	Loss: 0.3537
Training Epoch: 40 [14848/50048]	Loss: 0.4044
Training Epoch: 40 [14976/50048]	Loss: 0.3885
Training Epoch: 40 [15104/50048]	Loss: 0.3413
Training Epoch: 40 [15232/50048]	Loss: 0.3538
Training Epoch: 40 [15360/50048]	Loss: 0.3309
Training Epoch: 40 [15488/50048]	Loss: 0.3633
Training Epoch: 40 [15616/50048]	Loss: 0.2874
Training Epoch: 40 [15744/50048]	Loss: 0.4045
Training Epoch: 40 [15872/50048]	Loss: 0.4289
Training Epoch: 40 [16000/50048]	Loss: 0.4529
Training Epoch: 40 [16128/50048]	Loss: 0.3973
Training Epoch: 40 [16256/50048]	Loss: 0.2465
Training Epoch: 40 [16384/50048]	Loss: 0.4043
Training Epoch: 40 [16512/50048]	Loss: 0.4147
Training Epoch: 40 [16640/50048]	Loss: 0.3375
Training Epoch: 40 [16768/50048]	Loss: 0.3751
Training Epoch: 40 [16896/50048]	Loss: 0.4940
Training Epoch: 40 [17024/50048]	Loss: 0.3611
Training Epoch: 40 [17152/50048]	Loss: 0.3960
Training Epoch: 40 [17280/50048]	Loss: 0.4394
Training Epoch: 40 [17408/50048]	Loss: 0.4429
Training Epoch: 40 [17536/50048]	Loss: 0.4632
Training Epoch: 40 [17664/50048]	Loss: 0.4595
Training Epoch: 40 [17792/50048]	Loss: 0.3707
Training Epoch: 40 [17920/50048]	Loss: 0.3029
Training Epoch: 40 [18048/50048]	Loss: 0.3610
Training Epoch: 40 [18176/50048]	Loss: 0.4346
Training Epoch: 40 [18304/50048]	Loss: 0.4402
Training Epoch: 40 [18432/50048]	Loss: 0.4500
Training Epoch: 40 [18560/50048]	Loss: 0.4184
Training Epoch: 40 [18688/50048]	Loss: 0.3372
Training Epoch: 40 [18816/50048]	Loss: 0.2925
Training Epoch: 40 [18944/50048]	Loss: 0.4983
Training Epoch: 40 [19072/50048]	Loss: 0.4406
Training Epoch: 40 [19200/50048]	Loss: 0.3708
Training Epoch: 40 [19328/50048]	Loss: 0.3648
Training Epoch: 40 [19456/50048]	Loss: 0.4555
Training Epoch: 40 [19584/50048]	Loss: 0.4399
Training Epoch: 40 [19712/50048]	Loss: 0.3799
Training Epoch: 40 [19840/50048]	Loss: 0.3823
Training Epoch: 40 [19968/50048]	Loss: 0.4148
Training Epoch: 40 [20096/50048]	Loss: 0.3694
Training Epoch: 40 [20224/50048]	Loss: 0.4446
Training Epoch: 40 [20352/50048]	Loss: 0.4953
Training Epoch: 40 [20480/50048]	Loss: 0.3107
Training Epoch: 40 [20608/50048]	Loss: 0.4693
Training Epoch: 40 [20736/50048]	Loss: 0.4687
Training Epoch: 40 [20864/50048]	Loss: 0.3317
Training Epoch: 40 [20992/50048]	Loss: 0.3703
Training Epoch: 40 [21120/50048]	Loss: 0.2957
Training Epoch: 40 [21248/50048]	Loss: 0.3541
Training Epoch: 40 [21376/50048]	Loss: 0.4893
Training Epoch: 40 [21504/50048]	Loss: 0.4868
Training Epoch: 40 [21632/50048]	Loss: 0.3366
Training Epoch: 40 [21760/50048]	Loss: 0.3357
Training Epoch: 40 [21888/50048]	Loss: 0.3972
Training Epoch: 40 [22016/50048]	Loss: 0.3608
Training Epoch: 40 [22144/50048]	Loss: 0.4696
Training Epoch: 40 [22272/50048]	Loss: 0.3342
Training Epoch: 40 [22400/50048]	Loss: 0.3487
Training Epoch: 40 [22528/50048]	Loss: 0.4026
Training Epoch: 40 [22656/50048]	Loss: 0.3901
Training Epoch: 40 [22784/50048]	Loss: 0.3328
Training Epoch: 40 [22912/50048]	Loss: 0.3275
Training Epoch: 40 [23040/50048]	Loss: 0.4578
Training Epoch: 40 [23168/50048]	Loss: 0.3527
Training Epoch: 40 [23296/50048]	Loss: 0.3763
Training Epoch: 40 [23424/50048]	Loss: 0.4568
Training Epoch: 40 [23552/50048]	Loss: 0.4636
Training Epoch: 40 [23680/50048]	Loss: 0.3969
Training Epoch: 40 [23808/50048]	Loss: 0.3830
Training Epoch: 40 [23936/50048]	Loss: 0.7315
Training Epoch: 40 [24064/50048]	Loss: 0.3657
Training Epoch: 40 [24192/50048]	Loss: 0.4110
Training Epoch: 40 [24320/50048]	Loss: 0.4180
Training Epoch: 40 [24448/50048]	Loss: 0.4402
Training Epoch: 40 [24576/50048]	Loss: 0.5492
Training Epoch: 40 [24704/50048]	Loss: 0.3473
Training Epoch: 40 [24832/50048]	Loss: 0.4124
Training Epoch: 40 [24960/50048]	Loss: 0.3246
Training Epoch: 40 [25088/50048]	Loss: 0.4582
Training Epoch: 40 [25216/50048]	Loss: 0.4465
Training Epoch: 40 [25344/50048]	Loss: 0.5252
Training Epoch: 40 [25472/50048]	Loss: 0.3823
Training Epoch: 40 [25600/50048]	Loss: 0.4810
Training Epoch: 40 [25728/50048]	Loss: 0.3988
Training Epoch: 40 [25856/50048]	Loss: 0.4802
Training Epoch: 40 [25984/50048]	Loss: 0.4909
Training Epoch: 40 [26112/50048]	Loss: 0.3984
Training Epoch: 40 [26240/50048]	Loss: 0.4825
Training Epoch: 40 [26368/50048]	Loss: 0.3901
Training Epoch: 40 [26496/50048]	Loss: 0.3710
Training Epoch: 40 [26624/50048]	Loss: 0.4039
Training Epoch: 40 [26752/50048]	Loss: 0.3499
Training Epoch: 40 [26880/50048]	Loss: 0.4936
Training Epoch: 40 [27008/50048]	Loss: 0.3582
Training Epoch: 40 [27136/50048]	Loss: 0.3812
Training Epoch: 40 [27264/50048]	Loss: 0.5416
Training Epoch: 40 [27392/50048]	Loss: 0.4143
Training Epoch: 40 [27520/50048]	Loss: 0.5141
Training Epoch: 40 [27648/50048]	Loss: 0.4077
Training Epoch: 40 [27776/50048]	Loss: 0.4012
Training Epoch: 40 [27904/50048]	Loss: 0.3875
Training Epoch: 40 [28032/50048]	Loss: 0.4822
Training Epoch: 40 [28160/50048]	Loss: 0.3979
Training Epoch: 40 [28288/50048]	Loss: 0.3823
Training Epoch: 40 [28416/50048]	Loss: 0.3113
Training Epoch: 40 [28544/50048]	Loss: 0.4608
Training Epoch: 40 [28672/50048]	Loss: 0.4577
Training Epoch: 40 [28800/50048]	Loss: 0.3946
Training Epoch: 40 [28928/50048]	Loss: 0.3706
Training Epoch: 40 [29056/50048]	Loss: 0.4634
Training Epoch: 40 [29184/50048]	Loss: 0.5254
Training Epoch: 40 [29312/50048]	Loss: 0.5257
Training Epoch: 40 [29440/50048]	Loss: 0.4196
Training Epoch: 40 [29568/50048]	Loss: 0.3420
Training Epoch: 40 [29696/50048]	Loss: 0.3327
Training Epoch: 40 [29824/50048]	Loss: 0.4765
Training Epoch: 40 [29952/50048]	Loss: 0.2987
Training Epoch: 40 [30080/50048]	Loss: 0.3466
Training Epoch: 40 [30208/50048]	Loss: 0.3565
Training Epoch: 40 [30336/50048]	Loss: 0.2757
Training Epoch: 40 [30464/50048]	Loss: 0.4496
Training Epoch: 40 [30592/50048]	Loss: 0.4654
Training Epoch: 40 [30720/50048]	Loss: 0.3994
Training Epoch: 40 [30848/50048]	Loss: 0.4360
Training Epoch: 40 [30976/50048]	Loss: 0.4022
Training Epoch: 40 [31104/50048]	Loss: 0.3228
Training Epoch: 40 [31232/50048]	Loss: 0.3986
Training Epoch: 40 [31360/50048]	Loss: 0.4026
Training Epoch: 40 [31488/50048]	Loss: 0.2596
Training Epoch: 40 [31616/50048]	Loss: 0.4034
Training Epoch: 40 [31744/50048]	Loss: 0.4676
Training Epoch: 40 [31872/50048]	Loss: 0.5700
Training Epoch: 40 [32000/50048]	Loss: 0.3088
Training Epoch: 40 [32128/50048]	Loss: 0.6986
Training Epoch: 40 [32256/50048]	Loss: 0.4343
Training Epoch: 40 [32384/50048]	Loss: 0.5633
Training Epoch: 40 [32512/50048]	Loss: 0.3312
Training Epoch: 40 [32640/50048]	Loss: 0.4218
Training Epoch: 40 [32768/50048]	Loss: 0.4859
Training Epoch: 40 [32896/50048]	Loss: 0.4342
Training Epoch: 40 [33024/50048]	Loss: 0.6020
Training Epoch: 40 [33152/50048]	Loss: 0.5878
Training Epoch: 40 [33280/50048]	Loss: 0.4332
Training Epoch: 40 [33408/50048]	Loss: 0.4255
Training Epoch: 40 [33536/50048]	Loss: 0.4605
Training Epoch: 40 [33664/50048]	Loss: 0.4248
Training Epoch: 40 [33792/50048]	Loss: 0.3205
Training Epoch: 40 [33920/50048]	Loss: 0.4822
Training Epoch: 40 [34048/50048]	Loss: 0.3736
Training Epoch: 40 [34176/50048]	Loss: 0.4105
Training Epoch: 40 [34304/50048]	Loss: 0.4546
Training Epoch: 40 [34432/50048]	Loss: 0.4658
Training Epoch: 40 [34560/50048]	Loss: 0.4343
Training Epoch: 40 [34688/50048]	Loss: 0.4436
Training Epoch: 40 [34816/50048]	Loss: 0.3393
Training Epoch: 40 [34944/50048]	Loss: 0.3616
Training Epoch: 40 [35072/50048]	Loss: 0.4639
Training Epoch: 40 [35200/50048]	Loss: 0.4767
Training Epoch: 40 [35328/50048]	Loss: 0.5304
Training Epoch: 40 [35456/50048]	Loss: 0.4595
Training Epoch: 40 [35584/50048]	Loss: 0.3957
Training Epoch: 40 [35712/50048]	Loss: 0.4255
Training Epoch: 40 [35840/50048]	Loss: 0.4601
Training Epoch: 40 [35968/50048]	Loss: 0.5249
Training Epoch: 40 [36096/50048]	Loss: 0.3240
Training Epoch: 40 [36224/50048]	Loss: 0.2665
Training Epoch: 40 [36352/50048]	Loss: 0.4284
Training Epoch: 40 [36480/50048]	Loss: 0.3247
Training Epoch: 40 [36608/50048]	Loss: 0.3129
Training Epoch: 40 [36736/50048]	Loss: 0.3227
Training Epoch: 40 [36864/50048]	Loss: 0.3331
Training Epoch: 40 [36992/50048]	Loss: 0.2711
Training Epoch: 40 [37120/50048]	Loss: 0.4284
Training Epoch: 40 [37248/50048]	Loss: 0.4052
Training Epoch: 40 [37376/50048]	Loss: 0.3045
Training Epoch: 40 [37504/50048]	Loss: 0.3853
Training Epoch: 40 [37632/50048]	Loss: 0.4060
Training Epoch: 40 [37760/50048]	Loss: 0.4386
Training Epoch: 40 [37888/50048]	Loss: 0.4424
Training Epoch: 40 [38016/50048]	Loss: 0.4847
Training Epoch: 40 [38144/50048]	Loss: 0.5279
Training Epoch: 40 [38272/50048]	Loss: 0.4067
Training Epoch: 40 [38400/50048]	Loss: 0.5977
Training Epoch: 40 [38528/50048]	Loss: 0.3373
Training Epoch: 40 [38656/50048]	Loss: 0.5192
Training Epoch: 40 [38784/50048]	Loss: 0.3829
Training Epoch: 40 [38912/50048]	Loss: 0.4965
Training Epoch: 40 [39040/50048]	Loss: 0.4200
Training Epoch: 40 [39168/50048]	Loss: 0.4526
Training Epoch: 40 [39296/50048]	Loss: 0.2276
Training Epoch: 40 [39424/50048]	Loss: 0.5661
Training Epoch: 40 [39552/50048]	Loss: 0.4522
Training Epoch: 40 [39680/50048]	Loss: 0.5627
Training Epoch: 40 [39808/50048]	Loss: 0.5508
Training Epoch: 40 [39936/50048]	Loss: 0.4511
Training Epoch: 40 [40064/50048]	Loss: 0.3387
Training Epoch: 40 [40192/50048]	Loss: 0.4052
Training Epoch: 40 [40320/50048]	Loss: 0.5607
Training Epoch: 40 [40448/50048]	Loss: 0.4345
Training Epoch: 40 [40576/50048]	Loss: 0.3070
Training Epoch: 40 [40704/50048]	Loss: 0.4465
Training Epoch: 40 [40832/50048]	Loss: 0.2888
Training Epoch: 40 [40960/50048]	Loss: 0.5204
Training Epoch: 40 [41088/50048]	Loss: 0.3309
Training Epoch: 40 [41216/50048]	Loss: 0.5423
Training Epoch: 40 [41344/50048]	Loss: 0.4943
Training Epoch: 40 [41472/50048]	Loss: 0.5240
Training Epoch: 40 [41600/50048]	Loss: 0.3395
Training Epoch: 40 [41728/50048]	Loss: 0.4490
Training Epoch: 40 [41856/50048]	Loss: 0.4473
Training Epoch: 40 [41984/50048]	Loss: 0.3552
Training Epoch: 40 [42112/50048]	Loss: 0.5025
Training Epoch: 40 [42240/50048]	Loss: 0.4099
Training Epoch: 40 [42368/50048]	Loss: 0.4201
Training Epoch: 40 [42496/50048]	Loss: 0.5545
Training Epoch: 40 [42624/50048]	Loss: 0.5605
Training Epoch: 40 [42752/50048]	Loss: 0.5530
Training Epoch: 40 [42880/50048]	Loss: 0.4642
Training Epoch: 40 [43008/50048]	Loss: 0.4136
Training Epoch: 40 [43136/50048]	Loss: 0.3750
Training Epoch: 40 [43264/50048]	Loss: 0.5581
Training Epoch: 40 [43392/50048]	Loss: 0.4347
Training Epoch: 40 [43520/50048]	Loss: 0.4522
Training Epoch: 40 [43648/50048]	Loss: 0.5457
Training Epoch: 40 [43776/50048]	Loss: 0.3695
Training Epoch: 40 [43904/50048]	Loss: 0.4744
Training Epoch: 40 [44032/50048]	Loss: 0.5034
Training Epoch: 40 [44160/50048]	Loss: 0.5100
Training Epoch: 40 [44288/50048]	Loss: 0.3794
Training Epoch: 40 [44416/50048]	Loss: 0.5445
Training Epoch: 40 [44544/50048]	Loss: 0.4043
Training Epoch: 40 [44672/50048]	Loss: 0.4090
Training Epoch: 40 [44800/50048]	Loss: 0.5315
Training Epoch: 40 [44928/50048]	Loss: 0.4320
Training Epoch: 40 [45056/50048]	Loss: 0.5072
Training Epoch: 40 [45184/50048]	Loss: 0.5742
Training Epoch: 40 [45312/50048]	Loss: 0.4329
Training Epoch: 40 [45440/50048]	Loss: 0.4340
Training Epoch: 40 [45568/50048]	Loss: 0.4023
Training Epoch: 40 [45696/50048]	Loss: 0.4634
2022-12-06 04:38:59,802 [ZeusDataLoader(train)] train epoch 41 done: time=86.40 energy=10494.67
2022-12-06 04:38:59,804 [ZeusDataLoader(eval)] Epoch 41 begin.
Training Epoch: 40 [45824/50048]	Loss: 0.4582
Training Epoch: 40 [45952/50048]	Loss: 0.4691
Training Epoch: 40 [46080/50048]	Loss: 0.6002
Training Epoch: 40 [46208/50048]	Loss: 0.4340
Training Epoch: 40 [46336/50048]	Loss: 0.4656
Training Epoch: 40 [46464/50048]	Loss: 0.4248
Training Epoch: 40 [46592/50048]	Loss: 0.4877
Training Epoch: 40 [46720/50048]	Loss: 0.4079
Training Epoch: 40 [46848/50048]	Loss: 0.3725
Training Epoch: 40 [46976/50048]	Loss: 0.3976
Training Epoch: 40 [47104/50048]	Loss: 0.4405
Training Epoch: 40 [47232/50048]	Loss: 0.3666
Training Epoch: 40 [47360/50048]	Loss: 0.5081
Training Epoch: 40 [47488/50048]	Loss: 0.3804
Training Epoch: 40 [47616/50048]	Loss: 0.4522
Training Epoch: 40 [47744/50048]	Loss: 0.5603
Training Epoch: 40 [47872/50048]	Loss: 0.4633
Training Epoch: 40 [48000/50048]	Loss: 0.4146
Training Epoch: 40 [48128/50048]	Loss: 0.4719
Training Epoch: 40 [48256/50048]	Loss: 0.4487
Training Epoch: 40 [48384/50048]	Loss: 0.4822
Training Epoch: 40 [48512/50048]	Loss: 0.3315
Training Epoch: 40 [48640/50048]	Loss: 0.3543
Training Epoch: 40 [48768/50048]	Loss: 0.4567
Training Epoch: 40 [48896/50048]	Loss: 0.4173
Training Epoch: 40 [49024/50048]	Loss: 0.5257
Training Epoch: 40 [49152/50048]	Loss: 0.5229
Training Epoch: 40 [49280/50048]	Loss: 0.4747
Training Epoch: 40 [49408/50048]	Loss: 0.4310
Training Epoch: 40 [49536/50048]	Loss: 0.4783
Training Epoch: 40 [49664/50048]	Loss: 0.4928
Training Epoch: 40 [49792/50048]	Loss: 0.4527
Training Epoch: 40 [49920/50048]	Loss: 0.5373
Training Epoch: 40 [50048/50048]	Loss: 0.6560
2022-12-06 09:39:03.495 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:39:03,528 [ZeusDataLoader(eval)] eval epoch 41 done: time=3.72 energy=457.37
2022-12-06 04:39:03,528 [ZeusDataLoader(train)] Up to epoch 41: time=3700.41, energy=449131.53, cost=548351.57
2022-12-06 04:39:03,528 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:39:03,528 [ZeusDataLoader(train)] Expected next epoch: time=3790.21, energy=459929.55, cost=561607.95
2022-12-06 04:39:03,529 [ZeusDataLoader(train)] Epoch 42 begin.
Validation Epoch: 40, Average loss: 0.0136, Accuracy: 0.6295
2022-12-06 04:39:03,703 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:39:03,704 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:39:03.706 [ZeusMonitor] Monitor started.
2022-12-06 09:39:03.706 [ZeusMonitor] Running indefinitely. 2022-12-06 09:39:03.706 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:39:03.706 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e42+gpu0.power.log
Training Epoch: 41 [128/50048]	Loss: 0.3535
Training Epoch: 41 [256/50048]	Loss: 0.1953
Training Epoch: 41 [384/50048]	Loss: 0.3567
Training Epoch: 41 [512/50048]	Loss: 0.2830
Training Epoch: 41 [640/50048]	Loss: 0.4006
Training Epoch: 41 [768/50048]	Loss: 0.2246
Training Epoch: 41 [896/50048]	Loss: 0.3348
Training Epoch: 41 [1024/50048]	Loss: 0.2840
Training Epoch: 41 [1152/50048]	Loss: 0.3346
Training Epoch: 41 [1280/50048]	Loss: 0.3464
Training Epoch: 41 [1408/50048]	Loss: 0.4101
Training Epoch: 41 [1536/50048]	Loss: 0.3379
Training Epoch: 41 [1664/50048]	Loss: 0.3635
Training Epoch: 41 [1792/50048]	Loss: 0.3492
Training Epoch: 41 [1920/50048]	Loss: 0.3871
Training Epoch: 41 [2048/50048]	Loss: 0.3638
Training Epoch: 41 [2176/50048]	Loss: 0.4097
Training Epoch: 41 [2304/50048]	Loss: 0.4307
Training Epoch: 41 [2432/50048]	Loss: 0.3243
Training Epoch: 41 [2560/50048]	Loss: 0.2954
Training Epoch: 41 [2688/50048]	Loss: 0.2501
Training Epoch: 41 [2816/50048]	Loss: 0.3022
Training Epoch: 41 [2944/50048]	Loss: 0.2174
Training Epoch: 41 [3072/50048]	Loss: 0.3039
Training Epoch: 41 [3200/50048]	Loss: 0.3138
Training Epoch: 41 [3328/50048]	Loss: 0.2971
Training Epoch: 41 [3456/50048]	Loss: 0.4384
Training Epoch: 41 [3584/50048]	Loss: 0.3242
Training Epoch: 41 [3712/50048]	Loss: 0.5009
Training Epoch: 41 [3840/50048]	Loss: 0.2386
Training Epoch: 41 [3968/50048]	Loss: 0.3952
Training Epoch: 41 [4096/50048]	Loss: 0.3183
Training Epoch: 41 [4224/50048]	Loss: 0.3386
Training Epoch: 41 [4352/50048]	Loss: 0.2682
Training Epoch: 41 [4480/50048]	Loss: 0.3840
Training Epoch: 41 [4608/50048]	Loss: 0.3288
Training Epoch: 41 [4736/50048]	Loss: 0.3525
Training Epoch: 41 [4864/50048]	Loss: 0.3352
Training Epoch: 41 [4992/50048]	Loss: 0.3020
Training Epoch: 41 [5120/50048]	Loss: 0.2888
Training Epoch: 41 [5248/50048]	Loss: 0.3808
Training Epoch: 41 [5376/50048]	Loss: 0.4199
Training Epoch: 41 [5504/50048]	Loss: 0.4269
Training Epoch: 41 [5632/50048]	Loss: 0.3497
Training Epoch: 41 [5760/50048]	Loss: 0.3765
Training Epoch: 41 [5888/50048]	Loss: 0.3937
Training Epoch: 41 [6016/50048]	Loss: 0.3257
Training Epoch: 41 [6144/50048]	Loss: 0.4157
Training Epoch: 41 [6272/50048]	Loss: 0.3417
Training Epoch: 41 [6400/50048]	Loss: 0.3708
Training Epoch: 41 [6528/50048]	Loss: 0.4065
Training Epoch: 41 [6656/50048]	Loss: 0.3659
Training Epoch: 41 [6784/50048]	Loss: 0.3244
Training Epoch: 41 [6912/50048]	Loss: 0.3786
Training Epoch: 41 [7040/50048]	Loss: 0.3310
Training Epoch: 41 [7168/50048]	Loss: 0.3314
Training Epoch: 41 [7296/50048]	Loss: 0.4770
Training Epoch: 41 [7424/50048]	Loss: 0.3565
Training Epoch: 41 [7552/50048]	Loss: 0.3362
Training Epoch: 41 [7680/50048]	Loss: 0.3456
Training Epoch: 41 [7808/50048]	Loss: 0.4230
Training Epoch: 41 [7936/50048]	Loss: 0.3992
Training Epoch: 41 [8064/50048]	Loss: 0.3787
Training Epoch: 41 [8192/50048]	Loss: 0.4035
Training Epoch: 41 [8320/50048]	Loss: 0.3622
Training Epoch: 41 [8448/50048]	Loss: 0.3921
Training Epoch: 41 [8576/50048]	Loss: 0.4289
Training Epoch: 41 [8704/50048]	Loss: 0.4751
Training Epoch: 41 [8832/50048]	Loss: 0.4613
Training Epoch: 41 [8960/50048]	Loss: 0.3514
Training Epoch: 41 [9088/50048]	Loss: 0.2839
Training Epoch: 41 [9216/50048]	Loss: 0.5224
Training Epoch: 41 [9344/50048]	Loss: 0.4241
Training Epoch: 41 [9472/50048]	Loss: 0.4412
Training Epoch: 41 [9600/50048]	Loss: 0.4155
Training Epoch: 41 [9728/50048]	Loss: 0.3549
Training Epoch: 41 [9856/50048]	Loss: 0.3074
Training Epoch: 41 [9984/50048]	Loss: 0.4419
Training Epoch: 41 [10112/50048]	Loss: 0.3338
Training Epoch: 41 [10240/50048]	Loss: 0.2404
Training Epoch: 41 [10368/50048]	Loss: 0.4555
Training Epoch: 41 [10496/50048]	Loss: 0.3897
Training Epoch: 41 [10624/50048]	Loss: 0.4758
Training Epoch: 41 [10752/50048]	Loss: 0.3795
Training Epoch: 41 [10880/50048]	Loss: 0.3912
Training Epoch: 41 [11008/50048]	Loss: 0.3732
Training Epoch: 41 [11136/50048]	Loss: 0.3875
Training Epoch: 41 [11264/50048]	Loss: 0.4093
Training Epoch: 41 [11392/50048]	Loss: 0.3209
Training Epoch: 41 [11520/50048]	Loss: 0.2612
Training Epoch: 41 [11648/50048]	Loss: 0.4751
Training Epoch: 41 [11776/50048]	Loss: 0.3876
Training Epoch: 41 [11904/50048]	Loss: 0.4834
Training Epoch: 41 [12032/50048]	Loss: 0.4039
Training Epoch: 41 [12160/50048]	Loss: 0.3896
Training Epoch: 41 [12288/50048]	Loss: 0.3700
Training Epoch: 41 [12416/50048]	Loss: 0.4909
Training Epoch: 41 [12544/50048]	Loss: 0.3830
Training Epoch: 41 [12672/50048]	Loss: 0.3554
Training Epoch: 41 [12800/50048]	Loss: 0.3784
Training Epoch: 41 [12928/50048]	Loss: 0.4158
Training Epoch: 41 [13056/50048]	Loss: 0.3306
Training Epoch: 41 [13184/50048]	Loss: 0.3482
Training Epoch: 41 [13312/50048]	Loss: 0.3805
Training Epoch: 41 [13440/50048]	Loss: 0.4259
Training Epoch: 41 [13568/50048]	Loss: 0.4061
Training Epoch: 41 [13696/50048]	Loss: 0.3892
Training Epoch: 41 [13824/50048]	Loss: 0.3696
Training Epoch: 41 [13952/50048]	Loss: 0.4182
Training Epoch: 41 [14080/50048]	Loss: 0.4307
Training Epoch: 41 [14208/50048]	Loss: 0.5488
Training Epoch: 41 [14336/50048]	Loss: 0.3688
Training Epoch: 41 [14464/50048]	Loss: 0.2792
Training Epoch: 41 [14592/50048]	Loss: 0.4054
Training Epoch: 41 [14720/50048]	Loss: 0.3334
Training Epoch: 41 [14848/50048]	Loss: 0.3350
Training Epoch: 41 [14976/50048]	Loss: 0.3765
Training Epoch: 41 [15104/50048]	Loss: 0.4081
Training Epoch: 41 [15232/50048]	Loss: 0.3781
Training Epoch: 41 [15360/50048]	Loss: 0.4057
Training Epoch: 41 [15488/50048]	Loss: 0.4135
Training Epoch: 41 [15616/50048]	Loss: 0.4080
Training Epoch: 41 [15744/50048]	Loss: 0.4109
Training Epoch: 41 [15872/50048]	Loss: 0.4278
Training Epoch: 41 [16000/50048]	Loss: 0.3288
Training Epoch: 41 [16128/50048]	Loss: 0.4776
Training Epoch: 41 [16256/50048]	Loss: 0.4247
Training Epoch: 41 [16384/50048]	Loss: 0.3129
Training Epoch: 41 [16512/50048]	Loss: 0.5534
Training Epoch: 41 [16640/50048]	Loss: 0.3645
Training Epoch: 41 [16768/50048]	Loss: 0.3954
Training Epoch: 41 [16896/50048]	Loss: 0.3131
Training Epoch: 41 [17024/50048]	Loss: 0.5013
Training Epoch: 41 [17152/50048]	Loss: 0.4290
Training Epoch: 41 [17280/50048]	Loss: 0.3587
Training Epoch: 41 [17408/50048]	Loss: 0.4639
Training Epoch: 41 [17536/50048]	Loss: 0.4630
Training Epoch: 41 [17664/50048]	Loss: 0.4477
Training Epoch: 41 [17792/50048]	Loss: 0.4927
Training Epoch: 41 [17920/50048]	Loss: 0.4706
Training Epoch: 41 [18048/50048]	Loss: 0.2636
Training Epoch: 41 [18176/50048]	Loss: 0.3529
Training Epoch: 41 [18304/50048]	Loss: 0.4159
Training Epoch: 41 [18432/50048]	Loss: 0.3687
Training Epoch: 41 [18560/50048]	Loss: 0.3536
Training Epoch: 41 [18688/50048]	Loss: 0.2067
Training Epoch: 41 [18816/50048]	Loss: 0.3434
Training Epoch: 41 [18944/50048]	Loss: 0.4975
Training Epoch: 41 [19072/50048]	Loss: 0.3499
Training Epoch: 41 [19200/50048]	Loss: 0.4769
Training Epoch: 41 [19328/50048]	Loss: 0.3967
Training Epoch: 41 [19456/50048]	Loss: 0.2817
Training Epoch: 41 [19584/50048]	Loss: 0.3771
Training Epoch: 41 [19712/50048]	Loss: 0.3773
Training Epoch: 41 [19840/50048]	Loss: 0.3011
Training Epoch: 41 [19968/50048]	Loss: 0.4082
Training Epoch: 41 [20096/50048]	Loss: 0.3300
Training Epoch: 41 [20224/50048]	Loss: 0.5436
Training Epoch: 41 [20352/50048]	Loss: 0.3914
Training Epoch: 41 [20480/50048]	Loss: 0.3576
Training Epoch: 41 [20608/50048]	Loss: 0.5149
Training Epoch: 41 [20736/50048]	Loss: 0.4065
Training Epoch: 41 [20864/50048]	Loss: 0.5163
Training Epoch: 41 [20992/50048]	Loss: 0.3019
Training Epoch: 41 [21120/50048]	Loss: 0.3676
Training Epoch: 41 [21248/50048]	Loss: 0.3099
Training Epoch: 41 [21376/50048]	Loss: 0.3592
Training Epoch: 41 [21504/50048]	Loss: 0.5550
Training Epoch: 41 [21632/50048]	Loss: 0.4258
Training Epoch: 41 [21760/50048]	Loss: 0.3587
Training Epoch: 41 [21888/50048]	Loss: 0.2867
Training Epoch: 41 [22016/50048]	Loss: 0.2612
Training Epoch: 41 [22144/50048]	Loss: 0.4350
Training Epoch: 41 [22272/50048]	Loss: 0.4885
Training Epoch: 41 [22400/50048]	Loss: 0.3873
Training Epoch: 41 [22528/50048]	Loss: 0.4210
Training Epoch: 41 [22656/50048]	Loss: 0.3325
Training Epoch: 41 [22784/50048]	Loss: 0.3297
Training Epoch: 41 [22912/50048]	Loss: 0.5770
Training Epoch: 41 [23040/50048]	Loss: 0.3474
Training Epoch: 41 [23168/50048]	Loss: 0.3507
Training Epoch: 41 [23296/50048]	Loss: 0.4557
Training Epoch: 41 [23424/50048]	Loss: 0.3717
Training Epoch: 41 [23552/50048]	Loss: 0.4562
Training Epoch: 41 [23680/50048]	Loss: 0.5119
Training Epoch: 41 [23808/50048]	Loss: 0.4227
Training Epoch: 41 [23936/50048]	Loss: 0.3770
Training Epoch: 41 [24064/50048]	Loss: 0.5016
Training Epoch: 41 [24192/50048]	Loss: 0.5150
Training Epoch: 41 [24320/50048]	Loss: 0.4489
Training Epoch: 41 [24448/50048]	Loss: 0.4073
Training Epoch: 41 [24576/50048]	Loss: 0.3426
Training Epoch: 41 [24704/50048]	Loss: 0.4118
Training Epoch: 41 [24832/50048]	Loss: 0.4210
Training Epoch: 41 [24960/50048]	Loss: 0.4037
Training Epoch: 41 [25088/50048]	Loss: 0.3373
Training Epoch: 41 [25216/50048]	Loss: 0.2661
Training Epoch: 41 [25344/50048]	Loss: 0.3270
Training Epoch: 41 [25472/50048]	Loss: 0.4038
Training Epoch: 41 [25600/50048]	Loss: 0.3304
Training Epoch: 41 [25728/50048]	Loss: 0.4124
Training Epoch: 41 [25856/50048]	Loss: 0.3433
Training Epoch: 41 [25984/50048]	Loss: 0.2247
Training Epoch: 41 [26112/50048]	Loss: 0.4612
Training Epoch: 41 [26240/50048]	Loss: 0.3044
Training Epoch: 41 [26368/50048]	Loss: 0.4418
Training Epoch: 41 [26496/50048]	Loss: 0.4446
Training Epoch: 41 [26624/50048]	Loss: 0.5070
Training Epoch: 41 [26752/50048]	Loss: 0.4387
Training Epoch: 41 [26880/50048]	Loss: 0.3201
Training Epoch: 41 [27008/50048]	Loss: 0.3287
Training Epoch: 41 [27136/50048]	Loss: 0.4526
Training Epoch: 41 [27264/50048]	Loss: 0.3786
Training Epoch: 41 [27392/50048]	Loss: 0.4175
Training Epoch: 41 [27520/50048]	Loss: 0.3981
Training Epoch: 41 [27648/50048]	Loss: 0.4927
Training Epoch: 41 [27776/50048]	Loss: 0.3388
Training Epoch: 41 [27904/50048]	Loss: 0.4931
Training Epoch: 41 [28032/50048]	Loss: 0.4516
Training Epoch: 41 [28160/50048]	Loss: 0.5026
Training Epoch: 41 [28288/50048]	Loss: 0.3210
Training Epoch: 41 [28416/50048]	Loss: 0.4676
Training Epoch: 41 [28544/50048]	Loss: 0.4174
Training Epoch: 41 [28672/50048]	Loss: 0.4132
Training Epoch: 41 [28800/50048]	Loss: 0.4189
Training Epoch: 41 [28928/50048]	Loss: 0.4558
Training Epoch: 41 [29056/50048]	Loss: 0.3538
Training Epoch: 41 [29184/50048]	Loss: 0.4902
Training Epoch: 41 [29312/50048]	Loss: 0.2970
Training Epoch: 41 [29440/50048]	Loss: 0.4060
Training Epoch: 41 [29568/50048]	Loss: 0.2946
Training Epoch: 41 [29696/50048]	Loss: 0.4717
Training Epoch: 41 [29824/50048]	Loss: 0.2744
Training Epoch: 41 [29952/50048]	Loss: 0.4357
Training Epoch: 41 [30080/50048]	Loss: 0.3487
Training Epoch: 41 [30208/50048]	Loss: 0.3471
Training Epoch: 41 [30336/50048]	Loss: 0.4821
Training Epoch: 41 [30464/50048]	Loss: 0.3801
Training Epoch: 41 [30592/50048]	Loss: 0.4194
Training Epoch: 41 [30720/50048]	Loss: 0.4428
Training Epoch: 41 [30848/50048]	Loss: 0.4500
Training Epoch: 41 [30976/50048]	Loss: 0.4395
Training Epoch: 41 [31104/50048]	Loss: 0.3477
Training Epoch: 41 [31232/50048]	Loss: 0.3138
Training Epoch: 41 [31360/50048]	Loss: 0.5300
Training Epoch: 41 [31488/50048]	Loss: 0.3555
Training Epoch: 41 [31616/50048]	Loss: 0.4153
Training Epoch: 41 [31744/50048]	Loss: 0.5209
Training Epoch: 41 [31872/50048]	Loss: 0.3489
Training Epoch: 41 [32000/50048]	Loss: 0.3152
Training Epoch: 41 [32128/50048]	Loss: 0.3165
Training Epoch: 41 [32256/50048]	Loss: 0.3055
Training Epoch: 41 [32384/50048]	Loss: 0.2643
Training Epoch: 41 [32512/50048]	Loss: 0.3795
Training Epoch: 41 [32640/50048]	Loss: 0.4973
Training Epoch: 41 [32768/50048]	Loss: 0.3688
Training Epoch: 41 [32896/50048]	Loss: 0.5312
Training Epoch: 41 [33024/50048]	Loss: 0.3860
Training Epoch: 41 [33152/50048]	Loss: 0.2553
Training Epoch: 41 [33280/50048]	Loss: 0.3983
Training Epoch: 41 [33408/50048]	Loss: 0.3622
Training Epoch: 41 [33536/50048]	Loss: 0.2737
Training Epoch: 41 [33664/50048]	Loss: 0.4147
Training Epoch: 41 [33792/50048]	Loss: 0.4157
Training Epoch: 41 [33920/50048]	Loss: 0.3279
Training Epoch: 41 [34048/50048]	Loss: 0.3042
Training Epoch: 41 [34176/50048]	Loss: 0.4624
Training Epoch: 41 [34304/50048]	Loss: 0.5564
Training Epoch: 41 [34432/50048]	Loss: 0.4659
Training Epoch: 41 [34560/50048]	Loss: 0.2951
Training Epoch: 41 [34688/50048]	Loss: 0.3528
Training Epoch: 41 [34816/50048]	Loss: 0.3474
Training Epoch: 41 [34944/50048]	Loss: 0.3417
Training Epoch: 41 [35072/50048]	Loss: 0.4977
Training Epoch: 41 [35200/50048]	Loss: 0.3555
Training Epoch: 41 [35328/50048]	Loss: 0.6011
Training Epoch: 41 [35456/50048]	Loss: 0.3586
Training Epoch: 41 [35584/50048]	Loss: 0.5272
Training Epoch: 41 [35712/50048]	Loss: 0.4608
Training Epoch: 41 [35840/50048]	Loss: 0.5060
Training Epoch: 41 [35968/50048]	Loss: 0.5733
Training Epoch: 41 [36096/50048]	Loss: 0.3373
Training Epoch: 41 [36224/50048]	Loss: 0.4734
Training Epoch: 41 [36352/50048]	Loss: 0.4118
Training Epoch: 41 [36480/50048]	Loss: 0.3952
Training Epoch: 41 [36608/50048]	Loss: 0.4919
Training Epoch: 41 [36736/50048]	Loss: 0.4711
Training Epoch: 41 [36864/50048]	Loss: 0.3853
Training Epoch: 41 [36992/50048]	Loss: 0.3189
Training Epoch: 41 [37120/50048]	Loss: 0.4130
Training Epoch: 41 [37248/50048]	Loss: 0.3155
Training Epoch: 41 [37376/50048]	Loss: 0.3531
Training Epoch: 41 [37504/50048]	Loss: 0.4571
Training Epoch: 41 [37632/50048]	Loss: 0.4184
Training Epoch: 41 [37760/50048]	Loss: 0.4217
Training Epoch: 41 [37888/50048]	Loss: 0.3846
Training Epoch: 41 [38016/50048]	Loss: 0.3466
Training Epoch: 41 [38144/50048]	Loss: 0.2857
Training Epoch: 41 [38272/50048]	Loss: 0.4097
Training Epoch: 41 [38400/50048]	Loss: 0.3454
Training Epoch: 41 [38528/50048]	Loss: 0.4343
Training Epoch: 41 [38656/50048]	Loss: 0.4325
Training Epoch: 41 [38784/50048]	Loss: 0.4980
Training Epoch: 41 [38912/50048]	Loss: 0.5409
Training Epoch: 41 [39040/50048]	Loss: 0.2880
Training Epoch: 41 [39168/50048]	Loss: 0.4681
Training Epoch: 41 [39296/50048]	Loss: 0.4065
Training Epoch: 41 [39424/50048]	Loss: 0.4907
Training Epoch: 41 [39552/50048]	Loss: 0.4089
Training Epoch: 41 [39680/50048]	Loss: 0.3656
Training Epoch: 41 [39808/50048]	Loss: 0.4426
Training Epoch: 41 [39936/50048]	Loss: 0.4699
Training Epoch: 41 [40064/50048]	Loss: 0.3724
Training Epoch: 41 [40192/50048]	Loss: 0.4130
Training Epoch: 41 [40320/50048]	Loss: 0.3955
Training Epoch: 41 [40448/50048]	Loss: 0.4745
Training Epoch: 41 [40576/50048]	Loss: 0.6561
Training Epoch: 41 [40704/50048]	Loss: 0.4173
Training Epoch: 41 [40832/50048]	Loss: 0.5676
Training Epoch: 41 [40960/50048]	Loss: 0.4114
Training Epoch: 41 [41088/50048]	Loss: 0.4526
Training Epoch: 41 [41216/50048]	Loss: 0.3844
Training Epoch: 41 [41344/50048]	Loss: 0.4455
Training Epoch: 41 [41472/50048]	Loss: 0.3223
Training Epoch: 41 [41600/50048]	Loss: 0.2566
Training Epoch: 41 [41728/50048]	Loss: 0.2784
Training Epoch: 41 [41856/50048]	Loss: 0.6042
Training Epoch: 41 [41984/50048]	Loss: 0.4482
Training Epoch: 41 [42112/50048]	Loss: 0.3424
Training Epoch: 41 [42240/50048]	Loss: 0.4100
Training Epoch: 41 [42368/50048]	Loss: 0.5512
Training Epoch: 41 [42496/50048]	Loss: 0.4127
Training Epoch: 41 [42624/50048]	Loss: 0.3315
Training Epoch: 41 [42752/50048]	Loss: 0.3620
Training Epoch: 41 [42880/50048]	Loss: 0.4364
Training Epoch: 41 [43008/50048]	Loss: 0.4580
Training Epoch: 41 [43136/50048]	Loss: 0.4963
Training Epoch: 41 [43264/50048]	Loss: 0.4447
Training Epoch: 41 [43392/50048]	Loss: 0.3783
Training Epoch: 41 [43520/50048]	Loss: 0.3825
Training Epoch: 41 [43648/50048]	Loss: 0.4658
Training Epoch: 41 [43776/50048]	Loss: 0.3713
Training Epoch: 41 [43904/50048]	Loss: 0.4088
Training Epoch: 41 [44032/50048]	Loss: 0.4381
Training Epoch: 41 [44160/50048]	Loss: 0.4672
Training Epoch: 41 [44288/50048]	Loss: 0.5236
Training Epoch: 41 [44416/50048]	Loss: 0.3350
Training Epoch: 41 [44544/50048]	Loss: 0.3964
Training Epoch: 41 [44672/50048]	Loss: 0.4131
Training Epoch: 41 [44800/50048]	Loss: 0.5736
Training Epoch: 41 [44928/50048]	Loss: 0.3797
Training Epoch: 41 [45056/50048]	Loss: 0.4126
Training Epoch: 41 [45184/50048]	Loss: 0.4221
Training Epoch: 41 [45312/50048]	Loss: 0.5064
Training Epoch: 41 [45440/50048]	Loss: 0.3709
Training Epoch: 41 [45568/50048]	Loss: 0.4108
Training Epoch: 41 [45696/50048]	Loss: 0.4386
2022-12-06 04:40:29,953 [ZeusDataLoader(train)] train epoch 42 done: time=86.41 energy=10497.49
2022-12-06 04:40:29,955 [ZeusDataLoader(eval)] Epoch 42 begin.
Training Epoch: 41 [45824/50048]	Loss: 0.5055
Training Epoch: 41 [45952/50048]	Loss: 0.3575
Training Epoch: 41 [46080/50048]	Loss: 0.3531
Training Epoch: 41 [46208/50048]	Loss: 0.3936
Training Epoch: 41 [46336/50048]	Loss: 0.2791
Training Epoch: 41 [46464/50048]	Loss: 0.4288
Training Epoch: 41 [46592/50048]	Loss: 0.5706
Training Epoch: 41 [46720/50048]	Loss: 0.5154
Training Epoch: 41 [46848/50048]	Loss: 0.3882
Training Epoch: 41 [46976/50048]	Loss: 0.5236
Training Epoch: 41 [47104/50048]	Loss: 0.4493
Training Epoch: 41 [47232/50048]	Loss: 0.4673
Training Epoch: 41 [47360/50048]	Loss: 0.5825
Training Epoch: 41 [47488/50048]	Loss: 0.4401
Training Epoch: 41 [47616/50048]	Loss: 0.4626
Training Epoch: 41 [47744/50048]	Loss: 0.4715
Training Epoch: 41 [47872/50048]	Loss: 0.5137
Training Epoch: 41 [48000/50048]	Loss: 0.4566
Training Epoch: 41 [48128/50048]	Loss: 0.4488
Training Epoch: 41 [48256/50048]	Loss: 0.4381
Training Epoch: 41 [48384/50048]	Loss: 0.2303
Training Epoch: 41 [48512/50048]	Loss: 0.4627
Training Epoch: 41 [48640/50048]	Loss: 0.4827
Training Epoch: 41 [48768/50048]	Loss: 0.4039
Training Epoch: 41 [48896/50048]	Loss: 0.3794
Training Epoch: 41 [49024/50048]	Loss: 0.4150
Training Epoch: 41 [49152/50048]	Loss: 0.3827
Training Epoch: 41 [49280/50048]	Loss: 0.5356
Training Epoch: 41 [49408/50048]	Loss: 0.4792
Training Epoch: 41 [49536/50048]	Loss: 0.4394
Training Epoch: 41 [49664/50048]	Loss: 0.5773
Training Epoch: 41 [49792/50048]	Loss: 0.4460
Training Epoch: 41 [49920/50048]	Loss: 0.5410
Training Epoch: 41 [50048/50048]	Loss: 0.2791
2022-12-06 09:40:33.641 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:40:33,673 [ZeusDataLoader(eval)] eval epoch 42 done: time=3.71 energy=455.62
2022-12-06 04:40:33,673 [ZeusDataLoader(train)] Up to epoch 42: time=3790.53, energy=460084.65, cost=561713.90
2022-12-06 04:40:33,673 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:40:33,673 [ZeusDataLoader(train)] Expected next epoch: time=3880.33, energy=470882.66, cost=574970.28
2022-12-06 04:40:33,674 [ZeusDataLoader(train)] Epoch 43 begin.
Validation Epoch: 41, Average loss: 0.0138, Accuracy: 0.6259
2022-12-06 04:40:33,856 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:40:33,856 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:40:33.858 [ZeusMonitor] Monitor started.
2022-12-06 09:40:33.858 [ZeusMonitor] Running indefinitely. 2022-12-06 09:40:33.858 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:40:33.858 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e43+gpu0.power.log
Training Epoch: 42 [128/50048]	Loss: 0.3346
Training Epoch: 42 [256/50048]	Loss: 0.3423
Training Epoch: 42 [384/50048]	Loss: 0.2258
Training Epoch: 42 [512/50048]	Loss: 0.3582
Training Epoch: 42 [640/50048]	Loss: 0.2308
Training Epoch: 42 [768/50048]	Loss: 0.2883
Training Epoch: 42 [896/50048]	Loss: 0.3339
Training Epoch: 42 [1024/50048]	Loss: 0.2357
Training Epoch: 42 [1152/50048]	Loss: 0.2508
Training Epoch: 42 [1280/50048]	Loss: 0.2984
Training Epoch: 42 [1408/50048]	Loss: 0.3151
Training Epoch: 42 [1536/50048]	Loss: 0.4356
Training Epoch: 42 [1664/50048]	Loss: 0.2303
Training Epoch: 42 [1792/50048]	Loss: 0.4064
Training Epoch: 42 [1920/50048]	Loss: 0.2432
Training Epoch: 42 [2048/50048]	Loss: 0.3007
Training Epoch: 42 [2176/50048]	Loss: 0.3980
Training Epoch: 42 [2304/50048]	Loss: 0.3156
Training Epoch: 42 [2432/50048]	Loss: 0.2602
Training Epoch: 42 [2560/50048]	Loss: 0.2928
Training Epoch: 42 [2688/50048]	Loss: 0.3296
Training Epoch: 42 [2816/50048]	Loss: 0.3420
Training Epoch: 42 [2944/50048]	Loss: 0.3021
Training Epoch: 42 [3072/50048]	Loss: 0.2772
Training Epoch: 42 [3200/50048]	Loss: 0.3744
Training Epoch: 42 [3328/50048]	Loss: 0.3996
Training Epoch: 42 [3456/50048]	Loss: 0.5129
Training Epoch: 42 [3584/50048]	Loss: 0.3142
Training Epoch: 42 [3712/50048]	Loss: 0.2993
Training Epoch: 42 [3840/50048]	Loss: 0.3411
Training Epoch: 42 [3968/50048]	Loss: 0.3829
Training Epoch: 42 [4096/50048]	Loss: 0.3550
Training Epoch: 42 [4224/50048]	Loss: 0.4914
Training Epoch: 42 [4352/50048]	Loss: 0.3942
Training Epoch: 42 [4480/50048]	Loss: 0.4611
Training Epoch: 42 [4608/50048]	Loss: 0.4255
Training Epoch: 42 [4736/50048]	Loss: 0.4247
Training Epoch: 42 [4864/50048]	Loss: 0.2814
Training Epoch: 42 [4992/50048]	Loss: 0.3295
Training Epoch: 42 [5120/50048]	Loss: 0.3102
Training Epoch: 42 [5248/50048]	Loss: 0.3976
Training Epoch: 42 [5376/50048]	Loss: 0.2774
Training Epoch: 42 [5504/50048]	Loss: 0.4733
Training Epoch: 42 [5632/50048]	Loss: 0.4073
Training Epoch: 42 [5760/50048]	Loss: 0.4049
Training Epoch: 42 [5888/50048]	Loss: 0.3430
Training Epoch: 42 [6016/50048]	Loss: 0.4501
Training Epoch: 42 [6144/50048]	Loss: 0.3253
Training Epoch: 42 [6272/50048]	Loss: 0.2352
Training Epoch: 42 [6400/50048]	Loss: 0.4830
Training Epoch: 42 [6528/50048]	Loss: 0.3121
Training Epoch: 42 [6656/50048]	Loss: 0.2386
Training Epoch: 42 [6784/50048]	Loss: 0.3440
Training Epoch: 42 [6912/50048]	Loss: 0.2510
Training Epoch: 42 [7040/50048]	Loss: 0.3763
Training Epoch: 42 [7168/50048]	Loss: 0.2584
Training Epoch: 42 [7296/50048]	Loss: 0.4484
Training Epoch: 42 [7424/50048]	Loss: 0.3361
Training Epoch: 42 [7552/50048]	Loss: 0.3098
Training Epoch: 42 [7680/50048]	Loss: 0.3105
Training Epoch: 42 [7808/50048]	Loss: 0.3072
Training Epoch: 42 [7936/50048]	Loss: 0.2783
Training Epoch: 42 [8064/50048]	Loss: 0.2888
Training Epoch: 42 [8192/50048]	Loss: 0.3390
Training Epoch: 42 [8320/50048]	Loss: 0.3111
Training Epoch: 42 [8448/50048]	Loss: 0.2750
Training Epoch: 42 [8576/50048]	Loss: 0.3298
Training Epoch: 42 [8704/50048]	Loss: 0.3992
Training Epoch: 42 [8832/50048]	Loss: 0.3443
Training Epoch: 42 [8960/50048]	Loss: 0.3711
Training Epoch: 42 [9088/50048]	Loss: 0.2713
Training Epoch: 42 [9216/50048]	Loss: 0.3116
Training Epoch: 42 [9344/50048]	Loss: 0.4304
Training Epoch: 42 [9472/50048]	Loss: 0.3815
Training Epoch: 42 [9600/50048]	Loss: 0.3635
Training Epoch: 42 [9728/50048]	Loss: 0.3006
Training Epoch: 42 [9856/50048]	Loss: 0.3296
Training Epoch: 42 [9984/50048]	Loss: 0.4130
Training Epoch: 42 [10112/50048]	Loss: 0.4376
Training Epoch: 42 [10240/50048]	Loss: 0.3138
Training Epoch: 42 [10368/50048]	Loss: 0.3356
Training Epoch: 42 [10496/50048]	Loss: 0.3846
Training Epoch: 42 [10624/50048]	Loss: 0.6249
Training Epoch: 42 [10752/50048]	Loss: 0.3481
Training Epoch: 42 [10880/50048]	Loss: 0.3693
Training Epoch: 42 [11008/50048]	Loss: 0.3507
Training Epoch: 42 [11136/50048]	Loss: 0.2716
Training Epoch: 42 [11264/50048]	Loss: 0.2674
Training Epoch: 42 [11392/50048]	Loss: 0.3943
Training Epoch: 42 [11520/50048]	Loss: 0.2821
Training Epoch: 42 [11648/50048]	Loss: 0.5221
Training Epoch: 42 [11776/50048]	Loss: 0.3550
Training Epoch: 42 [11904/50048]	Loss: 0.3376
Training Epoch: 42 [12032/50048]	Loss: 0.3654
Training Epoch: 42 [12160/50048]	Loss: 0.3443
Training Epoch: 42 [12288/50048]	Loss: 0.3446
Training Epoch: 42 [12416/50048]	Loss: 0.2978
Training Epoch: 42 [12544/50048]	Loss: 0.4069
Training Epoch: 42 [12672/50048]	Loss: 0.3686
Training Epoch: 42 [12800/50048]	Loss: 0.3681
Training Epoch: 42 [12928/50048]	Loss: 0.3005
Training Epoch: 42 [13056/50048]	Loss: 0.2829
Training Epoch: 42 [13184/50048]	Loss: 0.2827
Training Epoch: 42 [13312/50048]	Loss: 0.3402
Training Epoch: 42 [13440/50048]	Loss: 0.3393
Training Epoch: 42 [13568/50048]	Loss: 0.3313
Training Epoch: 42 [13696/50048]	Loss: 0.2752
Training Epoch: 42 [13824/50048]	Loss: 0.4033
Training Epoch: 42 [13952/50048]	Loss: 0.3092
Training Epoch: 42 [14080/50048]	Loss: 0.4166
Training Epoch: 42 [14208/50048]	Loss: 0.4160
Training Epoch: 42 [14336/50048]	Loss: 0.2738
Training Epoch: 42 [14464/50048]	Loss: 0.3988
Training Epoch: 42 [14592/50048]	Loss: 0.3417
Training Epoch: 42 [14720/50048]	Loss: 0.2709
Training Epoch: 42 [14848/50048]	Loss: 0.2992
Training Epoch: 42 [14976/50048]	Loss: 0.3630
Training Epoch: 42 [15104/50048]	Loss: 0.4421
Training Epoch: 42 [15232/50048]	Loss: 0.4451
Training Epoch: 42 [15360/50048]	Loss: 0.3235
Training Epoch: 42 [15488/50048]	Loss: 0.3219
Training Epoch: 42 [15616/50048]	Loss: 0.3937
Training Epoch: 42 [15744/50048]	Loss: 0.3879
Training Epoch: 42 [15872/50048]	Loss: 0.3153
Training Epoch: 42 [16000/50048]	Loss: 0.3760
Training Epoch: 42 [16128/50048]	Loss: 0.2979
Training Epoch: 42 [16256/50048]	Loss: 0.3697
Training Epoch: 42 [16384/50048]	Loss: 0.4988
Training Epoch: 42 [16512/50048]	Loss: 0.3611
Training Epoch: 42 [16640/50048]	Loss: 0.2766
Training Epoch: 42 [16768/50048]	Loss: 0.3269
Training Epoch: 42 [16896/50048]	Loss: 0.3383
Training Epoch: 42 [17024/50048]	Loss: 0.2518
Training Epoch: 42 [17152/50048]	Loss: 0.4367
Training Epoch: 42 [17280/50048]	Loss: 0.2741
Training Epoch: 42 [17408/50048]	Loss: 0.4324
Training Epoch: 42 [17536/50048]	Loss: 0.3894
Training Epoch: 42 [17664/50048]	Loss: 0.4184
Training Epoch: 42 [17792/50048]	Loss: 0.4436
Training Epoch: 42 [17920/50048]	Loss: 0.2812
Training Epoch: 42 [18048/50048]	Loss: 0.3050
Training Epoch: 42 [18176/50048]	Loss: 0.3820
Training Epoch: 42 [18304/50048]	Loss: 0.3385
Training Epoch: 42 [18432/50048]	Loss: 0.4203
Training Epoch: 42 [18560/50048]	Loss: 0.5230
Training Epoch: 42 [18688/50048]	Loss: 0.3560
Training Epoch: 42 [18816/50048]	Loss: 0.3950
Training Epoch: 42 [18944/50048]	Loss: 0.3178
Training Epoch: 42 [19072/50048]	Loss: 0.3942
Training Epoch: 42 [19200/50048]	Loss: 0.4537
Training Epoch: 42 [19328/50048]	Loss: 0.4572
Training Epoch: 42 [19456/50048]	Loss: 0.3723
Training Epoch: 42 [19584/50048]	Loss: 0.3136
Training Epoch: 42 [19712/50048]	Loss: 0.3786
Training Epoch: 42 [19840/50048]	Loss: 0.3246
Training Epoch: 42 [19968/50048]	Loss: 0.2830
Training Epoch: 42 [20096/50048]	Loss: 0.4459
Training Epoch: 42 [20224/50048]	Loss: 0.3193
Training Epoch: 42 [20352/50048]	Loss: 0.3339
Training Epoch: 42 [20480/50048]	Loss: 0.3027
Training Epoch: 42 [20608/50048]	Loss: 0.3853
Training Epoch: 42 [20736/50048]	Loss: 0.3178
Training Epoch: 42 [20864/50048]	Loss: 0.3669
Training Epoch: 42 [20992/50048]	Loss: 0.2705
Training Epoch: 42 [21120/50048]	Loss: 0.3699
Training Epoch: 42 [21248/50048]	Loss: 0.3399
Training Epoch: 42 [21376/50048]	Loss: 0.4312
Training Epoch: 42 [21504/50048]	Loss: 0.4382
Training Epoch: 42 [21632/50048]	Loss: 0.4354
Training Epoch: 42 [21760/50048]	Loss: 0.5013
Training Epoch: 42 [21888/50048]	Loss: 0.3330
Training Epoch: 42 [22016/50048]	Loss: 0.4548
Training Epoch: 42 [22144/50048]	Loss: 0.3749
Training Epoch: 42 [22272/50048]	Loss: 0.3836
Training Epoch: 42 [22400/50048]	Loss: 0.3696
Training Epoch: 42 [22528/50048]	Loss: 0.2725
Training Epoch: 42 [22656/50048]	Loss: 0.5756
Training Epoch: 42 [22784/50048]	Loss: 0.5325
Training Epoch: 42 [22912/50048]	Loss: 0.4308
Training Epoch: 42 [23040/50048]	Loss: 0.2910
Training Epoch: 42 [23168/50048]	Loss: 0.4582
Training Epoch: 42 [23296/50048]	Loss: 0.4102
Training Epoch: 42 [23424/50048]	Loss: 0.3757
Training Epoch: 42 [23552/50048]	Loss: 0.3031
Training Epoch: 42 [23680/50048]	Loss: 0.3816
Training Epoch: 42 [23808/50048]	Loss: 0.4174
Training Epoch: 42 [23936/50048]	Loss: 0.4243
Training Epoch: 42 [24064/50048]	Loss: 0.4594
Training Epoch: 42 [24192/50048]	Loss: 0.4431
Training Epoch: 42 [24320/50048]	Loss: 0.4135
Training Epoch: 42 [24448/50048]	Loss: 0.3863
Training Epoch: 42 [24576/50048]	Loss: 0.4485
Training Epoch: 42 [24704/50048]	Loss: 0.5041
Training Epoch: 42 [24832/50048]	Loss: 0.3043
Training Epoch: 42 [24960/50048]	Loss: 0.4510
Training Epoch: 42 [25088/50048]	Loss: 0.2899
Training Epoch: 42 [25216/50048]	Loss: 0.4489
Training Epoch: 42 [25344/50048]	Loss: 0.3377
Training Epoch: 42 [25472/50048]	Loss: 0.3046
Training Epoch: 42 [25600/50048]	Loss: 0.3578
Training Epoch: 42 [25728/50048]	Loss: 0.2292
Training Epoch: 42 [25856/50048]	Loss: 0.3317
Training Epoch: 42 [25984/50048]	Loss: 0.4177
Training Epoch: 42 [26112/50048]	Loss: 0.4057
Training Epoch: 42 [26240/50048]	Loss: 0.3741
Training Epoch: 42 [26368/50048]	Loss: 0.3257
Training Epoch: 42 [26496/50048]	Loss: 0.2566
Training Epoch: 42 [26624/50048]	Loss: 0.2784
Training Epoch: 42 [26752/50048]	Loss: 0.3072
Training Epoch: 42 [26880/50048]	Loss: 0.3818
Training Epoch: 42 [27008/50048]	Loss: 0.3412
Training Epoch: 42 [27136/50048]	Loss: 0.3079
Training Epoch: 42 [27264/50048]	Loss: 0.4587
Training Epoch: 42 [27392/50048]	Loss: 0.2876
Training Epoch: 42 [27520/50048]	Loss: 0.4186
Training Epoch: 42 [27648/50048]	Loss: 0.2858
Training Epoch: 42 [27776/50048]	Loss: 0.5290
Training Epoch: 42 [27904/50048]	Loss: 0.3777
Training Epoch: 42 [28032/50048]	Loss: 0.3376
Training Epoch: 42 [28160/50048]	Loss: 0.2835
Training Epoch: 42 [28288/50048]	Loss: 0.3233
Training Epoch: 42 [28416/50048]	Loss: 0.4867
Training Epoch: 42 [28544/50048]	Loss: 0.4945
Training Epoch: 42 [28672/50048]	Loss: 0.3129
Training Epoch: 42 [28800/50048]	Loss: 0.3225
Training Epoch: 42 [28928/50048]	Loss: 0.3401
Training Epoch: 42 [29056/50048]	Loss: 0.3438
Training Epoch: 42 [29184/50048]	Loss: 0.3201
Training Epoch: 42 [29312/50048]	Loss: 0.4232
Training Epoch: 42 [29440/50048]	Loss: 0.3874
Training Epoch: 42 [29568/50048]	Loss: 0.4326
Training Epoch: 42 [29696/50048]	Loss: 0.4530
Training Epoch: 42 [29824/50048]	Loss: 0.4360
Training Epoch: 42 [29952/50048]	Loss: 0.3872
Training Epoch: 42 [30080/50048]	Loss: 0.2907
Training Epoch: 42 [30208/50048]	Loss: 0.4063
Training Epoch: 42 [30336/50048]	Loss: 0.3979
Training Epoch: 42 [30464/50048]	Loss: 0.3362
Training Epoch: 42 [30592/50048]	Loss: 0.3748
Training Epoch: 42 [30720/50048]	Loss: 0.4187
Training Epoch: 42 [30848/50048]	Loss: 0.3611
Training Epoch: 42 [30976/50048]	Loss: 0.4017
Training Epoch: 42 [31104/50048]	Loss: 0.2750
Training Epoch: 42 [31232/50048]	Loss: 0.4444
Training Epoch: 42 [31360/50048]	Loss: 0.4295
Training Epoch: 42 [31488/50048]	Loss: 0.3009
Training Epoch: 42 [31616/50048]	Loss: 0.2680
Training Epoch: 42 [31744/50048]	Loss: 0.4121
Training Epoch: 42 [31872/50048]	Loss: 0.4146
Training Epoch: 42 [32000/50048]	Loss: 0.3511
Training Epoch: 42 [32128/50048]	Loss: 0.3328
Training Epoch: 42 [32256/50048]	Loss: 0.4369
Training Epoch: 42 [32384/50048]	Loss: 0.2847
Training Epoch: 42 [32512/50048]	Loss: 0.3770
Training Epoch: 42 [32640/50048]	Loss: 0.5582
Training Epoch: 42 [32768/50048]	Loss: 0.4625
Training Epoch: 42 [32896/50048]	Loss: 0.3240
Training Epoch: 42 [33024/50048]	Loss: 0.3760
Training Epoch: 42 [33152/50048]	Loss: 0.3830
Training Epoch: 42 [33280/50048]	Loss: 0.5298
Training Epoch: 42 [33408/50048]	Loss: 0.3010
Training Epoch: 42 [33536/50048]	Loss: 0.3440
Training Epoch: 42 [33664/50048]	Loss: 0.2973
Training Epoch: 42 [33792/50048]	Loss: 0.3454
Training Epoch: 42 [33920/50048]	Loss: 0.3114
Training Epoch: 42 [34048/50048]	Loss: 0.4032
Training Epoch: 42 [34176/50048]	Loss: 0.3476
Training Epoch: 42 [34304/50048]	Loss: 0.3992
Training Epoch: 42 [34432/50048]	Loss: 0.5330
Training Epoch: 42 [34560/50048]	Loss: 0.3894
Training Epoch: 42 [34688/50048]	Loss: 0.5108
Training Epoch: 42 [34816/50048]	Loss: 0.5340
Training Epoch: 42 [34944/50048]	Loss: 0.4614
Training Epoch: 42 [35072/50048]	Loss: 0.3635
Training Epoch: 42 [35200/50048]	Loss: 0.4445
Training Epoch: 42 [35328/50048]	Loss: 0.4203
Training Epoch: 42 [35456/50048]	Loss: 0.4185
Training Epoch: 42 [35584/50048]	Loss: 0.3689
Training Epoch: 42 [35712/50048]	Loss: 0.4096
Training Epoch: 42 [35840/50048]	Loss: 0.4112
Training Epoch: 42 [35968/50048]	Loss: 0.3950
Training Epoch: 42 [36096/50048]	Loss: 0.3198
Training Epoch: 42 [36224/50048]	Loss: 0.3584
Training Epoch: 42 [36352/50048]	Loss: 0.4612
Training Epoch: 42 [36480/50048]	Loss: 0.4464
Training Epoch: 42 [36608/50048]	Loss: 0.3647
Training Epoch: 42 [36736/50048]	Loss: 0.4372
Training Epoch: 42 [36864/50048]	Loss: 0.3527
Training Epoch: 42 [36992/50048]	Loss: 0.3052
Training Epoch: 42 [37120/50048]	Loss: 0.4796
Training Epoch: 42 [37248/50048]	Loss: 0.4744
Training Epoch: 42 [37376/50048]	Loss: 0.4113
Training Epoch: 42 [37504/50048]	Loss: 0.4999
Training Epoch: 42 [37632/50048]	Loss: 0.3686
Training Epoch: 42 [37760/50048]	Loss: 0.5116
Training Epoch: 42 [37888/50048]	Loss: 0.4649
Training Epoch: 42 [38016/50048]	Loss: 0.3439
Training Epoch: 42 [38144/50048]	Loss: 0.4224
Training Epoch: 42 [38272/50048]	Loss: 0.3285
Training Epoch: 42 [38400/50048]	Loss: 0.3811
Training Epoch: 42 [38528/50048]	Loss: 0.3624
Training Epoch: 42 [38656/50048]	Loss: 0.4303
Training Epoch: 42 [38784/50048]	Loss: 0.4009
Training Epoch: 42 [38912/50048]	Loss: 0.4693
Training Epoch: 42 [39040/50048]	Loss: 0.4184
Training Epoch: 42 [39168/50048]	Loss: 0.4887
Training Epoch: 42 [39296/50048]	Loss: 0.3243
Training Epoch: 42 [39424/50048]	Loss: 0.3937
Training Epoch: 42 [39552/50048]	Loss: 0.4839
Training Epoch: 42 [39680/50048]	Loss: 0.4887
Training Epoch: 42 [39808/50048]	Loss: 0.3509
Training Epoch: 42 [39936/50048]	Loss: 0.2767
Training Epoch: 42 [40064/50048]	Loss: 0.2906
Training Epoch: 42 [40192/50048]	Loss: 0.2981
Training Epoch: 42 [40320/50048]	Loss: 0.4215
Training Epoch: 42 [40448/50048]	Loss: 0.3815
Training Epoch: 42 [40576/50048]	Loss: 0.5657
Training Epoch: 42 [40704/50048]	Loss: 0.3749
Training Epoch: 42 [40832/50048]	Loss: 0.3376
Training Epoch: 42 [40960/50048]	Loss: 0.3734
Training Epoch: 42 [41088/50048]	Loss: 0.4192
Training Epoch: 42 [41216/50048]	Loss: 0.4679
Training Epoch: 42 [41344/50048]	Loss: 0.4065
Training Epoch: 42 [41472/50048]	Loss: 0.3562
Training Epoch: 42 [41600/50048]	Loss: 0.5087
Training Epoch: 42 [41728/50048]	Loss: 0.3804
Training Epoch: 42 [41856/50048]	Loss: 0.4103
Training Epoch: 42 [41984/50048]	Loss: 0.4350
Training Epoch: 42 [42112/50048]	Loss: 0.3667
Training Epoch: 42 [42240/50048]	Loss: 0.4413
Training Epoch: 42 [42368/50048]	Loss: 0.5256
Training Epoch: 42 [42496/50048]	Loss: 0.5349
Training Epoch: 42 [42624/50048]	Loss: 0.2942
Training Epoch: 42 [42752/50048]	Loss: 0.3662
Training Epoch: 42 [42880/50048]	Loss: 0.4016
Training Epoch: 42 [43008/50048]	Loss: 0.3858
Training Epoch: 42 [43136/50048]	Loss: 0.4237
Training Epoch: 42 [43264/50048]	Loss: 0.3907
Training Epoch: 42 [43392/50048]	Loss: 0.4590
Training Epoch: 42 [43520/50048]	Loss: 0.3771
Training Epoch: 42 [43648/50048]	Loss: 0.4026
Training Epoch: 42 [43776/50048]	Loss: 0.5258
Training Epoch: 42 [43904/50048]	Loss: 0.4472
Training Epoch: 42 [44032/50048]	Loss: 0.4362
Training Epoch: 42 [44160/50048]	Loss: 0.4227
Training Epoch: 42 [44288/50048]	Loss: 0.5379
Training Epoch: 42 [44416/50048]	Loss: 0.4541
Training Epoch: 42 [44544/50048]	Loss: 0.3511
Training Epoch: 42 [44672/50048]	Loss: 0.3429
Training Epoch: 42 [44800/50048]	Loss: 0.4387
Training Epoch: 42 [44928/50048]	Loss: 0.4955
Training Epoch: 42 [45056/50048]	Loss: 0.4481
Training Epoch: 42 [45184/50048]	Loss: 0.2852
Training Epoch: 42 [45312/50048]	Loss: 0.4603
Training Epoch: 42 [45440/50048]	Loss: 0.4676
Training Epoch: 42 [45568/50048]	Loss: 0.2231
Training Epoch: 42 [45696/50048]	Loss: 0.3620
2022-12-06 04:42:00,128 [ZeusDataLoader(train)] train epoch 43 done: time=86.44 energy=10503.80
2022-12-06 04:42:00,129 [ZeusDataLoader(eval)] Epoch 43 begin.
Training Epoch: 42 [45824/50048]	Loss: 0.4808
Training Epoch: 42 [45952/50048]	Loss: 0.4783
Training Epoch: 42 [46080/50048]	Loss: 0.4453
Training Epoch: 42 [46208/50048]	Loss: 0.4967
Training Epoch: 42 [46336/50048]	Loss: 0.3540
Training Epoch: 42 [46464/50048]	Loss: 0.4386
Training Epoch: 42 [46592/50048]	Loss: 0.4739
Training Epoch: 42 [46720/50048]	Loss: 0.4044
Training Epoch: 42 [46848/50048]	Loss: 0.3377
Training Epoch: 42 [46976/50048]	Loss: 0.4529
Training Epoch: 42 [47104/50048]	Loss: 0.4322
Training Epoch: 42 [47232/50048]	Loss: 0.4947
Training Epoch: 42 [47360/50048]	Loss: 0.4084
Training Epoch: 42 [47488/50048]	Loss: 0.3971
Training Epoch: 42 [47616/50048]	Loss: 0.3309
Training Epoch: 42 [47744/50048]	Loss: 0.2991
Training Epoch: 42 [47872/50048]	Loss: 0.4673
Training Epoch: 42 [48000/50048]	Loss: 0.4407
Training Epoch: 42 [48128/50048]	Loss: 0.3935
Training Epoch: 42 [48256/50048]	Loss: 0.3653
Training Epoch: 42 [48384/50048]	Loss: 0.4567
Training Epoch: 42 [48512/50048]	Loss: 0.4508
Training Epoch: 42 [48640/50048]	Loss: 0.4558
Training Epoch: 42 [48768/50048]	Loss: 0.4858
Training Epoch: 42 [48896/50048]	Loss: 0.4188
Training Epoch: 42 [49024/50048]	Loss: 0.4601
Training Epoch: 42 [49152/50048]	Loss: 0.3750
Training Epoch: 42 [49280/50048]	Loss: 0.3653
Training Epoch: 42 [49408/50048]	Loss: 0.5926
Training Epoch: 42 [49536/50048]	Loss: 0.3427
Training Epoch: 42 [49664/50048]	Loss: 0.3279
Training Epoch: 42 [49792/50048]	Loss: 0.4878
Training Epoch: 42 [49920/50048]	Loss: 0.4628
Training Epoch: 42 [50048/50048]	Loss: 0.4488
2022-12-06 09:42:03.785 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:42:03,796 [ZeusDataLoader(eval)] eval epoch 43 done: time=3.66 energy=440.00
2022-12-06 04:42:03,796 [ZeusDataLoader(train)] Up to epoch 43: time=3880.63, energy=471028.44, cost=575069.66
2022-12-06 04:42:03,796 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:42:03,796 [ZeusDataLoader(train)] Expected next epoch: time=3970.43, energy=481826.45, cost=588326.04
2022-12-06 04:42:03,797 [ZeusDataLoader(train)] Epoch 44 begin.
Validation Epoch: 42, Average loss: 0.0142, Accuracy: 0.6256
2022-12-06 04:42:03,936 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:42:03,937 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:42:03.939 [ZeusMonitor] Monitor started.
2022-12-06 09:42:03.939 [ZeusMonitor] Running indefinitely. 2022-12-06 09:42:03.939 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:42:03.939 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e44+gpu0.power.log
Training Epoch: 43 [128/50048]	Loss: 0.3116
Training Epoch: 43 [256/50048]	Loss: 0.2844
Training Epoch: 43 [384/50048]	Loss: 0.4196
Training Epoch: 43 [512/50048]	Loss: 0.3801
Training Epoch: 43 [640/50048]	Loss: 0.3229
Training Epoch: 43 [768/50048]	Loss: 0.3955
Training Epoch: 43 [896/50048]	Loss: 0.2518
Training Epoch: 43 [1024/50048]	Loss: 0.2699
Training Epoch: 43 [1152/50048]	Loss: 0.3391
Training Epoch: 43 [1280/50048]	Loss: 0.3576
Training Epoch: 43 [1408/50048]	Loss: 0.3214
Training Epoch: 43 [1536/50048]	Loss: 0.3379
Training Epoch: 43 [1664/50048]	Loss: 0.2335
Training Epoch: 43 [1792/50048]	Loss: 0.3413
Training Epoch: 43 [1920/50048]	Loss: 0.3861
Training Epoch: 43 [2048/50048]	Loss: 0.2918
Training Epoch: 43 [2176/50048]	Loss: 0.2527
Training Epoch: 43 [2304/50048]	Loss: 0.4867
Training Epoch: 43 [2432/50048]	Loss: 0.2075
Training Epoch: 43 [2560/50048]	Loss: 0.4195
Training Epoch: 43 [2688/50048]	Loss: 0.2878
Training Epoch: 43 [2816/50048]	Loss: 0.2881
Training Epoch: 43 [2944/50048]	Loss: 0.2867
Training Epoch: 43 [3072/50048]	Loss: 0.2826
Training Epoch: 43 [3200/50048]	Loss: 0.3516
Training Epoch: 43 [3328/50048]	Loss: 0.3343
Training Epoch: 43 [3456/50048]	Loss: 0.3791
Training Epoch: 43 [3584/50048]	Loss: 0.2742
Training Epoch: 43 [3712/50048]	Loss: 0.2371
Training Epoch: 43 [3840/50048]	Loss: 0.2610
Training Epoch: 43 [3968/50048]	Loss: 0.4076
Training Epoch: 43 [4096/50048]	Loss: 0.3544
Training Epoch: 43 [4224/50048]	Loss: 0.3046
Training Epoch: 43 [4352/50048]	Loss: 0.2626
Training Epoch: 43 [4480/50048]	Loss: 0.3168
Training Epoch: 43 [4608/50048]	Loss: 0.2497
Training Epoch: 43 [4736/50048]	Loss: 0.3428
Training Epoch: 43 [4864/50048]	Loss: 0.3216
Training Epoch: 43 [4992/50048]	Loss: 0.2212
Training Epoch: 43 [5120/50048]	Loss: 0.3000
Training Epoch: 43 [5248/50048]	Loss: 0.3280
Training Epoch: 43 [5376/50048]	Loss: 0.3406
Training Epoch: 43 [5504/50048]	Loss: 0.3082
Training Epoch: 43 [5632/50048]	Loss: 0.4059
Training Epoch: 43 [5760/50048]	Loss: 0.2867
Training Epoch: 43 [5888/50048]	Loss: 0.4504
Training Epoch: 43 [6016/50048]	Loss: 0.2799
Training Epoch: 43 [6144/50048]	Loss: 0.2065
Training Epoch: 43 [6272/50048]	Loss: 0.4838
Training Epoch: 43 [6400/50048]	Loss: 0.4524
Training Epoch: 43 [6528/50048]	Loss: 0.4345
Training Epoch: 43 [6656/50048]	Loss: 0.2621
Training Epoch: 43 [6784/50048]	Loss: 0.3139
Training Epoch: 43 [6912/50048]	Loss: 0.3152
Training Epoch: 43 [7040/50048]	Loss: 0.3285
Training Epoch: 43 [7168/50048]	Loss: 0.3395
Training Epoch: 43 [7296/50048]	Loss: 0.3851
Training Epoch: 43 [7424/50048]	Loss: 0.2860
Training Epoch: 43 [7552/50048]	Loss: 0.2974
Training Epoch: 43 [7680/50048]	Loss: 0.4048
Training Epoch: 43 [7808/50048]	Loss: 0.4581
Training Epoch: 43 [7936/50048]	Loss: 0.3382
Training Epoch: 43 [8064/50048]	Loss: 0.1880
Training Epoch: 43 [8192/50048]	Loss: 0.3950
Training Epoch: 43 [8320/50048]	Loss: 0.3590
Training Epoch: 43 [8448/50048]	Loss: 0.2988
Training Epoch: 43 [8576/50048]	Loss: 0.2274
Training Epoch: 43 [8704/50048]	Loss: 0.4268
Training Epoch: 43 [8832/50048]	Loss: 0.3750
Training Epoch: 43 [8960/50048]	Loss: 0.2367
Training Epoch: 43 [9088/50048]	Loss: 0.4022
Training Epoch: 43 [9216/50048]	Loss: 0.3802
Training Epoch: 43 [9344/50048]	Loss: 0.3885
Training Epoch: 43 [9472/50048]	Loss: 0.2749
Training Epoch: 43 [9600/50048]	Loss: 0.3720
Training Epoch: 43 [9728/50048]	Loss: 0.3084
Training Epoch: 43 [9856/50048]	Loss: 0.3197
Training Epoch: 43 [9984/50048]	Loss: 0.3311
Training Epoch: 43 [10112/50048]	Loss: 0.3349
Training Epoch: 43 [10240/50048]	Loss: 0.3119
Training Epoch: 43 [10368/50048]	Loss: 0.2413
Training Epoch: 43 [10496/50048]	Loss: 0.3969
Training Epoch: 43 [10624/50048]	Loss: 0.2848
Training Epoch: 43 [10752/50048]	Loss: 0.2467
Training Epoch: 43 [10880/50048]	Loss: 0.4356
Training Epoch: 43 [11008/50048]	Loss: 0.3274
Training Epoch: 43 [11136/50048]	Loss: 0.3429
Training Epoch: 43 [11264/50048]	Loss: 0.2209
Training Epoch: 43 [11392/50048]	Loss: 0.3500
Training Epoch: 43 [11520/50048]	Loss: 0.3507
Training Epoch: 43 [11648/50048]	Loss: 0.3441
Training Epoch: 43 [11776/50048]	Loss: 0.3655
Training Epoch: 43 [11904/50048]	Loss: 0.3539
Training Epoch: 43 [12032/50048]	Loss: 0.2977
Training Epoch: 43 [12160/50048]	Loss: 0.4221
Training Epoch: 43 [12288/50048]	Loss: 0.3094
Training Epoch: 43 [12416/50048]	Loss: 0.2979
Training Epoch: 43 [12544/50048]	Loss: 0.3564
Training Epoch: 43 [12672/50048]	Loss: 0.2728
Training Epoch: 43 [12800/50048]	Loss: 0.4350
Training Epoch: 43 [12928/50048]	Loss: 0.3858
Training Epoch: 43 [13056/50048]	Loss: 0.3251
Training Epoch: 43 [13184/50048]	Loss: 0.3136
Training Epoch: 43 [13312/50048]	Loss: 0.3534
Training Epoch: 43 [13440/50048]	Loss: 0.3210
Training Epoch: 43 [13568/50048]	Loss: 0.3476
Training Epoch: 43 [13696/50048]	Loss: 0.3992
Training Epoch: 43 [13824/50048]	Loss: 0.4006
Training Epoch: 43 [13952/50048]	Loss: 0.3157
Training Epoch: 43 [14080/50048]	Loss: 0.2515
Training Epoch: 43 [14208/50048]	Loss: 0.2473
Training Epoch: 43 [14336/50048]	Loss: 0.3345
Training Epoch: 43 [14464/50048]	Loss: 0.3254
Training Epoch: 43 [14592/50048]	Loss: 0.2199
Training Epoch: 43 [14720/50048]	Loss: 0.3784
Training Epoch: 43 [14848/50048]	Loss: 0.3598
Training Epoch: 43 [14976/50048]	Loss: 0.3079
Training Epoch: 43 [15104/50048]	Loss: 0.4691
Training Epoch: 43 [15232/50048]	Loss: 0.3801
Training Epoch: 43 [15360/50048]	Loss: 0.3835
Training Epoch: 43 [15488/50048]	Loss: 0.3161
Training Epoch: 43 [15616/50048]	Loss: 0.4603
Training Epoch: 43 [15744/50048]	Loss: 0.3675
Training Epoch: 43 [15872/50048]	Loss: 0.3190
Training Epoch: 43 [16000/50048]	Loss: 0.2133
Training Epoch: 43 [16128/50048]	Loss: 0.4326
Training Epoch: 43 [16256/50048]	Loss: 0.3485
Training Epoch: 43 [16384/50048]	Loss: 0.3180
Training Epoch: 43 [16512/50048]	Loss: 0.3817
Training Epoch: 43 [16640/50048]	Loss: 0.3391
Training Epoch: 43 [16768/50048]	Loss: 0.2796
Training Epoch: 43 [16896/50048]	Loss: 0.3097
Training Epoch: 43 [17024/50048]	Loss: 0.4110
Training Epoch: 43 [17152/50048]	Loss: 0.4301
Training Epoch: 43 [17280/50048]	Loss: 0.3245
Training Epoch: 43 [17408/50048]	Loss: 0.4259
Training Epoch: 43 [17536/50048]	Loss: 0.3481
Training Epoch: 43 [17664/50048]	Loss: 0.1935
Training Epoch: 43 [17792/50048]	Loss: 0.3854
Training Epoch: 43 [17920/50048]	Loss: 0.4325
Training Epoch: 43 [18048/50048]	Loss: 0.2690
Training Epoch: 43 [18176/50048]	Loss: 0.1967
Training Epoch: 43 [18304/50048]	Loss: 0.4445
Training Epoch: 43 [18432/50048]	Loss: 0.2504
Training Epoch: 43 [18560/50048]	Loss: 0.5563
Training Epoch: 43 [18688/50048]	Loss: 0.3839
Training Epoch: 43 [18816/50048]	Loss: 0.3798
Training Epoch: 43 [18944/50048]	Loss: 0.4568
Training Epoch: 43 [19072/50048]	Loss: 0.4446
Training Epoch: 43 [19200/50048]	Loss: 0.2605
Training Epoch: 43 [19328/50048]	Loss: 0.3784
Training Epoch: 43 [19456/50048]	Loss: 0.4776
Training Epoch: 43 [19584/50048]	Loss: 0.2824
Training Epoch: 43 [19712/50048]	Loss: 0.3294
Training Epoch: 43 [19840/50048]	Loss: 0.3878
Training Epoch: 43 [19968/50048]	Loss: 0.3554
Training Epoch: 43 [20096/50048]	Loss: 0.3237
Training Epoch: 43 [20224/50048]	Loss: 0.3130
Training Epoch: 43 [20352/50048]	Loss: 0.3827
Training Epoch: 43 [20480/50048]	Loss: 0.3686
Training Epoch: 43 [20608/50048]	Loss: 0.4770
Training Epoch: 43 [20736/50048]	Loss: 0.3780
Training Epoch: 43 [20864/50048]	Loss: 0.3244
Training Epoch: 43 [20992/50048]	Loss: 0.4005
Training Epoch: 43 [21120/50048]	Loss: 0.4759
Training Epoch: 43 [21248/50048]	Loss: 0.3753
Training Epoch: 43 [21376/50048]	Loss: 0.2733
Training Epoch: 43 [21504/50048]	Loss: 0.3133
Training Epoch: 43 [21632/50048]	Loss: 0.4258
Training Epoch: 43 [21760/50048]	Loss: 0.2896
Training Epoch: 43 [21888/50048]	Loss: 0.5056
Training Epoch: 43 [22016/50048]	Loss: 0.4361
Training Epoch: 43 [22144/50048]	Loss: 0.3624
Training Epoch: 43 [22272/50048]	Loss: 0.2705
Training Epoch: 43 [22400/50048]	Loss: 0.2317
Training Epoch: 43 [22528/50048]	Loss: 0.2301
Training Epoch: 43 [22656/50048]	Loss: 0.4016
Training Epoch: 43 [22784/50048]	Loss: 0.3067
Training Epoch: 43 [22912/50048]	Loss: 0.3647
Training Epoch: 43 [23040/50048]	Loss: 0.4581
Training Epoch: 43 [23168/50048]	Loss: 0.3033
Training Epoch: 43 [23296/50048]	Loss: 0.3046
Training Epoch: 43 [23424/50048]	Loss: 0.2327
Training Epoch: 43 [23552/50048]	Loss: 0.3440
Training Epoch: 43 [23680/50048]	Loss: 0.3561
Training Epoch: 43 [23808/50048]	Loss: 0.2988
Training Epoch: 43 [23936/50048]	Loss: 0.3693
Training Epoch: 43 [24064/50048]	Loss: 0.4069
Training Epoch: 43 [24192/50048]	Loss: 0.4679
Training Epoch: 43 [24320/50048]	Loss: 0.4045
Training Epoch: 43 [24448/50048]	Loss: 0.3744
Training Epoch: 43 [24576/50048]	Loss: 0.3809
Training Epoch: 43 [24704/50048]	Loss: 0.4287
Training Epoch: 43 [24832/50048]	Loss: 0.3206
Training Epoch: 43 [24960/50048]	Loss: 0.3536
Training Epoch: 43 [25088/50048]	Loss: 0.5440
Training Epoch: 43 [25216/50048]	Loss: 0.4879
Training Epoch: 43 [25344/50048]	Loss: 0.4870
Training Epoch: 43 [25472/50048]	Loss: 0.3591
Training Epoch: 43 [25600/50048]	Loss: 0.4966
Training Epoch: 43 [25728/50048]	Loss: 0.4515
Training Epoch: 43 [25856/50048]	Loss: 0.3921
Training Epoch: 43 [25984/50048]	Loss: 0.3685
Training Epoch: 43 [26112/50048]	Loss: 0.4000
Training Epoch: 43 [26240/50048]	Loss: 0.3263
Training Epoch: 43 [26368/50048]	Loss: 0.3878
Training Epoch: 43 [26496/50048]	Loss: 0.4409
Training Epoch: 43 [26624/50048]	Loss: 0.4526
Training Epoch: 43 [26752/50048]	Loss: 0.3855
Training Epoch: 43 [26880/50048]	Loss: 0.3747
Training Epoch: 43 [27008/50048]	Loss: 0.3597
Training Epoch: 43 [27136/50048]	Loss: 0.5202
Training Epoch: 43 [27264/50048]	Loss: 0.3006
Training Epoch: 43 [27392/50048]	Loss: 0.3335
Training Epoch: 43 [27520/50048]	Loss: 0.2897
Training Epoch: 43 [27648/50048]	Loss: 0.3580
Training Epoch: 43 [27776/50048]	Loss: 0.4235
Training Epoch: 43 [27904/50048]	Loss: 0.3403
Training Epoch: 43 [28032/50048]	Loss: 0.3674
Training Epoch: 43 [28160/50048]	Loss: 0.3463
Training Epoch: 43 [28288/50048]	Loss: 0.3185
Training Epoch: 43 [28416/50048]	Loss: 0.3810
Training Epoch: 43 [28544/50048]	Loss: 0.3065
Training Epoch: 43 [28672/50048]	Loss: 0.3985
Training Epoch: 43 [28800/50048]	Loss: 0.5083
Training Epoch: 43 [28928/50048]	Loss: 0.4506
Training Epoch: 43 [29056/50048]	Loss: 0.3746
Training Epoch: 43 [29184/50048]	Loss: 0.3134
Training Epoch: 43 [29312/50048]	Loss: 0.4024
Training Epoch: 43 [29440/50048]	Loss: 0.3843
Training Epoch: 43 [29568/50048]	Loss: 0.2871
Training Epoch: 43 [29696/50048]	Loss: 0.3325
Training Epoch: 43 [29824/50048]	Loss: 0.2119
Training Epoch: 43 [29952/50048]	Loss: 0.4545
Training Epoch: 43 [30080/50048]	Loss: 0.2592
Training Epoch: 43 [30208/50048]	Loss: 0.3619
Training Epoch: 43 [30336/50048]	Loss: 0.3417
Training Epoch: 43 [30464/50048]	Loss: 0.2210
Training Epoch: 43 [30592/50048]	Loss: 0.3060
Training Epoch: 43 [30720/50048]	Loss: 0.3897
Training Epoch: 43 [30848/50048]	Loss: 0.4416
Training Epoch: 43 [30976/50048]	Loss: 0.3846
Training Epoch: 43 [31104/50048]	Loss: 0.3841
Training Epoch: 43 [31232/50048]	Loss: 0.4173
Training Epoch: 43 [31360/50048]	Loss: 0.2902
Training Epoch: 43 [31488/50048]	Loss: 0.3701
Training Epoch: 43 [31616/50048]	Loss: 0.3398
Training Epoch: 43 [31744/50048]	Loss: 0.3550
Training Epoch: 43 [31872/50048]	Loss: 0.4113
Training Epoch: 43 [32000/50048]	Loss: 0.4836
Training Epoch: 43 [32128/50048]	Loss: 0.3453
Training Epoch: 43 [32256/50048]	Loss: 0.3374
Training Epoch: 43 [32384/50048]	Loss: 0.4568
Training Epoch: 43 [32512/50048]	Loss: 0.4452
Training Epoch: 43 [32640/50048]	Loss: 0.3476
Training Epoch: 43 [32768/50048]	Loss: 0.4513
Training Epoch: 43 [32896/50048]	Loss: 0.3027
Training Epoch: 43 [33024/50048]	Loss: 0.3502
Training Epoch: 43 [33152/50048]	Loss: 0.4451
Training Epoch: 43 [33280/50048]	Loss: 0.3521
Training Epoch: 43 [33408/50048]	Loss: 0.2765
Training Epoch: 43 [33536/50048]	Loss: 0.3064
Training Epoch: 43 [33664/50048]	Loss: 0.4086
Training Epoch: 43 [33792/50048]	Loss: 0.2710
Training Epoch: 43 [33920/50048]	Loss: 0.2220
Training Epoch: 43 [34048/50048]	Loss: 0.2998
Training Epoch: 43 [34176/50048]	Loss: 0.3307
Training Epoch: 43 [34304/50048]	Loss: 0.3924
Training Epoch: 43 [34432/50048]	Loss: 0.4208
Training Epoch: 43 [34560/50048]	Loss: 0.2885
Training Epoch: 43 [34688/50048]	Loss: 0.4166
Training Epoch: 43 [34816/50048]	Loss: 0.2741
Training Epoch: 43 [34944/50048]	Loss: 0.3759
Training Epoch: 43 [35072/50048]	Loss: 0.5147
Training Epoch: 43 [35200/50048]	Loss: 0.4701
Training Epoch: 43 [35328/50048]	Loss: 0.3249
Training Epoch: 43 [35456/50048]	Loss: 0.4113
Training Epoch: 43 [35584/50048]	Loss: 0.3904
Training Epoch: 43 [35712/50048]	Loss: 0.3910
Training Epoch: 43 [35840/50048]	Loss: 0.4335
Training Epoch: 43 [35968/50048]	Loss: 0.2927
Training Epoch: 43 [36096/50048]	Loss: 0.3724
Training Epoch: 43 [36224/50048]	Loss: 0.4760
Training Epoch: 43 [36352/50048]	Loss: 0.4060
Training Epoch: 43 [36480/50048]	Loss: 0.3849
Training Epoch: 43 [36608/50048]	Loss: 0.3035
Training Epoch: 43 [36736/50048]	Loss: 0.4860
Training Epoch: 43 [36864/50048]	Loss: 0.3709
Training Epoch: 43 [36992/50048]	Loss: 0.3873
Training Epoch: 43 [37120/50048]	Loss: 0.4869
Training Epoch: 43 [37248/50048]	Loss: 0.1773
Training Epoch: 43 [37376/50048]	Loss: 0.4474
Training Epoch: 43 [37504/50048]	Loss: 0.3993
Training Epoch: 43 [37632/50048]	Loss: 0.3646
Training Epoch: 43 [37760/50048]	Loss: 0.3481
Training Epoch: 43 [37888/50048]	Loss: 0.3822
Training Epoch: 43 [38016/50048]	Loss: 0.3551
Training Epoch: 43 [38144/50048]	Loss: 0.4764
Training Epoch: 43 [38272/50048]	Loss: 0.4237
Training Epoch: 43 [38400/50048]	Loss: 0.2457
Training Epoch: 43 [38528/50048]	Loss: 0.3335
Training Epoch: 43 [38656/50048]	Loss: 0.3531
Training Epoch: 43 [38784/50048]	Loss: 0.3427
Training Epoch: 43 [38912/50048]	Loss: 0.3473
Training Epoch: 43 [39040/50048]	Loss: 0.4041
Training Epoch: 43 [39168/50048]	Loss: 0.5198
Training Epoch: 43 [39296/50048]	Loss: 0.3837
Training Epoch: 43 [39424/50048]	Loss: 0.3948
Training Epoch: 43 [39552/50048]	Loss: 0.2760
Training Epoch: 43 [39680/50048]	Loss: 0.3219
Training Epoch: 43 [39808/50048]	Loss: 0.3623
Training Epoch: 43 [39936/50048]	Loss: 0.4780
Training Epoch: 43 [40064/50048]	Loss: 0.4433
Training Epoch: 43 [40192/50048]	Loss: 0.3798
Training Epoch: 43 [40320/50048]	Loss: 0.2976
Training Epoch: 43 [40448/50048]	Loss: 0.4128
Training Epoch: 43 [40576/50048]	Loss: 0.3088
Training Epoch: 43 [40704/50048]	Loss: 0.4123
Training Epoch: 43 [40832/50048]	Loss: 0.2979
Training Epoch: 43 [40960/50048]	Loss: 0.3156
Training Epoch: 43 [41088/50048]	Loss: 0.3293
Training Epoch: 43 [41216/50048]	Loss: 0.3449
Training Epoch: 43 [41344/50048]	Loss: 0.2688
Training Epoch: 43 [41472/50048]	Loss: 0.4751
Training Epoch: 43 [41600/50048]	Loss: 0.3556
Training Epoch: 43 [41728/50048]	Loss: 0.2615
Training Epoch: 43 [41856/50048]	Loss: 0.4294
Training Epoch: 43 [41984/50048]	Loss: 0.3550
Training Epoch: 43 [42112/50048]	Loss: 0.3404
Training Epoch: 43 [42240/50048]	Loss: 0.5104
Training Epoch: 43 [42368/50048]	Loss: 0.4546
Training Epoch: 43 [42496/50048]	Loss: 0.3696
Training Epoch: 43 [42624/50048]	Loss: 0.2836
Training Epoch: 43 [42752/50048]	Loss: 0.3342
Training Epoch: 43 [42880/50048]	Loss: 0.4385
Training Epoch: 43 [43008/50048]	Loss: 0.5490
Training Epoch: 43 [43136/50048]	Loss: 0.4869
Training Epoch: 43 [43264/50048]	Loss: 0.3253
Training Epoch: 43 [43392/50048]	Loss: 0.4274
Training Epoch: 43 [43520/50048]	Loss: 0.4448
Training Epoch: 43 [43648/50048]	Loss: 0.4315
Training Epoch: 43 [43776/50048]	Loss: 0.3642
Training Epoch: 43 [43904/50048]	Loss: 0.4002
Training Epoch: 43 [44032/50048]	Loss: 0.3378
Training Epoch: 43 [44160/50048]	Loss: 0.5503
Training Epoch: 43 [44288/50048]	Loss: 0.5402
Training Epoch: 43 [44416/50048]	Loss: 0.3762
Training Epoch: 43 [44544/50048]	Loss: 0.3000
Training Epoch: 43 [44672/50048]	Loss: 0.5060
Training Epoch: 43 [44800/50048]	Loss: 0.5309
Training Epoch: 43 [44928/50048]	Loss: 0.4799
Training Epoch: 43 [45056/50048]	Loss: 0.2701
Training Epoch: 43 [45184/50048]	Loss: 0.4830
Training Epoch: 43 [45312/50048]	Loss: 0.4040
Training Epoch: 43 [45440/50048]	Loss: 0.5198
Training Epoch: 43 [45568/50048]	Loss: 0.4416
Training Epoch: 43 [45696/50048]	Loss: 0.5474
2022-12-06 04:43:30,209 [ZeusDataLoader(train)] train epoch 44 done: time=86.40 energy=10496.12
2022-12-06 04:43:30,210 [ZeusDataLoader(eval)] Epoch 44 begin.
Training Epoch: 43 [45824/50048]	Loss: 0.3598
Training Epoch: 43 [45952/50048]	Loss: 0.3944
Training Epoch: 43 [46080/50048]	Loss: 0.4956
Training Epoch: 43 [46208/50048]	Loss: 0.3815
Training Epoch: 43 [46336/50048]	Loss: 0.2921
Training Epoch: 43 [46464/50048]	Loss: 0.4167
Training Epoch: 43 [46592/50048]	Loss: 0.3610
Training Epoch: 43 [46720/50048]	Loss: 0.4139
Training Epoch: 43 [46848/50048]	Loss: 0.4146
Training Epoch: 43 [46976/50048]	Loss: 0.4037
Training Epoch: 43 [47104/50048]	Loss: 0.3290
Training Epoch: 43 [47232/50048]	Loss: 0.4252
Training Epoch: 43 [47360/50048]	Loss: 0.5181
Training Epoch: 43 [47488/50048]	Loss: 0.3333
Training Epoch: 43 [47616/50048]	Loss: 0.3225
Training Epoch: 43 [47744/50048]	Loss: 0.4994
Training Epoch: 43 [47872/50048]	Loss: 0.4771
Training Epoch: 43 [48000/50048]	Loss: 0.3525
Training Epoch: 43 [48128/50048]	Loss: 0.3927
Training Epoch: 43 [48256/50048]	Loss: 0.4565
Training Epoch: 43 [48384/50048]	Loss: 0.5672
Training Epoch: 43 [48512/50048]	Loss: 0.3883
Training Epoch: 43 [48640/50048]	Loss: 0.3755
Training Epoch: 43 [48768/50048]	Loss: 0.2852
Training Epoch: 43 [48896/50048]	Loss: 0.2415
Training Epoch: 43 [49024/50048]	Loss: 0.3918
Training Epoch: 43 [49152/50048]	Loss: 0.3906
Training Epoch: 43 [49280/50048]	Loss: 0.3248
Training Epoch: 43 [49408/50048]	Loss: 0.3635
Training Epoch: 43 [49536/50048]	Loss: 0.6437
Training Epoch: 43 [49664/50048]	Loss: 0.4077
Training Epoch: 43 [49792/50048]	Loss: 0.3590
Training Epoch: 43 [49920/50048]	Loss: 0.4091
Training Epoch: 43 [50048/50048]	Loss: 0.3227
2022-12-06 09:43:33.920 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:43:33,971 [ZeusDataLoader(eval)] eval epoch 44 done: time=3.75 energy=455.04
2022-12-06 04:43:33,971 [ZeusDataLoader(train)] Up to epoch 44: time=3970.79, energy=481979.60, cost=588433.67
2022-12-06 04:43:33,972 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:43:33,972 [ZeusDataLoader(train)] Expected next epoch: time=4060.59, energy=492777.61, cost=601690.06
2022-12-06 04:43:33,973 [ZeusDataLoader(train)] Epoch 45 begin.
Validation Epoch: 43, Average loss: 0.0141, Accuracy: 0.6233
2022-12-06 04:43:34,141 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:43:34,142 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:43:34.158 [ZeusMonitor] Monitor started.
2022-12-06 09:43:34.158 [ZeusMonitor] Running indefinitely. 2022-12-06 09:43:34.158 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:43:34.158 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e45+gpu0.power.log
Training Epoch: 44 [128/50048]	Loss: 0.2889
Training Epoch: 44 [256/50048]	Loss: 0.2006
Training Epoch: 44 [384/50048]	Loss: 0.2538
Training Epoch: 44 [512/50048]	Loss: 0.2813
Training Epoch: 44 [640/50048]	Loss: 0.3268
Training Epoch: 44 [768/50048]	Loss: 0.2790
Training Epoch: 44 [896/50048]	Loss: 0.3417
Training Epoch: 44 [1024/50048]	Loss: 0.3304
Training Epoch: 44 [1152/50048]	Loss: 0.3205
Training Epoch: 44 [1280/50048]	Loss: 0.4577
Training Epoch: 44 [1408/50048]	Loss: 0.2818
Training Epoch: 44 [1536/50048]	Loss: 0.3792
Training Epoch: 44 [1664/50048]	Loss: 0.3531
Training Epoch: 44 [1792/50048]	Loss: 0.3480
Training Epoch: 44 [1920/50048]	Loss: 0.2672
Training Epoch: 44 [2048/50048]	Loss: 0.2662
Training Epoch: 44 [2176/50048]	Loss: 0.2235
Training Epoch: 44 [2304/50048]	Loss: 0.3568
Training Epoch: 44 [2432/50048]	Loss: 0.2973
Training Epoch: 44 [2560/50048]	Loss: 0.3178
Training Epoch: 44 [2688/50048]	Loss: 0.3633
Training Epoch: 44 [2816/50048]	Loss: 0.2642
Training Epoch: 44 [2944/50048]	Loss: 0.3614
Training Epoch: 44 [3072/50048]	Loss: 0.2817
Training Epoch: 44 [3200/50048]	Loss: 0.3414
Training Epoch: 44 [3328/50048]	Loss: 0.3052
Training Epoch: 44 [3456/50048]	Loss: 0.2127
Training Epoch: 44 [3584/50048]	Loss: 0.2907
Training Epoch: 44 [3712/50048]	Loss: 0.4212
Training Epoch: 44 [3840/50048]	Loss: 0.3350
Training Epoch: 44 [3968/50048]	Loss: 0.3961
Training Epoch: 44 [4096/50048]	Loss: 0.3134
Training Epoch: 44 [4224/50048]	Loss: 0.3154
Training Epoch: 44 [4352/50048]	Loss: 0.3201
Training Epoch: 44 [4480/50048]	Loss: 0.3492
Training Epoch: 44 [4608/50048]	Loss: 0.2668
Training Epoch: 44 [4736/50048]	Loss: 0.3138
Training Epoch: 44 [4864/50048]	Loss: 0.3741
Training Epoch: 44 [4992/50048]	Loss: 0.3635
Training Epoch: 44 [5120/50048]	Loss: 0.2764
Training Epoch: 44 [5248/50048]	Loss: 0.3330
Training Epoch: 44 [5376/50048]	Loss: 0.3016
Training Epoch: 44 [5504/50048]	Loss: 0.2722
Training Epoch: 44 [5632/50048]	Loss: 0.3350
Training Epoch: 44 [5760/50048]	Loss: 0.2952
Training Epoch: 44 [5888/50048]	Loss: 0.2010
Training Epoch: 44 [6016/50048]	Loss: 0.3020
Training Epoch: 44 [6144/50048]	Loss: 0.3394
Training Epoch: 44 [6272/50048]	Loss: 0.3799
Training Epoch: 44 [6400/50048]	Loss: 0.3842
Training Epoch: 44 [6528/50048]	Loss: 0.3750
Training Epoch: 44 [6656/50048]	Loss: 0.3408
Training Epoch: 44 [6784/50048]	Loss: 0.3190
Training Epoch: 44 [6912/50048]	Loss: 0.3206
Training Epoch: 44 [7040/50048]	Loss: 0.2631
Training Epoch: 44 [7168/50048]	Loss: 0.2810
Training Epoch: 44 [7296/50048]	Loss: 0.2627
Training Epoch: 44 [7424/50048]	Loss: 0.3435
Training Epoch: 44 [7552/50048]	Loss: 0.4106
Training Epoch: 44 [7680/50048]	Loss: 0.3071
Training Epoch: 44 [7808/50048]	Loss: 0.3105
Training Epoch: 44 [7936/50048]	Loss: 0.2853
Training Epoch: 44 [8064/50048]	Loss: 0.3511
Training Epoch: 44 [8192/50048]	Loss: 0.3445
Training Epoch: 44 [8320/50048]	Loss: 0.2590
Training Epoch: 44 [8448/50048]	Loss: 0.3316
Training Epoch: 44 [8576/50048]	Loss: 0.3593
Training Epoch: 44 [8704/50048]	Loss: 0.3345
Training Epoch: 44 [8832/50048]	Loss: 0.2990
Training Epoch: 44 [8960/50048]	Loss: 0.4011
Training Epoch: 44 [9088/50048]	Loss: 0.3693
Training Epoch: 44 [9216/50048]	Loss: 0.2858
Training Epoch: 44 [9344/50048]	Loss: 0.3518
Training Epoch: 44 [9472/50048]	Loss: 0.3552
Training Epoch: 44 [9600/50048]	Loss: 0.3918
Training Epoch: 44 [9728/50048]	Loss: 0.2077
Training Epoch: 44 [9856/50048]	Loss: 0.2453
Training Epoch: 44 [9984/50048]	Loss: 0.4432
Training Epoch: 44 [10112/50048]	Loss: 0.2868
Training Epoch: 44 [10240/50048]	Loss: 0.2398
Training Epoch: 44 [10368/50048]	Loss: 0.3033
Training Epoch: 44 [10496/50048]	Loss: 0.4229
Training Epoch: 44 [10624/50048]	Loss: 0.3543
Training Epoch: 44 [10752/50048]	Loss: 0.3160
Training Epoch: 44 [10880/50048]	Loss: 0.3062
Training Epoch: 44 [11008/50048]	Loss: 0.3631
Training Epoch: 44 [11136/50048]	Loss: 0.3236
Training Epoch: 44 [11264/50048]	Loss: 0.4181
Training Epoch: 44 [11392/50048]	Loss: 0.3761
Training Epoch: 44 [11520/50048]	Loss: 0.2986
Training Epoch: 44 [11648/50048]	Loss: 0.4244
Training Epoch: 44 [11776/50048]	Loss: 0.4051
Training Epoch: 44 [11904/50048]	Loss: 0.2524
Training Epoch: 44 [12032/50048]	Loss: 0.2711
Training Epoch: 44 [12160/50048]	Loss: 0.2230
Training Epoch: 44 [12288/50048]	Loss: 0.2442
Training Epoch: 44 [12416/50048]	Loss: 0.4136
Training Epoch: 44 [12544/50048]	Loss: 0.4463
Training Epoch: 44 [12672/50048]	Loss: 0.3202
Training Epoch: 44 [12800/50048]	Loss: 0.3233
Training Epoch: 44 [12928/50048]	Loss: 0.3713
Training Epoch: 44 [13056/50048]	Loss: 0.2838
Training Epoch: 44 [13184/50048]	Loss: 0.2793
Training Epoch: 44 [13312/50048]	Loss: 0.2473
Training Epoch: 44 [13440/50048]	Loss: 0.2944
Training Epoch: 44 [13568/50048]	Loss: 0.2545
Training Epoch: 44 [13696/50048]	Loss: 0.4585
Training Epoch: 44 [13824/50048]	Loss: 0.3053
Training Epoch: 44 [13952/50048]	Loss: 0.2742
Training Epoch: 44 [14080/50048]	Loss: 0.3264
Training Epoch: 44 [14208/50048]	Loss: 0.2265
Training Epoch: 44 [14336/50048]	Loss: 0.2329
Training Epoch: 44 [14464/50048]	Loss: 0.3755
Training Epoch: 44 [14592/50048]	Loss: 0.3053
Training Epoch: 44 [14720/50048]	Loss: 0.3700
Training Epoch: 44 [14848/50048]	Loss: 0.2822
Training Epoch: 44 [14976/50048]	Loss: 0.4054
Training Epoch: 44 [15104/50048]	Loss: 0.3230
Training Epoch: 44 [15232/50048]	Loss: 0.3452
Training Epoch: 44 [15360/50048]	Loss: 0.3323
Training Epoch: 44 [15488/50048]	Loss: 0.3814
Training Epoch: 44 [15616/50048]	Loss: 0.2965
Training Epoch: 44 [15744/50048]	Loss: 0.2400
Training Epoch: 44 [15872/50048]	Loss: 0.3382
Training Epoch: 44 [16000/50048]	Loss: 0.4146
Training Epoch: 44 [16128/50048]	Loss: 0.2785
Training Epoch: 44 [16256/50048]	Loss: 0.3655
Training Epoch: 44 [16384/50048]	Loss: 0.3018
Training Epoch: 44 [16512/50048]	Loss: 0.2917
Training Epoch: 44 [16640/50048]	Loss: 0.4030
Training Epoch: 44 [16768/50048]	Loss: 0.3199
Training Epoch: 44 [16896/50048]	Loss: 0.3822
Training Epoch: 44 [17024/50048]	Loss: 0.2910
Training Epoch: 44 [17152/50048]	Loss: 0.4388
Training Epoch: 44 [17280/50048]	Loss: 0.3094
Training Epoch: 44 [17408/50048]	Loss: 0.3481
Training Epoch: 44 [17536/50048]	Loss: 0.3207
Training Epoch: 44 [17664/50048]	Loss: 0.2949
Training Epoch: 44 [17792/50048]	Loss: 0.4037
Training Epoch: 44 [17920/50048]	Loss: 0.2183
Training Epoch: 44 [18048/50048]	Loss: 0.2484
Training Epoch: 44 [18176/50048]	Loss: 0.3252
Training Epoch: 44 [18304/50048]	Loss: 0.2934
Training Epoch: 44 [18432/50048]	Loss: 0.2780
Training Epoch: 44 [18560/50048]	Loss: 0.2517
Training Epoch: 44 [18688/50048]	Loss: 0.4125
Training Epoch: 44 [18816/50048]	Loss: 0.3695
Training Epoch: 44 [18944/50048]	Loss: 0.3525
Training Epoch: 44 [19072/50048]	Loss: 0.2838
Training Epoch: 44 [19200/50048]	Loss: 0.3915
Training Epoch: 44 [19328/50048]	Loss: 0.3454
Training Epoch: 44 [19456/50048]	Loss: 0.3267
Training Epoch: 44 [19584/50048]	Loss: 0.4308
Training Epoch: 44 [19712/50048]	Loss: 0.2088
Training Epoch: 44 [19840/50048]	Loss: 0.3345
Training Epoch: 44 [19968/50048]	Loss: 0.4180
Training Epoch: 44 [20096/50048]	Loss: 0.3328
Training Epoch: 44 [20224/50048]	Loss: 0.3250
Training Epoch: 44 [20352/50048]	Loss: 0.2877
Training Epoch: 44 [20480/50048]	Loss: 0.3091
Training Epoch: 44 [20608/50048]	Loss: 0.2947
Training Epoch: 44 [20736/50048]	Loss: 0.2510
Training Epoch: 44 [20864/50048]	Loss: 0.3937
Training Epoch: 44 [20992/50048]	Loss: 0.2390
Training Epoch: 44 [21120/50048]	Loss: 0.4233
Training Epoch: 44 [21248/50048]	Loss: 0.4143
Training Epoch: 44 [21376/50048]	Loss: 0.2849
Training Epoch: 44 [21504/50048]	Loss: 0.3085
Training Epoch: 44 [21632/50048]	Loss: 0.2932
Training Epoch: 44 [21760/50048]	Loss: 0.2054
Training Epoch: 44 [21888/50048]	Loss: 0.3834
Training Epoch: 44 [22016/50048]	Loss: 0.3632
Training Epoch: 44 [22144/50048]	Loss: 0.3735
Training Epoch: 44 [22272/50048]	Loss: 0.3689
Training Epoch: 44 [22400/50048]	Loss: 0.2808
Training Epoch: 44 [22528/50048]	Loss: 0.4356
Training Epoch: 44 [22656/50048]	Loss: 0.3509
Training Epoch: 44 [22784/50048]	Loss: 0.2560
Training Epoch: 44 [22912/50048]	Loss: 0.3218
Training Epoch: 44 [23040/50048]	Loss: 0.3517
Training Epoch: 44 [23168/50048]	Loss: 0.3285
Training Epoch: 44 [23296/50048]	Loss: 0.3158
Training Epoch: 44 [23424/50048]	Loss: 0.3463
Training Epoch: 44 [23552/50048]	Loss: 0.3998
Training Epoch: 44 [23680/50048]	Loss: 0.3549
Training Epoch: 44 [23808/50048]	Loss: 0.2857
Training Epoch: 44 [23936/50048]	Loss: 0.4166
Training Epoch: 44 [24064/50048]	Loss: 0.4194
Training Epoch: 44 [24192/50048]	Loss: 0.4045
Training Epoch: 44 [24320/50048]	Loss: 0.3066
Training Epoch: 44 [24448/50048]	Loss: 0.4007
Training Epoch: 44 [24576/50048]	Loss: 0.2717
Training Epoch: 44 [24704/50048]	Loss: 0.4267
Training Epoch: 44 [24832/50048]	Loss: 0.2313
Training Epoch: 44 [24960/50048]	Loss: 0.2927
Training Epoch: 44 [25088/50048]	Loss: 0.4408
Training Epoch: 44 [25216/50048]	Loss: 0.3039
Training Epoch: 44 [25344/50048]	Loss: 0.3218
Training Epoch: 44 [25472/50048]	Loss: 0.2365
Training Epoch: 44 [25600/50048]	Loss: 0.3428
Training Epoch: 44 [25728/50048]	Loss: 0.2642
Training Epoch: 44 [25856/50048]	Loss: 0.3877
Training Epoch: 44 [25984/50048]	Loss: 0.4292
Training Epoch: 44 [26112/50048]	Loss: 0.4060
Training Epoch: 44 [26240/50048]	Loss: 0.3960
Training Epoch: 44 [26368/50048]	Loss: 0.3393
Training Epoch: 44 [26496/50048]	Loss: 0.3295
Training Epoch: 44 [26624/50048]	Loss: 0.3490
Training Epoch: 44 [26752/50048]	Loss: 0.4398
Training Epoch: 44 [26880/50048]	Loss: 0.4735
Training Epoch: 44 [27008/50048]	Loss: 0.3392
Training Epoch: 44 [27136/50048]	Loss: 0.3440
Training Epoch: 44 [27264/50048]	Loss: 0.3311
Training Epoch: 44 [27392/50048]	Loss: 0.5016
Training Epoch: 44 [27520/50048]	Loss: 0.4644
Training Epoch: 44 [27648/50048]	Loss: 0.3112
Training Epoch: 44 [27776/50048]	Loss: 0.3386
Training Epoch: 44 [27904/50048]	Loss: 0.3088
Training Epoch: 44 [28032/50048]	Loss: 0.4571
Training Epoch: 44 [28160/50048]	Loss: 0.3844
Training Epoch: 44 [28288/50048]	Loss: 0.4220
Training Epoch: 44 [28416/50048]	Loss: 0.4787
Training Epoch: 44 [28544/50048]	Loss: 0.3457
Training Epoch: 44 [28672/50048]	Loss: 0.3813
Training Epoch: 44 [28800/50048]	Loss: 0.3464
Training Epoch: 44 [28928/50048]	Loss: 0.3374
Training Epoch: 44 [29056/50048]	Loss: 0.3624
Training Epoch: 44 [29184/50048]	Loss: 0.2795
Training Epoch: 44 [29312/50048]	Loss: 0.2934
Training Epoch: 44 [29440/50048]	Loss: 0.2901
Training Epoch: 44 [29568/50048]	Loss: 0.4220
Training Epoch: 44 [29696/50048]	Loss: 0.3180
Training Epoch: 44 [29824/50048]	Loss: 0.4074
Training Epoch: 44 [29952/50048]	Loss: 0.2102
Training Epoch: 44 [30080/50048]	Loss: 0.3294
Training Epoch: 44 [30208/50048]	Loss: 0.3834
Training Epoch: 44 [30336/50048]	Loss: 0.2704
Training Epoch: 44 [30464/50048]	Loss: 0.2802
Training Epoch: 44 [30592/50048]	Loss: 0.4094
Training Epoch: 44 [30720/50048]	Loss: 0.3367
Training Epoch: 44 [30848/50048]	Loss: 0.4105
Training Epoch: 44 [30976/50048]	Loss: 0.4121
Training Epoch: 44 [31104/50048]	Loss: 0.2558
Training Epoch: 44 [31232/50048]	Loss: 0.2921
Training Epoch: 44 [31360/50048]	Loss: 0.4010
Training Epoch: 44 [31488/50048]	Loss: 0.4262
Training Epoch: 44 [31616/50048]	Loss: 0.3840
Training Epoch: 44 [31744/50048]	Loss: 0.4207
Training Epoch: 44 [31872/50048]	Loss: 0.3313
Training Epoch: 44 [32000/50048]	Loss: 0.5490
Training Epoch: 44 [32128/50048]	Loss: 0.3316
Training Epoch: 44 [32256/50048]	Loss: 0.3312
Training Epoch: 44 [32384/50048]	Loss: 0.4051
Training Epoch: 44 [32512/50048]	Loss: 0.3593
Training Epoch: 44 [32640/50048]	Loss: 0.3753
Training Epoch: 44 [32768/50048]	Loss: 0.3379
Training Epoch: 44 [32896/50048]	Loss: 0.4254
Training Epoch: 44 [33024/50048]	Loss: 0.3235
Training Epoch: 44 [33152/50048]	Loss: 0.2696
Training Epoch: 44 [33280/50048]	Loss: 0.4650
Training Epoch: 44 [33408/50048]	Loss: 0.4332
Training Epoch: 44 [33536/50048]	Loss: 0.3603
Training Epoch: 44 [33664/50048]	Loss: 0.5215
Training Epoch: 44 [33792/50048]	Loss: 0.4053
Training Epoch: 44 [33920/50048]	Loss: 0.2724
Training Epoch: 44 [34048/50048]	Loss: 0.3934
Training Epoch: 44 [34176/50048]	Loss: 0.3529
Training Epoch: 44 [34304/50048]	Loss: 0.3796
Training Epoch: 44 [34432/50048]	Loss: 0.5130
Training Epoch: 44 [34560/50048]	Loss: 0.2437
Training Epoch: 44 [34688/50048]	Loss: 0.3116
Training Epoch: 44 [34816/50048]	Loss: 0.3595
Training Epoch: 44 [34944/50048]	Loss: 0.3720
Training Epoch: 44 [35072/50048]	Loss: 0.3525
Training Epoch: 44 [35200/50048]	Loss: 0.2902
Training Epoch: 44 [35328/50048]	Loss: 0.3534
Training Epoch: 44 [35456/50048]	Loss: 0.3933
Training Epoch: 44 [35584/50048]	Loss: 0.3514
Training Epoch: 44 [35712/50048]	Loss: 0.4276
Training Epoch: 44 [35840/50048]	Loss: 0.4385
Training Epoch: 44 [35968/50048]	Loss: 0.3690
Training Epoch: 44 [36096/50048]	Loss: 0.3757
Training Epoch: 44 [36224/50048]	Loss: 0.2636
Training Epoch: 44 [36352/50048]	Loss: 0.2818
Training Epoch: 44 [36480/50048]	Loss: 0.3981
Training Epoch: 44 [36608/50048]	Loss: 0.3768
Training Epoch: 44 [36736/50048]	Loss: 0.3893
Training Epoch: 44 [36864/50048]	Loss: 0.4336
Training Epoch: 44 [36992/50048]	Loss: 0.4022
Training Epoch: 44 [37120/50048]	Loss: 0.4192
Training Epoch: 44 [37248/50048]	Loss: 0.3582
Training Epoch: 44 [37376/50048]	Loss: 0.4276
Training Epoch: 44 [37504/50048]	Loss: 0.4216
Training Epoch: 44 [37632/50048]	Loss: 0.3961
Training Epoch: 44 [37760/50048]	Loss: 0.2986
Training Epoch: 44 [37888/50048]	Loss: 0.5931
Training Epoch: 44 [38016/50048]	Loss: 0.2398
Training Epoch: 44 [38144/50048]	Loss: 0.3461
Training Epoch: 44 [38272/50048]	Loss: 0.3869
Training Epoch: 44 [38400/50048]	Loss: 0.3672
Training Epoch: 44 [38528/50048]	Loss: 0.3509
Training Epoch: 44 [38656/50048]	Loss: 0.3697
Training Epoch: 44 [38784/50048]	Loss: 0.3820
Training Epoch: 44 [38912/50048]	Loss: 0.3372
Training Epoch: 44 [39040/50048]	Loss: 0.3315
Training Epoch: 44 [39168/50048]	Loss: 0.4508
Training Epoch: 44 [39296/50048]	Loss: 0.2952
Training Epoch: 44 [39424/50048]	Loss: 0.3573
Training Epoch: 44 [39552/50048]	Loss: 0.3663
Training Epoch: 44 [39680/50048]	Loss: 0.3611
Training Epoch: 44 [39808/50048]	Loss: 0.2849
Training Epoch: 44 [39936/50048]	Loss: 0.5742
Training Epoch: 44 [40064/50048]	Loss: 0.4347
Training Epoch: 44 [40192/50048]	Loss: 0.3283
Training Epoch: 44 [40320/50048]	Loss: 0.4568
Training Epoch: 44 [40448/50048]	Loss: 0.4325
Training Epoch: 44 [40576/50048]	Loss: 0.4991
Training Epoch: 44 [40704/50048]	Loss: 0.2839
Training Epoch: 44 [40832/50048]	Loss: 0.2713
Training Epoch: 44 [40960/50048]	Loss: 0.3952
Training Epoch: 44 [41088/50048]	Loss: 0.2373
Training Epoch: 44 [41216/50048]	Loss: 0.3060
Training Epoch: 44 [41344/50048]	Loss: 0.3203
Training Epoch: 44 [41472/50048]	Loss: 0.4040
Training Epoch: 44 [41600/50048]	Loss: 0.2668
Training Epoch: 44 [41728/50048]	Loss: 0.2901
Training Epoch: 44 [41856/50048]	Loss: 0.3566
Training Epoch: 44 [41984/50048]	Loss: 0.4216
Training Epoch: 44 [42112/50048]	Loss: 0.3752
Training Epoch: 44 [42240/50048]	Loss: 0.3550
Training Epoch: 44 [42368/50048]	Loss: 0.3561
Training Epoch: 44 [42496/50048]	Loss: 0.2957
Training Epoch: 44 [42624/50048]	Loss: 0.4032
Training Epoch: 44 [42752/50048]	Loss: 0.2093
Training Epoch: 44 [42880/50048]	Loss: 0.3379
Training Epoch: 44 [43008/50048]	Loss: 0.3352
Training Epoch: 44 [43136/50048]	Loss: 0.4693
Training Epoch: 44 [43264/50048]	Loss: 0.3469
Training Epoch: 44 [43392/50048]	Loss: 0.2434
Training Epoch: 44 [43520/50048]	Loss: 0.3505
Training Epoch: 44 [43648/50048]	Loss: 0.4788
Training Epoch: 44 [43776/50048]	Loss: 0.2792
Training Epoch: 44 [43904/50048]	Loss: 0.4589
Training Epoch: 44 [44032/50048]	Loss: 0.4385
Training Epoch: 44 [44160/50048]	Loss: 0.3253
Training Epoch: 44 [44288/50048]	Loss: 0.4292
Training Epoch: 44 [44416/50048]	Loss: 0.3889
Training Epoch: 44 [44544/50048]	Loss: 0.4236
Training Epoch: 44 [44672/50048]	Loss: 0.3060
Training Epoch: 44 [44800/50048]	Loss: 0.3072
Training Epoch: 44 [44928/50048]	Loss: 0.3995
Training Epoch: 44 [45056/50048]	Loss: 0.3960
Training Epoch: 44 [45184/50048]	Loss: 0.3679
Training Epoch: 44 [45312/50048]	Loss: 0.5298
Training Epoch: 44 [45440/50048]	Loss: 0.3510
Training Epoch: 44 [45568/50048]	Loss: 0.4093
Training Epoch: 44 [45696/50048]	Loss: 0.3686
2022-12-06 04:45:00,510 [ZeusDataLoader(train)] train epoch 45 done: time=86.53 energy=10499.76
2022-12-06 04:45:00,512 [ZeusDataLoader(eval)] Epoch 45 begin.
Training Epoch: 44 [45824/50048]	Loss: 0.3341
Training Epoch: 44 [45952/50048]	Loss: 0.3013
Training Epoch: 44 [46080/50048]	Loss: 0.4376
Training Epoch: 44 [46208/50048]	Loss: 0.3485
Training Epoch: 44 [46336/50048]	Loss: 0.3383
Training Epoch: 44 [46464/50048]	Loss: 0.3836
Training Epoch: 44 [46592/50048]	Loss: 0.3785
Training Epoch: 44 [46720/50048]	Loss: 0.4774
Training Epoch: 44 [46848/50048]	Loss: 0.4310
Training Epoch: 44 [46976/50048]	Loss: 0.4321
Training Epoch: 44 [47104/50048]	Loss: 0.3224
Training Epoch: 44 [47232/50048]	Loss: 0.4263
Training Epoch: 44 [47360/50048]	Loss: 0.4832
Training Epoch: 44 [47488/50048]	Loss: 0.2758
Training Epoch: 44 [47616/50048]	Loss: 0.3491
Training Epoch: 44 [47744/50048]	Loss: 0.4064
Training Epoch: 44 [47872/50048]	Loss: 0.3015
Training Epoch: 44 [48000/50048]	Loss: 0.3732
Training Epoch: 44 [48128/50048]	Loss: 0.3129
Training Epoch: 44 [48256/50048]	Loss: 0.4551
Training Epoch: 44 [48384/50048]	Loss: 0.3774
Training Epoch: 44 [48512/50048]	Loss: 0.3058
Training Epoch: 44 [48640/50048]	Loss: 0.3237
Training Epoch: 44 [48768/50048]	Loss: 0.5035
Training Epoch: 44 [48896/50048]	Loss: 0.4916
Training Epoch: 44 [49024/50048]	Loss: 0.4846
Training Epoch: 44 [49152/50048]	Loss: 0.3380
Training Epoch: 44 [49280/50048]	Loss: 0.3680
Training Epoch: 44 [49408/50048]	Loss: 0.3989
Training Epoch: 44 [49536/50048]	Loss: 0.3338
Training Epoch: 44 [49664/50048]	Loss: 0.4115
Training Epoch: 44 [49792/50048]	Loss: 0.4158
Training Epoch: 44 [49920/50048]	Loss: 0.3310
Training Epoch: 44 [50048/50048]	Loss: 0.3140
2022-12-06 09:45:04.171 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:45:04,181 [ZeusDataLoader(eval)] eval epoch 45 done: time=3.66 energy=440.82
2022-12-06 04:45:04,181 [ZeusDataLoader(train)] Up to epoch 45: time=4060.98, energy=492920.18, cost=601795.43
2022-12-06 04:45:04,182 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:45:04,182 [ZeusDataLoader(train)] Expected next epoch: time=4150.77, energy=503718.19, cost=615051.81
2022-12-06 04:45:04,182 [ZeusDataLoader(train)] Epoch 46 begin.
Validation Epoch: 44, Average loss: 0.0146, Accuracy: 0.6259
2022-12-06 04:45:04,364 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:45:04,364 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:45:04.366 [ZeusMonitor] Monitor started.
2022-12-06 09:45:04.366 [ZeusMonitor] Running indefinitely. 2022-12-06 09:45:04.366 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:45:04.366 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e46+gpu0.power.log
Training Epoch: 45 [128/50048]	Loss: 0.3613
Training Epoch: 45 [256/50048]	Loss: 0.4054
Training Epoch: 45 [384/50048]	Loss: 0.3013
Training Epoch: 45 [512/50048]	Loss: 0.3739
Training Epoch: 45 [640/50048]	Loss: 0.2606
Training Epoch: 45 [768/50048]	Loss: 0.3009
Training Epoch: 45 [896/50048]	Loss: 0.2292
Training Epoch: 45 [1024/50048]	Loss: 0.2833
Training Epoch: 45 [1152/50048]	Loss: 0.3886
Training Epoch: 45 [1280/50048]	Loss: 0.4656
Training Epoch: 45 [1408/50048]	Loss: 0.4260
Training Epoch: 45 [1536/50048]	Loss: 0.2364
Training Epoch: 45 [1664/50048]	Loss: 0.3087
Training Epoch: 45 [1792/50048]	Loss: 0.2824
Training Epoch: 45 [1920/50048]	Loss: 0.2591
Training Epoch: 45 [2048/50048]	Loss: 0.2992
Training Epoch: 45 [2176/50048]	Loss: 0.1748
Training Epoch: 45 [2304/50048]	Loss: 0.2199
Training Epoch: 45 [2432/50048]	Loss: 0.2512
Training Epoch: 45 [2560/50048]	Loss: 0.2066
Training Epoch: 45 [2688/50048]	Loss: 0.2911
Training Epoch: 45 [2816/50048]	Loss: 0.3658
Training Epoch: 45 [2944/50048]	Loss: 0.2184
Training Epoch: 45 [3072/50048]	Loss: 0.1625
Training Epoch: 45 [3200/50048]	Loss: 0.3489
Training Epoch: 45 [3328/50048]	Loss: 0.2866
Training Epoch: 45 [3456/50048]	Loss: 0.2245
Training Epoch: 45 [3584/50048]	Loss: 0.1901
Training Epoch: 45 [3712/50048]	Loss: 0.3987
Training Epoch: 45 [3840/50048]	Loss: 0.4234
Training Epoch: 45 [3968/50048]	Loss: 0.3038
Training Epoch: 45 [4096/50048]	Loss: 0.3178
Training Epoch: 45 [4224/50048]	Loss: 0.1918
Training Epoch: 45 [4352/50048]	Loss: 0.3422
Training Epoch: 45 [4480/50048]	Loss: 0.3332
Training Epoch: 45 [4608/50048]	Loss: 0.3268
Training Epoch: 45 [4736/50048]	Loss: 0.3075
Training Epoch: 45 [4864/50048]	Loss: 0.2328
Training Epoch: 45 [4992/50048]	Loss: 0.2623
Training Epoch: 45 [5120/50048]	Loss: 0.4804
Training Epoch: 45 [5248/50048]	Loss: 0.2726
Training Epoch: 45 [5376/50048]	Loss: 0.2376
Training Epoch: 45 [5504/50048]	Loss: 0.2414
Training Epoch: 45 [5632/50048]	Loss: 0.3189
Training Epoch: 45 [5760/50048]	Loss: 0.3180
Training Epoch: 45 [5888/50048]	Loss: 0.2925
Training Epoch: 45 [6016/50048]	Loss: 0.2340
Training Epoch: 45 [6144/50048]	Loss: 0.4136
Training Epoch: 45 [6272/50048]	Loss: 0.2374
Training Epoch: 45 [6400/50048]	Loss: 0.2565
Training Epoch: 45 [6528/50048]	Loss: 0.3520
Training Epoch: 45 [6656/50048]	Loss: 0.3223
Training Epoch: 45 [6784/50048]	Loss: 0.3563
Training Epoch: 45 [6912/50048]	Loss: 0.3264
Training Epoch: 45 [7040/50048]	Loss: 0.3006
Training Epoch: 45 [7168/50048]	Loss: 0.3662
Training Epoch: 45 [7296/50048]	Loss: 0.3278
Training Epoch: 45 [7424/50048]	Loss: 0.2064
Training Epoch: 45 [7552/50048]	Loss: 0.3256
Training Epoch: 45 [7680/50048]	Loss: 0.4236
Training Epoch: 45 [7808/50048]	Loss: 0.3418
Training Epoch: 45 [7936/50048]	Loss: 0.3578
Training Epoch: 45 [8064/50048]	Loss: 0.2909
Training Epoch: 45 [8192/50048]	Loss: 0.3427
Training Epoch: 45 [8320/50048]	Loss: 0.2482
Training Epoch: 45 [8448/50048]	Loss: 0.3332
Training Epoch: 45 [8576/50048]	Loss: 0.3381
Training Epoch: 45 [8704/50048]	Loss: 0.3441
Training Epoch: 45 [8832/50048]	Loss: 0.2929
Training Epoch: 45 [8960/50048]	Loss: 0.2839
Training Epoch: 45 [9088/50048]	Loss: 0.3200
Training Epoch: 45 [9216/50048]	Loss: 0.2971
Training Epoch: 45 [9344/50048]	Loss: 0.2916
Training Epoch: 45 [9472/50048]	Loss: 0.4046
Training Epoch: 45 [9600/50048]	Loss: 0.3825
Training Epoch: 45 [9728/50048]	Loss: 0.3221
Training Epoch: 45 [9856/50048]	Loss: 0.4323
Training Epoch: 45 [9984/50048]	Loss: 0.3440
Training Epoch: 45 [10112/50048]	Loss: 0.3689
Training Epoch: 45 [10240/50048]	Loss: 0.4297
Training Epoch: 45 [10368/50048]	Loss: 0.3786
Training Epoch: 45 [10496/50048]	Loss: 0.2055
Training Epoch: 45 [10624/50048]	Loss: 0.2539
Training Epoch: 45 [10752/50048]	Loss: 0.3688
Training Epoch: 45 [10880/50048]	Loss: 0.3012
Training Epoch: 45 [11008/50048]	Loss: 0.3489
Training Epoch: 45 [11136/50048]	Loss: 0.3922
Training Epoch: 45 [11264/50048]	Loss: 0.3823
Training Epoch: 45 [11392/50048]	Loss: 0.3938
Training Epoch: 45 [11520/50048]	Loss: 0.3296
Training Epoch: 45 [11648/50048]	Loss: 0.2801
Training Epoch: 45 [11776/50048]	Loss: 0.4016
Training Epoch: 45 [11904/50048]	Loss: 0.1644
Training Epoch: 45 [12032/50048]	Loss: 0.2986
Training Epoch: 45 [12160/50048]	Loss: 0.2385
Training Epoch: 45 [12288/50048]	Loss: 0.2973
Training Epoch: 45 [12416/50048]	Loss: 0.4577
Training Epoch: 45 [12544/50048]	Loss: 0.2522
Training Epoch: 45 [12672/50048]	Loss: 0.3341
Training Epoch: 45 [12800/50048]	Loss: 0.2392
Training Epoch: 45 [12928/50048]	Loss: 0.2824
Training Epoch: 45 [13056/50048]	Loss: 0.3169
Training Epoch: 45 [13184/50048]	Loss: 0.3116
Training Epoch: 45 [13312/50048]	Loss: 0.3006
Training Epoch: 45 [13440/50048]	Loss: 0.2868
Training Epoch: 45 [13568/50048]	Loss: 0.2660
Training Epoch: 45 [13696/50048]	Loss: 0.2944
Training Epoch: 45 [13824/50048]	Loss: 0.2873
Training Epoch: 45 [13952/50048]	Loss: 0.2171
Training Epoch: 45 [14080/50048]	Loss: 0.3066
Training Epoch: 45 [14208/50048]	Loss: 0.3207
Training Epoch: 45 [14336/50048]	Loss: 0.3662
Training Epoch: 45 [14464/50048]	Loss: 0.2599
Training Epoch: 45 [14592/50048]	Loss: 0.3692
Training Epoch: 45 [14720/50048]	Loss: 0.3164
Training Epoch: 45 [14848/50048]	Loss: 0.3578
Training Epoch: 45 [14976/50048]	Loss: 0.3063
Training Epoch: 45 [15104/50048]	Loss: 0.3320
Training Epoch: 45 [15232/50048]	Loss: 0.2879
Training Epoch: 45 [15360/50048]	Loss: 0.4151
Training Epoch: 45 [15488/50048]	Loss: 0.4440
Training Epoch: 45 [15616/50048]	Loss: 0.2848
Training Epoch: 45 [15744/50048]	Loss: 0.4756
Training Epoch: 45 [15872/50048]	Loss: 0.4538
Training Epoch: 45 [16000/50048]	Loss: 0.2453
Training Epoch: 45 [16128/50048]	Loss: 0.2904
Training Epoch: 45 [16256/50048]	Loss: 0.2478
Training Epoch: 45 [16384/50048]	Loss: 0.2973
Training Epoch: 45 [16512/50048]	Loss: 0.2963
Training Epoch: 45 [16640/50048]	Loss: 0.2357
Training Epoch: 45 [16768/50048]	Loss: 0.3446
Training Epoch: 45 [16896/50048]	Loss: 0.2991
Training Epoch: 45 [17024/50048]	Loss: 0.2478
Training Epoch: 45 [17152/50048]	Loss: 0.2982
Training Epoch: 45 [17280/50048]	Loss: 0.2397
Training Epoch: 45 [17408/50048]	Loss: 0.3396
Training Epoch: 45 [17536/50048]	Loss: 0.2930
Training Epoch: 45 [17664/50048]	Loss: 0.3307
Training Epoch: 45 [17792/50048]	Loss: 0.2366
Training Epoch: 45 [17920/50048]	Loss: 0.3519
Training Epoch: 45 [18048/50048]	Loss: 0.3433
Training Epoch: 45 [18176/50048]	Loss: 0.3885
Training Epoch: 45 [18304/50048]	Loss: 0.1974
Training Epoch: 45 [18432/50048]	Loss: 0.2356
Training Epoch: 45 [18560/50048]	Loss: 0.3344
Training Epoch: 45 [18688/50048]	Loss: 0.2810
Training Epoch: 45 [18816/50048]	Loss: 0.3975
Training Epoch: 45 [18944/50048]	Loss: 0.4673
Training Epoch: 45 [19072/50048]	Loss: 0.2953
Training Epoch: 45 [19200/50048]	Loss: 0.3148
Training Epoch: 45 [19328/50048]	Loss: 0.2652
Training Epoch: 45 [19456/50048]	Loss: 0.3099
Training Epoch: 45 [19584/50048]	Loss: 0.3156
Training Epoch: 45 [19712/50048]	Loss: 0.3473
Training Epoch: 45 [19840/50048]	Loss: 0.4032
Training Epoch: 45 [19968/50048]	Loss: 0.3215
Training Epoch: 45 [20096/50048]	Loss: 0.2193
Training Epoch: 45 [20224/50048]	Loss: 0.2744
Training Epoch: 45 [20352/50048]	Loss: 0.3623
Training Epoch: 45 [20480/50048]	Loss: 0.4125
Training Epoch: 45 [20608/50048]	Loss: 0.2550
Training Epoch: 45 [20736/50048]	Loss: 0.3077
Training Epoch: 45 [20864/50048]	Loss: 0.2564
Training Epoch: 45 [20992/50048]	Loss: 0.3429
Training Epoch: 45 [21120/50048]	Loss: 0.3017
Training Epoch: 45 [21248/50048]	Loss: 0.4615
Training Epoch: 45 [21376/50048]	Loss: 0.2812
Training Epoch: 45 [21504/50048]	Loss: 0.3943
Training Epoch: 45 [21632/50048]	Loss: 0.2363
Training Epoch: 45 [21760/50048]	Loss: 0.3017
Training Epoch: 45 [21888/50048]	Loss: 0.2736
Training Epoch: 45 [22016/50048]	Loss: 0.3349
Training Epoch: 45 [22144/50048]	Loss: 0.4133
Training Epoch: 45 [22272/50048]	Loss: 0.3685
Training Epoch: 45 [22400/50048]	Loss: 0.2884
Training Epoch: 45 [22528/50048]	Loss: 0.2967
Training Epoch: 45 [22656/50048]	Loss: 0.3328
Training Epoch: 45 [22784/50048]	Loss: 0.3432
Training Epoch: 45 [22912/50048]	Loss: 0.2424
Training Epoch: 45 [23040/50048]	Loss: 0.4110
Training Epoch: 45 [23168/50048]	Loss: 0.3918
Training Epoch: 45 [23296/50048]	Loss: 0.2702
Training Epoch: 45 [23424/50048]	Loss: 0.3406
Training Epoch: 45 [23552/50048]	Loss: 0.3209
Training Epoch: 45 [23680/50048]	Loss: 0.3993
Training Epoch: 45 [23808/50048]	Loss: 0.2911
Training Epoch: 45 [23936/50048]	Loss: 0.2232
Training Epoch: 45 [24064/50048]	Loss: 0.3794
Training Epoch: 45 [24192/50048]	Loss: 0.3864
Training Epoch: 45 [24320/50048]	Loss: 0.3273
Training Epoch: 45 [24448/50048]	Loss: 0.4034
Training Epoch: 45 [24576/50048]	Loss: 0.2501
Training Epoch: 45 [24704/50048]	Loss: 0.4159
Training Epoch: 45 [24832/50048]	Loss: 0.2437
Training Epoch: 45 [24960/50048]	Loss: 0.3584
Training Epoch: 45 [25088/50048]	Loss: 0.2916
Training Epoch: 45 [25216/50048]	Loss: 0.3519
Training Epoch: 45 [25344/50048]	Loss: 0.2556
Training Epoch: 45 [25472/50048]	Loss: 0.2557
Training Epoch: 45 [25600/50048]	Loss: 0.3236
Training Epoch: 45 [25728/50048]	Loss: 0.3409
Training Epoch: 45 [25856/50048]	Loss: 0.4283
Training Epoch: 45 [25984/50048]	Loss: 0.3675
Training Epoch: 45 [26112/50048]	Loss: 0.3676
Training Epoch: 45 [26240/50048]	Loss: 0.4595
Training Epoch: 45 [26368/50048]	Loss: 0.4648
Training Epoch: 45 [26496/50048]	Loss: 0.3561
Training Epoch: 45 [26624/50048]	Loss: 0.3285
Training Epoch: 45 [26752/50048]	Loss: 0.2546
Training Epoch: 45 [26880/50048]	Loss: 0.3022
Training Epoch: 45 [27008/50048]	Loss: 0.3623
Training Epoch: 45 [27136/50048]	Loss: 0.2755
Training Epoch: 45 [27264/50048]	Loss: 0.3267
Training Epoch: 45 [27392/50048]	Loss: 0.3986
Training Epoch: 45 [27520/50048]	Loss: 0.3291
Training Epoch: 45 [27648/50048]	Loss: 0.4134
Training Epoch: 45 [27776/50048]	Loss: 0.5524
Training Epoch: 45 [27904/50048]	Loss: 0.3759
Training Epoch: 45 [28032/50048]	Loss: 0.3473
Training Epoch: 45 [28160/50048]	Loss: 0.2963
Training Epoch: 45 [28288/50048]	Loss: 0.3526
Training Epoch: 45 [28416/50048]	Loss: 0.3677
Training Epoch: 45 [28544/50048]	Loss: 0.2207
Training Epoch: 45 [28672/50048]	Loss: 0.2878
Training Epoch: 45 [28800/50048]	Loss: 0.2920
Training Epoch: 45 [28928/50048]	Loss: 0.2783
Training Epoch: 45 [29056/50048]	Loss: 0.3354
Training Epoch: 45 [29184/50048]	Loss: 0.3131
Training Epoch: 45 [29312/50048]	Loss: 0.4162
Training Epoch: 45 [29440/50048]	Loss: 0.2736
Training Epoch: 45 [29568/50048]	Loss: 0.2289
Training Epoch: 45 [29696/50048]	Loss: 0.4662
Training Epoch: 45 [29824/50048]	Loss: 0.3611
Training Epoch: 45 [29952/50048]	Loss: 0.3393
Training Epoch: 45 [30080/50048]	Loss: 0.4009
Training Epoch: 45 [30208/50048]	Loss: 0.3905
Training Epoch: 45 [30336/50048]	Loss: 0.3208
Training Epoch: 45 [30464/50048]	Loss: 0.3140
Training Epoch: 45 [30592/50048]	Loss: 0.4081
Training Epoch: 45 [30720/50048]	Loss: 0.3448
Training Epoch: 45 [30848/50048]	Loss: 0.3660
Training Epoch: 45 [30976/50048]	Loss: 0.2022
Training Epoch: 45 [31104/50048]	Loss: 0.3626
Training Epoch: 45 [31232/50048]	Loss: 0.3440
Training Epoch: 45 [31360/50048]	Loss: 0.3702
Training Epoch: 45 [31488/50048]	Loss: 0.2258
Training Epoch: 45 [31616/50048]	Loss: 0.3328
Training Epoch: 45 [31744/50048]	Loss: 0.4223
Training Epoch: 45 [31872/50048]	Loss: 0.3479
Training Epoch: 45 [32000/50048]	Loss: 0.3003
Training Epoch: 45 [32128/50048]	Loss: 0.2332
Training Epoch: 45 [32256/50048]	Loss: 0.3217
Training Epoch: 45 [32384/50048]	Loss: 0.4464
Training Epoch: 45 [32512/50048]	Loss: 0.2669
Training Epoch: 45 [32640/50048]	Loss: 0.3629
Training Epoch: 45 [32768/50048]	Loss: 0.3787
Training Epoch: 45 [32896/50048]	Loss: 0.3492
Training Epoch: 45 [33024/50048]	Loss: 0.2926
Training Epoch: 45 [33152/50048]	Loss: 0.2743
Training Epoch: 45 [33280/50048]	Loss: 0.4114
Training Epoch: 45 [33408/50048]	Loss: 0.3837
Training Epoch: 45 [33536/50048]	Loss: 0.3557
Training Epoch: 45 [33664/50048]	Loss: 0.4444
Training Epoch: 45 [33792/50048]	Loss: 0.3187
Training Epoch: 45 [33920/50048]	Loss: 0.2891
Training Epoch: 45 [34048/50048]	Loss: 0.4233
Training Epoch: 45 [34176/50048]	Loss: 0.3873
Training Epoch: 45 [34304/50048]	Loss: 0.3203
Training Epoch: 45 [34432/50048]	Loss: 0.3122
Training Epoch: 45 [34560/50048]	Loss: 0.3359
Training Epoch: 45 [34688/50048]	Loss: 0.2785
Training Epoch: 45 [34816/50048]	Loss: 0.3888
Training Epoch: 45 [34944/50048]	Loss: 0.3372
Training Epoch: 45 [35072/50048]	Loss: 0.3454
Training Epoch: 45 [35200/50048]	Loss: 0.3907
Training Epoch: 45 [35328/50048]	Loss: 0.3964
Training Epoch: 45 [35456/50048]	Loss: 0.2641
Training Epoch: 45 [35584/50048]	Loss: 0.4887
Training Epoch: 45 [35712/50048]	Loss: 0.4605
Training Epoch: 45 [35840/50048]	Loss: 0.4918
Training Epoch: 45 [35968/50048]	Loss: 0.3507
Training Epoch: 45 [36096/50048]	Loss: 0.3234
Training Epoch: 45 [36224/50048]	Loss: 0.3707
Training Epoch: 45 [36352/50048]	Loss: 0.3438
Training Epoch: 45 [36480/50048]	Loss: 0.3440
Training Epoch: 45 [36608/50048]	Loss: 0.3985
Training Epoch: 45 [36736/50048]	Loss: 0.4002
Training Epoch: 45 [36864/50048]	Loss: 0.3184
Training Epoch: 45 [36992/50048]	Loss: 0.3622
Training Epoch: 45 [37120/50048]	Loss: 0.2967
Training Epoch: 45 [37248/50048]	Loss: 0.3465
Training Epoch: 45 [37376/50048]	Loss: 0.3715
Training Epoch: 45 [37504/50048]	Loss: 0.3668
Training Epoch: 45 [37632/50048]	Loss: 0.3429
Training Epoch: 45 [37760/50048]	Loss: 0.3841
Training Epoch: 45 [37888/50048]	Loss: 0.3858
Training Epoch: 45 [38016/50048]	Loss: 0.3682
Training Epoch: 45 [38144/50048]	Loss: 0.2949
Training Epoch: 45 [38272/50048]	Loss: 0.2751
Training Epoch: 45 [38400/50048]	Loss: 0.2717
Training Epoch: 45 [38528/50048]	Loss: 0.3561
Training Epoch: 45 [38656/50048]	Loss: 0.4372
Training Epoch: 45 [38784/50048]	Loss: 0.2516
Training Epoch: 45 [38912/50048]	Loss: 0.3880
Training Epoch: 45 [39040/50048]	Loss: 0.3227
Training Epoch: 45 [39168/50048]	Loss: 0.2966
Training Epoch: 45 [39296/50048]	Loss: 0.4580
Training Epoch: 45 [39424/50048]	Loss: 0.2712
Training Epoch: 45 [39552/50048]	Loss: 0.3585
Training Epoch: 45 [39680/50048]	Loss: 0.3956
Training Epoch: 45 [39808/50048]	Loss: 0.4282
Training Epoch: 45 [39936/50048]	Loss: 0.4213
Training Epoch: 45 [40064/50048]	Loss: 0.3626
Training Epoch: 45 [40192/50048]	Loss: 0.3442
Training Epoch: 45 [40320/50048]	Loss: 0.2805
Training Epoch: 45 [40448/50048]	Loss: 0.2871
Training Epoch: 45 [40576/50048]	Loss: 0.4105
Training Epoch: 45 [40704/50048]	Loss: 0.3450
Training Epoch: 45 [40832/50048]	Loss: 0.3137
Training Epoch: 45 [40960/50048]	Loss: 0.4229
Training Epoch: 45 [41088/50048]	Loss: 0.4207
Training Epoch: 45 [41216/50048]	Loss: 0.4276
Training Epoch: 45 [41344/50048]	Loss: 0.4300
Training Epoch: 45 [41472/50048]	Loss: 0.4612
Training Epoch: 45 [41600/50048]	Loss: 0.3264
Training Epoch: 45 [41728/50048]	Loss: 0.1822
Training Epoch: 45 [41856/50048]	Loss: 0.2732
Training Epoch: 45 [41984/50048]	Loss: 0.3437
Training Epoch: 45 [42112/50048]	Loss: 0.3580
Training Epoch: 45 [42240/50048]	Loss: 0.3065
Training Epoch: 45 [42368/50048]	Loss: 0.4365
Training Epoch: 45 [42496/50048]	Loss: 0.3279
Training Epoch: 45 [42624/50048]	Loss: 0.2854
Training Epoch: 45 [42752/50048]	Loss: 0.3153
Training Epoch: 45 [42880/50048]	Loss: 0.4054
Training Epoch: 45 [43008/50048]	Loss: 0.4459
Training Epoch: 45 [43136/50048]	Loss: 0.2793
Training Epoch: 45 [43264/50048]	Loss: 0.4062
Training Epoch: 45 [43392/50048]	Loss: 0.3459
Training Epoch: 45 [43520/50048]	Loss: 0.3061
Training Epoch: 45 [43648/50048]	Loss: 0.3527
Training Epoch: 45 [43776/50048]	Loss: 0.2497
Training Epoch: 45 [43904/50048]	Loss: 0.4312
Training Epoch: 45 [44032/50048]	Loss: 0.4464
Training Epoch: 45 [44160/50048]	Loss: 0.3175
Training Epoch: 45 [44288/50048]	Loss: 0.4189
Training Epoch: 45 [44416/50048]	Loss: 0.4153
Training Epoch: 45 [44544/50048]	Loss: 0.2672
Training Epoch: 45 [44672/50048]	Loss: 0.4537
Training Epoch: 45 [44800/50048]	Loss: 0.2975
Training Epoch: 45 [44928/50048]	Loss: 0.3431
Training Epoch: 45 [45056/50048]	Loss: 0.3521
Training Epoch: 45 [45184/50048]	Loss: 0.3021
Training Epoch: 45 [45312/50048]	Loss: 0.3251
Training Epoch: 45 [45440/50048]	Loss: 0.4468
Training Epoch: 45 [45568/50048]	Loss: 0.4232
Training Epoch: 45 [45696/50048]	Loss: 0.3706
2022-12-06 04:46:30,672 [ZeusDataLoader(train)] train epoch 46 done: time=86.48 energy=10497.42
2022-12-06 04:46:30,673 [ZeusDataLoader(eval)] Epoch 46 begin.
Training Epoch: 45 [45824/50048]	Loss: 0.3812
Training Epoch: 45 [45952/50048]	Loss: 0.4004
Training Epoch: 45 [46080/50048]	Loss: 0.4325
Training Epoch: 45 [46208/50048]	Loss: 0.4423
Training Epoch: 45 [46336/50048]	Loss: 0.3092
Training Epoch: 45 [46464/50048]	Loss: 0.4552
Training Epoch: 45 [46592/50048]	Loss: 0.3229
Training Epoch: 45 [46720/50048]	Loss: 0.4406
Training Epoch: 45 [46848/50048]	Loss: 0.3327
Training Epoch: 45 [46976/50048]	Loss: 0.2587
Training Epoch: 45 [47104/50048]	Loss: 0.4062
Training Epoch: 45 [47232/50048]	Loss: 0.2180
Training Epoch: 45 [47360/50048]	Loss: 0.2803
Training Epoch: 45 [47488/50048]	Loss: 0.3212
Training Epoch: 45 [47616/50048]	Loss: 0.3700
Training Epoch: 45 [47744/50048]	Loss: 0.4453
Training Epoch: 45 [47872/50048]	Loss: 0.2884
Training Epoch: 45 [48000/50048]	Loss: 0.2969
Training Epoch: 45 [48128/50048]	Loss: 0.4137
Training Epoch: 45 [48256/50048]	Loss: 0.2829
Training Epoch: 45 [48384/50048]	Loss: 0.3255
Training Epoch: 45 [48512/50048]	Loss: 0.3815
Training Epoch: 45 [48640/50048]	Loss: 0.4630
Training Epoch: 45 [48768/50048]	Loss: 0.5067
Training Epoch: 45 [48896/50048]	Loss: 0.3037
Training Epoch: 45 [49024/50048]	Loss: 0.4220
Training Epoch: 45 [49152/50048]	Loss: 0.3006
Training Epoch: 45 [49280/50048]	Loss: 0.3418
Training Epoch: 45 [49408/50048]	Loss: 0.3275
Training Epoch: 45 [49536/50048]	Loss: 0.3898
Training Epoch: 45 [49664/50048]	Loss: 0.4108
Training Epoch: 45 [49792/50048]	Loss: 0.3430
Training Epoch: 45 [49920/50048]	Loss: 0.2911
Training Epoch: 45 [50048/50048]	Loss: 0.3943
2022-12-06 09:46:34.392 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:46:34,441 [ZeusDataLoader(eval)] eval epoch 46 done: time=3.76 energy=452.68
2022-12-06 04:46:34,441 [ZeusDataLoader(train)] Up to epoch 46: time=4151.21, energy=503870.27, cost=615166.32
2022-12-06 04:46:34,441 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:46:34,441 [ZeusDataLoader(train)] Expected next epoch: time=4241.01, energy=514668.28, cost=628422.71
2022-12-06 04:46:34,442 [ZeusDataLoader(train)] Epoch 47 begin.
Validation Epoch: 45, Average loss: 0.0144, Accuracy: 0.6287
2022-12-06 04:46:34,635 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:46:34,636 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:46:34.638 [ZeusMonitor] Monitor started.
2022-12-06 09:46:34.638 [ZeusMonitor] Running indefinitely. 2022-12-06 09:46:34.638 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:46:34.638 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e47+gpu0.power.log
Training Epoch: 46 [128/50048]	Loss: 0.2346
Training Epoch: 46 [256/50048]	Loss: 0.4410
Training Epoch: 46 [384/50048]	Loss: 0.2972
Training Epoch: 46 [512/50048]	Loss: 0.3316
Training Epoch: 46 [640/50048]	Loss: 0.2486
Training Epoch: 46 [768/50048]	Loss: 0.3960
Training Epoch: 46 [896/50048]	Loss: 0.2536
Training Epoch: 46 [1024/50048]	Loss: 0.2806
Training Epoch: 46 [1152/50048]	Loss: 0.2128
Training Epoch: 46 [1280/50048]	Loss: 0.2672
Training Epoch: 46 [1408/50048]	Loss: 0.3481
Training Epoch: 46 [1536/50048]	Loss: 0.2658
Training Epoch: 46 [1664/50048]	Loss: 0.3744
Training Epoch: 46 [1792/50048]	Loss: 0.2363
Training Epoch: 46 [1920/50048]	Loss: 0.3557
Training Epoch: 46 [2048/50048]	Loss: 0.3027
Training Epoch: 46 [2176/50048]	Loss: 0.3837
Training Epoch: 46 [2304/50048]	Loss: 0.2770
Training Epoch: 46 [2432/50048]	Loss: 0.3578
Training Epoch: 46 [2560/50048]	Loss: 0.2549
Training Epoch: 46 [2688/50048]	Loss: 0.2771
Training Epoch: 46 [2816/50048]	Loss: 0.1809
Training Epoch: 46 [2944/50048]	Loss: 0.2912
Training Epoch: 46 [3072/50048]	Loss: 0.3170
Training Epoch: 46 [3200/50048]	Loss: 0.3215
Training Epoch: 46 [3328/50048]	Loss: 0.2965
Training Epoch: 46 [3456/50048]	Loss: 0.2799
Training Epoch: 46 [3584/50048]	Loss: 0.4978
Training Epoch: 46 [3712/50048]	Loss: 0.3599
Training Epoch: 46 [3840/50048]	Loss: 0.3288
Training Epoch: 46 [3968/50048]	Loss: 0.2349
Training Epoch: 46 [4096/50048]	Loss: 0.2302
Training Epoch: 46 [4224/50048]	Loss: 0.1903
Training Epoch: 46 [4352/50048]	Loss: 0.3478
Training Epoch: 46 [4480/50048]	Loss: 0.2821
Training Epoch: 46 [4608/50048]	Loss: 0.2481
Training Epoch: 46 [4736/50048]	Loss: 0.3004
Training Epoch: 46 [4864/50048]	Loss: 0.3047
Training Epoch: 46 [4992/50048]	Loss: 0.1817
Training Epoch: 46 [5120/50048]	Loss: 0.3058
Training Epoch: 46 [5248/50048]	Loss: 0.2956
Training Epoch: 46 [5376/50048]	Loss: 0.3127
Training Epoch: 46 [5504/50048]	Loss: 0.2634
Training Epoch: 46 [5632/50048]	Loss: 0.2353
Training Epoch: 46 [5760/50048]	Loss: 0.4205
Training Epoch: 46 [5888/50048]	Loss: 0.2142
Training Epoch: 46 [6016/50048]	Loss: 0.2665
Training Epoch: 46 [6144/50048]	Loss: 0.2698
Training Epoch: 46 [6272/50048]	Loss: 0.2012
Training Epoch: 46 [6400/50048]	Loss: 0.4147
Training Epoch: 46 [6528/50048]	Loss: 0.3108
Training Epoch: 46 [6656/50048]	Loss: 0.1976
Training Epoch: 46 [6784/50048]	Loss: 0.2236
Training Epoch: 46 [6912/50048]	Loss: 0.3361
Training Epoch: 46 [7040/50048]	Loss: 0.2186
Training Epoch: 46 [7168/50048]	Loss: 0.2446
Training Epoch: 46 [7296/50048]	Loss: 0.2354
Training Epoch: 46 [7424/50048]	Loss: 0.2731
Training Epoch: 46 [7552/50048]	Loss: 0.2517
Training Epoch: 46 [7680/50048]	Loss: 0.2184
Training Epoch: 46 [7808/50048]	Loss: 0.1996
Training Epoch: 46 [7936/50048]	Loss: 0.2419
Training Epoch: 46 [8064/50048]	Loss: 0.2557
Training Epoch: 46 [8192/50048]	Loss: 0.3258
Training Epoch: 46 [8320/50048]	Loss: 0.2914
Training Epoch: 46 [8448/50048]	Loss: 0.3171
Training Epoch: 46 [8576/50048]	Loss: 0.3484
Training Epoch: 46 [8704/50048]	Loss: 0.4059
Training Epoch: 46 [8832/50048]	Loss: 0.2019
Training Epoch: 46 [8960/50048]	Loss: 0.2303
Training Epoch: 46 [9088/50048]	Loss: 0.2019
Training Epoch: 46 [9216/50048]	Loss: 0.2234
Training Epoch: 46 [9344/50048]	Loss: 0.3416
Training Epoch: 46 [9472/50048]	Loss: 0.3221
Training Epoch: 46 [9600/50048]	Loss: 0.4195
Training Epoch: 46 [9728/50048]	Loss: 0.3025
Training Epoch: 46 [9856/50048]	Loss: 0.2531
Training Epoch: 46 [9984/50048]	Loss: 0.3829
Training Epoch: 46 [10112/50048]	Loss: 0.3379
Training Epoch: 46 [10240/50048]	Loss: 0.3423
Training Epoch: 46 [10368/50048]	Loss: 0.2653
Training Epoch: 46 [10496/50048]	Loss: 0.4800
Training Epoch: 46 [10624/50048]	Loss: 0.3463
Training Epoch: 46 [10752/50048]	Loss: 0.3038
Training Epoch: 46 [10880/50048]	Loss: 0.2837
Training Epoch: 46 [11008/50048]	Loss: 0.3011
Training Epoch: 46 [11136/50048]	Loss: 0.3476
Training Epoch: 46 [11264/50048]	Loss: 0.3059
Training Epoch: 46 [11392/50048]	Loss: 0.3073
Training Epoch: 46 [11520/50048]	Loss: 0.3137
Training Epoch: 46 [11648/50048]	Loss: 0.3454
Training Epoch: 46 [11776/50048]	Loss: 0.3557
Training Epoch: 46 [11904/50048]	Loss: 0.2662
Training Epoch: 46 [12032/50048]	Loss: 0.3423
Training Epoch: 46 [12160/50048]	Loss: 0.3492
Training Epoch: 46 [12288/50048]	Loss: 0.3028
Training Epoch: 46 [12416/50048]	Loss: 0.2224
Training Epoch: 46 [12544/50048]	Loss: 0.2862
Training Epoch: 46 [12672/50048]	Loss: 0.2372
Training Epoch: 46 [12800/50048]	Loss: 0.2455
Training Epoch: 46 [12928/50048]	Loss: 0.2412
Training Epoch: 46 [13056/50048]	Loss: 0.2885
Training Epoch: 46 [13184/50048]	Loss: 0.3202
Training Epoch: 46 [13312/50048]	Loss: 0.3935
Training Epoch: 46 [13440/50048]	Loss: 0.2668
Training Epoch: 46 [13568/50048]	Loss: 0.2114
Training Epoch: 46 [13696/50048]	Loss: 0.2169
Training Epoch: 46 [13824/50048]	Loss: 0.1948
Training Epoch: 46 [13952/50048]	Loss: 0.5343
Training Epoch: 46 [14080/50048]	Loss: 0.4432
Training Epoch: 46 [14208/50048]	Loss: 0.2993
Training Epoch: 46 [14336/50048]	Loss: 0.3028
Training Epoch: 46 [14464/50048]	Loss: 0.2769
Training Epoch: 46 [14592/50048]	Loss: 0.2743
Training Epoch: 46 [14720/50048]	Loss: 0.2587
Training Epoch: 46 [14848/50048]	Loss: 0.2053
Training Epoch: 46 [14976/50048]	Loss: 0.2322
Training Epoch: 46 [15104/50048]	Loss: 0.2470
Training Epoch: 46 [15232/50048]	Loss: 0.3269
Training Epoch: 46 [15360/50048]	Loss: 0.3434
Training Epoch: 46 [15488/50048]	Loss: 0.3568
Training Epoch: 46 [15616/50048]	Loss: 0.2660
Training Epoch: 46 [15744/50048]	Loss: 0.2144
Training Epoch: 46 [15872/50048]	Loss: 0.2691
Training Epoch: 46 [16000/50048]	Loss: 0.2794
Training Epoch: 46 [16128/50048]	Loss: 0.2661
Training Epoch: 46 [16256/50048]	Loss: 0.5330
Training Epoch: 46 [16384/50048]	Loss: 0.2549
Training Epoch: 46 [16512/50048]	Loss: 0.3082
Training Epoch: 46 [16640/50048]	Loss: 0.3069
Training Epoch: 46 [16768/50048]	Loss: 0.2309
Training Epoch: 46 [16896/50048]	Loss: 0.2576
Training Epoch: 46 [17024/50048]	Loss: 0.1787
Training Epoch: 46 [17152/50048]	Loss: 0.4575
Training Epoch: 46 [17280/50048]	Loss: 0.3635
Training Epoch: 46 [17408/50048]	Loss: 0.3844
Training Epoch: 46 [17536/50048]	Loss: 0.2978
Training Epoch: 46 [17664/50048]	Loss: 0.2196
Training Epoch: 46 [17792/50048]	Loss: 0.2536
Training Epoch: 46 [17920/50048]	Loss: 0.2529
Training Epoch: 46 [18048/50048]	Loss: 0.3847
Training Epoch: 46 [18176/50048]	Loss: 0.3755
Training Epoch: 46 [18304/50048]	Loss: 0.3303
Training Epoch: 46 [18432/50048]	Loss: 0.3617
Training Epoch: 46 [18560/50048]	Loss: 0.3272
Training Epoch: 46 [18688/50048]	Loss: 0.4085
Training Epoch: 46 [18816/50048]	Loss: 0.2612
Training Epoch: 46 [18944/50048]	Loss: 0.4349
Training Epoch: 46 [19072/50048]	Loss: 0.2634
Training Epoch: 46 [19200/50048]	Loss: 0.3031
Training Epoch: 46 [19328/50048]	Loss: 0.2841
Training Epoch: 46 [19456/50048]	Loss: 0.3651
Training Epoch: 46 [19584/50048]	Loss: 0.3156
Training Epoch: 46 [19712/50048]	Loss: 0.3633
Training Epoch: 46 [19840/50048]	Loss: 0.2668
Training Epoch: 46 [19968/50048]	Loss: 0.2738
Training Epoch: 46 [20096/50048]	Loss: 0.2928
Training Epoch: 46 [20224/50048]	Loss: 0.3235
Training Epoch: 46 [20352/50048]	Loss: 0.4453
Training Epoch: 46 [20480/50048]	Loss: 0.3613
Training Epoch: 46 [20608/50048]	Loss: 0.3788
Training Epoch: 46 [20736/50048]	Loss: 0.3156
Training Epoch: 46 [20864/50048]	Loss: 0.3550
Training Epoch: 46 [20992/50048]	Loss: 0.2506
Training Epoch: 46 [21120/50048]	Loss: 0.3664
Training Epoch: 46 [21248/50048]	Loss: 0.2884
Training Epoch: 46 [21376/50048]	Loss: 0.3117
Training Epoch: 46 [21504/50048]	Loss: 0.2538
Training Epoch: 46 [21632/50048]	Loss: 0.2900
Training Epoch: 46 [21760/50048]	Loss: 0.2981
Training Epoch: 46 [21888/50048]	Loss: 0.2607
Training Epoch: 46 [22016/50048]	Loss: 0.3511
Training Epoch: 46 [22144/50048]	Loss: 0.3000
Training Epoch: 46 [22272/50048]	Loss: 0.4436
Training Epoch: 46 [22400/50048]	Loss: 0.2866
Training Epoch: 46 [22528/50048]	Loss: 0.3400
Training Epoch: 46 [22656/50048]	Loss: 0.3703
Training Epoch: 46 [22784/50048]	Loss: 0.4155
Training Epoch: 46 [22912/50048]	Loss: 0.3718
Training Epoch: 46 [23040/50048]	Loss: 0.3623
Training Epoch: 46 [23168/50048]	Loss: 0.3235
Training Epoch: 46 [23296/50048]	Loss: 0.2555
Training Epoch: 46 [23424/50048]	Loss: 0.3777
Training Epoch: 46 [23552/50048]	Loss: 0.3121
Training Epoch: 46 [23680/50048]	Loss: 0.2805
Training Epoch: 46 [23808/50048]	Loss: 0.5136
Training Epoch: 46 [23936/50048]	Loss: 0.2891
Training Epoch: 46 [24064/50048]	Loss: 0.4206
Training Epoch: 46 [24192/50048]	Loss: 0.2948
Training Epoch: 46 [24320/50048]	Loss: 0.4208
Training Epoch: 46 [24448/50048]	Loss: 0.2542
Training Epoch: 46 [24576/50048]	Loss: 0.3622
Training Epoch: 46 [24704/50048]	Loss: 0.4966
Training Epoch: 46 [24832/50048]	Loss: 0.4868
Training Epoch: 46 [24960/50048]	Loss: 0.3003
Training Epoch: 46 [25088/50048]	Loss: 0.2148
Training Epoch: 46 [25216/50048]	Loss: 0.3760
Training Epoch: 46 [25344/50048]	Loss: 0.2322
Training Epoch: 46 [25472/50048]	Loss: 0.3303
Training Epoch: 46 [25600/50048]	Loss: 0.2802
Training Epoch: 46 [25728/50048]	Loss: 0.3354
Training Epoch: 46 [25856/50048]	Loss: 0.2973
Training Epoch: 46 [25984/50048]	Loss: 0.3430
Training Epoch: 46 [26112/50048]	Loss: 0.3719
Training Epoch: 46 [26240/50048]	Loss: 0.2264
Training Epoch: 46 [26368/50048]	Loss: 0.3381
Training Epoch: 46 [26496/50048]	Loss: 0.3049
Training Epoch: 46 [26624/50048]	Loss: 0.4330
Training Epoch: 46 [26752/50048]	Loss: 0.2432
Training Epoch: 46 [26880/50048]	Loss: 0.3883
Training Epoch: 46 [27008/50048]	Loss: 0.3722
Training Epoch: 46 [27136/50048]	Loss: 0.2564
Training Epoch: 46 [27264/50048]	Loss: 0.3382
Training Epoch: 46 [27392/50048]	Loss: 0.3293
Training Epoch: 46 [27520/50048]	Loss: 0.2634
Training Epoch: 46 [27648/50048]	Loss: 0.3323
Training Epoch: 46 [27776/50048]	Loss: 0.3156
Training Epoch: 46 [27904/50048]	Loss: 0.3065
Training Epoch: 46 [28032/50048]	Loss: 0.2770
Training Epoch: 46 [28160/50048]	Loss: 0.4047
Training Epoch: 46 [28288/50048]	Loss: 0.3466
Training Epoch: 46 [28416/50048]	Loss: 0.3136
Training Epoch: 46 [28544/50048]	Loss: 0.3286
Training Epoch: 46 [28672/50048]	Loss: 0.3607
Training Epoch: 46 [28800/50048]	Loss: 0.4412
Training Epoch: 46 [28928/50048]	Loss: 0.2550
Training Epoch: 46 [29056/50048]	Loss: 0.3913
Training Epoch: 46 [29184/50048]	Loss: 0.2606
Training Epoch: 46 [29312/50048]	Loss: 0.2650
Training Epoch: 46 [29440/50048]	Loss: 0.1977
Training Epoch: 46 [29568/50048]	Loss: 0.3540
Training Epoch: 46 [29696/50048]	Loss: 0.4021
Training Epoch: 46 [29824/50048]	Loss: 0.3087
Training Epoch: 46 [29952/50048]	Loss: 0.2903
Training Epoch: 46 [30080/50048]	Loss: 0.3465
Training Epoch: 46 [30208/50048]	Loss: 0.3511
Training Epoch: 46 [30336/50048]	Loss: 0.2846
Training Epoch: 46 [30464/50048]	Loss: 0.2000
Training Epoch: 46 [30592/50048]	Loss: 0.4020
Training Epoch: 46 [30720/50048]	Loss: 0.2393
Training Epoch: 46 [30848/50048]	Loss: 0.4192
Training Epoch: 46 [30976/50048]	Loss: 0.4086
Training Epoch: 46 [31104/50048]	Loss: 0.2581
Training Epoch: 46 [31232/50048]	Loss: 0.2850
Training Epoch: 46 [31360/50048]	Loss: 0.3295
Training Epoch: 46 [31488/50048]	Loss: 0.3228
Training Epoch: 46 [31616/50048]	Loss: 0.3932
Training Epoch: 46 [31744/50048]	Loss: 0.2277
Training Epoch: 46 [31872/50048]	Loss: 0.3229
Training Epoch: 46 [32000/50048]	Loss: 0.2673
Training Epoch: 46 [32128/50048]	Loss: 0.2586
Training Epoch: 46 [32256/50048]	Loss: 0.2874
Training Epoch: 46 [32384/50048]	Loss: 0.3787
Training Epoch: 46 [32512/50048]	Loss: 0.4563
Training Epoch: 46 [32640/50048]	Loss: 0.3577
Training Epoch: 46 [32768/50048]	Loss: 0.2524
Training Epoch: 46 [32896/50048]	Loss: 0.3074
Training Epoch: 46 [33024/50048]	Loss: 0.3061
Training Epoch: 46 [33152/50048]	Loss: 0.3068
Training Epoch: 46 [33280/50048]	Loss: 0.2181
Training Epoch: 46 [33408/50048]	Loss: 0.3163
Training Epoch: 46 [33536/50048]	Loss: 0.3057
Training Epoch: 46 [33664/50048]	Loss: 0.4308
Training Epoch: 46 [33792/50048]	Loss: 0.4924
Training Epoch: 46 [33920/50048]	Loss: 0.4552
Training Epoch: 46 [34048/50048]	Loss: 0.3145
Training Epoch: 46 [34176/50048]	Loss: 0.4105
Training Epoch: 46 [34304/50048]	Loss: 0.3774
Training Epoch: 46 [34432/50048]	Loss: 0.2405
Training Epoch: 46 [34560/50048]	Loss: 0.4620
Training Epoch: 46 [34688/50048]	Loss: 0.3570
Training Epoch: 46 [34816/50048]	Loss: 0.4180
Training Epoch: 46 [34944/50048]	Loss: 0.3390
Training Epoch: 46 [35072/50048]	Loss: 0.3305
Training Epoch: 46 [35200/50048]	Loss: 0.4608
Training Epoch: 46 [35328/50048]	Loss: 0.3670
Training Epoch: 46 [35456/50048]	Loss: 0.3431
Training Epoch: 46 [35584/50048]	Loss: 0.3031
Training Epoch: 46 [35712/50048]	Loss: 0.4032
Training Epoch: 46 [35840/50048]	Loss: 0.2087
Training Epoch: 46 [35968/50048]	Loss: 0.3766
Training Epoch: 46 [36096/50048]	Loss: 0.3137
Training Epoch: 46 [36224/50048]	Loss: 0.2843
Training Epoch: 46 [36352/50048]	Loss: 0.4677
Training Epoch: 46 [36480/50048]	Loss: 0.3825
Training Epoch: 46 [36608/50048]	Loss: 0.2196
Training Epoch: 46 [36736/50048]	Loss: 0.3236
Training Epoch: 46 [36864/50048]	Loss: 0.3500
Training Epoch: 46 [36992/50048]	Loss: 0.3819
Training Epoch: 46 [37120/50048]	Loss: 0.2660
Training Epoch: 46 [37248/50048]	Loss: 0.4402
Training Epoch: 46 [37376/50048]	Loss: 0.4685
Training Epoch: 46 [37504/50048]	Loss: 0.2989
Training Epoch: 46 [37632/50048]	Loss: 0.2361
Training Epoch: 46 [37760/50048]	Loss: 0.3468
Training Epoch: 46 [37888/50048]	Loss: 0.3782
Training Epoch: 46 [38016/50048]	Loss: 0.4974
Training Epoch: 46 [38144/50048]	Loss: 0.5250
Training Epoch: 46 [38272/50048]	Loss: 0.4087
Training Epoch: 46 [38400/50048]	Loss: 0.2899
Training Epoch: 46 [38528/50048]	Loss: 0.4359
Training Epoch: 46 [38656/50048]	Loss: 0.5670
Training Epoch: 46 [38784/50048]	Loss: 0.4502
Training Epoch: 46 [38912/50048]	Loss: 0.2434
Training Epoch: 46 [39040/50048]	Loss: 0.4111
Training Epoch: 46 [39168/50048]	Loss: 0.2812
Training Epoch: 46 [39296/50048]	Loss: 0.3252
Training Epoch: 46 [39424/50048]	Loss: 0.3153
Training Epoch: 46 [39552/50048]	Loss: 0.3808
Training Epoch: 46 [39680/50048]	Loss: 0.2843
Training Epoch: 46 [39808/50048]	Loss: 0.5010
Training Epoch: 46 [39936/50048]	Loss: 0.1904
Training Epoch: 46 [40064/50048]	Loss: 0.4485
Training Epoch: 46 [40192/50048]	Loss: 0.4471
Training Epoch: 46 [40320/50048]	Loss: 0.3167
Training Epoch: 46 [40448/50048]	Loss: 0.3681
Training Epoch: 46 [40576/50048]	Loss: 0.3241
Training Epoch: 46 [40704/50048]	Loss: 0.2351
Training Epoch: 46 [40832/50048]	Loss: 0.4398
Training Epoch: 46 [40960/50048]	Loss: 0.2610
Training Epoch: 46 [41088/50048]	Loss: 0.4483
Training Epoch: 46 [41216/50048]	Loss: 0.5010
Training Epoch: 46 [41344/50048]	Loss: 0.3431
Training Epoch: 46 [41472/50048]	Loss: 0.2462
Training Epoch: 46 [41600/50048]	Loss: 0.4628
Training Epoch: 46 [41728/50048]	Loss: 0.3333
Training Epoch: 46 [41856/50048]	Loss: 0.3257
Training Epoch: 46 [41984/50048]	Loss: 0.3092
Training Epoch: 46 [42112/50048]	Loss: 0.3100
Training Epoch: 46 [42240/50048]	Loss: 0.2410
Training Epoch: 46 [42368/50048]	Loss: 0.4276
Training Epoch: 46 [42496/50048]	Loss: 0.2770
Training Epoch: 46 [42624/50048]	Loss: 0.3213
Training Epoch: 46 [42752/50048]	Loss: 0.4412
Training Epoch: 46 [42880/50048]	Loss: 0.3359
Training Epoch: 46 [43008/50048]	Loss: 0.2346
Training Epoch: 46 [43136/50048]	Loss: 0.1989
Training Epoch: 46 [43264/50048]	Loss: 0.3254
Training Epoch: 46 [43392/50048]	Loss: 0.3716
Training Epoch: 46 [43520/50048]	Loss: 0.2806
Training Epoch: 46 [43648/50048]	Loss: 0.2963
Training Epoch: 46 [43776/50048]	Loss: 0.2867
Training Epoch: 46 [43904/50048]	Loss: 0.3656
Training Epoch: 46 [44032/50048]	Loss: 0.2069
Training Epoch: 46 [44160/50048]	Loss: 0.2469
Training Epoch: 46 [44288/50048]	Loss: 0.4505
Training Epoch: 46 [44416/50048]	Loss: 0.2844
Training Epoch: 46 [44544/50048]	Loss: 0.3445
Training Epoch: 46 [44672/50048]	Loss: 0.3602
Training Epoch: 46 [44800/50048]	Loss: 0.3984
Training Epoch: 46 [44928/50048]	Loss: 0.5307
Training Epoch: 46 [45056/50048]	Loss: 0.2620
Training Epoch: 46 [45184/50048]	Loss: 0.3007
Training Epoch: 46 [45312/50048]	Loss: 0.3774
Training Epoch: 46 [45440/50048]	Loss: 0.4202
Training Epoch: 46 [45568/50048]	Loss: 0.2699
Training Epoch: 46 [45696/50048]	Loss: 0.3160
2022-12-06 04:48:00,912 [ZeusDataLoader(train)] train epoch 47 done: time=86.46 energy=10500.00
2022-12-06 04:48:00,914 [ZeusDataLoader(eval)] Epoch 47 begin.
Training Epoch: 46 [45824/50048]	Loss: 0.3274
Training Epoch: 46 [45952/50048]	Loss: 0.4275
Training Epoch: 46 [46080/50048]	Loss: 0.4086
Training Epoch: 46 [46208/50048]	Loss: 0.4128
Training Epoch: 46 [46336/50048]	Loss: 0.3058
Training Epoch: 46 [46464/50048]	Loss: 0.3348
Training Epoch: 46 [46592/50048]	Loss: 0.3224
Training Epoch: 46 [46720/50048]	Loss: 0.4748
Training Epoch: 46 [46848/50048]	Loss: 0.3450
Training Epoch: 46 [46976/50048]	Loss: 0.3781
Training Epoch: 46 [47104/50048]	Loss: 0.4050
Training Epoch: 46 [47232/50048]	Loss: 0.3111
Training Epoch: 46 [47360/50048]	Loss: 0.4275
Training Epoch: 46 [47488/50048]	Loss: 0.3937
Training Epoch: 46 [47616/50048]	Loss: 0.4059
Training Epoch: 46 [47744/50048]	Loss: 0.4350
Training Epoch: 46 [47872/50048]	Loss: 0.3117
Training Epoch: 46 [48000/50048]	Loss: 0.4161
Training Epoch: 46 [48128/50048]	Loss: 0.4472
Training Epoch: 46 [48256/50048]	Loss: 0.4027
Training Epoch: 46 [48384/50048]	Loss: 0.3743
Training Epoch: 46 [48512/50048]	Loss: 0.3899
Training Epoch: 46 [48640/50048]	Loss: 0.2598
Training Epoch: 46 [48768/50048]	Loss: 0.4438
Training Epoch: 46 [48896/50048]	Loss: 0.2892
Training Epoch: 46 [49024/50048]	Loss: 0.4435
Training Epoch: 46 [49152/50048]	Loss: 0.4531
Training Epoch: 46 [49280/50048]	Loss: 0.3398
Training Epoch: 46 [49408/50048]	Loss: 0.4507
Training Epoch: 46 [49536/50048]	Loss: 0.3406
Training Epoch: 46 [49664/50048]	Loss: 0.3868
Training Epoch: 46 [49792/50048]	Loss: 0.2082
Training Epoch: 46 [49920/50048]	Loss: 0.3273
Training Epoch: 46 [50048/50048]	Loss: 0.5290
2022-12-06 09:48:04.587 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:48:04,601 [ZeusDataLoader(eval)] eval epoch 47 done: time=3.68 energy=438.71
2022-12-06 04:48:04,601 [ZeusDataLoader(train)] Up to epoch 47: time=4241.35, energy=514808.97, cost=628522.64
2022-12-06 04:48:04,601 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:48:04,601 [ZeusDataLoader(train)] Expected next epoch: time=4331.15, energy=525606.99, cost=641779.02
2022-12-06 04:48:04,602 [ZeusDataLoader(train)] Epoch 48 begin.
Validation Epoch: 46, Average loss: 0.0150, Accuracy: 0.6259
2022-12-06 04:48:04,787 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:48:04,788 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:48:04.790 [ZeusMonitor] Monitor started.
2022-12-06 09:48:04.790 [ZeusMonitor] Running indefinitely. 2022-12-06 09:48:04.790 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:48:04.790 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e48+gpu0.power.log
Training Epoch: 47 [128/50048]	Loss: 0.2582
Training Epoch: 47 [256/50048]	Loss: 0.2899
Training Epoch: 47 [384/50048]	Loss: 0.2378
Training Epoch: 47 [512/50048]	Loss: 0.3456
Training Epoch: 47 [640/50048]	Loss: 0.3222
Training Epoch: 47 [768/50048]	Loss: 0.2855
Training Epoch: 47 [896/50048]	Loss: 0.2553
Training Epoch: 47 [1024/50048]	Loss: 0.2837
Training Epoch: 47 [1152/50048]	Loss: 0.2886
Training Epoch: 47 [1280/50048]	Loss: 0.2619
Training Epoch: 47 [1408/50048]	Loss: 0.2980
Training Epoch: 47 [1536/50048]	Loss: 0.2803
Training Epoch: 47 [1664/50048]	Loss: 0.2373
Training Epoch: 47 [1792/50048]	Loss: 0.1483
Training Epoch: 47 [1920/50048]	Loss: 0.2336
Training Epoch: 47 [2048/50048]	Loss: 0.2903
Training Epoch: 47 [2176/50048]	Loss: 0.2997
Training Epoch: 47 [2304/50048]	Loss: 0.2636
Training Epoch: 47 [2432/50048]	Loss: 0.2577
Training Epoch: 47 [2560/50048]	Loss: 0.2957
Training Epoch: 47 [2688/50048]	Loss: 0.2911
Training Epoch: 47 [2816/50048]	Loss: 0.3956
Training Epoch: 47 [2944/50048]	Loss: 0.2192
Training Epoch: 47 [3072/50048]	Loss: 0.2956
Training Epoch: 47 [3200/50048]	Loss: 0.2484
Training Epoch: 47 [3328/50048]	Loss: 0.3177
Training Epoch: 47 [3456/50048]	Loss: 0.3251
Training Epoch: 47 [3584/50048]	Loss: 0.1982
Training Epoch: 47 [3712/50048]	Loss: 0.3289
Training Epoch: 47 [3840/50048]	Loss: 0.2015
Training Epoch: 47 [3968/50048]	Loss: 0.2667
Training Epoch: 47 [4096/50048]	Loss: 0.3669
Training Epoch: 47 [4224/50048]	Loss: 0.3109
Training Epoch: 47 [4352/50048]	Loss: 0.3322
Training Epoch: 47 [4480/50048]	Loss: 0.2598
Training Epoch: 47 [4608/50048]	Loss: 0.2270
Training Epoch: 47 [4736/50048]	Loss: 0.2158
Training Epoch: 47 [4864/50048]	Loss: 0.2229
Training Epoch: 47 [4992/50048]	Loss: 0.3761
Training Epoch: 47 [5120/50048]	Loss: 0.1910
Training Epoch: 47 [5248/50048]	Loss: 0.2689
Training Epoch: 47 [5376/50048]	Loss: 0.3003
Training Epoch: 47 [5504/50048]	Loss: 0.3871
Training Epoch: 47 [5632/50048]	Loss: 0.3549
Training Epoch: 47 [5760/50048]	Loss: 0.1795
Training Epoch: 47 [5888/50048]	Loss: 0.3626
Training Epoch: 47 [6016/50048]	Loss: 0.2496
Training Epoch: 47 [6144/50048]	Loss: 0.2019
Training Epoch: 47 [6272/50048]	Loss: 0.3118
Training Epoch: 47 [6400/50048]	Loss: 0.2661
Training Epoch: 47 [6528/50048]	Loss: 0.2323
Training Epoch: 47 [6656/50048]	Loss: 0.2404
Training Epoch: 47 [6784/50048]	Loss: 0.3550
Training Epoch: 47 [6912/50048]	Loss: 0.3203
Training Epoch: 47 [7040/50048]	Loss: 0.2776
Training Epoch: 47 [7168/50048]	Loss: 0.3096
Training Epoch: 47 [7296/50048]	Loss: 0.3581
Training Epoch: 47 [7424/50048]	Loss: 0.2366
Training Epoch: 47 [7552/50048]	Loss: 0.2313
Training Epoch: 47 [7680/50048]	Loss: 0.3522
Training Epoch: 47 [7808/50048]	Loss: 0.3340
Training Epoch: 47 [7936/50048]	Loss: 0.3568
Training Epoch: 47 [8064/50048]	Loss: 0.3451
Training Epoch: 47 [8192/50048]	Loss: 0.2409
Training Epoch: 47 [8320/50048]	Loss: 0.3636
Training Epoch: 47 [8448/50048]	Loss: 0.2256
Training Epoch: 47 [8576/50048]	Loss: 0.1751
Training Epoch: 47 [8704/50048]	Loss: 0.3740
Training Epoch: 47 [8832/50048]	Loss: 0.2129
Training Epoch: 47 [8960/50048]	Loss: 0.3083
Training Epoch: 47 [9088/50048]	Loss: 0.2015
Training Epoch: 47 [9216/50048]	Loss: 0.2100
Training Epoch: 47 [9344/50048]	Loss: 0.2543
Training Epoch: 47 [9472/50048]	Loss: 0.2492
Training Epoch: 47 [9600/50048]	Loss: 0.1973
Training Epoch: 47 [9728/50048]	Loss: 0.2898
Training Epoch: 47 [9856/50048]	Loss: 0.2950
Training Epoch: 47 [9984/50048]	Loss: 0.2388
Training Epoch: 47 [10112/50048]	Loss: 0.2049
Training Epoch: 47 [10240/50048]	Loss: 0.3444
Training Epoch: 47 [10368/50048]	Loss: 0.3434
Training Epoch: 47 [10496/50048]	Loss: 0.1642
Training Epoch: 47 [10624/50048]	Loss: 0.2717
Training Epoch: 47 [10752/50048]	Loss: 0.1900
Training Epoch: 47 [10880/50048]	Loss: 0.3366
Training Epoch: 47 [11008/50048]	Loss: 0.3610
Training Epoch: 47 [11136/50048]	Loss: 0.2222
Training Epoch: 47 [11264/50048]	Loss: 0.2629
Training Epoch: 47 [11392/50048]	Loss: 0.3148
Training Epoch: 47 [11520/50048]	Loss: 0.1894
Training Epoch: 47 [11648/50048]	Loss: 0.3143
Training Epoch: 47 [11776/50048]	Loss: 0.2765
Training Epoch: 47 [11904/50048]	Loss: 0.3475
Training Epoch: 47 [12032/50048]	Loss: 0.3065
Training Epoch: 47 [12160/50048]	Loss: 0.2720
Training Epoch: 47 [12288/50048]	Loss: 0.2978
Training Epoch: 47 [12416/50048]	Loss: 0.3110
Training Epoch: 47 [12544/50048]	Loss: 0.2898
Training Epoch: 47 [12672/50048]	Loss: 0.3154
Training Epoch: 47 [12800/50048]	Loss: 0.3168
Training Epoch: 47 [12928/50048]	Loss: 0.2870
Training Epoch: 47 [13056/50048]	Loss: 0.2460
Training Epoch: 47 [13184/50048]	Loss: 0.2949
Training Epoch: 47 [13312/50048]	Loss: 0.2419
Training Epoch: 47 [13440/50048]	Loss: 0.3932
Training Epoch: 47 [13568/50048]	Loss: 0.2120
Training Epoch: 47 [13696/50048]	Loss: 0.4381
Training Epoch: 47 [13824/50048]	Loss: 0.3053
Training Epoch: 47 [13952/50048]	Loss: 0.3466
Training Epoch: 47 [14080/50048]	Loss: 0.2559
Training Epoch: 47 [14208/50048]	Loss: 0.2655
Training Epoch: 47 [14336/50048]	Loss: 0.2324
Training Epoch: 47 [14464/50048]	Loss: 0.3423
Training Epoch: 47 [14592/50048]	Loss: 0.3331
Training Epoch: 47 [14720/50048]	Loss: 0.3412
Training Epoch: 47 [14848/50048]	Loss: 0.3108
Training Epoch: 47 [14976/50048]	Loss: 0.2622
Training Epoch: 47 [15104/50048]	Loss: 0.2949
Training Epoch: 47 [15232/50048]	Loss: 0.2083
Training Epoch: 47 [15360/50048]	Loss: 0.2044
Training Epoch: 47 [15488/50048]	Loss: 0.3555
Training Epoch: 47 [15616/50048]	Loss: 0.3394
Training Epoch: 47 [15744/50048]	Loss: 0.3189
Training Epoch: 47 [15872/50048]	Loss: 0.2470
Training Epoch: 47 [16000/50048]	Loss: 0.1746
Training Epoch: 47 [16128/50048]	Loss: 0.2566
Training Epoch: 47 [16256/50048]	Loss: 0.2622
Training Epoch: 47 [16384/50048]	Loss: 0.3302
Training Epoch: 47 [16512/50048]	Loss: 0.3082
Training Epoch: 47 [16640/50048]	Loss: 0.3126
Training Epoch: 47 [16768/50048]	Loss: 0.2197
Training Epoch: 47 [16896/50048]	Loss: 0.2308
Training Epoch: 47 [17024/50048]	Loss: 0.2405
Training Epoch: 47 [17152/50048]	Loss: 0.2134
Training Epoch: 47 [17280/50048]	Loss: 0.2328
Training Epoch: 47 [17408/50048]	Loss: 0.2654
Training Epoch: 47 [17536/50048]	Loss: 0.3459
Training Epoch: 47 [17664/50048]	Loss: 0.2819
Training Epoch: 47 [17792/50048]	Loss: 0.2763
Training Epoch: 47 [17920/50048]	Loss: 0.3627
Training Epoch: 47 [18048/50048]	Loss: 0.2968
Training Epoch: 47 [18176/50048]	Loss: 0.2649
Training Epoch: 47 [18304/50048]	Loss: 0.1753
Training Epoch: 47 [18432/50048]	Loss: 0.3021
Training Epoch: 47 [18560/50048]	Loss: 0.2774
Training Epoch: 47 [18688/50048]	Loss: 0.4412
Training Epoch: 47 [18816/50048]	Loss: 0.2468
Training Epoch: 47 [18944/50048]	Loss: 0.2257
Training Epoch: 47 [19072/50048]	Loss: 0.3135
Training Epoch: 47 [19200/50048]	Loss: 0.2798
Training Epoch: 47 [19328/50048]	Loss: 0.2980
Training Epoch: 47 [19456/50048]	Loss: 0.3798
Training Epoch: 47 [19584/50048]	Loss: 0.3284
Training Epoch: 47 [19712/50048]	Loss: 0.5075
Training Epoch: 47 [19840/50048]	Loss: 0.3818
Training Epoch: 47 [19968/50048]	Loss: 0.3129
Training Epoch: 47 [20096/50048]	Loss: 0.3437
Training Epoch: 47 [20224/50048]	Loss: 0.3256
Training Epoch: 47 [20352/50048]	Loss: 0.4163
Training Epoch: 47 [20480/50048]	Loss: 0.2546
Training Epoch: 47 [20608/50048]	Loss: 0.3573
Training Epoch: 47 [20736/50048]	Loss: 0.3067
Training Epoch: 47 [20864/50048]	Loss: 0.3214
Training Epoch: 47 [20992/50048]	Loss: 0.1659
Training Epoch: 47 [21120/50048]	Loss: 0.2970
Training Epoch: 47 [21248/50048]	Loss: 0.3182
Training Epoch: 47 [21376/50048]	Loss: 0.1435
Training Epoch: 47 [21504/50048]	Loss: 0.3095
Training Epoch: 47 [21632/50048]	Loss: 0.2763
Training Epoch: 47 [21760/50048]	Loss: 0.2933
Training Epoch: 47 [21888/50048]	Loss: 0.3315
Training Epoch: 47 [22016/50048]	Loss: 0.2357
Training Epoch: 47 [22144/50048]	Loss: 0.3574
Training Epoch: 47 [22272/50048]	Loss: 0.2875
Training Epoch: 47 [22400/50048]	Loss: 0.3188
Training Epoch: 47 [22528/50048]	Loss: 0.3385
Training Epoch: 47 [22656/50048]	Loss: 0.2730
Training Epoch: 47 [22784/50048]	Loss: 0.3357
Training Epoch: 47 [22912/50048]	Loss: 0.1982
Training Epoch: 47 [23040/50048]	Loss: 0.2931
Training Epoch: 47 [23168/50048]	Loss: 0.2919
Training Epoch: 47 [23296/50048]	Loss: 0.2742
Training Epoch: 47 [23424/50048]	Loss: 0.1989
Training Epoch: 47 [23552/50048]	Loss: 0.3184
Training Epoch: 47 [23680/50048]	Loss: 0.1950
Training Epoch: 47 [23808/50048]	Loss: 0.1852
Training Epoch: 47 [23936/50048]	Loss: 0.2606
Training Epoch: 47 [24064/50048]	Loss: 0.3307
Training Epoch: 47 [24192/50048]	Loss: 0.2972
Training Epoch: 47 [24320/50048]	Loss: 0.3538
Training Epoch: 47 [24448/50048]	Loss: 0.2892
Training Epoch: 47 [24576/50048]	Loss: 0.2480
Training Epoch: 47 [24704/50048]	Loss: 0.4154
Training Epoch: 47 [24832/50048]	Loss: 0.2968
Training Epoch: 47 [24960/50048]	Loss: 0.3291
Training Epoch: 47 [25088/50048]	Loss: 0.2854
Training Epoch: 47 [25216/50048]	Loss: 0.2925
Training Epoch: 47 [25344/50048]	Loss: 0.2543
Training Epoch: 47 [25472/50048]	Loss: 0.3465
Training Epoch: 47 [25600/50048]	Loss: 0.2979
Training Epoch: 47 [25728/50048]	Loss: 0.3085
Training Epoch: 47 [25856/50048]	Loss: 0.4221
Training Epoch: 47 [25984/50048]	Loss: 0.4206
Training Epoch: 47 [26112/50048]	Loss: 0.3276
Training Epoch: 47 [26240/50048]	Loss: 0.2834
Training Epoch: 47 [26368/50048]	Loss: 0.3634
Training Epoch: 47 [26496/50048]	Loss: 0.1854
Training Epoch: 47 [26624/50048]	Loss: 0.2575
Training Epoch: 47 [26752/50048]	Loss: 0.2936
Training Epoch: 47 [26880/50048]	Loss: 0.2928
Training Epoch: 47 [27008/50048]	Loss: 0.2490
Training Epoch: 47 [27136/50048]	Loss: 0.4222
Training Epoch: 47 [27264/50048]	Loss: 0.2230
Training Epoch: 47 [27392/50048]	Loss: 0.2947
Training Epoch: 47 [27520/50048]	Loss: 0.3521
Training Epoch: 47 [27648/50048]	Loss: 0.2413
Training Epoch: 47 [27776/50048]	Loss: 0.2863
Training Epoch: 47 [27904/50048]	Loss: 0.3693
Training Epoch: 47 [28032/50048]	Loss: 0.3468
Training Epoch: 47 [28160/50048]	Loss: 0.3420
Training Epoch: 47 [28288/50048]	Loss: 0.2341
Training Epoch: 47 [28416/50048]	Loss: 0.2394
Training Epoch: 47 [28544/50048]	Loss: 0.2196
Training Epoch: 47 [28672/50048]	Loss: 0.2110
Training Epoch: 47 [28800/50048]	Loss: 0.3041
Training Epoch: 47 [28928/50048]	Loss: 0.3146
Training Epoch: 47 [29056/50048]	Loss: 0.3809
Training Epoch: 47 [29184/50048]	Loss: 0.3011
Training Epoch: 47 [29312/50048]	Loss: 0.3335
Training Epoch: 47 [29440/50048]	Loss: 0.3283
Training Epoch: 47 [29568/50048]	Loss: 0.2790
Training Epoch: 47 [29696/50048]	Loss: 0.3750
Training Epoch: 47 [29824/50048]	Loss: 0.2719
Training Epoch: 47 [29952/50048]	Loss: 0.2679
Training Epoch: 47 [30080/50048]	Loss: 0.3733
Training Epoch: 47 [30208/50048]	Loss: 0.2374
Training Epoch: 47 [30336/50048]	Loss: 0.3074
Training Epoch: 47 [30464/50048]	Loss: 0.3338
Training Epoch: 47 [30592/50048]	Loss: 0.3292
Training Epoch: 47 [30720/50048]	Loss: 0.2555
Training Epoch: 47 [30848/50048]	Loss: 0.2507
Training Epoch: 47 [30976/50048]	Loss: 0.3583
Training Epoch: 47 [31104/50048]	Loss: 0.4176
Training Epoch: 47 [31232/50048]	Loss: 0.2942
Training Epoch: 47 [31360/50048]	Loss: 0.2208
Training Epoch: 47 [31488/50048]	Loss: 0.3059
Training Epoch: 47 [31616/50048]	Loss: 0.2899
Training Epoch: 47 [31744/50048]	Loss: 0.2966
Training Epoch: 47 [31872/50048]	Loss: 0.2955
Training Epoch: 47 [32000/50048]	Loss: 0.3438
Training Epoch: 47 [32128/50048]	Loss: 0.3713
Training Epoch: 47 [32256/50048]	Loss: 0.2784
Training Epoch: 47 [32384/50048]	Loss: 0.3704
Training Epoch: 47 [32512/50048]	Loss: 0.3304
Training Epoch: 47 [32640/50048]	Loss: 0.3680
Training Epoch: 47 [32768/50048]	Loss: 0.2313
Training Epoch: 47 [32896/50048]	Loss: 0.4002
Training Epoch: 47 [33024/50048]	Loss: 0.4315
Training Epoch: 47 [33152/50048]	Loss: 0.3129
Training Epoch: 47 [33280/50048]	Loss: 0.4699
Training Epoch: 47 [33408/50048]	Loss: 0.2377
Training Epoch: 47 [33536/50048]	Loss: 0.3470
Training Epoch: 47 [33664/50048]	Loss: 0.3785
Training Epoch: 47 [33792/50048]	Loss: 0.2619
Training Epoch: 47 [33920/50048]	Loss: 0.3834
Training Epoch: 47 [34048/50048]	Loss: 0.1969
Training Epoch: 47 [34176/50048]	Loss: 0.4368
Training Epoch: 47 [34304/50048]	Loss: 0.3136
Training Epoch: 47 [34432/50048]	Loss: 0.1946
Training Epoch: 47 [34560/50048]	Loss: 0.3436
Training Epoch: 47 [34688/50048]	Loss: 0.2996
Training Epoch: 47 [34816/50048]	Loss: 0.2888
Training Epoch: 47 [34944/50048]	Loss: 0.3592
Training Epoch: 47 [35072/50048]	Loss: 0.3038
Training Epoch: 47 [35200/50048]	Loss: 0.3447
Training Epoch: 47 [35328/50048]	Loss: 0.3709
Training Epoch: 47 [35456/50048]	Loss: 0.3384
Training Epoch: 47 [35584/50048]	Loss: 0.4176
Training Epoch: 47 [35712/50048]	Loss: 0.2785
Training Epoch: 47 [35840/50048]	Loss: 0.3128
Training Epoch: 47 [35968/50048]	Loss: 0.3522
Training Epoch: 47 [36096/50048]	Loss: 0.3251
Training Epoch: 47 [36224/50048]	Loss: 0.2386
Training Epoch: 47 [36352/50048]	Loss: 0.3483
Training Epoch: 47 [36480/50048]	Loss: 0.3912
Training Epoch: 47 [36608/50048]	Loss: 0.3191
Training Epoch: 47 [36736/50048]	Loss: 0.2923
Training Epoch: 47 [36864/50048]	Loss: 0.3964
Training Epoch: 47 [36992/50048]	Loss: 0.3997
Training Epoch: 47 [37120/50048]	Loss: 0.2408
Training Epoch: 47 [37248/50048]	Loss: 0.4306
Training Epoch: 47 [37376/50048]	Loss: 0.2924
Training Epoch: 47 [37504/50048]	Loss: 0.3400
Training Epoch: 47 [37632/50048]	Loss: 0.3149
Training Epoch: 47 [37760/50048]	Loss: 0.2544
Training Epoch: 47 [37888/50048]	Loss: 0.3299
Training Epoch: 47 [38016/50048]	Loss: 0.4804
Training Epoch: 47 [38144/50048]	Loss: 0.2948
Training Epoch: 47 [38272/50048]	Loss: 0.2833
Training Epoch: 47 [38400/50048]	Loss: 0.4227
Training Epoch: 47 [38528/50048]	Loss: 0.2135
Training Epoch: 47 [38656/50048]	Loss: 0.3326
Training Epoch: 47 [38784/50048]	Loss: 0.2284
Training Epoch: 47 [38912/50048]	Loss: 0.2325
Training Epoch: 47 [39040/50048]	Loss: 0.2981
Training Epoch: 47 [39168/50048]	Loss: 0.4195
Training Epoch: 47 [39296/50048]	Loss: 0.3177
Training Epoch: 47 [39424/50048]	Loss: 0.2486
Training Epoch: 47 [39552/50048]	Loss: 0.2002
Training Epoch: 47 [39680/50048]	Loss: 0.3708
Training Epoch: 47 [39808/50048]	Loss: 0.2790
Training Epoch: 47 [39936/50048]	Loss: 0.3928
Training Epoch: 47 [40064/50048]	Loss: 0.3462
Training Epoch: 47 [40192/50048]	Loss: 0.3195
Training Epoch: 47 [40320/50048]	Loss: 0.2929
Training Epoch: 47 [40448/50048]	Loss: 0.2329
Training Epoch: 47 [40576/50048]	Loss: 0.3145
Training Epoch: 47 [40704/50048]	Loss: 0.2481
Training Epoch: 47 [40832/50048]	Loss: 0.2495
Training Epoch: 47 [40960/50048]	Loss: 0.3655
Training Epoch: 47 [41088/50048]	Loss: 0.3221
Training Epoch: 47 [41216/50048]	Loss: 0.3277
Training Epoch: 47 [41344/50048]	Loss: 0.3790
Training Epoch: 47 [41472/50048]	Loss: 0.3299
Training Epoch: 47 [41600/50048]	Loss: 0.3722
Training Epoch: 47 [41728/50048]	Loss: 0.4898
Training Epoch: 47 [41856/50048]	Loss: 0.2639
Training Epoch: 47 [41984/50048]	Loss: 0.3571
Training Epoch: 47 [42112/50048]	Loss: 0.3595
Training Epoch: 47 [42240/50048]	Loss: 0.2626
Training Epoch: 47 [42368/50048]	Loss: 0.4008
Training Epoch: 47 [42496/50048]	Loss: 0.4037
Training Epoch: 47 [42624/50048]	Loss: 0.3589
Training Epoch: 47 [42752/50048]	Loss: 0.3433
Training Epoch: 47 [42880/50048]	Loss: 0.3950
Training Epoch: 47 [43008/50048]	Loss: 0.2643
Training Epoch: 47 [43136/50048]	Loss: 0.3571
Training Epoch: 47 [43264/50048]	Loss: 0.3607
Training Epoch: 47 [43392/50048]	Loss: 0.2931
Training Epoch: 47 [43520/50048]	Loss: 0.3518
Training Epoch: 47 [43648/50048]	Loss: 0.3123
Training Epoch: 47 [43776/50048]	Loss: 0.4002
Training Epoch: 47 [43904/50048]	Loss: 0.3121
Training Epoch: 47 [44032/50048]	Loss: 0.4050
Training Epoch: 47 [44160/50048]	Loss: 0.3771
Training Epoch: 47 [44288/50048]	Loss: 0.4494
Training Epoch: 47 [44416/50048]	Loss: 0.3216
Training Epoch: 47 [44544/50048]	Loss: 0.4180
Training Epoch: 47 [44672/50048]	Loss: 0.3383
Training Epoch: 47 [44800/50048]	Loss: 0.3685
Training Epoch: 47 [44928/50048]	Loss: 0.4341
Training Epoch: 47 [45056/50048]	Loss: 0.2646
Training Epoch: 47 [45184/50048]	Loss: 0.3626
Training Epoch: 47 [45312/50048]	Loss: 0.3582
Training Epoch: 47 [45440/50048]	Loss: 0.2895
Training Epoch: 47 [45568/50048]	Loss: 0.3328
Training Epoch: 47 [45696/50048]	Loss: 0.3115
2022-12-06 04:49:31,153 [ZeusDataLoader(train)] train epoch 48 done: time=86.54 energy=10512.10
2022-12-06 04:49:31,155 [ZeusDataLoader(eval)] Epoch 48 begin.
Training Epoch: 47 [45824/50048]	Loss: 0.3283
Training Epoch: 47 [45952/50048]	Loss: 0.3072
Training Epoch: 47 [46080/50048]	Loss: 0.4278
Training Epoch: 47 [46208/50048]	Loss: 0.3008
Training Epoch: 47 [46336/50048]	Loss: 0.4538
Training Epoch: 47 [46464/50048]	Loss: 0.2643
Training Epoch: 47 [46592/50048]	Loss: 0.2331
Training Epoch: 47 [46720/50048]	Loss: 0.2702
Training Epoch: 47 [46848/50048]	Loss: 0.2169
Training Epoch: 47 [46976/50048]	Loss: 0.2648
Training Epoch: 47 [47104/50048]	Loss: 0.4235
Training Epoch: 47 [47232/50048]	Loss: 0.2298
Training Epoch: 47 [47360/50048]	Loss: 0.3187
Training Epoch: 47 [47488/50048]	Loss: 0.3179
Training Epoch: 47 [47616/50048]	Loss: 0.3029
Training Epoch: 47 [47744/50048]	Loss: 0.2756
Training Epoch: 47 [47872/50048]	Loss: 0.3405
Training Epoch: 47 [48000/50048]	Loss: 0.3004
Training Epoch: 47 [48128/50048]	Loss: 0.2778
Training Epoch: 47 [48256/50048]	Loss: 0.3181
Training Epoch: 47 [48384/50048]	Loss: 0.3469
Training Epoch: 47 [48512/50048]	Loss: 0.2901
Training Epoch: 47 [48640/50048]	Loss: 0.3864
Training Epoch: 47 [48768/50048]	Loss: 0.4910
Training Epoch: 47 [48896/50048]	Loss: 0.2901
Training Epoch: 47 [49024/50048]	Loss: 0.2915
Training Epoch: 47 [49152/50048]	Loss: 0.4406
Training Epoch: 47 [49280/50048]	Loss: 0.2369
Training Epoch: 47 [49408/50048]	Loss: 0.3889
Training Epoch: 47 [49536/50048]	Loss: 0.3311
Training Epoch: 47 [49664/50048]	Loss: 0.3165
Training Epoch: 47 [49792/50048]	Loss: 0.3076
Training Epoch: 47 [49920/50048]	Loss: 0.3206
Training Epoch: 47 [50048/50048]	Loss: 0.4534
2022-12-06 09:49:34.878 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:49:34,921 [ZeusDataLoader(eval)] eval epoch 48 done: time=3.76 energy=453.35
2022-12-06 04:49:34,921 [ZeusDataLoader(train)] Up to epoch 48: time=4331.65, energy=525774.42, cost=641906.50
2022-12-06 04:49:34,921 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:49:34,921 [ZeusDataLoader(train)] Expected next epoch: time=4421.45, energy=536572.43, cost=655162.88
2022-12-06 04:49:34,922 [ZeusDataLoader(train)] Epoch 49 begin.
Validation Epoch: 47, Average loss: 0.0148, Accuracy: 0.6281
2022-12-06 04:49:35,061 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:49:35,061 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:49:35.063 [ZeusMonitor] Monitor started.
2022-12-06 09:49:35.063 [ZeusMonitor] Running indefinitely. 2022-12-06 09:49:35.063 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:49:35.063 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e49+gpu0.power.log
Training Epoch: 48 [128/50048]	Loss: 0.2616
Training Epoch: 48 [256/50048]	Loss: 0.2639
Training Epoch: 48 [384/50048]	Loss: 0.2961
Training Epoch: 48 [512/50048]	Loss: 0.3208
Training Epoch: 48 [640/50048]	Loss: 0.1886
Training Epoch: 48 [768/50048]	Loss: 0.2434
Training Epoch: 48 [896/50048]	Loss: 0.3033
Training Epoch: 48 [1024/50048]	Loss: 0.2123
Training Epoch: 48 [1152/50048]	Loss: 0.2821
Training Epoch: 48 [1280/50048]	Loss: 0.2774
Training Epoch: 48 [1408/50048]	Loss: 0.2764
Training Epoch: 48 [1536/50048]	Loss: 0.2740
Training Epoch: 48 [1664/50048]	Loss: 0.2686
Training Epoch: 48 [1792/50048]	Loss: 0.2144
Training Epoch: 48 [1920/50048]	Loss: 0.2590
Training Epoch: 48 [2048/50048]	Loss: 0.2877
Training Epoch: 48 [2176/50048]	Loss: 0.1534
Training Epoch: 48 [2304/50048]	Loss: 0.3528
Training Epoch: 48 [2432/50048]	Loss: 0.3196
Training Epoch: 48 [2560/50048]	Loss: 0.4185
Training Epoch: 48 [2688/50048]	Loss: 0.2003
Training Epoch: 48 [2816/50048]	Loss: 0.2698
Training Epoch: 48 [2944/50048]	Loss: 0.2088
Training Epoch: 48 [3072/50048]	Loss: 0.2913
Training Epoch: 48 [3200/50048]	Loss: 0.3186
Training Epoch: 48 [3328/50048]	Loss: 0.2257
Training Epoch: 48 [3456/50048]	Loss: 0.2208
Training Epoch: 48 [3584/50048]	Loss: 0.3477
Training Epoch: 48 [3712/50048]	Loss: 0.2888
Training Epoch: 48 [3840/50048]	Loss: 0.2611
Training Epoch: 48 [3968/50048]	Loss: 0.2223
Training Epoch: 48 [4096/50048]	Loss: 0.1992
Training Epoch: 48 [4224/50048]	Loss: 0.1687
Training Epoch: 48 [4352/50048]	Loss: 0.2779
Training Epoch: 48 [4480/50048]	Loss: 0.2323
Training Epoch: 48 [4608/50048]	Loss: 0.2060
Training Epoch: 48 [4736/50048]	Loss: 0.2035
Training Epoch: 48 [4864/50048]	Loss: 0.2357
Training Epoch: 48 [4992/50048]	Loss: 0.2887
Training Epoch: 48 [5120/50048]	Loss: 0.3379
Training Epoch: 48 [5248/50048]	Loss: 0.2847
Training Epoch: 48 [5376/50048]	Loss: 0.1621
Training Epoch: 48 [5504/50048]	Loss: 0.2329
Training Epoch: 48 [5632/50048]	Loss: 0.2243
Training Epoch: 48 [5760/50048]	Loss: 0.2197
Training Epoch: 48 [5888/50048]	Loss: 0.2110
Training Epoch: 48 [6016/50048]	Loss: 0.2109
Training Epoch: 48 [6144/50048]	Loss: 0.3040
Training Epoch: 48 [6272/50048]	Loss: 0.2272
Training Epoch: 48 [6400/50048]	Loss: 0.1371
Training Epoch: 48 [6528/50048]	Loss: 0.3901
Training Epoch: 48 [6656/50048]	Loss: 0.3168
Training Epoch: 48 [6784/50048]	Loss: 0.2103
Training Epoch: 48 [6912/50048]	Loss: 0.3128
Training Epoch: 48 [7040/50048]	Loss: 0.2147
Training Epoch: 48 [7168/50048]	Loss: 0.3520
Training Epoch: 48 [7296/50048]	Loss: 0.3334
Training Epoch: 48 [7424/50048]	Loss: 0.2969
Training Epoch: 48 [7552/50048]	Loss: 0.2671
Training Epoch: 48 [7680/50048]	Loss: 0.3081
Training Epoch: 48 [7808/50048]	Loss: 0.3014
Training Epoch: 48 [7936/50048]	Loss: 0.2498
Training Epoch: 48 [8064/50048]	Loss: 0.1807
Training Epoch: 48 [8192/50048]	Loss: 0.3105
Training Epoch: 48 [8320/50048]	Loss: 0.1752
Training Epoch: 48 [8448/50048]	Loss: 0.2895
Training Epoch: 48 [8576/50048]	Loss: 0.1425
Training Epoch: 48 [8704/50048]	Loss: 0.1922
Training Epoch: 48 [8832/50048]	Loss: 0.2820
Training Epoch: 48 [8960/50048]	Loss: 0.2145
Training Epoch: 48 [9088/50048]	Loss: 0.1904
Training Epoch: 48 [9216/50048]	Loss: 0.3088
Training Epoch: 48 [9344/50048]	Loss: 0.2882
Training Epoch: 48 [9472/50048]	Loss: 0.2647
Training Epoch: 48 [9600/50048]	Loss: 0.2504
Training Epoch: 48 [9728/50048]	Loss: 0.2080
Training Epoch: 48 [9856/50048]	Loss: 0.3070
Training Epoch: 48 [9984/50048]	Loss: 0.2445
Training Epoch: 48 [10112/50048]	Loss: 0.2465
Training Epoch: 48 [10240/50048]	Loss: 0.2834
Training Epoch: 48 [10368/50048]	Loss: 0.3486
Training Epoch: 48 [10496/50048]	Loss: 0.2927
Training Epoch: 48 [10624/50048]	Loss: 0.2110
Training Epoch: 48 [10752/50048]	Loss: 0.3204
Training Epoch: 48 [10880/50048]	Loss: 0.2427
Training Epoch: 48 [11008/50048]	Loss: 0.3015
Training Epoch: 48 [11136/50048]	Loss: 0.2304
Training Epoch: 48 [11264/50048]	Loss: 0.2122
Training Epoch: 48 [11392/50048]	Loss: 0.2756
Training Epoch: 48 [11520/50048]	Loss: 0.1943
Training Epoch: 48 [11648/50048]	Loss: 0.2391
Training Epoch: 48 [11776/50048]	Loss: 0.2632
Training Epoch: 48 [11904/50048]	Loss: 0.3032
Training Epoch: 48 [12032/50048]	Loss: 0.3249
Training Epoch: 48 [12160/50048]	Loss: 0.2731
Training Epoch: 48 [12288/50048]	Loss: 0.3717
Training Epoch: 48 [12416/50048]	Loss: 0.3359
Training Epoch: 48 [12544/50048]	Loss: 0.2811
Training Epoch: 48 [12672/50048]	Loss: 0.2946
Training Epoch: 48 [12800/50048]	Loss: 0.2136
Training Epoch: 48 [12928/50048]	Loss: 0.2512
Training Epoch: 48 [13056/50048]	Loss: 0.3295
Training Epoch: 48 [13184/50048]	Loss: 0.3162
Training Epoch: 48 [13312/50048]	Loss: 0.3119
Training Epoch: 48 [13440/50048]	Loss: 0.3433
Training Epoch: 48 [13568/50048]	Loss: 0.2769
Training Epoch: 48 [13696/50048]	Loss: 0.2716
Training Epoch: 48 [13824/50048]	Loss: 0.2567
Training Epoch: 48 [13952/50048]	Loss: 0.3847
Training Epoch: 48 [14080/50048]	Loss: 0.2803
Training Epoch: 48 [14208/50048]	Loss: 0.2487
Training Epoch: 48 [14336/50048]	Loss: 0.2360
Training Epoch: 48 [14464/50048]	Loss: 0.2817
Training Epoch: 48 [14592/50048]	Loss: 0.3073
Training Epoch: 48 [14720/50048]	Loss: 0.3545
Training Epoch: 48 [14848/50048]	Loss: 0.3374
Training Epoch: 48 [14976/50048]	Loss: 0.3869
Training Epoch: 48 [15104/50048]	Loss: 0.2649
Training Epoch: 48 [15232/50048]	Loss: 0.2330
Training Epoch: 48 [15360/50048]	Loss: 0.4276
Training Epoch: 48 [15488/50048]	Loss: 0.2829
Training Epoch: 48 [15616/50048]	Loss: 0.2147
Training Epoch: 48 [15744/50048]	Loss: 0.2570
Training Epoch: 48 [15872/50048]	Loss: 0.3250
Training Epoch: 48 [16000/50048]	Loss: 0.3146
Training Epoch: 48 [16128/50048]	Loss: 0.2042
Training Epoch: 48 [16256/50048]	Loss: 0.2691
Training Epoch: 48 [16384/50048]	Loss: 0.3517
Training Epoch: 48 [16512/50048]	Loss: 0.1715
Training Epoch: 48 [16640/50048]	Loss: 0.2716
Training Epoch: 48 [16768/50048]	Loss: 0.3138
Training Epoch: 48 [16896/50048]	Loss: 0.3452
Training Epoch: 48 [17024/50048]	Loss: 0.2790
Training Epoch: 48 [17152/50048]	Loss: 0.2797
Training Epoch: 48 [17280/50048]	Loss: 0.2688
Training Epoch: 48 [17408/50048]	Loss: 0.3284
Training Epoch: 48 [17536/50048]	Loss: 0.3043
Training Epoch: 48 [17664/50048]	Loss: 0.2643
Training Epoch: 48 [17792/50048]	Loss: 0.3435
Training Epoch: 48 [17920/50048]	Loss: 0.3243
Training Epoch: 48 [18048/50048]	Loss: 0.2296
Training Epoch: 48 [18176/50048]	Loss: 0.3564
Training Epoch: 48 [18304/50048]	Loss: 0.2249
Training Epoch: 48 [18432/50048]	Loss: 0.2529
Training Epoch: 48 [18560/50048]	Loss: 0.3051
Training Epoch: 48 [18688/50048]	Loss: 0.1783
Training Epoch: 48 [18816/50048]	Loss: 0.4359
Training Epoch: 48 [18944/50048]	Loss: 0.2645
Training Epoch: 48 [19072/50048]	Loss: 0.3877
Training Epoch: 48 [19200/50048]	Loss: 0.3789
Training Epoch: 48 [19328/50048]	Loss: 0.2362
Training Epoch: 48 [19456/50048]	Loss: 0.3291
Training Epoch: 48 [19584/50048]	Loss: 0.2319
Training Epoch: 48 [19712/50048]	Loss: 0.3507
Training Epoch: 48 [19840/50048]	Loss: 0.2850
Training Epoch: 48 [19968/50048]	Loss: 0.2803
Training Epoch: 48 [20096/50048]	Loss: 0.3377
Training Epoch: 48 [20224/50048]	Loss: 0.2582
Training Epoch: 48 [20352/50048]	Loss: 0.2632
Training Epoch: 48 [20480/50048]	Loss: 0.2812
Training Epoch: 48 [20608/50048]	Loss: 0.4087
Training Epoch: 48 [20736/50048]	Loss: 0.2491
Training Epoch: 48 [20864/50048]	Loss: 0.3937
Training Epoch: 48 [20992/50048]	Loss: 0.2908
Training Epoch: 48 [21120/50048]	Loss: 0.3620
Training Epoch: 48 [21248/50048]	Loss: 0.2495
Training Epoch: 48 [21376/50048]	Loss: 0.2679
Training Epoch: 48 [21504/50048]	Loss: 0.3010
Training Epoch: 48 [21632/50048]	Loss: 0.2492
Training Epoch: 48 [21760/50048]	Loss: 0.2163
Training Epoch: 48 [21888/50048]	Loss: 0.2572
Training Epoch: 48 [22016/50048]	Loss: 0.3495
Training Epoch: 48 [22144/50048]	Loss: 0.2213
Training Epoch: 48 [22272/50048]	Loss: 0.1720
Training Epoch: 48 [22400/50048]	Loss: 0.2842
Training Epoch: 48 [22528/50048]	Loss: 0.2812
Training Epoch: 48 [22656/50048]	Loss: 0.3446
Training Epoch: 48 [22784/50048]	Loss: 0.3873
Training Epoch: 48 [22912/50048]	Loss: 0.2546
Training Epoch: 48 [23040/50048]	Loss: 0.2573
Training Epoch: 48 [23168/50048]	Loss: 0.3280
Training Epoch: 48 [23296/50048]	Loss: 0.3473
Training Epoch: 48 [23424/50048]	Loss: 0.2479
Training Epoch: 48 [23552/50048]	Loss: 0.2866
Training Epoch: 48 [23680/50048]	Loss: 0.2903
Training Epoch: 48 [23808/50048]	Loss: 0.2118
Training Epoch: 48 [23936/50048]	Loss: 0.3250
Training Epoch: 48 [24064/50048]	Loss: 0.2753
Training Epoch: 48 [24192/50048]	Loss: 0.3946
Training Epoch: 48 [24320/50048]	Loss: 0.3383
Training Epoch: 48 [24448/50048]	Loss: 0.3817
Training Epoch: 48 [24576/50048]	Loss: 0.3897
Training Epoch: 48 [24704/50048]	Loss: 0.3591
Training Epoch: 48 [24832/50048]	Loss: 0.2075
Training Epoch: 48 [24960/50048]	Loss: 0.3240
Training Epoch: 48 [25088/50048]	Loss: 0.3543
Training Epoch: 48 [25216/50048]	Loss: 0.3410
Training Epoch: 48 [25344/50048]	Loss: 0.3523
Training Epoch: 48 [25472/50048]	Loss: 0.3095
Training Epoch: 48 [25600/50048]	Loss: 0.3442
Training Epoch: 48 [25728/50048]	Loss: 0.3459
Training Epoch: 48 [25856/50048]	Loss: 0.3601
Training Epoch: 48 [25984/50048]	Loss: 0.3582
Training Epoch: 48 [26112/50048]	Loss: 0.2580
Training Epoch: 48 [26240/50048]	Loss: 0.2674
Training Epoch: 48 [26368/50048]	Loss: 0.2949
Training Epoch: 48 [26496/50048]	Loss: 0.3678
Training Epoch: 48 [26624/50048]	Loss: 0.3042
Training Epoch: 48 [26752/50048]	Loss: 0.4141
Training Epoch: 48 [26880/50048]	Loss: 0.2973
Training Epoch: 48 [27008/50048]	Loss: 0.3458
Training Epoch: 48 [27136/50048]	Loss: 0.4139
Training Epoch: 48 [27264/50048]	Loss: 0.4026
Training Epoch: 48 [27392/50048]	Loss: 0.2277
Training Epoch: 48 [27520/50048]	Loss: 0.3107
Training Epoch: 48 [27648/50048]	Loss: 0.3334
Training Epoch: 48 [27776/50048]	Loss: 0.2846
Training Epoch: 48 [27904/50048]	Loss: 0.3013
Training Epoch: 48 [28032/50048]	Loss: 0.2611
Training Epoch: 48 [28160/50048]	Loss: 0.4047
Training Epoch: 48 [28288/50048]	Loss: 0.2506
Training Epoch: 48 [28416/50048]	Loss: 0.2303
Training Epoch: 48 [28544/50048]	Loss: 0.2398
Training Epoch: 48 [28672/50048]	Loss: 0.2411
Training Epoch: 48 [28800/50048]	Loss: 0.2804
Training Epoch: 48 [28928/50048]	Loss: 0.3224
Training Epoch: 48 [29056/50048]	Loss: 0.5008
Training Epoch: 48 [29184/50048]	Loss: 0.3322
Training Epoch: 48 [29312/50048]	Loss: 0.3419
Training Epoch: 48 [29440/50048]	Loss: 0.3873
Training Epoch: 48 [29568/50048]	Loss: 0.3876
Training Epoch: 48 [29696/50048]	Loss: 0.2600
Training Epoch: 48 [29824/50048]	Loss: 0.2427
Training Epoch: 48 [29952/50048]	Loss: 0.3021
Training Epoch: 48 [30080/50048]	Loss: 0.2022
Training Epoch: 48 [30208/50048]	Loss: 0.2798
Training Epoch: 48 [30336/50048]	Loss: 0.4731
Training Epoch: 48 [30464/50048]	Loss: 0.3294
Training Epoch: 48 [30592/50048]	Loss: 0.2943
Training Epoch: 48 [30720/50048]	Loss: 0.3080
Training Epoch: 48 [30848/50048]	Loss: 0.2768
Training Epoch: 48 [30976/50048]	Loss: 0.3993
Training Epoch: 48 [31104/50048]	Loss: 0.3385
Training Epoch: 48 [31232/50048]	Loss: 0.2902
Training Epoch: 48 [31360/50048]	Loss: 0.3574
Training Epoch: 48 [31488/50048]	Loss: 0.2786
Training Epoch: 48 [31616/50048]	Loss: 0.2009
Training Epoch: 48 [31744/50048]	Loss: 0.3237
Training Epoch: 48 [31872/50048]	Loss: 0.4307
Training Epoch: 48 [32000/50048]	Loss: 0.3690
Training Epoch: 48 [32128/50048]	Loss: 0.2385
Training Epoch: 48 [32256/50048]	Loss: 0.4139
Training Epoch: 48 [32384/50048]	Loss: 0.3754
Training Epoch: 48 [32512/50048]	Loss: 0.3844
Training Epoch: 48 [32640/50048]	Loss: 0.4214
Training Epoch: 48 [32768/50048]	Loss: 0.3207
Training Epoch: 48 [32896/50048]	Loss: 0.2933
Training Epoch: 48 [33024/50048]	Loss: 0.2346
Training Epoch: 48 [33152/50048]	Loss: 0.3586
Training Epoch: 48 [33280/50048]	Loss: 0.2779
Training Epoch: 48 [33408/50048]	Loss: 0.3568
Training Epoch: 48 [33536/50048]	Loss: 0.2454
Training Epoch: 48 [33664/50048]	Loss: 0.4389
Training Epoch: 48 [33792/50048]	Loss: 0.3042
Training Epoch: 48 [33920/50048]	Loss: 0.2639
Training Epoch: 48 [34048/50048]	Loss: 0.2216
Training Epoch: 48 [34176/50048]	Loss: 0.2582
Training Epoch: 48 [34304/50048]	Loss: 0.2425
Training Epoch: 48 [34432/50048]	Loss: 0.3123
Training Epoch: 48 [34560/50048]	Loss: 0.3969
Training Epoch: 48 [34688/50048]	Loss: 0.2839
Training Epoch: 48 [34816/50048]	Loss: 0.3294
Training Epoch: 48 [34944/50048]	Loss: 0.2639
Training Epoch: 48 [35072/50048]	Loss: 0.2449
Training Epoch: 48 [35200/50048]	Loss: 0.2684
Training Epoch: 48 [35328/50048]	Loss: 0.3174
Training Epoch: 48 [35456/50048]	Loss: 0.3466
Training Epoch: 48 [35584/50048]	Loss: 0.2434
Training Epoch: 48 [35712/50048]	Loss: 0.2247
Training Epoch: 48 [35840/50048]	Loss: 0.3649
Training Epoch: 48 [35968/50048]	Loss: 0.3300
Training Epoch: 48 [36096/50048]	Loss: 0.2239
Training Epoch: 48 [36224/50048]	Loss: 0.3655
Training Epoch: 48 [36352/50048]	Loss: 0.1919
Training Epoch: 48 [36480/50048]	Loss: 0.3649
Training Epoch: 48 [36608/50048]	Loss: 0.2139
Training Epoch: 48 [36736/50048]	Loss: 0.2879
Training Epoch: 48 [36864/50048]	Loss: 0.3235
Training Epoch: 48 [36992/50048]	Loss: 0.2893
Training Epoch: 48 [37120/50048]	Loss: 0.3220
Training Epoch: 48 [37248/50048]	Loss: 0.3901
Training Epoch: 48 [37376/50048]	Loss: 0.2776
Training Epoch: 48 [37504/50048]	Loss: 0.2390
Training Epoch: 48 [37632/50048]	Loss: 0.4384
Training Epoch: 48 [37760/50048]	Loss: 0.2257
Training Epoch: 48 [37888/50048]	Loss: 0.4708
Training Epoch: 48 [38016/50048]	Loss: 0.2607
Training Epoch: 48 [38144/50048]	Loss: 0.2899
Training Epoch: 48 [38272/50048]	Loss: 0.2360
Training Epoch: 48 [38400/50048]	Loss: 0.3925
Training Epoch: 48 [38528/50048]	Loss: 0.3613
Training Epoch: 48 [38656/50048]	Loss: 0.4080
Training Epoch: 48 [38784/50048]	Loss: 0.3101
Training Epoch: 48 [38912/50048]	Loss: 0.4911
Training Epoch: 48 [39040/50048]	Loss: 0.4432
Training Epoch: 48 [39168/50048]	Loss: 0.2044
Training Epoch: 48 [39296/50048]	Loss: 0.3417
Training Epoch: 48 [39424/50048]	Loss: 0.4251
Training Epoch: 48 [39552/50048]	Loss: 0.3012
Training Epoch: 48 [39680/50048]	Loss: 0.3390
Training Epoch: 48 [39808/50048]	Loss: 0.3124
Training Epoch: 48 [39936/50048]	Loss: 0.3101
Training Epoch: 48 [40064/50048]	Loss: 0.4492
Training Epoch: 48 [40192/50048]	Loss: 0.2837
Training Epoch: 48 [40320/50048]	Loss: 0.2955
Training Epoch: 48 [40448/50048]	Loss: 0.3342
Training Epoch: 48 [40576/50048]	Loss: 0.3587
Training Epoch: 48 [40704/50048]	Loss: 0.4317
Training Epoch: 48 [40832/50048]	Loss: 0.4806
Training Epoch: 48 [40960/50048]	Loss: 0.5716
Training Epoch: 48 [41088/50048]	Loss: 0.3865
Training Epoch: 48 [41216/50048]	Loss: 0.1741
Training Epoch: 48 [41344/50048]	Loss: 0.2640
Training Epoch: 48 [41472/50048]	Loss: 0.3961
Training Epoch: 48 [41600/50048]	Loss: 0.2587
Training Epoch: 48 [41728/50048]	Loss: 0.2973
Training Epoch: 48 [41856/50048]	Loss: 0.2853
Training Epoch: 48 [41984/50048]	Loss: 0.1660
Training Epoch: 48 [42112/50048]	Loss: 0.4308
Training Epoch: 48 [42240/50048]	Loss: 0.2293
Training Epoch: 48 [42368/50048]	Loss: 0.3103
Training Epoch: 48 [42496/50048]	Loss: 0.3679
Training Epoch: 48 [42624/50048]	Loss: 0.2995
Training Epoch: 48 [42752/50048]	Loss: 0.3298
Training Epoch: 48 [42880/50048]	Loss: 0.4329
Training Epoch: 48 [43008/50048]	Loss: 0.4682
Training Epoch: 48 [43136/50048]	Loss: 0.2340
Training Epoch: 48 [43264/50048]	Loss: 0.2436
Training Epoch: 48 [43392/50048]	Loss: 0.2807
Training Epoch: 48 [43520/50048]	Loss: 0.3730
Training Epoch: 48 [43648/50048]	Loss: 0.4803
Training Epoch: 48 [43776/50048]	Loss: 0.5030
Training Epoch: 48 [43904/50048]	Loss: 0.3209
Training Epoch: 48 [44032/50048]	Loss: 0.3023
Training Epoch: 48 [44160/50048]	Loss: 0.2676
Training Epoch: 48 [44288/50048]	Loss: 0.4074
Training Epoch: 48 [44416/50048]	Loss: 0.2992
Training Epoch: 48 [44544/50048]	Loss: 0.3558
Training Epoch: 48 [44672/50048]	Loss: 0.3436
Training Epoch: 48 [44800/50048]	Loss: 0.2449
Training Epoch: 48 [44928/50048]	Loss: 0.4047
Training Epoch: 48 [45056/50048]	Loss: 0.3163
Training Epoch: 48 [45184/50048]	Loss: 0.2498
Training Epoch: 48 [45312/50048]	Loss: 0.3278
Training Epoch: 48 [45440/50048]	Loss: 0.3142
Training Epoch: 48 [45568/50048]	Loss: 0.3204
Training Epoch: 48 [45696/50048]	Loss: 0.3625
2022-12-06 04:51:01,327 [ZeusDataLoader(train)] train epoch 49 done: time=86.39 energy=10497.82
2022-12-06 04:51:01,329 [ZeusDataLoader(eval)] Epoch 49 begin.
Training Epoch: 48 [45824/50048]	Loss: 0.3935
Training Epoch: 48 [45952/50048]	Loss: 0.2891
Training Epoch: 48 [46080/50048]	Loss: 0.3529
Training Epoch: 48 [46208/50048]	Loss: 0.3232
Training Epoch: 48 [46336/50048]	Loss: 0.3260
Training Epoch: 48 [46464/50048]	Loss: 0.3574
Training Epoch: 48 [46592/50048]	Loss: 0.3117
Training Epoch: 48 [46720/50048]	Loss: 0.5007
Training Epoch: 48 [46848/50048]	Loss: 0.3995
Training Epoch: 48 [46976/50048]	Loss: 0.1909
Training Epoch: 48 [47104/50048]	Loss: 0.3290
Training Epoch: 48 [47232/50048]	Loss: 0.2361
Training Epoch: 48 [47360/50048]	Loss: 0.3810
Training Epoch: 48 [47488/50048]	Loss: 0.3647
Training Epoch: 48 [47616/50048]	Loss: 0.3406
Training Epoch: 48 [47744/50048]	Loss: 0.2454
Training Epoch: 48 [47872/50048]	Loss: 0.2264
Training Epoch: 48 [48000/50048]	Loss: 0.2307
Training Epoch: 48 [48128/50048]	Loss: 0.3348
Training Epoch: 48 [48256/50048]	Loss: 0.5339
Training Epoch: 48 [48384/50048]	Loss: 0.3400
Training Epoch: 48 [48512/50048]	Loss: 0.3478
Training Epoch: 48 [48640/50048]	Loss: 0.3114
Training Epoch: 48 [48768/50048]	Loss: 0.3736
Training Epoch: 48 [48896/50048]	Loss: 0.2975
Training Epoch: 48 [49024/50048]	Loss: 0.2144
Training Epoch: 48 [49152/50048]	Loss: 0.2271
Training Epoch: 48 [49280/50048]	Loss: 0.3785
Training Epoch: 48 [49408/50048]	Loss: 0.2849
Training Epoch: 48 [49536/50048]	Loss: 0.4226
Training Epoch: 48 [49664/50048]	Loss: 0.4321
Training Epoch: 48 [49792/50048]	Loss: 0.4948
Training Epoch: 48 [49920/50048]	Loss: 0.4045
Training Epoch: 48 [50048/50048]	Loss: 0.4737
2022-12-06 09:51:05.048 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:51:05,057 [ZeusDataLoader(eval)] eval epoch 49 done: time=3.72 energy=451.72
2022-12-06 04:51:05,057 [ZeusDataLoader(train)] Up to epoch 49: time=4421.76, energy=536723.96, cost=655266.27
2022-12-06 04:51:05,058 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:51:05,058 [ZeusDataLoader(train)] Expected next epoch: time=4511.56, energy=547521.98, cost=668522.66
2022-12-06 04:51:05,059 [ZeusDataLoader(train)] Epoch 50 begin.
Validation Epoch: 48, Average loss: 0.0148, Accuracy: 0.6297
2022-12-06 04:51:05,244 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:51:05,245 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:51:05.248 [ZeusMonitor] Monitor started.
2022-12-06 09:51:05.249 [ZeusMonitor] Running indefinitely. 2022-12-06 09:51:05.249 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:51:05.249 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e50+gpu0.power.log
Training Epoch: 49 [128/50048]	Loss: 0.1766
Training Epoch: 49 [256/50048]	Loss: 0.1969
Training Epoch: 49 [384/50048]	Loss: 0.2574
Training Epoch: 49 [512/50048]	Loss: 0.2354
Training Epoch: 49 [640/50048]	Loss: 0.2577
Training Epoch: 49 [768/50048]	Loss: 0.2325
Training Epoch: 49 [896/50048]	Loss: 0.2450
Training Epoch: 49 [1024/50048]	Loss: 0.2108
Training Epoch: 49 [1152/50048]	Loss: 0.2754
Training Epoch: 49 [1280/50048]	Loss: 0.2339
Training Epoch: 49 [1408/50048]	Loss: 0.3058
Training Epoch: 49 [1536/50048]	Loss: 0.2037
Training Epoch: 49 [1664/50048]	Loss: 0.2301
Training Epoch: 49 [1792/50048]	Loss: 0.1306
Training Epoch: 49 [1920/50048]	Loss: 0.3095
Training Epoch: 49 [2048/50048]	Loss: 0.2718
Training Epoch: 49 [2176/50048]	Loss: 0.2213
Training Epoch: 49 [2304/50048]	Loss: 0.2896
Training Epoch: 49 [2432/50048]	Loss: 0.2631
Training Epoch: 49 [2560/50048]	Loss: 0.3196
Training Epoch: 49 [2688/50048]	Loss: 0.2995
Training Epoch: 49 [2816/50048]	Loss: 0.1849
Training Epoch: 49 [2944/50048]	Loss: 0.2834
Training Epoch: 49 [3072/50048]	Loss: 0.3686
Training Epoch: 49 [3200/50048]	Loss: 0.3359
Training Epoch: 49 [3328/50048]	Loss: 0.2221
Training Epoch: 49 [3456/50048]	Loss: 0.2664
Training Epoch: 49 [3584/50048]	Loss: 0.2879
Training Epoch: 49 [3712/50048]	Loss: 0.2737
Training Epoch: 49 [3840/50048]	Loss: 0.3352
Training Epoch: 49 [3968/50048]	Loss: 0.2432
Training Epoch: 49 [4096/50048]	Loss: 0.2705
Training Epoch: 49 [4224/50048]	Loss: 0.3157
Training Epoch: 49 [4352/50048]	Loss: 0.2403
Training Epoch: 49 [4480/50048]	Loss: 0.2976
Training Epoch: 49 [4608/50048]	Loss: 0.2306
Training Epoch: 49 [4736/50048]	Loss: 0.2640
Training Epoch: 49 [4864/50048]	Loss: 0.2616
Training Epoch: 49 [4992/50048]	Loss: 0.3493
Training Epoch: 49 [5120/50048]	Loss: 0.3094
Training Epoch: 49 [5248/50048]	Loss: 0.2626
Training Epoch: 49 [5376/50048]	Loss: 0.2847
Training Epoch: 49 [5504/50048]	Loss: 0.2678
Training Epoch: 49 [5632/50048]	Loss: 0.1546
Training Epoch: 49 [5760/50048]	Loss: 0.3104
Training Epoch: 49 [5888/50048]	Loss: 0.4233
Training Epoch: 49 [6016/50048]	Loss: 0.3423
Training Epoch: 49 [6144/50048]	Loss: 0.2363
Training Epoch: 49 [6272/50048]	Loss: 0.2564
Training Epoch: 49 [6400/50048]	Loss: 0.2544
Training Epoch: 49 [6528/50048]	Loss: 0.2135
Training Epoch: 49 [6656/50048]	Loss: 0.2396
Training Epoch: 49 [6784/50048]	Loss: 0.2872
Training Epoch: 49 [6912/50048]	Loss: 0.2235
Training Epoch: 49 [7040/50048]	Loss: 0.2200
Training Epoch: 49 [7168/50048]	Loss: 0.3673
Training Epoch: 49 [7296/50048]	Loss: 0.2925
Training Epoch: 49 [7424/50048]	Loss: 0.2026
Training Epoch: 49 [7552/50048]	Loss: 0.2537
Training Epoch: 49 [7680/50048]	Loss: 0.1545
Training Epoch: 49 [7808/50048]	Loss: 0.1949
Training Epoch: 49 [7936/50048]	Loss: 0.2510
Training Epoch: 49 [8064/50048]	Loss: 0.3491
Training Epoch: 49 [8192/50048]	Loss: 0.3278
Training Epoch: 49 [8320/50048]	Loss: 0.2951
Training Epoch: 49 [8448/50048]	Loss: 0.2294
Training Epoch: 49 [8576/50048]	Loss: 0.2104
Training Epoch: 49 [8704/50048]	Loss: 0.3018
Training Epoch: 49 [8832/50048]	Loss: 0.2413
Training Epoch: 49 [8960/50048]	Loss: 0.2553
Training Epoch: 49 [9088/50048]	Loss: 0.2850
Training Epoch: 49 [9216/50048]	Loss: 0.2355
Training Epoch: 49 [9344/50048]	Loss: 0.1884
Training Epoch: 49 [9472/50048]	Loss: 0.2902
Training Epoch: 49 [9600/50048]	Loss: 0.3198
Training Epoch: 49 [9728/50048]	Loss: 0.2448
Training Epoch: 49 [9856/50048]	Loss: 0.1547
Training Epoch: 49 [9984/50048]	Loss: 0.2081
Training Epoch: 49 [10112/50048]	Loss: 0.1715
Training Epoch: 49 [10240/50048]	Loss: 0.1327
Training Epoch: 49 [10368/50048]	Loss: 0.2763
Training Epoch: 49 [10496/50048]	Loss: 0.2742
Training Epoch: 49 [10624/50048]	Loss: 0.2383
Training Epoch: 49 [10752/50048]	Loss: 0.4187
Training Epoch: 49 [10880/50048]	Loss: 0.3957
Training Epoch: 49 [11008/50048]	Loss: 0.2352
Training Epoch: 49 [11136/50048]	Loss: 0.3909
Training Epoch: 49 [11264/50048]	Loss: 0.2109
Training Epoch: 49 [11392/50048]	Loss: 0.2434
Training Epoch: 49 [11520/50048]	Loss: 0.3402
Training Epoch: 49 [11648/50048]	Loss: 0.2286
Training Epoch: 49 [11776/50048]	Loss: 0.3912
Training Epoch: 49 [11904/50048]	Loss: 0.2713
Training Epoch: 49 [12032/50048]	Loss: 0.2968
Training Epoch: 49 [12160/50048]	Loss: 0.2463
Training Epoch: 49 [12288/50048]	Loss: 0.1825
Training Epoch: 49 [12416/50048]	Loss: 0.1773
Training Epoch: 49 [12544/50048]	Loss: 0.3681
Training Epoch: 49 [12672/50048]	Loss: 0.3295
Training Epoch: 49 [12800/50048]	Loss: 0.3484
Training Epoch: 49 [12928/50048]	Loss: 0.2131
Training Epoch: 49 [13056/50048]	Loss: 0.2234
Training Epoch: 49 [13184/50048]	Loss: 0.3521
Training Epoch: 49 [13312/50048]	Loss: 0.3868
Training Epoch: 49 [13440/50048]	Loss: 0.2650
Training Epoch: 49 [13568/50048]	Loss: 0.2728
Training Epoch: 49 [13696/50048]	Loss: 0.2407
Training Epoch: 49 [13824/50048]	Loss: 0.2524
Training Epoch: 49 [13952/50048]	Loss: 0.1743
Training Epoch: 49 [14080/50048]	Loss: 0.2918
Training Epoch: 49 [14208/50048]	Loss: 0.3183
Training Epoch: 49 [14336/50048]	Loss: 0.3707
Training Epoch: 49 [14464/50048]	Loss: 0.2578
Training Epoch: 49 [14592/50048]	Loss: 0.2710
Training Epoch: 49 [14720/50048]	Loss: 0.2566
Training Epoch: 49 [14848/50048]	Loss: 0.2976
Training Epoch: 49 [14976/50048]	Loss: 0.2545
Training Epoch: 49 [15104/50048]	Loss: 0.2877
Training Epoch: 49 [15232/50048]	Loss: 0.2683
Training Epoch: 49 [15360/50048]	Loss: 0.2924
Training Epoch: 49 [15488/50048]	Loss: 0.2693
Training Epoch: 49 [15616/50048]	Loss: 0.1956
Training Epoch: 49 [15744/50048]	Loss: 0.3192
Training Epoch: 49 [15872/50048]	Loss: 0.2643
Training Epoch: 49 [16000/50048]	Loss: 0.2764
Training Epoch: 49 [16128/50048]	Loss: 0.3092
Training Epoch: 49 [16256/50048]	Loss: 0.3234
Training Epoch: 49 [16384/50048]	Loss: 0.1391
Training Epoch: 49 [16512/50048]	Loss: 0.2152
Training Epoch: 49 [16640/50048]	Loss: 0.3633
Training Epoch: 49 [16768/50048]	Loss: 0.2107
Training Epoch: 49 [16896/50048]	Loss: 0.2706
Training Epoch: 49 [17024/50048]	Loss: 0.2843
Training Epoch: 49 [17152/50048]	Loss: 0.3029
Training Epoch: 49 [17280/50048]	Loss: 0.1563
Training Epoch: 49 [17408/50048]	Loss: 0.2934
Training Epoch: 49 [17536/50048]	Loss: 0.2543
Training Epoch: 49 [17664/50048]	Loss: 0.2190
Training Epoch: 49 [17792/50048]	Loss: 0.2802
Training Epoch: 49 [17920/50048]	Loss: 0.2441
Training Epoch: 49 [18048/50048]	Loss: 0.3346
Training Epoch: 49 [18176/50048]	Loss: 0.4259
Training Epoch: 49 [18304/50048]	Loss: 0.3496
Training Epoch: 49 [18432/50048]	Loss: 0.3510
Training Epoch: 49 [18560/50048]	Loss: 0.2970
Training Epoch: 49 [18688/50048]	Loss: 0.1869
Training Epoch: 49 [18816/50048]	Loss: 0.2704
Training Epoch: 49 [18944/50048]	Loss: 0.3192
Training Epoch: 49 [19072/50048]	Loss: 0.2441
Training Epoch: 49 [19200/50048]	Loss: 0.1688
Training Epoch: 49 [19328/50048]	Loss: 0.2407
Training Epoch: 49 [19456/50048]	Loss: 0.3368
Training Epoch: 49 [19584/50048]	Loss: 0.2141
Training Epoch: 49 [19712/50048]	Loss: 0.2389
Training Epoch: 49 [19840/50048]	Loss: 0.2842
Training Epoch: 49 [19968/50048]	Loss: 0.2857
Training Epoch: 49 [20096/50048]	Loss: 0.1675
Training Epoch: 49 [20224/50048]	Loss: 0.2116
Training Epoch: 49 [20352/50048]	Loss: 0.3776
Training Epoch: 49 [20480/50048]	Loss: 0.2319
Training Epoch: 49 [20608/50048]	Loss: 0.2856
Training Epoch: 49 [20736/50048]	Loss: 0.3625
Training Epoch: 49 [20864/50048]	Loss: 0.3114
Training Epoch: 49 [20992/50048]	Loss: 0.2577
Training Epoch: 49 [21120/50048]	Loss: 0.3100
Training Epoch: 49 [21248/50048]	Loss: 0.1856
Training Epoch: 49 [21376/50048]	Loss: 0.2831
Training Epoch: 49 [21504/50048]	Loss: 0.2066
Training Epoch: 49 [21632/50048]	Loss: 0.2835
Training Epoch: 49 [21760/50048]	Loss: 0.2515
Training Epoch: 49 [21888/50048]	Loss: 0.2350
Training Epoch: 49 [22016/50048]	Loss: 0.4234
Training Epoch: 49 [22144/50048]	Loss: 0.2287
Training Epoch: 49 [22272/50048]	Loss: 0.4413
Training Epoch: 49 [22400/50048]	Loss: 0.3251
Training Epoch: 49 [22528/50048]	Loss: 0.2989
Training Epoch: 49 [22656/50048]	Loss: 0.3625
Training Epoch: 49 [22784/50048]	Loss: 0.2591
Training Epoch: 49 [22912/50048]	Loss: 0.2915
Training Epoch: 49 [23040/50048]	Loss: 0.3504
Training Epoch: 49 [23168/50048]	Loss: 0.2102
Training Epoch: 49 [23296/50048]	Loss: 0.2680
Training Epoch: 49 [23424/50048]	Loss: 0.2510
Training Epoch: 49 [23552/50048]	Loss: 0.3458
Training Epoch: 49 [23680/50048]	Loss: 0.2799
Training Epoch: 49 [23808/50048]	Loss: 0.3240
Training Epoch: 49 [23936/50048]	Loss: 0.2773
Training Epoch: 49 [24064/50048]	Loss: 0.2982
Training Epoch: 49 [24192/50048]	Loss: 0.3390
Training Epoch: 49 [24320/50048]	Loss: 0.2968
Training Epoch: 49 [24448/50048]	Loss: 0.1984
Training Epoch: 49 [24576/50048]	Loss: 0.2696
Training Epoch: 49 [24704/50048]	Loss: 0.3263
Training Epoch: 49 [24832/50048]	Loss: 0.3311
Training Epoch: 49 [24960/50048]	Loss: 0.2685
Training Epoch: 49 [25088/50048]	Loss: 0.2729
Training Epoch: 49 [25216/50048]	Loss: 0.1705
Training Epoch: 49 [25344/50048]	Loss: 0.3180
Training Epoch: 49 [25472/50048]	Loss: 0.3770
Training Epoch: 49 [25600/50048]	Loss: 0.3394
Training Epoch: 49 [25728/50048]	Loss: 0.2160
Training Epoch: 49 [25856/50048]	Loss: 0.1939
Training Epoch: 49 [25984/50048]	Loss: 0.2769
Training Epoch: 49 [26112/50048]	Loss: 0.4353
Training Epoch: 49 [26240/50048]	Loss: 0.1547
Training Epoch: 49 [26368/50048]	Loss: 0.2396
Training Epoch: 49 [26496/50048]	Loss: 0.2258
Training Epoch: 49 [26624/50048]	Loss: 0.2003
Training Epoch: 49 [26752/50048]	Loss: 0.3091
Training Epoch: 49 [26880/50048]	Loss: 0.1957
Training Epoch: 49 [27008/50048]	Loss: 0.2404
Training Epoch: 49 [27136/50048]	Loss: 0.4164
Training Epoch: 49 [27264/50048]	Loss: 0.2944
Training Epoch: 49 [27392/50048]	Loss: 0.3604
Training Epoch: 49 [27520/50048]	Loss: 0.2297
Training Epoch: 49 [27648/50048]	Loss: 0.2813
Training Epoch: 49 [27776/50048]	Loss: 0.2425
Training Epoch: 49 [27904/50048]	Loss: 0.2546
Training Epoch: 49 [28032/50048]	Loss: 0.3340
Training Epoch: 49 [28160/50048]	Loss: 0.2088
Training Epoch: 49 [28288/50048]	Loss: 0.4297
Training Epoch: 49 [28416/50048]	Loss: 0.2800
Training Epoch: 49 [28544/50048]	Loss: 0.3056
Training Epoch: 49 [28672/50048]	Loss: 0.2671
Training Epoch: 49 [28800/50048]	Loss: 0.2626
Training Epoch: 49 [28928/50048]	Loss: 0.2463
Training Epoch: 49 [29056/50048]	Loss: 0.1901
Training Epoch: 49 [29184/50048]	Loss: 0.2376
Training Epoch: 49 [29312/50048]	Loss: 0.2885
Training Epoch: 49 [29440/50048]	Loss: 0.2966
Training Epoch: 49 [29568/50048]	Loss: 0.2800
Training Epoch: 49 [29696/50048]	Loss: 0.3283
Training Epoch: 49 [29824/50048]	Loss: 0.2750
Training Epoch: 49 [29952/50048]	Loss: 0.3764
Training Epoch: 49 [30080/50048]	Loss: 0.2880
Training Epoch: 49 [30208/50048]	Loss: 0.3341
Training Epoch: 49 [30336/50048]	Loss: 0.2389
Training Epoch: 49 [30464/50048]	Loss: 0.3281
Training Epoch: 49 [30592/50048]	Loss: 0.3032
Training Epoch: 49 [30720/50048]	Loss: 0.3810
Training Epoch: 49 [30848/50048]	Loss: 0.3276
Training Epoch: 49 [30976/50048]	Loss: 0.4478
Training Epoch: 49 [31104/50048]	Loss: 0.2567
Training Epoch: 49 [31232/50048]	Loss: 0.3815
Training Epoch: 49 [31360/50048]	Loss: 0.2103
Training Epoch: 49 [31488/50048]	Loss: 0.3707
Training Epoch: 49 [31616/50048]	Loss: 0.3132
Training Epoch: 49 [31744/50048]	Loss: 0.3253
Training Epoch: 49 [31872/50048]	Loss: 0.2536
Training Epoch: 49 [32000/50048]	Loss: 0.3235
Training Epoch: 49 [32128/50048]	Loss: 0.2659
Training Epoch: 49 [32256/50048]	Loss: 0.2775
Training Epoch: 49 [32384/50048]	Loss: 0.3390
Training Epoch: 49 [32512/50048]	Loss: 0.3913
Training Epoch: 49 [32640/50048]	Loss: 0.2335
Training Epoch: 49 [32768/50048]	Loss: 0.2749
Training Epoch: 49 [32896/50048]	Loss: 0.2684
Training Epoch: 49 [33024/50048]	Loss: 0.1930
Training Epoch: 49 [33152/50048]	Loss: 0.2965
Training Epoch: 49 [33280/50048]	Loss: 0.2441
Training Epoch: 49 [33408/50048]	Loss: 0.1641
Training Epoch: 49 [33536/50048]	Loss: 0.3062
Training Epoch: 49 [33664/50048]	Loss: 0.1584
Training Epoch: 49 [33792/50048]	Loss: 0.3407
Training Epoch: 49 [33920/50048]	Loss: 0.3567
Training Epoch: 49 [34048/50048]	Loss: 0.2395
Training Epoch: 49 [34176/50048]	Loss: 0.3258
Training Epoch: 49 [34304/50048]	Loss: 0.2555
Training Epoch: 49 [34432/50048]	Loss: 0.2581
Training Epoch: 49 [34560/50048]	Loss: 0.2479
Training Epoch: 49 [34688/50048]	Loss: 0.3088
Training Epoch: 49 [34816/50048]	Loss: 0.3393
Training Epoch: 49 [34944/50048]	Loss: 0.3583
Training Epoch: 49 [35072/50048]	Loss: 0.2516
Training Epoch: 49 [35200/50048]	Loss: 0.3314
Training Epoch: 49 [35328/50048]	Loss: 0.2359
Training Epoch: 49 [35456/50048]	Loss: 0.3759
Training Epoch: 49 [35584/50048]	Loss: 0.4945
Training Epoch: 49 [35712/50048]	Loss: 0.2812
Training Epoch: 49 [35840/50048]	Loss: 0.2341
Training Epoch: 49 [35968/50048]	Loss: 0.3214
Training Epoch: 49 [36096/50048]	Loss: 0.3024
Training Epoch: 49 [36224/50048]	Loss: 0.4541
Training Epoch: 49 [36352/50048]	Loss: 0.2127
Training Epoch: 49 [36480/50048]	Loss: 0.1927
Training Epoch: 49 [36608/50048]	Loss: 0.3825
Training Epoch: 49 [36736/50048]	Loss: 0.3869
Training Epoch: 49 [36864/50048]	Loss: 0.2465
Training Epoch: 49 [36992/50048]	Loss: 0.3162
Training Epoch: 49 [37120/50048]	Loss: 0.1776
Training Epoch: 49 [37248/50048]	Loss: 0.3401
Training Epoch: 49 [37376/50048]	Loss: 0.4281
Training Epoch: 49 [37504/50048]	Loss: 0.2761
Training Epoch: 49 [37632/50048]	Loss: 0.1888
Training Epoch: 49 [37760/50048]	Loss: 0.3172
Training Epoch: 49 [37888/50048]	Loss: 0.3211
Training Epoch: 49 [38016/50048]	Loss: 0.2878
Training Epoch: 49 [38144/50048]	Loss: 0.4833
Training Epoch: 49 [38272/50048]	Loss: 0.3219
Training Epoch: 49 [38400/50048]	Loss: 0.4630
Training Epoch: 49 [38528/50048]	Loss: 0.3659
Training Epoch: 49 [38656/50048]	Loss: 0.5044
Training Epoch: 49 [38784/50048]	Loss: 0.2279
Training Epoch: 49 [38912/50048]	Loss: 0.3613
Training Epoch: 49 [39040/50048]	Loss: 0.2911
Training Epoch: 49 [39168/50048]	Loss: 0.2729
Training Epoch: 49 [39296/50048]	Loss: 0.2441
Training Epoch: 49 [39424/50048]	Loss: 0.3944
Training Epoch: 49 [39552/50048]	Loss: 0.2691
Training Epoch: 49 [39680/50048]	Loss: 0.3187
Training Epoch: 49 [39808/50048]	Loss: 0.3315
Training Epoch: 49 [39936/50048]	Loss: 0.2243
Training Epoch: 49 [40064/50048]	Loss: 0.1471
Training Epoch: 49 [40192/50048]	Loss: 0.3960
Training Epoch: 49 [40320/50048]	Loss: 0.1671
Training Epoch: 49 [40448/50048]	Loss: 0.3308
Training Epoch: 49 [40576/50048]	Loss: 0.2525
Training Epoch: 49 [40704/50048]	Loss: 0.2953
Training Epoch: 49 [40832/50048]	Loss: 0.2122
Training Epoch: 49 [40960/50048]	Loss: 0.4113
Training Epoch: 49 [41088/50048]	Loss: 0.3568
Training Epoch: 49 [41216/50048]	Loss: 0.4106
Training Epoch: 49 [41344/50048]	Loss: 0.3517
Training Epoch: 49 [41472/50048]	Loss: 0.2538
Training Epoch: 49 [41600/50048]	Loss: 0.3107
Training Epoch: 49 [41728/50048]	Loss: 0.3326
Training Epoch: 49 [41856/50048]	Loss: 0.2699
Training Epoch: 49 [41984/50048]	Loss: 0.3143
Training Epoch: 49 [42112/50048]	Loss: 0.2631
Training Epoch: 49 [42240/50048]	Loss: 0.2353
Training Epoch: 49 [42368/50048]	Loss: 0.2973
Training Epoch: 49 [42496/50048]	Loss: 0.3871
Training Epoch: 49 [42624/50048]	Loss: 0.3816
Training Epoch: 49 [42752/50048]	Loss: 0.3045
Training Epoch: 49 [42880/50048]	Loss: 0.2476
Training Epoch: 49 [43008/50048]	Loss: 0.4041
Training Epoch: 49 [43136/50048]	Loss: 0.2159
Training Epoch: 49 [43264/50048]	Loss: 0.2750
Training Epoch: 49 [43392/50048]	Loss: 0.3043
Training Epoch: 49 [43520/50048]	Loss: 0.2605
Training Epoch: 49 [43648/50048]	Loss: 0.3439
Training Epoch: 49 [43776/50048]	Loss: 0.3526
Training Epoch: 49 [43904/50048]	Loss: 0.3063
Training Epoch: 49 [44032/50048]	Loss: 0.2232
Training Epoch: 49 [44160/50048]	Loss: 0.3113
Training Epoch: 49 [44288/50048]	Loss: 0.2635
Training Epoch: 49 [44416/50048]	Loss: 0.4100
Training Epoch: 49 [44544/50048]	Loss: 0.3373
Training Epoch: 49 [44672/50048]	Loss: 0.3392
Training Epoch: 49 [44800/50048]	Loss: 0.1758
Training Epoch: 49 [44928/50048]	Loss: 0.3838
Training Epoch: 49 [45056/50048]	Loss: 0.2024
Training Epoch: 49 [45184/50048]	Loss: 0.3451
Training Epoch: 49 [45312/50048]	Loss: 0.3478
Training Epoch: 49 [45440/50048]	Loss: 0.3322
Training Epoch: 49 [45568/50048]	Loss: 0.2382
Training Epoch: 49 [45696/50048]	Loss: 0.3316
2022-12-06 04:52:31,551 [ZeusDataLoader(train)] train epoch 50 done: time=86.48 energy=10485.95
2022-12-06 04:52:31,552 [ZeusDataLoader(eval)] Epoch 50 begin.
Training Epoch: 49 [45824/50048]	Loss: 0.2670
Training Epoch: 49 [45952/50048]	Loss: 0.2267
Training Epoch: 49 [46080/50048]	Loss: 0.3336
Training Epoch: 49 [46208/50048]	Loss: 0.3879
Training Epoch: 49 [46336/50048]	Loss: 0.3179
Training Epoch: 49 [46464/50048]	Loss: 0.3035
Training Epoch: 49 [46592/50048]	Loss: 0.3070
Training Epoch: 49 [46720/50048]	Loss: 0.2516
Training Epoch: 49 [46848/50048]	Loss: 0.2956
Training Epoch: 49 [46976/50048]	Loss: 0.3289
Training Epoch: 49 [47104/50048]	Loss: 0.3363
Training Epoch: 49 [47232/50048]	Loss: 0.3248
Training Epoch: 49 [47360/50048]	Loss: 0.5234
Training Epoch: 49 [47488/50048]	Loss: 0.2998
Training Epoch: 49 [47616/50048]	Loss: 0.1848
Training Epoch: 49 [47744/50048]	Loss: 0.2225
Training Epoch: 49 [47872/50048]	Loss: 0.3569
Training Epoch: 49 [48000/50048]	Loss: 0.2720
Training Epoch: 49 [48128/50048]	Loss: 0.3233
Training Epoch: 49 [48256/50048]	Loss: 0.3955
Training Epoch: 49 [48384/50048]	Loss: 0.3874
Training Epoch: 49 [48512/50048]	Loss: 0.4105
Training Epoch: 49 [48640/50048]	Loss: 0.2895
Training Epoch: 49 [48768/50048]	Loss: 0.2713
Training Epoch: 49 [48896/50048]	Loss: 0.2559
Training Epoch: 49 [49024/50048]	Loss: 0.2536
Training Epoch: 49 [49152/50048]	Loss: 0.2061
Training Epoch: 49 [49280/50048]	Loss: 0.3389
Training Epoch: 49 [49408/50048]	Loss: 0.4261
Training Epoch: 49 [49536/50048]	Loss: 0.2812
Training Epoch: 49 [49664/50048]	Loss: 0.3160
Training Epoch: 49 [49792/50048]	Loss: 0.4375
Training Epoch: 49 [49920/50048]	Loss: 0.3448
Training Epoch: 49 [50048/50048]	Loss: 0.3328
2022-12-06 09:52:35.196 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:52:35,207 [ZeusDataLoader(eval)] eval epoch 50 done: time=3.65 energy=440.81
2022-12-06 04:52:35,207 [ZeusDataLoader(train)] Up to epoch 50: time=4511.89, energy=547650.72, cost=668615.87
2022-12-06 04:52:35,208 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:52:35,208 [ZeusDataLoader(train)] Expected next epoch: time=4601.69, energy=558448.74, cost=681872.26
2022-12-06 04:52:35,209 [ZeusDataLoader(train)] Epoch 51 begin.
Validation Epoch: 49, Average loss: 0.0151, Accuracy: 0.6285
2022-12-06 04:52:35,387 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:52:35,388 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:52:35.390 [ZeusMonitor] Monitor started.
2022-12-06 09:52:35.390 [ZeusMonitor] Running indefinitely. 2022-12-06 09:52:35.390 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:52:35.390 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e51+gpu0.power.log
Training Epoch: 50 [128/50048]	Loss: 0.2302
Training Epoch: 50 [256/50048]	Loss: 0.3050
Training Epoch: 50 [384/50048]	Loss: 0.1607
Training Epoch: 50 [512/50048]	Loss: 0.2352
Training Epoch: 50 [640/50048]	Loss: 0.2355
Training Epoch: 50 [768/50048]	Loss: 0.2365
Training Epoch: 50 [896/50048]	Loss: 0.3191
Training Epoch: 50 [1024/50048]	Loss: 0.2476
Training Epoch: 50 [1152/50048]	Loss: 0.2410
Training Epoch: 50 [1280/50048]	Loss: 0.3220
Training Epoch: 50 [1408/50048]	Loss: 0.2913
Training Epoch: 50 [1536/50048]	Loss: 0.2370
Training Epoch: 50 [1664/50048]	Loss: 0.2254
Training Epoch: 50 [1792/50048]	Loss: 0.2015
Training Epoch: 50 [1920/50048]	Loss: 0.1899
Training Epoch: 50 [2048/50048]	Loss: 0.2611
Training Epoch: 50 [2176/50048]	Loss: 0.2573
Training Epoch: 50 [2304/50048]	Loss: 0.2184
Training Epoch: 50 [2432/50048]	Loss: 0.1865
Training Epoch: 50 [2560/50048]	Loss: 0.2866
Training Epoch: 50 [2688/50048]	Loss: 0.3407
Training Epoch: 50 [2816/50048]	Loss: 0.2965
Training Epoch: 50 [2944/50048]	Loss: 0.4263
Training Epoch: 50 [3072/50048]	Loss: 0.3108
Training Epoch: 50 [3200/50048]	Loss: 0.2370
Training Epoch: 50 [3328/50048]	Loss: 0.3294
Training Epoch: 50 [3456/50048]	Loss: 0.2020
Training Epoch: 50 [3584/50048]	Loss: 0.3559
Training Epoch: 50 [3712/50048]	Loss: 0.2680
Training Epoch: 50 [3840/50048]	Loss: 0.1715
Training Epoch: 50 [3968/50048]	Loss: 0.2843
Training Epoch: 50 [4096/50048]	Loss: 0.2570
Training Epoch: 50 [4224/50048]	Loss: 0.2757
Training Epoch: 50 [4352/50048]	Loss: 0.2860
Training Epoch: 50 [4480/50048]	Loss: 0.2681
Training Epoch: 50 [4608/50048]	Loss: 0.2654
Training Epoch: 50 [4736/50048]	Loss: 0.2322
Training Epoch: 50 [4864/50048]	Loss: 0.2103
Training Epoch: 50 [4992/50048]	Loss: 0.2470
Training Epoch: 50 [5120/50048]	Loss: 0.2851
Training Epoch: 50 [5248/50048]	Loss: 0.2398
Training Epoch: 50 [5376/50048]	Loss: 0.2506
Training Epoch: 50 [5504/50048]	Loss: 0.2593
Training Epoch: 50 [5632/50048]	Loss: 0.2165
Training Epoch: 50 [5760/50048]	Loss: 0.3135
Training Epoch: 50 [5888/50048]	Loss: 0.2322
Training Epoch: 50 [6016/50048]	Loss: 0.2166
Training Epoch: 50 [6144/50048]	Loss: 0.2294
Training Epoch: 50 [6272/50048]	Loss: 0.2259
Training Epoch: 50 [6400/50048]	Loss: 0.1697
Training Epoch: 50 [6528/50048]	Loss: 0.3742
Training Epoch: 50 [6656/50048]	Loss: 0.2337
Training Epoch: 50 [6784/50048]	Loss: 0.2252
Training Epoch: 50 [6912/50048]	Loss: 0.2207
Training Epoch: 50 [7040/50048]	Loss: 0.2226
Training Epoch: 50 [7168/50048]	Loss: 0.3762
Training Epoch: 50 [7296/50048]	Loss: 0.2140
Training Epoch: 50 [7424/50048]	Loss: 0.2800
Training Epoch: 50 [7552/50048]	Loss: 0.2741
Training Epoch: 50 [7680/50048]	Loss: 0.2643
Training Epoch: 50 [7808/50048]	Loss: 0.3139
Training Epoch: 50 [7936/50048]	Loss: 0.3207
Training Epoch: 50 [8064/50048]	Loss: 0.2682
Training Epoch: 50 [8192/50048]	Loss: 0.3526
Training Epoch: 50 [8320/50048]	Loss: 0.2660
Training Epoch: 50 [8448/50048]	Loss: 0.2889
Training Epoch: 50 [8576/50048]	Loss: 0.2767
Training Epoch: 50 [8704/50048]	Loss: 0.2591
Training Epoch: 50 [8832/50048]	Loss: 0.2255
Training Epoch: 50 [8960/50048]	Loss: 0.2412
Training Epoch: 50 [9088/50048]	Loss: 0.3739
Training Epoch: 50 [9216/50048]	Loss: 0.2088
Training Epoch: 50 [9344/50048]	Loss: 0.2634
Training Epoch: 50 [9472/50048]	Loss: 0.2285
Training Epoch: 50 [9600/50048]	Loss: 0.2213
Training Epoch: 50 [9728/50048]	Loss: 0.2888
Training Epoch: 50 [9856/50048]	Loss: 0.1464
Training Epoch: 50 [9984/50048]	Loss: 0.2243
Training Epoch: 50 [10112/50048]	Loss: 0.4469
Training Epoch: 50 [10240/50048]	Loss: 0.2263
Training Epoch: 50 [10368/50048]	Loss: 0.3406
Training Epoch: 50 [10496/50048]	Loss: 0.3900
Training Epoch: 50 [10624/50048]	Loss: 0.1656
Training Epoch: 50 [10752/50048]	Loss: 0.1632
Training Epoch: 50 [10880/50048]	Loss: 0.1544
Training Epoch: 50 [11008/50048]	Loss: 0.1836
Training Epoch: 50 [11136/50048]	Loss: 0.2508
Training Epoch: 50 [11264/50048]	Loss: 0.2629
Training Epoch: 50 [11392/50048]	Loss: 0.1741
Training Epoch: 50 [11520/50048]	Loss: 0.2539
Training Epoch: 50 [11648/50048]	Loss: 0.2508
Training Epoch: 50 [11776/50048]	Loss: 0.3462
Training Epoch: 50 [11904/50048]	Loss: 0.2829
Training Epoch: 50 [12032/50048]	Loss: 0.3057
Training Epoch: 50 [12160/50048]	Loss: 0.1398
Training Epoch: 50 [12288/50048]	Loss: 0.2616
Training Epoch: 50 [12416/50048]	Loss: 0.2560
Training Epoch: 50 [12544/50048]	Loss: 0.2279
Training Epoch: 50 [12672/50048]	Loss: 0.2069
Training Epoch: 50 [12800/50048]	Loss: 0.2552
Training Epoch: 50 [12928/50048]	Loss: 0.3063
Training Epoch: 50 [13056/50048]	Loss: 0.4042
Training Epoch: 50 [13184/50048]	Loss: 0.2263
Training Epoch: 50 [13312/50048]	Loss: 0.2507
Training Epoch: 50 [13440/50048]	Loss: 0.2882
Training Epoch: 50 [13568/50048]	Loss: 0.2406
Training Epoch: 50 [13696/50048]	Loss: 0.3153
Training Epoch: 50 [13824/50048]	Loss: 0.2909
Training Epoch: 50 [13952/50048]	Loss: 0.3135
Training Epoch: 50 [14080/50048]	Loss: 0.2050
Training Epoch: 50 [14208/50048]	Loss: 0.2174
Training Epoch: 50 [14336/50048]	Loss: 0.3930
Training Epoch: 50 [14464/50048]	Loss: 0.3175
Training Epoch: 50 [14592/50048]	Loss: 0.2352
Training Epoch: 50 [14720/50048]	Loss: 0.2226
Training Epoch: 50 [14848/50048]	Loss: 0.2610
Training Epoch: 50 [14976/50048]	Loss: 0.3860
Training Epoch: 50 [15104/50048]	Loss: 0.4120
Training Epoch: 50 [15232/50048]	Loss: 0.2859
Training Epoch: 50 [15360/50048]	Loss: 0.1876
Training Epoch: 50 [15488/50048]	Loss: 0.3903
Training Epoch: 50 [15616/50048]	Loss: 0.2507
Training Epoch: 50 [15744/50048]	Loss: 0.2807
Training Epoch: 50 [15872/50048]	Loss: 0.3497
Training Epoch: 50 [16000/50048]	Loss: 0.2282
Training Epoch: 50 [16128/50048]	Loss: 0.2135
Training Epoch: 50 [16256/50048]	Loss: 0.4015
Training Epoch: 50 [16384/50048]	Loss: 0.2297
Training Epoch: 50 [16512/50048]	Loss: 0.2532
Training Epoch: 50 [16640/50048]	Loss: 0.2128
Training Epoch: 50 [16768/50048]	Loss: 0.2934
Training Epoch: 50 [16896/50048]	Loss: 0.3074
Training Epoch: 50 [17024/50048]	Loss: 0.2219
Training Epoch: 50 [17152/50048]	Loss: 0.3053
Training Epoch: 50 [17280/50048]	Loss: 0.2045
Training Epoch: 50 [17408/50048]	Loss: 0.1810
Training Epoch: 50 [17536/50048]	Loss: 0.2270
Training Epoch: 50 [17664/50048]	Loss: 0.2075
Training Epoch: 50 [17792/50048]	Loss: 0.2380
Training Epoch: 50 [17920/50048]	Loss: 0.2442
Training Epoch: 50 [18048/50048]	Loss: 0.1857
Training Epoch: 50 [18176/50048]	Loss: 0.2601
Training Epoch: 50 [18304/50048]	Loss: 0.2628
Training Epoch: 50 [18432/50048]	Loss: 0.1686
Training Epoch: 50 [18560/50048]	Loss: 0.2445
Training Epoch: 50 [18688/50048]	Loss: 0.1671
Training Epoch: 50 [18816/50048]	Loss: 0.2610
Training Epoch: 50 [18944/50048]	Loss: 0.1721
Training Epoch: 50 [19072/50048]	Loss: 0.1910
Training Epoch: 50 [19200/50048]	Loss: 0.2069
Training Epoch: 50 [19328/50048]	Loss: 0.3139
Training Epoch: 50 [19456/50048]	Loss: 0.2850
Training Epoch: 50 [19584/50048]	Loss: 0.2554
Training Epoch: 50 [19712/50048]	Loss: 0.2934
Training Epoch: 50 [19840/50048]	Loss: 0.2047
Training Epoch: 50 [19968/50048]	Loss: 0.2954
Training Epoch: 50 [20096/50048]	Loss: 0.2268
Training Epoch: 50 [20224/50048]	Loss: 0.2650
Training Epoch: 50 [20352/50048]	Loss: 0.2805
Training Epoch: 50 [20480/50048]	Loss: 0.2255
Training Epoch: 50 [20608/50048]	Loss: 0.3032
Training Epoch: 50 [20736/50048]	Loss: 0.2677
Training Epoch: 50 [20864/50048]	Loss: 0.4262
Training Epoch: 50 [20992/50048]	Loss: 0.2105
Training Epoch: 50 [21120/50048]	Loss: 0.1981
Training Epoch: 50 [21248/50048]	Loss: 0.2307
Training Epoch: 50 [21376/50048]	Loss: 0.3432
Training Epoch: 50 [21504/50048]	Loss: 0.2358
Training Epoch: 50 [21632/50048]	Loss: 0.1653
Training Epoch: 50 [21760/50048]	Loss: 0.2887
Training Epoch: 50 [21888/50048]	Loss: 0.2162
Training Epoch: 50 [22016/50048]	Loss: 0.3105
Training Epoch: 50 [22144/50048]	Loss: 0.3134
Training Epoch: 50 [22272/50048]	Loss: 0.3275
Training Epoch: 50 [22400/50048]	Loss: 0.2702
Training Epoch: 50 [22528/50048]	Loss: 0.2801
Training Epoch: 50 [22656/50048]	Loss: 0.1606
Training Epoch: 50 [22784/50048]	Loss: 0.2221
Training Epoch: 50 [22912/50048]	Loss: 0.2412
Training Epoch: 50 [23040/50048]	Loss: 0.2749
Training Epoch: 50 [23168/50048]	Loss: 0.2127
Training Epoch: 50 [23296/50048]	Loss: 0.2748
Training Epoch: 50 [23424/50048]	Loss: 0.2072
Training Epoch: 50 [23552/50048]	Loss: 0.1997
Training Epoch: 50 [23680/50048]	Loss: 0.3248
Training Epoch: 50 [23808/50048]	Loss: 0.2696
Training Epoch: 50 [23936/50048]	Loss: 0.1810
Training Epoch: 50 [24064/50048]	Loss: 0.2880
Training Epoch: 50 [24192/50048]	Loss: 0.1399
Training Epoch: 50 [24320/50048]	Loss: 0.1924
Training Epoch: 50 [24448/50048]	Loss: 0.4894
Training Epoch: 50 [24576/50048]	Loss: 0.3096
Training Epoch: 50 [24704/50048]	Loss: 0.3326
Training Epoch: 50 [24832/50048]	Loss: 0.2505
Training Epoch: 50 [24960/50048]	Loss: 0.3931
Training Epoch: 50 [25088/50048]	Loss: 0.2028
Training Epoch: 50 [25216/50048]	Loss: 0.1993
Training Epoch: 50 [25344/50048]	Loss: 0.2257
Training Epoch: 50 [25472/50048]	Loss: 0.2920
Training Epoch: 50 [25600/50048]	Loss: 0.4081
Training Epoch: 50 [25728/50048]	Loss: 0.3514
Training Epoch: 50 [25856/50048]	Loss: 0.3010
Training Epoch: 50 [25984/50048]	Loss: 0.2975
Training Epoch: 50 [26112/50048]	Loss: 0.3324
Training Epoch: 50 [26240/50048]	Loss: 0.1927
Training Epoch: 50 [26368/50048]	Loss: 0.2807
Training Epoch: 50 [26496/50048]	Loss: 0.3458
Training Epoch: 50 [26624/50048]	Loss: 0.2357
Training Epoch: 50 [26752/50048]	Loss: 0.3223
Training Epoch: 50 [26880/50048]	Loss: 0.3105
Training Epoch: 50 [27008/50048]	Loss: 0.3025
Training Epoch: 50 [27136/50048]	Loss: 0.2632
Training Epoch: 50 [27264/50048]	Loss: 0.3293
Training Epoch: 50 [27392/50048]	Loss: 0.3832
Training Epoch: 50 [27520/50048]	Loss: 0.2862
Training Epoch: 50 [27648/50048]	Loss: 0.2959
Training Epoch: 50 [27776/50048]	Loss: 0.2672
Training Epoch: 50 [27904/50048]	Loss: 0.3297
Training Epoch: 50 [28032/50048]	Loss: 0.2921
Training Epoch: 50 [28160/50048]	Loss: 0.2694
Training Epoch: 50 [28288/50048]	Loss: 0.2865
Training Epoch: 50 [28416/50048]	Loss: 0.2858
Training Epoch: 50 [28544/50048]	Loss: 0.2400
Training Epoch: 50 [28672/50048]	Loss: 0.2523
Training Epoch: 50 [28800/50048]	Loss: 0.3884
Training Epoch: 50 [28928/50048]	Loss: 0.2561
Training Epoch: 50 [29056/50048]	Loss: 0.2448
Training Epoch: 50 [29184/50048]	Loss: 0.2101
Training Epoch: 50 [29312/50048]	Loss: 0.2613
Training Epoch: 50 [29440/50048]	Loss: 0.2993
Training Epoch: 50 [29568/50048]	Loss: 0.2314
Training Epoch: 50 [29696/50048]	Loss: 0.3220
Training Epoch: 50 [29824/50048]	Loss: 0.3004
Training Epoch: 50 [29952/50048]	Loss: 0.2409
Training Epoch: 50 [30080/50048]	Loss: 0.3114
Training Epoch: 50 [30208/50048]	Loss: 0.2620
Training Epoch: 50 [30336/50048]	Loss: 0.2406
Training Epoch: 50 [30464/50048]	Loss: 0.4129
Training Epoch: 50 [30592/50048]	Loss: 0.3974
Training Epoch: 50 [30720/50048]	Loss: 0.2395
Training Epoch: 50 [30848/50048]	Loss: 0.3493
Training Epoch: 50 [30976/50048]	Loss: 0.2043
Training Epoch: 50 [31104/50048]	Loss: 0.3958
Training Epoch: 50 [31232/50048]	Loss: 0.2279
Training Epoch: 50 [31360/50048]	Loss: 0.2652
Training Epoch: 50 [31488/50048]	Loss: 0.1898
Training Epoch: 50 [31616/50048]	Loss: 0.2763
Training Epoch: 50 [31744/50048]	Loss: 0.3040
Training Epoch: 50 [31872/50048]	Loss: 0.2600
Training Epoch: 50 [32000/50048]	Loss: 0.2997
Training Epoch: 50 [32128/50048]	Loss: 0.3048
Training Epoch: 50 [32256/50048]	Loss: 0.1907
Training Epoch: 50 [32384/50048]	Loss: 0.3080
Training Epoch: 50 [32512/50048]	Loss: 0.2777
Training Epoch: 50 [32640/50048]	Loss: 0.4046
Training Epoch: 50 [32768/50048]	Loss: 0.2958
Training Epoch: 50 [32896/50048]	Loss: 0.2632
Training Epoch: 50 [33024/50048]	Loss: 0.2920
Training Epoch: 50 [33152/50048]	Loss: 0.3197
Training Epoch: 50 [33280/50048]	Loss: 0.2572
Training Epoch: 50 [33408/50048]	Loss: 0.3443
Training Epoch: 50 [33536/50048]	Loss: 0.3151
Training Epoch: 50 [33664/50048]	Loss: 0.3328
Training Epoch: 50 [33792/50048]	Loss: 0.2854
Training Epoch: 50 [33920/50048]	Loss: 0.2336
Training Epoch: 50 [34048/50048]	Loss: 0.2974
Training Epoch: 50 [34176/50048]	Loss: 0.3096
Training Epoch: 50 [34304/50048]	Loss: 0.3151
Training Epoch: 50 [34432/50048]	Loss: 0.4028
Training Epoch: 50 [34560/50048]	Loss: 0.2000
Training Epoch: 50 [34688/50048]	Loss: 0.2571
Training Epoch: 50 [34816/50048]	Loss: 0.3238
Training Epoch: 50 [34944/50048]	Loss: 0.3275
Training Epoch: 50 [35072/50048]	Loss: 0.2311
Training Epoch: 50 [35200/50048]	Loss: 0.3786
Training Epoch: 50 [35328/50048]	Loss: 0.2370
Training Epoch: 50 [35456/50048]	Loss: 0.1950
Training Epoch: 50 [35584/50048]	Loss: 0.3042
Training Epoch: 50 [35712/50048]	Loss: 0.2396
Training Epoch: 50 [35840/50048]	Loss: 0.2020
Training Epoch: 50 [35968/50048]	Loss: 0.3106
Training Epoch: 50 [36096/50048]	Loss: 0.1713
Training Epoch: 50 [36224/50048]	Loss: 0.3881
Training Epoch: 50 [36352/50048]	Loss: 0.2516
Training Epoch: 50 [36480/50048]	Loss: 0.3191
Training Epoch: 50 [36608/50048]	Loss: 0.2588
Training Epoch: 50 [36736/50048]	Loss: 0.2781
Training Epoch: 50 [36864/50048]	Loss: 0.2724
Training Epoch: 50 [36992/50048]	Loss: 0.3017
Training Epoch: 50 [37120/50048]	Loss: 0.3001
Training Epoch: 50 [37248/50048]	Loss: 0.2314
Training Epoch: 50 [37376/50048]	Loss: 0.2933
Training Epoch: 50 [37504/50048]	Loss: 0.2054
Training Epoch: 50 [37632/50048]	Loss: 0.2255
Training Epoch: 50 [37760/50048]	Loss: 0.3167
Training Epoch: 50 [37888/50048]	Loss: 0.2808
Training Epoch: 50 [38016/50048]	Loss: 0.2293
Training Epoch: 50 [38144/50048]	Loss: 0.2718
Training Epoch: 50 [38272/50048]	Loss: 0.2187
Training Epoch: 50 [38400/50048]	Loss: 0.1611
Training Epoch: 50 [38528/50048]	Loss: 0.3565
Training Epoch: 50 [38656/50048]	Loss: 0.2473
Training Epoch: 50 [38784/50048]	Loss: 0.2092
Training Epoch: 50 [38912/50048]	Loss: 0.1573
Training Epoch: 50 [39040/50048]	Loss: 0.2949
Training Epoch: 50 [39168/50048]	Loss: 0.1944
Training Epoch: 50 [39296/50048]	Loss: 0.2290
Training Epoch: 50 [39424/50048]	Loss: 0.3185
Training Epoch: 50 [39552/50048]	Loss: 0.2751
Training Epoch: 50 [39680/50048]	Loss: 0.3864
Training Epoch: 50 [39808/50048]	Loss: 0.2260
Training Epoch: 50 [39936/50048]	Loss: 0.3076
Training Epoch: 50 [40064/50048]	Loss: 0.4247
Training Epoch: 50 [40192/50048]	Loss: 0.2552
Training Epoch: 50 [40320/50048]	Loss: 0.1790
Training Epoch: 50 [40448/50048]	Loss: 0.2720
Training Epoch: 50 [40576/50048]	Loss: 0.3509
Training Epoch: 50 [40704/50048]	Loss: 0.2700
Training Epoch: 50 [40832/50048]	Loss: 0.3558
Training Epoch: 50 [40960/50048]	Loss: 0.2678
Training Epoch: 50 [41088/50048]	Loss: 0.4028
Training Epoch: 50 [41216/50048]	Loss: 0.1827
Training Epoch: 50 [41344/50048]	Loss: 0.3326
Training Epoch: 50 [41472/50048]	Loss: 0.2857
Training Epoch: 50 [41600/50048]	Loss: 0.3260
Training Epoch: 50 [41728/50048]	Loss: 0.1656
Training Epoch: 50 [41856/50048]	Loss: 0.2790
Training Epoch: 50 [41984/50048]	Loss: 0.3082
Training Epoch: 50 [42112/50048]	Loss: 0.2807
Training Epoch: 50 [42240/50048]	Loss: 0.2366
Training Epoch: 50 [42368/50048]	Loss: 0.2392
Training Epoch: 50 [42496/50048]	Loss: 0.2514
Training Epoch: 50 [42624/50048]	Loss: 0.3329
Training Epoch: 50 [42752/50048]	Loss: 0.3681
Training Epoch: 50 [42880/50048]	Loss: 0.3512
Training Epoch: 50 [43008/50048]	Loss: 0.3366
Training Epoch: 50 [43136/50048]	Loss: 0.4401
Training Epoch: 50 [43264/50048]	Loss: 0.2472
Training Epoch: 50 [43392/50048]	Loss: 0.2410
Training Epoch: 50 [43520/50048]	Loss: 0.3237
Training Epoch: 50 [43648/50048]	Loss: 0.3117
Training Epoch: 50 [43776/50048]	Loss: 0.2475
Training Epoch: 50 [43904/50048]	Loss: 0.2730
Training Epoch: 50 [44032/50048]	Loss: 0.3366
Training Epoch: 50 [44160/50048]	Loss: 0.3327
Training Epoch: 50 [44288/50048]	Loss: 0.2772
Training Epoch: 50 [44416/50048]	Loss: 0.3288
Training Epoch: 50 [44544/50048]	Loss: 0.2467
Training Epoch: 50 [44672/50048]	Loss: 0.2942
Training Epoch: 50 [44800/50048]	Loss: 0.2815
Training Epoch: 50 [44928/50048]	Loss: 0.3401
Training Epoch: 50 [45056/50048]	Loss: 0.3346
Training Epoch: 50 [45184/50048]	Loss: 0.3438
Training Epoch: 50 [45312/50048]	Loss: 0.3181
Training Epoch: 50 [45440/50048]	Loss: 0.2179
Training Epoch: 50 [45568/50048]	Loss: 0.2572
Training Epoch: 50 [45696/50048]	Loss: 0.3911
2022-12-06 04:54:01,692 [ZeusDataLoader(train)] train epoch 51 done: time=86.47 energy=10497.81
2022-12-06 04:54:01,693 [ZeusDataLoader(eval)] Epoch 51 begin.
Training Epoch: 50 [45824/50048]	Loss: 0.2822
Training Epoch: 50 [45952/50048]	Loss: 0.2380
Training Epoch: 50 [46080/50048]	Loss: 0.2291
Training Epoch: 50 [46208/50048]	Loss: 0.3603
Training Epoch: 50 [46336/50048]	Loss: 0.2905
Training Epoch: 50 [46464/50048]	Loss: 0.4399
Training Epoch: 50 [46592/50048]	Loss: 0.2190
Training Epoch: 50 [46720/50048]	Loss: 0.3392
Training Epoch: 50 [46848/50048]	Loss: 0.4221
Training Epoch: 50 [46976/50048]	Loss: 0.2134
Training Epoch: 50 [47104/50048]	Loss: 0.3387
Training Epoch: 50 [47232/50048]	Loss: 0.2760
Training Epoch: 50 [47360/50048]	Loss: 0.2575
Training Epoch: 50 [47488/50048]	Loss: 0.3103
Training Epoch: 50 [47616/50048]	Loss: 0.3148
Training Epoch: 50 [47744/50048]	Loss: 0.3304
Training Epoch: 50 [47872/50048]	Loss: 0.2608
Training Epoch: 50 [48000/50048]	Loss: 0.3133
Training Epoch: 50 [48128/50048]	Loss: 0.3713
Training Epoch: 50 [48256/50048]	Loss: 0.2807
Training Epoch: 50 [48384/50048]	Loss: 0.2298
Training Epoch: 50 [48512/50048]	Loss: 0.4870
Training Epoch: 50 [48640/50048]	Loss: 0.2370
Training Epoch: 50 [48768/50048]	Loss: 0.3606
Training Epoch: 50 [48896/50048]	Loss: 0.1783
Training Epoch: 50 [49024/50048]	Loss: 0.2431
Training Epoch: 50 [49152/50048]	Loss: 0.2906
Training Epoch: 50 [49280/50048]	Loss: 0.3630
Training Epoch: 50 [49408/50048]	Loss: 0.3523
Training Epoch: 50 [49536/50048]	Loss: 0.2334
Training Epoch: 50 [49664/50048]	Loss: 0.2713
Training Epoch: 50 [49792/50048]	Loss: 0.2397
Training Epoch: 50 [49920/50048]	Loss: 0.2747
Training Epoch: 50 [50048/50048]	Loss: 0.3515
2022-12-06 09:54:05.408 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:54:05,418 [ZeusDataLoader(eval)] eval epoch 51 done: time=3.72 energy=452.53
2022-12-06 04:54:05,418 [ZeusDataLoader(train)] Up to epoch 51: time=4602.08, energy=558601.06, cost=681982.55
2022-12-06 04:54:05,418 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:54:05,418 [ZeusDataLoader(train)] Expected next epoch: time=4691.88, energy=569399.08, cost=695238.93
2022-12-06 04:54:05,419 [ZeusDataLoader(train)] Epoch 52 begin.
Validation Epoch: 50, Average loss: 0.0145, Accuracy: 0.6419
2022-12-06 04:54:05,567 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:54:05,567 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:54:05.571 [ZeusMonitor] Monitor started.
2022-12-06 09:54:05.571 [ZeusMonitor] Running indefinitely. 2022-12-06 09:54:05.571 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:54:05.571 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e52+gpu0.power.log
Training Epoch: 51 [128/50048]	Loss: 0.2780
Training Epoch: 51 [256/50048]	Loss: 0.2174
Training Epoch: 51 [384/50048]	Loss: 0.2344
Training Epoch: 51 [512/50048]	Loss: 0.2098
Training Epoch: 51 [640/50048]	Loss: 0.2104
Training Epoch: 51 [768/50048]	Loss: 0.1796
Training Epoch: 51 [896/50048]	Loss: 0.2708
Training Epoch: 51 [1024/50048]	Loss: 0.2184
Training Epoch: 51 [1152/50048]	Loss: 0.2497
Training Epoch: 51 [1280/50048]	Loss: 0.1754
Training Epoch: 51 [1408/50048]	Loss: 0.1770
Training Epoch: 51 [1536/50048]	Loss: 0.2854
Training Epoch: 51 [1664/50048]	Loss: 0.2320
Training Epoch: 51 [1792/50048]	Loss: 0.1744
Training Epoch: 51 [1920/50048]	Loss: 0.2010
Training Epoch: 51 [2048/50048]	Loss: 0.2739
Training Epoch: 51 [2176/50048]	Loss: 0.2549
Training Epoch: 51 [2304/50048]	Loss: 0.1738
Training Epoch: 51 [2432/50048]	Loss: 0.2879
Training Epoch: 51 [2560/50048]	Loss: 0.2977
Training Epoch: 51 [2688/50048]	Loss: 0.2588
Training Epoch: 51 [2816/50048]	Loss: 0.2327
Training Epoch: 51 [2944/50048]	Loss: 0.3063
Training Epoch: 51 [3072/50048]	Loss: 0.2738
Training Epoch: 51 [3200/50048]	Loss: 0.2011
Training Epoch: 51 [3328/50048]	Loss: 0.2124
Training Epoch: 51 [3456/50048]	Loss: 0.2234
Training Epoch: 51 [3584/50048]	Loss: 0.2752
Training Epoch: 51 [3712/50048]	Loss: 0.3026
Training Epoch: 51 [3840/50048]	Loss: 0.2046
Training Epoch: 51 [3968/50048]	Loss: 0.2536
Training Epoch: 51 [4096/50048]	Loss: 0.3078
Training Epoch: 51 [4224/50048]	Loss: 0.1976
Training Epoch: 51 [4352/50048]	Loss: 0.2647
Training Epoch: 51 [4480/50048]	Loss: 0.2280
Training Epoch: 51 [4608/50048]	Loss: 0.1450
Training Epoch: 51 [4736/50048]	Loss: 0.2879
Training Epoch: 51 [4864/50048]	Loss: 0.2519
Training Epoch: 51 [4992/50048]	Loss: 0.1666
Training Epoch: 51 [5120/50048]	Loss: 0.2420
Training Epoch: 51 [5248/50048]	Loss: 0.1963
Training Epoch: 51 [5376/50048]	Loss: 0.2367
Training Epoch: 51 [5504/50048]	Loss: 0.1917
Training Epoch: 51 [5632/50048]	Loss: 0.2071
Training Epoch: 51 [5760/50048]	Loss: 0.1638
Training Epoch: 51 [5888/50048]	Loss: 0.1456
Training Epoch: 51 [6016/50048]	Loss: 0.1828
Training Epoch: 51 [6144/50048]	Loss: 0.2121
Training Epoch: 51 [6272/50048]	Loss: 0.1225
Training Epoch: 51 [6400/50048]	Loss: 0.2191
Training Epoch: 51 [6528/50048]	Loss: 0.2965
Training Epoch: 51 [6656/50048]	Loss: 0.2371
Training Epoch: 51 [6784/50048]	Loss: 0.2423
Training Epoch: 51 [6912/50048]	Loss: 0.2413
Training Epoch: 51 [7040/50048]	Loss: 0.1303
Training Epoch: 51 [7168/50048]	Loss: 0.2065
Training Epoch: 51 [7296/50048]	Loss: 0.1661
Training Epoch: 51 [7424/50048]	Loss: 0.1713
Training Epoch: 51 [7552/50048]	Loss: 0.2646
Training Epoch: 51 [7680/50048]	Loss: 0.1565
Training Epoch: 51 [7808/50048]	Loss: 0.1576
Training Epoch: 51 [7936/50048]	Loss: 0.3410
Training Epoch: 51 [8064/50048]	Loss: 0.1826
Training Epoch: 51 [8192/50048]	Loss: 0.2627
Training Epoch: 51 [8320/50048]	Loss: 0.2859
Training Epoch: 51 [8448/50048]	Loss: 0.3237
Training Epoch: 51 [8576/50048]	Loss: 0.2479
Training Epoch: 51 [8704/50048]	Loss: 0.2381
Training Epoch: 51 [8832/50048]	Loss: 0.2458
Training Epoch: 51 [8960/50048]	Loss: 0.1935
Training Epoch: 51 [9088/50048]	Loss: 0.2807
Training Epoch: 51 [9216/50048]	Loss: 0.1972
Training Epoch: 51 [9344/50048]	Loss: 0.2324
Training Epoch: 51 [9472/50048]	Loss: 0.2421
Training Epoch: 51 [9600/50048]	Loss: 0.2620
Training Epoch: 51 [9728/50048]	Loss: 0.2355
Training Epoch: 51 [9856/50048]	Loss: 0.3077
Training Epoch: 51 [9984/50048]	Loss: 0.2306
Training Epoch: 51 [10112/50048]	Loss: 0.2571
Training Epoch: 51 [10240/50048]	Loss: 0.2651
Training Epoch: 51 [10368/50048]	Loss: 0.2511
Training Epoch: 51 [10496/50048]	Loss: 0.2536
Training Epoch: 51 [10624/50048]	Loss: 0.2433
Training Epoch: 51 [10752/50048]	Loss: 0.2683
Training Epoch: 51 [10880/50048]	Loss: 0.3656
Training Epoch: 51 [11008/50048]	Loss: 0.3142
Training Epoch: 51 [11136/50048]	Loss: 0.1320
Training Epoch: 51 [11264/50048]	Loss: 0.2096
Training Epoch: 51 [11392/50048]	Loss: 0.2713
Training Epoch: 51 [11520/50048]	Loss: 0.3284
Training Epoch: 51 [11648/50048]	Loss: 0.2034
Training Epoch: 51 [11776/50048]	Loss: 0.2503
Training Epoch: 51 [11904/50048]	Loss: 0.1816
Training Epoch: 51 [12032/50048]	Loss: 0.2618
Training Epoch: 51 [12160/50048]	Loss: 0.2827
Training Epoch: 51 [12288/50048]	Loss: 0.2306
Training Epoch: 51 [12416/50048]	Loss: 0.2973
Training Epoch: 51 [12544/50048]	Loss: 0.2641
Training Epoch: 51 [12672/50048]	Loss: 0.2849
Training Epoch: 51 [12800/50048]	Loss: 0.1635
Training Epoch: 51 [12928/50048]	Loss: 0.2545
Training Epoch: 51 [13056/50048]	Loss: 0.2045
Training Epoch: 51 [13184/50048]	Loss: 0.2133
Training Epoch: 51 [13312/50048]	Loss: 0.2892
Training Epoch: 51 [13440/50048]	Loss: 0.1184
Training Epoch: 51 [13568/50048]	Loss: 0.1581
Training Epoch: 51 [13696/50048]	Loss: 0.1703
Training Epoch: 51 [13824/50048]	Loss: 0.2248
Training Epoch: 51 [13952/50048]	Loss: 0.1970
Training Epoch: 51 [14080/50048]	Loss: 0.2835
Training Epoch: 51 [14208/50048]	Loss: 0.2316
Training Epoch: 51 [14336/50048]	Loss: 0.2861
Training Epoch: 51 [14464/50048]	Loss: 0.1284
Training Epoch: 51 [14592/50048]	Loss: 0.2265
Training Epoch: 51 [14720/50048]	Loss: 0.2385
Training Epoch: 51 [14848/50048]	Loss: 0.2356
Training Epoch: 51 [14976/50048]	Loss: 0.1949
Training Epoch: 51 [15104/50048]	Loss: 0.2525
Training Epoch: 51 [15232/50048]	Loss: 0.3463
Training Epoch: 51 [15360/50048]	Loss: 0.2592
Training Epoch: 51 [15488/50048]	Loss: 0.3216
Training Epoch: 51 [15616/50048]	Loss: 0.3135
Training Epoch: 51 [15744/50048]	Loss: 0.2919
Training Epoch: 51 [15872/50048]	Loss: 0.3935
Training Epoch: 51 [16000/50048]	Loss: 0.3384
Training Epoch: 51 [16128/50048]	Loss: 0.2811
Training Epoch: 51 [16256/50048]	Loss: 0.2411
Training Epoch: 51 [16384/50048]	Loss: 0.1658
Training Epoch: 51 [16512/50048]	Loss: 0.1900
Training Epoch: 51 [16640/50048]	Loss: 0.3287
Training Epoch: 51 [16768/50048]	Loss: 0.2633
Training Epoch: 51 [16896/50048]	Loss: 0.3048
Training Epoch: 51 [17024/50048]	Loss: 0.2130
Training Epoch: 51 [17152/50048]	Loss: 0.3367
Training Epoch: 51 [17280/50048]	Loss: 0.2085
Training Epoch: 51 [17408/50048]	Loss: 0.2643
Training Epoch: 51 [17536/50048]	Loss: 0.2995
Training Epoch: 51 [17664/50048]	Loss: 0.2578
Training Epoch: 51 [17792/50048]	Loss: 0.2206
Training Epoch: 51 [17920/50048]	Loss: 0.2487
Training Epoch: 51 [18048/50048]	Loss: 0.2969
Training Epoch: 51 [18176/50048]	Loss: 0.3309
Training Epoch: 51 [18304/50048]	Loss: 0.2831
Training Epoch: 51 [18432/50048]	Loss: 0.2499
Training Epoch: 51 [18560/50048]	Loss: 0.3335
Training Epoch: 51 [18688/50048]	Loss: 0.2607
Training Epoch: 51 [18816/50048]	Loss: 0.2462
Training Epoch: 51 [18944/50048]	Loss: 0.3270
Training Epoch: 51 [19072/50048]	Loss: 0.2133
Training Epoch: 51 [19200/50048]	Loss: 0.2257
Training Epoch: 51 [19328/50048]	Loss: 0.2806
Training Epoch: 51 [19456/50048]	Loss: 0.2579
Training Epoch: 51 [19584/50048]	Loss: 0.2676
Training Epoch: 51 [19712/50048]	Loss: 0.2482
Training Epoch: 51 [19840/50048]	Loss: 0.2940
Training Epoch: 51 [19968/50048]	Loss: 0.2726
Training Epoch: 51 [20096/50048]	Loss: 0.1777
Training Epoch: 51 [20224/50048]	Loss: 0.2285
Training Epoch: 51 [20352/50048]	Loss: 0.2566
Training Epoch: 51 [20480/50048]	Loss: 0.1832
Training Epoch: 51 [20608/50048]	Loss: 0.2193
Training Epoch: 51 [20736/50048]	Loss: 0.1835
Training Epoch: 51 [20864/50048]	Loss: 0.2753
Training Epoch: 51 [20992/50048]	Loss: 0.2157
Training Epoch: 51 [21120/50048]	Loss: 0.1852
Training Epoch: 51 [21248/50048]	Loss: 0.2559
Training Epoch: 51 [21376/50048]	Loss: 0.2996
Training Epoch: 51 [21504/50048]	Loss: 0.1898
Training Epoch: 51 [21632/50048]	Loss: 0.2210
Training Epoch: 51 [21760/50048]	Loss: 0.1864
Training Epoch: 51 [21888/50048]	Loss: 0.2392
Training Epoch: 51 [22016/50048]	Loss: 0.1623
Training Epoch: 51 [22144/50048]	Loss: 0.2291
Training Epoch: 51 [22272/50048]	Loss: 0.1707
Training Epoch: 51 [22400/50048]	Loss: 0.2604
Training Epoch: 51 [22528/50048]	Loss: 0.1762
Training Epoch: 51 [22656/50048]	Loss: 0.1739
Training Epoch: 51 [22784/50048]	Loss: 0.3421
Training Epoch: 51 [22912/50048]	Loss: 0.2375
Training Epoch: 51 [23040/50048]	Loss: 0.3025
Training Epoch: 51 [23168/50048]	Loss: 0.2005
Training Epoch: 51 [23296/50048]	Loss: 0.3273
Training Epoch: 51 [23424/50048]	Loss: 0.2649
Training Epoch: 51 [23552/50048]	Loss: 0.2656
Training Epoch: 51 [23680/50048]	Loss: 0.1630
Training Epoch: 51 [23808/50048]	Loss: 0.3985
Training Epoch: 51 [23936/50048]	Loss: 0.2443
Training Epoch: 51 [24064/50048]	Loss: 0.3327
Training Epoch: 51 [24192/50048]	Loss: 0.4337
Training Epoch: 51 [24320/50048]	Loss: 0.3606
Training Epoch: 51 [24448/50048]	Loss: 0.1702
Training Epoch: 51 [24576/50048]	Loss: 0.1761
Training Epoch: 51 [24704/50048]	Loss: 0.2519
Training Epoch: 51 [24832/50048]	Loss: 0.2355
Training Epoch: 51 [24960/50048]	Loss: 0.2973
Training Epoch: 51 [25088/50048]	Loss: 0.2829
Training Epoch: 51 [25216/50048]	Loss: 0.2941
Training Epoch: 51 [25344/50048]	Loss: 0.2526
Training Epoch: 51 [25472/50048]	Loss: 0.2383
Training Epoch: 51 [25600/50048]	Loss: 0.3029
Training Epoch: 51 [25728/50048]	Loss: 0.1995
Training Epoch: 51 [25856/50048]	Loss: 0.2192
Training Epoch: 51 [25984/50048]	Loss: 0.2674
Training Epoch: 51 [26112/50048]	Loss: 0.3539
Training Epoch: 51 [26240/50048]	Loss: 0.2525
Training Epoch: 51 [26368/50048]	Loss: 0.2713
Training Epoch: 51 [26496/50048]	Loss: 0.3367
Training Epoch: 51 [26624/50048]	Loss: 0.1686
Training Epoch: 51 [26752/50048]	Loss: 0.2775
Training Epoch: 51 [26880/50048]	Loss: 0.2974
Training Epoch: 51 [27008/50048]	Loss: 0.1643
Training Epoch: 51 [27136/50048]	Loss: 0.3669
Training Epoch: 51 [27264/50048]	Loss: 0.2214
Training Epoch: 51 [27392/50048]	Loss: 0.3420
Training Epoch: 51 [27520/50048]	Loss: 0.2106
Training Epoch: 51 [27648/50048]	Loss: 0.2807
Training Epoch: 51 [27776/50048]	Loss: 0.2139
Training Epoch: 51 [27904/50048]	Loss: 0.2627
Training Epoch: 51 [28032/50048]	Loss: 0.2701
Training Epoch: 51 [28160/50048]	Loss: 0.2629
Training Epoch: 51 [28288/50048]	Loss: 0.2627
Training Epoch: 51 [28416/50048]	Loss: 0.2836
Training Epoch: 51 [28544/50048]	Loss: 0.3307
Training Epoch: 51 [28672/50048]	Loss: 0.2809
Training Epoch: 51 [28800/50048]	Loss: 0.3209
Training Epoch: 51 [28928/50048]	Loss: 0.2056
Training Epoch: 51 [29056/50048]	Loss: 0.1932
Training Epoch: 51 [29184/50048]	Loss: 0.1429
Training Epoch: 51 [29312/50048]	Loss: 0.2413
Training Epoch: 51 [29440/50048]	Loss: 0.2270
Training Epoch: 51 [29568/50048]	Loss: 0.2843
Training Epoch: 51 [29696/50048]	Loss: 0.2081
Training Epoch: 51 [29824/50048]	Loss: 0.1885
Training Epoch: 51 [29952/50048]	Loss: 0.2655
Training Epoch: 51 [30080/50048]	Loss: 0.2850
Training Epoch: 51 [30208/50048]	Loss: 0.1726
Training Epoch: 51 [30336/50048]	Loss: 0.2277
Training Epoch: 51 [30464/50048]	Loss: 0.1530
Training Epoch: 51 [30592/50048]	Loss: 0.2229
Training Epoch: 51 [30720/50048]	Loss: 0.2282
Training Epoch: 51 [30848/50048]	Loss: 0.2413
Training Epoch: 51 [30976/50048]	Loss: 0.2843
Training Epoch: 51 [31104/50048]	Loss: 0.3805
Training Epoch: 51 [31232/50048]	Loss: 0.4209
Training Epoch: 51 [31360/50048]	Loss: 0.2242
Training Epoch: 51 [31488/50048]	Loss: 0.2558
Training Epoch: 51 [31616/50048]	Loss: 0.3598
Training Epoch: 51 [31744/50048]	Loss: 0.1924
Training Epoch: 51 [31872/50048]	Loss: 0.2745
Training Epoch: 51 [32000/50048]	Loss: 0.2612
Training Epoch: 51 [32128/50048]	Loss: 0.2734
Training Epoch: 51 [32256/50048]	Loss: 0.2615
Training Epoch: 51 [32384/50048]	Loss: 0.2079
Training Epoch: 51 [32512/50048]	Loss: 0.2164
Training Epoch: 51 [32640/50048]	Loss: 0.3306
Training Epoch: 51 [32768/50048]	Loss: 0.1872
Training Epoch: 51 [32896/50048]	Loss: 0.3056
Training Epoch: 51 [33024/50048]	Loss: 0.2361
Training Epoch: 51 [33152/50048]	Loss: 0.2492
Training Epoch: 51 [33280/50048]	Loss: 0.4113
Training Epoch: 51 [33408/50048]	Loss: 0.2878
Training Epoch: 51 [33536/50048]	Loss: 0.3281
Training Epoch: 51 [33664/50048]	Loss: 0.2923
Training Epoch: 51 [33792/50048]	Loss: 0.2215
Training Epoch: 51 [33920/50048]	Loss: 0.3456
Training Epoch: 51 [34048/50048]	Loss: 0.3729
Training Epoch: 51 [34176/50048]	Loss: 0.3157
Training Epoch: 51 [34304/50048]	Loss: 0.1872
Training Epoch: 51 [34432/50048]	Loss: 0.2151
Training Epoch: 51 [34560/50048]	Loss: 0.2261
Training Epoch: 51 [34688/50048]	Loss: 0.3158
Training Epoch: 51 [34816/50048]	Loss: 0.2738
Training Epoch: 51 [34944/50048]	Loss: 0.2326
Training Epoch: 51 [35072/50048]	Loss: 0.3221
Training Epoch: 51 [35200/50048]	Loss: 0.2055
Training Epoch: 51 [35328/50048]	Loss: 0.1544
Training Epoch: 51 [35456/50048]	Loss: 0.2059
Training Epoch: 51 [35584/50048]	Loss: 0.2342
Training Epoch: 51 [35712/50048]	Loss: 0.2589
Training Epoch: 51 [35840/50048]	Loss: 0.2036
Training Epoch: 51 [35968/50048]	Loss: 0.3694
Training Epoch: 51 [36096/50048]	Loss: 0.2900
Training Epoch: 51 [36224/50048]	Loss: 0.1921
Training Epoch: 51 [36352/50048]	Loss: 0.1631
Training Epoch: 51 [36480/50048]	Loss: 0.1783
Training Epoch: 51 [36608/50048]	Loss: 0.3286
Training Epoch: 51 [36736/50048]	Loss: 0.2692
Training Epoch: 51 [36864/50048]	Loss: 0.2364
Training Epoch: 51 [36992/50048]	Loss: 0.2694
Training Epoch: 51 [37120/50048]	Loss: 0.3276
Training Epoch: 51 [37248/50048]	Loss: 0.3027
Training Epoch: 51 [37376/50048]	Loss: 0.2389
Training Epoch: 51 [37504/50048]	Loss: 0.1355
Training Epoch: 51 [37632/50048]	Loss: 0.2055
Training Epoch: 51 [37760/50048]	Loss: 0.2806
Training Epoch: 51 [37888/50048]	Loss: 0.2504
Training Epoch: 51 [38016/50048]	Loss: 0.2549
Training Epoch: 51 [38144/50048]	Loss: 0.3784
Training Epoch: 51 [38272/50048]	Loss: 0.3266
Training Epoch: 51 [38400/50048]	Loss: 0.1952
Training Epoch: 51 [38528/50048]	Loss: 0.2570
Training Epoch: 51 [38656/50048]	Loss: 0.2632
Training Epoch: 51 [38784/50048]	Loss: 0.2396
Training Epoch: 51 [38912/50048]	Loss: 0.3336
Training Epoch: 51 [39040/50048]	Loss: 0.1977
Training Epoch: 51 [39168/50048]	Loss: 0.3309
Training Epoch: 51 [39296/50048]	Loss: 0.2396
Training Epoch: 51 [39424/50048]	Loss: 0.4238
Training Epoch: 51 [39552/50048]	Loss: 0.2074
Training Epoch: 51 [39680/50048]	Loss: 0.3508
Training Epoch: 51 [39808/50048]	Loss: 0.3135
Training Epoch: 51 [39936/50048]	Loss: 0.2300
Training Epoch: 51 [40064/50048]	Loss: 0.2511
Training Epoch: 51 [40192/50048]	Loss: 0.2760
Training Epoch: 51 [40320/50048]	Loss: 0.2675
Training Epoch: 51 [40448/50048]	Loss: 0.3466
Training Epoch: 51 [40576/50048]	Loss: 0.2124
Training Epoch: 51 [40704/50048]	Loss: 0.2517
Training Epoch: 51 [40832/50048]	Loss: 0.2966
Training Epoch: 51 [40960/50048]	Loss: 0.3021
Training Epoch: 51 [41088/50048]	Loss: 0.3555
Training Epoch: 51 [41216/50048]	Loss: 0.1977
Training Epoch: 51 [41344/50048]	Loss: 0.3152
Training Epoch: 51 [41472/50048]	Loss: 0.2528
Training Epoch: 51 [41600/50048]	Loss: 0.2482
Training Epoch: 51 [41728/50048]	Loss: 0.2680
Training Epoch: 51 [41856/50048]	Loss: 0.2943
Training Epoch: 51 [41984/50048]	Loss: 0.3142
Training Epoch: 51 [42112/50048]	Loss: 0.3360
Training Epoch: 51 [42240/50048]	Loss: 0.2711
Training Epoch: 51 [42368/50048]	Loss: 0.3110
Training Epoch: 51 [42496/50048]	Loss: 0.2636
Training Epoch: 51 [42624/50048]	Loss: 0.3235
Training Epoch: 51 [42752/50048]	Loss: 0.3078
Training Epoch: 51 [42880/50048]	Loss: 0.3888
Training Epoch: 51 [43008/50048]	Loss: 0.3656
Training Epoch: 51 [43136/50048]	Loss: 0.2688
Training Epoch: 51 [43264/50048]	Loss: 0.2774
Training Epoch: 51 [43392/50048]	Loss: 0.3695
Training Epoch: 51 [43520/50048]	Loss: 0.3088
Training Epoch: 51 [43648/50048]	Loss: 0.2832
Training Epoch: 51 [43776/50048]	Loss: 0.1887
Training Epoch: 51 [43904/50048]	Loss: 0.3319
Training Epoch: 51 [44032/50048]	Loss: 0.3069
Training Epoch: 51 [44160/50048]	Loss: 0.4190
Training Epoch: 51 [44288/50048]	Loss: 0.2507
Training Epoch: 51 [44416/50048]	Loss: 0.2870
Training Epoch: 51 [44544/50048]	Loss: 0.2630
Training Epoch: 51 [44672/50048]	Loss: 0.3172
Training Epoch: 51 [44800/50048]	Loss: 0.2951
Training Epoch: 51 [44928/50048]	Loss: 0.3076
Training Epoch: 51 [45056/50048]	Loss: 0.2127
Training Epoch: 51 [45184/50048]	Loss: 0.3253
Training Epoch: 51 [45312/50048]	Loss: 0.3889
Training Epoch: 51 [45440/50048]	Loss: 0.2706
Training Epoch: 51 [45568/50048]	Loss: 0.2702
Training Epoch: 51 [45696/50048]	Loss: 0.3858
2022-12-06 04:55:31,963 [ZeusDataLoader(train)] train epoch 52 done: time=86.53 energy=10516.76
2022-12-06 04:55:31,965 [ZeusDataLoader(eval)] Epoch 52 begin.
Training Epoch: 51 [45824/50048]	Loss: 0.3203
Training Epoch: 51 [45952/50048]	Loss: 0.3106
Training Epoch: 51 [46080/50048]	Loss: 0.2864
Training Epoch: 51 [46208/50048]	Loss: 0.3701
Training Epoch: 51 [46336/50048]	Loss: 0.4021
Training Epoch: 51 [46464/50048]	Loss: 0.3332
Training Epoch: 51 [46592/50048]	Loss: 0.3494
Training Epoch: 51 [46720/50048]	Loss: 0.2840
Training Epoch: 51 [46848/50048]	Loss: 0.3911
Training Epoch: 51 [46976/50048]	Loss: 0.3337
Training Epoch: 51 [47104/50048]	Loss: 0.3005
Training Epoch: 51 [47232/50048]	Loss: 0.2473
Training Epoch: 51 [47360/50048]	Loss: 0.3926
Training Epoch: 51 [47488/50048]	Loss: 0.3170
Training Epoch: 51 [47616/50048]	Loss: 0.2701
Training Epoch: 51 [47744/50048]	Loss: 0.3588
Training Epoch: 51 [47872/50048]	Loss: 0.1576
Training Epoch: 51 [48000/50048]	Loss: 0.3133
Training Epoch: 51 [48128/50048]	Loss: 0.2716
Training Epoch: 51 [48256/50048]	Loss: 0.2570
Training Epoch: 51 [48384/50048]	Loss: 0.2651
Training Epoch: 51 [48512/50048]	Loss: 0.3034
Training Epoch: 51 [48640/50048]	Loss: 0.1919
Training Epoch: 51 [48768/50048]	Loss: 0.2476
Training Epoch: 51 [48896/50048]	Loss: 0.2848
Training Epoch: 51 [49024/50048]	Loss: 0.3432
Training Epoch: 51 [49152/50048]	Loss: 0.2067
Training Epoch: 51 [49280/50048]	Loss: 0.2824
Training Epoch: 51 [49408/50048]	Loss: 0.3034
Training Epoch: 51 [49536/50048]	Loss: 0.2137
Training Epoch: 51 [49664/50048]	Loss: 0.2493
Training Epoch: 51 [49792/50048]	Loss: 0.2910
Training Epoch: 51 [49920/50048]	Loss: 0.3393
Training Epoch: 51 [50048/50048]	Loss: 0.2568
2022-12-06 09:55:35.627 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:55:35,653 [ZeusDataLoader(eval)] eval epoch 52 done: time=3.68 energy=452.40
2022-12-06 04:55:35,653 [ZeusDataLoader(train)] Up to epoch 52: time=4692.29, energy=569570.23, cost=695360.75
2022-12-06 04:55:35,653 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:55:35,653 [ZeusDataLoader(train)] Expected next epoch: time=4782.09, energy=580368.24, cost=708617.13
2022-12-06 04:55:35,654 [ZeusDataLoader(train)] Epoch 53 begin.
Validation Epoch: 51, Average loss: 0.0150, Accuracy: 0.6370
2022-12-06 04:55:35,835 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:55:35,836 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:55:35.838 [ZeusMonitor] Monitor started.
2022-12-06 09:55:35.838 [ZeusMonitor] Running indefinitely. 2022-12-06 09:55:35.838 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:55:35.838 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e53+gpu0.power.log
Training Epoch: 52 [128/50048]	Loss: 0.3048
Training Epoch: 52 [256/50048]	Loss: 0.1807
Training Epoch: 52 [384/50048]	Loss: 0.2345
Training Epoch: 52 [512/50048]	Loss: 0.2776
Training Epoch: 52 [640/50048]	Loss: 0.1569
Training Epoch: 52 [768/50048]	Loss: 0.2253
Training Epoch: 52 [896/50048]	Loss: 0.1572
Training Epoch: 52 [1024/50048]	Loss: 0.2677
Training Epoch: 52 [1152/50048]	Loss: 0.2153
Training Epoch: 52 [1280/50048]	Loss: 0.2427
Training Epoch: 52 [1408/50048]	Loss: 0.2991
Training Epoch: 52 [1536/50048]	Loss: 0.2201
Training Epoch: 52 [1664/50048]	Loss: 0.2339
Training Epoch: 52 [1792/50048]	Loss: 0.2090
Training Epoch: 52 [1920/50048]	Loss: 0.2928
Training Epoch: 52 [2048/50048]	Loss: 0.1654
Training Epoch: 52 [2176/50048]	Loss: 0.2492
Training Epoch: 52 [2304/50048]	Loss: 0.1858
Training Epoch: 52 [2432/50048]	Loss: 0.2177
Training Epoch: 52 [2560/50048]	Loss: 0.2192
Training Epoch: 52 [2688/50048]	Loss: 0.2602
Training Epoch: 52 [2816/50048]	Loss: 0.1839
Training Epoch: 52 [2944/50048]	Loss: 0.2782
Training Epoch: 52 [3072/50048]	Loss: 0.2009
Training Epoch: 52 [3200/50048]	Loss: 0.1180
Training Epoch: 52 [3328/50048]	Loss: 0.1387
Training Epoch: 52 [3456/50048]	Loss: 0.2061
Training Epoch: 52 [3584/50048]	Loss: 0.3249
Training Epoch: 52 [3712/50048]	Loss: 0.2767
Training Epoch: 52 [3840/50048]	Loss: 0.2036
Training Epoch: 52 [3968/50048]	Loss: 0.3208
Training Epoch: 52 [4096/50048]	Loss: 0.1927
Training Epoch: 52 [4224/50048]	Loss: 0.3357
Training Epoch: 52 [4352/50048]	Loss: 0.2749
Training Epoch: 52 [4480/50048]	Loss: 0.2673
Training Epoch: 52 [4608/50048]	Loss: 0.2105
Training Epoch: 52 [4736/50048]	Loss: 0.1676
Training Epoch: 52 [4864/50048]	Loss: 0.0979
Training Epoch: 52 [4992/50048]	Loss: 0.2200
Training Epoch: 52 [5120/50048]	Loss: 0.1688
Training Epoch: 52 [5248/50048]	Loss: 0.2860
Training Epoch: 52 [5376/50048]	Loss: 0.2094
Training Epoch: 52 [5504/50048]	Loss: 0.1696
Training Epoch: 52 [5632/50048]	Loss: 0.1836
Training Epoch: 52 [5760/50048]	Loss: 0.2326
Training Epoch: 52 [5888/50048]	Loss: 0.3111
Training Epoch: 52 [6016/50048]	Loss: 0.2505
Training Epoch: 52 [6144/50048]	Loss: 0.2348
Training Epoch: 52 [6272/50048]	Loss: 0.2590
Training Epoch: 52 [6400/50048]	Loss: 0.2532
Training Epoch: 52 [6528/50048]	Loss: 0.3041
Training Epoch: 52 [6656/50048]	Loss: 0.1708
Training Epoch: 52 [6784/50048]	Loss: 0.1862
Training Epoch: 52 [6912/50048]	Loss: 0.3269
Training Epoch: 52 [7040/50048]	Loss: 0.2142
Training Epoch: 52 [7168/50048]	Loss: 0.1619
Training Epoch: 52 [7296/50048]	Loss: 0.1858
Training Epoch: 52 [7424/50048]	Loss: 0.2572
Training Epoch: 52 [7552/50048]	Loss: 0.3000
Training Epoch: 52 [7680/50048]	Loss: 0.1925
Training Epoch: 52 [7808/50048]	Loss: 0.2470
Training Epoch: 52 [7936/50048]	Loss: 0.1891
Training Epoch: 52 [8064/50048]	Loss: 0.3636
Training Epoch: 52 [8192/50048]	Loss: 0.3019
Training Epoch: 52 [8320/50048]	Loss: 0.2833
Training Epoch: 52 [8448/50048]	Loss: 0.2425
Training Epoch: 52 [8576/50048]	Loss: 0.3009
Training Epoch: 52 [8704/50048]	Loss: 0.1850
Training Epoch: 52 [8832/50048]	Loss: 0.2800
Training Epoch: 52 [8960/50048]	Loss: 0.1851
Training Epoch: 52 [9088/50048]	Loss: 0.2208
Training Epoch: 52 [9216/50048]	Loss: 0.1637
Training Epoch: 52 [9344/50048]	Loss: 0.2153
Training Epoch: 52 [9472/50048]	Loss: 0.2615
Training Epoch: 52 [9600/50048]	Loss: 0.2244
Training Epoch: 52 [9728/50048]	Loss: 0.2477
Training Epoch: 52 [9856/50048]	Loss: 0.2365
Training Epoch: 52 [9984/50048]	Loss: 0.3024
Training Epoch: 52 [10112/50048]	Loss: 0.3570
Training Epoch: 52 [10240/50048]	Loss: 0.2287
Training Epoch: 52 [10368/50048]	Loss: 0.2669
Training Epoch: 52 [10496/50048]	Loss: 0.2746
Training Epoch: 52 [10624/50048]	Loss: 0.1659
Training Epoch: 52 [10752/50048]	Loss: 0.2667
Training Epoch: 52 [10880/50048]	Loss: 0.2038
Training Epoch: 52 [11008/50048]	Loss: 0.2399
Training Epoch: 52 [11136/50048]	Loss: 0.1521
Training Epoch: 52 [11264/50048]	Loss: 0.2956
Training Epoch: 52 [11392/50048]	Loss: 0.2137
Training Epoch: 52 [11520/50048]	Loss: 0.1868
Training Epoch: 52 [11648/50048]	Loss: 0.1640
Training Epoch: 52 [11776/50048]	Loss: 0.2197
Training Epoch: 52 [11904/50048]	Loss: 0.2650
Training Epoch: 52 [12032/50048]	Loss: 0.2252
Training Epoch: 52 [12160/50048]	Loss: 0.3472
Training Epoch: 52 [12288/50048]	Loss: 0.2133
Training Epoch: 52 [12416/50048]	Loss: 0.2553
Training Epoch: 52 [12544/50048]	Loss: 0.1820
Training Epoch: 52 [12672/50048]	Loss: 0.2122
Training Epoch: 52 [12800/50048]	Loss: 0.2560
Training Epoch: 52 [12928/50048]	Loss: 0.1810
Training Epoch: 52 [13056/50048]	Loss: 0.2861
Training Epoch: 52 [13184/50048]	Loss: 0.1931
Training Epoch: 52 [13312/50048]	Loss: 0.2649
Training Epoch: 52 [13440/50048]	Loss: 0.2388
Training Epoch: 52 [13568/50048]	Loss: 0.2219
Training Epoch: 52 [13696/50048]	Loss: 0.3915
Training Epoch: 52 [13824/50048]	Loss: 0.1772
Training Epoch: 52 [13952/50048]	Loss: 0.1849
Training Epoch: 52 [14080/50048]	Loss: 0.2479
Training Epoch: 52 [14208/50048]	Loss: 0.2088
Training Epoch: 52 [14336/50048]	Loss: 0.2031
Training Epoch: 52 [14464/50048]	Loss: 0.2490
Training Epoch: 52 [14592/50048]	Loss: 0.2360
Training Epoch: 52 [14720/50048]	Loss: 0.1923
Training Epoch: 52 [14848/50048]	Loss: 0.3221
Training Epoch: 52 [14976/50048]	Loss: 0.1749
Training Epoch: 52 [15104/50048]	Loss: 0.1318
Training Epoch: 52 [15232/50048]	Loss: 0.1614
Training Epoch: 52 [15360/50048]	Loss: 0.2229
Training Epoch: 52 [15488/50048]	Loss: 0.2544
Training Epoch: 52 [15616/50048]	Loss: 0.1505
Training Epoch: 52 [15744/50048]	Loss: 0.2536
Training Epoch: 52 [15872/50048]	Loss: 0.2509
Training Epoch: 52 [16000/50048]	Loss: 0.2410
Training Epoch: 52 [16128/50048]	Loss: 0.2172
Training Epoch: 52 [16256/50048]	Loss: 0.2226
Training Epoch: 52 [16384/50048]	Loss: 0.1779
Training Epoch: 52 [16512/50048]	Loss: 0.2328
Training Epoch: 52 [16640/50048]	Loss: 0.1525
Training Epoch: 52 [16768/50048]	Loss: 0.1489
Training Epoch: 52 [16896/50048]	Loss: 0.2460
Training Epoch: 52 [17024/50048]	Loss: 0.2407
Training Epoch: 52 [17152/50048]	Loss: 0.2489
Training Epoch: 52 [17280/50048]	Loss: 0.2340
Training Epoch: 52 [17408/50048]	Loss: 0.2061
Training Epoch: 52 [17536/50048]	Loss: 0.2594
Training Epoch: 52 [17664/50048]	Loss: 0.3715
Training Epoch: 52 [17792/50048]	Loss: 0.2839
Training Epoch: 52 [17920/50048]	Loss: 0.2142
Training Epoch: 52 [18048/50048]	Loss: 0.1848
Training Epoch: 52 [18176/50048]	Loss: 0.2355
Training Epoch: 52 [18304/50048]	Loss: 0.1341
Training Epoch: 52 [18432/50048]	Loss: 0.2157
Training Epoch: 52 [18560/50048]	Loss: 0.3181
Training Epoch: 52 [18688/50048]	Loss: 0.3974
Training Epoch: 52 [18816/50048]	Loss: 0.1660
Training Epoch: 52 [18944/50048]	Loss: 0.1819
Training Epoch: 52 [19072/50048]	Loss: 0.2238
Training Epoch: 52 [19200/50048]	Loss: 0.1977
Training Epoch: 52 [19328/50048]	Loss: 0.1994
Training Epoch: 52 [19456/50048]	Loss: 0.2392
Training Epoch: 52 [19584/50048]	Loss: 0.2120
Training Epoch: 52 [19712/50048]	Loss: 0.2866
Training Epoch: 52 [19840/50048]	Loss: 0.1700
Training Epoch: 52 [19968/50048]	Loss: 0.2527
Training Epoch: 52 [20096/50048]	Loss: 0.3029
Training Epoch: 52 [20224/50048]	Loss: 0.1918
Training Epoch: 52 [20352/50048]	Loss: 0.2662
Training Epoch: 52 [20480/50048]	Loss: 0.3094
Training Epoch: 52 [20608/50048]	Loss: 0.2921
Training Epoch: 52 [20736/50048]	Loss: 0.3205
Training Epoch: 52 [20864/50048]	Loss: 0.2437
Training Epoch: 52 [20992/50048]	Loss: 0.4062
Training Epoch: 52 [21120/50048]	Loss: 0.2962
Training Epoch: 52 [21248/50048]	Loss: 0.3501
Training Epoch: 52 [21376/50048]	Loss: 0.2260
Training Epoch: 52 [21504/50048]	Loss: 0.2037
Training Epoch: 52 [21632/50048]	Loss: 0.2245
Training Epoch: 52 [21760/50048]	Loss: 0.1791
Training Epoch: 52 [21888/50048]	Loss: 0.2533
Training Epoch: 52 [22016/50048]	Loss: 0.1772
Training Epoch: 52 [22144/50048]	Loss: 0.2088
Training Epoch: 52 [22272/50048]	Loss: 0.2876
Training Epoch: 52 [22400/50048]	Loss: 0.2266
Training Epoch: 52 [22528/50048]	Loss: 0.3005
Training Epoch: 52 [22656/50048]	Loss: 0.3101
Training Epoch: 52 [22784/50048]	Loss: 0.2190
Training Epoch: 52 [22912/50048]	Loss: 0.2261
Training Epoch: 52 [23040/50048]	Loss: 0.2575
Training Epoch: 52 [23168/50048]	Loss: 0.2184
Training Epoch: 52 [23296/50048]	Loss: 0.1944
Training Epoch: 52 [23424/50048]	Loss: 0.2849
Training Epoch: 52 [23552/50048]	Loss: 0.2712
Training Epoch: 52 [23680/50048]	Loss: 0.2531
Training Epoch: 52 [23808/50048]	Loss: 0.2689
Training Epoch: 52 [23936/50048]	Loss: 0.2712
Training Epoch: 52 [24064/50048]	Loss: 0.2513
Training Epoch: 52 [24192/50048]	Loss: 0.1813
Training Epoch: 52 [24320/50048]	Loss: 0.3065
Training Epoch: 52 [24448/50048]	Loss: 0.2467
Training Epoch: 52 [24576/50048]	Loss: 0.2406
Training Epoch: 52 [24704/50048]	Loss: 0.2176
Training Epoch: 52 [24832/50048]	Loss: 0.2208
Training Epoch: 52 [24960/50048]	Loss: 0.3112
Training Epoch: 52 [25088/50048]	Loss: 0.2522
Training Epoch: 52 [25216/50048]	Loss: 0.2169
Training Epoch: 52 [25344/50048]	Loss: 0.1811
Training Epoch: 52 [25472/50048]	Loss: 0.2070
Training Epoch: 52 [25600/50048]	Loss: 0.2448
Training Epoch: 52 [25728/50048]	Loss: 0.3081
Training Epoch: 52 [25856/50048]	Loss: 0.1859
Training Epoch: 52 [25984/50048]	Loss: 0.1485
Training Epoch: 52 [26112/50048]	Loss: 0.2511
Training Epoch: 52 [26240/50048]	Loss: 0.3004
Training Epoch: 52 [26368/50048]	Loss: 0.3105
Training Epoch: 52 [26496/50048]	Loss: 0.2968
Training Epoch: 52 [26624/50048]	Loss: 0.1446
Training Epoch: 52 [26752/50048]	Loss: 0.3004
Training Epoch: 52 [26880/50048]	Loss: 0.2514
Training Epoch: 52 [27008/50048]	Loss: 0.2694
Training Epoch: 52 [27136/50048]	Loss: 0.2740
Training Epoch: 52 [27264/50048]	Loss: 0.3074
Training Epoch: 52 [27392/50048]	Loss: 0.1724
Training Epoch: 52 [27520/50048]	Loss: 0.2187
Training Epoch: 52 [27648/50048]	Loss: 0.3165
Training Epoch: 52 [27776/50048]	Loss: 0.2463
Training Epoch: 52 [27904/50048]	Loss: 0.2333
Training Epoch: 52 [28032/50048]	Loss: 0.2863
Training Epoch: 52 [28160/50048]	Loss: 0.2774
Training Epoch: 52 [28288/50048]	Loss: 0.1553
Training Epoch: 52 [28416/50048]	Loss: 0.2505
Training Epoch: 52 [28544/50048]	Loss: 0.2099
Training Epoch: 52 [28672/50048]	Loss: 0.2300
Training Epoch: 52 [28800/50048]	Loss: 0.2310
Training Epoch: 52 [28928/50048]	Loss: 0.3024
Training Epoch: 52 [29056/50048]	Loss: 0.1851
Training Epoch: 52 [29184/50048]	Loss: 0.2091
Training Epoch: 52 [29312/50048]	Loss: 0.3869
Training Epoch: 52 [29440/50048]	Loss: 0.2556
Training Epoch: 52 [29568/50048]	Loss: 0.1836
Training Epoch: 52 [29696/50048]	Loss: 0.2452
Training Epoch: 52 [29824/50048]	Loss: 0.3629
Training Epoch: 52 [29952/50048]	Loss: 0.2076
Training Epoch: 52 [30080/50048]	Loss: 0.2143
Training Epoch: 52 [30208/50048]	Loss: 0.1816
Training Epoch: 52 [30336/50048]	Loss: 0.2295
Training Epoch: 52 [30464/50048]	Loss: 0.1583
Training Epoch: 52 [30592/50048]	Loss: 0.2666
Training Epoch: 52 [30720/50048]	Loss: 0.1433
Training Epoch: 52 [30848/50048]	Loss: 0.3553
Training Epoch: 52 [30976/50048]	Loss: 0.1808
Training Epoch: 52 [31104/50048]	Loss: 0.2492
Training Epoch: 52 [31232/50048]	Loss: 0.2101
Training Epoch: 52 [31360/50048]	Loss: 0.2045
Training Epoch: 52 [31488/50048]	Loss: 0.2479
Training Epoch: 52 [31616/50048]	Loss: 0.2502
Training Epoch: 52 [31744/50048]	Loss: 0.2986
Training Epoch: 52 [31872/50048]	Loss: 0.2878
Training Epoch: 52 [32000/50048]	Loss: 0.1939
Training Epoch: 52 [32128/50048]	Loss: 0.3192
Training Epoch: 52 [32256/50048]	Loss: 0.3320
Training Epoch: 52 [32384/50048]	Loss: 0.2530
Training Epoch: 52 [32512/50048]	Loss: 0.2409
Training Epoch: 52 [32640/50048]	Loss: 0.3252
Training Epoch: 52 [32768/50048]	Loss: 0.1868
Training Epoch: 52 [32896/50048]	Loss: 0.2734
Training Epoch: 52 [33024/50048]	Loss: 0.3914
Training Epoch: 52 [33152/50048]	Loss: 0.2149
Training Epoch: 52 [33280/50048]	Loss: 0.2619
Training Epoch: 52 [33408/50048]	Loss: 0.2021
Training Epoch: 52 [33536/50048]	Loss: 0.2033
Training Epoch: 52 [33664/50048]	Loss: 0.2362
Training Epoch: 52 [33792/50048]	Loss: 0.1518
Training Epoch: 52 [33920/50048]	Loss: 0.2900
Training Epoch: 52 [34048/50048]	Loss: 0.2501
Training Epoch: 52 [34176/50048]	Loss: 0.2185
Training Epoch: 52 [34304/50048]	Loss: 0.1295
Training Epoch: 52 [34432/50048]	Loss: 0.3157
Training Epoch: 52 [34560/50048]	Loss: 0.3331
Training Epoch: 52 [34688/50048]	Loss: 0.2993
Training Epoch: 52 [34816/50048]	Loss: 0.2512
Training Epoch: 52 [34944/50048]	Loss: 0.4094
Training Epoch: 52 [35072/50048]	Loss: 0.2556
Training Epoch: 52 [35200/50048]	Loss: 0.2276
Training Epoch: 52 [35328/50048]	Loss: 0.3719
Training Epoch: 52 [35456/50048]	Loss: 0.2832
Training Epoch: 52 [35584/50048]	Loss: 0.2759
Training Epoch: 52 [35712/50048]	Loss: 0.4221
Training Epoch: 52 [35840/50048]	Loss: 0.2118
Training Epoch: 52 [35968/50048]	Loss: 0.3291
Training Epoch: 52 [36096/50048]	Loss: 0.1983
Training Epoch: 52 [36224/50048]	Loss: 0.3114
Training Epoch: 52 [36352/50048]	Loss: 0.1970
Training Epoch: 52 [36480/50048]	Loss: 0.2444
Training Epoch: 52 [36608/50048]	Loss: 0.2988
Training Epoch: 52 [36736/50048]	Loss: 0.2783
Training Epoch: 52 [36864/50048]	Loss: 0.2444
Training Epoch: 52 [36992/50048]	Loss: 0.3306
Training Epoch: 52 [37120/50048]	Loss: 0.2633
Training Epoch: 52 [37248/50048]	Loss: 0.2668
Training Epoch: 52 [37376/50048]	Loss: 0.2044
Training Epoch: 52 [37504/50048]	Loss: 0.2139
Training Epoch: 52 [37632/50048]	Loss: 0.2517
Training Epoch: 52 [37760/50048]	Loss: 0.3139
Training Epoch: 52 [37888/50048]	Loss: 0.2408
Training Epoch: 52 [38016/50048]	Loss: 0.2635
Training Epoch: 52 [38144/50048]	Loss: 0.3250
Training Epoch: 52 [38272/50048]	Loss: 0.2420
Training Epoch: 52 [38400/50048]	Loss: 0.2824
Training Epoch: 52 [38528/50048]	Loss: 0.2866
Training Epoch: 52 [38656/50048]	Loss: 0.2666
Training Epoch: 52 [38784/50048]	Loss: 0.3329
Training Epoch: 52 [38912/50048]	Loss: 0.2939
Training Epoch: 52 [39040/50048]	Loss: 0.3185
Training Epoch: 52 [39168/50048]	Loss: 0.2292
Training Epoch: 52 [39296/50048]	Loss: 0.2412
Training Epoch: 52 [39424/50048]	Loss: 0.2207
Training Epoch: 52 [39552/50048]	Loss: 0.2536
Training Epoch: 52 [39680/50048]	Loss: 0.2040
Training Epoch: 52 [39808/50048]	Loss: 0.2788
Training Epoch: 52 [39936/50048]	Loss: 0.2587
Training Epoch: 52 [40064/50048]	Loss: 0.3039
Training Epoch: 52 [40192/50048]	Loss: 0.2619
Training Epoch: 52 [40320/50048]	Loss: 0.2593
Training Epoch: 52 [40448/50048]	Loss: 0.2785
Training Epoch: 52 [40576/50048]	Loss: 0.2709
Training Epoch: 52 [40704/50048]	Loss: 0.1842
Training Epoch: 52 [40832/50048]	Loss: 0.4311
Training Epoch: 52 [40960/50048]	Loss: 0.3105
Training Epoch: 52 [41088/50048]	Loss: 0.2973
Training Epoch: 52 [41216/50048]	Loss: 0.3066
Training Epoch: 52 [41344/50048]	Loss: 0.2532
Training Epoch: 52 [41472/50048]	Loss: 0.3341
Training Epoch: 52 [41600/50048]	Loss: 0.2179
Training Epoch: 52 [41728/50048]	Loss: 0.2472
Training Epoch: 52 [41856/50048]	Loss: 0.2358
Training Epoch: 52 [41984/50048]	Loss: 0.3357
Training Epoch: 52 [42112/50048]	Loss: 0.1950
Training Epoch: 52 [42240/50048]	Loss: 0.3240
Training Epoch: 52 [42368/50048]	Loss: 0.2496
Training Epoch: 52 [42496/50048]	Loss: 0.1948
Training Epoch: 52 [42624/50048]	Loss: 0.2015
Training Epoch: 52 [42752/50048]	Loss: 0.2713
Training Epoch: 52 [42880/50048]	Loss: 0.3637
Training Epoch: 52 [43008/50048]	Loss: 0.2441
Training Epoch: 52 [43136/50048]	Loss: 0.2389
Training Epoch: 52 [43264/50048]	Loss: 0.3127
Training Epoch: 52 [43392/50048]	Loss: 0.3330
Training Epoch: 52 [43520/50048]	Loss: 0.2551
Training Epoch: 52 [43648/50048]	Loss: 0.3502
Training Epoch: 52 [43776/50048]	Loss: 0.3005
Training Epoch: 52 [43904/50048]	Loss: 0.2879
Training Epoch: 52 [44032/50048]	Loss: 0.2284
Training Epoch: 52 [44160/50048]	Loss: 0.2222
Training Epoch: 52 [44288/50048]	Loss: 0.2088
Training Epoch: 52 [44416/50048]	Loss: 0.2447
Training Epoch: 52 [44544/50048]	Loss: 0.2749
Training Epoch: 52 [44672/50048]	Loss: 0.1757
Training Epoch: 52 [44800/50048]	Loss: 0.2198
Training Epoch: 52 [44928/50048]	Loss: 0.2286
Training Epoch: 52 [45056/50048]	Loss: 0.2610
Training Epoch: 52 [45184/50048]	Loss: 0.3279
Training Epoch: 52 [45312/50048]	Loss: 0.3589
Training Epoch: 52 [45440/50048]	Loss: 0.2888
Training Epoch: 52 [45568/50048]	Loss: 0.3728
Training Epoch: 52 [45696/50048]	Loss: 0.2631
2022-12-06 04:57:02,151 [ZeusDataLoader(train)] train epoch 53 done: time=86.49 energy=10494.78
2022-12-06 04:57:02,153 [ZeusDataLoader(eval)] Epoch 53 begin.
Training Epoch: 52 [45824/50048]	Loss: 0.2720
Training Epoch: 52 [45952/50048]	Loss: 0.1510
Training Epoch: 52 [46080/50048]	Loss: 0.2463
Training Epoch: 52 [46208/50048]	Loss: 0.2747
Training Epoch: 52 [46336/50048]	Loss: 0.2952
Training Epoch: 52 [46464/50048]	Loss: 0.3990
Training Epoch: 52 [46592/50048]	Loss: 0.2960
Training Epoch: 52 [46720/50048]	Loss: 0.3076
Training Epoch: 52 [46848/50048]	Loss: 0.2931
Training Epoch: 52 [46976/50048]	Loss: 0.2062
Training Epoch: 52 [47104/50048]	Loss: 0.4799
Training Epoch: 52 [47232/50048]	Loss: 0.2120
Training Epoch: 52 [47360/50048]	Loss: 0.3062
Training Epoch: 52 [47488/50048]	Loss: 0.2920
Training Epoch: 52 [47616/50048]	Loss: 0.3074
Training Epoch: 52 [47744/50048]	Loss: 0.3714
Training Epoch: 52 [47872/50048]	Loss: 0.3678
Training Epoch: 52 [48000/50048]	Loss: 0.2066
Training Epoch: 52 [48128/50048]	Loss: 0.2185
Training Epoch: 52 [48256/50048]	Loss: 0.3916
Training Epoch: 52 [48384/50048]	Loss: 0.2407
Training Epoch: 52 [48512/50048]	Loss: 0.2839
Training Epoch: 52 [48640/50048]	Loss: 0.2172
Training Epoch: 52 [48768/50048]	Loss: 0.3010
Training Epoch: 52 [48896/50048]	Loss: 0.3491
Training Epoch: 52 [49024/50048]	Loss: 0.2185
Training Epoch: 52 [49152/50048]	Loss: 0.1523
Training Epoch: 52 [49280/50048]	Loss: 0.2657
Training Epoch: 52 [49408/50048]	Loss: 0.1989
Training Epoch: 52 [49536/50048]	Loss: 0.2822
Training Epoch: 52 [49664/50048]	Loss: 0.2325
Training Epoch: 52 [49792/50048]	Loss: 0.2065
Training Epoch: 52 [49920/50048]	Loss: 0.2847
Training Epoch: 52 [50048/50048]	Loss: 0.3470
2022-12-06 09:57:05.836 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:57:05,847 [ZeusDataLoader(eval)] eval epoch 53 done: time=3.69 energy=441.17
2022-12-06 04:57:05,848 [ZeusDataLoader(train)] Up to epoch 53: time=4782.47, energy=580506.18, cost=708718.87
2022-12-06 04:57:05,848 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:57:05,848 [ZeusDataLoader(train)] Expected next epoch: time=4872.26, energy=591304.20, cost=721975.26
2022-12-06 04:57:05,849 [ZeusDataLoader(train)] Epoch 54 begin.
Validation Epoch: 52, Average loss: 0.0155, Accuracy: 0.6366
2022-12-06 04:57:06,027 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:57:06,028 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:57:06.029 [ZeusMonitor] Monitor started.
2022-12-06 09:57:06.030 [ZeusMonitor] Running indefinitely. 2022-12-06 09:57:06.030 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:57:06.030 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e54+gpu0.power.log
Training Epoch: 53 [128/50048]	Loss: 0.2852
Training Epoch: 53 [256/50048]	Loss: 0.2420
Training Epoch: 53 [384/50048]	Loss: 0.1706
Training Epoch: 53 [512/50048]	Loss: 0.1759
Training Epoch: 53 [640/50048]	Loss: 0.1354
Training Epoch: 53 [768/50048]	Loss: 0.1387
Training Epoch: 53 [896/50048]	Loss: 0.1778
Training Epoch: 53 [1024/50048]	Loss: 0.1697
Training Epoch: 53 [1152/50048]	Loss: 0.1994
Training Epoch: 53 [1280/50048]	Loss: 0.2299
Training Epoch: 53 [1408/50048]	Loss: 0.1867
Training Epoch: 53 [1536/50048]	Loss: 0.2531
Training Epoch: 53 [1664/50048]	Loss: 0.2436
Training Epoch: 53 [1792/50048]	Loss: 0.2296
Training Epoch: 53 [1920/50048]	Loss: 0.2442
Training Epoch: 53 [2048/50048]	Loss: 0.1940
Training Epoch: 53 [2176/50048]	Loss: 0.2514
Training Epoch: 53 [2304/50048]	Loss: 0.1509
Training Epoch: 53 [2432/50048]	Loss: 0.2292
Training Epoch: 53 [2560/50048]	Loss: 0.2758
Training Epoch: 53 [2688/50048]	Loss: 0.1823
Training Epoch: 53 [2816/50048]	Loss: 0.2284
Training Epoch: 53 [2944/50048]	Loss: 0.1979
Training Epoch: 53 [3072/50048]	Loss: 0.1938
Training Epoch: 53 [3200/50048]	Loss: 0.2615
Training Epoch: 53 [3328/50048]	Loss: 0.1641
Training Epoch: 53 [3456/50048]	Loss: 0.2173
Training Epoch: 53 [3584/50048]	Loss: 0.1992
Training Epoch: 53 [3712/50048]	Loss: 0.2091
Training Epoch: 53 [3840/50048]	Loss: 0.2561
Training Epoch: 53 [3968/50048]	Loss: 0.1983
Training Epoch: 53 [4096/50048]	Loss: 0.2254
Training Epoch: 53 [4224/50048]	Loss: 0.1845
Training Epoch: 53 [4352/50048]	Loss: 0.1845
Training Epoch: 53 [4480/50048]	Loss: 0.2107
Training Epoch: 53 [4608/50048]	Loss: 0.2412
Training Epoch: 53 [4736/50048]	Loss: 0.1558
Training Epoch: 53 [4864/50048]	Loss: 0.2877
Training Epoch: 53 [4992/50048]	Loss: 0.1862
Training Epoch: 53 [5120/50048]	Loss: 0.2199
Training Epoch: 53 [5248/50048]	Loss: 0.1992
Training Epoch: 53 [5376/50048]	Loss: 0.3174
Training Epoch: 53 [5504/50048]	Loss: 0.1192
Training Epoch: 53 [5632/50048]	Loss: 0.3125
Training Epoch: 53 [5760/50048]	Loss: 0.2656
Training Epoch: 53 [5888/50048]	Loss: 0.2344
Training Epoch: 53 [6016/50048]	Loss: 0.1644
Training Epoch: 53 [6144/50048]	Loss: 0.2840
Training Epoch: 53 [6272/50048]	Loss: 0.1911
Training Epoch: 53 [6400/50048]	Loss: 0.3335
Training Epoch: 53 [6528/50048]	Loss: 0.2276
Training Epoch: 53 [6656/50048]	Loss: 0.1126
Training Epoch: 53 [6784/50048]	Loss: 0.1604
Training Epoch: 53 [6912/50048]	Loss: 0.2036
Training Epoch: 53 [7040/50048]	Loss: 0.1931
Training Epoch: 53 [7168/50048]	Loss: 0.1669
Training Epoch: 53 [7296/50048]	Loss: 0.1755
Training Epoch: 53 [7424/50048]	Loss: 0.1862
Training Epoch: 53 [7552/50048]	Loss: 0.2499
Training Epoch: 53 [7680/50048]	Loss: 0.1785
Training Epoch: 53 [7808/50048]	Loss: 0.1437
Training Epoch: 53 [7936/50048]	Loss: 0.2099
Training Epoch: 53 [8064/50048]	Loss: 0.2144
Training Epoch: 53 [8192/50048]	Loss: 0.1662
Training Epoch: 53 [8320/50048]	Loss: 0.2232
Training Epoch: 53 [8448/50048]	Loss: 0.2814
Training Epoch: 53 [8576/50048]	Loss: 0.1845
Training Epoch: 53 [8704/50048]	Loss: 0.1096
Training Epoch: 53 [8832/50048]	Loss: 0.2283
Training Epoch: 53 [8960/50048]	Loss: 0.2120
Training Epoch: 53 [9088/50048]	Loss: 0.1678
Training Epoch: 53 [9216/50048]	Loss: 0.2202
Training Epoch: 53 [9344/50048]	Loss: 0.2644
Training Epoch: 53 [9472/50048]	Loss: 0.3312
Training Epoch: 53 [9600/50048]	Loss: 0.2018
Training Epoch: 53 [9728/50048]	Loss: 0.3129
Training Epoch: 53 [9856/50048]	Loss: 0.1866
Training Epoch: 53 [9984/50048]	Loss: 0.2132
Training Epoch: 53 [10112/50048]	Loss: 0.2264
Training Epoch: 53 [10240/50048]	Loss: 0.1780
Training Epoch: 53 [10368/50048]	Loss: 0.1767
Training Epoch: 53 [10496/50048]	Loss: 0.2499
Training Epoch: 53 [10624/50048]	Loss: 0.3687
Training Epoch: 53 [10752/50048]	Loss: 0.2783
Training Epoch: 53 [10880/50048]	Loss: 0.2991
Training Epoch: 53 [11008/50048]	Loss: 0.1761
Training Epoch: 53 [11136/50048]	Loss: 0.2079
Training Epoch: 53 [11264/50048]	Loss: 0.1583
Training Epoch: 53 [11392/50048]	Loss: 0.1488
Training Epoch: 53 [11520/50048]	Loss: 0.2963
Training Epoch: 53 [11648/50048]	Loss: 0.1476
Training Epoch: 53 [11776/50048]	Loss: 0.2210
Training Epoch: 53 [11904/50048]	Loss: 0.1464
Training Epoch: 53 [12032/50048]	Loss: 0.2479
Training Epoch: 53 [12160/50048]	Loss: 0.1609
Training Epoch: 53 [12288/50048]	Loss: 0.2685
Training Epoch: 53 [12416/50048]	Loss: 0.1561
Training Epoch: 53 [12544/50048]	Loss: 0.2517
Training Epoch: 53 [12672/50048]	Loss: 0.2703
Training Epoch: 53 [12800/50048]	Loss: 0.3007
Training Epoch: 53 [12928/50048]	Loss: 0.2349
Training Epoch: 53 [13056/50048]	Loss: 0.2435
Training Epoch: 53 [13184/50048]	Loss: 0.0993
Training Epoch: 53 [13312/50048]	Loss: 0.2030
Training Epoch: 53 [13440/50048]	Loss: 0.1730
Training Epoch: 53 [13568/50048]	Loss: 0.2394
Training Epoch: 53 [13696/50048]	Loss: 0.2372
Training Epoch: 53 [13824/50048]	Loss: 0.3085
Training Epoch: 53 [13952/50048]	Loss: 0.2515
Training Epoch: 53 [14080/50048]	Loss: 0.3044
Training Epoch: 53 [14208/50048]	Loss: 0.3154
Training Epoch: 53 [14336/50048]	Loss: 0.3013
Training Epoch: 53 [14464/50048]	Loss: 0.1700
Training Epoch: 53 [14592/50048]	Loss: 0.2337
Training Epoch: 53 [14720/50048]	Loss: 0.1400
Training Epoch: 53 [14848/50048]	Loss: 0.1539
Training Epoch: 53 [14976/50048]	Loss: 0.2230
Training Epoch: 53 [15104/50048]	Loss: 0.2481
Training Epoch: 53 [15232/50048]	Loss: 0.1341
Training Epoch: 53 [15360/50048]	Loss: 0.2698
Training Epoch: 53 [15488/50048]	Loss: 0.1851
Training Epoch: 53 [15616/50048]	Loss: 0.2002
Training Epoch: 53 [15744/50048]	Loss: 0.2843
Training Epoch: 53 [15872/50048]	Loss: 0.2548
Training Epoch: 53 [16000/50048]	Loss: 0.2493
Training Epoch: 53 [16128/50048]	Loss: 0.1831
Training Epoch: 53 [16256/50048]	Loss: 0.2840
Training Epoch: 53 [16384/50048]	Loss: 0.2311
Training Epoch: 53 [16512/50048]	Loss: 0.2656
Training Epoch: 53 [16640/50048]	Loss: 0.2602
Training Epoch: 53 [16768/50048]	Loss: 0.1669
Training Epoch: 53 [16896/50048]	Loss: 0.2605
Training Epoch: 53 [17024/50048]	Loss: 0.2368
Training Epoch: 53 [17152/50048]	Loss: 0.2561
Training Epoch: 53 [17280/50048]	Loss: 0.2226
Training Epoch: 53 [17408/50048]	Loss: 0.2068
Training Epoch: 53 [17536/50048]	Loss: 0.3121
Training Epoch: 53 [17664/50048]	Loss: 0.2888
Training Epoch: 53 [17792/50048]	Loss: 0.2500
Training Epoch: 53 [17920/50048]	Loss: 0.2915
Training Epoch: 53 [18048/50048]	Loss: 0.1888
Training Epoch: 53 [18176/50048]	Loss: 0.2212
Training Epoch: 53 [18304/50048]	Loss: 0.2972
Training Epoch: 53 [18432/50048]	Loss: 0.1683
Training Epoch: 53 [18560/50048]	Loss: 0.2056
Training Epoch: 53 [18688/50048]	Loss: 0.1516
Training Epoch: 53 [18816/50048]	Loss: 0.1831
Training Epoch: 53 [18944/50048]	Loss: 0.1496
Training Epoch: 53 [19072/50048]	Loss: 0.2690
Training Epoch: 53 [19200/50048]	Loss: 0.1941
Training Epoch: 53 [19328/50048]	Loss: 0.2889
Training Epoch: 53 [19456/50048]	Loss: 0.2014
Training Epoch: 53 [19584/50048]	Loss: 0.2870
Training Epoch: 53 [19712/50048]	Loss: 0.2231
Training Epoch: 53 [19840/50048]	Loss: 0.2104
Training Epoch: 53 [19968/50048]	Loss: 0.2471
Training Epoch: 53 [20096/50048]	Loss: 0.1977
Training Epoch: 53 [20224/50048]	Loss: 0.2130
Training Epoch: 53 [20352/50048]	Loss: 0.2265
Training Epoch: 53 [20480/50048]	Loss: 0.2532
Training Epoch: 53 [20608/50048]	Loss: 0.1683
Training Epoch: 53 [20736/50048]	Loss: 0.2015
Training Epoch: 53 [20864/50048]	Loss: 0.2138
Training Epoch: 53 [20992/50048]	Loss: 0.1586
Training Epoch: 53 [21120/50048]	Loss: 0.3150
Training Epoch: 53 [21248/50048]	Loss: 0.2491
Training Epoch: 53 [21376/50048]	Loss: 0.2246
Training Epoch: 53 [21504/50048]	Loss: 0.3344
Training Epoch: 53 [21632/50048]	Loss: 0.1866
Training Epoch: 53 [21760/50048]	Loss: 0.1713
Training Epoch: 53 [21888/50048]	Loss: 0.2664
Training Epoch: 53 [22016/50048]	Loss: 0.2339
Training Epoch: 53 [22144/50048]	Loss: 0.2260
Training Epoch: 53 [22272/50048]	Loss: 0.1679
Training Epoch: 53 [22400/50048]	Loss: 0.2031
Training Epoch: 53 [22528/50048]	Loss: 0.1868
Training Epoch: 53 [22656/50048]	Loss: 0.2406
Training Epoch: 53 [22784/50048]	Loss: 0.1803
Training Epoch: 53 [22912/50048]	Loss: 0.1690
Training Epoch: 53 [23040/50048]	Loss: 0.2542
Training Epoch: 53 [23168/50048]	Loss: 0.2516
Training Epoch: 53 [23296/50048]	Loss: 0.2657
Training Epoch: 53 [23424/50048]	Loss: 0.2048
Training Epoch: 53 [23552/50048]	Loss: 0.1681
Training Epoch: 53 [23680/50048]	Loss: 0.2193
Training Epoch: 53 [23808/50048]	Loss: 0.2157
Training Epoch: 53 [23936/50048]	Loss: 0.2666
Training Epoch: 53 [24064/50048]	Loss: 0.2501
Training Epoch: 53 [24192/50048]	Loss: 0.2008
Training Epoch: 53 [24320/50048]	Loss: 0.2960
Training Epoch: 53 [24448/50048]	Loss: 0.2356
Training Epoch: 53 [24576/50048]	Loss: 0.2031
Training Epoch: 53 [24704/50048]	Loss: 0.2934
Training Epoch: 53 [24832/50048]	Loss: 0.2139
Training Epoch: 53 [24960/50048]	Loss: 0.2645
Training Epoch: 53 [25088/50048]	Loss: 0.1657
Training Epoch: 53 [25216/50048]	Loss: 0.2710
Training Epoch: 53 [25344/50048]	Loss: 0.1337
Training Epoch: 53 [25472/50048]	Loss: 0.2256
Training Epoch: 53 [25600/50048]	Loss: 0.2422
Training Epoch: 53 [25728/50048]	Loss: 0.1667
Training Epoch: 53 [25856/50048]	Loss: 0.2677
Training Epoch: 53 [25984/50048]	Loss: 0.2778
Training Epoch: 53 [26112/50048]	Loss: 0.2030
Training Epoch: 53 [26240/50048]	Loss: 0.2534
Training Epoch: 53 [26368/50048]	Loss: 0.1989
Training Epoch: 53 [26496/50048]	Loss: 0.2965
Training Epoch: 53 [26624/50048]	Loss: 0.2095
Training Epoch: 53 [26752/50048]	Loss: 0.1854
Training Epoch: 53 [26880/50048]	Loss: 0.1964
Training Epoch: 53 [27008/50048]	Loss: 0.1963
Training Epoch: 53 [27136/50048]	Loss: 0.3227
Training Epoch: 53 [27264/50048]	Loss: 0.3113
Training Epoch: 53 [27392/50048]	Loss: 0.2758
Training Epoch: 53 [27520/50048]	Loss: 0.1648
Training Epoch: 53 [27648/50048]	Loss: 0.2732
Training Epoch: 53 [27776/50048]	Loss: 0.2937
Training Epoch: 53 [27904/50048]	Loss: 0.3043
Training Epoch: 53 [28032/50048]	Loss: 0.1679
Training Epoch: 53 [28160/50048]	Loss: 0.2609
Training Epoch: 53 [28288/50048]	Loss: 0.2556
Training Epoch: 53 [28416/50048]	Loss: 0.2487
Training Epoch: 53 [28544/50048]	Loss: 0.2876
Training Epoch: 53 [28672/50048]	Loss: 0.3177
Training Epoch: 53 [28800/50048]	Loss: 0.1590
Training Epoch: 53 [28928/50048]	Loss: 0.1857
Training Epoch: 53 [29056/50048]	Loss: 0.2183
Training Epoch: 53 [29184/50048]	Loss: 0.3158
Training Epoch: 53 [29312/50048]	Loss: 0.2681
Training Epoch: 53 [29440/50048]	Loss: 0.2466
Training Epoch: 53 [29568/50048]	Loss: 0.1839
Training Epoch: 53 [29696/50048]	Loss: 0.2798
Training Epoch: 53 [29824/50048]	Loss: 0.2900
Training Epoch: 53 [29952/50048]	Loss: 0.2164
Training Epoch: 53 [30080/50048]	Loss: 0.1956
Training Epoch: 53 [30208/50048]	Loss: 0.1780
Training Epoch: 53 [30336/50048]	Loss: 0.3001
Training Epoch: 53 [30464/50048]	Loss: 0.2470
Training Epoch: 53 [30592/50048]	Loss: 0.2999
Training Epoch: 53 [30720/50048]	Loss: 0.2004
Training Epoch: 53 [30848/50048]	Loss: 0.1546
Training Epoch: 53 [30976/50048]	Loss: 0.2645
Training Epoch: 53 [31104/50048]	Loss: 0.1942
Training Epoch: 53 [31232/50048]	Loss: 0.2744
Training Epoch: 53 [31360/50048]	Loss: 0.2835
Training Epoch: 53 [31488/50048]	Loss: 0.2064
Training Epoch: 53 [31616/50048]	Loss: 0.2148
Training Epoch: 53 [31744/50048]	Loss: 0.4187
Training Epoch: 53 [31872/50048]	Loss: 0.1729
Training Epoch: 53 [32000/50048]	Loss: 0.2449
Training Epoch: 53 [32128/50048]	Loss: 0.2684
Training Epoch: 53 [32256/50048]	Loss: 0.3256
Training Epoch: 53 [32384/50048]	Loss: 0.3066
Training Epoch: 53 [32512/50048]	Loss: 0.2512
Training Epoch: 53 [32640/50048]	Loss: 0.4609
Training Epoch: 53 [32768/50048]	Loss: 0.3091
Training Epoch: 53 [32896/50048]	Loss: 0.2025
Training Epoch: 53 [33024/50048]	Loss: 0.2278
Training Epoch: 53 [33152/50048]	Loss: 0.1777
Training Epoch: 53 [33280/50048]	Loss: 0.3609
Training Epoch: 53 [33408/50048]	Loss: 0.2271
Training Epoch: 53 [33536/50048]	Loss: 0.1857
Training Epoch: 53 [33664/50048]	Loss: 0.3396
Training Epoch: 53 [33792/50048]	Loss: 0.2132
Training Epoch: 53 [33920/50048]	Loss: 0.3649
Training Epoch: 53 [34048/50048]	Loss: 0.3162
Training Epoch: 53 [34176/50048]	Loss: 0.2095
Training Epoch: 53 [34304/50048]	Loss: 0.2015
Training Epoch: 53 [34432/50048]	Loss: 0.2805
Training Epoch: 53 [34560/50048]	Loss: 0.3018
Training Epoch: 53 [34688/50048]	Loss: 0.2574
Training Epoch: 53 [34816/50048]	Loss: 0.2834
Training Epoch: 53 [34944/50048]	Loss: 0.1998
Training Epoch: 53 [35072/50048]	Loss: 0.2741
Training Epoch: 53 [35200/50048]	Loss: 0.2497
Training Epoch: 53 [35328/50048]	Loss: 0.3181
Training Epoch: 53 [35456/50048]	Loss: 0.2015
Training Epoch: 53 [35584/50048]	Loss: 0.1340
Training Epoch: 53 [35712/50048]	Loss: 0.1852
Training Epoch: 53 [35840/50048]	Loss: 0.2148
Training Epoch: 53 [35968/50048]	Loss: 0.2188
Training Epoch: 53 [36096/50048]	Loss: 0.1330
Training Epoch: 53 [36224/50048]	Loss: 0.2499
Training Epoch: 53 [36352/50048]	Loss: 0.2117
Training Epoch: 53 [36480/50048]	Loss: 0.1911
Training Epoch: 53 [36608/50048]	Loss: 0.2159
Training Epoch: 53 [36736/50048]	Loss: 0.3497
Training Epoch: 53 [36864/50048]	Loss: 0.2059
Training Epoch: 53 [36992/50048]	Loss: 0.2910
Training Epoch: 53 [37120/50048]	Loss: 0.2293
Training Epoch: 53 [37248/50048]	Loss: 0.3096
Training Epoch: 53 [37376/50048]	Loss: 0.2165
Training Epoch: 53 [37504/50048]	Loss: 0.1818
Training Epoch: 53 [37632/50048]	Loss: 0.2251
Training Epoch: 53 [37760/50048]	Loss: 0.3025
Training Epoch: 53 [37888/50048]	Loss: 0.2301
Training Epoch: 53 [38016/50048]	Loss: 0.2597
Training Epoch: 53 [38144/50048]	Loss: 0.4400
Training Epoch: 53 [38272/50048]	Loss: 0.2183
Training Epoch: 53 [38400/50048]	Loss: 0.2947
Training Epoch: 53 [38528/50048]	Loss: 0.2838
Training Epoch: 53 [38656/50048]	Loss: 0.3360
Training Epoch: 53 [38784/50048]	Loss: 0.4574
Training Epoch: 53 [38912/50048]	Loss: 0.2741
Training Epoch: 53 [39040/50048]	Loss: 0.2474
Training Epoch: 53 [39168/50048]	Loss: 0.3466
Training Epoch: 53 [39296/50048]	Loss: 0.2437
Training Epoch: 53 [39424/50048]	Loss: 0.2656
Training Epoch: 53 [39552/50048]	Loss: 0.1915
Training Epoch: 53 [39680/50048]	Loss: 0.3517
Training Epoch: 53 [39808/50048]	Loss: 0.1489
Training Epoch: 53 [39936/50048]	Loss: 0.3088
Training Epoch: 53 [40064/50048]	Loss: 0.3043
Training Epoch: 53 [40192/50048]	Loss: 0.2334
Training Epoch: 53 [40320/50048]	Loss: 0.2762
Training Epoch: 53 [40448/50048]	Loss: 0.2077
Training Epoch: 53 [40576/50048]	Loss: 0.2839
Training Epoch: 53 [40704/50048]	Loss: 0.1422
Training Epoch: 53 [40832/50048]	Loss: 0.2288
Training Epoch: 53 [40960/50048]	Loss: 0.2158
Training Epoch: 53 [41088/50048]	Loss: 0.2174
Training Epoch: 53 [41216/50048]	Loss: 0.3204
Training Epoch: 53 [41344/50048]	Loss: 0.2955
Training Epoch: 53 [41472/50048]	Loss: 0.2346
Training Epoch: 53 [41600/50048]	Loss: 0.2803
Training Epoch: 53 [41728/50048]	Loss: 0.2484
Training Epoch: 53 [41856/50048]	Loss: 0.3093
Training Epoch: 53 [41984/50048]	Loss: 0.2194
Training Epoch: 53 [42112/50048]	Loss: 0.2791
Training Epoch: 53 [42240/50048]	Loss: 0.1972
Training Epoch: 53 [42368/50048]	Loss: 0.2306
Training Epoch: 53 [42496/50048]	Loss: 0.2165
Training Epoch: 53 [42624/50048]	Loss: 0.3398
Training Epoch: 53 [42752/50048]	Loss: 0.2526
Training Epoch: 53 [42880/50048]	Loss: 0.3243
Training Epoch: 53 [43008/50048]	Loss: 0.2706
Training Epoch: 53 [43136/50048]	Loss: 0.1012
Training Epoch: 53 [43264/50048]	Loss: 0.3545
Training Epoch: 53 [43392/50048]	Loss: 0.3073
Training Epoch: 53 [43520/50048]	Loss: 0.2640
Training Epoch: 53 [43648/50048]	Loss: 0.2794
Training Epoch: 53 [43776/50048]	Loss: 0.2317
Training Epoch: 53 [43904/50048]	Loss: 0.1994
Training Epoch: 53 [44032/50048]	Loss: 0.1947
Training Epoch: 53 [44160/50048]	Loss: 0.1820
Training Epoch: 53 [44288/50048]	Loss: 0.2761
Training Epoch: 53 [44416/50048]	Loss: 0.3461
Training Epoch: 53 [44544/50048]	Loss: 0.2135
Training Epoch: 53 [44672/50048]	Loss: 0.2587
Training Epoch: 53 [44800/50048]	Loss: 0.3086
Training Epoch: 53 [44928/50048]	Loss: 0.2004
Training Epoch: 53 [45056/50048]	Loss: 0.2500
Training Epoch: 53 [45184/50048]	Loss: 0.1765
Training Epoch: 53 [45312/50048]	Loss: 0.2733
Training Epoch: 53 [45440/50048]	Loss: 0.2932
Training Epoch: 53 [45568/50048]	Loss: 0.2262
Training Epoch: 53 [45696/50048]	Loss: 0.1435
2022-12-06 04:58:32,378 [ZeusDataLoader(train)] train epoch 54 done: time=86.52 energy=10507.91
2022-12-06 04:58:32,379 [ZeusDataLoader(eval)] Epoch 54 begin.
Training Epoch: 53 [45824/50048]	Loss: 0.3091
Training Epoch: 53 [45952/50048]	Loss: 0.1851
Training Epoch: 53 [46080/50048]	Loss: 0.3143
Training Epoch: 53 [46208/50048]	Loss: 0.2273
Training Epoch: 53 [46336/50048]	Loss: 0.3150
Training Epoch: 53 [46464/50048]	Loss: 0.2301
Training Epoch: 53 [46592/50048]	Loss: 0.2854
Training Epoch: 53 [46720/50048]	Loss: 0.1440
Training Epoch: 53 [46848/50048]	Loss: 0.2630
Training Epoch: 53 [46976/50048]	Loss: 0.2638
Training Epoch: 53 [47104/50048]	Loss: 0.1631
Training Epoch: 53 [47232/50048]	Loss: 0.1921
Training Epoch: 53 [47360/50048]	Loss: 0.1477
Training Epoch: 53 [47488/50048]	Loss: 0.2472
Training Epoch: 53 [47616/50048]	Loss: 0.3859
Training Epoch: 53 [47744/50048]	Loss: 0.2764
Training Epoch: 53 [47872/50048]	Loss: 0.2865
Training Epoch: 53 [48000/50048]	Loss: 0.2266
Training Epoch: 53 [48128/50048]	Loss: 0.1631
Training Epoch: 53 [48256/50048]	Loss: 0.1999
Training Epoch: 53 [48384/50048]	Loss: 0.3046
Training Epoch: 53 [48512/50048]	Loss: 0.3454
Training Epoch: 53 [48640/50048]	Loss: 0.3579
Training Epoch: 53 [48768/50048]	Loss: 0.2241
Training Epoch: 53 [48896/50048]	Loss: 0.2115
Training Epoch: 53 [49024/50048]	Loss: 0.3054
Training Epoch: 53 [49152/50048]	Loss: 0.2054
Training Epoch: 53 [49280/50048]	Loss: 0.3802
Training Epoch: 53 [49408/50048]	Loss: 0.2921
Training Epoch: 53 [49536/50048]	Loss: 0.2060
Training Epoch: 53 [49664/50048]	Loss: 0.3004
Training Epoch: 53 [49792/50048]	Loss: 0.2179
Training Epoch: 53 [49920/50048]	Loss: 0.3981
Training Epoch: 53 [50048/50048]	Loss: 0.3573
2022-12-06 09:58:36.027 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 04:58:36,040 [ZeusDataLoader(eval)] eval epoch 54 done: time=3.65 energy=440.93
2022-12-06 04:58:36,040 [ZeusDataLoader(train)] Up to epoch 54: time=4872.64, energy=591455.03, cost=722083.18
2022-12-06 04:58:36,040 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 04:58:36,040 [ZeusDataLoader(train)] Expected next epoch: time=4962.43, energy=602253.04, cost=735339.56
2022-12-06 04:58:36,041 [ZeusDataLoader(train)] Epoch 55 begin.
Validation Epoch: 53, Average loss: 0.0157, Accuracy: 0.6276
2022-12-06 04:58:36,222 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 04:58:36,223 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 09:58:36.225 [ZeusMonitor] Monitor started.
2022-12-06 09:58:36.225 [ZeusMonitor] Running indefinitely. 2022-12-06 09:58:36.225 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 09:58:36.225 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e55+gpu0.power.log
Training Epoch: 54 [128/50048]	Loss: 0.2636
Training Epoch: 54 [256/50048]	Loss: 0.3249
Training Epoch: 54 [384/50048]	Loss: 0.2082
Training Epoch: 54 [512/50048]	Loss: 0.1842
Training Epoch: 54 [640/50048]	Loss: 0.2457
Training Epoch: 54 [768/50048]	Loss: 0.2600
Training Epoch: 54 [896/50048]	Loss: 0.2849
Training Epoch: 54 [1024/50048]	Loss: 0.1929
Training Epoch: 54 [1152/50048]	Loss: 0.2438
Training Epoch: 54 [1280/50048]	Loss: 0.1448
Training Epoch: 54 [1408/50048]	Loss: 0.1598
Training Epoch: 54 [1536/50048]	Loss: 0.1809
Training Epoch: 54 [1664/50048]	Loss: 0.2083
Training Epoch: 54 [1792/50048]	Loss: 0.2257
Training Epoch: 54 [1920/50048]	Loss: 0.1169
Training Epoch: 54 [2048/50048]	Loss: 0.2704
Training Epoch: 54 [2176/50048]	Loss: 0.3127
Training Epoch: 54 [2304/50048]	Loss: 0.1892
Training Epoch: 54 [2432/50048]	Loss: 0.1450
Training Epoch: 54 [2560/50048]	Loss: 0.1969
Training Epoch: 54 [2688/50048]	Loss: 0.1446
Training Epoch: 54 [2816/50048]	Loss: 0.1972
Training Epoch: 54 [2944/50048]	Loss: 0.1468
Training Epoch: 54 [3072/50048]	Loss: 0.2699
Training Epoch: 54 [3200/50048]	Loss: 0.2058
Training Epoch: 54 [3328/50048]	Loss: 0.2072
Training Epoch: 54 [3456/50048]	Loss: 0.1767
Training Epoch: 54 [3584/50048]	Loss: 0.2541
Training Epoch: 54 [3712/50048]	Loss: 0.2877
Training Epoch: 54 [3840/50048]	Loss: 0.2585
Training Epoch: 54 [3968/50048]	Loss: 0.2158
Training Epoch: 54 [4096/50048]	Loss: 0.1176
Training Epoch: 54 [4224/50048]	Loss: 0.1813
Training Epoch: 54 [4352/50048]	Loss: 0.2407
Training Epoch: 54 [4480/50048]	Loss: 0.2381
Training Epoch: 54 [4608/50048]	Loss: 0.2124
Training Epoch: 54 [4736/50048]	Loss: 0.2187
Training Epoch: 54 [4864/50048]	Loss: 0.1975
Training Epoch: 54 [4992/50048]	Loss: 0.1851
Training Epoch: 54 [5120/50048]	Loss: 0.1844
Training Epoch: 54 [5248/50048]	Loss: 0.2828
Training Epoch: 54 [5376/50048]	Loss: 0.1505
Training Epoch: 54 [5504/50048]	Loss: 0.2241
Training Epoch: 54 [5632/50048]	Loss: 0.1798
Training Epoch: 54 [5760/50048]	Loss: 0.1829
Training Epoch: 54 [5888/50048]	Loss: 0.2824
Training Epoch: 54 [6016/50048]	Loss: 0.2221
Training Epoch: 54 [6144/50048]	Loss: 0.2067
Training Epoch: 54 [6272/50048]	Loss: 0.1530
Training Epoch: 54 [6400/50048]	Loss: 0.2495
Training Epoch: 54 [6528/50048]	Loss: 0.2643
Training Epoch: 54 [6656/50048]	Loss: 0.2316
Training Epoch: 54 [6784/50048]	Loss: 0.1473
Training Epoch: 54 [6912/50048]	Loss: 0.1693
Training Epoch: 54 [7040/50048]	Loss: 0.2264
Training Epoch: 54 [7168/50048]	Loss: 0.2593
Training Epoch: 54 [7296/50048]	Loss: 0.1980
Training Epoch: 54 [7424/50048]	Loss: 0.2196
Training Epoch: 54 [7552/50048]	Loss: 0.2261
Training Epoch: 54 [7680/50048]	Loss: 0.1593
Training Epoch: 54 [7808/50048]	Loss: 0.1977
Training Epoch: 54 [7936/50048]	Loss: 0.2486
Training Epoch: 54 [8064/50048]	Loss: 0.2208
Training Epoch: 54 [8192/50048]	Loss: 0.2239
Training Epoch: 54 [8320/50048]	Loss: 0.1833
Training Epoch: 54 [8448/50048]	Loss: 0.2244
Training Epoch: 54 [8576/50048]	Loss: 0.2122
Training Epoch: 54 [8704/50048]	Loss: 0.1913
Training Epoch: 54 [8832/50048]	Loss: 0.2136
Training Epoch: 54 [8960/50048]	Loss: 0.2376
Training Epoch: 54 [9088/50048]	Loss: 0.1717
Training Epoch: 54 [9216/50048]	Loss: 0.1244
Training Epoch: 54 [9344/50048]	Loss: 0.2229
Training Epoch: 54 [9472/50048]	Loss: 0.2386
Training Epoch: 54 [9600/50048]	Loss: 0.2049
Training Epoch: 54 [9728/50048]	Loss: 0.3040
Training Epoch: 54 [9856/50048]	Loss: 0.2312
Training Epoch: 54 [9984/50048]	Loss: 0.2253
Training Epoch: 54 [10112/50048]	Loss: 0.2168
Training Epoch: 54 [10240/50048]	Loss: 0.1552
Training Epoch: 54 [10368/50048]	Loss: 0.1542
Training Epoch: 54 [10496/50048]	Loss: 0.1708
Training Epoch: 54 [10624/50048]	Loss: 0.1955
Training Epoch: 54 [10752/50048]	Loss: 0.2401
Training Epoch: 54 [10880/50048]	Loss: 0.2064
Training Epoch: 54 [11008/50048]	Loss: 0.2852
Training Epoch: 54 [11136/50048]	Loss: 0.2075
Training Epoch: 54 [11264/50048]	Loss: 0.2272
Training Epoch: 54 [11392/50048]	Loss: 0.1191
Training Epoch: 54 [11520/50048]	Loss: 0.2508
Training Epoch: 54 [11648/50048]	Loss: 0.1763
Training Epoch: 54 [11776/50048]	Loss: 0.1671
Training Epoch: 54 [11904/50048]	Loss: 0.1692
Training Epoch: 54 [12032/50048]	Loss: 0.2089
Training Epoch: 54 [12160/50048]	Loss: 0.2257
Training Epoch: 54 [12288/50048]	Loss: 0.2586
Training Epoch: 54 [12416/50048]	Loss: 0.1834
Training Epoch: 54 [12544/50048]	Loss: 0.2045
Training Epoch: 54 [12672/50048]	Loss: 0.2077
Training Epoch: 54 [12800/50048]	Loss: 0.1889
Training Epoch: 54 [12928/50048]	Loss: 0.2178
Training Epoch: 54 [13056/50048]	Loss: 0.1717
Training Epoch: 54 [13184/50048]	Loss: 0.1295
Training Epoch: 54 [13312/50048]	Loss: 0.1728
Training Epoch: 54 [13440/50048]	Loss: 0.1718
Training Epoch: 54 [13568/50048]	Loss: 0.1604
Training Epoch: 54 [13696/50048]	Loss: 0.2836
Training Epoch: 54 [13824/50048]	Loss: 0.0755
Training Epoch: 54 [13952/50048]	Loss: 0.1796
Training Epoch: 54 [14080/50048]	Loss: 0.2633
Training Epoch: 54 [14208/50048]	Loss: 0.2220
Training Epoch: 54 [14336/50048]	Loss: 0.1694
Training Epoch: 54 [14464/50048]	Loss: 0.1818
Training Epoch: 54 [14592/50048]	Loss: 0.2396
Training Epoch: 54 [14720/50048]	Loss: 0.2603
Training Epoch: 54 [14848/50048]	Loss: 0.2033
Training Epoch: 54 [14976/50048]	Loss: 0.2717
Training Epoch: 54 [15104/50048]	Loss: 0.2303
Training Epoch: 54 [15232/50048]	Loss: 0.1722
Training Epoch: 54 [15360/50048]	Loss: 0.2854
Training Epoch: 54 [15488/50048]	Loss: 0.1813
Training Epoch: 54 [15616/50048]	Loss: 0.2092
Training Epoch: 54 [15744/50048]	Loss: 0.2220
Training Epoch: 54 [15872/50048]	Loss: 0.2091
Training Epoch: 54 [16000/50048]	Loss: 0.2854
Training Epoch: 54 [16128/50048]	Loss: 0.2426
Training Epoch: 54 [16256/50048]	Loss: 0.1851
Training Epoch: 54 [16384/50048]	Loss: 0.3673
Training Epoch: 54 [16512/50048]	Loss: 0.2265
Training Epoch: 54 [16640/50048]	Loss: 0.1200
Training Epoch: 54 [16768/50048]	Loss: 0.3488
Training Epoch: 54 [16896/50048]	Loss: 0.1970
Training Epoch: 54 [17024/50048]	Loss: 0.1379
Training Epoch: 54 [17152/50048]	Loss: 0.2112
Training Epoch: 54 [17280/50048]	Loss: 0.1798
Training Epoch: 54 [17408/50048]	Loss: 0.1623
Training Epoch: 54 [17536/50048]	Loss: 0.1742
Training Epoch: 54 [17664/50048]	Loss: 0.1982
Training Epoch: 54 [17792/50048]	Loss: 0.1890
Training Epoch: 54 [17920/50048]	Loss: 0.2045
Training Epoch: 54 [18048/50048]	Loss: 0.1480
Training Epoch: 54 [18176/50048]	Loss: 0.2542
Training Epoch: 54 [18304/50048]	Loss: 0.3211
Training Epoch: 54 [18432/50048]	Loss: 0.2589
Training Epoch: 54 [18560/50048]	Loss: 0.1877
Training Epoch: 54 [18688/50048]	Loss: 0.3193
Training Epoch: 54 [18816/50048]	Loss: 0.2775
Training Epoch: 54 [18944/50048]	Loss: 0.2055
Training Epoch: 54 [19072/50048]	Loss: 0.2542
Training Epoch: 54 [19200/50048]	Loss: 0.1760
Training Epoch: 54 [19328/50048]	Loss: 0.3264
Training Epoch: 54 [19456/50048]	Loss: 0.2615
Training Epoch: 54 [19584/50048]	Loss: 0.2892
Training Epoch: 54 [19712/50048]	Loss: 0.2268
Training Epoch: 54 [19840/50048]	Loss: 0.1254
Training Epoch: 54 [19968/50048]	Loss: 0.1708
Training Epoch: 54 [20096/50048]	Loss: 0.2506
Training Epoch: 54 [20224/50048]	Loss: 0.1651
Training Epoch: 54 [20352/50048]	Loss: 0.2206
Training Epoch: 54 [20480/50048]	Loss: 0.2852
Training Epoch: 54 [20608/50048]	Loss: 0.1986
Training Epoch: 54 [20736/50048]	Loss: 0.3874
Training Epoch: 54 [20864/50048]	Loss: 0.2030
Training Epoch: 54 [20992/50048]	Loss: 0.2373
Training Epoch: 54 [21120/50048]	Loss: 0.2091
Training Epoch: 54 [21248/50048]	Loss: 0.2124
Training Epoch: 54 [21376/50048]	Loss: 0.2487
Training Epoch: 54 [21504/50048]	Loss: 0.2742
Training Epoch: 54 [21632/50048]	Loss: 0.2261
Training Epoch: 54 [21760/50048]	Loss: 0.2437
Training Epoch: 54 [21888/50048]	Loss: 0.1926
Training Epoch: 54 [22016/50048]	Loss: 0.2376
Training Epoch: 54 [22144/50048]	Loss: 0.2869
Training Epoch: 54 [22272/50048]	Loss: 0.2090
Training Epoch: 54 [22400/50048]	Loss: 0.1610
Training Epoch: 54 [22528/50048]	Loss: 0.3657
Training Epoch: 54 [22656/50048]	Loss: 0.3079
Training Epoch: 54 [22784/50048]	Loss: 0.3244
Training Epoch: 54 [22912/50048]	Loss: 0.2692
Training Epoch: 54 [23040/50048]	Loss: 0.1820
Training Epoch: 54 [23168/50048]	Loss: 0.1770
Training Epoch: 54 [23296/50048]	Loss: 0.2126
Training Epoch: 54 [23424/50048]	Loss: 0.1996
Training Epoch: 54 [23552/50048]	Loss: 0.2562
Training Epoch: 54 [23680/50048]	Loss: 0.2157
Training Epoch: 54 [23808/50048]	Loss: 0.2194
Training Epoch: 54 [23936/50048]	Loss: 0.2323
Training Epoch: 54 [24064/50048]	Loss: 0.2486
Training Epoch: 54 [24192/50048]	Loss: 0.2713
Training Epoch: 54 [24320/50048]	Loss: 0.2212
Training Epoch: 54 [24448/50048]	Loss: 0.2254
Training Epoch: 54 [24576/50048]	Loss: 0.2358
Training Epoch: 54 [24704/50048]	Loss: 0.3013
Training Epoch: 54 [24832/50048]	Loss: 0.2133
Training Epoch: 54 [24960/50048]	Loss: 0.2073
Training Epoch: 54 [25088/50048]	Loss: 0.1781
Training Epoch: 54 [25216/50048]	Loss: 0.2440
Training Epoch: 54 [25344/50048]	Loss: 0.3080
Training Epoch: 54 [25472/50048]	Loss: 0.3065
Training Epoch: 54 [25600/50048]	Loss: 0.2545
Training Epoch: 54 [25728/50048]	Loss: 0.1604
Training Epoch: 54 [25856/50048]	Loss: 0.1809
Training Epoch: 54 [25984/50048]	Loss: 0.2257
Training Epoch: 54 [26112/50048]	Loss: 0.2007
Training Epoch: 54 [26240/50048]	Loss: 0.2001
Training Epoch: 54 [26368/50048]	Loss: 0.2900
Training Epoch: 54 [26496/50048]	Loss: 0.2861
Training Epoch: 54 [26624/50048]	Loss: 0.2893
Training Epoch: 54 [26752/50048]	Loss: 0.2492
Training Epoch: 54 [26880/50048]	Loss: 0.1891
Training Epoch: 54 [27008/50048]	Loss: 0.3129
Training Epoch: 54 [27136/50048]	Loss: 0.1840
Training Epoch: 54 [27264/50048]	Loss: 0.2152
Training Epoch: 54 [27392/50048]	Loss: 0.2586
Training Epoch: 54 [27520/50048]	Loss: 0.2106
Training Epoch: 54 [27648/50048]	Loss: 0.3544
Training Epoch: 54 [27776/50048]	Loss: 0.2110
Training Epoch: 54 [27904/50048]	Loss: 0.2701
Training Epoch: 54 [28032/50048]	Loss: 0.2503
Training Epoch: 54 [28160/50048]	Loss: 0.2362
Training Epoch: 54 [28288/50048]	Loss: 0.3302
Training Epoch: 54 [28416/50048]	Loss: 0.3360
Training Epoch: 54 [28544/50048]	Loss: 0.2258
Training Epoch: 54 [28672/50048]	Loss: 0.3394
Training Epoch: 54 [28800/50048]	Loss: 0.2277
Training Epoch: 54 [28928/50048]	Loss: 0.1937
Training Epoch: 54 [29056/50048]	Loss: 0.1572
Training Epoch: 54 [29184/50048]	Loss: 0.2093
Training Epoch: 54 [29312/50048]	Loss: 0.2091
Training Epoch: 54 [29440/50048]	Loss: 0.2730
Training Epoch: 54 [29568/50048]	Loss: 0.2055
Training Epoch: 54 [29696/50048]	Loss: 0.1567
Training Epoch: 54 [29824/50048]	Loss: 0.2571
Training Epoch: 54 [29952/50048]	Loss: 0.2640
Training Epoch: 54 [30080/50048]	Loss: 0.1680
Training Epoch: 54 [30208/50048]	Loss: 0.3008
Training Epoch: 54 [30336/50048]	Loss: 0.2100
Training Epoch: 54 [30464/50048]	Loss: 0.2267
Training Epoch: 54 [30592/50048]	Loss: 0.2279
Training Epoch: 54 [30720/50048]	Loss: 0.3115
Training Epoch: 54 [30848/50048]	Loss: 0.2483
Training Epoch: 54 [30976/50048]	Loss: 0.1669
Training Epoch: 54 [31104/50048]	Loss: 0.3067
Training Epoch: 54 [31232/50048]	Loss: 0.2133
Training Epoch: 54 [31360/50048]	Loss: 0.2368
Training Epoch: 54 [31488/50048]	Loss: 0.2129
Training Epoch: 54 [31616/50048]	Loss: 0.1870
Training Epoch: 54 [31744/50048]	Loss: 0.1905
Training Epoch: 54 [31872/50048]	Loss: 0.1523
Training Epoch: 54 [32000/50048]	Loss: 0.2853
Training Epoch: 54 [32128/50048]	Loss: 0.2657
Training Epoch: 54 [32256/50048]	Loss: 0.2735
Training Epoch: 54 [32384/50048]	Loss: 0.2287
Training Epoch: 54 [32512/50048]	Loss: 0.1747
Training Epoch: 54 [32640/50048]	Loss: 0.1429
Training Epoch: 54 [32768/50048]	Loss: 0.2164
Training Epoch: 54 [32896/50048]	Loss: 0.2135
Training Epoch: 54 [33024/50048]	Loss: 0.1706
Training Epoch: 54 [33152/50048]	Loss: 0.2511
Training Epoch: 54 [33280/50048]	Loss: 0.2869
Training Epoch: 54 [33408/50048]	Loss: 0.3541
Training Epoch: 54 [33536/50048]	Loss: 0.1876
Training Epoch: 54 [33664/50048]	Loss: 0.3001
Training Epoch: 54 [33792/50048]	Loss: 0.3349
Training Epoch: 54 [33920/50048]	Loss: 0.3971
Training Epoch: 54 [34048/50048]	Loss: 0.2823
Training Epoch: 54 [34176/50048]	Loss: 0.2303
Training Epoch: 54 [34304/50048]	Loss: 0.3878
Training Epoch: 54 [34432/50048]	Loss: 0.2919
Training Epoch: 54 [34560/50048]	Loss: 0.2590
Training Epoch: 54 [34688/50048]	Loss: 0.2309
Training Epoch: 54 [34816/50048]	Loss: 0.2553
Training Epoch: 54 [34944/50048]	Loss: 0.2842
Training Epoch: 54 [35072/50048]	Loss: 0.1899
Training Epoch: 54 [35200/50048]	Loss: 0.2208
Training Epoch: 54 [35328/50048]	Loss: 0.2760
Training Epoch: 54 [35456/50048]	Loss: 0.2149
Training Epoch: 54 [35584/50048]	Loss: 0.2742
Training Epoch: 54 [35712/50048]	Loss: 0.3416
Training Epoch: 54 [35840/50048]	Loss: 0.2631
Training Epoch: 54 [35968/50048]	Loss: 0.2725
Training Epoch: 54 [36096/50048]	Loss: 0.3014
Training Epoch: 54 [36224/50048]	Loss: 0.2365
Training Epoch: 54 [36352/50048]	Loss: 0.1990
Training Epoch: 54 [36480/50048]	Loss: 0.2286
Training Epoch: 54 [36608/50048]	Loss: 0.3391
Training Epoch: 54 [36736/50048]	Loss: 0.3120
Training Epoch: 54 [36864/50048]	Loss: 0.3507
Training Epoch: 54 [36992/50048]	Loss: 0.2252
Training Epoch: 54 [37120/50048]	Loss: 0.2371
Training Epoch: 54 [37248/50048]	Loss: 0.2270
Training Epoch: 54 [37376/50048]	Loss: 0.2881
Training Epoch: 54 [37504/50048]	Loss: 0.2928
Training Epoch: 54 [37632/50048]	Loss: 0.2579
Training Epoch: 54 [37760/50048]	Loss: 0.1514
Training Epoch: 54 [37888/50048]	Loss: 0.2631
Training Epoch: 54 [38016/50048]	Loss: 0.2027
Training Epoch: 54 [38144/50048]	Loss: 0.2643
Training Epoch: 54 [38272/50048]	Loss: 0.3009
Training Epoch: 54 [38400/50048]	Loss: 0.3102
Training Epoch: 54 [38528/50048]	Loss: 0.2010
Training Epoch: 54 [38656/50048]	Loss: 0.1523
Training Epoch: 54 [38784/50048]	Loss: 0.2935
Training Epoch: 54 [38912/50048]	Loss: 0.3150
Training Epoch: 54 [39040/50048]	Loss: 0.1663
Training Epoch: 54 [39168/50048]	Loss: 0.1472
Training Epoch: 54 [39296/50048]	Loss: 0.1940
Training Epoch: 54 [39424/50048]	Loss: 0.2654
Training Epoch: 54 [39552/50048]	Loss: 0.1998
Training Epoch: 54 [39680/50048]	Loss: 0.3450
Training Epoch: 54 [39808/50048]	Loss: 0.2301
Training Epoch: 54 [39936/50048]	Loss: 0.2292
Training Epoch: 54 [40064/50048]	Loss: 0.2744
Training Epoch: 54 [40192/50048]	Loss: 0.3751
Training Epoch: 54 [40320/50048]	Loss: 0.2368
Training Epoch: 54 [40448/50048]	Loss: 0.3046
Training Epoch: 54 [40576/50048]	Loss: 0.2593
Training Epoch: 54 [40704/50048]	Loss: 0.1956
Training Epoch: 54 [40832/50048]	Loss: 0.2501
Training Epoch: 54 [40960/50048]	Loss: 0.2566
Training Epoch: 54 [41088/50048]	Loss: 0.2236
Training Epoch: 54 [41216/50048]	Loss: 0.2747
Training Epoch: 54 [41344/50048]	Loss: 0.1737
Training Epoch: 54 [41472/50048]	Loss: 0.3292
Training Epoch: 54 [41600/50048]	Loss: 0.3389
Training Epoch: 54 [41728/50048]	Loss: 0.2705
Training Epoch: 54 [41856/50048]	Loss: 0.1347
Training Epoch: 54 [41984/50048]	Loss: 0.3074
Training Epoch: 54 [42112/50048]	Loss: 0.1769
Training Epoch: 54 [42240/50048]	Loss: 0.2712
Training Epoch: 54 [42368/50048]	Loss: 0.1751
Training Epoch: 54 [42496/50048]	Loss: 0.1582
Training Epoch: 54 [42624/50048]	Loss: 0.2252
Training Epoch: 54 [42752/50048]	Loss: 0.2717
Training Epoch: 54 [42880/50048]	Loss: 0.2607
Training Epoch: 54 [43008/50048]	Loss: 0.3367
Training Epoch: 54 [43136/50048]	Loss: 0.2995
Training Epoch: 54 [43264/50048]	Loss: 0.2205
Training Epoch: 54 [43392/50048]	Loss: 0.2993
Training Epoch: 54 [43520/50048]	Loss: 0.2245
Training Epoch: 54 [43648/50048]	Loss: 0.2326
Training Epoch: 54 [43776/50048]	Loss: 0.2759
Training Epoch: 54 [43904/50048]	Loss: 0.3650
Training Epoch: 54 [44032/50048]	Loss: 0.3281
Training Epoch: 54 [44160/50048]	Loss: 0.2832
Training Epoch: 54 [44288/50048]	Loss: 0.3645
Training Epoch: 54 [44416/50048]	Loss: 0.2772
Training Epoch: 54 [44544/50048]	Loss: 0.2483
Training Epoch: 54 [44672/50048]	Loss: 0.2106
Training Epoch: 54 [44800/50048]	Loss: 0.2218
Training Epoch: 54 [44928/50048]	Loss: 0.1891
Training Epoch: 54 [45056/50048]	Loss: 0.1766
Training Epoch: 54 [45184/50048]	Loss: 0.2272
Training Epoch: 54 [45312/50048]	Loss: 0.3247
Training Epoch: 54 [45440/50048]	Loss: 0.3566
Training Epoch: 54 [45568/50048]	Loss: 0.2904
Training Epoch: 54 [45696/50048]	Loss: 0.2244
2022-12-06 05:00:02,506 [ZeusDataLoader(train)] train epoch 55 done: time=86.45 energy=10500.85
2022-12-06 05:00:02,507 [ZeusDataLoader(eval)] Epoch 55 begin.
Training Epoch: 54 [45824/50048]	Loss: 0.1393
Training Epoch: 54 [45952/50048]	Loss: 0.1972
Training Epoch: 54 [46080/50048]	Loss: 0.2370
Training Epoch: 54 [46208/50048]	Loss: 0.2817
Training Epoch: 54 [46336/50048]	Loss: 0.3126
Training Epoch: 54 [46464/50048]	Loss: 0.2666
Training Epoch: 54 [46592/50048]	Loss: 0.2355
Training Epoch: 54 [46720/50048]	Loss: 0.1591
Training Epoch: 54 [46848/50048]	Loss: 0.2544
Training Epoch: 54 [46976/50048]	Loss: 0.2281
Training Epoch: 54 [47104/50048]	Loss: 0.1775
Training Epoch: 54 [47232/50048]	Loss: 0.2831
Training Epoch: 54 [47360/50048]	Loss: 0.2953
Training Epoch: 54 [47488/50048]	Loss: 0.2415
Training Epoch: 54 [47616/50048]	Loss: 0.3073
Training Epoch: 54 [47744/50048]	Loss: 0.2687
Training Epoch: 54 [47872/50048]	Loss: 0.1708
Training Epoch: 54 [48000/50048]	Loss: 0.2250
Training Epoch: 54 [48128/50048]	Loss: 0.2921
Training Epoch: 54 [48256/50048]	Loss: 0.2638
Training Epoch: 54 [48384/50048]	Loss: 0.3591
Training Epoch: 54 [48512/50048]	Loss: 0.2139
Training Epoch: 54 [48640/50048]	Loss: 0.2538
Training Epoch: 54 [48768/50048]	Loss: 0.3254
Training Epoch: 54 [48896/50048]	Loss: 0.3276
Training Epoch: 54 [49024/50048]	Loss: 0.4126
Training Epoch: 54 [49152/50048]	Loss: 0.1364
Training Epoch: 54 [49280/50048]	Loss: 0.1553
Training Epoch: 54 [49408/50048]	Loss: 0.1668
Training Epoch: 54 [49536/50048]	Loss: 0.1643
Training Epoch: 54 [49664/50048]	Loss: 0.3171
Training Epoch: 54 [49792/50048]	Loss: 0.2180
Training Epoch: 54 [49920/50048]	Loss: 0.1976
Training Epoch: 54 [50048/50048]	Loss: 0.2766
2022-12-06 10:00:06.242 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:00:06,269 [ZeusDataLoader(eval)] eval epoch 55 done: time=3.75 energy=454.55
2022-12-06 05:00:06,269 [ZeusDataLoader(train)] Up to epoch 55: time=4962.84, energy=602410.42, cost=735453.95
2022-12-06 05:00:06,269 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:00:06,269 [ZeusDataLoader(train)] Expected next epoch: time=5052.64, energy=613208.44, cost=748710.33
2022-12-06 05:00:06,270 [ZeusDataLoader(train)] Epoch 56 begin.
Validation Epoch: 54, Average loss: 0.0158, Accuracy: 0.6338
2022-12-06 05:00:06,452 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:00:06,453 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:00:06.455 [ZeusMonitor] Monitor started.
2022-12-06 10:00:06.455 [ZeusMonitor] Running indefinitely. 2022-12-06 10:00:06.455 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:00:06.455 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e56+gpu0.power.log
Training Epoch: 55 [128/50048]	Loss: 0.1673
Training Epoch: 55 [256/50048]	Loss: 0.2102
Training Epoch: 55 [384/50048]	Loss: 0.1549
Training Epoch: 55 [512/50048]	Loss: 0.3455
Training Epoch: 55 [640/50048]	Loss: 0.2105
Training Epoch: 55 [768/50048]	Loss: 0.2264
Training Epoch: 55 [896/50048]	Loss: 0.2808
Training Epoch: 55 [1024/50048]	Loss: 0.2153
Training Epoch: 55 [1152/50048]	Loss: 0.1132
Training Epoch: 55 [1280/50048]	Loss: 0.2640
Training Epoch: 55 [1408/50048]	Loss: 0.1959
Training Epoch: 55 [1536/50048]	Loss: 0.1969
Training Epoch: 55 [1664/50048]	Loss: 0.2084
Training Epoch: 55 [1792/50048]	Loss: 0.1600
Training Epoch: 55 [1920/50048]	Loss: 0.2398
Training Epoch: 55 [2048/50048]	Loss: 0.2189
Training Epoch: 55 [2176/50048]	Loss: 0.1575
Training Epoch: 55 [2304/50048]	Loss: 0.1862
Training Epoch: 55 [2432/50048]	Loss: 0.2019
Training Epoch: 55 [2560/50048]	Loss: 0.1533
Training Epoch: 55 [2688/50048]	Loss: 0.1390
Training Epoch: 55 [2816/50048]	Loss: 0.1833
Training Epoch: 55 [2944/50048]	Loss: 0.1458
Training Epoch: 55 [3072/50048]	Loss: 0.1485
Training Epoch: 55 [3200/50048]	Loss: 0.2060
Training Epoch: 55 [3328/50048]	Loss: 0.2791
Training Epoch: 55 [3456/50048]	Loss: 0.1733
Training Epoch: 55 [3584/50048]	Loss: 0.2414
Training Epoch: 55 [3712/50048]	Loss: 0.1915
Training Epoch: 55 [3840/50048]	Loss: 0.1896
Training Epoch: 55 [3968/50048]	Loss: 0.1307
Training Epoch: 55 [4096/50048]	Loss: 0.1506
Training Epoch: 55 [4224/50048]	Loss: 0.1353
Training Epoch: 55 [4352/50048]	Loss: 0.1994
Training Epoch: 55 [4480/50048]	Loss: 0.1467
Training Epoch: 55 [4608/50048]	Loss: 0.1156
Training Epoch: 55 [4736/50048]	Loss: 0.1310
Training Epoch: 55 [4864/50048]	Loss: 0.2198
Training Epoch: 55 [4992/50048]	Loss: 0.1249
Training Epoch: 55 [5120/50048]	Loss: 0.1954
Training Epoch: 55 [5248/50048]	Loss: 0.2185
Training Epoch: 55 [5376/50048]	Loss: 0.1988
Training Epoch: 55 [5504/50048]	Loss: 0.2211
Training Epoch: 55 [5632/50048]	Loss: 0.2353
Training Epoch: 55 [5760/50048]	Loss: 0.2217
Training Epoch: 55 [5888/50048]	Loss: 0.1845
Training Epoch: 55 [6016/50048]	Loss: 0.1101
Training Epoch: 55 [6144/50048]	Loss: 0.2751
Training Epoch: 55 [6272/50048]	Loss: 0.1932
Training Epoch: 55 [6400/50048]	Loss: 0.1469
Training Epoch: 55 [6528/50048]	Loss: 0.2336
Training Epoch: 55 [6656/50048]	Loss: 0.2368
Training Epoch: 55 [6784/50048]	Loss: 0.1058
Training Epoch: 55 [6912/50048]	Loss: 0.1718
Training Epoch: 55 [7040/50048]	Loss: 0.1875
Training Epoch: 55 [7168/50048]	Loss: 0.1865
Training Epoch: 55 [7296/50048]	Loss: 0.2089
Training Epoch: 55 [7424/50048]	Loss: 0.2662
Training Epoch: 55 [7552/50048]	Loss: 0.1733
Training Epoch: 55 [7680/50048]	Loss: 0.3194
Training Epoch: 55 [7808/50048]	Loss: 0.1953
Training Epoch: 55 [7936/50048]	Loss: 0.1333
Training Epoch: 55 [8064/50048]	Loss: 0.2856
Training Epoch: 55 [8192/50048]	Loss: 0.2158
Training Epoch: 55 [8320/50048]	Loss: 0.1503
Training Epoch: 55 [8448/50048]	Loss: 0.2888
Training Epoch: 55 [8576/50048]	Loss: 0.1647
Training Epoch: 55 [8704/50048]	Loss: 0.1955
Training Epoch: 55 [8832/50048]	Loss: 0.1814
Training Epoch: 55 [8960/50048]	Loss: 0.1800
Training Epoch: 55 [9088/50048]	Loss: 0.1049
Training Epoch: 55 [9216/50048]	Loss: 0.2142
Training Epoch: 55 [9344/50048]	Loss: 0.2342
Training Epoch: 55 [9472/50048]	Loss: 0.1871
Training Epoch: 55 [9600/50048]	Loss: 0.2363
Training Epoch: 55 [9728/50048]	Loss: 0.2088
Training Epoch: 55 [9856/50048]	Loss: 0.2099
Training Epoch: 55 [9984/50048]	Loss: 0.1646
Training Epoch: 55 [10112/50048]	Loss: 0.1346
Training Epoch: 55 [10240/50048]	Loss: 0.2812
Training Epoch: 55 [10368/50048]	Loss: 0.2599
Training Epoch: 55 [10496/50048]	Loss: 0.2761
Training Epoch: 55 [10624/50048]	Loss: 0.3183
Training Epoch: 55 [10752/50048]	Loss: 0.2444
Training Epoch: 55 [10880/50048]	Loss: 0.1555
Training Epoch: 55 [11008/50048]	Loss: 0.2011
Training Epoch: 55 [11136/50048]	Loss: 0.1379
Training Epoch: 55 [11264/50048]	Loss: 0.1905
Training Epoch: 55 [11392/50048]	Loss: 0.1607
Training Epoch: 55 [11520/50048]	Loss: 0.2568
Training Epoch: 55 [11648/50048]	Loss: 0.1464
Training Epoch: 55 [11776/50048]	Loss: 0.1644
Training Epoch: 55 [11904/50048]	Loss: 0.0929
Training Epoch: 55 [12032/50048]	Loss: 0.1958
Training Epoch: 55 [12160/50048]	Loss: 0.2549
Training Epoch: 55 [12288/50048]	Loss: 0.1865
Training Epoch: 55 [12416/50048]	Loss: 0.1143
Training Epoch: 55 [12544/50048]	Loss: 0.1887
Training Epoch: 55 [12672/50048]	Loss: 0.1570
Training Epoch: 55 [12800/50048]	Loss: 0.2000
Training Epoch: 55 [12928/50048]	Loss: 0.2518
Training Epoch: 55 [13056/50048]	Loss: 0.1962
Training Epoch: 55 [13184/50048]	Loss: 0.1674
Training Epoch: 55 [13312/50048]	Loss: 0.2793
Training Epoch: 55 [13440/50048]	Loss: 0.1931
Training Epoch: 55 [13568/50048]	Loss: 0.2527
Training Epoch: 55 [13696/50048]	Loss: 0.2651
Training Epoch: 55 [13824/50048]	Loss: 0.2107
Training Epoch: 55 [13952/50048]	Loss: 0.1558
Training Epoch: 55 [14080/50048]	Loss: 0.1742
Training Epoch: 55 [14208/50048]	Loss: 0.1419
Training Epoch: 55 [14336/50048]	Loss: 0.2561
Training Epoch: 55 [14464/50048]	Loss: 0.2381
Training Epoch: 55 [14592/50048]	Loss: 0.2357
Training Epoch: 55 [14720/50048]	Loss: 0.1900
Training Epoch: 55 [14848/50048]	Loss: 0.1881
Training Epoch: 55 [14976/50048]	Loss: 0.1762
Training Epoch: 55 [15104/50048]	Loss: 0.1724
Training Epoch: 55 [15232/50048]	Loss: 0.1260
Training Epoch: 55 [15360/50048]	Loss: 0.2453
Training Epoch: 55 [15488/50048]	Loss: 0.2638
Training Epoch: 55 [15616/50048]	Loss: 0.1543
Training Epoch: 55 [15744/50048]	Loss: 0.2101
Training Epoch: 55 [15872/50048]	Loss: 0.2292
Training Epoch: 55 [16000/50048]	Loss: 0.1119
Training Epoch: 55 [16128/50048]	Loss: 0.1511
Training Epoch: 55 [16256/50048]	Loss: 0.3091
Training Epoch: 55 [16384/50048]	Loss: 0.2859
Training Epoch: 55 [16512/50048]	Loss: 0.3116
Training Epoch: 55 [16640/50048]	Loss: 0.1806
Training Epoch: 55 [16768/50048]	Loss: 0.2235
Training Epoch: 55 [16896/50048]	Loss: 0.1411
Training Epoch: 55 [17024/50048]	Loss: 0.1552
Training Epoch: 55 [17152/50048]	Loss: 0.2172
Training Epoch: 55 [17280/50048]	Loss: 0.2100
Training Epoch: 55 [17408/50048]	Loss: 0.1836
Training Epoch: 55 [17536/50048]	Loss: 0.1553
Training Epoch: 55 [17664/50048]	Loss: 0.1889
Training Epoch: 55 [17792/50048]	Loss: 0.1522
Training Epoch: 55 [17920/50048]	Loss: 0.1819
Training Epoch: 55 [18048/50048]	Loss: 0.2679
Training Epoch: 55 [18176/50048]	Loss: 0.1886
Training Epoch: 55 [18304/50048]	Loss: 0.1492
Training Epoch: 55 [18432/50048]	Loss: 0.2409
Training Epoch: 55 [18560/50048]	Loss: 0.2250
Training Epoch: 55 [18688/50048]	Loss: 0.1740
Training Epoch: 55 [18816/50048]	Loss: 0.1499
Training Epoch: 55 [18944/50048]	Loss: 0.2621
Training Epoch: 55 [19072/50048]	Loss: 0.2054
Training Epoch: 55 [19200/50048]	Loss: 0.1788
Training Epoch: 55 [19328/50048]	Loss: 0.1960
Training Epoch: 55 [19456/50048]	Loss: 0.2633
Training Epoch: 55 [19584/50048]	Loss: 0.1968
Training Epoch: 55 [19712/50048]	Loss: 0.1801
Training Epoch: 55 [19840/50048]	Loss: 0.2321
Training Epoch: 55 [19968/50048]	Loss: 0.1760
Training Epoch: 55 [20096/50048]	Loss: 0.2411
Training Epoch: 55 [20224/50048]	Loss: 0.2242
Training Epoch: 55 [20352/50048]	Loss: 0.2446
Training Epoch: 55 [20480/50048]	Loss: 0.1582
Training Epoch: 55 [20608/50048]	Loss: 0.1909
Training Epoch: 55 [20736/50048]	Loss: 0.1446
Training Epoch: 55 [20864/50048]	Loss: 0.2292
Training Epoch: 55 [20992/50048]	Loss: 0.2233
Training Epoch: 55 [21120/50048]	Loss: 0.1574
Training Epoch: 55 [21248/50048]	Loss: 0.3123
Training Epoch: 55 [21376/50048]	Loss: 0.1533
Training Epoch: 55 [21504/50048]	Loss: 0.2412
Training Epoch: 55 [21632/50048]	Loss: 0.2492
Training Epoch: 55 [21760/50048]	Loss: 0.1797
Training Epoch: 55 [21888/50048]	Loss: 0.2309
Training Epoch: 55 [22016/50048]	Loss: 0.1806
Training Epoch: 55 [22144/50048]	Loss: 0.1372
Training Epoch: 55 [22272/50048]	Loss: 0.2117
Training Epoch: 55 [22400/50048]	Loss: 0.2061
Training Epoch: 55 [22528/50048]	Loss: 0.1501
Training Epoch: 55 [22656/50048]	Loss: 0.2931
Training Epoch: 55 [22784/50048]	Loss: 0.1441
Training Epoch: 55 [22912/50048]	Loss: 0.1814
Training Epoch: 55 [23040/50048]	Loss: 0.1900
Training Epoch: 55 [23168/50048]	Loss: 0.1863
Training Epoch: 55 [23296/50048]	Loss: 0.1874
Training Epoch: 55 [23424/50048]	Loss: 0.1754
Training Epoch: 55 [23552/50048]	Loss: 0.1812
Training Epoch: 55 [23680/50048]	Loss: 0.1503
Training Epoch: 55 [23808/50048]	Loss: 0.2126
Training Epoch: 55 [23936/50048]	Loss: 0.3546
Training Epoch: 55 [24064/50048]	Loss: 0.2227
Training Epoch: 55 [24192/50048]	Loss: 0.1924
Training Epoch: 55 [24320/50048]	Loss: 0.1719
Training Epoch: 55 [24448/50048]	Loss: 0.2055
Training Epoch: 55 [24576/50048]	Loss: 0.2095
Training Epoch: 55 [24704/50048]	Loss: 0.1224
Training Epoch: 55 [24832/50048]	Loss: 0.1947
Training Epoch: 55 [24960/50048]	Loss: 0.1641
Training Epoch: 55 [25088/50048]	Loss: 0.1471
Training Epoch: 55 [25216/50048]	Loss: 0.1499
Training Epoch: 55 [25344/50048]	Loss: 0.2487
Training Epoch: 55 [25472/50048]	Loss: 0.2209
Training Epoch: 55 [25600/50048]	Loss: 0.1793
Training Epoch: 55 [25728/50048]	Loss: 0.3122
Training Epoch: 55 [25856/50048]	Loss: 0.2001
Training Epoch: 55 [25984/50048]	Loss: 0.2325
Training Epoch: 55 [26112/50048]	Loss: 0.2836
Training Epoch: 55 [26240/50048]	Loss: 0.1638
Training Epoch: 55 [26368/50048]	Loss: 0.2421
Training Epoch: 55 [26496/50048]	Loss: 0.1751
Training Epoch: 55 [26624/50048]	Loss: 0.3642
Training Epoch: 55 [26752/50048]	Loss: 0.1110
Training Epoch: 55 [26880/50048]	Loss: 0.1940
Training Epoch: 55 [27008/50048]	Loss: 0.2817
Training Epoch: 55 [27136/50048]	Loss: 0.2048
Training Epoch: 55 [27264/50048]	Loss: 0.2540
Training Epoch: 55 [27392/50048]	Loss: 0.2631
Training Epoch: 55 [27520/50048]	Loss: 0.2187
Training Epoch: 55 [27648/50048]	Loss: 0.1160
Training Epoch: 55 [27776/50048]	Loss: 0.1596
Training Epoch: 55 [27904/50048]	Loss: 0.1906
Training Epoch: 55 [28032/50048]	Loss: 0.2127
Training Epoch: 55 [28160/50048]	Loss: 0.2212
Training Epoch: 55 [28288/50048]	Loss: 0.2329
Training Epoch: 55 [28416/50048]	Loss: 0.1692
Training Epoch: 55 [28544/50048]	Loss: 0.1254
Training Epoch: 55 [28672/50048]	Loss: 0.1979
Training Epoch: 55 [28800/50048]	Loss: 0.1497
Training Epoch: 55 [28928/50048]	Loss: 0.2019
Training Epoch: 55 [29056/50048]	Loss: 0.2630
Training Epoch: 55 [29184/50048]	Loss: 0.2672
Training Epoch: 55 [29312/50048]	Loss: 0.3359
Training Epoch: 55 [29440/50048]	Loss: 0.3346
Training Epoch: 55 [29568/50048]	Loss: 0.2823
Training Epoch: 55 [29696/50048]	Loss: 0.2786
Training Epoch: 55 [29824/50048]	Loss: 0.2692
Training Epoch: 55 [29952/50048]	Loss: 0.2825
Training Epoch: 55 [30080/50048]	Loss: 0.2668
Training Epoch: 55 [30208/50048]	Loss: 0.1382
Training Epoch: 55 [30336/50048]	Loss: 0.3038
Training Epoch: 55 [30464/50048]	Loss: 0.3009
Training Epoch: 55 [30592/50048]	Loss: 0.2140
Training Epoch: 55 [30720/50048]	Loss: 0.3067
Training Epoch: 55 [30848/50048]	Loss: 0.2141
Training Epoch: 55 [30976/50048]	Loss: 0.1951
Training Epoch: 55 [31104/50048]	Loss: 0.3009
Training Epoch: 55 [31232/50048]	Loss: 0.2858
Training Epoch: 55 [31360/50048]	Loss: 0.1771
Training Epoch: 55 [31488/50048]	Loss: 0.3032
Training Epoch: 55 [31616/50048]	Loss: 0.2975
Training Epoch: 55 [31744/50048]	Loss: 0.2937
Training Epoch: 55 [31872/50048]	Loss: 0.2353
Training Epoch: 55 [32000/50048]	Loss: 0.2834
Training Epoch: 55 [32128/50048]	Loss: 0.1876
Training Epoch: 55 [32256/50048]	Loss: 0.2155
Training Epoch: 55 [32384/50048]	Loss: 0.1923
Training Epoch: 55 [32512/50048]	Loss: 0.1578
Training Epoch: 55 [32640/50048]	Loss: 0.3078
Training Epoch: 55 [32768/50048]	Loss: 0.2633
Training Epoch: 55 [32896/50048]	Loss: 0.3469
Training Epoch: 55 [33024/50048]	Loss: 0.2223
Training Epoch: 55 [33152/50048]	Loss: 0.1797
Training Epoch: 55 [33280/50048]	Loss: 0.2105
Training Epoch: 55 [33408/50048]	Loss: 0.1938
Training Epoch: 55 [33536/50048]	Loss: 0.1231
Training Epoch: 55 [33664/50048]	Loss: 0.1708
Training Epoch: 55 [33792/50048]	Loss: 0.2445
Training Epoch: 55 [33920/50048]	Loss: 0.2029
Training Epoch: 55 [34048/50048]	Loss: 0.2025
Training Epoch: 55 [34176/50048]	Loss: 0.2188
Training Epoch: 55 [34304/50048]	Loss: 0.1690
Training Epoch: 55 [34432/50048]	Loss: 0.2072
Training Epoch: 55 [34560/50048]	Loss: 0.1792
Training Epoch: 55 [34688/50048]	Loss: 0.2206
Training Epoch: 55 [34816/50048]	Loss: 0.2058
Training Epoch: 55 [34944/50048]	Loss: 0.1531
Training Epoch: 55 [35072/50048]	Loss: 0.2183
Training Epoch: 55 [35200/50048]	Loss: 0.2390
Training Epoch: 55 [35328/50048]	Loss: 0.3743
Training Epoch: 55 [35456/50048]	Loss: 0.2881
Training Epoch: 55 [35584/50048]	Loss: 0.2751
Training Epoch: 55 [35712/50048]	Loss: 0.3300
Training Epoch: 55 [35840/50048]	Loss: 0.2599
Training Epoch: 55 [35968/50048]	Loss: 0.3129
Training Epoch: 55 [36096/50048]	Loss: 0.2157
Training Epoch: 55 [36224/50048]	Loss: 0.2767
Training Epoch: 55 [36352/50048]	Loss: 0.1677
Training Epoch: 55 [36480/50048]	Loss: 0.3163
Training Epoch: 55 [36608/50048]	Loss: 0.2520
Training Epoch: 55 [36736/50048]	Loss: 0.2757
Training Epoch: 55 [36864/50048]	Loss: 0.3825
Training Epoch: 55 [36992/50048]	Loss: 0.3089
Training Epoch: 55 [37120/50048]	Loss: 0.3500
Training Epoch: 55 [37248/50048]	Loss: 0.2788
Training Epoch: 55 [37376/50048]	Loss: 0.1969
Training Epoch: 55 [37504/50048]	Loss: 0.2055
Training Epoch: 55 [37632/50048]	Loss: 0.2979
Training Epoch: 55 [37760/50048]	Loss: 0.1595
Training Epoch: 55 [37888/50048]	Loss: 0.2395
Training Epoch: 55 [38016/50048]	Loss: 0.3241
Training Epoch: 55 [38144/50048]	Loss: 0.2295
Training Epoch: 55 [38272/50048]	Loss: 0.2579
Training Epoch: 55 [38400/50048]	Loss: 0.2461
Training Epoch: 55 [38528/50048]	Loss: 0.1394
Training Epoch: 55 [38656/50048]	Loss: 0.2787
Training Epoch: 55 [38784/50048]	Loss: 0.1745
Training Epoch: 55 [38912/50048]	Loss: 0.2395
Training Epoch: 55 [39040/50048]	Loss: 0.2943
Training Epoch: 55 [39168/50048]	Loss: 0.1836
Training Epoch: 55 [39296/50048]	Loss: 0.2377
Training Epoch: 55 [39424/50048]	Loss: 0.2842
Training Epoch: 55 [39552/50048]	Loss: 0.1415
Training Epoch: 55 [39680/50048]	Loss: 0.2170
Training Epoch: 55 [39808/50048]	Loss: 0.2809
Training Epoch: 55 [39936/50048]	Loss: 0.1649
Training Epoch: 55 [40064/50048]	Loss: 0.2856
Training Epoch: 55 [40192/50048]	Loss: 0.2897
Training Epoch: 55 [40320/50048]	Loss: 0.1722
Training Epoch: 55 [40448/50048]	Loss: 0.3097
Training Epoch: 55 [40576/50048]	Loss: 0.1965
Training Epoch: 55 [40704/50048]	Loss: 0.2313
Training Epoch: 55 [40832/50048]	Loss: 0.2061
Training Epoch: 55 [40960/50048]	Loss: 0.2231
Training Epoch: 55 [41088/50048]	Loss: 0.2498
Training Epoch: 55 [41216/50048]	Loss: 0.3054
Training Epoch: 55 [41344/50048]	Loss: 0.1675
Training Epoch: 55 [41472/50048]	Loss: 0.2663
Training Epoch: 55 [41600/50048]	Loss: 0.2024
Training Epoch: 55 [41728/50048]	Loss: 0.2549
Training Epoch: 55 [41856/50048]	Loss: 0.2403
Training Epoch: 55 [41984/50048]	Loss: 0.2190
Training Epoch: 55 [42112/50048]	Loss: 0.3268
Training Epoch: 55 [42240/50048]	Loss: 0.1977
Training Epoch: 55 [42368/50048]	Loss: 0.1749
Training Epoch: 55 [42496/50048]	Loss: 0.3666
Training Epoch: 55 [42624/50048]	Loss: 0.2223
Training Epoch: 55 [42752/50048]	Loss: 0.1929
Training Epoch: 55 [42880/50048]	Loss: 0.2172
Training Epoch: 55 [43008/50048]	Loss: 0.3633
Training Epoch: 55 [43136/50048]	Loss: 0.3078
Training Epoch: 55 [43264/50048]	Loss: 0.3464
Training Epoch: 55 [43392/50048]	Loss: 0.3583
Training Epoch: 55 [43520/50048]	Loss: 0.3257
Training Epoch: 55 [43648/50048]	Loss: 0.2486
Training Epoch: 55 [43776/50048]	Loss: 0.2036
Training Epoch: 55 [43904/50048]	Loss: 0.2756
Training Epoch: 55 [44032/50048]	Loss: 0.1503
Training Epoch: 55 [44160/50048]	Loss: 0.2132
Training Epoch: 55 [44288/50048]	Loss: 0.1837
Training Epoch: 55 [44416/50048]	Loss: 0.2579
Training Epoch: 55 [44544/50048]	Loss: 0.2771
Training Epoch: 55 [44672/50048]	Loss: 0.5060
Training Epoch: 55 [44800/50048]	Loss: 0.2160
Training Epoch: 55 [44928/50048]	Loss: 0.2732
Training Epoch: 55 [45056/50048]	Loss: 0.3234
Training Epoch: 55 [45184/50048]	Loss: 0.2257
Training Epoch: 55 [45312/50048]	Loss: 0.2210
Training Epoch: 55 [45440/50048]	Loss: 0.2209
Training Epoch: 55 [45568/50048]	Loss: 0.2152
Training Epoch: 55 [45696/50048]	Loss: 0.1711
2022-12-06 05:01:32,769 [ZeusDataLoader(train)] train epoch 56 done: time=86.49 energy=10503.94
2022-12-06 05:01:32,770 [ZeusDataLoader(eval)] Epoch 56 begin.
Training Epoch: 55 [45824/50048]	Loss: 0.2466
Training Epoch: 55 [45952/50048]	Loss: 0.2667
Training Epoch: 55 [46080/50048]	Loss: 0.1942
Training Epoch: 55 [46208/50048]	Loss: 0.1618
Training Epoch: 55 [46336/50048]	Loss: 0.2348
Training Epoch: 55 [46464/50048]	Loss: 0.2323
Training Epoch: 55 [46592/50048]	Loss: 0.2526
Training Epoch: 55 [46720/50048]	Loss: 0.1440
Training Epoch: 55 [46848/50048]	Loss: 0.3852
Training Epoch: 55 [46976/50048]	Loss: 0.1961
Training Epoch: 55 [47104/50048]	Loss: 0.2100
Training Epoch: 55 [47232/50048]	Loss: 0.1726
Training Epoch: 55 [47360/50048]	Loss: 0.2421
Training Epoch: 55 [47488/50048]	Loss: 0.2934
Training Epoch: 55 [47616/50048]	Loss: 0.2884
Training Epoch: 55 [47744/50048]	Loss: 0.2872
Training Epoch: 55 [47872/50048]	Loss: 0.2157
Training Epoch: 55 [48000/50048]	Loss: 0.1675
Training Epoch: 55 [48128/50048]	Loss: 0.3328
Training Epoch: 55 [48256/50048]	Loss: 0.2184
Training Epoch: 55 [48384/50048]	Loss: 0.2324
Training Epoch: 55 [48512/50048]	Loss: 0.2129
Training Epoch: 55 [48640/50048]	Loss: 0.2164
Training Epoch: 55 [48768/50048]	Loss: 0.3090
Training Epoch: 55 [48896/50048]	Loss: 0.2561
Training Epoch: 55 [49024/50048]	Loss: 0.1612
Training Epoch: 55 [49152/50048]	Loss: 0.2884
Training Epoch: 55 [49280/50048]	Loss: 0.2167
Training Epoch: 55 [49408/50048]	Loss: 0.2460
Training Epoch: 55 [49536/50048]	Loss: 0.2509
Training Epoch: 55 [49664/50048]	Loss: 0.2543
Training Epoch: 55 [49792/50048]	Loss: 0.2595
Training Epoch: 55 [49920/50048]	Loss: 0.2981
Training Epoch: 55 [50048/50048]	Loss: 0.2075
2022-12-06 10:01:36.480 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:01:36,515 [ZeusDataLoader(eval)] eval epoch 56 done: time=3.74 energy=454.23
2022-12-06 05:01:36,515 [ZeusDataLoader(train)] Up to epoch 56: time=5053.07, energy=613368.60, cost=748827.72
2022-12-06 05:01:36,516 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:01:36,516 [ZeusDataLoader(train)] Expected next epoch: time=5142.87, energy=624166.61, cost=762084.10
2022-12-06 05:01:36,517 [ZeusDataLoader(train)] Epoch 57 begin.
Validation Epoch: 55, Average loss: 0.0157, Accuracy: 0.6352
2022-12-06 05:01:36,695 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:01:36,696 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:01:36.704 [ZeusMonitor] Monitor started.
2022-12-06 10:01:36.704 [ZeusMonitor] Running indefinitely. 2022-12-06 10:01:36.704 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:01:36.704 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e57+gpu0.power.log
Training Epoch: 56 [128/50048]	Loss: 0.2979
Training Epoch: 56 [256/50048]	Loss: 0.2419
Training Epoch: 56 [384/50048]	Loss: 0.1956
Training Epoch: 56 [512/50048]	Loss: 0.1455
Training Epoch: 56 [640/50048]	Loss: 0.2261
Training Epoch: 56 [768/50048]	Loss: 0.1958
Training Epoch: 56 [896/50048]	Loss: 0.2782
Training Epoch: 56 [1024/50048]	Loss: 0.2043
Training Epoch: 56 [1152/50048]	Loss: 0.2935
Training Epoch: 56 [1280/50048]	Loss: 0.3096
Training Epoch: 56 [1408/50048]	Loss: 0.1522
Training Epoch: 56 [1536/50048]	Loss: 0.2151
Training Epoch: 56 [1664/50048]	Loss: 0.2384
Training Epoch: 56 [1792/50048]	Loss: 0.2329
Training Epoch: 56 [1920/50048]	Loss: 0.1992
Training Epoch: 56 [2048/50048]	Loss: 0.2402
Training Epoch: 56 [2176/50048]	Loss: 0.1746
Training Epoch: 56 [2304/50048]	Loss: 0.2838
Training Epoch: 56 [2432/50048]	Loss: 0.2099
Training Epoch: 56 [2560/50048]	Loss: 0.1481
Training Epoch: 56 [2688/50048]	Loss: 0.1983
Training Epoch: 56 [2816/50048]	Loss: 0.1559
Training Epoch: 56 [2944/50048]	Loss: 0.1847
Training Epoch: 56 [3072/50048]	Loss: 0.1786
Training Epoch: 56 [3200/50048]	Loss: 0.1717
Training Epoch: 56 [3328/50048]	Loss: 0.1570
Training Epoch: 56 [3456/50048]	Loss: 0.2041
Training Epoch: 56 [3584/50048]	Loss: 0.2236
Training Epoch: 56 [3712/50048]	Loss: 0.1807
Training Epoch: 56 [3840/50048]	Loss: 0.1937
Training Epoch: 56 [3968/50048]	Loss: 0.1438
Training Epoch: 56 [4096/50048]	Loss: 0.2464
Training Epoch: 56 [4224/50048]	Loss: 0.2053
Training Epoch: 56 [4352/50048]	Loss: 0.1997
Training Epoch: 56 [4480/50048]	Loss: 0.2270
Training Epoch: 56 [4608/50048]	Loss: 0.2371
Training Epoch: 56 [4736/50048]	Loss: 0.1464
Training Epoch: 56 [4864/50048]	Loss: 0.2241
Training Epoch: 56 [4992/50048]	Loss: 0.1922
Training Epoch: 56 [5120/50048]	Loss: 0.2492
Training Epoch: 56 [5248/50048]	Loss: 0.2158
Training Epoch: 56 [5376/50048]	Loss: 0.1266
Training Epoch: 56 [5504/50048]	Loss: 0.2031
Training Epoch: 56 [5632/50048]	Loss: 0.2479
Training Epoch: 56 [5760/50048]	Loss: 0.1336
Training Epoch: 56 [5888/50048]	Loss: 0.1903
Training Epoch: 56 [6016/50048]	Loss: 0.1564
Training Epoch: 56 [6144/50048]	Loss: 0.1356
Training Epoch: 56 [6272/50048]	Loss: 0.1603
Training Epoch: 56 [6400/50048]	Loss: 0.1614
Training Epoch: 56 [6528/50048]	Loss: 0.2149
Training Epoch: 56 [6656/50048]	Loss: 0.2143
Training Epoch: 56 [6784/50048]	Loss: 0.2172
Training Epoch: 56 [6912/50048]	Loss: 0.1373
Training Epoch: 56 [7040/50048]	Loss: 0.2130
Training Epoch: 56 [7168/50048]	Loss: 0.2076
Training Epoch: 56 [7296/50048]	Loss: 0.1251
Training Epoch: 56 [7424/50048]	Loss: 0.1247
Training Epoch: 56 [7552/50048]	Loss: 0.2351
Training Epoch: 56 [7680/50048]	Loss: 0.2233
Training Epoch: 56 [7808/50048]	Loss: 0.0952
Training Epoch: 56 [7936/50048]	Loss: 0.2638
Training Epoch: 56 [8064/50048]	Loss: 0.1524
Training Epoch: 56 [8192/50048]	Loss: 0.2654
Training Epoch: 56 [8320/50048]	Loss: 0.1609
Training Epoch: 56 [8448/50048]	Loss: 0.2462
Training Epoch: 56 [8576/50048]	Loss: 0.2655
Training Epoch: 56 [8704/50048]	Loss: 0.1796
Training Epoch: 56 [8832/50048]	Loss: 0.2167
Training Epoch: 56 [8960/50048]	Loss: 0.2581
Training Epoch: 56 [9088/50048]	Loss: 0.2073
Training Epoch: 56 [9216/50048]	Loss: 0.2748
Training Epoch: 56 [9344/50048]	Loss: 0.1504
Training Epoch: 56 [9472/50048]	Loss: 0.1604
Training Epoch: 56 [9600/50048]	Loss: 0.1176
Training Epoch: 56 [9728/50048]	Loss: 0.1536
Training Epoch: 56 [9856/50048]	Loss: 0.1985
Training Epoch: 56 [9984/50048]	Loss: 0.1962
Training Epoch: 56 [10112/50048]	Loss: 0.2419
Training Epoch: 56 [10240/50048]	Loss: 0.2037
Training Epoch: 56 [10368/50048]	Loss: 0.1217
Training Epoch: 56 [10496/50048]	Loss: 0.1727
Training Epoch: 56 [10624/50048]	Loss: 0.1862
Training Epoch: 56 [10752/50048]	Loss: 0.2119
Training Epoch: 56 [10880/50048]	Loss: 0.2372
Training Epoch: 56 [11008/50048]	Loss: 0.1743
Training Epoch: 56 [11136/50048]	Loss: 0.2796
Training Epoch: 56 [11264/50048]	Loss: 0.3791
Training Epoch: 56 [11392/50048]	Loss: 0.2873
Training Epoch: 56 [11520/50048]	Loss: 0.1735
Training Epoch: 56 [11648/50048]	Loss: 0.2046
Training Epoch: 56 [11776/50048]	Loss: 0.2935
Training Epoch: 56 [11904/50048]	Loss: 0.2649
Training Epoch: 56 [12032/50048]	Loss: 0.2427
Training Epoch: 56 [12160/50048]	Loss: 0.2073
Training Epoch: 56 [12288/50048]	Loss: 0.1238
Training Epoch: 56 [12416/50048]	Loss: 0.2792
Training Epoch: 56 [12544/50048]	Loss: 0.2925
Training Epoch: 56 [12672/50048]	Loss: 0.2963
Training Epoch: 56 [12800/50048]	Loss: 0.1261
Training Epoch: 56 [12928/50048]	Loss: 0.2506
Training Epoch: 56 [13056/50048]	Loss: 0.2004
Training Epoch: 56 [13184/50048]	Loss: 0.3110
Training Epoch: 56 [13312/50048]	Loss: 0.2559
Training Epoch: 56 [13440/50048]	Loss: 0.1465
Training Epoch: 56 [13568/50048]	Loss: 0.1024
Training Epoch: 56 [13696/50048]	Loss: 0.1313
Training Epoch: 56 [13824/50048]	Loss: 0.1820
Training Epoch: 56 [13952/50048]	Loss: 0.1772
Training Epoch: 56 [14080/50048]	Loss: 0.2106
Training Epoch: 56 [14208/50048]	Loss: 0.1934
Training Epoch: 56 [14336/50048]	Loss: 0.2554
Training Epoch: 56 [14464/50048]	Loss: 0.1993
Training Epoch: 56 [14592/50048]	Loss: 0.3090
Training Epoch: 56 [14720/50048]	Loss: 0.3506
Training Epoch: 56 [14848/50048]	Loss: 0.2377
Training Epoch: 56 [14976/50048]	Loss: 0.2404
Training Epoch: 56 [15104/50048]	Loss: 0.2018
Training Epoch: 56 [15232/50048]	Loss: 0.1283
Training Epoch: 56 [15360/50048]	Loss: 0.2090
Training Epoch: 56 [15488/50048]	Loss: 0.3031
Training Epoch: 56 [15616/50048]	Loss: 0.1906
Training Epoch: 56 [15744/50048]	Loss: 0.2398
Training Epoch: 56 [15872/50048]	Loss: 0.2453
Training Epoch: 56 [16000/50048]	Loss: 0.2323
Training Epoch: 56 [16128/50048]	Loss: 0.2090
Training Epoch: 56 [16256/50048]	Loss: 0.2106
Training Epoch: 56 [16384/50048]	Loss: 0.2632
Training Epoch: 56 [16512/50048]	Loss: 0.1710
Training Epoch: 56 [16640/50048]	Loss: 0.1625
Training Epoch: 56 [16768/50048]	Loss: 0.1998
Training Epoch: 56 [16896/50048]	Loss: 0.1773
Training Epoch: 56 [17024/50048]	Loss: 0.2337
Training Epoch: 56 [17152/50048]	Loss: 0.1582
Training Epoch: 56 [17280/50048]	Loss: 0.2986
Training Epoch: 56 [17408/50048]	Loss: 0.2742
Training Epoch: 56 [17536/50048]	Loss: 0.1696
Training Epoch: 56 [17664/50048]	Loss: 0.1857
Training Epoch: 56 [17792/50048]	Loss: 0.3074
Training Epoch: 56 [17920/50048]	Loss: 0.1844
Training Epoch: 56 [18048/50048]	Loss: 0.2643
Training Epoch: 56 [18176/50048]	Loss: 0.3964
Training Epoch: 56 [18304/50048]	Loss: 0.2337
Training Epoch: 56 [18432/50048]	Loss: 0.2854
Training Epoch: 56 [18560/50048]	Loss: 0.2033
Training Epoch: 56 [18688/50048]	Loss: 0.1572
Training Epoch: 56 [18816/50048]	Loss: 0.1192
Training Epoch: 56 [18944/50048]	Loss: 0.2039
Training Epoch: 56 [19072/50048]	Loss: 0.2954
Training Epoch: 56 [19200/50048]	Loss: 0.2261
Training Epoch: 56 [19328/50048]	Loss: 0.1991
Training Epoch: 56 [19456/50048]	Loss: 0.2374
Training Epoch: 56 [19584/50048]	Loss: 0.1880
Training Epoch: 56 [19712/50048]	Loss: 0.1397
Training Epoch: 56 [19840/50048]	Loss: 0.2129
Training Epoch: 56 [19968/50048]	Loss: 0.1642
Training Epoch: 56 [20096/50048]	Loss: 0.3097
Training Epoch: 56 [20224/50048]	Loss: 0.2298
Training Epoch: 56 [20352/50048]	Loss: 0.2257
Training Epoch: 56 [20480/50048]	Loss: 0.1243
Training Epoch: 56 [20608/50048]	Loss: 0.1928
Training Epoch: 56 [20736/50048]	Loss: 0.1688
Training Epoch: 56 [20864/50048]	Loss: 0.1063
Training Epoch: 56 [20992/50048]	Loss: 0.2583
Training Epoch: 56 [21120/50048]	Loss: 0.2187
Training Epoch: 56 [21248/50048]	Loss: 0.2548
Training Epoch: 56 [21376/50048]	Loss: 0.2064
Training Epoch: 56 [21504/50048]	Loss: 0.2059
Training Epoch: 56 [21632/50048]	Loss: 0.2668
Training Epoch: 56 [21760/50048]	Loss: 0.2445
Training Epoch: 56 [21888/50048]	Loss: 0.2134
Training Epoch: 56 [22016/50048]	Loss: 0.2329
Training Epoch: 56 [22144/50048]	Loss: 0.2106
Training Epoch: 56 [22272/50048]	Loss: 0.2326
Training Epoch: 56 [22400/50048]	Loss: 0.1438
Training Epoch: 56 [22528/50048]	Loss: 0.3137
Training Epoch: 56 [22656/50048]	Loss: 0.2669
Training Epoch: 56 [22784/50048]	Loss: 0.2892
Training Epoch: 56 [22912/50048]	Loss: 0.3075
Training Epoch: 56 [23040/50048]	Loss: 0.2284
Training Epoch: 56 [23168/50048]	Loss: 0.1668
Training Epoch: 56 [23296/50048]	Loss: 0.2392
Training Epoch: 56 [23424/50048]	Loss: 0.1399
Training Epoch: 56 [23552/50048]	Loss: 0.1590
Training Epoch: 56 [23680/50048]	Loss: 0.3212
Training Epoch: 56 [23808/50048]	Loss: 0.3378
Training Epoch: 56 [23936/50048]	Loss: 0.2484
Training Epoch: 56 [24064/50048]	Loss: 0.1374
Training Epoch: 56 [24192/50048]	Loss: 0.2244
Training Epoch: 56 [24320/50048]	Loss: 0.2532
Training Epoch: 56 [24448/50048]	Loss: 0.2283
Training Epoch: 56 [24576/50048]	Loss: 0.2401
Training Epoch: 56 [24704/50048]	Loss: 0.3289
Training Epoch: 56 [24832/50048]	Loss: 0.1935
Training Epoch: 56 [24960/50048]	Loss: 0.2557
Training Epoch: 56 [25088/50048]	Loss: 0.2961
Training Epoch: 56 [25216/50048]	Loss: 0.1851
Training Epoch: 56 [25344/50048]	Loss: 0.1497
Training Epoch: 56 [25472/50048]	Loss: 0.3794
Training Epoch: 56 [25600/50048]	Loss: 0.2730
Training Epoch: 56 [25728/50048]	Loss: 0.1831
Training Epoch: 56 [25856/50048]	Loss: 0.2068
Training Epoch: 56 [25984/50048]	Loss: 0.3402
Training Epoch: 56 [26112/50048]	Loss: 0.2470
Training Epoch: 56 [26240/50048]	Loss: 0.2223
Training Epoch: 56 [26368/50048]	Loss: 0.3001
Training Epoch: 56 [26496/50048]	Loss: 0.1825
Training Epoch: 56 [26624/50048]	Loss: 0.2289
Training Epoch: 56 [26752/50048]	Loss: 0.2378
Training Epoch: 56 [26880/50048]	Loss: 0.2400
Training Epoch: 56 [27008/50048]	Loss: 0.1295
Training Epoch: 56 [27136/50048]	Loss: 0.2500
Training Epoch: 56 [27264/50048]	Loss: 0.3067
Training Epoch: 56 [27392/50048]	Loss: 0.3075
Training Epoch: 56 [27520/50048]	Loss: 0.2772
Training Epoch: 56 [27648/50048]	Loss: 0.1920
Training Epoch: 56 [27776/50048]	Loss: 0.2279
Training Epoch: 56 [27904/50048]	Loss: 0.1550
Training Epoch: 56 [28032/50048]	Loss: 0.2513
Training Epoch: 56 [28160/50048]	Loss: 0.1370
Training Epoch: 56 [28288/50048]	Loss: 0.2609
Training Epoch: 56 [28416/50048]	Loss: 0.2024
Training Epoch: 56 [28544/50048]	Loss: 0.1810
Training Epoch: 56 [28672/50048]	Loss: 0.2571
Training Epoch: 56 [28800/50048]	Loss: 0.3602
Training Epoch: 56 [28928/50048]	Loss: 0.1608
Training Epoch: 56 [29056/50048]	Loss: 0.1954
Training Epoch: 56 [29184/50048]	Loss: 0.1675
Training Epoch: 56 [29312/50048]	Loss: 0.1200
Training Epoch: 56 [29440/50048]	Loss: 0.1639
Training Epoch: 56 [29568/50048]	Loss: 0.1626
Training Epoch: 56 [29696/50048]	Loss: 0.2213
Training Epoch: 56 [29824/50048]	Loss: 0.2351
Training Epoch: 56 [29952/50048]	Loss: 0.1882
Training Epoch: 56 [30080/50048]	Loss: 0.3534
Training Epoch: 56 [30208/50048]	Loss: 0.3143
Training Epoch: 56 [30336/50048]	Loss: 0.1665
Training Epoch: 56 [30464/50048]	Loss: 0.3299
Training Epoch: 56 [30592/50048]	Loss: 0.2374
Training Epoch: 56 [30720/50048]	Loss: 0.2495
Training Epoch: 56 [30848/50048]	Loss: 0.1696
Training Epoch: 56 [30976/50048]	Loss: 0.2688
Training Epoch: 56 [31104/50048]	Loss: 0.2676
Training Epoch: 56 [31232/50048]	Loss: 0.2620
Training Epoch: 56 [31360/50048]	Loss: 0.2180
Training Epoch: 56 [31488/50048]	Loss: 0.2507
Training Epoch: 56 [31616/50048]	Loss: 0.2454
Training Epoch: 56 [31744/50048]	Loss: 0.2492
Training Epoch: 56 [31872/50048]	Loss: 0.1758
Training Epoch: 56 [32000/50048]	Loss: 0.1739
Training Epoch: 56 [32128/50048]	Loss: 0.1304
Training Epoch: 56 [32256/50048]	Loss: 0.2572
Training Epoch: 56 [32384/50048]	Loss: 0.2925
Training Epoch: 56 [32512/50048]	Loss: 0.2906
Training Epoch: 56 [32640/50048]	Loss: 0.2115
Training Epoch: 56 [32768/50048]	Loss: 0.2599
Training Epoch: 56 [32896/50048]	Loss: 0.1286
Training Epoch: 56 [33024/50048]	Loss: 0.1890
Training Epoch: 56 [33152/50048]	Loss: 0.2574
Training Epoch: 56 [33280/50048]	Loss: 0.1707
Training Epoch: 56 [33408/50048]	Loss: 0.2541
Training Epoch: 56 [33536/50048]	Loss: 0.1318
Training Epoch: 56 [33664/50048]	Loss: 0.1503
Training Epoch: 56 [33792/50048]	Loss: 0.2318
Training Epoch: 56 [33920/50048]	Loss: 0.2159
Training Epoch: 56 [34048/50048]	Loss: 0.3183
Training Epoch: 56 [34176/50048]	Loss: 0.1756
Training Epoch: 56 [34304/50048]	Loss: 0.2801
Training Epoch: 56 [34432/50048]	Loss: 0.1969
Training Epoch: 56 [34560/50048]	Loss: 0.1561
Training Epoch: 56 [34688/50048]	Loss: 0.1285
Training Epoch: 56 [34816/50048]	Loss: 0.1892
Training Epoch: 56 [34944/50048]	Loss: 0.2254
Training Epoch: 56 [35072/50048]	Loss: 0.2372
Training Epoch: 56 [35200/50048]	Loss: 0.2157
Training Epoch: 56 [35328/50048]	Loss: 0.2302
Training Epoch: 56 [35456/50048]	Loss: 0.4454
Training Epoch: 56 [35584/50048]	Loss: 0.1817
Training Epoch: 56 [35712/50048]	Loss: 0.2627
Training Epoch: 56 [35840/50048]	Loss: 0.1956
Training Epoch: 56 [35968/50048]	Loss: 0.2012
Training Epoch: 56 [36096/50048]	Loss: 0.2341
Training Epoch: 56 [36224/50048]	Loss: 0.2981
Training Epoch: 56 [36352/50048]	Loss: 0.1579
Training Epoch: 56 [36480/50048]	Loss: 0.2892
Training Epoch: 56 [36608/50048]	Loss: 0.2509
Training Epoch: 56 [36736/50048]	Loss: 0.2170
Training Epoch: 56 [36864/50048]	Loss: 0.1300
Training Epoch: 56 [36992/50048]	Loss: 0.1462
Training Epoch: 56 [37120/50048]	Loss: 0.2382
Training Epoch: 56 [37248/50048]	Loss: 0.1436
Training Epoch: 56 [37376/50048]	Loss: 0.1990
Training Epoch: 56 [37504/50048]	Loss: 0.2006
Training Epoch: 56 [37632/50048]	Loss: 0.2115
Training Epoch: 56 [37760/50048]	Loss: 0.2067
Training Epoch: 56 [37888/50048]	Loss: 0.2211
Training Epoch: 56 [38016/50048]	Loss: 0.2054
Training Epoch: 56 [38144/50048]	Loss: 0.2399
Training Epoch: 56 [38272/50048]	Loss: 0.3582
Training Epoch: 56 [38400/50048]	Loss: 0.2555
Training Epoch: 56 [38528/50048]	Loss: 0.1951
Training Epoch: 56 [38656/50048]	Loss: 0.2942
Training Epoch: 56 [38784/50048]	Loss: 0.3226
Training Epoch: 56 [38912/50048]	Loss: 0.3521
Training Epoch: 56 [39040/50048]	Loss: 0.1956
Training Epoch: 56 [39168/50048]	Loss: 0.2932
Training Epoch: 56 [39296/50048]	Loss: 0.2033
Training Epoch: 56 [39424/50048]	Loss: 0.1891
Training Epoch: 56 [39552/50048]	Loss: 0.2682
Training Epoch: 56 [39680/50048]	Loss: 0.1948
Training Epoch: 56 [39808/50048]	Loss: 0.2236
Training Epoch: 56 [39936/50048]	Loss: 0.1413
Training Epoch: 56 [40064/50048]	Loss: 0.2138
Training Epoch: 56 [40192/50048]	Loss: 0.1979
Training Epoch: 56 [40320/50048]	Loss: 0.2178
Training Epoch: 56 [40448/50048]	Loss: 0.2466
Training Epoch: 56 [40576/50048]	Loss: 0.2488
Training Epoch: 56 [40704/50048]	Loss: 0.2591
Training Epoch: 56 [40832/50048]	Loss: 0.2739
Training Epoch: 56 [40960/50048]	Loss: 0.1962
Training Epoch: 56 [41088/50048]	Loss: 0.2382
Training Epoch: 56 [41216/50048]	Loss: 0.3692
Training Epoch: 56 [41344/50048]	Loss: 0.3530
Training Epoch: 56 [41472/50048]	Loss: 0.2546
Training Epoch: 56 [41600/50048]	Loss: 0.3259
Training Epoch: 56 [41728/50048]	Loss: 0.4770
Training Epoch: 56 [41856/50048]	Loss: 0.1592
Training Epoch: 56 [41984/50048]	Loss: 0.2443
Training Epoch: 56 [42112/50048]	Loss: 0.4083
Training Epoch: 56 [42240/50048]	Loss: 0.2318
Training Epoch: 56 [42368/50048]	Loss: 0.3006
Training Epoch: 56 [42496/50048]	Loss: 0.2381
Training Epoch: 56 [42624/50048]	Loss: 0.2224
Training Epoch: 56 [42752/50048]	Loss: 0.1185
Training Epoch: 56 [42880/50048]	Loss: 0.2298
Training Epoch: 56 [43008/50048]	Loss: 0.2639
Training Epoch: 56 [43136/50048]	Loss: 0.2256
Training Epoch: 56 [43264/50048]	Loss: 0.2621
Training Epoch: 56 [43392/50048]	Loss: 0.1326
Training Epoch: 56 [43520/50048]	Loss: 0.2202
Training Epoch: 56 [43648/50048]	Loss: 0.2769
Training Epoch: 56 [43776/50048]	Loss: 0.3800
Training Epoch: 56 [43904/50048]	Loss: 0.3020
Training Epoch: 56 [44032/50048]	Loss: 0.2736
Training Epoch: 56 [44160/50048]	Loss: 0.1630
Training Epoch: 56 [44288/50048]	Loss: 0.2720
Training Epoch: 56 [44416/50048]	Loss: 0.2564
Training Epoch: 56 [44544/50048]	Loss: 0.2481
Training Epoch: 56 [44672/50048]	Loss: 0.2913
Training Epoch: 56 [44800/50048]	Loss: 0.3293
Training Epoch: 56 [44928/50048]	Loss: 0.1935
Training Epoch: 56 [45056/50048]	Loss: 0.2129
Training Epoch: 56 [45184/50048]	Loss: 0.1726
Training Epoch: 56 [45312/50048]	Loss: 0.3421
Training Epoch: 56 [45440/50048]	Loss: 0.3051
Training Epoch: 56 [45568/50048]	Loss: 0.2431
Training Epoch: 56 [45696/50048]	Loss: 0.2469
2022-12-06 05:03:02,961 [ZeusDataLoader(train)] train epoch 57 done: time=86.43 energy=10488.02
2022-12-06 05:03:02,963 [ZeusDataLoader(eval)] Epoch 57 begin.
Training Epoch: 56 [45824/50048]	Loss: 0.2847
Training Epoch: 56 [45952/50048]	Loss: 0.2988
Training Epoch: 56 [46080/50048]	Loss: 0.2392
Training Epoch: 56 [46208/50048]	Loss: 0.3589
Training Epoch: 56 [46336/50048]	Loss: 0.1536
Training Epoch: 56 [46464/50048]	Loss: 0.2212
Training Epoch: 56 [46592/50048]	Loss: 0.2208
Training Epoch: 56 [46720/50048]	Loss: 0.2523
Training Epoch: 56 [46848/50048]	Loss: 0.2556
Training Epoch: 56 [46976/50048]	Loss: 0.1553
Training Epoch: 56 [47104/50048]	Loss: 0.2193
Training Epoch: 56 [47232/50048]	Loss: 0.2540
Training Epoch: 56 [47360/50048]	Loss: 0.1903
Training Epoch: 56 [47488/50048]	Loss: 0.2232
Training Epoch: 56 [47616/50048]	Loss: 0.3066
Training Epoch: 56 [47744/50048]	Loss: 0.2947
Training Epoch: 56 [47872/50048]	Loss: 0.2168
Training Epoch: 56 [48000/50048]	Loss: 0.2398
Training Epoch: 56 [48128/50048]	Loss: 0.2390
Training Epoch: 56 [48256/50048]	Loss: 0.3024
Training Epoch: 56 [48384/50048]	Loss: 0.2567
Training Epoch: 56 [48512/50048]	Loss: 0.2085
Training Epoch: 56 [48640/50048]	Loss: 0.2958
Training Epoch: 56 [48768/50048]	Loss: 0.1832
Training Epoch: 56 [48896/50048]	Loss: 0.1898
Training Epoch: 56 [49024/50048]	Loss: 0.2344
Training Epoch: 56 [49152/50048]	Loss: 0.1842
Training Epoch: 56 [49280/50048]	Loss: 0.2062
Training Epoch: 56 [49408/50048]	Loss: 0.2096
Training Epoch: 56 [49536/50048]	Loss: 0.3650
Training Epoch: 56 [49664/50048]	Loss: 0.2338
Training Epoch: 56 [49792/50048]	Loss: 0.1238
Training Epoch: 56 [49920/50048]	Loss: 0.2403
Training Epoch: 56 [50048/50048]	Loss: 0.2807
2022-12-06 10:03:06.682 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:03:06,740 [ZeusDataLoader(eval)] eval epoch 57 done: time=3.77 energy=452.96
2022-12-06 05:03:06,740 [ZeusDataLoader(train)] Up to epoch 57: time=5143.27, energy=624309.57, cost=762190.97
2022-12-06 05:03:06,740 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:03:06,740 [ZeusDataLoader(train)] Expected next epoch: time=5233.07, energy=635107.59, cost=775447.35
2022-12-06 05:03:06,741 [ZeusDataLoader(train)] Epoch 58 begin.
Validation Epoch: 56, Average loss: 0.0156, Accuracy: 0.6377
2022-12-06 05:03:06,928 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:03:06,929 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:03:06.930 [ZeusMonitor] Monitor started.
2022-12-06 10:03:06.931 [ZeusMonitor] Running indefinitely. 2022-12-06 10:03:06.931 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:03:06.931 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e58+gpu0.power.log
Training Epoch: 57 [128/50048]	Loss: 0.2952
Training Epoch: 57 [256/50048]	Loss: 0.1337
Training Epoch: 57 [384/50048]	Loss: 0.1630
Training Epoch: 57 [512/50048]	Loss: 0.2121
Training Epoch: 57 [640/50048]	Loss: 0.2490
Training Epoch: 57 [768/50048]	Loss: 0.1732
Training Epoch: 57 [896/50048]	Loss: 0.2091
Training Epoch: 57 [1024/50048]	Loss: 0.1530
Training Epoch: 57 [1152/50048]	Loss: 0.1972
Training Epoch: 57 [1280/50048]	Loss: 0.2399
Training Epoch: 57 [1408/50048]	Loss: 0.2776
Training Epoch: 57 [1536/50048]	Loss: 0.2099
Training Epoch: 57 [1664/50048]	Loss: 0.1266
Training Epoch: 57 [1792/50048]	Loss: 0.1914
Training Epoch: 57 [1920/50048]	Loss: 0.1732
Training Epoch: 57 [2048/50048]	Loss: 0.1894
Training Epoch: 57 [2176/50048]	Loss: 0.1556
Training Epoch: 57 [2304/50048]	Loss: 0.1726
Training Epoch: 57 [2432/50048]	Loss: 0.1799
Training Epoch: 57 [2560/50048]	Loss: 0.1659
Training Epoch: 57 [2688/50048]	Loss: 0.1382
Training Epoch: 57 [2816/50048]	Loss: 0.2345
Training Epoch: 57 [2944/50048]	Loss: 0.2497
Training Epoch: 57 [3072/50048]	Loss: 0.2093
Training Epoch: 57 [3200/50048]	Loss: 0.2861
Training Epoch: 57 [3328/50048]	Loss: 0.2642
Training Epoch: 57 [3456/50048]	Loss: 0.1745
Training Epoch: 57 [3584/50048]	Loss: 0.1685
Training Epoch: 57 [3712/50048]	Loss: 0.1505
Training Epoch: 57 [3840/50048]	Loss: 0.1626
Training Epoch: 57 [3968/50048]	Loss: 0.1103
Training Epoch: 57 [4096/50048]	Loss: 0.2491
Training Epoch: 57 [4224/50048]	Loss: 0.2438
Training Epoch: 57 [4352/50048]	Loss: 0.2416
Training Epoch: 57 [4480/50048]	Loss: 0.1335
Training Epoch: 57 [4608/50048]	Loss: 0.1385
Training Epoch: 57 [4736/50048]	Loss: 0.2696
Training Epoch: 57 [4864/50048]	Loss: 0.2983
Training Epoch: 57 [4992/50048]	Loss: 0.2342
Training Epoch: 57 [5120/50048]	Loss: 0.2068
Training Epoch: 57 [5248/50048]	Loss: 0.2321
Training Epoch: 57 [5376/50048]	Loss: 0.2902
Training Epoch: 57 [5504/50048]	Loss: 0.1646
Training Epoch: 57 [5632/50048]	Loss: 0.1146
Training Epoch: 57 [5760/50048]	Loss: 0.1791
Training Epoch: 57 [5888/50048]	Loss: 0.1631
Training Epoch: 57 [6016/50048]	Loss: 0.2086
Training Epoch: 57 [6144/50048]	Loss: 0.2504
Training Epoch: 57 [6272/50048]	Loss: 0.2416
Training Epoch: 57 [6400/50048]	Loss: 0.1807
Training Epoch: 57 [6528/50048]	Loss: 0.3698
Training Epoch: 57 [6656/50048]	Loss: 0.1990
Training Epoch: 57 [6784/50048]	Loss: 0.1701
Training Epoch: 57 [6912/50048]	Loss: 0.2116
Training Epoch: 57 [7040/50048]	Loss: 0.1985
Training Epoch: 57 [7168/50048]	Loss: 0.2676
Training Epoch: 57 [7296/50048]	Loss: 0.1510
Training Epoch: 57 [7424/50048]	Loss: 0.2245
Training Epoch: 57 [7552/50048]	Loss: 0.1940
Training Epoch: 57 [7680/50048]	Loss: 0.2177
Training Epoch: 57 [7808/50048]	Loss: 0.1324
Training Epoch: 57 [7936/50048]	Loss: 0.1042
Training Epoch: 57 [8064/50048]	Loss: 0.1421
Training Epoch: 57 [8192/50048]	Loss: 0.1926
Training Epoch: 57 [8320/50048]	Loss: 0.1944
Training Epoch: 57 [8448/50048]	Loss: 0.1121
Training Epoch: 57 [8576/50048]	Loss: 0.2589
Training Epoch: 57 [8704/50048]	Loss: 0.1642
Training Epoch: 57 [8832/50048]	Loss: 0.2181
Training Epoch: 57 [8960/50048]	Loss: 0.1420
Training Epoch: 57 [9088/50048]	Loss: 0.2237
Training Epoch: 57 [9216/50048]	Loss: 0.1890
Training Epoch: 57 [9344/50048]	Loss: 0.1673
Training Epoch: 57 [9472/50048]	Loss: 0.1266
Training Epoch: 57 [9600/50048]	Loss: 0.1425
Training Epoch: 57 [9728/50048]	Loss: 0.2645
Training Epoch: 57 [9856/50048]	Loss: 0.1121
Training Epoch: 57 [9984/50048]	Loss: 0.1400
Training Epoch: 57 [10112/50048]	Loss: 0.1859
Training Epoch: 57 [10240/50048]	Loss: 0.1891
Training Epoch: 57 [10368/50048]	Loss: 0.2077
Training Epoch: 57 [10496/50048]	Loss: 0.1885
Training Epoch: 57 [10624/50048]	Loss: 0.2402
Training Epoch: 57 [10752/50048]	Loss: 0.2491
Training Epoch: 57 [10880/50048]	Loss: 0.1104
Training Epoch: 57 [11008/50048]	Loss: 0.2089
Training Epoch: 57 [11136/50048]	Loss: 0.1560
Training Epoch: 57 [11264/50048]	Loss: 0.1537
Training Epoch: 57 [11392/50048]	Loss: 0.1425
Training Epoch: 57 [11520/50048]	Loss: 0.1366
Training Epoch: 57 [11648/50048]	Loss: 0.1325
Training Epoch: 57 [11776/50048]	Loss: 0.1641
Training Epoch: 57 [11904/50048]	Loss: 0.2443
Training Epoch: 57 [12032/50048]	Loss: 0.2540
Training Epoch: 57 [12160/50048]	Loss: 0.2429
Training Epoch: 57 [12288/50048]	Loss: 0.1257
Training Epoch: 57 [12416/50048]	Loss: 0.1905
Training Epoch: 57 [12544/50048]	Loss: 0.1638
Training Epoch: 57 [12672/50048]	Loss: 0.1073
Training Epoch: 57 [12800/50048]	Loss: 0.0859
Training Epoch: 57 [12928/50048]	Loss: 0.1325
Training Epoch: 57 [13056/50048]	Loss: 0.2875
Training Epoch: 57 [13184/50048]	Loss: 0.1328
Training Epoch: 57 [13312/50048]	Loss: 0.1795
Training Epoch: 57 [13440/50048]	Loss: 0.1946
Training Epoch: 57 [13568/50048]	Loss: 0.2536
Training Epoch: 57 [13696/50048]	Loss: 0.1228
Training Epoch: 57 [13824/50048]	Loss: 0.1773
Training Epoch: 57 [13952/50048]	Loss: 0.1357
Training Epoch: 57 [14080/50048]	Loss: 0.2147
Training Epoch: 57 [14208/50048]	Loss: 0.2614
Training Epoch: 57 [14336/50048]	Loss: 0.2340
Training Epoch: 57 [14464/50048]	Loss: 0.2300
Training Epoch: 57 [14592/50048]	Loss: 0.1453
Training Epoch: 57 [14720/50048]	Loss: 0.2203
Training Epoch: 57 [14848/50048]	Loss: 0.1447
Training Epoch: 57 [14976/50048]	Loss: 0.1249
Training Epoch: 57 [15104/50048]	Loss: 0.1301
Training Epoch: 57 [15232/50048]	Loss: 0.1982
Training Epoch: 57 [15360/50048]	Loss: 0.1871
Training Epoch: 57 [15488/50048]	Loss: 0.2112
Training Epoch: 57 [15616/50048]	Loss: 0.1316
Training Epoch: 57 [15744/50048]	Loss: 0.2122
Training Epoch: 57 [15872/50048]	Loss: 0.2907
Training Epoch: 57 [16000/50048]	Loss: 0.1580
Training Epoch: 57 [16128/50048]	Loss: 0.1401
Training Epoch: 57 [16256/50048]	Loss: 0.2713
Training Epoch: 57 [16384/50048]	Loss: 0.1527
Training Epoch: 57 [16512/50048]	Loss: 0.2155
Training Epoch: 57 [16640/50048]	Loss: 0.2006
Training Epoch: 57 [16768/50048]	Loss: 0.1949
Training Epoch: 57 [16896/50048]	Loss: 0.2563
Training Epoch: 57 [17024/50048]	Loss: 0.2290
Training Epoch: 57 [17152/50048]	Loss: 0.1361
Training Epoch: 57 [17280/50048]	Loss: 0.2311
Training Epoch: 57 [17408/50048]	Loss: 0.1598
Training Epoch: 57 [17536/50048]	Loss: 0.2702
Training Epoch: 57 [17664/50048]	Loss: 0.1390
Training Epoch: 57 [17792/50048]	Loss: 0.1755
Training Epoch: 57 [17920/50048]	Loss: 0.1321
Training Epoch: 57 [18048/50048]	Loss: 0.1935
Training Epoch: 57 [18176/50048]	Loss: 0.1099
Training Epoch: 57 [18304/50048]	Loss: 0.1561
Training Epoch: 57 [18432/50048]	Loss: 0.1547
Training Epoch: 57 [18560/50048]	Loss: 0.1183
Training Epoch: 57 [18688/50048]	Loss: 0.1606
Training Epoch: 57 [18816/50048]	Loss: 0.3038
Training Epoch: 57 [18944/50048]	Loss: 0.1862
Training Epoch: 57 [19072/50048]	Loss: 0.1495
Training Epoch: 57 [19200/50048]	Loss: 0.1715
Training Epoch: 57 [19328/50048]	Loss: 0.3010
Training Epoch: 57 [19456/50048]	Loss: 0.1408
Training Epoch: 57 [19584/50048]	Loss: 0.1575
Training Epoch: 57 [19712/50048]	Loss: 0.2636
Training Epoch: 57 [19840/50048]	Loss: 0.1815
Training Epoch: 57 [19968/50048]	Loss: 0.1601
Training Epoch: 57 [20096/50048]	Loss: 0.1922
Training Epoch: 57 [20224/50048]	Loss: 0.2200
Training Epoch: 57 [20352/50048]	Loss: 0.1374
Training Epoch: 57 [20480/50048]	Loss: 0.1787
Training Epoch: 57 [20608/50048]	Loss: 0.1820
Training Epoch: 57 [20736/50048]	Loss: 0.1610
Training Epoch: 57 [20864/50048]	Loss: 0.2045
Training Epoch: 57 [20992/50048]	Loss: 0.2092
Training Epoch: 57 [21120/50048]	Loss: 0.2845
Training Epoch: 57 [21248/50048]	Loss: 0.2208
Training Epoch: 57 [21376/50048]	Loss: 0.1724
Training Epoch: 57 [21504/50048]	Loss: 0.1488
Training Epoch: 57 [21632/50048]	Loss: 0.2287
Training Epoch: 57 [21760/50048]	Loss: 0.1552
Training Epoch: 57 [21888/50048]	Loss: 0.3548
Training Epoch: 57 [22016/50048]	Loss: 0.1414
Training Epoch: 57 [22144/50048]	Loss: 0.2428
Training Epoch: 57 [22272/50048]	Loss: 0.1844
Training Epoch: 57 [22400/50048]	Loss: 0.2402
Training Epoch: 57 [22528/50048]	Loss: 0.3310
Training Epoch: 57 [22656/50048]	Loss: 0.1374
Training Epoch: 57 [22784/50048]	Loss: 0.1945
Training Epoch: 57 [22912/50048]	Loss: 0.3071
Training Epoch: 57 [23040/50048]	Loss: 0.1895
Training Epoch: 57 [23168/50048]	Loss: 0.2477
Training Epoch: 57 [23296/50048]	Loss: 0.1751
Training Epoch: 57 [23424/50048]	Loss: 0.2384
Training Epoch: 57 [23552/50048]	Loss: 0.1568
Training Epoch: 57 [23680/50048]	Loss: 0.1783
Training Epoch: 57 [23808/50048]	Loss: 0.1512
Training Epoch: 57 [23936/50048]	Loss: 0.1534
Training Epoch: 57 [24064/50048]	Loss: 0.1614
Training Epoch: 57 [24192/50048]	Loss: 0.2011
Training Epoch: 57 [24320/50048]	Loss: 0.1597
Training Epoch: 57 [24448/50048]	Loss: 0.3621
Training Epoch: 57 [24576/50048]	Loss: 0.2658
Training Epoch: 57 [24704/50048]	Loss: 0.3014
Training Epoch: 57 [24832/50048]	Loss: 0.1487
Training Epoch: 57 [24960/50048]	Loss: 0.1748
Training Epoch: 57 [25088/50048]	Loss: 0.1456
Training Epoch: 57 [25216/50048]	Loss: 0.2175
Training Epoch: 57 [25344/50048]	Loss: 0.1993
Training Epoch: 57 [25472/50048]	Loss: 0.1999
Training Epoch: 57 [25600/50048]	Loss: 0.2622
Training Epoch: 57 [25728/50048]	Loss: 0.1233
Training Epoch: 57 [25856/50048]	Loss: 0.1643
Training Epoch: 57 [25984/50048]	Loss: 0.1705
Training Epoch: 57 [26112/50048]	Loss: 0.0881
Training Epoch: 57 [26240/50048]	Loss: 0.2482
Training Epoch: 57 [26368/50048]	Loss: 0.2282
Training Epoch: 57 [26496/50048]	Loss: 0.3538
Training Epoch: 57 [26624/50048]	Loss: 0.2324
Training Epoch: 57 [26752/50048]	Loss: 0.4130
Training Epoch: 57 [26880/50048]	Loss: 0.1706
Training Epoch: 57 [27008/50048]	Loss: 0.1966
Training Epoch: 57 [27136/50048]	Loss: 0.2032
Training Epoch: 57 [27264/50048]	Loss: 0.2083
Training Epoch: 57 [27392/50048]	Loss: 0.1636
Training Epoch: 57 [27520/50048]	Loss: 0.2912
Training Epoch: 57 [27648/50048]	Loss: 0.3003
Training Epoch: 57 [27776/50048]	Loss: 0.2538
Training Epoch: 57 [27904/50048]	Loss: 0.3254
Training Epoch: 57 [28032/50048]	Loss: 0.1765
Training Epoch: 57 [28160/50048]	Loss: 0.2627
Training Epoch: 57 [28288/50048]	Loss: 0.2508
Training Epoch: 57 [28416/50048]	Loss: 0.1856
Training Epoch: 57 [28544/50048]	Loss: 0.2788
Training Epoch: 57 [28672/50048]	Loss: 0.1706
Training Epoch: 57 [28800/50048]	Loss: 0.2424
Training Epoch: 57 [28928/50048]	Loss: 0.2195
Training Epoch: 57 [29056/50048]	Loss: 0.1550
Training Epoch: 57 [29184/50048]	Loss: 0.1919
Training Epoch: 57 [29312/50048]	Loss: 0.2188
Training Epoch: 57 [29440/50048]	Loss: 0.2323
Training Epoch: 57 [29568/50048]	Loss: 0.2045
Training Epoch: 57 [29696/50048]	Loss: 0.2397
Training Epoch: 57 [29824/50048]	Loss: 0.2288
Training Epoch: 57 [29952/50048]	Loss: 0.1654
Training Epoch: 57 [30080/50048]	Loss: 0.3516
Training Epoch: 57 [30208/50048]	Loss: 0.2879
Training Epoch: 57 [30336/50048]	Loss: 0.2364
Training Epoch: 57 [30464/50048]	Loss: 0.2067
Training Epoch: 57 [30592/50048]	Loss: 0.2272
Training Epoch: 57 [30720/50048]	Loss: 0.2990
Training Epoch: 57 [30848/50048]	Loss: 0.1679
Training Epoch: 57 [30976/50048]	Loss: 0.2007
Training Epoch: 57 [31104/50048]	Loss: 0.1856
Training Epoch: 57 [31232/50048]	Loss: 0.1740
Training Epoch: 57 [31360/50048]	Loss: 0.1974
Training Epoch: 57 [31488/50048]	Loss: 0.2783
Training Epoch: 57 [31616/50048]	Loss: 0.1225
Training Epoch: 57 [31744/50048]	Loss: 0.1923
Training Epoch: 57 [31872/50048]	Loss: 0.2588
Training Epoch: 57 [32000/50048]	Loss: 0.2883
Training Epoch: 57 [32128/50048]	Loss: 0.2353
Training Epoch: 57 [32256/50048]	Loss: 0.1983
Training Epoch: 57 [32384/50048]	Loss: 0.2224
Training Epoch: 57 [32512/50048]	Loss: 0.2070
Training Epoch: 57 [32640/50048]	Loss: 0.2463
Training Epoch: 57 [32768/50048]	Loss: 0.2050
Training Epoch: 57 [32896/50048]	Loss: 0.2054
Training Epoch: 57 [33024/50048]	Loss: 0.1630
Training Epoch: 57 [33152/50048]	Loss: 0.1739
Training Epoch: 57 [33280/50048]	Loss: 0.2273
Training Epoch: 57 [33408/50048]	Loss: 0.2572
Training Epoch: 57 [33536/50048]	Loss: 0.3608
Training Epoch: 57 [33664/50048]	Loss: 0.1621
Training Epoch: 57 [33792/50048]	Loss: 0.1794
Training Epoch: 57 [33920/50048]	Loss: 0.3279
Training Epoch: 57 [34048/50048]	Loss: 0.2172
Training Epoch: 57 [34176/50048]	Loss: 0.2867
Training Epoch: 57 [34304/50048]	Loss: 0.3148
Training Epoch: 57 [34432/50048]	Loss: 0.1975
Training Epoch: 57 [34560/50048]	Loss: 0.1900
Training Epoch: 57 [34688/50048]	Loss: 0.2408
Training Epoch: 57 [34816/50048]	Loss: 0.3097
Training Epoch: 57 [34944/50048]	Loss: 0.1721
Training Epoch: 57 [35072/50048]	Loss: 0.1830
Training Epoch: 57 [35200/50048]	Loss: 0.2511
Training Epoch: 57 [35328/50048]	Loss: 0.1715
Training Epoch: 57 [35456/50048]	Loss: 0.3295
Training Epoch: 57 [35584/50048]	Loss: 0.1764
Training Epoch: 57 [35712/50048]	Loss: 0.2833
Training Epoch: 57 [35840/50048]	Loss: 0.1787
Training Epoch: 57 [35968/50048]	Loss: 0.1785
Training Epoch: 57 [36096/50048]	Loss: 0.1579
Training Epoch: 57 [36224/50048]	Loss: 0.1981
Training Epoch: 57 [36352/50048]	Loss: 0.1878
Training Epoch: 57 [36480/50048]	Loss: 0.1823
Training Epoch: 57 [36608/50048]	Loss: 0.2440
Training Epoch: 57 [36736/50048]	Loss: 0.2714
Training Epoch: 57 [36864/50048]	Loss: 0.2909
Training Epoch: 57 [36992/50048]	Loss: 0.1728
Training Epoch: 57 [37120/50048]	Loss: 0.2551
Training Epoch: 57 [37248/50048]	Loss: 0.1922
Training Epoch: 57 [37376/50048]	Loss: 0.2055
Training Epoch: 57 [37504/50048]	Loss: 0.2731
Training Epoch: 57 [37632/50048]	Loss: 0.2309
Training Epoch: 57 [37760/50048]	Loss: 0.1907
Training Epoch: 57 [37888/50048]	Loss: 0.1357
Training Epoch: 57 [38016/50048]	Loss: 0.2506
Training Epoch: 57 [38144/50048]	Loss: 0.2343
Training Epoch: 57 [38272/50048]	Loss: 0.2328
Training Epoch: 57 [38400/50048]	Loss: 0.1732
Training Epoch: 57 [38528/50048]	Loss: 0.2727
Training Epoch: 57 [38656/50048]	Loss: 0.2714
Training Epoch: 57 [38784/50048]	Loss: 0.2673
Training Epoch: 57 [38912/50048]	Loss: 0.2721
Training Epoch: 57 [39040/50048]	Loss: 0.2503
Training Epoch: 57 [39168/50048]	Loss: 0.3186
Training Epoch: 57 [39296/50048]	Loss: 0.2432
Training Epoch: 57 [39424/50048]	Loss: 0.2837
Training Epoch: 57 [39552/50048]	Loss: 0.2133
Training Epoch: 57 [39680/50048]	Loss: 0.2150
Training Epoch: 57 [39808/50048]	Loss: 0.3132
Training Epoch: 57 [39936/50048]	Loss: 0.2630
Training Epoch: 57 [40064/50048]	Loss: 0.1700
Training Epoch: 57 [40192/50048]	Loss: 0.2015
Training Epoch: 57 [40320/50048]	Loss: 0.1383
Training Epoch: 57 [40448/50048]	Loss: 0.2129
Training Epoch: 57 [40576/50048]	Loss: 0.1830
Training Epoch: 57 [40704/50048]	Loss: 0.1420
Training Epoch: 57 [40832/50048]	Loss: 0.1681
Training Epoch: 57 [40960/50048]	Loss: 0.2379
Training Epoch: 57 [41088/50048]	Loss: 0.1280
Training Epoch: 57 [41216/50048]	Loss: 0.2808
Training Epoch: 57 [41344/50048]	Loss: 0.1903
Training Epoch: 57 [41472/50048]	Loss: 0.2492
Training Epoch: 57 [41600/50048]	Loss: 0.2148
Training Epoch: 57 [41728/50048]	Loss: 0.2050
Training Epoch: 57 [41856/50048]	Loss: 0.1766
Training Epoch: 57 [41984/50048]	Loss: 0.1905
Training Epoch: 57 [42112/50048]	Loss: 0.1808
Training Epoch: 57 [42240/50048]	Loss: 0.1672
Training Epoch: 57 [42368/50048]	Loss: 0.2576
Training Epoch: 57 [42496/50048]	Loss: 0.1556
Training Epoch: 57 [42624/50048]	Loss: 0.2084
Training Epoch: 57 [42752/50048]	Loss: 0.2623
Training Epoch: 57 [42880/50048]	Loss: 0.3028
Training Epoch: 57 [43008/50048]	Loss: 0.2294
Training Epoch: 57 [43136/50048]	Loss: 0.1830
Training Epoch: 57 [43264/50048]	Loss: 0.2415
Training Epoch: 57 [43392/50048]	Loss: 0.1819
Training Epoch: 57 [43520/50048]	Loss: 0.1794
Training Epoch: 57 [43648/50048]	Loss: 0.1616
Training Epoch: 57 [43776/50048]	Loss: 0.2158
Training Epoch: 57 [43904/50048]	Loss: 0.2702
Training Epoch: 57 [44032/50048]	Loss: 0.2140
Training Epoch: 57 [44160/50048]	Loss: 0.2122
Training Epoch: 57 [44288/50048]	Loss: 0.3019
Training Epoch: 57 [44416/50048]	Loss: 0.3559
Training Epoch: 57 [44544/50048]	Loss: 0.1564
Training Epoch: 57 [44672/50048]	Loss: 0.1681
Training Epoch: 57 [44800/50048]	Loss: 0.3095
Training Epoch: 57 [44928/50048]	Loss: 0.1317
Training Epoch: 57 [45056/50048]	Loss: 0.1725
Training Epoch: 57 [45184/50048]	Loss: 0.1765
Training Epoch: 57 [45312/50048]	Loss: 0.1179
Training Epoch: 57 [45440/50048]	Loss: 0.1426
Training Epoch: 57 [45568/50048]	Loss: 0.2583
Training Epoch: 57 [45696/50048]	Loss: 0.3286
2022-12-06 05:04:33,294 [ZeusDataLoader(train)] train epoch 58 done: time=86.54 energy=10500.40
2022-12-06 05:04:33,295 [ZeusDataLoader(eval)] Epoch 58 begin.
Training Epoch: 57 [45824/50048]	Loss: 0.1768
Training Epoch: 57 [45952/50048]	Loss: 0.2443
Training Epoch: 57 [46080/50048]	Loss: 0.2234
Training Epoch: 57 [46208/50048]	Loss: 0.2806
Training Epoch: 57 [46336/50048]	Loss: 0.3025
Training Epoch: 57 [46464/50048]	Loss: 0.1988
Training Epoch: 57 [46592/50048]	Loss: 0.2299
Training Epoch: 57 [46720/50048]	Loss: 0.1814
Training Epoch: 57 [46848/50048]	Loss: 0.2351
Training Epoch: 57 [46976/50048]	Loss: 0.1750
Training Epoch: 57 [47104/50048]	Loss: 0.2026
Training Epoch: 57 [47232/50048]	Loss: 0.2169
Training Epoch: 57 [47360/50048]	Loss: 0.3606
Training Epoch: 57 [47488/50048]	Loss: 0.2180
Training Epoch: 57 [47616/50048]	Loss: 0.3006
Training Epoch: 57 [47744/50048]	Loss: 0.2653
Training Epoch: 57 [47872/50048]	Loss: 0.1911
Training Epoch: 57 [48000/50048]	Loss: 0.2618
Training Epoch: 57 [48128/50048]	Loss: 0.1814
Training Epoch: 57 [48256/50048]	Loss: 0.2983
Training Epoch: 57 [48384/50048]	Loss: 0.1941
Training Epoch: 57 [48512/50048]	Loss: 0.2910
Training Epoch: 57 [48640/50048]	Loss: 0.2455
Training Epoch: 57 [48768/50048]	Loss: 0.2393
Training Epoch: 57 [48896/50048]	Loss: 0.1859
Training Epoch: 57 [49024/50048]	Loss: 0.4240
Training Epoch: 57 [49152/50048]	Loss: 0.2503
Training Epoch: 57 [49280/50048]	Loss: 0.2542
Training Epoch: 57 [49408/50048]	Loss: 0.2415
Training Epoch: 57 [49536/50048]	Loss: 0.2181
Training Epoch: 57 [49664/50048]	Loss: 0.3281
Training Epoch: 57 [49792/50048]	Loss: 0.1982
Training Epoch: 57 [49920/50048]	Loss: 0.2670
Training Epoch: 57 [50048/50048]	Loss: 0.3355
2022-12-06 10:04:37.058 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:04:37,085 [ZeusDataLoader(eval)] eval epoch 58 done: time=3.78 energy=453.14
2022-12-06 05:04:37,086 [ZeusDataLoader(train)] Up to epoch 58: time=5233.59, energy=635263.11, cost=775571.05
2022-12-06 05:04:37,086 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:04:37,086 [ZeusDataLoader(train)] Expected next epoch: time=5323.39, energy=646061.13, cost=788827.43
2022-12-06 05:04:37,087 [ZeusDataLoader(train)] Epoch 59 begin.
Validation Epoch: 57, Average loss: 0.0160, Accuracy: 0.6355
2022-12-06 05:04:37,263 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:04:37,264 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:04:37.266 [ZeusMonitor] Monitor started.
2022-12-06 10:04:37.266 [ZeusMonitor] Running indefinitely. 2022-12-06 10:04:37.266 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:04:37.266 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e59+gpu0.power.log
Training Epoch: 58 [128/50048]	Loss: 0.2040
Training Epoch: 58 [256/50048]	Loss: 0.0949
Training Epoch: 58 [384/50048]	Loss: 0.1686
Training Epoch: 58 [512/50048]	Loss: 0.1406
Training Epoch: 58 [640/50048]	Loss: 0.1654
Training Epoch: 58 [768/50048]	Loss: 0.2259
Training Epoch: 58 [896/50048]	Loss: 0.2192
Training Epoch: 58 [1024/50048]	Loss: 0.1822
Training Epoch: 58 [1152/50048]	Loss: 0.1957
Training Epoch: 58 [1280/50048]	Loss: 0.1956
Training Epoch: 58 [1408/50048]	Loss: 0.2088
Training Epoch: 58 [1536/50048]	Loss: 0.2079
Training Epoch: 58 [1664/50048]	Loss: 0.2178
Training Epoch: 58 [1792/50048]	Loss: 0.1538
Training Epoch: 58 [1920/50048]	Loss: 0.1448
Training Epoch: 58 [2048/50048]	Loss: 0.1651
Training Epoch: 58 [2176/50048]	Loss: 0.1760
Training Epoch: 58 [2304/50048]	Loss: 0.1307
Training Epoch: 58 [2432/50048]	Loss: 0.1493
Training Epoch: 58 [2560/50048]	Loss: 0.2299
Training Epoch: 58 [2688/50048]	Loss: 0.2063
Training Epoch: 58 [2816/50048]	Loss: 0.1743
Training Epoch: 58 [2944/50048]	Loss: 0.3593
Training Epoch: 58 [3072/50048]	Loss: 0.1034
Training Epoch: 58 [3200/50048]	Loss: 0.2306
Training Epoch: 58 [3328/50048]	Loss: 0.2203
Training Epoch: 58 [3456/50048]	Loss: 0.1670
Training Epoch: 58 [3584/50048]	Loss: 0.2707
Training Epoch: 58 [3712/50048]	Loss: 0.1735
Training Epoch: 58 [3840/50048]	Loss: 0.1165
Training Epoch: 58 [3968/50048]	Loss: 0.1544
Training Epoch: 58 [4096/50048]	Loss: 0.2322
Training Epoch: 58 [4224/50048]	Loss: 0.1231
Training Epoch: 58 [4352/50048]	Loss: 0.2168
Training Epoch: 58 [4480/50048]	Loss: 0.1385
Training Epoch: 58 [4608/50048]	Loss: 0.1452
Training Epoch: 58 [4736/50048]	Loss: 0.1957
Training Epoch: 58 [4864/50048]	Loss: 0.1419
Training Epoch: 58 [4992/50048]	Loss: 0.1155
Training Epoch: 58 [5120/50048]	Loss: 0.1820
Training Epoch: 58 [5248/50048]	Loss: 0.2237
Training Epoch: 58 [5376/50048]	Loss: 0.2310
Training Epoch: 58 [5504/50048]	Loss: 0.1437
Training Epoch: 58 [5632/50048]	Loss: 0.1990
Training Epoch: 58 [5760/50048]	Loss: 0.2396
Training Epoch: 58 [5888/50048]	Loss: 0.2017
Training Epoch: 58 [6016/50048]	Loss: 0.1976
Training Epoch: 58 [6144/50048]	Loss: 0.1571
Training Epoch: 58 [6272/50048]	Loss: 0.1717
Training Epoch: 58 [6400/50048]	Loss: 0.1654
Training Epoch: 58 [6528/50048]	Loss: 0.1676
Training Epoch: 58 [6656/50048]	Loss: 0.1002
Training Epoch: 58 [6784/50048]	Loss: 0.1412
Training Epoch: 58 [6912/50048]	Loss: 0.0865
Training Epoch: 58 [7040/50048]	Loss: 0.1502
Training Epoch: 58 [7168/50048]	Loss: 0.1415
Training Epoch: 58 [7296/50048]	Loss: 0.1294
Training Epoch: 58 [7424/50048]	Loss: 0.2376
Training Epoch: 58 [7552/50048]	Loss: 0.1371
Training Epoch: 58 [7680/50048]	Loss: 0.1568
Training Epoch: 58 [7808/50048]	Loss: 0.1107
Training Epoch: 58 [7936/50048]	Loss: 0.2501
Training Epoch: 58 [8064/50048]	Loss: 0.1529
Training Epoch: 58 [8192/50048]	Loss: 0.2383
Training Epoch: 58 [8320/50048]	Loss: 0.2272
Training Epoch: 58 [8448/50048]	Loss: 0.1780
Training Epoch: 58 [8576/50048]	Loss: 0.1359
Training Epoch: 58 [8704/50048]	Loss: 0.1335
Training Epoch: 58 [8832/50048]	Loss: 0.1475
Training Epoch: 58 [8960/50048]	Loss: 0.2414
Training Epoch: 58 [9088/50048]	Loss: 0.1587
Training Epoch: 58 [9216/50048]	Loss: 0.1660
Training Epoch: 58 [9344/50048]	Loss: 0.2102
Training Epoch: 58 [9472/50048]	Loss: 0.1243
Training Epoch: 58 [9600/50048]	Loss: 0.1510
Training Epoch: 58 [9728/50048]	Loss: 0.1970
Training Epoch: 58 [9856/50048]	Loss: 0.2299
Training Epoch: 58 [9984/50048]	Loss: 0.1544
Training Epoch: 58 [10112/50048]	Loss: 0.1860
Training Epoch: 58 [10240/50048]	Loss: 0.1969
Training Epoch: 58 [10368/50048]	Loss: 0.1692
Training Epoch: 58 [10496/50048]	Loss: 0.1355
Training Epoch: 58 [10624/50048]	Loss: 0.1616
Training Epoch: 58 [10752/50048]	Loss: 0.1368
Training Epoch: 58 [10880/50048]	Loss: 0.1642
Training Epoch: 58 [11008/50048]	Loss: 0.2839
Training Epoch: 58 [11136/50048]	Loss: 0.1084
Training Epoch: 58 [11264/50048]	Loss: 0.1864
Training Epoch: 58 [11392/50048]	Loss: 0.2712
Training Epoch: 58 [11520/50048]	Loss: 0.2547
Training Epoch: 58 [11648/50048]	Loss: 0.2133
Training Epoch: 58 [11776/50048]	Loss: 0.2002
Training Epoch: 58 [11904/50048]	Loss: 0.2168
Training Epoch: 58 [12032/50048]	Loss: 0.1711
Training Epoch: 58 [12160/50048]	Loss: 0.1959
Training Epoch: 58 [12288/50048]	Loss: 0.1868
Training Epoch: 58 [12416/50048]	Loss: 0.2563
Training Epoch: 58 [12544/50048]	Loss: 0.1582
Training Epoch: 58 [12672/50048]	Loss: 0.1892
Training Epoch: 58 [12800/50048]	Loss: 0.2208
Training Epoch: 58 [12928/50048]	Loss: 0.1321
Training Epoch: 58 [13056/50048]	Loss: 0.2680
Training Epoch: 58 [13184/50048]	Loss: 0.1925
Training Epoch: 58 [13312/50048]	Loss: 0.1625
Training Epoch: 58 [13440/50048]	Loss: 0.1854
Training Epoch: 58 [13568/50048]	Loss: 0.1619
Training Epoch: 58 [13696/50048]	Loss: 0.1600
Training Epoch: 58 [13824/50048]	Loss: 0.2481
Training Epoch: 58 [13952/50048]	Loss: 0.1198
Training Epoch: 58 [14080/50048]	Loss: 0.3374
Training Epoch: 58 [14208/50048]	Loss: 0.2087
Training Epoch: 58 [14336/50048]	Loss: 0.2029
Training Epoch: 58 [14464/50048]	Loss: 0.1676
Training Epoch: 58 [14592/50048]	Loss: 0.1094
Training Epoch: 58 [14720/50048]	Loss: 0.3060
Training Epoch: 58 [14848/50048]	Loss: 0.1702
Training Epoch: 58 [14976/50048]	Loss: 0.1633
Training Epoch: 58 [15104/50048]	Loss: 0.2792
Training Epoch: 58 [15232/50048]	Loss: 0.1651
Training Epoch: 58 [15360/50048]	Loss: 0.1505
Training Epoch: 58 [15488/50048]	Loss: 0.2151
Training Epoch: 58 [15616/50048]	Loss: 0.1332
Training Epoch: 58 [15744/50048]	Loss: 0.1689
Training Epoch: 58 [15872/50048]	Loss: 0.1766
Training Epoch: 58 [16000/50048]	Loss: 0.1803
Training Epoch: 58 [16128/50048]	Loss: 0.2308
Training Epoch: 58 [16256/50048]	Loss: 0.1178
Training Epoch: 58 [16384/50048]	Loss: 0.1714
Training Epoch: 58 [16512/50048]	Loss: 0.0814
Training Epoch: 58 [16640/50048]	Loss: 0.2354
Training Epoch: 58 [16768/50048]	Loss: 0.2339
Training Epoch: 58 [16896/50048]	Loss: 0.1953
Training Epoch: 58 [17024/50048]	Loss: 0.2056
Training Epoch: 58 [17152/50048]	Loss: 0.1612
Training Epoch: 58 [17280/50048]	Loss: 0.1857
Training Epoch: 58 [17408/50048]	Loss: 0.0920
Training Epoch: 58 [17536/50048]	Loss: 0.1133
Training Epoch: 58 [17664/50048]	Loss: 0.1423
Training Epoch: 58 [17792/50048]	Loss: 0.2554
Training Epoch: 58 [17920/50048]	Loss: 0.2278
Training Epoch: 58 [18048/50048]	Loss: 0.1994
Training Epoch: 58 [18176/50048]	Loss: 0.2367
Training Epoch: 58 [18304/50048]	Loss: 0.1307
Training Epoch: 58 [18432/50048]	Loss: 0.2062
Training Epoch: 58 [18560/50048]	Loss: 0.1316
Training Epoch: 58 [18688/50048]	Loss: 0.2124
Training Epoch: 58 [18816/50048]	Loss: 0.1826
Training Epoch: 58 [18944/50048]	Loss: 0.2273
Training Epoch: 58 [19072/50048]	Loss: 0.2464
Training Epoch: 58 [19200/50048]	Loss: 0.2309
Training Epoch: 58 [19328/50048]	Loss: 0.1509
Training Epoch: 58 [19456/50048]	Loss: 0.1633
Training Epoch: 58 [19584/50048]	Loss: 0.1212
Training Epoch: 58 [19712/50048]	Loss: 0.1747
Training Epoch: 58 [19840/50048]	Loss: 0.1614
Training Epoch: 58 [19968/50048]	Loss: 0.2189
Training Epoch: 58 [20096/50048]	Loss: 0.1376
Training Epoch: 58 [20224/50048]	Loss: 0.2447
Training Epoch: 58 [20352/50048]	Loss: 0.2484
Training Epoch: 58 [20480/50048]	Loss: 0.1472
Training Epoch: 58 [20608/50048]	Loss: 0.1635
Training Epoch: 58 [20736/50048]	Loss: 0.2656
Training Epoch: 58 [20864/50048]	Loss: 0.2995
Training Epoch: 58 [20992/50048]	Loss: 0.1907
Training Epoch: 58 [21120/50048]	Loss: 0.2419
Training Epoch: 58 [21248/50048]	Loss: 0.2646
Training Epoch: 58 [21376/50048]	Loss: 0.1882
Training Epoch: 58 [21504/50048]	Loss: 0.2139
Training Epoch: 58 [21632/50048]	Loss: 0.2013
Training Epoch: 58 [21760/50048]	Loss: 0.2161
Training Epoch: 58 [21888/50048]	Loss: 0.1332
Training Epoch: 58 [22016/50048]	Loss: 0.1701
Training Epoch: 58 [22144/50048]	Loss: 0.2841
Training Epoch: 58 [22272/50048]	Loss: 0.2171
Training Epoch: 58 [22400/50048]	Loss: 0.0953
Training Epoch: 58 [22528/50048]	Loss: 0.1775
Training Epoch: 58 [22656/50048]	Loss: 0.0986
Training Epoch: 58 [22784/50048]	Loss: 0.2326
Training Epoch: 58 [22912/50048]	Loss: 0.1918
Training Epoch: 58 [23040/50048]	Loss: 0.1732
Training Epoch: 58 [23168/50048]	Loss: 0.2228
Training Epoch: 58 [23296/50048]	Loss: 0.2503
Training Epoch: 58 [23424/50048]	Loss: 0.2635
Training Epoch: 58 [23552/50048]	Loss: 0.2148
Training Epoch: 58 [23680/50048]	Loss: 0.1494
Training Epoch: 58 [23808/50048]	Loss: 0.2707
Training Epoch: 58 [23936/50048]	Loss: 0.0963
Training Epoch: 58 [24064/50048]	Loss: 0.1979
Training Epoch: 58 [24192/50048]	Loss: 0.1819
Training Epoch: 58 [24320/50048]	Loss: 0.1880
Training Epoch: 58 [24448/50048]	Loss: 0.1267
Training Epoch: 58 [24576/50048]	Loss: 0.1351
Training Epoch: 58 [24704/50048]	Loss: 0.2187
Training Epoch: 58 [24832/50048]	Loss: 0.2632
Training Epoch: 58 [24960/50048]	Loss: 0.1620
Training Epoch: 58 [25088/50048]	Loss: 0.3682
Training Epoch: 58 [25216/50048]	Loss: 0.1382
Training Epoch: 58 [25344/50048]	Loss: 0.2102
Training Epoch: 58 [25472/50048]	Loss: 0.1871
Training Epoch: 58 [25600/50048]	Loss: 0.1799
Training Epoch: 58 [25728/50048]	Loss: 0.1563
Training Epoch: 58 [25856/50048]	Loss: 0.2683
Training Epoch: 58 [25984/50048]	Loss: 0.1810
Training Epoch: 58 [26112/50048]	Loss: 0.1993
Training Epoch: 58 [26240/50048]	Loss: 0.2465
Training Epoch: 58 [26368/50048]	Loss: 0.2078
Training Epoch: 58 [26496/50048]	Loss: 0.1793
Training Epoch: 58 [26624/50048]	Loss: 0.1900
Training Epoch: 58 [26752/50048]	Loss: 0.1651
Training Epoch: 58 [26880/50048]	Loss: 0.2887
Training Epoch: 58 [27008/50048]	Loss: 0.1719
Training Epoch: 58 [27136/50048]	Loss: 0.2123
Training Epoch: 58 [27264/50048]	Loss: 0.2938
Training Epoch: 58 [27392/50048]	Loss: 0.1753
Training Epoch: 58 [27520/50048]	Loss: 0.2054
Training Epoch: 58 [27648/50048]	Loss: 0.2395
Training Epoch: 58 [27776/50048]	Loss: 0.2326
Training Epoch: 58 [27904/50048]	Loss: 0.2915
Training Epoch: 58 [28032/50048]	Loss: 0.2203
Training Epoch: 58 [28160/50048]	Loss: 0.2127
Training Epoch: 58 [28288/50048]	Loss: 0.1810
Training Epoch: 58 [28416/50048]	Loss: 0.1814
Training Epoch: 58 [28544/50048]	Loss: 0.2085
Training Epoch: 58 [28672/50048]	Loss: 0.3223
Training Epoch: 58 [28800/50048]	Loss: 0.1565
Training Epoch: 58 [28928/50048]	Loss: 0.1738
Training Epoch: 58 [29056/50048]	Loss: 0.2695
Training Epoch: 58 [29184/50048]	Loss: 0.1966
Training Epoch: 58 [29312/50048]	Loss: 0.1899
Training Epoch: 58 [29440/50048]	Loss: 0.2019
Training Epoch: 58 [29568/50048]	Loss: 0.2134
Training Epoch: 58 [29696/50048]	Loss: 0.2873
Training Epoch: 58 [29824/50048]	Loss: 0.1320
Training Epoch: 58 [29952/50048]	Loss: 0.2563
Training Epoch: 58 [30080/50048]	Loss: 0.2013
Training Epoch: 58 [30208/50048]	Loss: 0.1631
Training Epoch: 58 [30336/50048]	Loss: 0.1884
Training Epoch: 58 [30464/50048]	Loss: 0.2438
Training Epoch: 58 [30592/50048]	Loss: 0.2898
Training Epoch: 58 [30720/50048]	Loss: 0.1288
Training Epoch: 58 [30848/50048]	Loss: 0.2108
Training Epoch: 58 [30976/50048]	Loss: 0.1306
Training Epoch: 58 [31104/50048]	Loss: 0.1344
Training Epoch: 58 [31232/50048]	Loss: 0.2205
Training Epoch: 58 [31360/50048]	Loss: 0.2439
Training Epoch: 58 [31488/50048]	Loss: 0.2536
Training Epoch: 58 [31616/50048]	Loss: 0.1954
Training Epoch: 58 [31744/50048]	Loss: 0.1436
Training Epoch: 58 [31872/50048]	Loss: 0.2341
Training Epoch: 58 [32000/50048]	Loss: 0.1857
Training Epoch: 58 [32128/50048]	Loss: 0.2734
Training Epoch: 58 [32256/50048]	Loss: 0.2039
Training Epoch: 58 [32384/50048]	Loss: 0.1937
Training Epoch: 58 [32512/50048]	Loss: 0.1420
Training Epoch: 58 [32640/50048]	Loss: 0.2310
Training Epoch: 58 [32768/50048]	Loss: 0.2482
Training Epoch: 58 [32896/50048]	Loss: 0.2173
Training Epoch: 58 [33024/50048]	Loss: 0.2250
Training Epoch: 58 [33152/50048]	Loss: 0.1612
Training Epoch: 58 [33280/50048]	Loss: 0.1585
Training Epoch: 58 [33408/50048]	Loss: 0.3007
Training Epoch: 58 [33536/50048]	Loss: 0.1672
Training Epoch: 58 [33664/50048]	Loss: 0.3785
Training Epoch: 58 [33792/50048]	Loss: 0.2205
Training Epoch: 58 [33920/50048]	Loss: 0.1613
Training Epoch: 58 [34048/50048]	Loss: 0.1619
Training Epoch: 58 [34176/50048]	Loss: 0.2087
Training Epoch: 58 [34304/50048]	Loss: 0.1504
Training Epoch: 58 [34432/50048]	Loss: 0.1752
Training Epoch: 58 [34560/50048]	Loss: 0.2344
Training Epoch: 58 [34688/50048]	Loss: 0.1115
Training Epoch: 58 [34816/50048]	Loss: 0.1587
Training Epoch: 58 [34944/50048]	Loss: 0.1532
Training Epoch: 58 [35072/50048]	Loss: 0.2437
Training Epoch: 58 [35200/50048]	Loss: 0.3454
Training Epoch: 58 [35328/50048]	Loss: 0.1647
Training Epoch: 58 [35456/50048]	Loss: 0.1964
Training Epoch: 58 [35584/50048]	Loss: 0.1748
Training Epoch: 58 [35712/50048]	Loss: 0.1691
Training Epoch: 58 [35840/50048]	Loss: 0.2128
Training Epoch: 58 [35968/50048]	Loss: 0.2572
Training Epoch: 58 [36096/50048]	Loss: 0.1693
Training Epoch: 58 [36224/50048]	Loss: 0.1907
Training Epoch: 58 [36352/50048]	Loss: 0.2191
Training Epoch: 58 [36480/50048]	Loss: 0.1612
Training Epoch: 58 [36608/50048]	Loss: 0.1070
Training Epoch: 58 [36736/50048]	Loss: 0.2682
Training Epoch: 58 [36864/50048]	Loss: 0.1633
Training Epoch: 58 [36992/50048]	Loss: 0.2270
Training Epoch: 58 [37120/50048]	Loss: 0.1271
Training Epoch: 58 [37248/50048]	Loss: 0.2375
Training Epoch: 58 [37376/50048]	Loss: 0.3270
Training Epoch: 58 [37504/50048]	Loss: 0.2487
Training Epoch: 58 [37632/50048]	Loss: 0.1797
Training Epoch: 58 [37760/50048]	Loss: 0.3292
Training Epoch: 58 [37888/50048]	Loss: 0.2202
Training Epoch: 58 [38016/50048]	Loss: 0.2075
Training Epoch: 58 [38144/50048]	Loss: 0.2828
Training Epoch: 58 [38272/50048]	Loss: 0.1973
Training Epoch: 58 [38400/50048]	Loss: 0.2351
Training Epoch: 58 [38528/50048]	Loss: 0.1853
Training Epoch: 58 [38656/50048]	Loss: 0.2030
Training Epoch: 58 [38784/50048]	Loss: 0.1864
Training Epoch: 58 [38912/50048]	Loss: 0.1761
Training Epoch: 58 [39040/50048]	Loss: 0.2821
Training Epoch: 58 [39168/50048]	Loss: 0.1969
Training Epoch: 58 [39296/50048]	Loss: 0.1835
Training Epoch: 58 [39424/50048]	Loss: 0.1280
Training Epoch: 58 [39552/50048]	Loss: 0.2031
Training Epoch: 58 [39680/50048]	Loss: 0.2169
Training Epoch: 58 [39808/50048]	Loss: 0.1544
Training Epoch: 58 [39936/50048]	Loss: 0.1937
Training Epoch: 58 [40064/50048]	Loss: 0.2502
Training Epoch: 58 [40192/50048]	Loss: 0.2234
Training Epoch: 58 [40320/50048]	Loss: 0.2909
Training Epoch: 58 [40448/50048]	Loss: 0.2625
Training Epoch: 58 [40576/50048]	Loss: 0.2384
Training Epoch: 58 [40704/50048]	Loss: 0.2747
Training Epoch: 58 [40832/50048]	Loss: 0.1714
Training Epoch: 58 [40960/50048]	Loss: 0.2230
Training Epoch: 58 [41088/50048]	Loss: 0.2127
Training Epoch: 58 [41216/50048]	Loss: 0.1891
Training Epoch: 58 [41344/50048]	Loss: 0.2617
Training Epoch: 58 [41472/50048]	Loss: 0.1689
Training Epoch: 58 [41600/50048]	Loss: 0.1963
Training Epoch: 58 [41728/50048]	Loss: 0.2332
Training Epoch: 58 [41856/50048]	Loss: 0.2644
Training Epoch: 58 [41984/50048]	Loss: 0.2961
Training Epoch: 58 [42112/50048]	Loss: 0.2705
Training Epoch: 58 [42240/50048]	Loss: 0.1746
Training Epoch: 58 [42368/50048]	Loss: 0.2700
Training Epoch: 58 [42496/50048]	Loss: 0.2188
Training Epoch: 58 [42624/50048]	Loss: 0.2462
Training Epoch: 58 [42752/50048]	Loss: 0.2837
Training Epoch: 58 [42880/50048]	Loss: 0.2372
Training Epoch: 58 [43008/50048]	Loss: 0.2033
Training Epoch: 58 [43136/50048]	Loss: 0.1689
Training Epoch: 58 [43264/50048]	Loss: 0.2213
Training Epoch: 58 [43392/50048]	Loss: 0.2617
Training Epoch: 58 [43520/50048]	Loss: 0.1994
Training Epoch: 58 [43648/50048]	Loss: 0.1481
Training Epoch: 58 [43776/50048]	Loss: 0.2451
Training Epoch: 58 [43904/50048]	Loss: 0.1956
Training Epoch: 58 [44032/50048]	Loss: 0.2857
Training Epoch: 58 [44160/50048]	Loss: 0.2401
Training Epoch: 58 [44288/50048]	Loss: 0.2049
Training Epoch: 58 [44416/50048]	Loss: 0.2269
Training Epoch: 58 [44544/50048]	Loss: 0.3675
Training Epoch: 58 [44672/50048]	Loss: 0.1804
Training Epoch: 58 [44800/50048]	Loss: 0.2184
Training Epoch: 58 [44928/50048]	Loss: 0.2491
Training Epoch: 58 [45056/50048]	Loss: 0.2698
Training Epoch: 58 [45184/50048]	Loss: 0.2754
Training Epoch: 58 [45312/50048]	Loss: 0.1485
Training Epoch: 58 [45440/50048]	Loss: 0.3427
Training Epoch: 58 [45568/50048]	Loss: 0.2241
Training Epoch: 58 [45696/50048]	Loss: 0.3164
2022-12-06 05:06:03,581 [ZeusDataLoader(train)] train epoch 59 done: time=86.48 energy=10504.16
2022-12-06 05:06:03,583 [ZeusDataLoader(eval)] Epoch 59 begin.
Training Epoch: 58 [45824/50048]	Loss: 0.2876
Training Epoch: 58 [45952/50048]	Loss: 0.2565
Training Epoch: 58 [46080/50048]	Loss: 0.2245
Training Epoch: 58 [46208/50048]	Loss: 0.2132
Training Epoch: 58 [46336/50048]	Loss: 0.3097
Training Epoch: 58 [46464/50048]	Loss: 0.2433
Training Epoch: 58 [46592/50048]	Loss: 0.2029
Training Epoch: 58 [46720/50048]	Loss: 0.1595
Training Epoch: 58 [46848/50048]	Loss: 0.3825
Training Epoch: 58 [46976/50048]	Loss: 0.1806
Training Epoch: 58 [47104/50048]	Loss: 0.2715
Training Epoch: 58 [47232/50048]	Loss: 0.2058
Training Epoch: 58 [47360/50048]	Loss: 0.2253
Training Epoch: 58 [47488/50048]	Loss: 0.2832
Training Epoch: 58 [47616/50048]	Loss: 0.2755
Training Epoch: 58 [47744/50048]	Loss: 0.2191
Training Epoch: 58 [47872/50048]	Loss: 0.2297
Training Epoch: 58 [48000/50048]	Loss: 0.2583
Training Epoch: 58 [48128/50048]	Loss: 0.2308
Training Epoch: 58 [48256/50048]	Loss: 0.2461
Training Epoch: 58 [48384/50048]	Loss: 0.2017
Training Epoch: 58 [48512/50048]	Loss: 0.1355
Training Epoch: 58 [48640/50048]	Loss: 0.3056
Training Epoch: 58 [48768/50048]	Loss: 0.2875
Training Epoch: 58 [48896/50048]	Loss: 0.3812
Training Epoch: 58 [49024/50048]	Loss: 0.3862
Training Epoch: 58 [49152/50048]	Loss: 0.3056
Training Epoch: 58 [49280/50048]	Loss: 0.2434
Training Epoch: 58 [49408/50048]	Loss: 0.2619
Training Epoch: 58 [49536/50048]	Loss: 0.1222
Training Epoch: 58 [49664/50048]	Loss: 0.1875
Training Epoch: 58 [49792/50048]	Loss: 0.2564
Training Epoch: 58 [49920/50048]	Loss: 0.1602
Training Epoch: 58 [50048/50048]	Loss: 0.3282
2022-12-06 10:06:07.322 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:06:07,350 [ZeusDataLoader(eval)] eval epoch 59 done: time=3.76 energy=454.56
2022-12-06 05:06:07,350 [ZeusDataLoader(train)] Up to epoch 59: time=5323.84, energy=646221.83, cost=788946.64
2022-12-06 05:06:07,350 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:06:07,350 [ZeusDataLoader(train)] Expected next epoch: time=5413.64, energy=657019.85, cost=802203.02
2022-12-06 05:06:07,351 [ZeusDataLoader(train)] Epoch 60 begin.
Validation Epoch: 58, Average loss: 0.0165, Accuracy: 0.6333
2022-12-06 05:06:07,493 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:06:07,494 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:06:07.497 [ZeusMonitor] Monitor started.
2022-12-06 10:06:07.497 [ZeusMonitor] Running indefinitely. 2022-12-06 10:06:07.497 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:06:07.497 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e60+gpu0.power.log
Training Epoch: 59 [128/50048]	Loss: 0.1635
Training Epoch: 59 [256/50048]	Loss: 0.2099
Training Epoch: 59 [384/50048]	Loss: 0.1531
Training Epoch: 59 [512/50048]	Loss: 0.2017
Training Epoch: 59 [640/50048]	Loss: 0.1485
Training Epoch: 59 [768/50048]	Loss: 0.1724
Training Epoch: 59 [896/50048]	Loss: 0.1911
Training Epoch: 59 [1024/50048]	Loss: 0.1611
Training Epoch: 59 [1152/50048]	Loss: 0.2930
Training Epoch: 59 [1280/50048]	Loss: 0.1292
Training Epoch: 59 [1408/50048]	Loss: 0.0987
Training Epoch: 59 [1536/50048]	Loss: 0.2054
Training Epoch: 59 [1664/50048]	Loss: 0.1615
Training Epoch: 59 [1792/50048]	Loss: 0.1248
Training Epoch: 59 [1920/50048]	Loss: 0.1508
Training Epoch: 59 [2048/50048]	Loss: 0.1006
Training Epoch: 59 [2176/50048]	Loss: 0.1144
Training Epoch: 59 [2304/50048]	Loss: 0.1787
Training Epoch: 59 [2432/50048]	Loss: 0.2821
Training Epoch: 59 [2560/50048]	Loss: 0.1440
Training Epoch: 59 [2688/50048]	Loss: 0.1328
Training Epoch: 59 [2816/50048]	Loss: 0.3829
Training Epoch: 59 [2944/50048]	Loss: 0.1820
Training Epoch: 59 [3072/50048]	Loss: 0.1377
Training Epoch: 59 [3200/50048]	Loss: 0.2067
Training Epoch: 59 [3328/50048]	Loss: 0.2067
Training Epoch: 59 [3456/50048]	Loss: 0.1106
Training Epoch: 59 [3584/50048]	Loss: 0.1577
Training Epoch: 59 [3712/50048]	Loss: 0.1201
Training Epoch: 59 [3840/50048]	Loss: 0.1798
Training Epoch: 59 [3968/50048]	Loss: 0.1753
Training Epoch: 59 [4096/50048]	Loss: 0.2217
Training Epoch: 59 [4224/50048]	Loss: 0.1420
Training Epoch: 59 [4352/50048]	Loss: 0.2124
Training Epoch: 59 [4480/50048]	Loss: 0.1555
Training Epoch: 59 [4608/50048]	Loss: 0.1699
Training Epoch: 59 [4736/50048]	Loss: 0.2406
Training Epoch: 59 [4864/50048]	Loss: 0.2048
Training Epoch: 59 [4992/50048]	Loss: 0.1321
Training Epoch: 59 [5120/50048]	Loss: 0.1718
Training Epoch: 59 [5248/50048]	Loss: 0.1713
Training Epoch: 59 [5376/50048]	Loss: 0.1460
Training Epoch: 59 [5504/50048]	Loss: 0.2091
Training Epoch: 59 [5632/50048]	Loss: 0.1843
Training Epoch: 59 [5760/50048]	Loss: 0.2058
Training Epoch: 59 [5888/50048]	Loss: 0.1798
Training Epoch: 59 [6016/50048]	Loss: 0.3037
Training Epoch: 59 [6144/50048]	Loss: 0.1381
Training Epoch: 59 [6272/50048]	Loss: 0.1482
Training Epoch: 59 [6400/50048]	Loss: 0.1422
Training Epoch: 59 [6528/50048]	Loss: 0.1568
Training Epoch: 59 [6656/50048]	Loss: 0.2109
Training Epoch: 59 [6784/50048]	Loss: 0.1461
Training Epoch: 59 [6912/50048]	Loss: 0.1820
Training Epoch: 59 [7040/50048]	Loss: 0.1492
Training Epoch: 59 [7168/50048]	Loss: 0.1944
Training Epoch: 59 [7296/50048]	Loss: 0.1851
Training Epoch: 59 [7424/50048]	Loss: 0.1436
Training Epoch: 59 [7552/50048]	Loss: 0.1901
Training Epoch: 59 [7680/50048]	Loss: 0.1177
Training Epoch: 59 [7808/50048]	Loss: 0.1540
Training Epoch: 59 [7936/50048]	Loss: 0.1941
Training Epoch: 59 [8064/50048]	Loss: 0.1874
Training Epoch: 59 [8192/50048]	Loss: 0.1984
Training Epoch: 59 [8320/50048]	Loss: 0.1343
Training Epoch: 59 [8448/50048]	Loss: 0.2260
Training Epoch: 59 [8576/50048]	Loss: 0.1774
Training Epoch: 59 [8704/50048]	Loss: 0.1697
Training Epoch: 59 [8832/50048]	Loss: 0.1641
Training Epoch: 59 [8960/50048]	Loss: 0.0920
Training Epoch: 59 [9088/50048]	Loss: 0.1889
Training Epoch: 59 [9216/50048]	Loss: 0.2492
Training Epoch: 59 [9344/50048]	Loss: 0.1654
Training Epoch: 59 [9472/50048]	Loss: 0.1254
Training Epoch: 59 [9600/50048]	Loss: 0.1036
Training Epoch: 59 [9728/50048]	Loss: 0.2291
Training Epoch: 59 [9856/50048]	Loss: 0.1649
Training Epoch: 59 [9984/50048]	Loss: 0.1605
Training Epoch: 59 [10112/50048]	Loss: 0.2042
Training Epoch: 59 [10240/50048]	Loss: 0.1783
Training Epoch: 59 [10368/50048]	Loss: 0.1621
Training Epoch: 59 [10496/50048]	Loss: 0.0711
Training Epoch: 59 [10624/50048]	Loss: 0.1833
Training Epoch: 59 [10752/50048]	Loss: 0.2226
Training Epoch: 59 [10880/50048]	Loss: 0.2319
Training Epoch: 59 [11008/50048]	Loss: 0.1165
Training Epoch: 59 [11136/50048]	Loss: 0.2037
Training Epoch: 59 [11264/50048]	Loss: 0.2303
Training Epoch: 59 [11392/50048]	Loss: 0.2479
Training Epoch: 59 [11520/50048]	Loss: 0.1939
Training Epoch: 59 [11648/50048]	Loss: 0.1640
Training Epoch: 59 [11776/50048]	Loss: 0.1387
Training Epoch: 59 [11904/50048]	Loss: 0.1876
Training Epoch: 59 [12032/50048]	Loss: 0.1625
Training Epoch: 59 [12160/50048]	Loss: 0.1998
Training Epoch: 59 [12288/50048]	Loss: 0.1471
Training Epoch: 59 [12416/50048]	Loss: 0.1750
Training Epoch: 59 [12544/50048]	Loss: 0.0997
Training Epoch: 59 [12672/50048]	Loss: 0.2068
Training Epoch: 59 [12800/50048]	Loss: 0.2023
Training Epoch: 59 [12928/50048]	Loss: 0.2675
Training Epoch: 59 [13056/50048]	Loss: 0.1578
Training Epoch: 59 [13184/50048]	Loss: 0.1919
Training Epoch: 59 [13312/50048]	Loss: 0.2691
Training Epoch: 59 [13440/50048]	Loss: 0.2466
Training Epoch: 59 [13568/50048]	Loss: 0.1959
Training Epoch: 59 [13696/50048]	Loss: 0.2115
Training Epoch: 59 [13824/50048]	Loss: 0.2083
Training Epoch: 59 [13952/50048]	Loss: 0.2072
Training Epoch: 59 [14080/50048]	Loss: 0.2506
Training Epoch: 59 [14208/50048]	Loss: 0.1219
Training Epoch: 59 [14336/50048]	Loss: 0.2354
Training Epoch: 59 [14464/50048]	Loss: 0.1833
Training Epoch: 59 [14592/50048]	Loss: 0.1651
Training Epoch: 59 [14720/50048]	Loss: 0.2126
Training Epoch: 59 [14848/50048]	Loss: 0.1672
Training Epoch: 59 [14976/50048]	Loss: 0.1570
Training Epoch: 59 [15104/50048]	Loss: 0.2535
Training Epoch: 59 [15232/50048]	Loss: 0.3024
Training Epoch: 59 [15360/50048]	Loss: 0.2044
Training Epoch: 59 [15488/50048]	Loss: 0.1363
Training Epoch: 59 [15616/50048]	Loss: 0.2172
Training Epoch: 59 [15744/50048]	Loss: 0.1751
Training Epoch: 59 [15872/50048]	Loss: 0.2097
Training Epoch: 59 [16000/50048]	Loss: 0.1365
Training Epoch: 59 [16128/50048]	Loss: 0.2129
Training Epoch: 59 [16256/50048]	Loss: 0.2268
Training Epoch: 59 [16384/50048]	Loss: 0.1301
Training Epoch: 59 [16512/50048]	Loss: 0.1874
Training Epoch: 59 [16640/50048]	Loss: 0.2279
Training Epoch: 59 [16768/50048]	Loss: 0.1635
Training Epoch: 59 [16896/50048]	Loss: 0.1948
Training Epoch: 59 [17024/50048]	Loss: 0.1227
Training Epoch: 59 [17152/50048]	Loss: 0.1163
Training Epoch: 59 [17280/50048]	Loss: 0.1117
Training Epoch: 59 [17408/50048]	Loss: 0.1323
Training Epoch: 59 [17536/50048]	Loss: 0.1854
Training Epoch: 59 [17664/50048]	Loss: 0.2276
Training Epoch: 59 [17792/50048]	Loss: 0.2775
Training Epoch: 59 [17920/50048]	Loss: 0.1530
Training Epoch: 59 [18048/50048]	Loss: 0.2126
Training Epoch: 59 [18176/50048]	Loss: 0.3548
Training Epoch: 59 [18304/50048]	Loss: 0.1484
Training Epoch: 59 [18432/50048]	Loss: 0.1534
Training Epoch: 59 [18560/50048]	Loss: 0.1521
Training Epoch: 59 [18688/50048]	Loss: 0.1530
Training Epoch: 59 [18816/50048]	Loss: 0.1618
Training Epoch: 59 [18944/50048]	Loss: 0.1847
Training Epoch: 59 [19072/50048]	Loss: 0.2387
Training Epoch: 59 [19200/50048]	Loss: 0.2464
Training Epoch: 59 [19328/50048]	Loss: 0.2662
Training Epoch: 59 [19456/50048]	Loss: 0.1809
Training Epoch: 59 [19584/50048]	Loss: 0.2561
Training Epoch: 59 [19712/50048]	Loss: 0.2417
Training Epoch: 59 [19840/50048]	Loss: 0.2695
Training Epoch: 59 [19968/50048]	Loss: 0.1710
Training Epoch: 59 [20096/50048]	Loss: 0.3187
Training Epoch: 59 [20224/50048]	Loss: 0.2019
Training Epoch: 59 [20352/50048]	Loss: 0.2017
Training Epoch: 59 [20480/50048]	Loss: 0.1315
Training Epoch: 59 [20608/50048]	Loss: 0.1954
Training Epoch: 59 [20736/50048]	Loss: 0.2184
Training Epoch: 59 [20864/50048]	Loss: 0.2137
Training Epoch: 59 [20992/50048]	Loss: 0.2802
Training Epoch: 59 [21120/50048]	Loss: 0.3532
Training Epoch: 59 [21248/50048]	Loss: 0.2454
Training Epoch: 59 [21376/50048]	Loss: 0.2121
Training Epoch: 59 [21504/50048]	Loss: 0.1438
Training Epoch: 59 [21632/50048]	Loss: 0.3624
Training Epoch: 59 [21760/50048]	Loss: 0.1121
Training Epoch: 59 [21888/50048]	Loss: 0.1418
Training Epoch: 59 [22016/50048]	Loss: 0.2436
Training Epoch: 59 [22144/50048]	Loss: 0.1201
Training Epoch: 59 [22272/50048]	Loss: 0.2084
Training Epoch: 59 [22400/50048]	Loss: 0.1824
Training Epoch: 59 [22528/50048]	Loss: 0.2162
Training Epoch: 59 [22656/50048]	Loss: 0.2326
Training Epoch: 59 [22784/50048]	Loss: 0.2149
Training Epoch: 59 [22912/50048]	Loss: 0.2188
Training Epoch: 59 [23040/50048]	Loss: 0.1700
Training Epoch: 59 [23168/50048]	Loss: 0.2762
Training Epoch: 59 [23296/50048]	Loss: 0.1876
Training Epoch: 59 [23424/50048]	Loss: 0.1443
Training Epoch: 59 [23552/50048]	Loss: 0.2079
Training Epoch: 59 [23680/50048]	Loss: 0.2709
Training Epoch: 59 [23808/50048]	Loss: 0.1191
Training Epoch: 59 [23936/50048]	Loss: 0.3170
Training Epoch: 59 [24064/50048]	Loss: 0.2697
Training Epoch: 59 [24192/50048]	Loss: 0.2296
Training Epoch: 59 [24320/50048]	Loss: 0.2170
Training Epoch: 59 [24448/50048]	Loss: 0.2696
Training Epoch: 59 [24576/50048]	Loss: 0.1927
Training Epoch: 59 [24704/50048]	Loss: 0.3433
Training Epoch: 59 [24832/50048]	Loss: 0.2367
Training Epoch: 59 [24960/50048]	Loss: 0.1254
Training Epoch: 59 [25088/50048]	Loss: 0.2689
Training Epoch: 59 [25216/50048]	Loss: 0.1803
Training Epoch: 59 [25344/50048]	Loss: 0.1592
Training Epoch: 59 [25472/50048]	Loss: 0.0815
Training Epoch: 59 [25600/50048]	Loss: 0.1609
Training Epoch: 59 [25728/50048]	Loss: 0.1408
Training Epoch: 59 [25856/50048]	Loss: 0.1697
Training Epoch: 59 [25984/50048]	Loss: 0.1881
Training Epoch: 59 [26112/50048]	Loss: 0.2353
Training Epoch: 59 [26240/50048]	Loss: 0.2197
Training Epoch: 59 [26368/50048]	Loss: 0.1448
Training Epoch: 59 [26496/50048]	Loss: 0.1383
Training Epoch: 59 [26624/50048]	Loss: 0.1847
Training Epoch: 59 [26752/50048]	Loss: 0.2272
Training Epoch: 59 [26880/50048]	Loss: 0.1897
Training Epoch: 59 [27008/50048]	Loss: 0.1739
Training Epoch: 59 [27136/50048]	Loss: 0.1572
Training Epoch: 59 [27264/50048]	Loss: 0.1788
Training Epoch: 59 [27392/50048]	Loss: 0.2513
Training Epoch: 59 [27520/50048]	Loss: 0.1924
Training Epoch: 59 [27648/50048]	Loss: 0.1993
Training Epoch: 59 [27776/50048]	Loss: 0.1179
Training Epoch: 59 [27904/50048]	Loss: 0.2292
Training Epoch: 59 [28032/50048]	Loss: 0.2121
Training Epoch: 59 [28160/50048]	Loss: 0.1888
Training Epoch: 59 [28288/50048]	Loss: 0.1586
Training Epoch: 59 [28416/50048]	Loss: 0.1957
Training Epoch: 59 [28544/50048]	Loss: 0.2274
Training Epoch: 59 [28672/50048]	Loss: 0.2037
Training Epoch: 59 [28800/50048]	Loss: 0.2136
Training Epoch: 59 [28928/50048]	Loss: 0.1965
Training Epoch: 59 [29056/50048]	Loss: 0.1272
Training Epoch: 59 [29184/50048]	Loss: 0.2882
Training Epoch: 59 [29312/50048]	Loss: 0.1966
Training Epoch: 59 [29440/50048]	Loss: 0.1987
Training Epoch: 59 [29568/50048]	Loss: 0.2632
Training Epoch: 59 [29696/50048]	Loss: 0.2141
Training Epoch: 59 [29824/50048]	Loss: 0.2255
Training Epoch: 59 [29952/50048]	Loss: 0.2373
Training Epoch: 59 [30080/50048]	Loss: 0.1569
Training Epoch: 59 [30208/50048]	Loss: 0.2030
Training Epoch: 59 [30336/50048]	Loss: 0.2336
Training Epoch: 59 [30464/50048]	Loss: 0.2712
Training Epoch: 59 [30592/50048]	Loss: 0.1932
Training Epoch: 59 [30720/50048]	Loss: 0.2562
Training Epoch: 59 [30848/50048]	Loss: 0.1321
Training Epoch: 59 [30976/50048]	Loss: 0.2480
Training Epoch: 59 [31104/50048]	Loss: 0.2576
Training Epoch: 59 [31232/50048]	Loss: 0.2379
Training Epoch: 59 [31360/50048]	Loss: 0.1311
Training Epoch: 59 [31488/50048]	Loss: 0.1613
Training Epoch: 59 [31616/50048]	Loss: 0.2271
Training Epoch: 59 [31744/50048]	Loss: 0.1983
Training Epoch: 59 [31872/50048]	Loss: 0.2230
Training Epoch: 59 [32000/50048]	Loss: 0.2348
Training Epoch: 59 [32128/50048]	Loss: 0.1226
Training Epoch: 59 [32256/50048]	Loss: 0.1263
Training Epoch: 59 [32384/50048]	Loss: 0.2093
Training Epoch: 59 [32512/50048]	Loss: 0.1631
Training Epoch: 59 [32640/50048]	Loss: 0.1716
Training Epoch: 59 [32768/50048]	Loss: 0.1859
Training Epoch: 59 [32896/50048]	Loss: 0.2159
Training Epoch: 59 [33024/50048]	Loss: 0.1420
Training Epoch: 59 [33152/50048]	Loss: 0.1780
Training Epoch: 59 [33280/50048]	Loss: 0.2063
Training Epoch: 59 [33408/50048]	Loss: 0.2568
Training Epoch: 59 [33536/50048]	Loss: 0.2417
Training Epoch: 59 [33664/50048]	Loss: 0.3213
Training Epoch: 59 [33792/50048]	Loss: 0.1356
Training Epoch: 59 [33920/50048]	Loss: 0.1356
Training Epoch: 59 [34048/50048]	Loss: 0.2308
Training Epoch: 59 [34176/50048]	Loss: 0.2886
Training Epoch: 59 [34304/50048]	Loss: 0.1502
Training Epoch: 59 [34432/50048]	Loss: 0.2064
Training Epoch: 59 [34560/50048]	Loss: 0.2076
Training Epoch: 59 [34688/50048]	Loss: 0.2455
Training Epoch: 59 [34816/50048]	Loss: 0.1920
Training Epoch: 59 [34944/50048]	Loss: 0.2800
Training Epoch: 59 [35072/50048]	Loss: 0.2164
Training Epoch: 59 [35200/50048]	Loss: 0.1381
Training Epoch: 59 [35328/50048]	Loss: 0.1991
Training Epoch: 59 [35456/50048]	Loss: 0.1813
Training Epoch: 59 [35584/50048]	Loss: 0.2515
Training Epoch: 59 [35712/50048]	Loss: 0.3621
Training Epoch: 59 [35840/50048]	Loss: 0.2120
Training Epoch: 59 [35968/50048]	Loss: 0.2763
Training Epoch: 59 [36096/50048]	Loss: 0.2129
Training Epoch: 59 [36224/50048]	Loss: 0.2469
Training Epoch: 59 [36352/50048]	Loss: 0.1484
Training Epoch: 59 [36480/50048]	Loss: 0.2021
Training Epoch: 59 [36608/50048]	Loss: 0.2457
Training Epoch: 59 [36736/50048]	Loss: 0.2023
Training Epoch: 59 [36864/50048]	Loss: 0.2763
Training Epoch: 59 [36992/50048]	Loss: 0.1878
Training Epoch: 59 [37120/50048]	Loss: 0.2031
Training Epoch: 59 [37248/50048]	Loss: 0.2886
Training Epoch: 59 [37376/50048]	Loss: 0.2333
Training Epoch: 59 [37504/50048]	Loss: 0.1318
Training Epoch: 59 [37632/50048]	Loss: 0.1753
Training Epoch: 59 [37760/50048]	Loss: 0.2546
Training Epoch: 59 [37888/50048]	Loss: 0.1750
Training Epoch: 59 [38016/50048]	Loss: 0.1685
Training Epoch: 59 [38144/50048]	Loss: 0.2001
Training Epoch: 59 [38272/50048]	Loss: 0.2661
Training Epoch: 59 [38400/50048]	Loss: 0.2044
Training Epoch: 59 [38528/50048]	Loss: 0.1562
Training Epoch: 59 [38656/50048]	Loss: 0.2526
Training Epoch: 59 [38784/50048]	Loss: 0.1946
Training Epoch: 59 [38912/50048]	Loss: 0.1483
Training Epoch: 59 [39040/50048]	Loss: 0.2150
Training Epoch: 59 [39168/50048]	Loss: 0.1936
Training Epoch: 59 [39296/50048]	Loss: 0.1614
Training Epoch: 59 [39424/50048]	Loss: 0.2975
Training Epoch: 59 [39552/50048]	Loss: 0.2256
Training Epoch: 59 [39680/50048]	Loss: 0.3529
Training Epoch: 59 [39808/50048]	Loss: 0.1689
Training Epoch: 59 [39936/50048]	Loss: 0.1748
Training Epoch: 59 [40064/50048]	Loss: 0.2064
Training Epoch: 59 [40192/50048]	Loss: 0.1987
Training Epoch: 59 [40320/50048]	Loss: 0.3072
Training Epoch: 59 [40448/50048]	Loss: 0.2443
Training Epoch: 59 [40576/50048]	Loss: 0.1403
Training Epoch: 59 [40704/50048]	Loss: 0.2088
Training Epoch: 59 [40832/50048]	Loss: 0.3511
Training Epoch: 59 [40960/50048]	Loss: 0.2208
Training Epoch: 59 [41088/50048]	Loss: 0.1388
Training Epoch: 59 [41216/50048]	Loss: 0.3233
Training Epoch: 59 [41344/50048]	Loss: 0.1368
Training Epoch: 59 [41472/50048]	Loss: 0.1242
Training Epoch: 59 [41600/50048]	Loss: 0.1779
Training Epoch: 59 [41728/50048]	Loss: 0.3648
Training Epoch: 59 [41856/50048]	Loss: 0.1860
Training Epoch: 59 [41984/50048]	Loss: 0.2312
Training Epoch: 59 [42112/50048]	Loss: 0.3158
Training Epoch: 59 [42240/50048]	Loss: 0.1731
Training Epoch: 59 [42368/50048]	Loss: 0.2937
Training Epoch: 59 [42496/50048]	Loss: 0.2031
Training Epoch: 59 [42624/50048]	Loss: 0.1817
Training Epoch: 59 [42752/50048]	Loss: 0.1953
Training Epoch: 59 [42880/50048]	Loss: 0.2703
Training Epoch: 59 [43008/50048]	Loss: 0.1345
Training Epoch: 59 [43136/50048]	Loss: 0.1883
Training Epoch: 59 [43264/50048]	Loss: 0.1678
Training Epoch: 59 [43392/50048]	Loss: 0.1491
Training Epoch: 59 [43520/50048]	Loss: 0.1708
Training Epoch: 59 [43648/50048]	Loss: 0.1749
Training Epoch: 59 [43776/50048]	Loss: 0.1808
Training Epoch: 59 [43904/50048]	Loss: 0.2520
Training Epoch: 59 [44032/50048]	Loss: 0.2719
Training Epoch: 59 [44160/50048]	Loss: 0.1770
Training Epoch: 59 [44288/50048]	Loss: 0.1833
Training Epoch: 59 [44416/50048]	Loss: 0.2085
Training Epoch: 59 [44544/50048]	Loss: 0.1678
Training Epoch: 59 [44672/50048]	Loss: 0.1591
Training Epoch: 59 [44800/50048]	Loss: 0.2469
Training Epoch: 59 [44928/50048]	Loss: 0.2336
Training Epoch: 59 [45056/50048]	Loss: 0.1764
Training Epoch: 59 [45184/50048]	Loss: 0.2143
Training Epoch: 59 [45312/50048]	Loss: 0.2831
Training Epoch: 59 [45440/50048]	Loss: 0.2085
Training Epoch: 59 [45568/50048]	Loss: 0.1916
Training Epoch: 59 [45696/50048]	Loss: 0.1586
2022-12-06 05:07:33,788 [ZeusDataLoader(train)] train epoch 60 done: time=86.43 energy=10502.22
2022-12-06 05:07:33,789 [ZeusDataLoader(eval)] Epoch 60 begin.
Training Epoch: 59 [45824/50048]	Loss: 0.1377
Training Epoch: 59 [45952/50048]	Loss: 0.1473
Training Epoch: 59 [46080/50048]	Loss: 0.2047
Training Epoch: 59 [46208/50048]	Loss: 0.2241
Training Epoch: 59 [46336/50048]	Loss: 0.2110
Training Epoch: 59 [46464/50048]	Loss: 0.3207
Training Epoch: 59 [46592/50048]	Loss: 0.2497
Training Epoch: 59 [46720/50048]	Loss: 0.2200
Training Epoch: 59 [46848/50048]	Loss: 0.2150
Training Epoch: 59 [46976/50048]	Loss: 0.1938
Training Epoch: 59 [47104/50048]	Loss: 0.3511
Training Epoch: 59 [47232/50048]	Loss: 0.2482
Training Epoch: 59 [47360/50048]	Loss: 0.1925
Training Epoch: 59 [47488/50048]	Loss: 0.2046
Training Epoch: 59 [47616/50048]	Loss: 0.1699
Training Epoch: 59 [47744/50048]	Loss: 0.2030
Training Epoch: 59 [47872/50048]	Loss: 0.2746
Training Epoch: 59 [48000/50048]	Loss: 0.2723
Training Epoch: 59 [48128/50048]	Loss: 0.2489
Training Epoch: 59 [48256/50048]	Loss: 0.2389
Training Epoch: 59 [48384/50048]	Loss: 0.2734
Training Epoch: 59 [48512/50048]	Loss: 0.2419
Training Epoch: 59 [48640/50048]	Loss: 0.1251
Training Epoch: 59 [48768/50048]	Loss: 0.1840
Training Epoch: 59 [48896/50048]	Loss: 0.1940
Training Epoch: 59 [49024/50048]	Loss: 0.1302
Training Epoch: 59 [49152/50048]	Loss: 0.2515
Training Epoch: 59 [49280/50048]	Loss: 0.2128
Training Epoch: 59 [49408/50048]	Loss: 0.1580
Training Epoch: 59 [49536/50048]	Loss: 0.2358
Training Epoch: 59 [49664/50048]	Loss: 0.2282
Training Epoch: 59 [49792/50048]	Loss: 0.2126
Training Epoch: 59 [49920/50048]	Loss: 0.2325
Training Epoch: 59 [50048/50048]	Loss: 0.2708
2022-12-06 10:07:37.465 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:07:37,493 [ZeusDataLoader(eval)] eval epoch 60 done: time=3.69 energy=453.66
2022-12-06 05:07:37,493 [ZeusDataLoader(train)] Up to epoch 60: time=5413.96, energy=657177.72, cost=802310.07
2022-12-06 05:07:37,493 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:07:37,493 [ZeusDataLoader(train)] Expected next epoch: time=5503.76, energy=667975.73, cost=815566.45
2022-12-06 05:07:37,494 [ZeusDataLoader(train)] Epoch 61 begin.
Validation Epoch: 59, Average loss: 0.0165, Accuracy: 0.6301
2022-12-06 05:07:37,671 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:07:37,672 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:07:37.673 [ZeusMonitor] Monitor started.
2022-12-06 10:07:37.674 [ZeusMonitor] Running indefinitely. 2022-12-06 10:07:37.674 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:07:37.674 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e61+gpu0.power.log
Training Epoch: 60 [128/50048]	Loss: 0.1412
Training Epoch: 60 [256/50048]	Loss: 0.1519
Training Epoch: 60 [384/50048]	Loss: 0.2258
Training Epoch: 60 [512/50048]	Loss: 0.1829
Training Epoch: 60 [640/50048]	Loss: 0.1021
Training Epoch: 60 [768/50048]	Loss: 0.2392
Training Epoch: 60 [896/50048]	Loss: 0.1261
Training Epoch: 60 [1024/50048]	Loss: 0.2097
Training Epoch: 60 [1152/50048]	Loss: 0.1706
Training Epoch: 60 [1280/50048]	Loss: 0.1958
Training Epoch: 60 [1408/50048]	Loss: 0.1892
Training Epoch: 60 [1536/50048]	Loss: 0.2170
Training Epoch: 60 [1664/50048]	Loss: 0.1587
Training Epoch: 60 [1792/50048]	Loss: 0.1750
Training Epoch: 60 [1920/50048]	Loss: 0.1778
Training Epoch: 60 [2048/50048]	Loss: 0.1941
Training Epoch: 60 [2176/50048]	Loss: 0.1938
Training Epoch: 60 [2304/50048]	Loss: 0.1255
Training Epoch: 60 [2432/50048]	Loss: 0.1642
Training Epoch: 60 [2560/50048]	Loss: 0.1493
Training Epoch: 60 [2688/50048]	Loss: 0.1690
Training Epoch: 60 [2816/50048]	Loss: 0.1483
Training Epoch: 60 [2944/50048]	Loss: 0.1039
Training Epoch: 60 [3072/50048]	Loss: 0.1455
Training Epoch: 60 [3200/50048]	Loss: 0.2561
Training Epoch: 60 [3328/50048]	Loss: 0.0651
Training Epoch: 60 [3456/50048]	Loss: 0.1317
Training Epoch: 60 [3584/50048]	Loss: 0.1878
Training Epoch: 60 [3712/50048]	Loss: 0.1349
Training Epoch: 60 [3840/50048]	Loss: 0.1079
Training Epoch: 60 [3968/50048]	Loss: 0.0864
Training Epoch: 60 [4096/50048]	Loss: 0.1797
Training Epoch: 60 [4224/50048]	Loss: 0.1606
Training Epoch: 60 [4352/50048]	Loss: 0.1303
Training Epoch: 60 [4480/50048]	Loss: 0.1829
Training Epoch: 60 [4608/50048]	Loss: 0.1926
Training Epoch: 60 [4736/50048]	Loss: 0.1994
Training Epoch: 60 [4864/50048]	Loss: 0.1349
Training Epoch: 60 [4992/50048]	Loss: 0.1500
Training Epoch: 60 [5120/50048]	Loss: 0.1440
Training Epoch: 60 [5248/50048]	Loss: 0.2183
Training Epoch: 60 [5376/50048]	Loss: 0.1333
Training Epoch: 60 [5504/50048]	Loss: 0.2311
Training Epoch: 60 [5632/50048]	Loss: 0.1431
Training Epoch: 60 [5760/50048]	Loss: 0.1586
Training Epoch: 60 [5888/50048]	Loss: 0.1102
Training Epoch: 60 [6016/50048]	Loss: 0.1415
Training Epoch: 60 [6144/50048]	Loss: 0.2226
Training Epoch: 60 [6272/50048]	Loss: 0.2285
Training Epoch: 60 [6400/50048]	Loss: 0.1226
Training Epoch: 60 [6528/50048]	Loss: 0.1771
Training Epoch: 60 [6656/50048]	Loss: 0.1304
Training Epoch: 60 [6784/50048]	Loss: 0.0940
Training Epoch: 60 [6912/50048]	Loss: 0.1453
Training Epoch: 60 [7040/50048]	Loss: 0.3167
Training Epoch: 60 [7168/50048]	Loss: 0.1390
Training Epoch: 60 [7296/50048]	Loss: 0.1732
Training Epoch: 60 [7424/50048]	Loss: 0.2073
Training Epoch: 60 [7552/50048]	Loss: 0.2562
Training Epoch: 60 [7680/50048]	Loss: 0.2061
Training Epoch: 60 [7808/50048]	Loss: 0.1276
Training Epoch: 60 [7936/50048]	Loss: 0.2527
Training Epoch: 60 [8064/50048]	Loss: 0.2379
Training Epoch: 60 [8192/50048]	Loss: 0.1443
Training Epoch: 60 [8320/50048]	Loss: 0.1805
Training Epoch: 60 [8448/50048]	Loss: 0.1504
Training Epoch: 60 [8576/50048]	Loss: 0.1831
Training Epoch: 60 [8704/50048]	Loss: 0.1599
Training Epoch: 60 [8832/50048]	Loss: 0.1701
Training Epoch: 60 [8960/50048]	Loss: 0.1612
Training Epoch: 60 [9088/50048]	Loss: 0.1841
Training Epoch: 60 [9216/50048]	Loss: 0.1164
Training Epoch: 60 [9344/50048]	Loss: 0.1545
Training Epoch: 60 [9472/50048]	Loss: 0.1696
Training Epoch: 60 [9600/50048]	Loss: 0.2057
Training Epoch: 60 [9728/50048]	Loss: 0.1946
Training Epoch: 60 [9856/50048]	Loss: 0.1642
Training Epoch: 60 [9984/50048]	Loss: 0.2176
Training Epoch: 60 [10112/50048]	Loss: 0.1556
Training Epoch: 60 [10240/50048]	Loss: 0.2640
Training Epoch: 60 [10368/50048]	Loss: 0.1994
Training Epoch: 60 [10496/50048]	Loss: 0.1425
Training Epoch: 60 [10624/50048]	Loss: 0.2586
Training Epoch: 60 [10752/50048]	Loss: 0.1575
Training Epoch: 60 [10880/50048]	Loss: 0.1224
Training Epoch: 60 [11008/50048]	Loss: 0.1431
Training Epoch: 60 [11136/50048]	Loss: 0.2152
Training Epoch: 60 [11264/50048]	Loss: 0.1111
Training Epoch: 60 [11392/50048]	Loss: 0.1989
Training Epoch: 60 [11520/50048]	Loss: 0.1428
Training Epoch: 60 [11648/50048]	Loss: 0.0897
Training Epoch: 60 [11776/50048]	Loss: 0.1592
Training Epoch: 60 [11904/50048]	Loss: 0.1283
Training Epoch: 60 [12032/50048]	Loss: 0.1893
Training Epoch: 60 [12160/50048]	Loss: 0.1445
Training Epoch: 60 [12288/50048]	Loss: 0.1622
Training Epoch: 60 [12416/50048]	Loss: 0.1388
Training Epoch: 60 [12544/50048]	Loss: 0.2120
Training Epoch: 60 [12672/50048]	Loss: 0.1747
Training Epoch: 60 [12800/50048]	Loss: 0.1987
Training Epoch: 60 [12928/50048]	Loss: 0.1826
Training Epoch: 60 [13056/50048]	Loss: 0.2924
Training Epoch: 60 [13184/50048]	Loss: 0.1596
Training Epoch: 60 [13312/50048]	Loss: 0.0896
Training Epoch: 60 [13440/50048]	Loss: 0.1772
Training Epoch: 60 [13568/50048]	Loss: 0.2594
Training Epoch: 60 [13696/50048]	Loss: 0.2151
Training Epoch: 60 [13824/50048]	Loss: 0.2060
Training Epoch: 60 [13952/50048]	Loss: 0.2557
Training Epoch: 60 [14080/50048]	Loss: 0.1759
Training Epoch: 60 [14208/50048]	Loss: 0.2025
Training Epoch: 60 [14336/50048]	Loss: 0.2603
Training Epoch: 60 [14464/50048]	Loss: 0.1742
Training Epoch: 60 [14592/50048]	Loss: 0.1198
Training Epoch: 60 [14720/50048]	Loss: 0.1647
Training Epoch: 60 [14848/50048]	Loss: 0.2062
Training Epoch: 60 [14976/50048]	Loss: 0.2615
Training Epoch: 60 [15104/50048]	Loss: 0.1981
Training Epoch: 60 [15232/50048]	Loss: 0.2157
Training Epoch: 60 [15360/50048]	Loss: 0.1734
Training Epoch: 60 [15488/50048]	Loss: 0.1296
Training Epoch: 60 [15616/50048]	Loss: 0.2080
Training Epoch: 60 [15744/50048]	Loss: 0.2575
Training Epoch: 60 [15872/50048]	Loss: 0.2101
Training Epoch: 60 [16000/50048]	Loss: 0.1564
Training Epoch: 60 [16128/50048]	Loss: 0.1939
Training Epoch: 60 [16256/50048]	Loss: 0.1818
Training Epoch: 60 [16384/50048]	Loss: 0.1394
Training Epoch: 60 [16512/50048]	Loss: 0.2066
Training Epoch: 60 [16640/50048]	Loss: 0.1927
Training Epoch: 60 [16768/50048]	Loss: 0.1694
Training Epoch: 60 [16896/50048]	Loss: 0.1608
Training Epoch: 60 [17024/50048]	Loss: 0.2589
Training Epoch: 60 [17152/50048]	Loss: 0.1594
Training Epoch: 60 [17280/50048]	Loss: 0.2220
Training Epoch: 60 [17408/50048]	Loss: 0.1561
Training Epoch: 60 [17536/50048]	Loss: 0.1801
Training Epoch: 60 [17664/50048]	Loss: 0.2160
Training Epoch: 60 [17792/50048]	Loss: 0.2119
Training Epoch: 60 [17920/50048]	Loss: 0.2784
Training Epoch: 60 [18048/50048]	Loss: 0.1424
Training Epoch: 60 [18176/50048]	Loss: 0.1817
Training Epoch: 60 [18304/50048]	Loss: 0.2362
Training Epoch: 60 [18432/50048]	Loss: 0.1460
Training Epoch: 60 [18560/50048]	Loss: 0.2334
Training Epoch: 60 [18688/50048]	Loss: 0.2054
Training Epoch: 60 [18816/50048]	Loss: 0.1463
Training Epoch: 60 [18944/50048]	Loss: 0.1559
Training Epoch: 60 [19072/50048]	Loss: 0.2000
Training Epoch: 60 [19200/50048]	Loss: 0.2003
Training Epoch: 60 [19328/50048]	Loss: 0.2671
Training Epoch: 60 [19456/50048]	Loss: 0.1607
Training Epoch: 60 [19584/50048]	Loss: 0.1370
Training Epoch: 60 [19712/50048]	Loss: 0.1449
Training Epoch: 60 [19840/50048]	Loss: 0.1695
Training Epoch: 60 [19968/50048]	Loss: 0.1503
Training Epoch: 60 [20096/50048]	Loss: 0.2448
Training Epoch: 60 [20224/50048]	Loss: 0.1692
Training Epoch: 60 [20352/50048]	Loss: 0.2442
Training Epoch: 60 [20480/50048]	Loss: 0.1821
Training Epoch: 60 [20608/50048]	Loss: 0.2443
Training Epoch: 60 [20736/50048]	Loss: 0.2328
Training Epoch: 60 [20864/50048]	Loss: 0.1739
Training Epoch: 60 [20992/50048]	Loss: 0.2420
Training Epoch: 60 [21120/50048]	Loss: 0.1638
Training Epoch: 60 [21248/50048]	Loss: 0.1834
Training Epoch: 60 [21376/50048]	Loss: 0.3380
Training Epoch: 60 [21504/50048]	Loss: 0.1240
Training Epoch: 60 [21632/50048]	Loss: 0.1678
Training Epoch: 60 [21760/50048]	Loss: 0.1386
Training Epoch: 60 [21888/50048]	Loss: 0.2582
Training Epoch: 60 [22016/50048]	Loss: 0.2271
Training Epoch: 60 [22144/50048]	Loss: 0.1281
Training Epoch: 60 [22272/50048]	Loss: 0.1501
Training Epoch: 60 [22400/50048]	Loss: 0.2306
Training Epoch: 60 [22528/50048]	Loss: 0.1978
Training Epoch: 60 [22656/50048]	Loss: 0.1821
Training Epoch: 60 [22784/50048]	Loss: 0.1841
Training Epoch: 60 [22912/50048]	Loss: 0.2408
Training Epoch: 60 [23040/50048]	Loss: 0.1940
Training Epoch: 60 [23168/50048]	Loss: 0.2458
Training Epoch: 60 [23296/50048]	Loss: 0.2490
Training Epoch: 60 [23424/50048]	Loss: 0.1951
Training Epoch: 60 [23552/50048]	Loss: 0.1718
Training Epoch: 60 [23680/50048]	Loss: 0.1658
Training Epoch: 60 [23808/50048]	Loss: 0.1014
Training Epoch: 60 [23936/50048]	Loss: 0.1064
Training Epoch: 60 [24064/50048]	Loss: 0.2597
Training Epoch: 60 [24192/50048]	Loss: 0.1794
Training Epoch: 60 [24320/50048]	Loss: 0.1189
Training Epoch: 60 [24448/50048]	Loss: 0.2318
Training Epoch: 60 [24576/50048]	Loss: 0.2325
Training Epoch: 60 [24704/50048]	Loss: 0.1672
Training Epoch: 60 [24832/50048]	Loss: 0.1415
Training Epoch: 60 [24960/50048]	Loss: 0.1538
Training Epoch: 60 [25088/50048]	Loss: 0.1458
Training Epoch: 60 [25216/50048]	Loss: 0.1757
Training Epoch: 60 [25344/50048]	Loss: 0.2832
Training Epoch: 60 [25472/50048]	Loss: 0.2285
Training Epoch: 60 [25600/50048]	Loss: 0.2669
Training Epoch: 60 [25728/50048]	Loss: 0.2114
Training Epoch: 60 [25856/50048]	Loss: 0.1521
Training Epoch: 60 [25984/50048]	Loss: 0.1405
Training Epoch: 60 [26112/50048]	Loss: 0.1591
Training Epoch: 60 [26240/50048]	Loss: 0.1432
Training Epoch: 60 [26368/50048]	Loss: 0.2355
Training Epoch: 60 [26496/50048]	Loss: 0.1679
Training Epoch: 60 [26624/50048]	Loss: 0.2474
Training Epoch: 60 [26752/50048]	Loss: 0.2671
Training Epoch: 60 [26880/50048]	Loss: 0.2099
Training Epoch: 60 [27008/50048]	Loss: 0.1611
Training Epoch: 60 [27136/50048]	Loss: 0.1470
Training Epoch: 60 [27264/50048]	Loss: 0.2098
Training Epoch: 60 [27392/50048]	Loss: 0.2383
Training Epoch: 60 [27520/50048]	Loss: 0.1533
Training Epoch: 60 [27648/50048]	Loss: 0.3366
Training Epoch: 60 [27776/50048]	Loss: 0.3192
Training Epoch: 60 [27904/50048]	Loss: 0.1236
Training Epoch: 60 [28032/50048]	Loss: 0.1906
Training Epoch: 60 [28160/50048]	Loss: 0.2233
Training Epoch: 60 [28288/50048]	Loss: 0.1930
Training Epoch: 60 [28416/50048]	Loss: 0.2090
Training Epoch: 60 [28544/50048]	Loss: 0.2350
Training Epoch: 60 [28672/50048]	Loss: 0.1844
Training Epoch: 60 [28800/50048]	Loss: 0.1895
Training Epoch: 60 [28928/50048]	Loss: 0.1683
Training Epoch: 60 [29056/50048]	Loss: 0.1944
Training Epoch: 60 [29184/50048]	Loss: 0.1452
Training Epoch: 60 [29312/50048]	Loss: 0.2671
Training Epoch: 60 [29440/50048]	Loss: 0.1427
Training Epoch: 60 [29568/50048]	Loss: 0.2158
Training Epoch: 60 [29696/50048]	Loss: 0.2465
Training Epoch: 60 [29824/50048]	Loss: 0.1707
Training Epoch: 60 [29952/50048]	Loss: 0.2132
Training Epoch: 60 [30080/50048]	Loss: 0.1091
Training Epoch: 60 [30208/50048]	Loss: 0.1727
Training Epoch: 60 [30336/50048]	Loss: 0.1343
Training Epoch: 60 [30464/50048]	Loss: 0.1617
Training Epoch: 60 [30592/50048]	Loss: 0.1549
Training Epoch: 60 [30720/50048]	Loss: 0.1557
Training Epoch: 60 [30848/50048]	Loss: 0.2378
Training Epoch: 60 [30976/50048]	Loss: 0.2508
Training Epoch: 60 [31104/50048]	Loss: 0.1858
Training Epoch: 60 [31232/50048]	Loss: 0.2598
Training Epoch: 60 [31360/50048]	Loss: 0.1737
Training Epoch: 60 [31488/50048]	Loss: 0.1854
Training Epoch: 60 [31616/50048]	Loss: 0.2550
Training Epoch: 60 [31744/50048]	Loss: 0.1872
Training Epoch: 60 [31872/50048]	Loss: 0.1632
Training Epoch: 60 [32000/50048]	Loss: 0.1779
Training Epoch: 60 [32128/50048]	Loss: 0.1915
Training Epoch: 60 [32256/50048]	Loss: 0.2810
Training Epoch: 60 [32384/50048]	Loss: 0.1577
Training Epoch: 60 [32512/50048]	Loss: 0.2099
Training Epoch: 60 [32640/50048]	Loss: 0.2542
Training Epoch: 60 [32768/50048]	Loss: 0.2236
Training Epoch: 60 [32896/50048]	Loss: 0.1660
Training Epoch: 60 [33024/50048]	Loss: 0.2317
Training Epoch: 60 [33152/50048]	Loss: 0.1807
Training Epoch: 60 [33280/50048]	Loss: 0.1532
Training Epoch: 60 [33408/50048]	Loss: 0.1928
Training Epoch: 60 [33536/50048]	Loss: 0.2339
Training Epoch: 60 [33664/50048]	Loss: 0.1252
Training Epoch: 60 [33792/50048]	Loss: 0.1567
Training Epoch: 60 [33920/50048]	Loss: 0.1940
Training Epoch: 60 [34048/50048]	Loss: 0.2617
Training Epoch: 60 [34176/50048]	Loss: 0.1385
Training Epoch: 60 [34304/50048]	Loss: 0.2554
Training Epoch: 60 [34432/50048]	Loss: 0.2438
Training Epoch: 60 [34560/50048]	Loss: 0.2388
Training Epoch: 60 [34688/50048]	Loss: 0.2054
Training Epoch: 60 [34816/50048]	Loss: 0.2004
Training Epoch: 60 [34944/50048]	Loss: 0.1923
Training Epoch: 60 [35072/50048]	Loss: 0.1988
Training Epoch: 60 [35200/50048]	Loss: 0.1797
Training Epoch: 60 [35328/50048]	Loss: 0.2758
Training Epoch: 60 [35456/50048]	Loss: 0.2010
Training Epoch: 60 [35584/50048]	Loss: 0.1373
Training Epoch: 60 [35712/50048]	Loss: 0.1997
Training Epoch: 60 [35840/50048]	Loss: 0.2045
Training Epoch: 60 [35968/50048]	Loss: 0.1283
Training Epoch: 60 [36096/50048]	Loss: 0.1875
Training Epoch: 60 [36224/50048]	Loss: 0.2549
Training Epoch: 60 [36352/50048]	Loss: 0.1850
Training Epoch: 60 [36480/50048]	Loss: 0.2521
Training Epoch: 60 [36608/50048]	Loss: 0.2304
Training Epoch: 60 [36736/50048]	Loss: 0.1756
Training Epoch: 60 [36864/50048]	Loss: 0.1665
Training Epoch: 60 [36992/50048]	Loss: 0.1911
Training Epoch: 60 [37120/50048]	Loss: 0.2134
Training Epoch: 60 [37248/50048]	Loss: 0.2156
Training Epoch: 60 [37376/50048]	Loss: 0.2950
Training Epoch: 60 [37504/50048]	Loss: 0.1005
Training Epoch: 60 [37632/50048]	Loss: 0.1981
Training Epoch: 60 [37760/50048]	Loss: 0.2900
Training Epoch: 60 [37888/50048]	Loss: 0.2168
Training Epoch: 60 [38016/50048]	Loss: 0.2080
Training Epoch: 60 [38144/50048]	Loss: 0.1554
Training Epoch: 60 [38272/50048]	Loss: 0.2673
Training Epoch: 60 [38400/50048]	Loss: 0.2600
Training Epoch: 60 [38528/50048]	Loss: 0.2951
Training Epoch: 60 [38656/50048]	Loss: 0.1282
Training Epoch: 60 [38784/50048]	Loss: 0.1913
Training Epoch: 60 [38912/50048]	Loss: 0.2998
Training Epoch: 60 [39040/50048]	Loss: 0.1744
Training Epoch: 60 [39168/50048]	Loss: 0.2442
Training Epoch: 60 [39296/50048]	Loss: 0.1743
Training Epoch: 60 [39424/50048]	Loss: 0.2970
Training Epoch: 60 [39552/50048]	Loss: 0.2537
Training Epoch: 60 [39680/50048]	Loss: 0.1658
Training Epoch: 60 [39808/50048]	Loss: 0.3253
Training Epoch: 60 [39936/50048]	Loss: 0.1646
Training Epoch: 60 [40064/50048]	Loss: 0.2836
Training Epoch: 60 [40192/50048]	Loss: 0.2197
Training Epoch: 60 [40320/50048]	Loss: 0.2493
Training Epoch: 60 [40448/50048]	Loss: 0.3254
Training Epoch: 60 [40576/50048]	Loss: 0.2144
Training Epoch: 60 [40704/50048]	Loss: 0.1664
Training Epoch: 60 [40832/50048]	Loss: 0.2307
Training Epoch: 60 [40960/50048]	Loss: 0.1965
Training Epoch: 60 [41088/50048]	Loss: 0.1339
Training Epoch: 60 [41216/50048]	Loss: 0.1902
Training Epoch: 60 [41344/50048]	Loss: 0.2030
Training Epoch: 60 [41472/50048]	Loss: 0.1734
Training Epoch: 60 [41600/50048]	Loss: 0.2305
Training Epoch: 60 [41728/50048]	Loss: 0.1450
Training Epoch: 60 [41856/50048]	Loss: 0.2302
Training Epoch: 60 [41984/50048]	Loss: 0.2646
Training Epoch: 60 [42112/50048]	Loss: 0.1983
Training Epoch: 60 [42240/50048]	Loss: 0.2560
Training Epoch: 60 [42368/50048]	Loss: 0.2738
Training Epoch: 60 [42496/50048]	Loss: 0.2539
Training Epoch: 60 [42624/50048]	Loss: 0.2355
Training Epoch: 60 [42752/50048]	Loss: 0.3058
Training Epoch: 60 [42880/50048]	Loss: 0.2341
Training Epoch: 60 [43008/50048]	Loss: 0.1895
Training Epoch: 60 [43136/50048]	Loss: 0.2997
Training Epoch: 60 [43264/50048]	Loss: 0.2217
Training Epoch: 60 [43392/50048]	Loss: 0.2493
Training Epoch: 60 [43520/50048]	Loss: 0.3026
Training Epoch: 60 [43648/50048]	Loss: 0.1264
Training Epoch: 60 [43776/50048]	Loss: 0.2546
Training Epoch: 60 [43904/50048]	Loss: 0.2850
Training Epoch: 60 [44032/50048]	Loss: 0.2369
Training Epoch: 60 [44160/50048]	Loss: 0.2058
Training Epoch: 60 [44288/50048]	Loss: 0.2601
Training Epoch: 60 [44416/50048]	Loss: 0.1742
Training Epoch: 60 [44544/50048]	Loss: 0.1598
Training Epoch: 60 [44672/50048]	Loss: 0.1619
Training Epoch: 60 [44800/50048]	Loss: 0.1934
Training Epoch: 60 [44928/50048]	Loss: 0.1314
Training Epoch: 60 [45056/50048]	Loss: 0.1542
Training Epoch: 60 [45184/50048]	Loss: 0.0793
Training Epoch: 60 [45312/50048]	Loss: 0.2572
Training Epoch: 60 [45440/50048]	Loss: 0.2102
Training Epoch: 60 [45568/50048]	Loss: 0.1634
Training Epoch: 60 [45696/50048]	Loss: 0.1912
2022-12-06 05:09:04,067 [ZeusDataLoader(train)] train epoch 61 done: time=86.56 energy=10513.81
2022-12-06 05:09:04,068 [ZeusDataLoader(eval)] Epoch 61 begin.
Training Epoch: 60 [45824/50048]	Loss: 0.1854
Training Epoch: 60 [45952/50048]	Loss: 0.1968
Training Epoch: 60 [46080/50048]	Loss: 0.1574
Training Epoch: 60 [46208/50048]	Loss: 0.2375
Training Epoch: 60 [46336/50048]	Loss: 0.2248
Training Epoch: 60 [46464/50048]	Loss: 0.2037
Training Epoch: 60 [46592/50048]	Loss: 0.2548
Training Epoch: 60 [46720/50048]	Loss: 0.1823
Training Epoch: 60 [46848/50048]	Loss: 0.2119
Training Epoch: 60 [46976/50048]	Loss: 0.1737
Training Epoch: 60 [47104/50048]	Loss: 0.2020
Training Epoch: 60 [47232/50048]	Loss: 0.2809
Training Epoch: 60 [47360/50048]	Loss: 0.2285
Training Epoch: 60 [47488/50048]	Loss: 0.1827
Training Epoch: 60 [47616/50048]	Loss: 0.1862
Training Epoch: 60 [47744/50048]	Loss: 0.1579
Training Epoch: 60 [47872/50048]	Loss: 0.1688
Training Epoch: 60 [48000/50048]	Loss: 0.2820
Training Epoch: 60 [48128/50048]	Loss: 0.2706
Training Epoch: 60 [48256/50048]	Loss: 0.3057
Training Epoch: 60 [48384/50048]	Loss: 0.1865
Training Epoch: 60 [48512/50048]	Loss: 0.2196
Training Epoch: 60 [48640/50048]	Loss: 0.2173
Training Epoch: 60 [48768/50048]	Loss: 0.2428
Training Epoch: 60 [48896/50048]	Loss: 0.2669
Training Epoch: 60 [49024/50048]	Loss: 0.1548
Training Epoch: 60 [49152/50048]	Loss: 0.2083
Training Epoch: 60 [49280/50048]	Loss: 0.2157
Training Epoch: 60 [49408/50048]	Loss: 0.2685
Training Epoch: 60 [49536/50048]	Loss: 0.1909
Training Epoch: 60 [49664/50048]	Loss: 0.1791
Training Epoch: 60 [49792/50048]	Loss: 0.2765
Training Epoch: 60 [49920/50048]	Loss: 0.2300
Training Epoch: 60 [50048/50048]	Loss: 0.1607
2022-12-06 10:09:07.742 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:09:07,771 [ZeusDataLoader(eval)] eval epoch 61 done: time=3.69 energy=452.92
2022-12-06 05:09:07,771 [ZeusDataLoader(train)] Up to epoch 61: time=5504.21, energy=668144.45, cost=815690.83
2022-12-06 05:09:07,772 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:09:07,772 [ZeusDataLoader(train)] Expected next epoch: time=5594.01, energy=678942.46, cost=828947.21
2022-12-06 05:09:07,773 [ZeusDataLoader(train)] Epoch 62 begin.
Validation Epoch: 60, Average loss: 0.0161, Accuracy: 0.6372
2022-12-06 05:09:07,955 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:09:07,956 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:09:07.958 [ZeusMonitor] Monitor started.
2022-12-06 10:09:07.958 [ZeusMonitor] Running indefinitely. 2022-12-06 10:09:07.958 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:09:07.958 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e62+gpu0.power.log
Training Epoch: 61 [128/50048]	Loss: 0.2011
Training Epoch: 61 [256/50048]	Loss: 0.1748
Training Epoch: 61 [384/50048]	Loss: 0.1299
Training Epoch: 61 [512/50048]	Loss: 0.1294
Training Epoch: 61 [640/50048]	Loss: 0.1572
Training Epoch: 61 [768/50048]	Loss: 0.1590
Training Epoch: 61 [896/50048]	Loss: 0.1727
Training Epoch: 61 [1024/50048]	Loss: 0.1612
Training Epoch: 61 [1152/50048]	Loss: 0.1705
Training Epoch: 61 [1280/50048]	Loss: 0.1053
Training Epoch: 61 [1408/50048]	Loss: 0.1525
Training Epoch: 61 [1536/50048]	Loss: 0.1418
Training Epoch: 61 [1664/50048]	Loss: 0.1524
Training Epoch: 61 [1792/50048]	Loss: 0.2179
Training Epoch: 61 [1920/50048]	Loss: 0.1155
Training Epoch: 61 [2048/50048]	Loss: 0.0650
Training Epoch: 61 [2176/50048]	Loss: 0.1172
Training Epoch: 61 [2304/50048]	Loss: 0.1456
Training Epoch: 61 [2432/50048]	Loss: 0.3370
Training Epoch: 61 [2560/50048]	Loss: 0.1636
Training Epoch: 61 [2688/50048]	Loss: 0.1606
Training Epoch: 61 [2816/50048]	Loss: 0.1096
Training Epoch: 61 [2944/50048]	Loss: 0.1397
Training Epoch: 61 [3072/50048]	Loss: 0.1865
Training Epoch: 61 [3200/50048]	Loss: 0.2238
Training Epoch: 61 [3328/50048]	Loss: 0.1229
Training Epoch: 61 [3456/50048]	Loss: 0.1615
Training Epoch: 61 [3584/50048]	Loss: 0.1664
Training Epoch: 61 [3712/50048]	Loss: 0.1417
Training Epoch: 61 [3840/50048]	Loss: 0.1531
Training Epoch: 61 [3968/50048]	Loss: 0.1173
Training Epoch: 61 [4096/50048]	Loss: 0.1407
Training Epoch: 61 [4224/50048]	Loss: 0.1124
Training Epoch: 61 [4352/50048]	Loss: 0.2110
Training Epoch: 61 [4480/50048]	Loss: 0.1330
Training Epoch: 61 [4608/50048]	Loss: 0.0862
Training Epoch: 61 [4736/50048]	Loss: 0.1639
Training Epoch: 61 [4864/50048]	Loss: 0.1620
Training Epoch: 61 [4992/50048]	Loss: 0.1950
Training Epoch: 61 [5120/50048]	Loss: 0.1518
Training Epoch: 61 [5248/50048]	Loss: 0.1002
Training Epoch: 61 [5376/50048]	Loss: 0.2079
Training Epoch: 61 [5504/50048]	Loss: 0.2075
Training Epoch: 61 [5632/50048]	Loss: 0.1262
Training Epoch: 61 [5760/50048]	Loss: 0.1154
Training Epoch: 61 [5888/50048]	Loss: 0.1553
Training Epoch: 61 [6016/50048]	Loss: 0.2070
Training Epoch: 61 [6144/50048]	Loss: 0.1097
Training Epoch: 61 [6272/50048]	Loss: 0.1438
Training Epoch: 61 [6400/50048]	Loss: 0.2159
Training Epoch: 61 [6528/50048]	Loss: 0.2545
Training Epoch: 61 [6656/50048]	Loss: 0.3046
Training Epoch: 61 [6784/50048]	Loss: 0.2428
Training Epoch: 61 [6912/50048]	Loss: 0.1696
Training Epoch: 61 [7040/50048]	Loss: 0.1927
Training Epoch: 61 [7168/50048]	Loss: 0.0998
Training Epoch: 61 [7296/50048]	Loss: 0.2433
Training Epoch: 61 [7424/50048]	Loss: 0.1299
Training Epoch: 61 [7552/50048]	Loss: 0.1483
Training Epoch: 61 [7680/50048]	Loss: 0.1282
Training Epoch: 61 [7808/50048]	Loss: 0.1796
Training Epoch: 61 [7936/50048]	Loss: 0.1037
Training Epoch: 61 [8064/50048]	Loss: 0.1382
Training Epoch: 61 [8192/50048]	Loss: 0.1642
Training Epoch: 61 [8320/50048]	Loss: 0.0987
Training Epoch: 61 [8448/50048]	Loss: 0.1034
Training Epoch: 61 [8576/50048]	Loss: 0.1831
Training Epoch: 61 [8704/50048]	Loss: 0.1629
Training Epoch: 61 [8832/50048]	Loss: 0.1789
Training Epoch: 61 [8960/50048]	Loss: 0.1123
Training Epoch: 61 [9088/50048]	Loss: 0.2460
Training Epoch: 61 [9216/50048]	Loss: 0.2609
Training Epoch: 61 [9344/50048]	Loss: 0.2168
Training Epoch: 61 [9472/50048]	Loss: 0.2180
Training Epoch: 61 [9600/50048]	Loss: 0.1298
Training Epoch: 61 [9728/50048]	Loss: 0.2664
Training Epoch: 61 [9856/50048]	Loss: 0.1072
Training Epoch: 61 [9984/50048]	Loss: 0.1565
Training Epoch: 61 [10112/50048]	Loss: 0.1724
Training Epoch: 61 [10240/50048]	Loss: 0.2125
Training Epoch: 61 [10368/50048]	Loss: 0.1078
Training Epoch: 61 [10496/50048]	Loss: 0.2144
Training Epoch: 61 [10624/50048]	Loss: 0.1922
Training Epoch: 61 [10752/50048]	Loss: 0.1147
Training Epoch: 61 [10880/50048]	Loss: 0.2001
Training Epoch: 61 [11008/50048]	Loss: 0.1259
Training Epoch: 61 [11136/50048]	Loss: 0.1948
Training Epoch: 61 [11264/50048]	Loss: 0.1823
Training Epoch: 61 [11392/50048]	Loss: 0.1846
Training Epoch: 61 [11520/50048]	Loss: 0.1735
Training Epoch: 61 [11648/50048]	Loss: 0.1727
Training Epoch: 61 [11776/50048]	Loss: 0.1432
Training Epoch: 61 [11904/50048]	Loss: 0.1905
Training Epoch: 61 [12032/50048]	Loss: 0.1506
Training Epoch: 61 [12160/50048]	Loss: 0.2201
Training Epoch: 61 [12288/50048]	Loss: 0.2500
Training Epoch: 61 [12416/50048]	Loss: 0.1264
Training Epoch: 61 [12544/50048]	Loss: 0.1952
Training Epoch: 61 [12672/50048]	Loss: 0.1374
Training Epoch: 61 [12800/50048]	Loss: 0.1426
Training Epoch: 61 [12928/50048]	Loss: 0.1786
Training Epoch: 61 [13056/50048]	Loss: 0.0999
Training Epoch: 61 [13184/50048]	Loss: 0.1249
Training Epoch: 61 [13312/50048]	Loss: 0.1453
Training Epoch: 61 [13440/50048]	Loss: 0.2027
Training Epoch: 61 [13568/50048]	Loss: 0.1846
Training Epoch: 61 [13696/50048]	Loss: 0.1607
Training Epoch: 61 [13824/50048]	Loss: 0.1036
Training Epoch: 61 [13952/50048]	Loss: 0.1098
Training Epoch: 61 [14080/50048]	Loss: 0.1757
Training Epoch: 61 [14208/50048]	Loss: 0.1657
Training Epoch: 61 [14336/50048]	Loss: 0.1661
Training Epoch: 61 [14464/50048]	Loss: 0.0991
Training Epoch: 61 [14592/50048]	Loss: 0.2083
Training Epoch: 61 [14720/50048]	Loss: 0.2022
Training Epoch: 61 [14848/50048]	Loss: 0.2671
Training Epoch: 61 [14976/50048]	Loss: 0.1641
Training Epoch: 61 [15104/50048]	Loss: 0.1906
Training Epoch: 61 [15232/50048]	Loss: 0.2873
Training Epoch: 61 [15360/50048]	Loss: 0.1735
Training Epoch: 61 [15488/50048]	Loss: 0.1086
Training Epoch: 61 [15616/50048]	Loss: 0.1712
Training Epoch: 61 [15744/50048]	Loss: 0.2017
Training Epoch: 61 [15872/50048]	Loss: 0.2126
Training Epoch: 61 [16000/50048]	Loss: 0.2028
Training Epoch: 61 [16128/50048]	Loss: 0.1891
Training Epoch: 61 [16256/50048]	Loss: 0.1382
Training Epoch: 61 [16384/50048]	Loss: 0.1434
Training Epoch: 61 [16512/50048]	Loss: 0.1451
Training Epoch: 61 [16640/50048]	Loss: 0.0801
Training Epoch: 61 [16768/50048]	Loss: 0.2308
Training Epoch: 61 [16896/50048]	Loss: 0.1317
Training Epoch: 61 [17024/50048]	Loss: 0.2225
Training Epoch: 61 [17152/50048]	Loss: 0.2714
Training Epoch: 61 [17280/50048]	Loss: 0.1669
Training Epoch: 61 [17408/50048]	Loss: 0.1548
Training Epoch: 61 [17536/50048]	Loss: 0.1725
Training Epoch: 61 [17664/50048]	Loss: 0.1877
Training Epoch: 61 [17792/50048]	Loss: 0.2024
Training Epoch: 61 [17920/50048]	Loss: 0.1118
Training Epoch: 61 [18048/50048]	Loss: 0.2654
Training Epoch: 61 [18176/50048]	Loss: 0.2012
Training Epoch: 61 [18304/50048]	Loss: 0.0865
Training Epoch: 61 [18432/50048]	Loss: 0.1829
Training Epoch: 61 [18560/50048]	Loss: 0.1546
Training Epoch: 61 [18688/50048]	Loss: 0.1829
Training Epoch: 61 [18816/50048]	Loss: 0.1796
Training Epoch: 61 [18944/50048]	Loss: 0.1376
Training Epoch: 61 [19072/50048]	Loss: 0.1015
Training Epoch: 61 [19200/50048]	Loss: 0.1391
Training Epoch: 61 [19328/50048]	Loss: 0.1899
Training Epoch: 61 [19456/50048]	Loss: 0.1475
Training Epoch: 61 [19584/50048]	Loss: 0.0954
Training Epoch: 61 [19712/50048]	Loss: 0.2576
Training Epoch: 61 [19840/50048]	Loss: 0.2962
Training Epoch: 61 [19968/50048]	Loss: 0.1664
Training Epoch: 61 [20096/50048]	Loss: 0.1953
Training Epoch: 61 [20224/50048]	Loss: 0.1526
Training Epoch: 61 [20352/50048]	Loss: 0.1928
Training Epoch: 61 [20480/50048]	Loss: 0.1319
Training Epoch: 61 [20608/50048]	Loss: 0.1524
Training Epoch: 61 [20736/50048]	Loss: 0.1765
Training Epoch: 61 [20864/50048]	Loss: 0.1139
Training Epoch: 61 [20992/50048]	Loss: 0.3250
Training Epoch: 61 [21120/50048]	Loss: 0.1498
Training Epoch: 61 [21248/50048]	Loss: 0.1625
Training Epoch: 61 [21376/50048]	Loss: 0.2161
Training Epoch: 61 [21504/50048]	Loss: 0.2698
Training Epoch: 61 [21632/50048]	Loss: 0.1701
Training Epoch: 61 [21760/50048]	Loss: 0.2545
Training Epoch: 61 [21888/50048]	Loss: 0.1778
Training Epoch: 61 [22016/50048]	Loss: 0.2083
Training Epoch: 61 [22144/50048]	Loss: 0.2627
Training Epoch: 61 [22272/50048]	Loss: 0.1466
Training Epoch: 61 [22400/50048]	Loss: 0.1426
Training Epoch: 61 [22528/50048]	Loss: 0.2152
Training Epoch: 61 [22656/50048]	Loss: 0.1841
Training Epoch: 61 [22784/50048]	Loss: 0.1159
Training Epoch: 61 [22912/50048]	Loss: 0.1641
Training Epoch: 61 [23040/50048]	Loss: 0.1056
Training Epoch: 61 [23168/50048]	Loss: 0.2267
Training Epoch: 61 [23296/50048]	Loss: 0.1246
Training Epoch: 61 [23424/50048]	Loss: 0.2098
Training Epoch: 61 [23552/50048]	Loss: 0.1976
Training Epoch: 61 [23680/50048]	Loss: 0.3368
Training Epoch: 61 [23808/50048]	Loss: 0.1277
Training Epoch: 61 [23936/50048]	Loss: 0.1730
Training Epoch: 61 [24064/50048]	Loss: 0.1803
Training Epoch: 61 [24192/50048]	Loss: 0.1325
Training Epoch: 61 [24320/50048]	Loss: 0.1845
Training Epoch: 61 [24448/50048]	Loss: 0.1045
Training Epoch: 61 [24576/50048]	Loss: 0.1539
Training Epoch: 61 [24704/50048]	Loss: 0.1807
Training Epoch: 61 [24832/50048]	Loss: 0.2078
Training Epoch: 61 [24960/50048]	Loss: 0.1830
Training Epoch: 61 [25088/50048]	Loss: 0.1900
Training Epoch: 61 [25216/50048]	Loss: 0.2128
Training Epoch: 61 [25344/50048]	Loss: 0.2260
Training Epoch: 61 [25472/50048]	Loss: 0.1191
Training Epoch: 61 [25600/50048]	Loss: 0.1834
Training Epoch: 61 [25728/50048]	Loss: 0.2477
Training Epoch: 61 [25856/50048]	Loss: 0.1166
Training Epoch: 61 [25984/50048]	Loss: 0.1832
Training Epoch: 61 [26112/50048]	Loss: 0.2328
Training Epoch: 61 [26240/50048]	Loss: 0.1412
Training Epoch: 61 [26368/50048]	Loss: 0.1332
Training Epoch: 61 [26496/50048]	Loss: 0.1650
Training Epoch: 61 [26624/50048]	Loss: 0.1621
Training Epoch: 61 [26752/50048]	Loss: 0.1891
Training Epoch: 61 [26880/50048]	Loss: 0.1581
Training Epoch: 61 [27008/50048]	Loss: 0.1701
Training Epoch: 61 [27136/50048]	Loss: 0.2158
Training Epoch: 61 [27264/50048]	Loss: 0.1361
Training Epoch: 61 [27392/50048]	Loss: 0.2282
Training Epoch: 61 [27520/50048]	Loss: 0.2627
Training Epoch: 61 [27648/50048]	Loss: 0.1623
Training Epoch: 61 [27776/50048]	Loss: 0.2827
Training Epoch: 61 [27904/50048]	Loss: 0.1577
Training Epoch: 61 [28032/50048]	Loss: 0.2204
Training Epoch: 61 [28160/50048]	Loss: 0.1196
Training Epoch: 61 [28288/50048]	Loss: 0.1770
Training Epoch: 61 [28416/50048]	Loss: 0.1538
Training Epoch: 61 [28544/50048]	Loss: 0.3389
Training Epoch: 61 [28672/50048]	Loss: 0.2007
Training Epoch: 61 [28800/50048]	Loss: 0.1293
Training Epoch: 61 [28928/50048]	Loss: 0.0645
Training Epoch: 61 [29056/50048]	Loss: 0.1931
Training Epoch: 61 [29184/50048]	Loss: 0.1512
Training Epoch: 61 [29312/50048]	Loss: 0.1487
Training Epoch: 61 [29440/50048]	Loss: 0.1112
Training Epoch: 61 [29568/50048]	Loss: 0.1722
Training Epoch: 61 [29696/50048]	Loss: 0.1435
Training Epoch: 61 [29824/50048]	Loss: 0.1743
Training Epoch: 61 [29952/50048]	Loss: 0.1997
Training Epoch: 61 [30080/50048]	Loss: 0.1747
Training Epoch: 61 [30208/50048]	Loss: 0.1326
Training Epoch: 61 [30336/50048]	Loss: 0.2747
Training Epoch: 61 [30464/50048]	Loss: 0.1307
Training Epoch: 61 [30592/50048]	Loss: 0.1838
Training Epoch: 61 [30720/50048]	Loss: 0.2856
Training Epoch: 61 [30848/50048]	Loss: 0.2136
Training Epoch: 61 [30976/50048]	Loss: 0.3030
Training Epoch: 61 [31104/50048]	Loss: 0.2427
Training Epoch: 61 [31232/50048]	Loss: 0.2011
Training Epoch: 61 [31360/50048]	Loss: 0.1385
Training Epoch: 61 [31488/50048]	Loss: 0.1431
Training Epoch: 61 [31616/50048]	Loss: 0.1680
Training Epoch: 61 [31744/50048]	Loss: 0.1878
Training Epoch: 61 [31872/50048]	Loss: 0.2355
Training Epoch: 61 [32000/50048]	Loss: 0.1615
Training Epoch: 61 [32128/50048]	Loss: 0.1918
Training Epoch: 61 [32256/50048]	Loss: 0.1837
Training Epoch: 61 [32384/50048]	Loss: 0.2286
Training Epoch: 61 [32512/50048]	Loss: 0.2258
Training Epoch: 61 [32640/50048]	Loss: 0.2108
Training Epoch: 61 [32768/50048]	Loss: 0.2446
Training Epoch: 61 [32896/50048]	Loss: 0.1563
Training Epoch: 61 [33024/50048]	Loss: 0.1954
Training Epoch: 61 [33152/50048]	Loss: 0.2305
Training Epoch: 61 [33280/50048]	Loss: 0.2467
Training Epoch: 61 [33408/50048]	Loss: 0.2334
Training Epoch: 61 [33536/50048]	Loss: 0.3502
Training Epoch: 61 [33664/50048]	Loss: 0.2082
Training Epoch: 61 [33792/50048]	Loss: 0.2332
Training Epoch: 61 [33920/50048]	Loss: 0.1934
Training Epoch: 61 [34048/50048]	Loss: 0.1474
Training Epoch: 61 [34176/50048]	Loss: 0.3160
Training Epoch: 61 [34304/50048]	Loss: 0.1806
Training Epoch: 61 [34432/50048]	Loss: 0.1149
Training Epoch: 61 [34560/50048]	Loss: 0.1510
Training Epoch: 61 [34688/50048]	Loss: 0.2588
Training Epoch: 61 [34816/50048]	Loss: 0.2713
Training Epoch: 61 [34944/50048]	Loss: 0.1654
Training Epoch: 61 [35072/50048]	Loss: 0.2013
Training Epoch: 61 [35200/50048]	Loss: 0.1912
Training Epoch: 61 [35328/50048]	Loss: 0.2324
Training Epoch: 61 [35456/50048]	Loss: 0.1692
Training Epoch: 61 [35584/50048]	Loss: 0.1887
Training Epoch: 61 [35712/50048]	Loss: 0.2960
Training Epoch: 61 [35840/50048]	Loss: 0.2224
Training Epoch: 61 [35968/50048]	Loss: 0.2324
Training Epoch: 61 [36096/50048]	Loss: 0.2259
Training Epoch: 61 [36224/50048]	Loss: 0.1984
Training Epoch: 61 [36352/50048]	Loss: 0.1235
Training Epoch: 61 [36480/50048]	Loss: 0.1564
Training Epoch: 61 [36608/50048]	Loss: 0.2210
Training Epoch: 61 [36736/50048]	Loss: 0.2500
Training Epoch: 61 [36864/50048]	Loss: 0.2375
Training Epoch: 61 [36992/50048]	Loss: 0.1739
Training Epoch: 61 [37120/50048]	Loss: 0.1436
Training Epoch: 61 [37248/50048]	Loss: 0.2237
Training Epoch: 61 [37376/50048]	Loss: 0.2135
Training Epoch: 61 [37504/50048]	Loss: 0.1579
Training Epoch: 61 [37632/50048]	Loss: 0.1376
Training Epoch: 61 [37760/50048]	Loss: 0.1923
Training Epoch: 61 [37888/50048]	Loss: 0.1488
Training Epoch: 61 [38016/50048]	Loss: 0.1474
Training Epoch: 61 [38144/50048]	Loss: 0.1431
Training Epoch: 61 [38272/50048]	Loss: 0.2299
Training Epoch: 61 [38400/50048]	Loss: 0.1281
Training Epoch: 61 [38528/50048]	Loss: 0.2004
Training Epoch: 61 [38656/50048]	Loss: 0.2341
Training Epoch: 61 [38784/50048]	Loss: 0.1308
Training Epoch: 61 [38912/50048]	Loss: 0.1299
Training Epoch: 61 [39040/50048]	Loss: 0.1411
Training Epoch: 61 [39168/50048]	Loss: 0.1390
Training Epoch: 61 [39296/50048]	Loss: 0.1531
Training Epoch: 61 [39424/50048]	Loss: 0.1310
Training Epoch: 61 [39552/50048]	Loss: 0.0793
Training Epoch: 61 [39680/50048]	Loss: 0.3098
Training Epoch: 61 [39808/50048]	Loss: 0.2761
Training Epoch: 61 [39936/50048]	Loss: 0.1868
Training Epoch: 61 [40064/50048]	Loss: 0.2051
Training Epoch: 61 [40192/50048]	Loss: 0.2258
Training Epoch: 61 [40320/50048]	Loss: 0.2436
Training Epoch: 61 [40448/50048]	Loss: 0.2166
Training Epoch: 61 [40576/50048]	Loss: 0.2008
Training Epoch: 61 [40704/50048]	Loss: 0.2246
Training Epoch: 61 [40832/50048]	Loss: 0.2401
Training Epoch: 61 [40960/50048]	Loss: 0.1766
Training Epoch: 61 [41088/50048]	Loss: 0.2334
Training Epoch: 61 [41216/50048]	Loss: 0.2150
Training Epoch: 61 [41344/50048]	Loss: 0.1541
Training Epoch: 61 [41472/50048]	Loss: 0.2240
Training Epoch: 61 [41600/50048]	Loss: 0.1584
Training Epoch: 61 [41728/50048]	Loss: 0.1329
Training Epoch: 61 [41856/50048]	Loss: 0.3328
Training Epoch: 61 [41984/50048]	Loss: 0.1648
Training Epoch: 61 [42112/50048]	Loss: 0.2280
Training Epoch: 61 [42240/50048]	Loss: 0.1657
Training Epoch: 61 [42368/50048]	Loss: 0.1577
Training Epoch: 61 [42496/50048]	Loss: 0.2016
Training Epoch: 61 [42624/50048]	Loss: 0.2051
Training Epoch: 61 [42752/50048]	Loss: 0.1978
Training Epoch: 61 [42880/50048]	Loss: 0.1632
Training Epoch: 61 [43008/50048]	Loss: 0.2924
Training Epoch: 61 [43136/50048]	Loss: 0.1737
Training Epoch: 61 [43264/50048]	Loss: 0.1385
Training Epoch: 61 [43392/50048]	Loss: 0.1102
Training Epoch: 61 [43520/50048]	Loss: 0.2708
Training Epoch: 61 [43648/50048]	Loss: 0.2370
Training Epoch: 61 [43776/50048]	Loss: 0.3067
Training Epoch: 61 [43904/50048]	Loss: 0.1971
Training Epoch: 61 [44032/50048]	Loss: 0.1084
Training Epoch: 61 [44160/50048]	Loss: 0.1895
Training Epoch: 61 [44288/50048]	Loss: 0.2547
Training Epoch: 61 [44416/50048]	Loss: 0.2015
Training Epoch: 61 [44544/50048]	Loss: 0.2823
Training Epoch: 61 [44672/50048]	Loss: 0.3383
Training Epoch: 61 [44800/50048]	Loss: 0.2200
Training Epoch: 61 [44928/50048]	Loss: 0.1398
Training Epoch: 61 [45056/50048]	Loss: 0.2934
Training Epoch: 61 [45184/50048]	Loss: 0.2032
Training Epoch: 61 [45312/50048]	Loss: 0.1589
Training Epoch: 61 [45440/50048]	Loss: 0.2539
Training Epoch: 61 [45568/50048]	Loss: 0.2516
Training Epoch: 61 [45696/50048]	Loss: 0.2132
2022-12-06 05:10:34,249 [ZeusDataLoader(train)] train epoch 62 done: time=86.47 energy=10493.46
2022-12-06 05:10:34,250 [ZeusDataLoader(eval)] Epoch 62 begin.
Training Epoch: 61 [45824/50048]	Loss: 0.2490
Training Epoch: 61 [45952/50048]	Loss: 0.2253
Training Epoch: 61 [46080/50048]	Loss: 0.2706
Training Epoch: 61 [46208/50048]	Loss: 0.4058
Training Epoch: 61 [46336/50048]	Loss: 0.1970
Training Epoch: 61 [46464/50048]	Loss: 0.2250
Training Epoch: 61 [46592/50048]	Loss: 0.2679
Training Epoch: 61 [46720/50048]	Loss: 0.2195
Training Epoch: 61 [46848/50048]	Loss: 0.2519
Training Epoch: 61 [46976/50048]	Loss: 0.1106
Training Epoch: 61 [47104/50048]	Loss: 0.2413
Training Epoch: 61 [47232/50048]	Loss: 0.2596
Training Epoch: 61 [47360/50048]	Loss: 0.2006
Training Epoch: 61 [47488/50048]	Loss: 0.1505
Training Epoch: 61 [47616/50048]	Loss: 0.1770
Training Epoch: 61 [47744/50048]	Loss: 0.2639
Training Epoch: 61 [47872/50048]	Loss: 0.1862
Training Epoch: 61 [48000/50048]	Loss: 0.1952
Training Epoch: 61 [48128/50048]	Loss: 0.2265
Training Epoch: 61 [48256/50048]	Loss: 0.2322
Training Epoch: 61 [48384/50048]	Loss: 0.2139
Training Epoch: 61 [48512/50048]	Loss: 0.2928
Training Epoch: 61 [48640/50048]	Loss: 0.2329
Training Epoch: 61 [48768/50048]	Loss: 0.2070
Training Epoch: 61 [48896/50048]	Loss: 0.2198
Training Epoch: 61 [49024/50048]	Loss: 0.2613
Training Epoch: 61 [49152/50048]	Loss: 0.2606
Training Epoch: 61 [49280/50048]	Loss: 0.1376
Training Epoch: 61 [49408/50048]	Loss: 0.2426
Training Epoch: 61 [49536/50048]	Loss: 0.2313
Training Epoch: 61 [49664/50048]	Loss: 0.2416
Training Epoch: 61 [49792/50048]	Loss: 0.1790
Training Epoch: 61 [49920/50048]	Loss: 0.1734
Training Epoch: 61 [50048/50048]	Loss: 0.2705
2022-12-06 10:10:37.900 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:10:37,910 [ZeusDataLoader(eval)] eval epoch 62 done: time=3.65 energy=443.21
2022-12-06 05:10:37,911 [ZeusDataLoader(train)] Up to epoch 62: time=5594.33, energy=679081.12, cost=829044.42
2022-12-06 05:10:37,911 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:10:37,911 [ZeusDataLoader(train)] Expected next epoch: time=5684.13, energy=689879.13, cost=842300.81
2022-12-06 05:10:37,912 [ZeusDataLoader(train)] Epoch 63 begin.
Validation Epoch: 61, Average loss: 0.0168, Accuracy: 0.6317
2022-12-06 05:10:38,092 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:10:38,093 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:10:38.095 [ZeusMonitor] Monitor started.
2022-12-06 10:10:38.095 [ZeusMonitor] Running indefinitely. 2022-12-06 10:10:38.095 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:10:38.103 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e63+gpu0.power.log
Training Epoch: 62 [128/50048]	Loss: 0.0958
Training Epoch: 62 [256/50048]	Loss: 0.1251
Training Epoch: 62 [384/50048]	Loss: 0.1171
Training Epoch: 62 [512/50048]	Loss: 0.2985
Training Epoch: 62 [640/50048]	Loss: 0.0993
Training Epoch: 62 [768/50048]	Loss: 0.1426
Training Epoch: 62 [896/50048]	Loss: 0.1396
Training Epoch: 62 [1024/50048]	Loss: 0.1577
Training Epoch: 62 [1152/50048]	Loss: 0.2008
Training Epoch: 62 [1280/50048]	Loss: 0.1516
Training Epoch: 62 [1408/50048]	Loss: 0.1494
Training Epoch: 62 [1536/50048]	Loss: 0.1915
Training Epoch: 62 [1664/50048]	Loss: 0.1762
Training Epoch: 62 [1792/50048]	Loss: 0.1396
Training Epoch: 62 [1920/50048]	Loss: 0.1293
Training Epoch: 62 [2048/50048]	Loss: 0.2451
Training Epoch: 62 [2176/50048]	Loss: 0.1490
Training Epoch: 62 [2304/50048]	Loss: 0.0651
Training Epoch: 62 [2432/50048]	Loss: 0.1276
Training Epoch: 62 [2560/50048]	Loss: 0.1116
Training Epoch: 62 [2688/50048]	Loss: 0.1300
Training Epoch: 62 [2816/50048]	Loss: 0.1373
Training Epoch: 62 [2944/50048]	Loss: 0.1800
Training Epoch: 62 [3072/50048]	Loss: 0.1794
Training Epoch: 62 [3200/50048]	Loss: 0.1364
Training Epoch: 62 [3328/50048]	Loss: 0.1401
Training Epoch: 62 [3456/50048]	Loss: 0.1909
Training Epoch: 62 [3584/50048]	Loss: 0.1621
Training Epoch: 62 [3712/50048]	Loss: 0.2009
Training Epoch: 62 [3840/50048]	Loss: 0.2049
Training Epoch: 62 [3968/50048]	Loss: 0.1815
Training Epoch: 62 [4096/50048]	Loss: 0.2006
Training Epoch: 62 [4224/50048]	Loss: 0.2277
Training Epoch: 62 [4352/50048]	Loss: 0.1114
Training Epoch: 62 [4480/50048]	Loss: 0.1546
Training Epoch: 62 [4608/50048]	Loss: 0.1176
Training Epoch: 62 [4736/50048]	Loss: 0.0806
Training Epoch: 62 [4864/50048]	Loss: 0.1324
Training Epoch: 62 [4992/50048]	Loss: 0.2552
Training Epoch: 62 [5120/50048]	Loss: 0.1988
Training Epoch: 62 [5248/50048]	Loss: 0.1989
Training Epoch: 62 [5376/50048]	Loss: 0.1700
Training Epoch: 62 [5504/50048]	Loss: 0.2201
Training Epoch: 62 [5632/50048]	Loss: 0.1273
Training Epoch: 62 [5760/50048]	Loss: 0.2561
Training Epoch: 62 [5888/50048]	Loss: 0.1533
Training Epoch: 62 [6016/50048]	Loss: 0.0755
Training Epoch: 62 [6144/50048]	Loss: 0.1530
Training Epoch: 62 [6272/50048]	Loss: 0.1685
Training Epoch: 62 [6400/50048]	Loss: 0.2355
Training Epoch: 62 [6528/50048]	Loss: 0.2086
Training Epoch: 62 [6656/50048]	Loss: 0.2165
Training Epoch: 62 [6784/50048]	Loss: 0.0995
Training Epoch: 62 [6912/50048]	Loss: 0.1856
Training Epoch: 62 [7040/50048]	Loss: 0.1953
Training Epoch: 62 [7168/50048]	Loss: 0.1081
Training Epoch: 62 [7296/50048]	Loss: 0.2118
Training Epoch: 62 [7424/50048]	Loss: 0.2022
Training Epoch: 62 [7552/50048]	Loss: 0.2076
Training Epoch: 62 [7680/50048]	Loss: 0.1532
Training Epoch: 62 [7808/50048]	Loss: 0.2279
Training Epoch: 62 [7936/50048]	Loss: 0.1634
Training Epoch: 62 [8064/50048]	Loss: 0.1554
Training Epoch: 62 [8192/50048]	Loss: 0.1878
Training Epoch: 62 [8320/50048]	Loss: 0.2147
Training Epoch: 62 [8448/50048]	Loss: 0.1680
Training Epoch: 62 [8576/50048]	Loss: 0.1490
Training Epoch: 62 [8704/50048]	Loss: 0.1538
Training Epoch: 62 [8832/50048]	Loss: 0.1665
Training Epoch: 62 [8960/50048]	Loss: 0.1761
Training Epoch: 62 [9088/50048]	Loss: 0.1545
Training Epoch: 62 [9216/50048]	Loss: 0.1678
Training Epoch: 62 [9344/50048]	Loss: 0.1900
Training Epoch: 62 [9472/50048]	Loss: 0.1041
Training Epoch: 62 [9600/50048]	Loss: 0.1190
Training Epoch: 62 [9728/50048]	Loss: 0.0599
Training Epoch: 62 [9856/50048]	Loss: 0.1244
Training Epoch: 62 [9984/50048]	Loss: 0.1839
Training Epoch: 62 [10112/50048]	Loss: 0.1891
Training Epoch: 62 [10240/50048]	Loss: 0.1902
Training Epoch: 62 [10368/50048]	Loss: 0.1309
Training Epoch: 62 [10496/50048]	Loss: 0.1562
Training Epoch: 62 [10624/50048]	Loss: 0.1810
Training Epoch: 62 [10752/50048]	Loss: 0.1483
Training Epoch: 62 [10880/50048]	Loss: 0.1584
Training Epoch: 62 [11008/50048]	Loss: 0.2630
Training Epoch: 62 [11136/50048]	Loss: 0.2099
Training Epoch: 62 [11264/50048]	Loss: 0.0988
Training Epoch: 62 [11392/50048]	Loss: 0.1099
Training Epoch: 62 [11520/50048]	Loss: 0.2060
Training Epoch: 62 [11648/50048]	Loss: 0.1432
Training Epoch: 62 [11776/50048]	Loss: 0.1579
Training Epoch: 62 [11904/50048]	Loss: 0.1131
Training Epoch: 62 [12032/50048]	Loss: 0.2084
Training Epoch: 62 [12160/50048]	Loss: 0.1618
Training Epoch: 62 [12288/50048]	Loss: 0.3447
Training Epoch: 62 [12416/50048]	Loss: 0.1797
Training Epoch: 62 [12544/50048]	Loss: 0.1485
Training Epoch: 62 [12672/50048]	Loss: 0.1676
Training Epoch: 62 [12800/50048]	Loss: 0.1139
Training Epoch: 62 [12928/50048]	Loss: 0.1250
Training Epoch: 62 [13056/50048]	Loss: 0.0724
Training Epoch: 62 [13184/50048]	Loss: 0.1856
Training Epoch: 62 [13312/50048]	Loss: 0.0906
Training Epoch: 62 [13440/50048]	Loss: 0.1976
Training Epoch: 62 [13568/50048]	Loss: 0.2321
Training Epoch: 62 [13696/50048]	Loss: 0.1570
Training Epoch: 62 [13824/50048]	Loss: 0.1923
Training Epoch: 62 [13952/50048]	Loss: 0.1890
Training Epoch: 62 [14080/50048]	Loss: 0.1602
Training Epoch: 62 [14208/50048]	Loss: 0.1634
Training Epoch: 62 [14336/50048]	Loss: 0.1452
Training Epoch: 62 [14464/50048]	Loss: 0.1214
Training Epoch: 62 [14592/50048]	Loss: 0.1616
Training Epoch: 62 [14720/50048]	Loss: 0.1646
Training Epoch: 62 [14848/50048]	Loss: 0.0984
Training Epoch: 62 [14976/50048]	Loss: 0.2113
Training Epoch: 62 [15104/50048]	Loss: 0.1979
Training Epoch: 62 [15232/50048]	Loss: 0.1828
Training Epoch: 62 [15360/50048]	Loss: 0.2029
Training Epoch: 62 [15488/50048]	Loss: 0.1662
Training Epoch: 62 [15616/50048]	Loss: 0.1361
Training Epoch: 62 [15744/50048]	Loss: 0.1763
Training Epoch: 62 [15872/50048]	Loss: 0.1064
Training Epoch: 62 [16000/50048]	Loss: 0.2128
Training Epoch: 62 [16128/50048]	Loss: 0.1951
Training Epoch: 62 [16256/50048]	Loss: 0.1928
Training Epoch: 62 [16384/50048]	Loss: 0.2139
Training Epoch: 62 [16512/50048]	Loss: 0.2034
Training Epoch: 62 [16640/50048]	Loss: 0.1487
Training Epoch: 62 [16768/50048]	Loss: 0.1869
Training Epoch: 62 [16896/50048]	Loss: 0.1926
Training Epoch: 62 [17024/50048]	Loss: 0.1983
Training Epoch: 62 [17152/50048]	Loss: 0.1018
Training Epoch: 62 [17280/50048]	Loss: 0.1812
Training Epoch: 62 [17408/50048]	Loss: 0.0817
Training Epoch: 62 [17536/50048]	Loss: 0.1258
Training Epoch: 62 [17664/50048]	Loss: 0.1196
Training Epoch: 62 [17792/50048]	Loss: 0.1541
Training Epoch: 62 [17920/50048]	Loss: 0.1600
Training Epoch: 62 [18048/50048]	Loss: 0.2406
Training Epoch: 62 [18176/50048]	Loss: 0.1605
Training Epoch: 62 [18304/50048]	Loss: 0.1534
Training Epoch: 62 [18432/50048]	Loss: 0.0839
Training Epoch: 62 [18560/50048]	Loss: 0.2542
Training Epoch: 62 [18688/50048]	Loss: 0.1252
Training Epoch: 62 [18816/50048]	Loss: 0.2164
Training Epoch: 62 [18944/50048]	Loss: 0.0882
Training Epoch: 62 [19072/50048]	Loss: 0.1649
Training Epoch: 62 [19200/50048]	Loss: 0.1749
Training Epoch: 62 [19328/50048]	Loss: 0.1625
Training Epoch: 62 [19456/50048]	Loss: 0.0975
Training Epoch: 62 [19584/50048]	Loss: 0.1339
Training Epoch: 62 [19712/50048]	Loss: 0.1764
Training Epoch: 62 [19840/50048]	Loss: 0.1927
Training Epoch: 62 [19968/50048]	Loss: 0.2051
Training Epoch: 62 [20096/50048]	Loss: 0.1826
Training Epoch: 62 [20224/50048]	Loss: 0.1399
Training Epoch: 62 [20352/50048]	Loss: 0.1365
Training Epoch: 62 [20480/50048]	Loss: 0.1056
Training Epoch: 62 [20608/50048]	Loss: 0.1356
Training Epoch: 62 [20736/50048]	Loss: 0.1372
Training Epoch: 62 [20864/50048]	Loss: 0.1501
Training Epoch: 62 [20992/50048]	Loss: 0.1343
Training Epoch: 62 [21120/50048]	Loss: 0.2082
Training Epoch: 62 [21248/50048]	Loss: 0.1084
Training Epoch: 62 [21376/50048]	Loss: 0.2270
Training Epoch: 62 [21504/50048]	Loss: 0.2388
Training Epoch: 62 [21632/50048]	Loss: 0.2115
Training Epoch: 62 [21760/50048]	Loss: 0.1312
Training Epoch: 62 [21888/50048]	Loss: 0.1606
Training Epoch: 62 [22016/50048]	Loss: 0.2570
Training Epoch: 62 [22144/50048]	Loss: 0.2029
Training Epoch: 62 [22272/50048]	Loss: 0.1906
Training Epoch: 62 [22400/50048]	Loss: 0.3069
Training Epoch: 62 [22528/50048]	Loss: 0.1836
Training Epoch: 62 [22656/50048]	Loss: 0.1229
Training Epoch: 62 [22784/50048]	Loss: 0.2274
Training Epoch: 62 [22912/50048]	Loss: 0.1888
Training Epoch: 62 [23040/50048]	Loss: 0.2017
Training Epoch: 62 [23168/50048]	Loss: 0.0660
Training Epoch: 62 [23296/50048]	Loss: 0.1717
Training Epoch: 62 [23424/50048]	Loss: 0.1364
Training Epoch: 62 [23552/50048]	Loss: 0.2032
Training Epoch: 62 [23680/50048]	Loss: 0.2598
Training Epoch: 62 [23808/50048]	Loss: 0.2619
Training Epoch: 62 [23936/50048]	Loss: 0.1426
Training Epoch: 62 [24064/50048]	Loss: 0.1959
Training Epoch: 62 [24192/50048]	Loss: 0.1841
Training Epoch: 62 [24320/50048]	Loss: 0.2348
Training Epoch: 62 [24448/50048]	Loss: 0.2198
Training Epoch: 62 [24576/50048]	Loss: 0.2507
Training Epoch: 62 [24704/50048]	Loss: 0.1174
Training Epoch: 62 [24832/50048]	Loss: 0.2379
Training Epoch: 62 [24960/50048]	Loss: 0.1583
Training Epoch: 62 [25088/50048]	Loss: 0.1511
Training Epoch: 62 [25216/50048]	Loss: 0.1044
Training Epoch: 62 [25344/50048]	Loss: 0.1485
Training Epoch: 62 [25472/50048]	Loss: 0.2107
Training Epoch: 62 [25600/50048]	Loss: 0.2283
Training Epoch: 62 [25728/50048]	Loss: 0.2335
Training Epoch: 62 [25856/50048]	Loss: 0.1682
Training Epoch: 62 [25984/50048]	Loss: 0.2956
Training Epoch: 62 [26112/50048]	Loss: 0.1696
Training Epoch: 62 [26240/50048]	Loss: 0.1544
Training Epoch: 62 [26368/50048]	Loss: 0.1769
Training Epoch: 62 [26496/50048]	Loss: 0.1348
Training Epoch: 62 [26624/50048]	Loss: 0.2098
Training Epoch: 62 [26752/50048]	Loss: 0.1906
Training Epoch: 62 [26880/50048]	Loss: 0.1541
Training Epoch: 62 [27008/50048]	Loss: 0.2165
Training Epoch: 62 [27136/50048]	Loss: 0.1311
Training Epoch: 62 [27264/50048]	Loss: 0.1600
Training Epoch: 62 [27392/50048]	Loss: 0.1531
Training Epoch: 62 [27520/50048]	Loss: 0.1747
Training Epoch: 62 [27648/50048]	Loss: 0.1353
Training Epoch: 62 [27776/50048]	Loss: 0.1642
Training Epoch: 62 [27904/50048]	Loss: 0.2807
Training Epoch: 62 [28032/50048]	Loss: 0.2002
Training Epoch: 62 [28160/50048]	Loss: 0.2552
Training Epoch: 62 [28288/50048]	Loss: 0.2112
Training Epoch: 62 [28416/50048]	Loss: 0.1314
Training Epoch: 62 [28544/50048]	Loss: 0.1571
Training Epoch: 62 [28672/50048]	Loss: 0.2069
Training Epoch: 62 [28800/50048]	Loss: 0.1127
Training Epoch: 62 [28928/50048]	Loss: 0.1691
Training Epoch: 62 [29056/50048]	Loss: 0.1560
Training Epoch: 62 [29184/50048]	Loss: 0.3269
Training Epoch: 62 [29312/50048]	Loss: 0.1287
Training Epoch: 62 [29440/50048]	Loss: 0.2168
Training Epoch: 62 [29568/50048]	Loss: 0.2726
Training Epoch: 62 [29696/50048]	Loss: 0.0986
Training Epoch: 62 [29824/50048]	Loss: 0.1547
Training Epoch: 62 [29952/50048]	Loss: 0.1233
Training Epoch: 62 [30080/50048]	Loss: 0.1635
Training Epoch: 62 [30208/50048]	Loss: 0.2173
Training Epoch: 62 [30336/50048]	Loss: 0.2256
Training Epoch: 62 [30464/50048]	Loss: 0.1604
Training Epoch: 62 [30592/50048]	Loss: 0.1892
Training Epoch: 62 [30720/50048]	Loss: 0.1289
Training Epoch: 62 [30848/50048]	Loss: 0.1406
Training Epoch: 62 [30976/50048]	Loss: 0.2826
Training Epoch: 62 [31104/50048]	Loss: 0.1701
Training Epoch: 62 [31232/50048]	Loss: 0.2120
Training Epoch: 62 [31360/50048]	Loss: 0.2931
Training Epoch: 62 [31488/50048]	Loss: 0.1143
Training Epoch: 62 [31616/50048]	Loss: 0.1337
Training Epoch: 62 [31744/50048]	Loss: 0.1781
Training Epoch: 62 [31872/50048]	Loss: 0.1829
Training Epoch: 62 [32000/50048]	Loss: 0.1866
Training Epoch: 62 [32128/50048]	Loss: 0.1934
Training Epoch: 62 [32256/50048]	Loss: 0.1552
Training Epoch: 62 [32384/50048]	Loss: 0.1692
Training Epoch: 62 [32512/50048]	Loss: 0.1330
Training Epoch: 62 [32640/50048]	Loss: 0.0646
Training Epoch: 62 [32768/50048]	Loss: 0.0965
Training Epoch: 62 [32896/50048]	Loss: 0.1592
Training Epoch: 62 [33024/50048]	Loss: 0.1063
Training Epoch: 62 [33152/50048]	Loss: 0.1074
Training Epoch: 62 [33280/50048]	Loss: 0.0892
Training Epoch: 62 [33408/50048]	Loss: 0.1038
Training Epoch: 62 [33536/50048]	Loss: 0.1967
Training Epoch: 62 [33664/50048]	Loss: 0.1125
Training Epoch: 62 [33792/50048]	Loss: 0.1525
Training Epoch: 62 [33920/50048]	Loss: 0.1643
Training Epoch: 62 [34048/50048]	Loss: 0.0855
Training Epoch: 62 [34176/50048]	Loss: 0.1377
Training Epoch: 62 [34304/50048]	Loss: 0.0767
Training Epoch: 62 [34432/50048]	Loss: 0.2662
Training Epoch: 62 [34560/50048]	Loss: 0.1774
Training Epoch: 62 [34688/50048]	Loss: 0.1404
Training Epoch: 62 [34816/50048]	Loss: 0.1995
Training Epoch: 62 [34944/50048]	Loss: 0.2225
Training Epoch: 62 [35072/50048]	Loss: 0.1940
Training Epoch: 62 [35200/50048]	Loss: 0.2440
Training Epoch: 62 [35328/50048]	Loss: 0.1622
Training Epoch: 62 [35456/50048]	Loss: 0.1745
Training Epoch: 62 [35584/50048]	Loss: 0.1831
Training Epoch: 62 [35712/50048]	Loss: 0.1152
Training Epoch: 62 [35840/50048]	Loss: 0.1799
Training Epoch: 62 [35968/50048]	Loss: 0.2604
Training Epoch: 62 [36096/50048]	Loss: 0.1424
Training Epoch: 62 [36224/50048]	Loss: 0.1484
Training Epoch: 62 [36352/50048]	Loss: 0.2820
Training Epoch: 62 [36480/50048]	Loss: 0.0793
Training Epoch: 62 [36608/50048]	Loss: 0.1252
Training Epoch: 62 [36736/50048]	Loss: 0.1867
Training Epoch: 62 [36864/50048]	Loss: 0.1266
Training Epoch: 62 [36992/50048]	Loss: 0.1620
Training Epoch: 62 [37120/50048]	Loss: 0.1157
Training Epoch: 62 [37248/50048]	Loss: 0.1707
Training Epoch: 62 [37376/50048]	Loss: 0.1805
Training Epoch: 62 [37504/50048]	Loss: 0.1608
Training Epoch: 62 [37632/50048]	Loss: 0.1339
Training Epoch: 62 [37760/50048]	Loss: 0.1610
Training Epoch: 62 [37888/50048]	Loss: 0.1619
Training Epoch: 62 [38016/50048]	Loss: 0.2547
Training Epoch: 62 [38144/50048]	Loss: 0.2915
Training Epoch: 62 [38272/50048]	Loss: 0.1720
Training Epoch: 62 [38400/50048]	Loss: 0.1392
Training Epoch: 62 [38528/50048]	Loss: 0.1802
Training Epoch: 62 [38656/50048]	Loss: 0.1749
Training Epoch: 62 [38784/50048]	Loss: 0.1870
Training Epoch: 62 [38912/50048]	Loss: 0.1514
Training Epoch: 62 [39040/50048]	Loss: 0.1567
Training Epoch: 62 [39168/50048]	Loss: 0.1952
Training Epoch: 62 [39296/50048]	Loss: 0.1427
Training Epoch: 62 [39424/50048]	Loss: 0.1718
Training Epoch: 62 [39552/50048]	Loss: 0.1757
Training Epoch: 62 [39680/50048]	Loss: 0.2149
Training Epoch: 62 [39808/50048]	Loss: 0.1097
Training Epoch: 62 [39936/50048]	Loss: 0.0703
Training Epoch: 62 [40064/50048]	Loss: 0.2248
Training Epoch: 62 [40192/50048]	Loss: 0.0947
Training Epoch: 62 [40320/50048]	Loss: 0.1732
Training Epoch: 62 [40448/50048]	Loss: 0.2658
Training Epoch: 62 [40576/50048]	Loss: 0.1919
Training Epoch: 62 [40704/50048]	Loss: 0.2015
Training Epoch: 62 [40832/50048]	Loss: 0.1672
Training Epoch: 62 [40960/50048]	Loss: 0.2962
Training Epoch: 62 [41088/50048]	Loss: 0.1788
Training Epoch: 62 [41216/50048]	Loss: 0.3163
Training Epoch: 62 [41344/50048]	Loss: 0.2632
Training Epoch: 62 [41472/50048]	Loss: 0.1917
Training Epoch: 62 [41600/50048]	Loss: 0.2141
Training Epoch: 62 [41728/50048]	Loss: 0.1404
Training Epoch: 62 [41856/50048]	Loss: 0.1575
Training Epoch: 62 [41984/50048]	Loss: 0.1350
Training Epoch: 62 [42112/50048]	Loss: 0.3502
Training Epoch: 62 [42240/50048]	Loss: 0.1226
Training Epoch: 62 [42368/50048]	Loss: 0.2939
Training Epoch: 62 [42496/50048]	Loss: 0.1572
Training Epoch: 62 [42624/50048]	Loss: 0.1637
Training Epoch: 62 [42752/50048]	Loss: 0.2066
Training Epoch: 62 [42880/50048]	Loss: 0.2101
Training Epoch: 62 [43008/50048]	Loss: 0.1500
Training Epoch: 62 [43136/50048]	Loss: 0.2088
Training Epoch: 62 [43264/50048]	Loss: 0.1825
Training Epoch: 62 [43392/50048]	Loss: 0.1092
Training Epoch: 62 [43520/50048]	Loss: 0.1979
Training Epoch: 62 [43648/50048]	Loss: 0.1018
Training Epoch: 62 [43776/50048]	Loss: 0.2091
Training Epoch: 62 [43904/50048]	Loss: 0.2091
Training Epoch: 62 [44032/50048]	Loss: 0.1962
Training Epoch: 62 [44160/50048]	Loss: 0.3125
Training Epoch: 62 [44288/50048]	Loss: 0.1617
Training Epoch: 62 [44416/50048]	Loss: 0.2516
Training Epoch: 62 [44544/50048]	Loss: 0.1546
Training Epoch: 62 [44672/50048]	Loss: 0.2386
Training Epoch: 62 [44800/50048]	Loss: 0.1496
Training Epoch: 62 [44928/50048]	Loss: 0.1523
Training Epoch: 62 [45056/50048]	Loss: 0.1371
Training Epoch: 62 [45184/50048]	Loss: 0.1695
Training Epoch: 62 [45312/50048]	Loss: 0.2045
Training Epoch: 62 [45440/50048]	Loss: 0.1933
Training Epoch: 62 [45568/50048]	Loss: 0.1891
Training Epoch: 62 [45696/50048]	Loss: 0.2482
2022-12-06 05:12:04,541 [ZeusDataLoader(train)] train epoch 63 done: time=86.62 energy=10508.36
2022-12-06 05:12:04,543 [ZeusDataLoader(eval)] Epoch 63 begin.
Training Epoch: 62 [45824/50048]	Loss: 0.2271
Training Epoch: 62 [45952/50048]	Loss: 0.3041
Training Epoch: 62 [46080/50048]	Loss: 0.2621
Training Epoch: 62 [46208/50048]	Loss: 0.1904
Training Epoch: 62 [46336/50048]	Loss: 0.2220
Training Epoch: 62 [46464/50048]	Loss: 0.1769
Training Epoch: 62 [46592/50048]	Loss: 0.2553
Training Epoch: 62 [46720/50048]	Loss: 0.1775
Training Epoch: 62 [46848/50048]	Loss: 0.2588
Training Epoch: 62 [46976/50048]	Loss: 0.2341
Training Epoch: 62 [47104/50048]	Loss: 0.2216
Training Epoch: 62 [47232/50048]	Loss: 0.1718
Training Epoch: 62 [47360/50048]	Loss: 0.1196
Training Epoch: 62 [47488/50048]	Loss: 0.1547
Training Epoch: 62 [47616/50048]	Loss: 0.1791
Training Epoch: 62 [47744/50048]	Loss: 0.2260
Training Epoch: 62 [47872/50048]	Loss: 0.2115
Training Epoch: 62 [48000/50048]	Loss: 0.2364
Training Epoch: 62 [48128/50048]	Loss: 0.2431
Training Epoch: 62 [48256/50048]	Loss: 0.1658
Training Epoch: 62 [48384/50048]	Loss: 0.2087
Training Epoch: 62 [48512/50048]	Loss: 0.2377
Training Epoch: 62 [48640/50048]	Loss: 0.1986
Training Epoch: 62 [48768/50048]	Loss: 0.2088
Training Epoch: 62 [48896/50048]	Loss: 0.3385
Training Epoch: 62 [49024/50048]	Loss: 0.2094
Training Epoch: 62 [49152/50048]	Loss: 0.1918
Training Epoch: 62 [49280/50048]	Loss: 0.1754
Training Epoch: 62 [49408/50048]	Loss: 0.1995
Training Epoch: 62 [49536/50048]	Loss: 0.1496
Training Epoch: 62 [49664/50048]	Loss: 0.1765
Training Epoch: 62 [49792/50048]	Loss: 0.2791
Training Epoch: 62 [49920/50048]	Loss: 0.1999
Training Epoch: 62 [50048/50048]	Loss: 0.1655
2022-12-06 10:12:08.270 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:12:08,310 [ZeusDataLoader(eval)] eval epoch 63 done: time=3.76 energy=452.95
2022-12-06 05:12:08,310 [ZeusDataLoader(train)] Up to epoch 63: time=5684.71, energy=690042.43, cost=842433.12
2022-12-06 05:12:08,311 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:12:08,311 [ZeusDataLoader(train)] Expected next epoch: time=5774.51, energy=700840.44, cost=855689.50
2022-12-06 05:12:08,312 [ZeusDataLoader(train)] Epoch 64 begin.
Validation Epoch: 62, Average loss: 0.0166, Accuracy: 0.6309
2022-12-06 05:12:08,452 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:12:08,452 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:12:08.454 [ZeusMonitor] Monitor started.
2022-12-06 10:12:08.454 [ZeusMonitor] Running indefinitely. 2022-12-06 10:12:08.454 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:12:08.454 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e64+gpu0.power.log
Training Epoch: 63 [128/50048]	Loss: 0.1497
Training Epoch: 63 [256/50048]	Loss: 0.1542
Training Epoch: 63 [384/50048]	Loss: 0.1578
Training Epoch: 63 [512/50048]	Loss: 0.1528
Training Epoch: 63 [640/50048]	Loss: 0.1380
Training Epoch: 63 [768/50048]	Loss: 0.1523
Training Epoch: 63 [896/50048]	Loss: 0.2201
Training Epoch: 63 [1024/50048]	Loss: 0.1295
Training Epoch: 63 [1152/50048]	Loss: 0.1618
Training Epoch: 63 [1280/50048]	Loss: 0.1113
Training Epoch: 63 [1408/50048]	Loss: 0.1899
Training Epoch: 63 [1536/50048]	Loss: 0.1517
Training Epoch: 63 [1664/50048]	Loss: 0.1830
Training Epoch: 63 [1792/50048]	Loss: 0.1880
Training Epoch: 63 [1920/50048]	Loss: 0.1336
Training Epoch: 63 [2048/50048]	Loss: 0.1809
Training Epoch: 63 [2176/50048]	Loss: 0.1489
Training Epoch: 63 [2304/50048]	Loss: 0.1146
Training Epoch: 63 [2432/50048]	Loss: 0.1668
Training Epoch: 63 [2560/50048]	Loss: 0.2730
Training Epoch: 63 [2688/50048]	Loss: 0.1281
Training Epoch: 63 [2816/50048]	Loss: 0.2628
Training Epoch: 63 [2944/50048]	Loss: 0.0812
Training Epoch: 63 [3072/50048]	Loss: 0.1306
Training Epoch: 63 [3200/50048]	Loss: 0.1357
Training Epoch: 63 [3328/50048]	Loss: 0.1395
Training Epoch: 63 [3456/50048]	Loss: 0.1047
Training Epoch: 63 [3584/50048]	Loss: 0.1621
Training Epoch: 63 [3712/50048]	Loss: 0.1008
Training Epoch: 63 [3840/50048]	Loss: 0.1952
Training Epoch: 63 [3968/50048]	Loss: 0.0901
Training Epoch: 63 [4096/50048]	Loss: 0.1219
Training Epoch: 63 [4224/50048]	Loss: 0.2607
Training Epoch: 63 [4352/50048]	Loss: 0.1708
Training Epoch: 63 [4480/50048]	Loss: 0.1479
Training Epoch: 63 [4608/50048]	Loss: 0.1185
Training Epoch: 63 [4736/50048]	Loss: 0.1315
Training Epoch: 63 [4864/50048]	Loss: 0.1310
Training Epoch: 63 [4992/50048]	Loss: 0.1531
Training Epoch: 63 [5120/50048]	Loss: 0.1081
Training Epoch: 63 [5248/50048]	Loss: 0.1137
Training Epoch: 63 [5376/50048]	Loss: 0.2295
Training Epoch: 63 [5504/50048]	Loss: 0.1545
Training Epoch: 63 [5632/50048]	Loss: 0.1482
Training Epoch: 63 [5760/50048]	Loss: 0.1525
Training Epoch: 63 [5888/50048]	Loss: 0.1496
Training Epoch: 63 [6016/50048]	Loss: 0.1358
Training Epoch: 63 [6144/50048]	Loss: 0.2263
Training Epoch: 63 [6272/50048]	Loss: 0.1984
Training Epoch: 63 [6400/50048]	Loss: 0.1570
Training Epoch: 63 [6528/50048]	Loss: 0.1518
Training Epoch: 63 [6656/50048]	Loss: 0.2891
Training Epoch: 63 [6784/50048]	Loss: 0.1627
Training Epoch: 63 [6912/50048]	Loss: 0.2772
Training Epoch: 63 [7040/50048]	Loss: 0.2589
Training Epoch: 63 [7168/50048]	Loss: 0.1590
Training Epoch: 63 [7296/50048]	Loss: 0.0681
Training Epoch: 63 [7424/50048]	Loss: 0.1685
Training Epoch: 63 [7552/50048]	Loss: 0.1665
Training Epoch: 63 [7680/50048]	Loss: 0.1377
Training Epoch: 63 [7808/50048]	Loss: 0.1638
Training Epoch: 63 [7936/50048]	Loss: 0.1765
Training Epoch: 63 [8064/50048]	Loss: 0.1802
Training Epoch: 63 [8192/50048]	Loss: 0.2060
Training Epoch: 63 [8320/50048]	Loss: 0.0986
Training Epoch: 63 [8448/50048]	Loss: 0.0849
Training Epoch: 63 [8576/50048]	Loss: 0.1086
Training Epoch: 63 [8704/50048]	Loss: 0.1016
Training Epoch: 63 [8832/50048]	Loss: 0.0807
Training Epoch: 63 [8960/50048]	Loss: 0.1470
Training Epoch: 63 [9088/50048]	Loss: 0.2026
Training Epoch: 63 [9216/50048]	Loss: 0.1537
Training Epoch: 63 [9344/50048]	Loss: 0.1309
Training Epoch: 63 [9472/50048]	Loss: 0.1208
Training Epoch: 63 [9600/50048]	Loss: 0.1433
Training Epoch: 63 [9728/50048]	Loss: 0.1667
Training Epoch: 63 [9856/50048]	Loss: 0.1316
Training Epoch: 63 [9984/50048]	Loss: 0.1539
Training Epoch: 63 [10112/50048]	Loss: 0.2244
Training Epoch: 63 [10240/50048]	Loss: 0.1605
Training Epoch: 63 [10368/50048]	Loss: 0.1227
Training Epoch: 63 [10496/50048]	Loss: 0.1456
Training Epoch: 63 [10624/50048]	Loss: 0.2261
Training Epoch: 63 [10752/50048]	Loss: 0.2960
Training Epoch: 63 [10880/50048]	Loss: 0.1861
Training Epoch: 63 [11008/50048]	Loss: 0.1823
Training Epoch: 63 [11136/50048]	Loss: 0.1294
Training Epoch: 63 [11264/50048]	Loss: 0.1268
Training Epoch: 63 [11392/50048]	Loss: 0.2036
Training Epoch: 63 [11520/50048]	Loss: 0.2033
Training Epoch: 63 [11648/50048]	Loss: 0.0859
Training Epoch: 63 [11776/50048]	Loss: 0.2561
Training Epoch: 63 [11904/50048]	Loss: 0.2167
Training Epoch: 63 [12032/50048]	Loss: 0.1667
Training Epoch: 63 [12160/50048]	Loss: 0.1390
Training Epoch: 63 [12288/50048]	Loss: 0.1323
Training Epoch: 63 [12416/50048]	Loss: 0.1479
Training Epoch: 63 [12544/50048]	Loss: 0.1658
Training Epoch: 63 [12672/50048]	Loss: 0.1100
Training Epoch: 63 [12800/50048]	Loss: 0.1382
Training Epoch: 63 [12928/50048]	Loss: 0.0993
Training Epoch: 63 [13056/50048]	Loss: 0.1191
Training Epoch: 63 [13184/50048]	Loss: 0.2016
Training Epoch: 63 [13312/50048]	Loss: 0.2119
Training Epoch: 63 [13440/50048]	Loss: 0.1151
Training Epoch: 63 [13568/50048]	Loss: 0.1470
Training Epoch: 63 [13696/50048]	Loss: 0.1335
Training Epoch: 63 [13824/50048]	Loss: 0.1886
Training Epoch: 63 [13952/50048]	Loss: 0.1721
Training Epoch: 63 [14080/50048]	Loss: 0.2146
Training Epoch: 63 [14208/50048]	Loss: 0.1301
Training Epoch: 63 [14336/50048]	Loss: 0.1578
Training Epoch: 63 [14464/50048]	Loss: 0.1500
Training Epoch: 63 [14592/50048]	Loss: 0.1747
Training Epoch: 63 [14720/50048]	Loss: 0.2198
Training Epoch: 63 [14848/50048]	Loss: 0.1343
Training Epoch: 63 [14976/50048]	Loss: 0.2076
Training Epoch: 63 [15104/50048]	Loss: 0.1427
Training Epoch: 63 [15232/50048]	Loss: 0.2020
Training Epoch: 63 [15360/50048]	Loss: 0.1773
Training Epoch: 63 [15488/50048]	Loss: 0.1330
Training Epoch: 63 [15616/50048]	Loss: 0.1724
Training Epoch: 63 [15744/50048]	Loss: 0.1536
Training Epoch: 63 [15872/50048]	Loss: 0.0906
Training Epoch: 63 [16000/50048]	Loss: 0.2415
Training Epoch: 63 [16128/50048]	Loss: 0.1490
Training Epoch: 63 [16256/50048]	Loss: 0.1760
Training Epoch: 63 [16384/50048]	Loss: 0.1331
Training Epoch: 63 [16512/50048]	Loss: 0.1035
Training Epoch: 63 [16640/50048]	Loss: 0.1022
Training Epoch: 63 [16768/50048]	Loss: 0.2032
Training Epoch: 63 [16896/50048]	Loss: 0.1262
Training Epoch: 63 [17024/50048]	Loss: 0.2617
Training Epoch: 63 [17152/50048]	Loss: 0.1789
Training Epoch: 63 [17280/50048]	Loss: 0.1542
Training Epoch: 63 [17408/50048]	Loss: 0.1009
Training Epoch: 63 [17536/50048]	Loss: 0.1446
Training Epoch: 63 [17664/50048]	Loss: 0.1840
Training Epoch: 63 [17792/50048]	Loss: 0.1555
Training Epoch: 63 [17920/50048]	Loss: 0.1685
Training Epoch: 63 [18048/50048]	Loss: 0.2516
Training Epoch: 63 [18176/50048]	Loss: 0.1988
Training Epoch: 63 [18304/50048]	Loss: 0.1680
Training Epoch: 63 [18432/50048]	Loss: 0.1679
Training Epoch: 63 [18560/50048]	Loss: 0.1457
Training Epoch: 63 [18688/50048]	Loss: 0.1510
Training Epoch: 63 [18816/50048]	Loss: 0.1717
Training Epoch: 63 [18944/50048]	Loss: 0.1087
Training Epoch: 63 [19072/50048]	Loss: 0.1436
Training Epoch: 63 [19200/50048]	Loss: 0.2343
Training Epoch: 63 [19328/50048]	Loss: 0.1891
Training Epoch: 63 [19456/50048]	Loss: 0.2310
Training Epoch: 63 [19584/50048]	Loss: 0.1732
Training Epoch: 63 [19712/50048]	Loss: 0.1352
Training Epoch: 63 [19840/50048]	Loss: 0.1597
Training Epoch: 63 [19968/50048]	Loss: 0.1352
Training Epoch: 63 [20096/50048]	Loss: 0.1890
Training Epoch: 63 [20224/50048]	Loss: 0.1196
Training Epoch: 63 [20352/50048]	Loss: 0.1227
Training Epoch: 63 [20480/50048]	Loss: 0.2005
Training Epoch: 63 [20608/50048]	Loss: 0.1817
Training Epoch: 63 [20736/50048]	Loss: 0.1501
Training Epoch: 63 [20864/50048]	Loss: 0.1284
Training Epoch: 63 [20992/50048]	Loss: 0.1474
Training Epoch: 63 [21120/50048]	Loss: 0.1373
Training Epoch: 63 [21248/50048]	Loss: 0.1387
Training Epoch: 63 [21376/50048]	Loss: 0.0885
Training Epoch: 63 [21504/50048]	Loss: 0.2327
Training Epoch: 63 [21632/50048]	Loss: 0.1288
Training Epoch: 63 [21760/50048]	Loss: 0.1444
Training Epoch: 63 [21888/50048]	Loss: 0.1308
Training Epoch: 63 [22016/50048]	Loss: 0.2715
Training Epoch: 63 [22144/50048]	Loss: 0.1439
Training Epoch: 63 [22272/50048]	Loss: 0.1400
Training Epoch: 63 [22400/50048]	Loss: 0.1295
Training Epoch: 63 [22528/50048]	Loss: 0.1974
Training Epoch: 63 [22656/50048]	Loss: 0.1532
Training Epoch: 63 [22784/50048]	Loss: 0.1422
Training Epoch: 63 [22912/50048]	Loss: 0.1214
Training Epoch: 63 [23040/50048]	Loss: 0.1936
Training Epoch: 63 [23168/50048]	Loss: 0.1958
Training Epoch: 63 [23296/50048]	Loss: 0.1893
Training Epoch: 63 [23424/50048]	Loss: 0.2605
Training Epoch: 63 [23552/50048]	Loss: 0.2591
Training Epoch: 63 [23680/50048]	Loss: 0.1210
Training Epoch: 63 [23808/50048]	Loss: 0.1165
Training Epoch: 63 [23936/50048]	Loss: 0.1830
Training Epoch: 63 [24064/50048]	Loss: 0.1067
Training Epoch: 63 [24192/50048]	Loss: 0.2605
Training Epoch: 63 [24320/50048]	Loss: 0.2004
Training Epoch: 63 [24448/50048]	Loss: 0.2683
Training Epoch: 63 [24576/50048]	Loss: 0.1185
Training Epoch: 63 [24704/50048]	Loss: 0.1902
Training Epoch: 63 [24832/50048]	Loss: 0.1792
Training Epoch: 63 [24960/50048]	Loss: 0.1414
Training Epoch: 63 [25088/50048]	Loss: 0.1499
Training Epoch: 63 [25216/50048]	Loss: 0.1458
Training Epoch: 63 [25344/50048]	Loss: 0.2240
Training Epoch: 63 [25472/50048]	Loss: 0.1722
Training Epoch: 63 [25600/50048]	Loss: 0.1970
Training Epoch: 63 [25728/50048]	Loss: 0.1683
Training Epoch: 63 [25856/50048]	Loss: 0.1171
Training Epoch: 63 [25984/50048]	Loss: 0.1221
Training Epoch: 63 [26112/50048]	Loss: 0.2390
Training Epoch: 63 [26240/50048]	Loss: 0.1805
Training Epoch: 63 [26368/50048]	Loss: 0.2244
Training Epoch: 63 [26496/50048]	Loss: 0.1606
Training Epoch: 63 [26624/50048]	Loss: 0.1704
Training Epoch: 63 [26752/50048]	Loss: 0.1848
Training Epoch: 63 [26880/50048]	Loss: 0.1500
Training Epoch: 63 [27008/50048]	Loss: 0.2129
Training Epoch: 63 [27136/50048]	Loss: 0.2253
Training Epoch: 63 [27264/50048]	Loss: 0.1587
Training Epoch: 63 [27392/50048]	Loss: 0.1303
Training Epoch: 63 [27520/50048]	Loss: 0.1424
Training Epoch: 63 [27648/50048]	Loss: 0.1252
Training Epoch: 63 [27776/50048]	Loss: 0.2000
Training Epoch: 63 [27904/50048]	Loss: 0.2078
Training Epoch: 63 [28032/50048]	Loss: 0.1485
Training Epoch: 63 [28160/50048]	Loss: 0.1372
Training Epoch: 63 [28288/50048]	Loss: 0.1266
Training Epoch: 63 [28416/50048]	Loss: 0.1697
Training Epoch: 63 [28544/50048]	Loss: 0.1600
Training Epoch: 63 [28672/50048]	Loss: 0.2063
Training Epoch: 63 [28800/50048]	Loss: 0.1022
Training Epoch: 63 [28928/50048]	Loss: 0.1048
Training Epoch: 63 [29056/50048]	Loss: 0.1634
Training Epoch: 63 [29184/50048]	Loss: 0.0893
Training Epoch: 63 [29312/50048]	Loss: 0.1242
Training Epoch: 63 [29440/50048]	Loss: 0.1687
Training Epoch: 63 [29568/50048]	Loss: 0.0950
Training Epoch: 63 [29696/50048]	Loss: 0.1153
Training Epoch: 63 [29824/50048]	Loss: 0.1554
Training Epoch: 63 [29952/50048]	Loss: 0.1010
Training Epoch: 63 [30080/50048]	Loss: 0.1140
Training Epoch: 63 [30208/50048]	Loss: 0.2105
Training Epoch: 63 [30336/50048]	Loss: 0.1770
Training Epoch: 63 [30464/50048]	Loss: 0.2004
Training Epoch: 63 [30592/50048]	Loss: 0.1662
Training Epoch: 63 [30720/50048]	Loss: 0.2059
Training Epoch: 63 [30848/50048]	Loss: 0.1366
Training Epoch: 63 [30976/50048]	Loss: 0.1239
Training Epoch: 63 [31104/50048]	Loss: 0.1444
Training Epoch: 63 [31232/50048]	Loss: 0.1298
Training Epoch: 63 [31360/50048]	Loss: 0.1725
Training Epoch: 63 [31488/50048]	Loss: 0.1643
Training Epoch: 63 [31616/50048]	Loss: 0.2055
Training Epoch: 63 [31744/50048]	Loss: 0.1229
Training Epoch: 63 [31872/50048]	Loss: 0.2311
Training Epoch: 63 [32000/50048]	Loss: 0.1195
Training Epoch: 63 [32128/50048]	Loss: 0.1380
Training Epoch: 63 [32256/50048]	Loss: 0.2286
Training Epoch: 63 [32384/50048]	Loss: 0.2246
Training Epoch: 63 [32512/50048]	Loss: 0.2341
Training Epoch: 63 [32640/50048]	Loss: 0.1985
Training Epoch: 63 [32768/50048]	Loss: 0.2320
Training Epoch: 63 [32896/50048]	Loss: 0.1862
Training Epoch: 63 [33024/50048]	Loss: 0.1593
Training Epoch: 63 [33152/50048]	Loss: 0.1318
Training Epoch: 63 [33280/50048]	Loss: 0.1865
Training Epoch: 63 [33408/50048]	Loss: 0.0706
Training Epoch: 63 [33536/50048]	Loss: 0.0885
Training Epoch: 63 [33664/50048]	Loss: 0.2495
Training Epoch: 63 [33792/50048]	Loss: 0.1878
Training Epoch: 63 [33920/50048]	Loss: 0.1522
Training Epoch: 63 [34048/50048]	Loss: 0.1353
Training Epoch: 63 [34176/50048]	Loss: 0.1196
Training Epoch: 63 [34304/50048]	Loss: 0.2268
Training Epoch: 63 [34432/50048]	Loss: 0.2455
Training Epoch: 63 [34560/50048]	Loss: 0.1431
Training Epoch: 63 [34688/50048]	Loss: 0.2191
Training Epoch: 63 [34816/50048]	Loss: 0.2389
Training Epoch: 63 [34944/50048]	Loss: 0.2382
Training Epoch: 63 [35072/50048]	Loss: 0.2015
Training Epoch: 63 [35200/50048]	Loss: 0.1594
Training Epoch: 63 [35328/50048]	Loss: 0.1918
Training Epoch: 63 [35456/50048]	Loss: 0.2023
Training Epoch: 63 [35584/50048]	Loss: 0.2021
Training Epoch: 63 [35712/50048]	Loss: 0.1468
Training Epoch: 63 [35840/50048]	Loss: 0.1913
Training Epoch: 63 [35968/50048]	Loss: 0.1502
Training Epoch: 63 [36096/50048]	Loss: 0.1414
Training Epoch: 63 [36224/50048]	Loss: 0.1181
Training Epoch: 63 [36352/50048]	Loss: 0.1804
Training Epoch: 63 [36480/50048]	Loss: 0.1589
Training Epoch: 63 [36608/50048]	Loss: 0.0878
Training Epoch: 63 [36736/50048]	Loss: 0.2369
Training Epoch: 63 [36864/50048]	Loss: 0.1153
Training Epoch: 63 [36992/50048]	Loss: 0.2808
Training Epoch: 63 [37120/50048]	Loss: 0.1677
Training Epoch: 63 [37248/50048]	Loss: 0.1687
Training Epoch: 63 [37376/50048]	Loss: 0.1274
Training Epoch: 63 [37504/50048]	Loss: 0.1590
Training Epoch: 63 [37632/50048]	Loss: 0.2078
Training Epoch: 63 [37760/50048]	Loss: 0.1631
Training Epoch: 63 [37888/50048]	Loss: 0.1391
Training Epoch: 63 [38016/50048]	Loss: 0.1265
Training Epoch: 63 [38144/50048]	Loss: 0.1470
Training Epoch: 63 [38272/50048]	Loss: 0.2332
Training Epoch: 63 [38400/50048]	Loss: 0.2470
Training Epoch: 63 [38528/50048]	Loss: 0.2668
Training Epoch: 63 [38656/50048]	Loss: 0.1269
Training Epoch: 63 [38784/50048]	Loss: 0.1541
Training Epoch: 63 [38912/50048]	Loss: 0.2043
Training Epoch: 63 [39040/50048]	Loss: 0.2036
Training Epoch: 63 [39168/50048]	Loss: 0.1933
Training Epoch: 63 [39296/50048]	Loss: 0.2158
Training Epoch: 63 [39424/50048]	Loss: 0.1422
Training Epoch: 63 [39552/50048]	Loss: 0.1891
Training Epoch: 63 [39680/50048]	Loss: 0.1406
Training Epoch: 63 [39808/50048]	Loss: 0.1943
Training Epoch: 63 [39936/50048]	Loss: 0.1935
Training Epoch: 63 [40064/50048]	Loss: 0.2515
Training Epoch: 63 [40192/50048]	Loss: 0.2100
Training Epoch: 63 [40320/50048]	Loss: 0.1666
Training Epoch: 63 [40448/50048]	Loss: 0.2359
Training Epoch: 63 [40576/50048]	Loss: 0.2573
Training Epoch: 63 [40704/50048]	Loss: 0.1612
Training Epoch: 63 [40832/50048]	Loss: 0.2465
Training Epoch: 63 [40960/50048]	Loss: 0.2872
Training Epoch: 63 [41088/50048]	Loss: 0.1917
Training Epoch: 63 [41216/50048]	Loss: 0.2303
Training Epoch: 63 [41344/50048]	Loss: 0.1848
Training Epoch: 63 [41472/50048]	Loss: 0.2931
Training Epoch: 63 [41600/50048]	Loss: 0.1223
Training Epoch: 63 [41728/50048]	Loss: 0.1896
Training Epoch: 63 [41856/50048]	Loss: 0.2226
Training Epoch: 63 [41984/50048]	Loss: 0.2739
Training Epoch: 63 [42112/50048]	Loss: 0.2352
Training Epoch: 63 [42240/50048]	Loss: 0.1134
Training Epoch: 63 [42368/50048]	Loss: 0.1122
Training Epoch: 63 [42496/50048]	Loss: 0.2176
Training Epoch: 63 [42624/50048]	Loss: 0.1891
Training Epoch: 63 [42752/50048]	Loss: 0.3346
Training Epoch: 63 [42880/50048]	Loss: 0.1134
Training Epoch: 63 [43008/50048]	Loss: 0.2238
Training Epoch: 63 [43136/50048]	Loss: 0.1951
Training Epoch: 63 [43264/50048]	Loss: 0.1430
Training Epoch: 63 [43392/50048]	Loss: 0.1551
Training Epoch: 63 [43520/50048]	Loss: 0.1250
Training Epoch: 63 [43648/50048]	Loss: 0.1930
Training Epoch: 63 [43776/50048]	Loss: 0.2091
Training Epoch: 63 [43904/50048]	Loss: 0.2864
Training Epoch: 63 [44032/50048]	Loss: 0.1823
Training Epoch: 63 [44160/50048]	Loss: 0.2850
Training Epoch: 63 [44288/50048]	Loss: 0.1769
Training Epoch: 63 [44416/50048]	Loss: 0.1948
Training Epoch: 63 [44544/50048]	Loss: 0.2406
Training Epoch: 63 [44672/50048]	Loss: 0.2599
Training Epoch: 63 [44800/50048]	Loss: 0.2396
Training Epoch: 63 [44928/50048]	Loss: 0.2058
Training Epoch: 63 [45056/50048]	Loss: 0.1170
Training Epoch: 63 [45184/50048]	Loss: 0.1778
Training Epoch: 63 [45312/50048]	Loss: 0.2829
Training Epoch: 63 [45440/50048]	Loss: 0.1597
Training Epoch: 63 [45568/50048]	Loss: 0.2077
Training Epoch: 63 [45696/50048]	Loss: 0.2396
2022-12-06 05:13:34,812 [ZeusDataLoader(train)] train epoch 64 done: time=86.49 energy=10506.99
2022-12-06 05:13:34,813 [ZeusDataLoader(eval)] Epoch 64 begin.
Training Epoch: 63 [45824/50048]	Loss: 0.1825
Training Epoch: 63 [45952/50048]	Loss: 0.1257
Training Epoch: 63 [46080/50048]	Loss: 0.2080
Training Epoch: 63 [46208/50048]	Loss: 0.2364
Training Epoch: 63 [46336/50048]	Loss: 0.2567
Training Epoch: 63 [46464/50048]	Loss: 0.1962
Training Epoch: 63 [46592/50048]	Loss: 0.3334
Training Epoch: 63 [46720/50048]	Loss: 0.1770
Training Epoch: 63 [46848/50048]	Loss: 0.1332
Training Epoch: 63 [46976/50048]	Loss: 0.1094
Training Epoch: 63 [47104/50048]	Loss: 0.0937
Training Epoch: 63 [47232/50048]	Loss: 0.2275
Training Epoch: 63 [47360/50048]	Loss: 0.1235
Training Epoch: 63 [47488/50048]	Loss: 0.2477
Training Epoch: 63 [47616/50048]	Loss: 0.3196
Training Epoch: 63 [47744/50048]	Loss: 0.1854
Training Epoch: 63 [47872/50048]	Loss: 0.1938
Training Epoch: 63 [48000/50048]	Loss: 0.1440
Training Epoch: 63 [48128/50048]	Loss: 0.1328
Training Epoch: 63 [48256/50048]	Loss: 0.1767
Training Epoch: 63 [48384/50048]	Loss: 0.1365
Training Epoch: 63 [48512/50048]	Loss: 0.2590
Training Epoch: 63 [48640/50048]	Loss: 0.0896
Training Epoch: 63 [48768/50048]	Loss: 0.1532
Training Epoch: 63 [48896/50048]	Loss: 0.1355
Training Epoch: 63 [49024/50048]	Loss: 0.2411
Training Epoch: 63 [49152/50048]	Loss: 0.2146
Training Epoch: 63 [49280/50048]	Loss: 0.2396
Training Epoch: 63 [49408/50048]	Loss: 0.1788
Training Epoch: 63 [49536/50048]	Loss: 0.1331
Training Epoch: 63 [49664/50048]	Loss: 0.1491
Training Epoch: 63 [49792/50048]	Loss: 0.1551
Training Epoch: 63 [49920/50048]	Loss: 0.1961
Training Epoch: 63 [50048/50048]	Loss: 0.1115
2022-12-06 10:13:38.529 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:13:38,576 [ZeusDataLoader(eval)] eval epoch 64 done: time=3.75 energy=453.15
2022-12-06 05:13:38,576 [ZeusDataLoader(train)] Up to epoch 64: time=5774.95, energy=701002.58, cost=855809.53
2022-12-06 05:13:38,576 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:13:38,576 [ZeusDataLoader(train)] Expected next epoch: time=5864.75, energy=711800.59, cost=869065.91
2022-12-06 05:13:38,577 [ZeusDataLoader(train)] Epoch 65 begin.
Validation Epoch: 63, Average loss: 0.0166, Accuracy: 0.6384
2022-12-06 05:13:38,772 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:13:38,773 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:13:38.775 [ZeusMonitor] Monitor started.
2022-12-06 10:13:38.775 [ZeusMonitor] Running indefinitely. 2022-12-06 10:13:38.775 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:13:38.775 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e65+gpu0.power.log
Training Epoch: 64 [128/50048]	Loss: 0.2212
Training Epoch: 64 [256/50048]	Loss: 0.1503
Training Epoch: 64 [384/50048]	Loss: 0.1983
Training Epoch: 64 [512/50048]	Loss: 0.1229
Training Epoch: 64 [640/50048]	Loss: 0.1223
Training Epoch: 64 [768/50048]	Loss: 0.1166
Training Epoch: 64 [896/50048]	Loss: 0.1493
Training Epoch: 64 [1024/50048]	Loss: 0.1198
Training Epoch: 64 [1152/50048]	Loss: 0.1540
Training Epoch: 64 [1280/50048]	Loss: 0.0910
Training Epoch: 64 [1408/50048]	Loss: 0.1309
Training Epoch: 64 [1536/50048]	Loss: 0.2067
Training Epoch: 64 [1664/50048]	Loss: 0.1549
Training Epoch: 64 [1792/50048]	Loss: 0.2138
Training Epoch: 64 [1920/50048]	Loss: 0.1342
Training Epoch: 64 [2048/50048]	Loss: 0.1520
Training Epoch: 64 [2176/50048]	Loss: 0.1420
Training Epoch: 64 [2304/50048]	Loss: 0.1477
Training Epoch: 64 [2432/50048]	Loss: 0.1017
Training Epoch: 64 [2560/50048]	Loss: 0.1280
Training Epoch: 64 [2688/50048]	Loss: 0.1283
Training Epoch: 64 [2816/50048]	Loss: 0.1132
Training Epoch: 64 [2944/50048]	Loss: 0.1497
Training Epoch: 64 [3072/50048]	Loss: 0.1303
Training Epoch: 64 [3200/50048]	Loss: 0.1021
Training Epoch: 64 [3328/50048]	Loss: 0.1164
Training Epoch: 64 [3456/50048]	Loss: 0.1950
Training Epoch: 64 [3584/50048]	Loss: 0.1117
Training Epoch: 64 [3712/50048]	Loss: 0.1135
Training Epoch: 64 [3840/50048]	Loss: 0.1850
Training Epoch: 64 [3968/50048]	Loss: 0.1442
Training Epoch: 64 [4096/50048]	Loss: 0.1297
Training Epoch: 64 [4224/50048]	Loss: 0.1065
Training Epoch: 64 [4352/50048]	Loss: 0.1550
Training Epoch: 64 [4480/50048]	Loss: 0.0963
Training Epoch: 64 [4608/50048]	Loss: 0.1372
Training Epoch: 64 [4736/50048]	Loss: 0.1583
Training Epoch: 64 [4864/50048]	Loss: 0.2705
Training Epoch: 64 [4992/50048]	Loss: 0.1612
Training Epoch: 64 [5120/50048]	Loss: 0.1144
Training Epoch: 64 [5248/50048]	Loss: 0.1944
Training Epoch: 64 [5376/50048]	Loss: 0.1890
Training Epoch: 64 [5504/50048]	Loss: 0.1007
Training Epoch: 64 [5632/50048]	Loss: 0.2215
Training Epoch: 64 [5760/50048]	Loss: 0.1105
Training Epoch: 64 [5888/50048]	Loss: 0.1775
Training Epoch: 64 [6016/50048]	Loss: 0.2745
Training Epoch: 64 [6144/50048]	Loss: 0.1646
Training Epoch: 64 [6272/50048]	Loss: 0.1450
Training Epoch: 64 [6400/50048]	Loss: 0.1601
Training Epoch: 64 [6528/50048]	Loss: 0.1734
Training Epoch: 64 [6656/50048]	Loss: 0.1654
Training Epoch: 64 [6784/50048]	Loss: 0.1654
Training Epoch: 64 [6912/50048]	Loss: 0.1452
Training Epoch: 64 [7040/50048]	Loss: 0.1002
Training Epoch: 64 [7168/50048]	Loss: 0.0676
Training Epoch: 64 [7296/50048]	Loss: 0.2315
Training Epoch: 64 [7424/50048]	Loss: 0.0913
Training Epoch: 64 [7552/50048]	Loss: 0.1324
Training Epoch: 64 [7680/50048]	Loss: 0.1697
Training Epoch: 64 [7808/50048]	Loss: 0.1571
Training Epoch: 64 [7936/50048]	Loss: 0.0754
Training Epoch: 64 [8064/50048]	Loss: 0.1465
Training Epoch: 64 [8192/50048]	Loss: 0.1573
Training Epoch: 64 [8320/50048]	Loss: 0.1409
Training Epoch: 64 [8448/50048]	Loss: 0.0609
Training Epoch: 64 [8576/50048]	Loss: 0.1530
Training Epoch: 64 [8704/50048]	Loss: 0.2163
Training Epoch: 64 [8832/50048]	Loss: 0.1320
Training Epoch: 64 [8960/50048]	Loss: 0.1862
Training Epoch: 64 [9088/50048]	Loss: 0.0961
Training Epoch: 64 [9216/50048]	Loss: 0.1487
Training Epoch: 64 [9344/50048]	Loss: 0.1536
Training Epoch: 64 [9472/50048]	Loss: 0.3104
Training Epoch: 64 [9600/50048]	Loss: 0.2683
Training Epoch: 64 [9728/50048]	Loss: 0.1879
Training Epoch: 64 [9856/50048]	Loss: 0.2036
Training Epoch: 64 [9984/50048]	Loss: 0.1180
Training Epoch: 64 [10112/50048]	Loss: 0.1364
Training Epoch: 64 [10240/50048]	Loss: 0.1460
Training Epoch: 64 [10368/50048]	Loss: 0.1420
Training Epoch: 64 [10496/50048]	Loss: 0.1911
Training Epoch: 64 [10624/50048]	Loss: 0.1616
Training Epoch: 64 [10752/50048]	Loss: 0.1003
Training Epoch: 64 [10880/50048]	Loss: 0.1552
Training Epoch: 64 [11008/50048]	Loss: 0.1562
Training Epoch: 64 [11136/50048]	Loss: 0.2047
Training Epoch: 64 [11264/50048]	Loss: 0.1760
Training Epoch: 64 [11392/50048]	Loss: 0.1111
Training Epoch: 64 [11520/50048]	Loss: 0.0587
Training Epoch: 64 [11648/50048]	Loss: 0.1010
Training Epoch: 64 [11776/50048]	Loss: 0.1458
Training Epoch: 64 [11904/50048]	Loss: 0.2555
Training Epoch: 64 [12032/50048]	Loss: 0.1259
Training Epoch: 64 [12160/50048]	Loss: 0.1604
Training Epoch: 64 [12288/50048]	Loss: 0.2066
Training Epoch: 64 [12416/50048]	Loss: 0.1440
Training Epoch: 64 [12544/50048]	Loss: 0.1433
Training Epoch: 64 [12672/50048]	Loss: 0.1251
Training Epoch: 64 [12800/50048]	Loss: 0.1408
Training Epoch: 64 [12928/50048]	Loss: 0.2037
Training Epoch: 64 [13056/50048]	Loss: 0.0798
Training Epoch: 64 [13184/50048]	Loss: 0.1731
Training Epoch: 64 [13312/50048]	Loss: 0.1266
Training Epoch: 64 [13440/50048]	Loss: 0.2538
Training Epoch: 64 [13568/50048]	Loss: 0.0862
Training Epoch: 64 [13696/50048]	Loss: 0.1207
Training Epoch: 64 [13824/50048]	Loss: 0.1603
Training Epoch: 64 [13952/50048]	Loss: 0.1501
Training Epoch: 64 [14080/50048]	Loss: 0.1207
Training Epoch: 64 [14208/50048]	Loss: 0.1187
Training Epoch: 64 [14336/50048]	Loss: 0.1167
Training Epoch: 64 [14464/50048]	Loss: 0.1912
Training Epoch: 64 [14592/50048]	Loss: 0.1605
Training Epoch: 64 [14720/50048]	Loss: 0.0966
Training Epoch: 64 [14848/50048]	Loss: 0.1720
Training Epoch: 64 [14976/50048]	Loss: 0.1380
Training Epoch: 64 [15104/50048]	Loss: 0.1817
Training Epoch: 64 [15232/50048]	Loss: 0.1518
Training Epoch: 64 [15360/50048]	Loss: 0.2160
Training Epoch: 64 [15488/50048]	Loss: 0.1266
Training Epoch: 64 [15616/50048]	Loss: 0.1445
Training Epoch: 64 [15744/50048]	Loss: 0.1268
Training Epoch: 64 [15872/50048]	Loss: 0.0955
Training Epoch: 64 [16000/50048]	Loss: 0.0796
Training Epoch: 64 [16128/50048]	Loss: 0.1003
Training Epoch: 64 [16256/50048]	Loss: 0.0983
Training Epoch: 64 [16384/50048]	Loss: 0.1683
Training Epoch: 64 [16512/50048]	Loss: 0.1683
Training Epoch: 64 [16640/50048]	Loss: 0.1245
Training Epoch: 64 [16768/50048]	Loss: 0.2434
Training Epoch: 64 [16896/50048]	Loss: 0.1729
Training Epoch: 64 [17024/50048]	Loss: 0.2107
Training Epoch: 64 [17152/50048]	Loss: 0.1243
Training Epoch: 64 [17280/50048]	Loss: 0.1264
Training Epoch: 64 [17408/50048]	Loss: 0.0982
Training Epoch: 64 [17536/50048]	Loss: 0.1837
Training Epoch: 64 [17664/50048]	Loss: 0.1477
Training Epoch: 64 [17792/50048]	Loss: 0.0863
Training Epoch: 64 [17920/50048]	Loss: 0.1697
Training Epoch: 64 [18048/50048]	Loss: 0.2202
Training Epoch: 64 [18176/50048]	Loss: 0.1658
Training Epoch: 64 [18304/50048]	Loss: 0.1681
Training Epoch: 64 [18432/50048]	Loss: 0.1826
Training Epoch: 64 [18560/50048]	Loss: 0.2059
Training Epoch: 64 [18688/50048]	Loss: 0.1106
Training Epoch: 64 [18816/50048]	Loss: 0.2975
Training Epoch: 64 [18944/50048]	Loss: 0.1553
Training Epoch: 64 [19072/50048]	Loss: 0.0712
Training Epoch: 64 [19200/50048]	Loss: 0.0695
Training Epoch: 64 [19328/50048]	Loss: 0.1956
Training Epoch: 64 [19456/50048]	Loss: 0.1667
Training Epoch: 64 [19584/50048]	Loss: 0.1695
Training Epoch: 64 [19712/50048]	Loss: 0.1687
Training Epoch: 64 [19840/50048]	Loss: 0.1562
Training Epoch: 64 [19968/50048]	Loss: 0.2269
Training Epoch: 64 [20096/50048]	Loss: 0.1787
Training Epoch: 64 [20224/50048]	Loss: 0.1226
Training Epoch: 64 [20352/50048]	Loss: 0.1942
Training Epoch: 64 [20480/50048]	Loss: 0.2008
Training Epoch: 64 [20608/50048]	Loss: 0.1179
Training Epoch: 64 [20736/50048]	Loss: 0.1820
Training Epoch: 64 [20864/50048]	Loss: 0.1163
Training Epoch: 64 [20992/50048]	Loss: 0.1883
Training Epoch: 64 [21120/50048]	Loss: 0.2285
Training Epoch: 64 [21248/50048]	Loss: 0.1104
Training Epoch: 64 [21376/50048]	Loss: 0.1342
Training Epoch: 64 [21504/50048]	Loss: 0.2366
Training Epoch: 64 [21632/50048]	Loss: 0.1420
Training Epoch: 64 [21760/50048]	Loss: 0.1674
Training Epoch: 64 [21888/50048]	Loss: 0.1010
Training Epoch: 64 [22016/50048]	Loss: 0.1219
Training Epoch: 64 [22144/50048]	Loss: 0.2700
Training Epoch: 64 [22272/50048]	Loss: 0.2256
Training Epoch: 64 [22400/50048]	Loss: 0.1432
Training Epoch: 64 [22528/50048]	Loss: 0.1213
Training Epoch: 64 [22656/50048]	Loss: 0.1454
Training Epoch: 64 [22784/50048]	Loss: 0.2180
Training Epoch: 64 [22912/50048]	Loss: 0.1489
Training Epoch: 64 [23040/50048]	Loss: 0.0919
Training Epoch: 64 [23168/50048]	Loss: 0.1360
Training Epoch: 64 [23296/50048]	Loss: 0.1668
Training Epoch: 64 [23424/50048]	Loss: 0.1860
Training Epoch: 64 [23552/50048]	Loss: 0.1028
Training Epoch: 64 [23680/50048]	Loss: 0.1267
Training Epoch: 64 [23808/50048]	Loss: 0.1291
Training Epoch: 64 [23936/50048]	Loss: 0.1921
Training Epoch: 64 [24064/50048]	Loss: 0.0776
Training Epoch: 64 [24192/50048]	Loss: 0.2431
Training Epoch: 64 [24320/50048]	Loss: 0.2559
Training Epoch: 64 [24448/50048]	Loss: 0.1467
Training Epoch: 64 [24576/50048]	Loss: 0.1554
Training Epoch: 64 [24704/50048]	Loss: 0.1300
Training Epoch: 64 [24832/50048]	Loss: 0.1861
Training Epoch: 64 [24960/50048]	Loss: 0.1501
Training Epoch: 64 [25088/50048]	Loss: 0.1511
Training Epoch: 64 [25216/50048]	Loss: 0.1308
Training Epoch: 64 [25344/50048]	Loss: 0.1943
Training Epoch: 64 [25472/50048]	Loss: 0.1048
Training Epoch: 64 [25600/50048]	Loss: 0.1678
Training Epoch: 64 [25728/50048]	Loss: 0.1616
Training Epoch: 64 [25856/50048]	Loss: 0.1170
Training Epoch: 64 [25984/50048]	Loss: 0.1566
Training Epoch: 64 [26112/50048]	Loss: 0.1644
Training Epoch: 64 [26240/50048]	Loss: 0.1310
Training Epoch: 64 [26368/50048]	Loss: 0.0994
Training Epoch: 64 [26496/50048]	Loss: 0.1091
Training Epoch: 64 [26624/50048]	Loss: 0.2307
Training Epoch: 64 [26752/50048]	Loss: 0.1378
Training Epoch: 64 [26880/50048]	Loss: 0.1326
Training Epoch: 64 [27008/50048]	Loss: 0.2323
Training Epoch: 64 [27136/50048]	Loss: 0.1205
Training Epoch: 64 [27264/50048]	Loss: 0.2004
Training Epoch: 64 [27392/50048]	Loss: 0.1896
Training Epoch: 64 [27520/50048]	Loss: 0.0970
Training Epoch: 64 [27648/50048]	Loss: 0.1084
Training Epoch: 64 [27776/50048]	Loss: 0.2050
Training Epoch: 64 [27904/50048]	Loss: 0.2216
Training Epoch: 64 [28032/50048]	Loss: 0.1325
Training Epoch: 64 [28160/50048]	Loss: 0.1386
Training Epoch: 64 [28288/50048]	Loss: 0.1442
Training Epoch: 64 [28416/50048]	Loss: 0.2279
Training Epoch: 64 [28544/50048]	Loss: 0.1270
Training Epoch: 64 [28672/50048]	Loss: 0.1123
Training Epoch: 64 [28800/50048]	Loss: 0.2039
Training Epoch: 64 [28928/50048]	Loss: 0.1639
Training Epoch: 64 [29056/50048]	Loss: 0.1679
Training Epoch: 64 [29184/50048]	Loss: 0.1773
Training Epoch: 64 [29312/50048]	Loss: 0.2599
Training Epoch: 64 [29440/50048]	Loss: 0.2372
Training Epoch: 64 [29568/50048]	Loss: 0.1456
Training Epoch: 64 [29696/50048]	Loss: 0.1098
Training Epoch: 64 [29824/50048]	Loss: 0.3549
Training Epoch: 64 [29952/50048]	Loss: 0.2474
Training Epoch: 64 [30080/50048]	Loss: 0.1389
Training Epoch: 64 [30208/50048]	Loss: 0.1463
Training Epoch: 64 [30336/50048]	Loss: 0.2390
Training Epoch: 64 [30464/50048]	Loss: 0.3183
Training Epoch: 64 [30592/50048]	Loss: 0.2132
Training Epoch: 64 [30720/50048]	Loss: 0.1894
Training Epoch: 64 [30848/50048]	Loss: 0.2068
Training Epoch: 64 [30976/50048]	Loss: 0.1937
Training Epoch: 64 [31104/50048]	Loss: 0.2406
Training Epoch: 64 [31232/50048]	Loss: 0.1476
Training Epoch: 64 [31360/50048]	Loss: 0.2080
Training Epoch: 64 [31488/50048]	Loss: 0.1972
Training Epoch: 64 [31616/50048]	Loss: 0.2101
Training Epoch: 64 [31744/50048]	Loss: 0.1976
Training Epoch: 64 [31872/50048]	Loss: 0.0977
Training Epoch: 64 [32000/50048]	Loss: 0.1318
Training Epoch: 64 [32128/50048]	Loss: 0.2357
Training Epoch: 64 [32256/50048]	Loss: 0.1700
Training Epoch: 64 [32384/50048]	Loss: 0.2004
Training Epoch: 64 [32512/50048]	Loss: 0.0555
Training Epoch: 64 [32640/50048]	Loss: 0.2012
Training Epoch: 64 [32768/50048]	Loss: 0.1653
Training Epoch: 64 [32896/50048]	Loss: 0.1897
Training Epoch: 64 [33024/50048]	Loss: 0.1165
Training Epoch: 64 [33152/50048]	Loss: 0.1212
Training Epoch: 64 [33280/50048]	Loss: 0.1403
Training Epoch: 64 [33408/50048]	Loss: 0.1362
Training Epoch: 64 [33536/50048]	Loss: 0.1800
Training Epoch: 64 [33664/50048]	Loss: 0.0714
Training Epoch: 64 [33792/50048]	Loss: 0.1648
Training Epoch: 64 [33920/50048]	Loss: 0.1665
Training Epoch: 64 [34048/50048]	Loss: 0.2374
Training Epoch: 64 [34176/50048]	Loss: 0.1691
Training Epoch: 64 [34304/50048]	Loss: 0.1432
Training Epoch: 64 [34432/50048]	Loss: 0.1370
Training Epoch: 64 [34560/50048]	Loss: 0.1590
Training Epoch: 64 [34688/50048]	Loss: 0.1704
Training Epoch: 64 [34816/50048]	Loss: 0.1691
Training Epoch: 64 [34944/50048]	Loss: 0.2049
Training Epoch: 64 [35072/50048]	Loss: 0.1692
Training Epoch: 64 [35200/50048]	Loss: 0.1177
Training Epoch: 64 [35328/50048]	Loss: 0.1910
Training Epoch: 64 [35456/50048]	Loss: 0.1110
Training Epoch: 64 [35584/50048]	Loss: 0.1740
Training Epoch: 64 [35712/50048]	Loss: 0.1697
Training Epoch: 64 [35840/50048]	Loss: 0.1337
Training Epoch: 64 [35968/50048]	Loss: 0.1452
Training Epoch: 64 [36096/50048]	Loss: 0.2173
Training Epoch: 64 [36224/50048]	Loss: 0.0933
Training Epoch: 64 [36352/50048]	Loss: 0.1570
Training Epoch: 64 [36480/50048]	Loss: 0.1474
Training Epoch: 64 [36608/50048]	Loss: 0.2103
Training Epoch: 64 [36736/50048]	Loss: 0.1306
Training Epoch: 64 [36864/50048]	Loss: 0.2118
Training Epoch: 64 [36992/50048]	Loss: 0.2834
Training Epoch: 64 [37120/50048]	Loss: 0.1925
Training Epoch: 64 [37248/50048]	Loss: 0.1853
Training Epoch: 64 [37376/50048]	Loss: 0.1381
Training Epoch: 64 [37504/50048]	Loss: 0.2013
Training Epoch: 64 [37632/50048]	Loss: 0.1549
Training Epoch: 64 [37760/50048]	Loss: 0.1765
Training Epoch: 64 [37888/50048]	Loss: 0.1700
Training Epoch: 64 [38016/50048]	Loss: 0.1754
Training Epoch: 64 [38144/50048]	Loss: 0.1591
Training Epoch: 64 [38272/50048]	Loss: 0.1789
Training Epoch: 64 [38400/50048]	Loss: 0.0973
Training Epoch: 64 [38528/50048]	Loss: 0.2230
Training Epoch: 64 [38656/50048]	Loss: 0.2855
Training Epoch: 64 [38784/50048]	Loss: 0.2355
Training Epoch: 64 [38912/50048]	Loss: 0.3198
Training Epoch: 64 [39040/50048]	Loss: 0.1453
Training Epoch: 64 [39168/50048]	Loss: 0.1611
Training Epoch: 64 [39296/50048]	Loss: 0.0977
Training Epoch: 64 [39424/50048]	Loss: 0.2275
Training Epoch: 64 [39552/50048]	Loss: 0.1387
Training Epoch: 64 [39680/50048]	Loss: 0.2066
Training Epoch: 64 [39808/50048]	Loss: 0.2399
Training Epoch: 64 [39936/50048]	Loss: 0.2727
Training Epoch: 64 [40064/50048]	Loss: 0.1361
Training Epoch: 64 [40192/50048]	Loss: 0.1860
Training Epoch: 64 [40320/50048]	Loss: 0.1723
Training Epoch: 64 [40448/50048]	Loss: 0.1261
Training Epoch: 64 [40576/50048]	Loss: 0.1726
Training Epoch: 64 [40704/50048]	Loss: 0.2393
Training Epoch: 64 [40832/50048]	Loss: 0.1887
Training Epoch: 64 [40960/50048]	Loss: 0.1795
Training Epoch: 64 [41088/50048]	Loss: 0.1694
Training Epoch: 64 [41216/50048]	Loss: 0.0952
Training Epoch: 64 [41344/50048]	Loss: 0.1326
Training Epoch: 64 [41472/50048]	Loss: 0.1753
Training Epoch: 64 [41600/50048]	Loss: 0.2029
Training Epoch: 64 [41728/50048]	Loss: 0.2127
Training Epoch: 64 [41856/50048]	Loss: 0.1494
Training Epoch: 64 [41984/50048]	Loss: 0.2054
Training Epoch: 64 [42112/50048]	Loss: 0.1891
Training Epoch: 64 [42240/50048]	Loss: 0.1695
Training Epoch: 64 [42368/50048]	Loss: 0.1715
Training Epoch: 64 [42496/50048]	Loss: 0.1594
Training Epoch: 64 [42624/50048]	Loss: 0.2140
Training Epoch: 64 [42752/50048]	Loss: 0.2278
Training Epoch: 64 [42880/50048]	Loss: 0.1814
Training Epoch: 64 [43008/50048]	Loss: 0.2423
Training Epoch: 64 [43136/50048]	Loss: 0.2722
Training Epoch: 64 [43264/50048]	Loss: 0.1927
Training Epoch: 64 [43392/50048]	Loss: 0.1157
Training Epoch: 64 [43520/50048]	Loss: 0.1262
Training Epoch: 64 [43648/50048]	Loss: 0.1872
Training Epoch: 64 [43776/50048]	Loss: 0.1359
Training Epoch: 64 [43904/50048]	Loss: 0.2603
Training Epoch: 64 [44032/50048]	Loss: 0.2408
Training Epoch: 64 [44160/50048]	Loss: 0.2041
Training Epoch: 64 [44288/50048]	Loss: 0.1570
Training Epoch: 64 [44416/50048]	Loss: 0.1117
Training Epoch: 64 [44544/50048]	Loss: 0.3345
Training Epoch: 64 [44672/50048]	Loss: 0.1757
Training Epoch: 64 [44800/50048]	Loss: 0.2031
Training Epoch: 64 [44928/50048]	Loss: 0.2472
Training Epoch: 64 [45056/50048]	Loss: 0.2298
Training Epoch: 64 [45184/50048]	Loss: 0.1708
Training Epoch: 64 [45312/50048]	Loss: 0.1676
Training Epoch: 64 [45440/50048]	Loss: 0.1497
Training Epoch: 64 [45568/50048]	Loss: 0.1753
Training Epoch: 64 [45696/50048]	Loss: 0.2336
2022-12-06 05:15:05,072 [ZeusDataLoader(train)] train epoch 65 done: time=86.49 energy=10503.14
2022-12-06 05:15:05,074 [ZeusDataLoader(eval)] Epoch 65 begin.
Training Epoch: 64 [45824/50048]	Loss: 0.1102
Training Epoch: 64 [45952/50048]	Loss: 0.2082
Training Epoch: 64 [46080/50048]	Loss: 0.3048
Training Epoch: 64 [46208/50048]	Loss: 0.2195
Training Epoch: 64 [46336/50048]	Loss: 0.1872
Training Epoch: 64 [46464/50048]	Loss: 0.1113
Training Epoch: 64 [46592/50048]	Loss: 0.1695
Training Epoch: 64 [46720/50048]	Loss: 0.1448
Training Epoch: 64 [46848/50048]	Loss: 0.2192
Training Epoch: 64 [46976/50048]	Loss: 0.1071
Training Epoch: 64 [47104/50048]	Loss: 0.1626
Training Epoch: 64 [47232/50048]	Loss: 0.2372
Training Epoch: 64 [47360/50048]	Loss: 0.2078
Training Epoch: 64 [47488/50048]	Loss: 0.1576
Training Epoch: 64 [47616/50048]	Loss: 0.1729
Training Epoch: 64 [47744/50048]	Loss: 0.1879
Training Epoch: 64 [47872/50048]	Loss: 0.1488
Training Epoch: 64 [48000/50048]	Loss: 0.1263
Training Epoch: 64 [48128/50048]	Loss: 0.1440
Training Epoch: 64 [48256/50048]	Loss: 0.2732
Training Epoch: 64 [48384/50048]	Loss: 0.3183
Training Epoch: 64 [48512/50048]	Loss: 0.1081
Training Epoch: 64 [48640/50048]	Loss: 0.2144
Training Epoch: 64 [48768/50048]	Loss: 0.2523
Training Epoch: 64 [48896/50048]	Loss: 0.2227
Training Epoch: 64 [49024/50048]	Loss: 0.1559
Training Epoch: 64 [49152/50048]	Loss: 0.1611
Training Epoch: 64 [49280/50048]	Loss: 0.1554
Training Epoch: 64 [49408/50048]	Loss: 0.1797
Training Epoch: 64 [49536/50048]	Loss: 0.2254
Training Epoch: 64 [49664/50048]	Loss: 0.1230
Training Epoch: 64 [49792/50048]	Loss: 0.1697
Training Epoch: 64 [49920/50048]	Loss: 0.1861
Training Epoch: 64 [50048/50048]	Loss: 0.1288
2022-12-06 10:15:08.716 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:15:08,729 [ZeusDataLoader(eval)] eval epoch 65 done: time=3.65 energy=441.13
2022-12-06 05:15:08,729 [ZeusDataLoader(train)] Up to epoch 65: time=5865.08, energy=711946.85, cost=869168.18
2022-12-06 05:15:08,729 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:15:08,729 [ZeusDataLoader(train)] Expected next epoch: time=5954.88, energy=722744.86, cost=882424.56
2022-12-06 05:15:08,730 [ZeusDataLoader(train)] Epoch 66 begin.
Validation Epoch: 64, Average loss: 0.0165, Accuracy: 0.6376
2022-12-06 05:15:08,908 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:15:08,908 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:15:08.910 [ZeusMonitor] Monitor started.
2022-12-06 10:15:08.910 [ZeusMonitor] Running indefinitely. 2022-12-06 10:15:08.910 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:15:08.910 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e66+gpu0.power.log
Training Epoch: 65 [128/50048]	Loss: 0.1271
Training Epoch: 65 [256/50048]	Loss: 0.1432
Training Epoch: 65 [384/50048]	Loss: 0.1097
Training Epoch: 65 [512/50048]	Loss: 0.1644
Training Epoch: 65 [640/50048]	Loss: 0.1816
Training Epoch: 65 [768/50048]	Loss: 0.1616
Training Epoch: 65 [896/50048]	Loss: 0.2306
Training Epoch: 65 [1024/50048]	Loss: 0.1153
Training Epoch: 65 [1152/50048]	Loss: 0.0950
Training Epoch: 65 [1280/50048]	Loss: 0.0968
Training Epoch: 65 [1408/50048]	Loss: 0.1569
Training Epoch: 65 [1536/50048]	Loss: 0.1364
Training Epoch: 65 [1664/50048]	Loss: 0.1168
Training Epoch: 65 [1792/50048]	Loss: 0.0912
Training Epoch: 65 [1920/50048]	Loss: 0.1278
Training Epoch: 65 [2048/50048]	Loss: 0.1811
Training Epoch: 65 [2176/50048]	Loss: 0.1971
Training Epoch: 65 [2304/50048]	Loss: 0.0843
Training Epoch: 65 [2432/50048]	Loss: 0.1263
Training Epoch: 65 [2560/50048]	Loss: 0.1548
Training Epoch: 65 [2688/50048]	Loss: 0.1516
Training Epoch: 65 [2816/50048]	Loss: 0.1099
Training Epoch: 65 [2944/50048]	Loss: 0.1429
Training Epoch: 65 [3072/50048]	Loss: 0.0955
Training Epoch: 65 [3200/50048]	Loss: 0.1483
Training Epoch: 65 [3328/50048]	Loss: 0.1015
Training Epoch: 65 [3456/50048]	Loss: 0.1245
Training Epoch: 65 [3584/50048]	Loss: 0.2764
Training Epoch: 65 [3712/50048]	Loss: 0.1765
Training Epoch: 65 [3840/50048]	Loss: 0.1348
Training Epoch: 65 [3968/50048]	Loss: 0.1175
Training Epoch: 65 [4096/50048]	Loss: 0.1105
Training Epoch: 65 [4224/50048]	Loss: 0.1155
Training Epoch: 65 [4352/50048]	Loss: 0.1309
Training Epoch: 65 [4480/50048]	Loss: 0.1168
Training Epoch: 65 [4608/50048]	Loss: 0.2143
Training Epoch: 65 [4736/50048]	Loss: 0.1015
Training Epoch: 65 [4864/50048]	Loss: 0.0755
Training Epoch: 65 [4992/50048]	Loss: 0.0997
Training Epoch: 65 [5120/50048]	Loss: 0.0893
Training Epoch: 65 [5248/50048]	Loss: 0.0991
Training Epoch: 65 [5376/50048]	Loss: 0.1213
Training Epoch: 65 [5504/50048]	Loss: 0.1303
Training Epoch: 65 [5632/50048]	Loss: 0.1837
Training Epoch: 65 [5760/50048]	Loss: 0.1401
Training Epoch: 65 [5888/50048]	Loss: 0.1187
Training Epoch: 65 [6016/50048]	Loss: 0.1965
Training Epoch: 65 [6144/50048]	Loss: 0.1294
Training Epoch: 65 [6272/50048]	Loss: 0.0724
Training Epoch: 65 [6400/50048]	Loss: 0.1569
Training Epoch: 65 [6528/50048]	Loss: 0.1089
Training Epoch: 65 [6656/50048]	Loss: 0.0794
Training Epoch: 65 [6784/50048]	Loss: 0.2293
Training Epoch: 65 [6912/50048]	Loss: 0.1431
Training Epoch: 65 [7040/50048]	Loss: 0.1718
Training Epoch: 65 [7168/50048]	Loss: 0.0638
Training Epoch: 65 [7296/50048]	Loss: 0.2009
Training Epoch: 65 [7424/50048]	Loss: 0.0637
Training Epoch: 65 [7552/50048]	Loss: 0.1606
Training Epoch: 65 [7680/50048]	Loss: 0.1344
Training Epoch: 65 [7808/50048]	Loss: 0.0982
Training Epoch: 65 [7936/50048]	Loss: 0.1214
Training Epoch: 65 [8064/50048]	Loss: 0.0717
Training Epoch: 65 [8192/50048]	Loss: 0.1363
Training Epoch: 65 [8320/50048]	Loss: 0.1727
Training Epoch: 65 [8448/50048]	Loss: 0.1319
Training Epoch: 65 [8576/50048]	Loss: 0.1658
Training Epoch: 65 [8704/50048]	Loss: 0.1339
Training Epoch: 65 [8832/50048]	Loss: 0.1146
Training Epoch: 65 [8960/50048]	Loss: 0.0788
Training Epoch: 65 [9088/50048]	Loss: 0.1628
Training Epoch: 65 [9216/50048]	Loss: 0.1457
Training Epoch: 65 [9344/50048]	Loss: 0.1360
Training Epoch: 65 [9472/50048]	Loss: 0.1347
Training Epoch: 65 [9600/50048]	Loss: 0.1643
Training Epoch: 65 [9728/50048]	Loss: 0.1585
Training Epoch: 65 [9856/50048]	Loss: 0.1261
Training Epoch: 65 [9984/50048]	Loss: 0.1770
Training Epoch: 65 [10112/50048]	Loss: 0.2226
Training Epoch: 65 [10240/50048]	Loss: 0.1330
Training Epoch: 65 [10368/50048]	Loss: 0.0994
Training Epoch: 65 [10496/50048]	Loss: 0.0804
Training Epoch: 65 [10624/50048]	Loss: 0.1452
Training Epoch: 65 [10752/50048]	Loss: 0.1443
Training Epoch: 65 [10880/50048]	Loss: 0.1238
Training Epoch: 65 [11008/50048]	Loss: 0.1893
Training Epoch: 65 [11136/50048]	Loss: 0.1621
Training Epoch: 65 [11264/50048]	Loss: 0.0958
Training Epoch: 65 [11392/50048]	Loss: 0.0838
Training Epoch: 65 [11520/50048]	Loss: 0.2095
Training Epoch: 65 [11648/50048]	Loss: 0.1590
Training Epoch: 65 [11776/50048]	Loss: 0.0886
Training Epoch: 65 [11904/50048]	Loss: 0.0806
Training Epoch: 65 [12032/50048]	Loss: 0.1483
Training Epoch: 65 [12160/50048]	Loss: 0.1850
Training Epoch: 65 [12288/50048]	Loss: 0.1624
Training Epoch: 65 [12416/50048]	Loss: 0.1645
Training Epoch: 65 [12544/50048]	Loss: 0.2249
Training Epoch: 65 [12672/50048]	Loss: 0.1448
Training Epoch: 65 [12800/50048]	Loss: 0.2801
Training Epoch: 65 [12928/50048]	Loss: 0.1078
Training Epoch: 65 [13056/50048]	Loss: 0.1657
Training Epoch: 65 [13184/50048]	Loss: 0.1294
Training Epoch: 65 [13312/50048]	Loss: 0.2395
Training Epoch: 65 [13440/50048]	Loss: 0.2217
Training Epoch: 65 [13568/50048]	Loss: 0.1271
Training Epoch: 65 [13696/50048]	Loss: 0.0884
Training Epoch: 65 [13824/50048]	Loss: 0.1225
Training Epoch: 65 [13952/50048]	Loss: 0.1234
Training Epoch: 65 [14080/50048]	Loss: 0.1339
Training Epoch: 65 [14208/50048]	Loss: 0.1617
Training Epoch: 65 [14336/50048]	Loss: 0.1604
Training Epoch: 65 [14464/50048]	Loss: 0.1442
Training Epoch: 65 [14592/50048]	Loss: 0.1922
Training Epoch: 65 [14720/50048]	Loss: 0.1475
Training Epoch: 65 [14848/50048]	Loss: 0.1425
Training Epoch: 65 [14976/50048]	Loss: 0.1217
Training Epoch: 65 [15104/50048]	Loss: 0.2119
Training Epoch: 65 [15232/50048]	Loss: 0.2068
Training Epoch: 65 [15360/50048]	Loss: 0.1796
Training Epoch: 65 [15488/50048]	Loss: 0.1516
Training Epoch: 65 [15616/50048]	Loss: 0.0971
Training Epoch: 65 [15744/50048]	Loss: 0.2320
Training Epoch: 65 [15872/50048]	Loss: 0.1756
Training Epoch: 65 [16000/50048]	Loss: 0.1665
Training Epoch: 65 [16128/50048]	Loss: 0.1788
Training Epoch: 65 [16256/50048]	Loss: 0.2148
Training Epoch: 65 [16384/50048]	Loss: 0.2305
Training Epoch: 65 [16512/50048]	Loss: 0.1442
Training Epoch: 65 [16640/50048]	Loss: 0.1466
Training Epoch: 65 [16768/50048]	Loss: 0.1370
Training Epoch: 65 [16896/50048]	Loss: 0.0772
Training Epoch: 65 [17024/50048]	Loss: 0.2100
Training Epoch: 65 [17152/50048]	Loss: 0.1767
Training Epoch: 65 [17280/50048]	Loss: 0.1119
Training Epoch: 65 [17408/50048]	Loss: 0.2130
Training Epoch: 65 [17536/50048]	Loss: 0.1149
Training Epoch: 65 [17664/50048]	Loss: 0.1588
Training Epoch: 65 [17792/50048]	Loss: 0.1813
Training Epoch: 65 [17920/50048]	Loss: 0.1499
Training Epoch: 65 [18048/50048]	Loss: 0.1219
Training Epoch: 65 [18176/50048]	Loss: 0.0893
Training Epoch: 65 [18304/50048]	Loss: 0.1460
Training Epoch: 65 [18432/50048]	Loss: 0.2397
Training Epoch: 65 [18560/50048]	Loss: 0.1757
Training Epoch: 65 [18688/50048]	Loss: 0.2333
Training Epoch: 65 [18816/50048]	Loss: 0.2143
Training Epoch: 65 [18944/50048]	Loss: 0.1187
Training Epoch: 65 [19072/50048]	Loss: 0.1926
Training Epoch: 65 [19200/50048]	Loss: 0.1709
Training Epoch: 65 [19328/50048]	Loss: 0.0551
Training Epoch: 65 [19456/50048]	Loss: 0.1393
Training Epoch: 65 [19584/50048]	Loss: 0.2118
Training Epoch: 65 [19712/50048]	Loss: 0.1737
Training Epoch: 65 [19840/50048]	Loss: 0.1876
Training Epoch: 65 [19968/50048]	Loss: 0.1342
Training Epoch: 65 [20096/50048]	Loss: 0.1315
Training Epoch: 65 [20224/50048]	Loss: 0.1802
Training Epoch: 65 [20352/50048]	Loss: 0.1525
Training Epoch: 65 [20480/50048]	Loss: 0.1229
Training Epoch: 65 [20608/50048]	Loss: 0.1585
Training Epoch: 65 [20736/50048]	Loss: 0.0971
Training Epoch: 65 [20864/50048]	Loss: 0.1341
Training Epoch: 65 [20992/50048]	Loss: 0.1959
Training Epoch: 65 [21120/50048]	Loss: 0.1946
Training Epoch: 65 [21248/50048]	Loss: 0.1800
Training Epoch: 65 [21376/50048]	Loss: 0.1214
Training Epoch: 65 [21504/50048]	Loss: 0.2037
Training Epoch: 65 [21632/50048]	Loss: 0.2103
Training Epoch: 65 [21760/50048]	Loss: 0.1549
Training Epoch: 65 [21888/50048]	Loss: 0.2376
Training Epoch: 65 [22016/50048]	Loss: 0.1968
Training Epoch: 65 [22144/50048]	Loss: 0.1970
Training Epoch: 65 [22272/50048]	Loss: 0.1859
Training Epoch: 65 [22400/50048]	Loss: 0.2645
Training Epoch: 65 [22528/50048]	Loss: 0.1044
Training Epoch: 65 [22656/50048]	Loss: 0.1290
Training Epoch: 65 [22784/50048]	Loss: 0.1925
Training Epoch: 65 [22912/50048]	Loss: 0.2001
Training Epoch: 65 [23040/50048]	Loss: 0.1997
Training Epoch: 65 [23168/50048]	Loss: 0.1636
Training Epoch: 65 [23296/50048]	Loss: 0.1764
Training Epoch: 65 [23424/50048]	Loss: 0.2048
Training Epoch: 65 [23552/50048]	Loss: 0.2223
Training Epoch: 65 [23680/50048]	Loss: 0.1794
Training Epoch: 65 [23808/50048]	Loss: 0.1363
Training Epoch: 65 [23936/50048]	Loss: 0.1662
Training Epoch: 65 [24064/50048]	Loss: 0.1174
Training Epoch: 65 [24192/50048]	Loss: 0.1630
Training Epoch: 65 [24320/50048]	Loss: 0.1405
Training Epoch: 65 [24448/50048]	Loss: 0.0982
Training Epoch: 65 [24576/50048]	Loss: 0.1474
Training Epoch: 65 [24704/50048]	Loss: 0.1471
Training Epoch: 65 [24832/50048]	Loss: 0.2085
Training Epoch: 65 [24960/50048]	Loss: 0.1774
Training Epoch: 65 [25088/50048]	Loss: 0.3455
Training Epoch: 65 [25216/50048]	Loss: 0.1768
Training Epoch: 65 [25344/50048]	Loss: 0.1494
Training Epoch: 65 [25472/50048]	Loss: 0.1782
Training Epoch: 65 [25600/50048]	Loss: 0.2283
Training Epoch: 65 [25728/50048]	Loss: 0.1099
Training Epoch: 65 [25856/50048]	Loss: 0.0932
Training Epoch: 65 [25984/50048]	Loss: 0.1250
Training Epoch: 65 [26112/50048]	Loss: 0.2012
Training Epoch: 65 [26240/50048]	Loss: 0.1347
Training Epoch: 65 [26368/50048]	Loss: 0.1490
Training Epoch: 65 [26496/50048]	Loss: 0.1569
Training Epoch: 65 [26624/50048]	Loss: 0.1475
Training Epoch: 65 [26752/50048]	Loss: 0.1871
Training Epoch: 65 [26880/50048]	Loss: 0.2534
Training Epoch: 65 [27008/50048]	Loss: 0.1971
Training Epoch: 65 [27136/50048]	Loss: 0.1607
Training Epoch: 65 [27264/50048]	Loss: 0.1685
Training Epoch: 65 [27392/50048]	Loss: 0.1520
Training Epoch: 65 [27520/50048]	Loss: 0.2170
Training Epoch: 65 [27648/50048]	Loss: 0.1172
Training Epoch: 65 [27776/50048]	Loss: 0.0642
Training Epoch: 65 [27904/50048]	Loss: 0.1776
Training Epoch: 65 [28032/50048]	Loss: 0.2210
Training Epoch: 65 [28160/50048]	Loss: 0.1412
Training Epoch: 65 [28288/50048]	Loss: 0.1812
Training Epoch: 65 [28416/50048]	Loss: 0.1516
Training Epoch: 65 [28544/50048]	Loss: 0.2161
Training Epoch: 65 [28672/50048]	Loss: 0.2132
Training Epoch: 65 [28800/50048]	Loss: 0.2477
Training Epoch: 65 [28928/50048]	Loss: 0.2770
Training Epoch: 65 [29056/50048]	Loss: 0.3008
Training Epoch: 65 [29184/50048]	Loss: 0.2084
Training Epoch: 65 [29312/50048]	Loss: 0.2360
Training Epoch: 65 [29440/50048]	Loss: 0.1179
Training Epoch: 65 [29568/50048]	Loss: 0.1135
Training Epoch: 65 [29696/50048]	Loss: 0.1323
Training Epoch: 65 [29824/50048]	Loss: 0.1818
Training Epoch: 65 [29952/50048]	Loss: 0.2015
Training Epoch: 65 [30080/50048]	Loss: 0.1555
Training Epoch: 65 [30208/50048]	Loss: 0.2483
Training Epoch: 65 [30336/50048]	Loss: 0.1746
Training Epoch: 65 [30464/50048]	Loss: 0.3669
Training Epoch: 65 [30592/50048]	Loss: 0.1894
Training Epoch: 65 [30720/50048]	Loss: 0.3400
Training Epoch: 65 [30848/50048]	Loss: 0.1623
Training Epoch: 65 [30976/50048]	Loss: 0.2518
Training Epoch: 65 [31104/50048]	Loss: 0.1691
Training Epoch: 65 [31232/50048]	Loss: 0.2912
Training Epoch: 65 [31360/50048]	Loss: 0.1424
Training Epoch: 65 [31488/50048]	Loss: 0.2392
Training Epoch: 65 [31616/50048]	Loss: 0.2609
Training Epoch: 65 [31744/50048]	Loss: 0.1828
Training Epoch: 65 [31872/50048]	Loss: 0.1085
Training Epoch: 65 [32000/50048]	Loss: 0.1075
Training Epoch: 65 [32128/50048]	Loss: 0.1521
Training Epoch: 65 [32256/50048]	Loss: 0.2179
Training Epoch: 65 [32384/50048]	Loss: 0.1968
Training Epoch: 65 [32512/50048]	Loss: 0.1519
Training Epoch: 65 [32640/50048]	Loss: 0.1360
Training Epoch: 65 [32768/50048]	Loss: 0.1352
Training Epoch: 65 [32896/50048]	Loss: 0.1178
Training Epoch: 65 [33024/50048]	Loss: 0.0848
Training Epoch: 65 [33152/50048]	Loss: 0.2110
Training Epoch: 65 [33280/50048]	Loss: 0.2041
Training Epoch: 65 [33408/50048]	Loss: 0.0829
Training Epoch: 65 [33536/50048]	Loss: 0.2563
Training Epoch: 65 [33664/50048]	Loss: 0.1838
Training Epoch: 65 [33792/50048]	Loss: 0.1677
Training Epoch: 65 [33920/50048]	Loss: 0.1740
Training Epoch: 65 [34048/50048]	Loss: 0.2775
Training Epoch: 65 [34176/50048]	Loss: 0.1340
Training Epoch: 65 [34304/50048]	Loss: 0.1704
Training Epoch: 65 [34432/50048]	Loss: 0.1693
Training Epoch: 65 [34560/50048]	Loss: 0.1662
Training Epoch: 65 [34688/50048]	Loss: 0.1805
Training Epoch: 65 [34816/50048]	Loss: 0.1820
Training Epoch: 65 [34944/50048]	Loss: 0.1365
Training Epoch: 65 [35072/50048]	Loss: 0.1409
Training Epoch: 65 [35200/50048]	Loss: 0.1884
Training Epoch: 65 [35328/50048]	Loss: 0.1735
Training Epoch: 65 [35456/50048]	Loss: 0.2479
Training Epoch: 65 [35584/50048]	Loss: 0.1795
Training Epoch: 65 [35712/50048]	Loss: 0.2113
Training Epoch: 65 [35840/50048]	Loss: 0.2926
Training Epoch: 65 [35968/50048]	Loss: 0.1618
Training Epoch: 65 [36096/50048]	Loss: 0.1631
Training Epoch: 65 [36224/50048]	Loss: 0.1104
Training Epoch: 65 [36352/50048]	Loss: 0.1508
Training Epoch: 65 [36480/50048]	Loss: 0.1564
Training Epoch: 65 [36608/50048]	Loss: 0.2478
Training Epoch: 65 [36736/50048]	Loss: 0.1926
Training Epoch: 65 [36864/50048]	Loss: 0.3014
Training Epoch: 65 [36992/50048]	Loss: 0.1515
Training Epoch: 65 [37120/50048]	Loss: 0.1564
Training Epoch: 65 [37248/50048]	Loss: 0.3100
Training Epoch: 65 [37376/50048]	Loss: 0.1511
Training Epoch: 65 [37504/50048]	Loss: 0.1770
Training Epoch: 65 [37632/50048]	Loss: 0.0731
Training Epoch: 65 [37760/50048]	Loss: 0.2405
Training Epoch: 65 [37888/50048]	Loss: 0.2026
Training Epoch: 65 [38016/50048]	Loss: 0.1074
Training Epoch: 65 [38144/50048]	Loss: 0.1828
Training Epoch: 65 [38272/50048]	Loss: 0.2885
Training Epoch: 65 [38400/50048]	Loss: 0.2114
Training Epoch: 65 [38528/50048]	Loss: 0.2269
Training Epoch: 65 [38656/50048]	Loss: 0.1741
Training Epoch: 65 [38784/50048]	Loss: 0.2889
Training Epoch: 65 [38912/50048]	Loss: 0.1475
Training Epoch: 65 [39040/50048]	Loss: 0.0985
Training Epoch: 65 [39168/50048]	Loss: 0.1853
Training Epoch: 65 [39296/50048]	Loss: 0.3015
Training Epoch: 65 [39424/50048]	Loss: 0.1030
Training Epoch: 65 [39552/50048]	Loss: 0.1157
Training Epoch: 65 [39680/50048]	Loss: 0.1632
Training Epoch: 65 [39808/50048]	Loss: 0.1869
Training Epoch: 65 [39936/50048]	Loss: 0.1169
Training Epoch: 65 [40064/50048]	Loss: 0.2567
Training Epoch: 65 [40192/50048]	Loss: 0.1731
Training Epoch: 65 [40320/50048]	Loss: 0.1285
Training Epoch: 65 [40448/50048]	Loss: 0.1536
Training Epoch: 65 [40576/50048]	Loss: 0.2543
Training Epoch: 65 [40704/50048]	Loss: 0.2542
Training Epoch: 65 [40832/50048]	Loss: 0.0997
Training Epoch: 65 [40960/50048]	Loss: 0.2071
Training Epoch: 65 [41088/50048]	Loss: 0.1292
Training Epoch: 65 [41216/50048]	Loss: 0.1566
Training Epoch: 65 [41344/50048]	Loss: 0.1897
Training Epoch: 65 [41472/50048]	Loss: 0.1221
Training Epoch: 65 [41600/50048]	Loss: 0.0972
Training Epoch: 65 [41728/50048]	Loss: 0.1951
Training Epoch: 65 [41856/50048]	Loss: 0.2399
Training Epoch: 65 [41984/50048]	Loss: 0.2378
Training Epoch: 65 [42112/50048]	Loss: 0.1786
Training Epoch: 65 [42240/50048]	Loss: 0.1429
Training Epoch: 65 [42368/50048]	Loss: 0.2160
Training Epoch: 65 [42496/50048]	Loss: 0.1911
Training Epoch: 65 [42624/50048]	Loss: 0.1527
Training Epoch: 65 [42752/50048]	Loss: 0.1027
Training Epoch: 65 [42880/50048]	Loss: 0.3024
Training Epoch: 65 [43008/50048]	Loss: 0.2314
Training Epoch: 65 [43136/50048]	Loss: 0.2018
Training Epoch: 65 [43264/50048]	Loss: 0.1679
Training Epoch: 65 [43392/50048]	Loss: 0.1875
Training Epoch: 65 [43520/50048]	Loss: 0.2713
Training Epoch: 65 [43648/50048]	Loss: 0.1822
Training Epoch: 65 [43776/50048]	Loss: 0.1080
Training Epoch: 65 [43904/50048]	Loss: 0.2425
Training Epoch: 65 [44032/50048]	Loss: 0.2171
Training Epoch: 65 [44160/50048]	Loss: 0.2072
Training Epoch: 65 [44288/50048]	Loss: 0.3585
Training Epoch: 65 [44416/50048]	Loss: 0.2012
Training Epoch: 65 [44544/50048]	Loss: 0.2046
Training Epoch: 65 [44672/50048]	Loss: 0.1818
Training Epoch: 65 [44800/50048]	Loss: 0.2105
Training Epoch: 65 [44928/50048]	Loss: 0.1873
Training Epoch: 65 [45056/50048]	Loss: 0.1987
Training Epoch: 65 [45184/50048]	Loss: 0.2271
Training Epoch: 65 [45312/50048]	Loss: 0.1230
Training Epoch: 65 [45440/50048]	Loss: 0.1832
Training Epoch: 65 [45568/50048]	Loss: 0.1826
Training Epoch: 65 [45696/50048]	Loss: 0.2109
2022-12-06 05:16:35,176 [ZeusDataLoader(train)] train epoch 66 done: time=86.44 energy=10499.35
2022-12-06 05:16:35,177 [ZeusDataLoader(eval)] Epoch 66 begin.
Training Epoch: 65 [45824/50048]	Loss: 0.2064
Training Epoch: 65 [45952/50048]	Loss: 0.2594
Training Epoch: 65 [46080/50048]	Loss: 0.1359
Training Epoch: 65 [46208/50048]	Loss: 0.1441
Training Epoch: 65 [46336/50048]	Loss: 0.1185
Training Epoch: 65 [46464/50048]	Loss: 0.1994
Training Epoch: 65 [46592/50048]	Loss: 0.1164
Training Epoch: 65 [46720/50048]	Loss: 0.2350
Training Epoch: 65 [46848/50048]	Loss: 0.1406
Training Epoch: 65 [46976/50048]	Loss: 0.1566
Training Epoch: 65 [47104/50048]	Loss: 0.1743
Training Epoch: 65 [47232/50048]	Loss: 0.2681
Training Epoch: 65 [47360/50048]	Loss: 0.1854
Training Epoch: 65 [47488/50048]	Loss: 0.1837
Training Epoch: 65 [47616/50048]	Loss: 0.2328
Training Epoch: 65 [47744/50048]	Loss: 0.1573
Training Epoch: 65 [47872/50048]	Loss: 0.4077
Training Epoch: 65 [48000/50048]	Loss: 0.2280
Training Epoch: 65 [48128/50048]	Loss: 0.2847
Training Epoch: 65 [48256/50048]	Loss: 0.1397
Training Epoch: 65 [48384/50048]	Loss: 0.1931
Training Epoch: 65 [48512/50048]	Loss: 0.1798
Training Epoch: 65 [48640/50048]	Loss: 0.2174
Training Epoch: 65 [48768/50048]	Loss: 0.2369
Training Epoch: 65 [48896/50048]	Loss: 0.1755
Training Epoch: 65 [49024/50048]	Loss: 0.2471
Training Epoch: 65 [49152/50048]	Loss: 0.1534
Training Epoch: 65 [49280/50048]	Loss: 0.1550
Training Epoch: 65 [49408/50048]	Loss: 0.1869
Training Epoch: 65 [49536/50048]	Loss: 0.1854
Training Epoch: 65 [49664/50048]	Loss: 0.1674
Training Epoch: 65 [49792/50048]	Loss: 0.1813
Training Epoch: 65 [49920/50048]	Loss: 0.2031
Training Epoch: 65 [50048/50048]	Loss: 0.3180
2022-12-06 10:16:38.900 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:16:38,944 [ZeusDataLoader(eval)] eval epoch 66 done: time=3.76 energy=455.18
2022-12-06 05:16:38,944 [ZeusDataLoader(train)] Up to epoch 66: time=5955.28, energy=722901.37, cost=882537.35
2022-12-06 05:16:38,944 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:16:38,945 [ZeusDataLoader(train)] Expected next epoch: time=6045.07, energy=733699.39, cost=895793.73
2022-12-06 05:16:38,945 [ZeusDataLoader(train)] Epoch 67 begin.
Validation Epoch: 65, Average loss: 0.0167, Accuracy: 0.6320
2022-12-06 05:16:39,127 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:16:39,128 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:16:39.130 [ZeusMonitor] Monitor started.
2022-12-06 10:16:39.130 [ZeusMonitor] Running indefinitely. 2022-12-06 10:16:39.130 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:16:39.130 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e67+gpu0.power.log
Training Epoch: 66 [128/50048]	Loss: 0.1903
Training Epoch: 66 [256/50048]	Loss: 0.1889
Training Epoch: 66 [384/50048]	Loss: 0.1545
Training Epoch: 66 [512/50048]	Loss: 0.1310
Training Epoch: 66 [640/50048]	Loss: 0.1012
Training Epoch: 66 [768/50048]	Loss: 0.1467
Training Epoch: 66 [896/50048]	Loss: 0.1734
Training Epoch: 66 [1024/50048]	Loss: 0.0941
Training Epoch: 66 [1152/50048]	Loss: 0.0858
Training Epoch: 66 [1280/50048]	Loss: 0.1380
Training Epoch: 66 [1408/50048]	Loss: 0.0811
Training Epoch: 66 [1536/50048]	Loss: 0.1567
Training Epoch: 66 [1664/50048]	Loss: 0.1525
Training Epoch: 66 [1792/50048]	Loss: 0.0862
Training Epoch: 66 [1920/50048]	Loss: 0.1119
Training Epoch: 66 [2048/50048]	Loss: 0.2436
Training Epoch: 66 [2176/50048]	Loss: 0.1111
Training Epoch: 66 [2304/50048]	Loss: 0.1370
Training Epoch: 66 [2432/50048]	Loss: 0.2000
Training Epoch: 66 [2560/50048]	Loss: 0.1575
Training Epoch: 66 [2688/50048]	Loss: 0.1203
Training Epoch: 66 [2816/50048]	Loss: 0.1302
Training Epoch: 66 [2944/50048]	Loss: 0.1947
Training Epoch: 66 [3072/50048]	Loss: 0.1218
Training Epoch: 66 [3200/50048]	Loss: 0.2271
Training Epoch: 66 [3328/50048]	Loss: 0.1192
Training Epoch: 66 [3456/50048]	Loss: 0.0711
Training Epoch: 66 [3584/50048]	Loss: 0.0987
Training Epoch: 66 [3712/50048]	Loss: 0.0673
Training Epoch: 66 [3840/50048]	Loss: 0.1213
Training Epoch: 66 [3968/50048]	Loss: 0.0896
Training Epoch: 66 [4096/50048]	Loss: 0.1277
Training Epoch: 66 [4224/50048]	Loss: 0.1690
Training Epoch: 66 [4352/50048]	Loss: 0.1370
Training Epoch: 66 [4480/50048]	Loss: 0.1266
Training Epoch: 66 [4608/50048]	Loss: 0.1055
Training Epoch: 66 [4736/50048]	Loss: 0.0925
Training Epoch: 66 [4864/50048]	Loss: 0.2458
Training Epoch: 66 [4992/50048]	Loss: 0.1369
Training Epoch: 66 [5120/50048]	Loss: 0.1216
Training Epoch: 66 [5248/50048]	Loss: 0.1102
Training Epoch: 66 [5376/50048]	Loss: 0.1726
Training Epoch: 66 [5504/50048]	Loss: 0.1019
Training Epoch: 66 [5632/50048]	Loss: 0.1034
Training Epoch: 66 [5760/50048]	Loss: 0.1431
Training Epoch: 66 [5888/50048]	Loss: 0.1095
Training Epoch: 66 [6016/50048]	Loss: 0.1597
Training Epoch: 66 [6144/50048]	Loss: 0.1273
Training Epoch: 66 [6272/50048]	Loss: 0.2136
Training Epoch: 66 [6400/50048]	Loss: 0.1492
Training Epoch: 66 [6528/50048]	Loss: 0.1332
Training Epoch: 66 [6656/50048]	Loss: 0.1236
Training Epoch: 66 [6784/50048]	Loss: 0.1677
Training Epoch: 66 [6912/50048]	Loss: 0.1947
Training Epoch: 66 [7040/50048]	Loss: 0.0876
Training Epoch: 66 [7168/50048]	Loss: 0.1972
Training Epoch: 66 [7296/50048]	Loss: 0.2142
Training Epoch: 66 [7424/50048]	Loss: 0.1630
Training Epoch: 66 [7552/50048]	Loss: 0.1969
Training Epoch: 66 [7680/50048]	Loss: 0.1323
Training Epoch: 66 [7808/50048]	Loss: 0.1745
Training Epoch: 66 [7936/50048]	Loss: 0.1635
Training Epoch: 66 [8064/50048]	Loss: 0.1392
Training Epoch: 66 [8192/50048]	Loss: 0.1275
Training Epoch: 66 [8320/50048]	Loss: 0.1270
Training Epoch: 66 [8448/50048]	Loss: 0.0991
Training Epoch: 66 [8576/50048]	Loss: 0.1542
Training Epoch: 66 [8704/50048]	Loss: 0.1039
Training Epoch: 66 [8832/50048]	Loss: 0.1475
Training Epoch: 66 [8960/50048]	Loss: 0.1692
Training Epoch: 66 [9088/50048]	Loss: 0.1180
Training Epoch: 66 [9216/50048]	Loss: 0.1166
Training Epoch: 66 [9344/50048]	Loss: 0.0816
Training Epoch: 66 [9472/50048]	Loss: 0.2316
Training Epoch: 66 [9600/50048]	Loss: 0.1583
Training Epoch: 66 [9728/50048]	Loss: 0.1203
Training Epoch: 66 [9856/50048]	Loss: 0.2240
Training Epoch: 66 [9984/50048]	Loss: 0.1090
Training Epoch: 66 [10112/50048]	Loss: 0.0758
Training Epoch: 66 [10240/50048]	Loss: 0.1824
Training Epoch: 66 [10368/50048]	Loss: 0.1356
Training Epoch: 66 [10496/50048]	Loss: 0.1381
Training Epoch: 66 [10624/50048]	Loss: 0.1211
Training Epoch: 66 [10752/50048]	Loss: 0.0827
Training Epoch: 66 [10880/50048]	Loss: 0.1137
Training Epoch: 66 [11008/50048]	Loss: 0.1197
Training Epoch: 66 [11136/50048]	Loss: 0.2020
Training Epoch: 66 [11264/50048]	Loss: 0.1645
Training Epoch: 66 [11392/50048]	Loss: 0.2035
Training Epoch: 66 [11520/50048]	Loss: 0.1129
Training Epoch: 66 [11648/50048]	Loss: 0.1641
Training Epoch: 66 [11776/50048]	Loss: 0.2054
Training Epoch: 66 [11904/50048]	Loss: 0.1390
Training Epoch: 66 [12032/50048]	Loss: 0.1676
Training Epoch: 66 [12160/50048]	Loss: 0.1572
Training Epoch: 66 [12288/50048]	Loss: 0.1808
Training Epoch: 66 [12416/50048]	Loss: 0.1194
Training Epoch: 66 [12544/50048]	Loss: 0.1183
Training Epoch: 66 [12672/50048]	Loss: 0.1485
Training Epoch: 66 [12800/50048]	Loss: 0.1603
Training Epoch: 66 [12928/50048]	Loss: 0.0927
Training Epoch: 66 [13056/50048]	Loss: 0.1348
Training Epoch: 66 [13184/50048]	Loss: 0.1430
Training Epoch: 66 [13312/50048]	Loss: 0.1000
Training Epoch: 66 [13440/50048]	Loss: 0.1431
Training Epoch: 66 [13568/50048]	Loss: 0.1260
Training Epoch: 66 [13696/50048]	Loss: 0.0869
Training Epoch: 66 [13824/50048]	Loss: 0.1533
Training Epoch: 66 [13952/50048]	Loss: 0.1736
Training Epoch: 66 [14080/50048]	Loss: 0.1000
Training Epoch: 66 [14208/50048]	Loss: 0.1090
Training Epoch: 66 [14336/50048]	Loss: 0.1648
Training Epoch: 66 [14464/50048]	Loss: 0.1435
Training Epoch: 66 [14592/50048]	Loss: 0.2273
Training Epoch: 66 [14720/50048]	Loss: 0.1213
Training Epoch: 66 [14848/50048]	Loss: 0.1384
Training Epoch: 66 [14976/50048]	Loss: 0.1574
Training Epoch: 66 [15104/50048]	Loss: 0.1228
Training Epoch: 66 [15232/50048]	Loss: 0.1457
Training Epoch: 66 [15360/50048]	Loss: 0.1266
Training Epoch: 66 [15488/50048]	Loss: 0.1746
Training Epoch: 66 [15616/50048]	Loss: 0.1478
Training Epoch: 66 [15744/50048]	Loss: 0.1918
Training Epoch: 66 [15872/50048]	Loss: 0.1693
Training Epoch: 66 [16000/50048]	Loss: 0.1610
Training Epoch: 66 [16128/50048]	Loss: 0.1730
Training Epoch: 66 [16256/50048]	Loss: 0.1537
Training Epoch: 66 [16384/50048]	Loss: 0.1172
Training Epoch: 66 [16512/50048]	Loss: 0.1895
Training Epoch: 66 [16640/50048]	Loss: 0.1420
Training Epoch: 66 [16768/50048]	Loss: 0.1318
Training Epoch: 66 [16896/50048]	Loss: 0.1212
Training Epoch: 66 [17024/50048]	Loss: 0.2376
Training Epoch: 66 [17152/50048]	Loss: 0.2188
Training Epoch: 66 [17280/50048]	Loss: 0.1320
Training Epoch: 66 [17408/50048]	Loss: 0.1646
Training Epoch: 66 [17536/50048]	Loss: 0.1394
Training Epoch: 66 [17664/50048]	Loss: 0.1402
Training Epoch: 66 [17792/50048]	Loss: 0.1296
Training Epoch: 66 [17920/50048]	Loss: 0.0873
Training Epoch: 66 [18048/50048]	Loss: 0.1990
Training Epoch: 66 [18176/50048]	Loss: 0.1688
Training Epoch: 66 [18304/50048]	Loss: 0.1853
Training Epoch: 66 [18432/50048]	Loss: 0.1631
Training Epoch: 66 [18560/50048]	Loss: 0.1903
Training Epoch: 66 [18688/50048]	Loss: 0.1492
Training Epoch: 66 [18816/50048]	Loss: 0.1153
Training Epoch: 66 [18944/50048]	Loss: 0.1724
Training Epoch: 66 [19072/50048]	Loss: 0.1461
Training Epoch: 66 [19200/50048]	Loss: 0.1666
Training Epoch: 66 [19328/50048]	Loss: 0.1226
Training Epoch: 66 [19456/50048]	Loss: 0.1639
Training Epoch: 66 [19584/50048]	Loss: 0.1450
Training Epoch: 66 [19712/50048]	Loss: 0.1660
Training Epoch: 66 [19840/50048]	Loss: 0.1596
Training Epoch: 66 [19968/50048]	Loss: 0.2116
Training Epoch: 66 [20096/50048]	Loss: 0.2072
Training Epoch: 66 [20224/50048]	Loss: 0.1004
Training Epoch: 66 [20352/50048]	Loss: 0.1000
Training Epoch: 66 [20480/50048]	Loss: 0.1635
Training Epoch: 66 [20608/50048]	Loss: 0.1522
Training Epoch: 66 [20736/50048]	Loss: 0.1798
Training Epoch: 66 [20864/50048]	Loss: 0.1525
Training Epoch: 66 [20992/50048]	Loss: 0.1725
Training Epoch: 66 [21120/50048]	Loss: 0.1132
Training Epoch: 66 [21248/50048]	Loss: 0.1433
Training Epoch: 66 [21376/50048]	Loss: 0.0690
Training Epoch: 66 [21504/50048]	Loss: 0.0743
Training Epoch: 66 [21632/50048]	Loss: 0.1730
Training Epoch: 66 [21760/50048]	Loss: 0.0952
Training Epoch: 66 [21888/50048]	Loss: 0.1292
Training Epoch: 66 [22016/50048]	Loss: 0.1562
Training Epoch: 66 [22144/50048]	Loss: 0.1001
Training Epoch: 66 [22272/50048]	Loss: 0.1100
Training Epoch: 66 [22400/50048]	Loss: 0.1079
Training Epoch: 66 [22528/50048]	Loss: 0.2027
Training Epoch: 66 [22656/50048]	Loss: 0.1745
Training Epoch: 66 [22784/50048]	Loss: 0.1499
Training Epoch: 66 [22912/50048]	Loss: 0.1904
Training Epoch: 66 [23040/50048]	Loss: 0.1311
Training Epoch: 66 [23168/50048]	Loss: 0.2201
Training Epoch: 66 [23296/50048]	Loss: 0.2286
Training Epoch: 66 [23424/50048]	Loss: 0.2344
Training Epoch: 66 [23552/50048]	Loss: 0.1473
Training Epoch: 66 [23680/50048]	Loss: 0.2298
Training Epoch: 66 [23808/50048]	Loss: 0.1352
Training Epoch: 66 [23936/50048]	Loss: 0.1570
Training Epoch: 66 [24064/50048]	Loss: 0.1837
Training Epoch: 66 [24192/50048]	Loss: 0.1715
Training Epoch: 66 [24320/50048]	Loss: 0.1598
Training Epoch: 66 [24448/50048]	Loss: 0.1020
Training Epoch: 66 [24576/50048]	Loss: 0.1410
Training Epoch: 66 [24704/50048]	Loss: 0.1907
Training Epoch: 66 [24832/50048]	Loss: 0.1617
Training Epoch: 66 [24960/50048]	Loss: 0.1144
Training Epoch: 66 [25088/50048]	Loss: 0.1632
Training Epoch: 66 [25216/50048]	Loss: 0.1606
Training Epoch: 66 [25344/50048]	Loss: 0.2254
Training Epoch: 66 [25472/50048]	Loss: 0.0862
Training Epoch: 66 [25600/50048]	Loss: 0.0786
Training Epoch: 66 [25728/50048]	Loss: 0.1625
Training Epoch: 66 [25856/50048]	Loss: 0.1793
Training Epoch: 66 [25984/50048]	Loss: 0.1627
Training Epoch: 66 [26112/50048]	Loss: 0.2118
Training Epoch: 66 [26240/50048]	Loss: 0.1259
Training Epoch: 66 [26368/50048]	Loss: 0.0930
Training Epoch: 66 [26496/50048]	Loss: 0.2073
Training Epoch: 66 [26624/50048]	Loss: 0.1556
Training Epoch: 66 [26752/50048]	Loss: 0.1789
Training Epoch: 66 [26880/50048]	Loss: 0.3013
Training Epoch: 66 [27008/50048]	Loss: 0.0669
Training Epoch: 66 [27136/50048]	Loss: 0.1688
Training Epoch: 66 [27264/50048]	Loss: 0.1708
Training Epoch: 66 [27392/50048]	Loss: 0.1208
Training Epoch: 66 [27520/50048]	Loss: 0.1494
Training Epoch: 66 [27648/50048]	Loss: 0.1727
Training Epoch: 66 [27776/50048]	Loss: 0.2288
Training Epoch: 66 [27904/50048]	Loss: 0.1777
Training Epoch: 66 [28032/50048]	Loss: 0.1528
Training Epoch: 66 [28160/50048]	Loss: 0.1549
Training Epoch: 66 [28288/50048]	Loss: 0.2099
Training Epoch: 66 [28416/50048]	Loss: 0.1543
Training Epoch: 66 [28544/50048]	Loss: 0.2827
Training Epoch: 66 [28672/50048]	Loss: 0.1836
Training Epoch: 66 [28800/50048]	Loss: 0.1664
Training Epoch: 66 [28928/50048]	Loss: 0.0984
Training Epoch: 66 [29056/50048]	Loss: 0.1843
Training Epoch: 66 [29184/50048]	Loss: 0.1509
Training Epoch: 66 [29312/50048]	Loss: 0.1808
Training Epoch: 66 [29440/50048]	Loss: 0.2326
Training Epoch: 66 [29568/50048]	Loss: 0.1384
Training Epoch: 66 [29696/50048]	Loss: 0.2413
Training Epoch: 66 [29824/50048]	Loss: 0.2343
Training Epoch: 66 [29952/50048]	Loss: 0.1799
Training Epoch: 66 [30080/50048]	Loss: 0.1961
Training Epoch: 66 [30208/50048]	Loss: 0.2416
Training Epoch: 66 [30336/50048]	Loss: 0.2045
Training Epoch: 66 [30464/50048]	Loss: 0.1301
Training Epoch: 66 [30592/50048]	Loss: 0.1748
Training Epoch: 66 [30720/50048]	Loss: 0.2273
Training Epoch: 66 [30848/50048]	Loss: 0.1751
Training Epoch: 66 [30976/50048]	Loss: 0.1742
Training Epoch: 66 [31104/50048]	Loss: 0.2521
Training Epoch: 66 [31232/50048]	Loss: 0.1440
Training Epoch: 66 [31360/50048]	Loss: 0.1424
Training Epoch: 66 [31488/50048]	Loss: 0.1829
Training Epoch: 66 [31616/50048]	Loss: 0.1490
Training Epoch: 66 [31744/50048]	Loss: 0.1047
Training Epoch: 66 [31872/50048]	Loss: 0.1013
Training Epoch: 66 [32000/50048]	Loss: 0.0918
Training Epoch: 66 [32128/50048]	Loss: 0.1930
Training Epoch: 66 [32256/50048]	Loss: 0.2485
Training Epoch: 66 [32384/50048]	Loss: 0.1655
Training Epoch: 66 [32512/50048]	Loss: 0.0914
Training Epoch: 66 [32640/50048]	Loss: 0.0862
Training Epoch: 66 [32768/50048]	Loss: 0.1788
Training Epoch: 66 [32896/50048]	Loss: 0.1422
Training Epoch: 66 [33024/50048]	Loss: 0.2875
Training Epoch: 66 [33152/50048]	Loss: 0.1509
Training Epoch: 66 [33280/50048]	Loss: 0.1424
Training Epoch: 66 [33408/50048]	Loss: 0.1595
Training Epoch: 66 [33536/50048]	Loss: 0.0951
Training Epoch: 66 [33664/50048]	Loss: 0.1546
Training Epoch: 66 [33792/50048]	Loss: 0.1729
Training Epoch: 66 [33920/50048]	Loss: 0.1793
Training Epoch: 66 [34048/50048]	Loss: 0.1356
Training Epoch: 66 [34176/50048]	Loss: 0.2096
Training Epoch: 66 [34304/50048]	Loss: 0.1617
Training Epoch: 66 [34432/50048]	Loss: 0.1585
Training Epoch: 66 [34560/50048]	Loss: 0.0988
Training Epoch: 66 [34688/50048]	Loss: 0.1843
Training Epoch: 66 [34816/50048]	Loss: 0.1167
Training Epoch: 66 [34944/50048]	Loss: 0.0916
Training Epoch: 66 [35072/50048]	Loss: 0.1392
Training Epoch: 66 [35200/50048]	Loss: 0.1159
Training Epoch: 66 [35328/50048]	Loss: 0.1493
Training Epoch: 66 [35456/50048]	Loss: 0.2691
Training Epoch: 66 [35584/50048]	Loss: 0.1383
Training Epoch: 66 [35712/50048]	Loss: 0.1948
Training Epoch: 66 [35840/50048]	Loss: 0.2138
Training Epoch: 66 [35968/50048]	Loss: 0.1648
Training Epoch: 66 [36096/50048]	Loss: 0.0898
Training Epoch: 66 [36224/50048]	Loss: 0.1373
Training Epoch: 66 [36352/50048]	Loss: 0.1201
Training Epoch: 66 [36480/50048]	Loss: 0.1211
Training Epoch: 66 [36608/50048]	Loss: 0.0764
Training Epoch: 66 [36736/50048]	Loss: 0.1452
Training Epoch: 66 [36864/50048]	Loss: 0.2238
Training Epoch: 66 [36992/50048]	Loss: 0.1687
Training Epoch: 66 [37120/50048]	Loss: 0.1065
Training Epoch: 66 [37248/50048]	Loss: 0.1247
Training Epoch: 66 [37376/50048]	Loss: 0.2428
Training Epoch: 66 [37504/50048]	Loss: 0.1497
Training Epoch: 66 [37632/50048]	Loss: 0.1355
Training Epoch: 66 [37760/50048]	Loss: 0.2020
Training Epoch: 66 [37888/50048]	Loss: 0.1213
Training Epoch: 66 [38016/50048]	Loss: 0.2102
Training Epoch: 66 [38144/50048]	Loss: 0.1632
Training Epoch: 66 [38272/50048]	Loss: 0.1888
Training Epoch: 66 [38400/50048]	Loss: 0.2516
Training Epoch: 66 [38528/50048]	Loss: 0.1222
Training Epoch: 66 [38656/50048]	Loss: 0.2650
Training Epoch: 66 [38784/50048]	Loss: 0.1951
Training Epoch: 66 [38912/50048]	Loss: 0.1219
Training Epoch: 66 [39040/50048]	Loss: 0.1743
Training Epoch: 66 [39168/50048]	Loss: 0.1538
Training Epoch: 66 [39296/50048]	Loss: 0.1833
Training Epoch: 66 [39424/50048]	Loss: 0.2318
Training Epoch: 66 [39552/50048]	Loss: 0.1842
Training Epoch: 66 [39680/50048]	Loss: 0.1299
Training Epoch: 66 [39808/50048]	Loss: 0.1589
Training Epoch: 66 [39936/50048]	Loss: 0.1888
Training Epoch: 66 [40064/50048]	Loss: 0.1358
Training Epoch: 66 [40192/50048]	Loss: 0.1188
Training Epoch: 66 [40320/50048]	Loss: 0.1478
Training Epoch: 66 [40448/50048]	Loss: 0.1581
Training Epoch: 66 [40576/50048]	Loss: 0.2103
Training Epoch: 66 [40704/50048]	Loss: 0.0965
Training Epoch: 66 [40832/50048]	Loss: 0.2249
Training Epoch: 66 [40960/50048]	Loss: 0.1777
Training Epoch: 66 [41088/50048]	Loss: 0.2035
Training Epoch: 66 [41216/50048]	Loss: 0.1953
Training Epoch: 66 [41344/50048]	Loss: 0.1713
Training Epoch: 66 [41472/50048]	Loss: 0.1844
Training Epoch: 66 [41600/50048]	Loss: 0.1530
Training Epoch: 66 [41728/50048]	Loss: 0.1726
Training Epoch: 66 [41856/50048]	Loss: 0.1174
Training Epoch: 66 [41984/50048]	Loss: 0.2322
Training Epoch: 66 [42112/50048]	Loss: 0.1235
Training Epoch: 66 [42240/50048]	Loss: 0.1440
Training Epoch: 66 [42368/50048]	Loss: 0.1645
Training Epoch: 66 [42496/50048]	Loss: 0.2334
Training Epoch: 66 [42624/50048]	Loss: 0.1660
Training Epoch: 66 [42752/50048]	Loss: 0.2294
Training Epoch: 66 [42880/50048]	Loss: 0.1417
Training Epoch: 66 [43008/50048]	Loss: 0.1948
Training Epoch: 66 [43136/50048]	Loss: 0.1405
Training Epoch: 66 [43264/50048]	Loss: 0.1874
Training Epoch: 66 [43392/50048]	Loss: 0.1190
Training Epoch: 66 [43520/50048]	Loss: 0.1439
Training Epoch: 66 [43648/50048]	Loss: 0.1744
Training Epoch: 66 [43776/50048]	Loss: 0.1007
Training Epoch: 66 [43904/50048]	Loss: 0.2817
Training Epoch: 66 [44032/50048]	Loss: 0.1810
Training Epoch: 66 [44160/50048]	Loss: 0.2037
Training Epoch: 66 [44288/50048]	Loss: 0.1934
Training Epoch: 66 [44416/50048]	Loss: 0.1827
Training Epoch: 66 [44544/50048]	Loss: 0.1644
Training Epoch: 66 [44672/50048]	Loss: 0.0771
Training Epoch: 66 [44800/50048]	Loss: 0.0867
Training Epoch: 66 [44928/50048]	Loss: 0.1839
Training Epoch: 66 [45056/50048]	Loss: 0.2973
Training Epoch: 66 [45184/50048]	Loss: 0.1813
Training Epoch: 66 [45312/50048]	Loss: 0.1465
Training Epoch: 66 [45440/50048]	Loss: 0.1115
Training Epoch: 66 [45568/50048]	Loss: 0.0923
Training Epoch: 66 [45696/50048]	Loss: 0.2269
2022-12-06 05:18:05,501 [ZeusDataLoader(train)] train epoch 67 done: time=86.55 energy=10512.66
2022-12-06 05:18:05,503 [ZeusDataLoader(eval)] Epoch 67 begin.
Training Epoch: 66 [45824/50048]	Loss: 0.1627
Training Epoch: 66 [45952/50048]	Loss: 0.1841
Training Epoch: 66 [46080/50048]	Loss: 0.2451
Training Epoch: 66 [46208/50048]	Loss: 0.1430
Training Epoch: 66 [46336/50048]	Loss: 0.1305
Training Epoch: 66 [46464/50048]	Loss: 0.1345
Training Epoch: 66 [46592/50048]	Loss: 0.2298
Training Epoch: 66 [46720/50048]	Loss: 0.2190
Training Epoch: 66 [46848/50048]	Loss: 0.1032
Training Epoch: 66 [46976/50048]	Loss: 0.1773
Training Epoch: 66 [47104/50048]	Loss: 0.1629
Training Epoch: 66 [47232/50048]	Loss: 0.2445
Training Epoch: 66 [47360/50048]	Loss: 0.1200
Training Epoch: 66 [47488/50048]	Loss: 0.1106
Training Epoch: 66 [47616/50048]	Loss: 0.2443
Training Epoch: 66 [47744/50048]	Loss: 0.1190
Training Epoch: 66 [47872/50048]	Loss: 0.2426
Training Epoch: 66 [48000/50048]	Loss: 0.2182
Training Epoch: 66 [48128/50048]	Loss: 0.1381
Training Epoch: 66 [48256/50048]	Loss: 0.1641
Training Epoch: 66 [48384/50048]	Loss: 0.1285
Training Epoch: 66 [48512/50048]	Loss: 0.0833
Training Epoch: 66 [48640/50048]	Loss: 0.1038
Training Epoch: 66 [48768/50048]	Loss: 0.1962
Training Epoch: 66 [48896/50048]	Loss: 0.1956
Training Epoch: 66 [49024/50048]	Loss: 0.1898
Training Epoch: 66 [49152/50048]	Loss: 0.1324
Training Epoch: 66 [49280/50048]	Loss: 0.1663
Training Epoch: 66 [49408/50048]	Loss: 0.2131
Training Epoch: 66 [49536/50048]	Loss: 0.1701
Training Epoch: 66 [49664/50048]	Loss: 0.1599
Training Epoch: 66 [49792/50048]	Loss: 0.2518
Training Epoch: 66 [49920/50048]	Loss: 0.2348
Training Epoch: 66 [50048/50048]	Loss: 0.2413
2022-12-06 10:18:09.227 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:18:09,269 [ZeusDataLoader(eval)] eval epoch 67 done: time=3.76 energy=452.69
2022-12-06 05:18:09,269 [ZeusDataLoader(train)] Up to epoch 67: time=6045.58, energy=733866.72, cost=895921.53
2022-12-06 05:18:09,269 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:18:09,269 [ZeusDataLoader(train)] Expected next epoch: time=6135.38, energy=744664.74, cost=909177.91
2022-12-06 05:18:09,270 [ZeusDataLoader(train)] Epoch 68 begin.
Validation Epoch: 66, Average loss: 0.0174, Accuracy: 0.6295
2022-12-06 05:18:09,449 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:18:09,449 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:18:09.454 [ZeusMonitor] Monitor started.
2022-12-06 10:18:09.454 [ZeusMonitor] Running indefinitely. 2022-12-06 10:18:09.454 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:18:09.454 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e68+gpu0.power.log
Training Epoch: 67 [128/50048]	Loss: 0.1057
Training Epoch: 67 [256/50048]	Loss: 0.1743
Training Epoch: 67 [384/50048]	Loss: 0.1626
Training Epoch: 67 [512/50048]	Loss: 0.2286
Training Epoch: 67 [640/50048]	Loss: 0.0887
Training Epoch: 67 [768/50048]	Loss: 0.1520
Training Epoch: 67 [896/50048]	Loss: 0.1783
Training Epoch: 67 [1024/50048]	Loss: 0.1500
Training Epoch: 67 [1152/50048]	Loss: 0.1470
Training Epoch: 67 [1280/50048]	Loss: 0.1611
Training Epoch: 67 [1408/50048]	Loss: 0.1245
Training Epoch: 67 [1536/50048]	Loss: 0.1368
Training Epoch: 67 [1664/50048]	Loss: 0.2319
Training Epoch: 67 [1792/50048]	Loss: 0.1415
Training Epoch: 67 [1920/50048]	Loss: 0.1476
Training Epoch: 67 [2048/50048]	Loss: 0.0903
Training Epoch: 67 [2176/50048]	Loss: 0.1145
Training Epoch: 67 [2304/50048]	Loss: 0.1243
Training Epoch: 67 [2432/50048]	Loss: 0.0824
Training Epoch: 67 [2560/50048]	Loss: 0.0946
Training Epoch: 67 [2688/50048]	Loss: 0.2023
Training Epoch: 67 [2816/50048]	Loss: 0.1567
Training Epoch: 67 [2944/50048]	Loss: 0.2919
Training Epoch: 67 [3072/50048]	Loss: 0.1050
Training Epoch: 67 [3200/50048]	Loss: 0.1664
Training Epoch: 67 [3328/50048]	Loss: 0.0882
Training Epoch: 67 [3456/50048]	Loss: 0.1537
Training Epoch: 67 [3584/50048]	Loss: 0.1946
Training Epoch: 67 [3712/50048]	Loss: 0.1529
Training Epoch: 67 [3840/50048]	Loss: 0.1982
Training Epoch: 67 [3968/50048]	Loss: 0.1629
Training Epoch: 67 [4096/50048]	Loss: 0.1185
Training Epoch: 67 [4224/50048]	Loss: 0.1075
Training Epoch: 67 [4352/50048]	Loss: 0.1647
Training Epoch: 67 [4480/50048]	Loss: 0.1034
Training Epoch: 67 [4608/50048]	Loss: 0.1515
Training Epoch: 67 [4736/50048]	Loss: 0.1160
Training Epoch: 67 [4864/50048]	Loss: 0.1989
Training Epoch: 67 [4992/50048]	Loss: 0.1137
Training Epoch: 67 [5120/50048]	Loss: 0.1194
Training Epoch: 67 [5248/50048]	Loss: 0.1505
Training Epoch: 67 [5376/50048]	Loss: 0.1650
Training Epoch: 67 [5504/50048]	Loss: 0.0838
Training Epoch: 67 [5632/50048]	Loss: 0.1182
Training Epoch: 67 [5760/50048]	Loss: 0.1226
Training Epoch: 67 [5888/50048]	Loss: 0.1666
Training Epoch: 67 [6016/50048]	Loss: 0.1223
Training Epoch: 67 [6144/50048]	Loss: 0.0779
Training Epoch: 67 [6272/50048]	Loss: 0.1090
Training Epoch: 67 [6400/50048]	Loss: 0.2367
Training Epoch: 67 [6528/50048]	Loss: 0.2024
Training Epoch: 67 [6656/50048]	Loss: 0.1596
Training Epoch: 67 [6784/50048]	Loss: 0.1473
Training Epoch: 67 [6912/50048]	Loss: 0.2069
Training Epoch: 67 [7040/50048]	Loss: 0.1691
Training Epoch: 67 [7168/50048]	Loss: 0.1545
Training Epoch: 67 [7296/50048]	Loss: 0.1551
Training Epoch: 67 [7424/50048]	Loss: 0.1561
Training Epoch: 67 [7552/50048]	Loss: 0.1839
Training Epoch: 67 [7680/50048]	Loss: 0.1332
Training Epoch: 67 [7808/50048]	Loss: 0.1329
Training Epoch: 67 [7936/50048]	Loss: 0.1186
Training Epoch: 67 [8064/50048]	Loss: 0.0797
Training Epoch: 67 [8192/50048]	Loss: 0.1368
Training Epoch: 67 [8320/50048]	Loss: 0.1259
Training Epoch: 67 [8448/50048]	Loss: 0.0922
Training Epoch: 67 [8576/50048]	Loss: 0.1249
Training Epoch: 67 [8704/50048]	Loss: 0.0847
Training Epoch: 67 [8832/50048]	Loss: 0.1695
Training Epoch: 67 [8960/50048]	Loss: 0.0796
Training Epoch: 67 [9088/50048]	Loss: 0.2525
Training Epoch: 67 [9216/50048]	Loss: 0.1439
Training Epoch: 67 [9344/50048]	Loss: 0.0929
Training Epoch: 67 [9472/50048]	Loss: 0.1365
Training Epoch: 67 [9600/50048]	Loss: 0.1457
Training Epoch: 67 [9728/50048]	Loss: 0.1258
Training Epoch: 67 [9856/50048]	Loss: 0.0807
Training Epoch: 67 [9984/50048]	Loss: 0.0938
Training Epoch: 67 [10112/50048]	Loss: 0.1543
Training Epoch: 67 [10240/50048]	Loss: 0.1444
Training Epoch: 67 [10368/50048]	Loss: 0.0981
Training Epoch: 67 [10496/50048]	Loss: 0.2483
Training Epoch: 67 [10624/50048]	Loss: 0.1429
Training Epoch: 67 [10752/50048]	Loss: 0.0913
Training Epoch: 67 [10880/50048]	Loss: 0.1338
Training Epoch: 67 [11008/50048]	Loss: 0.0810
Training Epoch: 67 [11136/50048]	Loss: 0.1933
Training Epoch: 67 [11264/50048]	Loss: 0.1389
Training Epoch: 67 [11392/50048]	Loss: 0.1239
Training Epoch: 67 [11520/50048]	Loss: 0.1581
Training Epoch: 67 [11648/50048]	Loss: 0.1162
Training Epoch: 67 [11776/50048]	Loss: 0.1298
Training Epoch: 67 [11904/50048]	Loss: 0.1969
Training Epoch: 67 [12032/50048]	Loss: 0.1109
Training Epoch: 67 [12160/50048]	Loss: 0.1129
Training Epoch: 67 [12288/50048]	Loss: 0.1163
Training Epoch: 67 [12416/50048]	Loss: 0.1465
Training Epoch: 67 [12544/50048]	Loss: 0.0824
Training Epoch: 67 [12672/50048]	Loss: 0.1504
Training Epoch: 67 [12800/50048]	Loss: 0.1432
Training Epoch: 67 [12928/50048]	Loss: 0.1715
Training Epoch: 67 [13056/50048]	Loss: 0.1112
Training Epoch: 67 [13184/50048]	Loss: 0.1367
Training Epoch: 67 [13312/50048]	Loss: 0.2060
Training Epoch: 67 [13440/50048]	Loss: 0.0453
Training Epoch: 67 [13568/50048]	Loss: 0.1386
Training Epoch: 67 [13696/50048]	Loss: 0.1983
Training Epoch: 67 [13824/50048]	Loss: 0.0769
Training Epoch: 67 [13952/50048]	Loss: 0.1740
Training Epoch: 67 [14080/50048]	Loss: 0.1363
Training Epoch: 67 [14208/50048]	Loss: 0.3073
Training Epoch: 67 [14336/50048]	Loss: 0.1382
Training Epoch: 67 [14464/50048]	Loss: 0.1679
Training Epoch: 67 [14592/50048]	Loss: 0.2179
Training Epoch: 67 [14720/50048]	Loss: 0.1218
Training Epoch: 67 [14848/50048]	Loss: 0.1459
Training Epoch: 67 [14976/50048]	Loss: 0.1218
Training Epoch: 67 [15104/50048]	Loss: 0.1331
Training Epoch: 67 [15232/50048]	Loss: 0.1297
Training Epoch: 67 [15360/50048]	Loss: 0.1349
Training Epoch: 67 [15488/50048]	Loss: 0.1948
Training Epoch: 67 [15616/50048]	Loss: 0.1057
Training Epoch: 67 [15744/50048]	Loss: 0.1780
Training Epoch: 67 [15872/50048]	Loss: 0.0860
Training Epoch: 67 [16000/50048]	Loss: 0.1194
Training Epoch: 67 [16128/50048]	Loss: 0.1750
Training Epoch: 67 [16256/50048]	Loss: 0.1920
Training Epoch: 67 [16384/50048]	Loss: 0.1824
Training Epoch: 67 [16512/50048]	Loss: 0.1563
Training Epoch: 67 [16640/50048]	Loss: 0.1389
Training Epoch: 67 [16768/50048]	Loss: 0.1840
Training Epoch: 67 [16896/50048]	Loss: 0.1242
Training Epoch: 67 [17024/50048]	Loss: 0.0873
Training Epoch: 67 [17152/50048]	Loss: 0.2070
Training Epoch: 67 [17280/50048]	Loss: 0.2036
Training Epoch: 67 [17408/50048]	Loss: 0.2580
Training Epoch: 67 [17536/50048]	Loss: 0.1678
Training Epoch: 67 [17664/50048]	Loss: 0.1276
Training Epoch: 67 [17792/50048]	Loss: 0.1560
Training Epoch: 67 [17920/50048]	Loss: 0.1154
Training Epoch: 67 [18048/50048]	Loss: 0.1956
Training Epoch: 67 [18176/50048]	Loss: 0.2152
Training Epoch: 67 [18304/50048]	Loss: 0.1847
Training Epoch: 67 [18432/50048]	Loss: 0.1499
Training Epoch: 67 [18560/50048]	Loss: 0.1841
Training Epoch: 67 [18688/50048]	Loss: 0.1634
Training Epoch: 67 [18816/50048]	Loss: 0.1760
Training Epoch: 67 [18944/50048]	Loss: 0.0991
Training Epoch: 67 [19072/50048]	Loss: 0.1748
Training Epoch: 67 [19200/50048]	Loss: 0.2039
Training Epoch: 67 [19328/50048]	Loss: 0.1247
Training Epoch: 67 [19456/50048]	Loss: 0.1507
Training Epoch: 67 [19584/50048]	Loss: 0.2766
Training Epoch: 67 [19712/50048]	Loss: 0.1354
Training Epoch: 67 [19840/50048]	Loss: 0.1079
Training Epoch: 67 [19968/50048]	Loss: 0.1884
Training Epoch: 67 [20096/50048]	Loss: 0.1354
Training Epoch: 67 [20224/50048]	Loss: 0.0791
Training Epoch: 67 [20352/50048]	Loss: 0.2249
Training Epoch: 67 [20480/50048]	Loss: 0.0911
Training Epoch: 67 [20608/50048]	Loss: 0.2084
Training Epoch: 67 [20736/50048]	Loss: 0.1799
Training Epoch: 67 [20864/50048]	Loss: 0.1288
Training Epoch: 67 [20992/50048]	Loss: 0.1622
Training Epoch: 67 [21120/50048]	Loss: 0.1650
Training Epoch: 67 [21248/50048]	Loss: 0.1637
Training Epoch: 67 [21376/50048]	Loss: 0.1476
Training Epoch: 67 [21504/50048]	Loss: 0.1621
Training Epoch: 67 [21632/50048]	Loss: 0.1629
Training Epoch: 67 [21760/50048]	Loss: 0.1908
Training Epoch: 67 [21888/50048]	Loss: 0.2028
Training Epoch: 67 [22016/50048]	Loss: 0.1427
Training Epoch: 67 [22144/50048]	Loss: 0.2051
Training Epoch: 67 [22272/50048]	Loss: 0.0970
Training Epoch: 67 [22400/50048]	Loss: 0.2417
Training Epoch: 67 [22528/50048]	Loss: 0.1468
Training Epoch: 67 [22656/50048]	Loss: 0.1082
Training Epoch: 67 [22784/50048]	Loss: 0.1543
Training Epoch: 67 [22912/50048]	Loss: 0.2067
Training Epoch: 67 [23040/50048]	Loss: 0.2758
Training Epoch: 67 [23168/50048]	Loss: 0.1497
Training Epoch: 67 [23296/50048]	Loss: 0.1779
Training Epoch: 67 [23424/50048]	Loss: 0.2050
Training Epoch: 67 [23552/50048]	Loss: 0.0910
Training Epoch: 67 [23680/50048]	Loss: 0.0868
Training Epoch: 67 [23808/50048]	Loss: 0.1837
Training Epoch: 67 [23936/50048]	Loss: 0.1643
Training Epoch: 67 [24064/50048]	Loss: 0.1165
Training Epoch: 67 [24192/50048]	Loss: 0.1199
Training Epoch: 67 [24320/50048]	Loss: 0.0681
Training Epoch: 67 [24448/50048]	Loss: 0.1396
Training Epoch: 67 [24576/50048]	Loss: 0.1352
Training Epoch: 67 [24704/50048]	Loss: 0.1942
Training Epoch: 67 [24832/50048]	Loss: 0.1498
Training Epoch: 67 [24960/50048]	Loss: 0.1520
Training Epoch: 67 [25088/50048]	Loss: 0.1096
Training Epoch: 67 [25216/50048]	Loss: 0.1820
Training Epoch: 67 [25344/50048]	Loss: 0.1108
Training Epoch: 67 [25472/50048]	Loss: 0.1511
Training Epoch: 67 [25600/50048]	Loss: 0.1618
Training Epoch: 67 [25728/50048]	Loss: 0.1591
Training Epoch: 67 [25856/50048]	Loss: 0.1382
Training Epoch: 67 [25984/50048]	Loss: 0.1048
Training Epoch: 67 [26112/50048]	Loss: 0.1362
Training Epoch: 67 [26240/50048]	Loss: 0.2293
Training Epoch: 67 [26368/50048]	Loss: 0.1839
Training Epoch: 67 [26496/50048]	Loss: 0.1194
Training Epoch: 67 [26624/50048]	Loss: 0.1798
Training Epoch: 67 [26752/50048]	Loss: 0.2132
Training Epoch: 67 [26880/50048]	Loss: 0.1398
Training Epoch: 67 [27008/50048]	Loss: 0.1743
Training Epoch: 67 [27136/50048]	Loss: 0.2332
Training Epoch: 67 [27264/50048]	Loss: 0.1825
Training Epoch: 67 [27392/50048]	Loss: 0.1514
Training Epoch: 67 [27520/50048]	Loss: 0.1036
Training Epoch: 67 [27648/50048]	Loss: 0.1821
Training Epoch: 67 [27776/50048]	Loss: 0.0860
Training Epoch: 67 [27904/50048]	Loss: 0.1354
Training Epoch: 67 [28032/50048]	Loss: 0.0930
Training Epoch: 67 [28160/50048]	Loss: 0.1554
Training Epoch: 67 [28288/50048]	Loss: 0.2164
Training Epoch: 67 [28416/50048]	Loss: 0.1555
Training Epoch: 67 [28544/50048]	Loss: 0.1635
Training Epoch: 67 [28672/50048]	Loss: 0.1973
Training Epoch: 67 [28800/50048]	Loss: 0.1371
Training Epoch: 67 [28928/50048]	Loss: 0.1523
Training Epoch: 67 [29056/50048]	Loss: 0.2063
Training Epoch: 67 [29184/50048]	Loss: 0.1498
Training Epoch: 67 [29312/50048]	Loss: 0.2066
Training Epoch: 67 [29440/50048]	Loss: 0.1484
Training Epoch: 67 [29568/50048]	Loss: 0.1604
Training Epoch: 67 [29696/50048]	Loss: 0.1797
Training Epoch: 67 [29824/50048]	Loss: 0.1281
Training Epoch: 67 [29952/50048]	Loss: 0.2326
Training Epoch: 67 [30080/50048]	Loss: 0.1441
Training Epoch: 67 [30208/50048]	Loss: 0.2027
Training Epoch: 67 [30336/50048]	Loss: 0.0819
Training Epoch: 67 [30464/50048]	Loss: 0.1586
Training Epoch: 67 [30592/50048]	Loss: 0.1525
Training Epoch: 67 [30720/50048]	Loss: 0.1681
Training Epoch: 67 [30848/50048]	Loss: 0.2065
Training Epoch: 67 [30976/50048]	Loss: 0.1726
Training Epoch: 67 [31104/50048]	Loss: 0.1766
Training Epoch: 67 [31232/50048]	Loss: 0.2312
Training Epoch: 67 [31360/50048]	Loss: 0.1648
Training Epoch: 67 [31488/50048]	Loss: 0.0924
Training Epoch: 67 [31616/50048]	Loss: 0.1513
Training Epoch: 67 [31744/50048]	Loss: 0.1199
Training Epoch: 67 [31872/50048]	Loss: 0.1223
Training Epoch: 67 [32000/50048]	Loss: 0.0945
Training Epoch: 67 [32128/50048]	Loss: 0.1285
Training Epoch: 67 [32256/50048]	Loss: 0.1244
Training Epoch: 67 [32384/50048]	Loss: 0.1253
Training Epoch: 67 [32512/50048]	Loss: 0.1840
Training Epoch: 67 [32640/50048]	Loss: 0.1499
Training Epoch: 67 [32768/50048]	Loss: 0.2274
Training Epoch: 67 [32896/50048]	Loss: 0.2113
Training Epoch: 67 [33024/50048]	Loss: 0.1479
Training Epoch: 67 [33152/50048]	Loss: 0.2037
Training Epoch: 67 [33280/50048]	Loss: 0.1391
Training Epoch: 67 [33408/50048]	Loss: 0.1406
Training Epoch: 67 [33536/50048]	Loss: 0.1704
Training Epoch: 67 [33664/50048]	Loss: 0.1301
Training Epoch: 67 [33792/50048]	Loss: 0.0909
Training Epoch: 67 [33920/50048]	Loss: 0.1503
Training Epoch: 67 [34048/50048]	Loss: 0.1643
Training Epoch: 67 [34176/50048]	Loss: 0.1023
Training Epoch: 67 [34304/50048]	Loss: 0.2703
Training Epoch: 67 [34432/50048]	Loss: 0.1112
Training Epoch: 67 [34560/50048]	Loss: 0.1542
Training Epoch: 67 [34688/50048]	Loss: 0.1162
Training Epoch: 67 [34816/50048]	Loss: 0.1947
Training Epoch: 67 [34944/50048]	Loss: 0.1147
Training Epoch: 67 [35072/50048]	Loss: 0.1811
Training Epoch: 67 [35200/50048]	Loss: 0.0929
Training Epoch: 67 [35328/50048]	Loss: 0.1695
Training Epoch: 67 [35456/50048]	Loss: 0.1335
Training Epoch: 67 [35584/50048]	Loss: 0.1075
Training Epoch: 67 [35712/50048]	Loss: 0.1342
Training Epoch: 67 [35840/50048]	Loss: 0.1467
Training Epoch: 67 [35968/50048]	Loss: 0.1414
Training Epoch: 67 [36096/50048]	Loss: 0.1092
Training Epoch: 67 [36224/50048]	Loss: 0.1208
Training Epoch: 67 [36352/50048]	Loss: 0.0884
Training Epoch: 67 [36480/50048]	Loss: 0.1451
Training Epoch: 67 [36608/50048]	Loss: 0.1530
Training Epoch: 67 [36736/50048]	Loss: 0.1817
Training Epoch: 67 [36864/50048]	Loss: 0.1200
Training Epoch: 67 [36992/50048]	Loss: 0.2075
Training Epoch: 67 [37120/50048]	Loss: 0.1044
Training Epoch: 67 [37248/50048]	Loss: 0.0718
Training Epoch: 67 [37376/50048]	Loss: 0.1846
Training Epoch: 67 [37504/50048]	Loss: 0.2261
Training Epoch: 67 [37632/50048]	Loss: 0.1929
Training Epoch: 67 [37760/50048]	Loss: 0.1676
Training Epoch: 67 [37888/50048]	Loss: 0.1249
Training Epoch: 67 [38016/50048]	Loss: 0.1239
Training Epoch: 67 [38144/50048]	Loss: 0.1394
Training Epoch: 67 [38272/50048]	Loss: 0.1579
Training Epoch: 67 [38400/50048]	Loss: 0.1271
Training Epoch: 67 [38528/50048]	Loss: 0.1533
Training Epoch: 67 [38656/50048]	Loss: 0.1839
Training Epoch: 67 [38784/50048]	Loss: 0.1787
Training Epoch: 67 [38912/50048]	Loss: 0.0838
Training Epoch: 67 [39040/50048]	Loss: 0.0831
Training Epoch: 67 [39168/50048]	Loss: 0.2553
Training Epoch: 67 [39296/50048]	Loss: 0.0843
Training Epoch: 67 [39424/50048]	Loss: 0.1982
Training Epoch: 67 [39552/50048]	Loss: 0.0961
Training Epoch: 67 [39680/50048]	Loss: 0.1168
Training Epoch: 67 [39808/50048]	Loss: 0.1373
Training Epoch: 67 [39936/50048]	Loss: 0.1337
Training Epoch: 67 [40064/50048]	Loss: 0.1740
Training Epoch: 67 [40192/50048]	Loss: 0.1436
Training Epoch: 67 [40320/50048]	Loss: 0.2632
Training Epoch: 67 [40448/50048]	Loss: 0.0976
Training Epoch: 67 [40576/50048]	Loss: 0.1792
Training Epoch: 67 [40704/50048]	Loss: 0.0997
Training Epoch: 67 [40832/50048]	Loss: 0.1523
Training Epoch: 67 [40960/50048]	Loss: 0.2083
Training Epoch: 67 [41088/50048]	Loss: 0.2453
Training Epoch: 67 [41216/50048]	Loss: 0.1343
Training Epoch: 67 [41344/50048]	Loss: 0.1317
Training Epoch: 67 [41472/50048]	Loss: 0.1788
Training Epoch: 67 [41600/50048]	Loss: 0.1369
Training Epoch: 67 [41728/50048]	Loss: 0.1439
Training Epoch: 67 [41856/50048]	Loss: 0.2226
Training Epoch: 67 [41984/50048]	Loss: 0.1850
Training Epoch: 67 [42112/50048]	Loss: 0.1497
Training Epoch: 67 [42240/50048]	Loss: 0.1364
Training Epoch: 67 [42368/50048]	Loss: 0.1664
Training Epoch: 67 [42496/50048]	Loss: 0.1777
Training Epoch: 67 [42624/50048]	Loss: 0.1406
Training Epoch: 67 [42752/50048]	Loss: 0.2657
Training Epoch: 67 [42880/50048]	Loss: 0.2083
Training Epoch: 67 [43008/50048]	Loss: 0.1837
Training Epoch: 67 [43136/50048]	Loss: 0.1551
Training Epoch: 67 [43264/50048]	Loss: 0.1030
Training Epoch: 67 [43392/50048]	Loss: 0.1329
Training Epoch: 67 [43520/50048]	Loss: 0.2128
Training Epoch: 67 [43648/50048]	Loss: 0.1372
Training Epoch: 67 [43776/50048]	Loss: 0.1481
Training Epoch: 67 [43904/50048]	Loss: 0.1322
Training Epoch: 67 [44032/50048]	Loss: 0.2206
Training Epoch: 67 [44160/50048]	Loss: 0.1410
Training Epoch: 67 [44288/50048]	Loss: 0.1665
Training Epoch: 67 [44416/50048]	Loss: 0.1467
Training Epoch: 67 [44544/50048]	Loss: 0.1189
Training Epoch: 67 [44672/50048]	Loss: 0.1741
Training Epoch: 67 [44800/50048]	Loss: 0.3615
Training Epoch: 67 [44928/50048]	Loss: 0.2036
Training Epoch: 67 [45056/50048]	Loss: 0.1735
Training Epoch: 67 [45184/50048]	Loss: 0.1563
Training Epoch: 67 [45312/50048]	Loss: 0.1897
Training Epoch: 67 [45440/50048]	Loss: 0.1729
Training Epoch: 67 [45568/50048]	Loss: 0.2828
Training Epoch: 67 [45696/50048]	Loss: 0.1704
2022-12-06 05:19:35,833 [ZeusDataLoader(train)] train epoch 68 done: time=86.55 energy=10499.06
2022-12-06 05:19:35,834 [ZeusDataLoader(eval)] Epoch 68 begin.
Training Epoch: 67 [45824/50048]	Loss: 0.2154
Training Epoch: 67 [45952/50048]	Loss: 0.2312
Training Epoch: 67 [46080/50048]	Loss: 0.2605
Training Epoch: 67 [46208/50048]	Loss: 0.1521
Training Epoch: 67 [46336/50048]	Loss: 0.1087
Training Epoch: 67 [46464/50048]	Loss: 0.1456
Training Epoch: 67 [46592/50048]	Loss: 0.1198
Training Epoch: 67 [46720/50048]	Loss: 0.2053
Training Epoch: 67 [46848/50048]	Loss: 0.1082
Training Epoch: 67 [46976/50048]	Loss: 0.1743
Training Epoch: 67 [47104/50048]	Loss: 0.2055
Training Epoch: 67 [47232/50048]	Loss: 0.1829
Training Epoch: 67 [47360/50048]	Loss: 0.2449
Training Epoch: 67 [47488/50048]	Loss: 0.1423
Training Epoch: 67 [47616/50048]	Loss: 0.1167
Training Epoch: 67 [47744/50048]	Loss: 0.2278
Training Epoch: 67 [47872/50048]	Loss: 0.2148
Training Epoch: 67 [48000/50048]	Loss: 0.1403
Training Epoch: 67 [48128/50048]	Loss: 0.1303
Training Epoch: 67 [48256/50048]	Loss: 0.1287
Training Epoch: 67 [48384/50048]	Loss: 0.1422
Training Epoch: 67 [48512/50048]	Loss: 0.1653
Training Epoch: 67 [48640/50048]	Loss: 0.1196
Training Epoch: 67 [48768/50048]	Loss: 0.2071
Training Epoch: 67 [48896/50048]	Loss: 0.1539
Training Epoch: 67 [49024/50048]	Loss: 0.1721
Training Epoch: 67 [49152/50048]	Loss: 0.1370
Training Epoch: 67 [49280/50048]	Loss: 0.1838
Training Epoch: 67 [49408/50048]	Loss: 0.2032
Training Epoch: 67 [49536/50048]	Loss: 0.1090
Training Epoch: 67 [49664/50048]	Loss: 0.1306
Training Epoch: 67 [49792/50048]	Loss: 0.1122
Training Epoch: 67 [49920/50048]	Loss: 0.2067
Training Epoch: 67 [50048/50048]	Loss: 0.1745
2022-12-06 10:19:39.513 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:19:39,550 [ZeusDataLoader(eval)] eval epoch 68 done: time=3.71 energy=453.30
2022-12-06 05:19:39,550 [ZeusDataLoader(train)] Up to epoch 68: time=6135.84, energy=744819.08, cost=909295.34
2022-12-06 05:19:39,550 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:19:39,550 [ZeusDataLoader(train)] Expected next epoch: time=6225.64, energy=755617.09, cost=922551.72
2022-12-06 05:19:39,551 [ZeusDataLoader(train)] Epoch 69 begin.
Validation Epoch: 67, Average loss: 0.0169, Accuracy: 0.6364
2022-12-06 05:19:39,690 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:19:39,691 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:19:39.695 [ZeusMonitor] Monitor started.
2022-12-06 10:19:39.695 [ZeusMonitor] Running indefinitely. 2022-12-06 10:19:39.695 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:19:39.695 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e69+gpu0.power.log
Training Epoch: 68 [128/50048]	Loss: 0.2034
Training Epoch: 68 [256/50048]	Loss: 0.1014
Training Epoch: 68 [384/50048]	Loss: 0.1530
Training Epoch: 68 [512/50048]	Loss: 0.1570
Training Epoch: 68 [640/50048]	Loss: 0.0632
Training Epoch: 68 [768/50048]	Loss: 0.1610
Training Epoch: 68 [896/50048]	Loss: 0.1790
Training Epoch: 68 [1024/50048]	Loss: 0.1741
Training Epoch: 68 [1152/50048]	Loss: 0.1117
Training Epoch: 68 [1280/50048]	Loss: 0.1089
Training Epoch: 68 [1408/50048]	Loss: 0.1730
Training Epoch: 68 [1536/50048]	Loss: 0.1473
Training Epoch: 68 [1664/50048]	Loss: 0.1010
Training Epoch: 68 [1792/50048]	Loss: 0.1921
Training Epoch: 68 [1920/50048]	Loss: 0.1740
Training Epoch: 68 [2048/50048]	Loss: 0.1814
Training Epoch: 68 [2176/50048]	Loss: 0.1669
Training Epoch: 68 [2304/50048]	Loss: 0.1668
Training Epoch: 68 [2432/50048]	Loss: 0.1575
Training Epoch: 68 [2560/50048]	Loss: 0.1088
Training Epoch: 68 [2688/50048]	Loss: 0.1815
Training Epoch: 68 [2816/50048]	Loss: 0.0665
Training Epoch: 68 [2944/50048]	Loss: 0.1048
Training Epoch: 68 [3072/50048]	Loss: 0.1142
Training Epoch: 68 [3200/50048]	Loss: 0.2103
Training Epoch: 68 [3328/50048]	Loss: 0.2162
Training Epoch: 68 [3456/50048]	Loss: 0.1103
Training Epoch: 68 [3584/50048]	Loss: 0.0845
Training Epoch: 68 [3712/50048]	Loss: 0.1502
Training Epoch: 68 [3840/50048]	Loss: 0.1711
Training Epoch: 68 [3968/50048]	Loss: 0.1796
Training Epoch: 68 [4096/50048]	Loss: 0.0786
Training Epoch: 68 [4224/50048]	Loss: 0.1424
Training Epoch: 68 [4352/50048]	Loss: 0.1184
Training Epoch: 68 [4480/50048]	Loss: 0.1719
Training Epoch: 68 [4608/50048]	Loss: 0.0733
Training Epoch: 68 [4736/50048]	Loss: 0.1879
Training Epoch: 68 [4864/50048]	Loss: 0.1057
Training Epoch: 68 [4992/50048]	Loss: 0.1929
Training Epoch: 68 [5120/50048]	Loss: 0.1951
Training Epoch: 68 [5248/50048]	Loss: 0.1073
Training Epoch: 68 [5376/50048]	Loss: 0.0631
Training Epoch: 68 [5504/50048]	Loss: 0.2379
Training Epoch: 68 [5632/50048]	Loss: 0.0913
Training Epoch: 68 [5760/50048]	Loss: 0.1726
Training Epoch: 68 [5888/50048]	Loss: 0.1689
Training Epoch: 68 [6016/50048]	Loss: 0.2145
Training Epoch: 68 [6144/50048]	Loss: 0.1681
Training Epoch: 68 [6272/50048]	Loss: 0.1287
Training Epoch: 68 [6400/50048]	Loss: 0.1383
Training Epoch: 68 [6528/50048]	Loss: 0.1194
Training Epoch: 68 [6656/50048]	Loss: 0.1147
Training Epoch: 68 [6784/50048]	Loss: 0.1259
Training Epoch: 68 [6912/50048]	Loss: 0.1569
Training Epoch: 68 [7040/50048]	Loss: 0.0936
Training Epoch: 68 [7168/50048]	Loss: 0.1764
Training Epoch: 68 [7296/50048]	Loss: 0.1551
Training Epoch: 68 [7424/50048]	Loss: 0.1127
Training Epoch: 68 [7552/50048]	Loss: 0.1305
Training Epoch: 68 [7680/50048]	Loss: 0.0972
Training Epoch: 68 [7808/50048]	Loss: 0.1560
Training Epoch: 68 [7936/50048]	Loss: 0.1546
Training Epoch: 68 [8064/50048]	Loss: 0.1128
Training Epoch: 68 [8192/50048]	Loss: 0.2452
Training Epoch: 68 [8320/50048]	Loss: 0.1643
Training Epoch: 68 [8448/50048]	Loss: 0.2103
Training Epoch: 68 [8576/50048]	Loss: 0.1122
Training Epoch: 68 [8704/50048]	Loss: 0.1376
Training Epoch: 68 [8832/50048]	Loss: 0.0881
Training Epoch: 68 [8960/50048]	Loss: 0.1627
Training Epoch: 68 [9088/50048]	Loss: 0.0922
Training Epoch: 68 [9216/50048]	Loss: 0.0835
Training Epoch: 68 [9344/50048]	Loss: 0.1539
Training Epoch: 68 [9472/50048]	Loss: 0.1344
Training Epoch: 68 [9600/50048]	Loss: 0.1682
Training Epoch: 68 [9728/50048]	Loss: 0.0928
Training Epoch: 68 [9856/50048]	Loss: 0.1171
Training Epoch: 68 [9984/50048]	Loss: 0.1364
Training Epoch: 68 [10112/50048]	Loss: 0.1437
Training Epoch: 68 [10240/50048]	Loss: 0.1799
Training Epoch: 68 [10368/50048]	Loss: 0.1262
Training Epoch: 68 [10496/50048]	Loss: 0.1092
Training Epoch: 68 [10624/50048]	Loss: 0.1363
Training Epoch: 68 [10752/50048]	Loss: 0.1098
Training Epoch: 68 [10880/50048]	Loss: 0.1027
Training Epoch: 68 [11008/50048]	Loss: 0.1388
Training Epoch: 68 [11136/50048]	Loss: 0.1302
Training Epoch: 68 [11264/50048]	Loss: 0.1774
Training Epoch: 68 [11392/50048]	Loss: 0.1469
Training Epoch: 68 [11520/50048]	Loss: 0.1041
Training Epoch: 68 [11648/50048]	Loss: 0.1341
Training Epoch: 68 [11776/50048]	Loss: 0.1379
Training Epoch: 68 [11904/50048]	Loss: 0.0903
Training Epoch: 68 [12032/50048]	Loss: 0.0660
Training Epoch: 68 [12160/50048]	Loss: 0.0890
Training Epoch: 68 [12288/50048]	Loss: 0.1021
Training Epoch: 68 [12416/50048]	Loss: 0.0988
Training Epoch: 68 [12544/50048]	Loss: 0.0803
Training Epoch: 68 [12672/50048]	Loss: 0.1386
Training Epoch: 68 [12800/50048]	Loss: 0.1165
Training Epoch: 68 [12928/50048]	Loss: 0.1113
Training Epoch: 68 [13056/50048]	Loss: 0.1140
Training Epoch: 68 [13184/50048]	Loss: 0.1644
Training Epoch: 68 [13312/50048]	Loss: 0.2066
Training Epoch: 68 [13440/50048]	Loss: 0.1002
Training Epoch: 68 [13568/50048]	Loss: 0.1412
Training Epoch: 68 [13696/50048]	Loss: 0.1019
Training Epoch: 68 [13824/50048]	Loss: 0.2228
Training Epoch: 68 [13952/50048]	Loss: 0.0899
Training Epoch: 68 [14080/50048]	Loss: 0.1276
Training Epoch: 68 [14208/50048]	Loss: 0.1123
Training Epoch: 68 [14336/50048]	Loss: 0.2253
Training Epoch: 68 [14464/50048]	Loss: 0.1647
Training Epoch: 68 [14592/50048]	Loss: 0.1660
Training Epoch: 68 [14720/50048]	Loss: 0.1773
Training Epoch: 68 [14848/50048]	Loss: 0.1281
Training Epoch: 68 [14976/50048]	Loss: 0.1030
Training Epoch: 68 [15104/50048]	Loss: 0.1166
Training Epoch: 68 [15232/50048]	Loss: 0.1883
Training Epoch: 68 [15360/50048]	Loss: 0.1736
Training Epoch: 68 [15488/50048]	Loss: 0.1313
Training Epoch: 68 [15616/50048]	Loss: 0.1215
Training Epoch: 68 [15744/50048]	Loss: 0.1832
Training Epoch: 68 [15872/50048]	Loss: 0.1797
Training Epoch: 68 [16000/50048]	Loss: 0.1697
Training Epoch: 68 [16128/50048]	Loss: 0.1359
Training Epoch: 68 [16256/50048]	Loss: 0.1117
Training Epoch: 68 [16384/50048]	Loss: 0.1196
Training Epoch: 68 [16512/50048]	Loss: 0.1445
Training Epoch: 68 [16640/50048]	Loss: 0.1584
Training Epoch: 68 [16768/50048]	Loss: 0.1176
Training Epoch: 68 [16896/50048]	Loss: 0.1718
Training Epoch: 68 [17024/50048]	Loss: 0.2139
Training Epoch: 68 [17152/50048]	Loss: 0.1519
Training Epoch: 68 [17280/50048]	Loss: 0.2222
Training Epoch: 68 [17408/50048]	Loss: 0.1915
Training Epoch: 68 [17536/50048]	Loss: 0.1345
Training Epoch: 68 [17664/50048]	Loss: 0.1014
Training Epoch: 68 [17792/50048]	Loss: 0.1299
Training Epoch: 68 [17920/50048]	Loss: 0.1055
Training Epoch: 68 [18048/50048]	Loss: 0.1748
Training Epoch: 68 [18176/50048]	Loss: 0.1150
Training Epoch: 68 [18304/50048]	Loss: 0.1458
Training Epoch: 68 [18432/50048]	Loss: 0.1210
Training Epoch: 68 [18560/50048]	Loss: 0.1126
Training Epoch: 68 [18688/50048]	Loss: 0.1574
Training Epoch: 68 [18816/50048]	Loss: 0.0678
Training Epoch: 68 [18944/50048]	Loss: 0.1518
Training Epoch: 68 [19072/50048]	Loss: 0.1023
Training Epoch: 68 [19200/50048]	Loss: 0.2167
Training Epoch: 68 [19328/50048]	Loss: 0.1088
Training Epoch: 68 [19456/50048]	Loss: 0.1331
Training Epoch: 68 [19584/50048]	Loss: 0.1836
Training Epoch: 68 [19712/50048]	Loss: 0.1747
Training Epoch: 68 [19840/50048]	Loss: 0.2008
Training Epoch: 68 [19968/50048]	Loss: 0.1366
Training Epoch: 68 [20096/50048]	Loss: 0.1589
Training Epoch: 68 [20224/50048]	Loss: 0.1677
Training Epoch: 68 [20352/50048]	Loss: 0.1050
Training Epoch: 68 [20480/50048]	Loss: 0.1252
Training Epoch: 68 [20608/50048]	Loss: 0.1803
Training Epoch: 68 [20736/50048]	Loss: 0.2432
Training Epoch: 68 [20864/50048]	Loss: 0.1536
Training Epoch: 68 [20992/50048]	Loss: 0.1548
Training Epoch: 68 [21120/50048]	Loss: 0.1099
Training Epoch: 68 [21248/50048]	Loss: 0.1093
Training Epoch: 68 [21376/50048]	Loss: 0.1479
Training Epoch: 68 [21504/50048]	Loss: 0.1019
Training Epoch: 68 [21632/50048]	Loss: 0.1961
Training Epoch: 68 [21760/50048]	Loss: 0.1898
Training Epoch: 68 [21888/50048]	Loss: 0.1066
Training Epoch: 68 [22016/50048]	Loss: 0.1110
Training Epoch: 68 [22144/50048]	Loss: 0.1303
Training Epoch: 68 [22272/50048]	Loss: 0.1241
Training Epoch: 68 [22400/50048]	Loss: 0.1557
Training Epoch: 68 [22528/50048]	Loss: 0.1390
Training Epoch: 68 [22656/50048]	Loss: 0.1692
Training Epoch: 68 [22784/50048]	Loss: 0.0816
Training Epoch: 68 [22912/50048]	Loss: 0.2245
Training Epoch: 68 [23040/50048]	Loss: 0.1402
Training Epoch: 68 [23168/50048]	Loss: 0.1198
Training Epoch: 68 [23296/50048]	Loss: 0.2092
Training Epoch: 68 [23424/50048]	Loss: 0.0668
Training Epoch: 68 [23552/50048]	Loss: 0.1617
Training Epoch: 68 [23680/50048]	Loss: 0.1143
Training Epoch: 68 [23808/50048]	Loss: 0.1218
Training Epoch: 68 [23936/50048]	Loss: 0.2084
Training Epoch: 68 [24064/50048]	Loss: 0.2006
Training Epoch: 68 [24192/50048]	Loss: 0.1565
Training Epoch: 68 [24320/50048]	Loss: 0.0658
Training Epoch: 68 [24448/50048]	Loss: 0.1424
Training Epoch: 68 [24576/50048]	Loss: 0.1547
Training Epoch: 68 [24704/50048]	Loss: 0.1295
Training Epoch: 68 [24832/50048]	Loss: 0.2259
Training Epoch: 68 [24960/50048]	Loss: 0.1080
Training Epoch: 68 [25088/50048]	Loss: 0.1396
Training Epoch: 68 [25216/50048]	Loss: 0.1781
Training Epoch: 68 [25344/50048]	Loss: 0.1198
Training Epoch: 68 [25472/50048]	Loss: 0.1328
Training Epoch: 68 [25600/50048]	Loss: 0.0832
Training Epoch: 68 [25728/50048]	Loss: 0.1556
Training Epoch: 68 [25856/50048]	Loss: 0.2072
Training Epoch: 68 [25984/50048]	Loss: 0.1028
Training Epoch: 68 [26112/50048]	Loss: 0.1827
Training Epoch: 68 [26240/50048]	Loss: 0.1538
Training Epoch: 68 [26368/50048]	Loss: 0.1168
Training Epoch: 68 [26496/50048]	Loss: 0.0818
Training Epoch: 68 [26624/50048]	Loss: 0.1174
Training Epoch: 68 [26752/50048]	Loss: 0.1432
Training Epoch: 68 [26880/50048]	Loss: 0.1591
Training Epoch: 68 [27008/50048]	Loss: 0.2027
Training Epoch: 68 [27136/50048]	Loss: 0.1707
Training Epoch: 68 [27264/50048]	Loss: 0.1897
Training Epoch: 68 [27392/50048]	Loss: 0.1877
Training Epoch: 68 [27520/50048]	Loss: 0.1761
Training Epoch: 68 [27648/50048]	Loss: 0.1717
Training Epoch: 68 [27776/50048]	Loss: 0.1718
Training Epoch: 68 [27904/50048]	Loss: 0.1090
Training Epoch: 68 [28032/50048]	Loss: 0.1349
Training Epoch: 68 [28160/50048]	Loss: 0.2113
Training Epoch: 68 [28288/50048]	Loss: 0.0890
Training Epoch: 68 [28416/50048]	Loss: 0.1659
Training Epoch: 68 [28544/50048]	Loss: 0.3071
Training Epoch: 68 [28672/50048]	Loss: 0.2534
Training Epoch: 68 [28800/50048]	Loss: 0.1784
Training Epoch: 68 [28928/50048]	Loss: 0.2051
Training Epoch: 68 [29056/50048]	Loss: 0.1061
Training Epoch: 68 [29184/50048]	Loss: 0.2446
Training Epoch: 68 [29312/50048]	Loss: 0.2288
Training Epoch: 68 [29440/50048]	Loss: 0.1969
Training Epoch: 68 [29568/50048]	Loss: 0.1169
Training Epoch: 68 [29696/50048]	Loss: 0.2214
Training Epoch: 68 [29824/50048]	Loss: 0.2135
Training Epoch: 68 [29952/50048]	Loss: 0.1823
Training Epoch: 68 [30080/50048]	Loss: 0.0756
Training Epoch: 68 [30208/50048]	Loss: 0.2104
Training Epoch: 68 [30336/50048]	Loss: 0.3130
Training Epoch: 68 [30464/50048]	Loss: 0.2343
Training Epoch: 68 [30592/50048]	Loss: 0.1757
Training Epoch: 68 [30720/50048]	Loss: 0.1576
Training Epoch: 68 [30848/50048]	Loss: 0.1618
Training Epoch: 68 [30976/50048]	Loss: 0.1294
Training Epoch: 68 [31104/50048]	Loss: 0.1328
Training Epoch: 68 [31232/50048]	Loss: 0.1336
Training Epoch: 68 [31360/50048]	Loss: 0.0863
Training Epoch: 68 [31488/50048]	Loss: 0.1297
Training Epoch: 68 [31616/50048]	Loss: 0.1238
Training Epoch: 68 [31744/50048]	Loss: 0.2095
Training Epoch: 68 [31872/50048]	Loss: 0.0810
Training Epoch: 68 [32000/50048]	Loss: 0.1615
Training Epoch: 68 [32128/50048]	Loss: 0.1077
Training Epoch: 68 [32256/50048]	Loss: 0.1255
Training Epoch: 68 [32384/50048]	Loss: 0.2611
Training Epoch: 68 [32512/50048]	Loss: 0.1736
Training Epoch: 68 [32640/50048]	Loss: 0.2308
Training Epoch: 68 [32768/50048]	Loss: 0.1368
Training Epoch: 68 [32896/50048]	Loss: 0.1381
Training Epoch: 68 [33024/50048]	Loss: 0.1092
Training Epoch: 68 [33152/50048]	Loss: 0.2118
Training Epoch: 68 [33280/50048]	Loss: 0.1587
Training Epoch: 68 [33408/50048]	Loss: 0.1558
Training Epoch: 68 [33536/50048]	Loss: 0.1653
Training Epoch: 68 [33664/50048]	Loss: 0.1172
Training Epoch: 68 [33792/50048]	Loss: 0.2391
Training Epoch: 68 [33920/50048]	Loss: 0.1145
Training Epoch: 68 [34048/50048]	Loss: 0.0961
Training Epoch: 68 [34176/50048]	Loss: 0.0871
Training Epoch: 68 [34304/50048]	Loss: 0.1389
Training Epoch: 68 [34432/50048]	Loss: 0.1512
Training Epoch: 68 [34560/50048]	Loss: 0.1487
Training Epoch: 68 [34688/50048]	Loss: 0.1281
Training Epoch: 68 [34816/50048]	Loss: 0.2033
Training Epoch: 68 [34944/50048]	Loss: 0.1194
Training Epoch: 68 [35072/50048]	Loss: 0.1360
Training Epoch: 68 [35200/50048]	Loss: 0.1527
Training Epoch: 68 [35328/50048]	Loss: 0.2248
Training Epoch: 68 [35456/50048]	Loss: 0.1910
Training Epoch: 68 [35584/50048]	Loss: 0.1929
Training Epoch: 68 [35712/50048]	Loss: 0.1353
Training Epoch: 68 [35840/50048]	Loss: 0.1580
Training Epoch: 68 [35968/50048]	Loss: 0.1777
Training Epoch: 68 [36096/50048]	Loss: 0.1506
Training Epoch: 68 [36224/50048]	Loss: 0.2223
Training Epoch: 68 [36352/50048]	Loss: 0.1245
Training Epoch: 68 [36480/50048]	Loss: 0.1842
Training Epoch: 68 [36608/50048]	Loss: 0.2519
Training Epoch: 68 [36736/50048]	Loss: 0.1097
Training Epoch: 68 [36864/50048]	Loss: 0.1115
Training Epoch: 68 [36992/50048]	Loss: 0.1870
Training Epoch: 68 [37120/50048]	Loss: 0.1935
Training Epoch: 68 [37248/50048]	Loss: 0.1591
Training Epoch: 68 [37376/50048]	Loss: 0.1913
Training Epoch: 68 [37504/50048]	Loss: 0.1249
Training Epoch: 68 [37632/50048]	Loss: 0.2148
Training Epoch: 68 [37760/50048]	Loss: 0.2018
Training Epoch: 68 [37888/50048]	Loss: 0.1232
Training Epoch: 68 [38016/50048]	Loss: 0.1393
Training Epoch: 68 [38144/50048]	Loss: 0.1349
Training Epoch: 68 [38272/50048]	Loss: 0.0885
Training Epoch: 68 [38400/50048]	Loss: 0.1689
Training Epoch: 68 [38528/50048]	Loss: 0.1731
Training Epoch: 68 [38656/50048]	Loss: 0.1314
Training Epoch: 68 [38784/50048]	Loss: 0.1257
Training Epoch: 68 [38912/50048]	Loss: 0.2357
Training Epoch: 68 [39040/50048]	Loss: 0.2352
Training Epoch: 68 [39168/50048]	Loss: 0.1155
Training Epoch: 68 [39296/50048]	Loss: 0.1134
Training Epoch: 68 [39424/50048]	Loss: 0.1481
Training Epoch: 68 [39552/50048]	Loss: 0.2228
Training Epoch: 68 [39680/50048]	Loss: 0.1572
Training Epoch: 68 [39808/50048]	Loss: 0.1497
Training Epoch: 68 [39936/50048]	Loss: 0.3954
Training Epoch: 68 [40064/50048]	Loss: 0.1541
Training Epoch: 68 [40192/50048]	Loss: 0.0838
Training Epoch: 68 [40320/50048]	Loss: 0.1541
Training Epoch: 68 [40448/50048]	Loss: 0.1792
Training Epoch: 68 [40576/50048]	Loss: 0.1337
Training Epoch: 68 [40704/50048]	Loss: 0.1551
Training Epoch: 68 [40832/50048]	Loss: 0.2285
Training Epoch: 68 [40960/50048]	Loss: 0.1358
Training Epoch: 68 [41088/50048]	Loss: 0.2864
Training Epoch: 68 [41216/50048]	Loss: 0.1349
Training Epoch: 68 [41344/50048]	Loss: 0.1758
Training Epoch: 68 [41472/50048]	Loss: 0.0808
Training Epoch: 68 [41600/50048]	Loss: 0.1750
Training Epoch: 68 [41728/50048]	Loss: 0.1482
Training Epoch: 68 [41856/50048]	Loss: 0.1227
Training Epoch: 68 [41984/50048]	Loss: 0.2622
Training Epoch: 68 [42112/50048]	Loss: 0.1341
Training Epoch: 68 [42240/50048]	Loss: 0.1626
Training Epoch: 68 [42368/50048]	Loss: 0.2296
Training Epoch: 68 [42496/50048]	Loss: 0.1064
Training Epoch: 68 [42624/50048]	Loss: 0.1249
Training Epoch: 68 [42752/50048]	Loss: 0.1802
Training Epoch: 68 [42880/50048]	Loss: 0.1677
Training Epoch: 68 [43008/50048]	Loss: 0.1445
Training Epoch: 68 [43136/50048]	Loss: 0.2223
Training Epoch: 68 [43264/50048]	Loss: 0.2080
Training Epoch: 68 [43392/50048]	Loss: 0.2074
Training Epoch: 68 [43520/50048]	Loss: 0.0995
Training Epoch: 68 [43648/50048]	Loss: 0.1382
Training Epoch: 68 [43776/50048]	Loss: 0.1822
Training Epoch: 68 [43904/50048]	Loss: 0.1833
Training Epoch: 68 [44032/50048]	Loss: 0.1232
Training Epoch: 68 [44160/50048]	Loss: 0.0890
Training Epoch: 68 [44288/50048]	Loss: 0.1625
Training Epoch: 68 [44416/50048]	Loss: 0.1459
Training Epoch: 68 [44544/50048]	Loss: 0.0998
Training Epoch: 68 [44672/50048]	Loss: 0.2030
Training Epoch: 68 [44800/50048]	Loss: 0.1347
Training Epoch: 68 [44928/50048]	Loss: 0.1269
Training Epoch: 68 [45056/50048]	Loss: 0.1740
Training Epoch: 68 [45184/50048]	Loss: 0.1074
Training Epoch: 68 [45312/50048]	Loss: 0.2252
Training Epoch: 68 [45440/50048]	Loss: 0.2005
Training Epoch: 68 [45568/50048]	Loss: 0.1797
Training Epoch: 68 [45696/50048]	Loss: 0.1538
2022-12-06 05:21:06,010 [ZeusDataLoader(train)] train epoch 69 done: time=86.45 energy=10497.56
2022-12-06 05:21:06,011 [ZeusDataLoader(eval)] Epoch 69 begin.
Training Epoch: 68 [45824/50048]	Loss: 0.2092
Training Epoch: 68 [45952/50048]	Loss: 0.1020
Training Epoch: 68 [46080/50048]	Loss: 0.1538
Training Epoch: 68 [46208/50048]	Loss: 0.1762
Training Epoch: 68 [46336/50048]	Loss: 0.1917
Training Epoch: 68 [46464/50048]	Loss: 0.1134
Training Epoch: 68 [46592/50048]	Loss: 0.1574
Training Epoch: 68 [46720/50048]	Loss: 0.1128
Training Epoch: 68 [46848/50048]	Loss: 0.1051
Training Epoch: 68 [46976/50048]	Loss: 0.0920
Training Epoch: 68 [47104/50048]	Loss: 0.1164
Training Epoch: 68 [47232/50048]	Loss: 0.1850
Training Epoch: 68 [47360/50048]	Loss: 0.1002
Training Epoch: 68 [47488/50048]	Loss: 0.1145
Training Epoch: 68 [47616/50048]	Loss: 0.2128
Training Epoch: 68 [47744/50048]	Loss: 0.1134
Training Epoch: 68 [47872/50048]	Loss: 0.1659
Training Epoch: 68 [48000/50048]	Loss: 0.1598
Training Epoch: 68 [48128/50048]	Loss: 0.1788
Training Epoch: 68 [48256/50048]	Loss: 0.1517
Training Epoch: 68 [48384/50048]	Loss: 0.1080
Training Epoch: 68 [48512/50048]	Loss: 0.1487
Training Epoch: 68 [48640/50048]	Loss: 0.1774
Training Epoch: 68 [48768/50048]	Loss: 0.1406
Training Epoch: 68 [48896/50048]	Loss: 0.1194
Training Epoch: 68 [49024/50048]	Loss: 0.2216
Training Epoch: 68 [49152/50048]	Loss: 0.1100
Training Epoch: 68 [49280/50048]	Loss: 0.1772
Training Epoch: 68 [49408/50048]	Loss: 0.2634
Training Epoch: 68 [49536/50048]	Loss: 0.1602
Training Epoch: 68 [49664/50048]	Loss: 0.1376
Training Epoch: 68 [49792/50048]	Loss: 0.1720
Training Epoch: 68 [49920/50048]	Loss: 0.1581
Training Epoch: 68 [50048/50048]	Loss: 0.1134
2022-12-06 10:21:09.664 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:21:09,689 [ZeusDataLoader(eval)] eval epoch 69 done: time=3.67 energy=441.06
2022-12-06 05:21:09,689 [ZeusDataLoader(train)] Up to epoch 69: time=6225.96, energy=755757.71, cost=922649.94
2022-12-06 05:21:09,689 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:21:09,689 [ZeusDataLoader(train)] Expected next epoch: time=6315.75, energy=766555.72, cost=935906.32
2022-12-06 05:21:09,690 [ZeusDataLoader(train)] Epoch 70 begin.
Validation Epoch: 68, Average loss: 0.0168, Accuracy: 0.6401
2022-12-06 05:21:09,868 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:21:09,868 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:21:09.870 [ZeusMonitor] Monitor started.
2022-12-06 10:21:09.870 [ZeusMonitor] Running indefinitely. 2022-12-06 10:21:09.870 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:21:09.870 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e70+gpu0.power.log
Training Epoch: 69 [128/50048]	Loss: 0.0703
Training Epoch: 69 [256/50048]	Loss: 0.1579
Training Epoch: 69 [384/50048]	Loss: 0.1703
Training Epoch: 69 [512/50048]	Loss: 0.1612
Training Epoch: 69 [640/50048]	Loss: 0.1118
Training Epoch: 69 [768/50048]	Loss: 0.1249
Training Epoch: 69 [896/50048]	Loss: 0.0905
Training Epoch: 69 [1024/50048]	Loss: 0.1296
Training Epoch: 69 [1152/50048]	Loss: 0.1332
Training Epoch: 69 [1280/50048]	Loss: 0.1289
Training Epoch: 69 [1408/50048]	Loss: 0.0965
Training Epoch: 69 [1536/50048]	Loss: 0.1265
Training Epoch: 69 [1664/50048]	Loss: 0.1846
Training Epoch: 69 [1792/50048]	Loss: 0.2025
Training Epoch: 69 [1920/50048]	Loss: 0.0923
Training Epoch: 69 [2048/50048]	Loss: 0.1495
Training Epoch: 69 [2176/50048]	Loss: 0.1581
Training Epoch: 69 [2304/50048]	Loss: 0.1928
Training Epoch: 69 [2432/50048]	Loss: 0.0898
Training Epoch: 69 [2560/50048]	Loss: 0.1696
Training Epoch: 69 [2688/50048]	Loss: 0.0886
Training Epoch: 69 [2816/50048]	Loss: 0.1764
Training Epoch: 69 [2944/50048]	Loss: 0.1402
Training Epoch: 69 [3072/50048]	Loss: 0.1692
Training Epoch: 69 [3200/50048]	Loss: 0.2465
Training Epoch: 69 [3328/50048]	Loss: 0.1164
Training Epoch: 69 [3456/50048]	Loss: 0.1371
Training Epoch: 69 [3584/50048]	Loss: 0.0826
Training Epoch: 69 [3712/50048]	Loss: 0.1608
Training Epoch: 69 [3840/50048]	Loss: 0.1530
Training Epoch: 69 [3968/50048]	Loss: 0.0846
Training Epoch: 69 [4096/50048]	Loss: 0.1251
Training Epoch: 69 [4224/50048]	Loss: 0.1220
Training Epoch: 69 [4352/50048]	Loss: 0.1812
Training Epoch: 69 [4480/50048]	Loss: 0.1488
Training Epoch: 69 [4608/50048]	Loss: 0.1564
Training Epoch: 69 [4736/50048]	Loss: 0.1028
Training Epoch: 69 [4864/50048]	Loss: 0.1402
Training Epoch: 69 [4992/50048]	Loss: 0.1138
Training Epoch: 69 [5120/50048]	Loss: 0.1073
Training Epoch: 69 [5248/50048]	Loss: 0.1124
Training Epoch: 69 [5376/50048]	Loss: 0.0809
Training Epoch: 69 [5504/50048]	Loss: 0.0574
Training Epoch: 69 [5632/50048]	Loss: 0.1800
Training Epoch: 69 [5760/50048]	Loss: 0.1548
Training Epoch: 69 [5888/50048]	Loss: 0.1119
Training Epoch: 69 [6016/50048]	Loss: 0.1306
Training Epoch: 69 [6144/50048]	Loss: 0.0735
Training Epoch: 69 [6272/50048]	Loss: 0.1538
Training Epoch: 69 [6400/50048]	Loss: 0.1759
Training Epoch: 69 [6528/50048]	Loss: 0.1796
Training Epoch: 69 [6656/50048]	Loss: 0.1267
Training Epoch: 69 [6784/50048]	Loss: 0.1098
Training Epoch: 69 [6912/50048]	Loss: 0.1797
Training Epoch: 69 [7040/50048]	Loss: 0.1674
Training Epoch: 69 [7168/50048]	Loss: 0.1230
Training Epoch: 69 [7296/50048]	Loss: 0.1525
Training Epoch: 69 [7424/50048]	Loss: 0.0930
Training Epoch: 69 [7552/50048]	Loss: 0.1051
Training Epoch: 69 [7680/50048]	Loss: 0.1281
Training Epoch: 69 [7808/50048]	Loss: 0.1103
Training Epoch: 69 [7936/50048]	Loss: 0.1881
Training Epoch: 69 [8064/50048]	Loss: 0.1826
Training Epoch: 69 [8192/50048]	Loss: 0.0990
Training Epoch: 69 [8320/50048]	Loss: 0.1289
Training Epoch: 69 [8448/50048]	Loss: 0.1518
Training Epoch: 69 [8576/50048]	Loss: 0.1147
Training Epoch: 69 [8704/50048]	Loss: 0.1566
Training Epoch: 69 [8832/50048]	Loss: 0.0795
Training Epoch: 69 [8960/50048]	Loss: 0.1554
Training Epoch: 69 [9088/50048]	Loss: 0.0858
Training Epoch: 69 [9216/50048]	Loss: 0.1080
Training Epoch: 69 [9344/50048]	Loss: 0.0559
Training Epoch: 69 [9472/50048]	Loss: 0.1986
Training Epoch: 69 [9600/50048]	Loss: 0.1813
Training Epoch: 69 [9728/50048]	Loss: 0.0995
Training Epoch: 69 [9856/50048]	Loss: 0.1196
Training Epoch: 69 [9984/50048]	Loss: 0.1277
Training Epoch: 69 [10112/50048]	Loss: 0.1459
Training Epoch: 69 [10240/50048]	Loss: 0.1351
Training Epoch: 69 [10368/50048]	Loss: 0.1243
Training Epoch: 69 [10496/50048]	Loss: 0.0908
Training Epoch: 69 [10624/50048]	Loss: 0.1103
Training Epoch: 69 [10752/50048]	Loss: 0.1546
Training Epoch: 69 [10880/50048]	Loss: 0.1573
Training Epoch: 69 [11008/50048]	Loss: 0.1390
Training Epoch: 69 [11136/50048]	Loss: 0.1546
Training Epoch: 69 [11264/50048]	Loss: 0.1970
Training Epoch: 69 [11392/50048]	Loss: 0.1436
Training Epoch: 69 [11520/50048]	Loss: 0.1359
Training Epoch: 69 [11648/50048]	Loss: 0.0961
Training Epoch: 69 [11776/50048]	Loss: 0.1225
Training Epoch: 69 [11904/50048]	Loss: 0.1678
Training Epoch: 69 [12032/50048]	Loss: 0.1473
Training Epoch: 69 [12160/50048]	Loss: 0.2297
Training Epoch: 69 [12288/50048]	Loss: 0.1767
Training Epoch: 69 [12416/50048]	Loss: 0.0842
Training Epoch: 69 [12544/50048]	Loss: 0.1985
Training Epoch: 69 [12672/50048]	Loss: 0.1514
Training Epoch: 69 [12800/50048]	Loss: 0.1390
Training Epoch: 69 [12928/50048]	Loss: 0.0998
Training Epoch: 69 [13056/50048]	Loss: 0.1264
Training Epoch: 69 [13184/50048]	Loss: 0.1302
Training Epoch: 69 [13312/50048]	Loss: 0.1589
Training Epoch: 69 [13440/50048]	Loss: 0.1265
Training Epoch: 69 [13568/50048]	Loss: 0.1085
Training Epoch: 69 [13696/50048]	Loss: 0.1286
Training Epoch: 69 [13824/50048]	Loss: 0.1529
Training Epoch: 69 [13952/50048]	Loss: 0.1489
Training Epoch: 69 [14080/50048]	Loss: 0.1491
Training Epoch: 69 [14208/50048]	Loss: 0.1592
Training Epoch: 69 [14336/50048]	Loss: 0.1040
Training Epoch: 69 [14464/50048]	Loss: 0.0943
Training Epoch: 69 [14592/50048]	Loss: 0.0754
Training Epoch: 69 [14720/50048]	Loss: 0.1980
Training Epoch: 69 [14848/50048]	Loss: 0.1274
Training Epoch: 69 [14976/50048]	Loss: 0.1248
Training Epoch: 69 [15104/50048]	Loss: 0.1478
Training Epoch: 69 [15232/50048]	Loss: 0.1325
Training Epoch: 69 [15360/50048]	Loss: 0.1893
Training Epoch: 69 [15488/50048]	Loss: 0.0907
Training Epoch: 69 [15616/50048]	Loss: 0.0777
Training Epoch: 69 [15744/50048]	Loss: 0.0846
Training Epoch: 69 [15872/50048]	Loss: 0.2587
Training Epoch: 69 [16000/50048]	Loss: 0.1933
Training Epoch: 69 [16128/50048]	Loss: 0.2034
Training Epoch: 69 [16256/50048]	Loss: 0.1913
Training Epoch: 69 [16384/50048]	Loss: 0.1176
Training Epoch: 69 [16512/50048]	Loss: 0.1200
Training Epoch: 69 [16640/50048]	Loss: 0.0932
Training Epoch: 69 [16768/50048]	Loss: 0.1181
Training Epoch: 69 [16896/50048]	Loss: 0.1891
Training Epoch: 69 [17024/50048]	Loss: 0.2252
Training Epoch: 69 [17152/50048]	Loss: 0.1594
Training Epoch: 69 [17280/50048]	Loss: 0.1486
Training Epoch: 69 [17408/50048]	Loss: 0.1626
Training Epoch: 69 [17536/50048]	Loss: 0.1426
Training Epoch: 69 [17664/50048]	Loss: 0.2128
Training Epoch: 69 [17792/50048]	Loss: 0.1780
Training Epoch: 69 [17920/50048]	Loss: 0.1282
Training Epoch: 69 [18048/50048]	Loss: 0.1383
Training Epoch: 69 [18176/50048]	Loss: 0.1161
Training Epoch: 69 [18304/50048]	Loss: 0.1657
Training Epoch: 69 [18432/50048]	Loss: 0.1304
Training Epoch: 69 [18560/50048]	Loss: 0.2002
Training Epoch: 69 [18688/50048]	Loss: 0.1319
Training Epoch: 69 [18816/50048]	Loss: 0.1509
Training Epoch: 69 [18944/50048]	Loss: 0.0813
Training Epoch: 69 [19072/50048]	Loss: 0.0812
Training Epoch: 69 [19200/50048]	Loss: 0.1060
Training Epoch: 69 [19328/50048]	Loss: 0.1604
Training Epoch: 69 [19456/50048]	Loss: 0.1280
Training Epoch: 69 [19584/50048]	Loss: 0.1271
Training Epoch: 69 [19712/50048]	Loss: 0.1185
Training Epoch: 69 [19840/50048]	Loss: 0.0928
Training Epoch: 69 [19968/50048]	Loss: 0.0637
Training Epoch: 69 [20096/50048]	Loss: 0.1442
Training Epoch: 69 [20224/50048]	Loss: 0.1363
Training Epoch: 69 [20352/50048]	Loss: 0.1258
Training Epoch: 69 [20480/50048]	Loss: 0.2342
Training Epoch: 69 [20608/50048]	Loss: 0.1134
Training Epoch: 69 [20736/50048]	Loss: 0.1686
Training Epoch: 69 [20864/50048]	Loss: 0.0971
Training Epoch: 69 [20992/50048]	Loss: 0.1525
Training Epoch: 69 [21120/50048]	Loss: 0.2446
Training Epoch: 69 [21248/50048]	Loss: 0.1360
Training Epoch: 69 [21376/50048]	Loss: 0.1519
Training Epoch: 69 [21504/50048]	Loss: 0.0874
Training Epoch: 69 [21632/50048]	Loss: 0.1908
Training Epoch: 69 [21760/50048]	Loss: 0.1143
Training Epoch: 69 [21888/50048]	Loss: 0.1038
Training Epoch: 69 [22016/50048]	Loss: 0.1024
Training Epoch: 69 [22144/50048]	Loss: 0.2392
Training Epoch: 69 [22272/50048]	Loss: 0.0593
Training Epoch: 69 [22400/50048]	Loss: 0.0519
Training Epoch: 69 [22528/50048]	Loss: 0.1635
Training Epoch: 69 [22656/50048]	Loss: 0.0867
Training Epoch: 69 [22784/50048]	Loss: 0.1824
Training Epoch: 69 [22912/50048]	Loss: 0.0712
Training Epoch: 69 [23040/50048]	Loss: 0.2058
Training Epoch: 69 [23168/50048]	Loss: 0.0887
Training Epoch: 69 [23296/50048]	Loss: 0.1322
Training Epoch: 69 [23424/50048]	Loss: 0.2407
Training Epoch: 69 [23552/50048]	Loss: 0.1677
Training Epoch: 69 [23680/50048]	Loss: 0.1088
Training Epoch: 69 [23808/50048]	Loss: 0.0905
Training Epoch: 69 [23936/50048]	Loss: 0.1903
Training Epoch: 69 [24064/50048]	Loss: 0.1349
Training Epoch: 69 [24192/50048]	Loss: 0.1840
Training Epoch: 69 [24320/50048]	Loss: 0.1146
Training Epoch: 69 [24448/50048]	Loss: 0.1537
Training Epoch: 69 [24576/50048]	Loss: 0.1371
Training Epoch: 69 [24704/50048]	Loss: 0.1368
Training Epoch: 69 [24832/50048]	Loss: 0.1371
Training Epoch: 69 [24960/50048]	Loss: 0.1292
Training Epoch: 69 [25088/50048]	Loss: 0.1690
Training Epoch: 69 [25216/50048]	Loss: 0.1508
Training Epoch: 69 [25344/50048]	Loss: 0.1208
Training Epoch: 69 [25472/50048]	Loss: 0.1036
Training Epoch: 69 [25600/50048]	Loss: 0.2080
Training Epoch: 69 [25728/50048]	Loss: 0.1770
Training Epoch: 69 [25856/50048]	Loss: 0.2041
Training Epoch: 69 [25984/50048]	Loss: 0.1037
Training Epoch: 69 [26112/50048]	Loss: 0.1305
Training Epoch: 69 [26240/50048]	Loss: 0.0952
Training Epoch: 69 [26368/50048]	Loss: 0.1116
Training Epoch: 69 [26496/50048]	Loss: 0.2030
Training Epoch: 69 [26624/50048]	Loss: 0.2472
Training Epoch: 69 [26752/50048]	Loss: 0.2741
Training Epoch: 69 [26880/50048]	Loss: 0.1529
Training Epoch: 69 [27008/50048]	Loss: 0.1554
Training Epoch: 69 [27136/50048]	Loss: 0.1349
Training Epoch: 69 [27264/50048]	Loss: 0.1434
Training Epoch: 69 [27392/50048]	Loss: 0.1576
Training Epoch: 69 [27520/50048]	Loss: 0.1223
Training Epoch: 69 [27648/50048]	Loss: 0.1111
Training Epoch: 69 [27776/50048]	Loss: 0.1073
Training Epoch: 69 [27904/50048]	Loss: 0.1077
Training Epoch: 69 [28032/50048]	Loss: 0.1100
Training Epoch: 69 [28160/50048]	Loss: 0.1991
Training Epoch: 69 [28288/50048]	Loss: 0.2268
Training Epoch: 69 [28416/50048]	Loss: 0.1110
Training Epoch: 69 [28544/50048]	Loss: 0.1789
Training Epoch: 69 [28672/50048]	Loss: 0.0902
Training Epoch: 69 [28800/50048]	Loss: 0.0521
Training Epoch: 69 [28928/50048]	Loss: 0.1599
Training Epoch: 69 [29056/50048]	Loss: 0.1406
Training Epoch: 69 [29184/50048]	Loss: 0.0628
Training Epoch: 69 [29312/50048]	Loss: 0.1331
Training Epoch: 69 [29440/50048]	Loss: 0.1224
Training Epoch: 69 [29568/50048]	Loss: 0.1899
Training Epoch: 69 [29696/50048]	Loss: 0.2714
Training Epoch: 69 [29824/50048]	Loss: 0.1902
Training Epoch: 69 [29952/50048]	Loss: 0.1516
Training Epoch: 69 [30080/50048]	Loss: 0.1335
Training Epoch: 69 [30208/50048]	Loss: 0.1577
Training Epoch: 69 [30336/50048]	Loss: 0.1603
Training Epoch: 69 [30464/50048]	Loss: 0.1949
Training Epoch: 69 [30592/50048]	Loss: 0.1559
Training Epoch: 69 [30720/50048]	Loss: 0.1604
Training Epoch: 69 [30848/50048]	Loss: 0.1792
Training Epoch: 69 [30976/50048]	Loss: 0.2253
Training Epoch: 69 [31104/50048]	Loss: 0.1645
Training Epoch: 69 [31232/50048]	Loss: 0.1518
Training Epoch: 69 [31360/50048]	Loss: 0.1086
Training Epoch: 69 [31488/50048]	Loss: 0.1074
Training Epoch: 69 [31616/50048]	Loss: 0.2180
Training Epoch: 69 [31744/50048]	Loss: 0.1498
Training Epoch: 69 [31872/50048]	Loss: 0.1463
Training Epoch: 69 [32000/50048]	Loss: 0.0974
Training Epoch: 69 [32128/50048]	Loss: 0.1173
Training Epoch: 69 [32256/50048]	Loss: 0.1105
Training Epoch: 69 [32384/50048]	Loss: 0.1412
Training Epoch: 69 [32512/50048]	Loss: 0.1161
Training Epoch: 69 [32640/50048]	Loss: 0.0879
Training Epoch: 69 [32768/50048]	Loss: 0.1100
Training Epoch: 69 [32896/50048]	Loss: 0.2025
Training Epoch: 69 [33024/50048]	Loss: 0.1126
Training Epoch: 69 [33152/50048]	Loss: 0.1611
Training Epoch: 69 [33280/50048]	Loss: 0.1003
Training Epoch: 69 [33408/50048]	Loss: 0.0667
Training Epoch: 69 [33536/50048]	Loss: 0.1288
Training Epoch: 69 [33664/50048]	Loss: 0.1514
Training Epoch: 69 [33792/50048]	Loss: 0.1607
Training Epoch: 69 [33920/50048]	Loss: 0.0754
Training Epoch: 69 [34048/50048]	Loss: 0.1152
Training Epoch: 69 [34176/50048]	Loss: 0.1928
Training Epoch: 69 [34304/50048]	Loss: 0.1523
Training Epoch: 69 [34432/50048]	Loss: 0.2011
Training Epoch: 69 [34560/50048]	Loss: 0.0977
Training Epoch: 69 [34688/50048]	Loss: 0.1745
Training Epoch: 69 [34816/50048]	Loss: 0.1363
Training Epoch: 69 [34944/50048]	Loss: 0.1204
Training Epoch: 69 [35072/50048]	Loss: 0.1608
Training Epoch: 69 [35200/50048]	Loss: 0.1525
Training Epoch: 69 [35328/50048]	Loss: 0.1227
Training Epoch: 69 [35456/50048]	Loss: 0.1812
Training Epoch: 69 [35584/50048]	Loss: 0.2066
Training Epoch: 69 [35712/50048]	Loss: 0.1461
Training Epoch: 69 [35840/50048]	Loss: 0.1372
Training Epoch: 69 [35968/50048]	Loss: 0.1678
Training Epoch: 69 [36096/50048]	Loss: 0.1557
Training Epoch: 69 [36224/50048]	Loss: 0.1560
Training Epoch: 69 [36352/50048]	Loss: 0.1223
Training Epoch: 69 [36480/50048]	Loss: 0.1892
Training Epoch: 69 [36608/50048]	Loss: 0.1241
Training Epoch: 69 [36736/50048]	Loss: 0.1945
Training Epoch: 69 [36864/50048]	Loss: 0.2829
Training Epoch: 69 [36992/50048]	Loss: 0.1686
Training Epoch: 69 [37120/50048]	Loss: 0.1207
Training Epoch: 69 [37248/50048]	Loss: 0.1208
Training Epoch: 69 [37376/50048]	Loss: 0.1229
Training Epoch: 69 [37504/50048]	Loss: 0.0988
Training Epoch: 69 [37632/50048]	Loss: 0.1653
Training Epoch: 69 [37760/50048]	Loss: 0.0736
Training Epoch: 69 [37888/50048]	Loss: 0.1384
Training Epoch: 69 [38016/50048]	Loss: 0.2306
Training Epoch: 69 [38144/50048]	Loss: 0.1886
Training Epoch: 69 [38272/50048]	Loss: 0.2026
Training Epoch: 69 [38400/50048]	Loss: 0.1161
Training Epoch: 69 [38528/50048]	Loss: 0.1319
Training Epoch: 69 [38656/50048]	Loss: 0.1157
Training Epoch: 69 [38784/50048]	Loss: 0.2084
Training Epoch: 69 [38912/50048]	Loss: 0.2167
Training Epoch: 69 [39040/50048]	Loss: 0.2479
Training Epoch: 69 [39168/50048]	Loss: 0.0950
Training Epoch: 69 [39296/50048]	Loss: 0.1520
Training Epoch: 69 [39424/50048]	Loss: 0.1137
Training Epoch: 69 [39552/50048]	Loss: 0.1277
Training Epoch: 69 [39680/50048]	Loss: 0.2645
Training Epoch: 69 [39808/50048]	Loss: 0.1862
Training Epoch: 69 [39936/50048]	Loss: 0.1892
Training Epoch: 69 [40064/50048]	Loss: 0.2443
Training Epoch: 69 [40192/50048]	Loss: 0.1553
Training Epoch: 69 [40320/50048]	Loss: 0.1174
Training Epoch: 69 [40448/50048]	Loss: 0.1273
Training Epoch: 69 [40576/50048]	Loss: 0.1599
Training Epoch: 69 [40704/50048]	Loss: 0.1179
Training Epoch: 69 [40832/50048]	Loss: 0.1255
Training Epoch: 69 [40960/50048]	Loss: 0.1953
Training Epoch: 69 [41088/50048]	Loss: 0.2217
Training Epoch: 69 [41216/50048]	Loss: 0.1164
Training Epoch: 69 [41344/50048]	Loss: 0.1758
Training Epoch: 69 [41472/50048]	Loss: 0.2299
Training Epoch: 69 [41600/50048]	Loss: 0.1318
Training Epoch: 69 [41728/50048]	Loss: 0.1853
Training Epoch: 69 [41856/50048]	Loss: 0.2064
Training Epoch: 69 [41984/50048]	Loss: 0.2823
Training Epoch: 69 [42112/50048]	Loss: 0.2038
Training Epoch: 69 [42240/50048]	Loss: 0.2536
Training Epoch: 69 [42368/50048]	Loss: 0.1821
Training Epoch: 69 [42496/50048]	Loss: 0.1303
Training Epoch: 69 [42624/50048]	Loss: 0.1211
Training Epoch: 69 [42752/50048]	Loss: 0.2183
Training Epoch: 69 [42880/50048]	Loss: 0.2339
Training Epoch: 69 [43008/50048]	Loss: 0.1339
Training Epoch: 69 [43136/50048]	Loss: 0.1793
Training Epoch: 69 [43264/50048]	Loss: 0.2789
Training Epoch: 69 [43392/50048]	Loss: 0.1124
Training Epoch: 69 [43520/50048]	Loss: 0.2251
Training Epoch: 69 [43648/50048]	Loss: 0.1708
Training Epoch: 69 [43776/50048]	Loss: 0.1588
Training Epoch: 69 [43904/50048]	Loss: 0.1577
Training Epoch: 69 [44032/50048]	Loss: 0.1528
Training Epoch: 69 [44160/50048]	Loss: 0.1320
Training Epoch: 69 [44288/50048]	Loss: 0.1559
Training Epoch: 69 [44416/50048]	Loss: 0.2234
Training Epoch: 69 [44544/50048]	Loss: 0.1555
Training Epoch: 69 [44672/50048]	Loss: 0.1311
Training Epoch: 69 [44800/50048]	Loss: 0.1346
Training Epoch: 69 [44928/50048]	Loss: 0.1145
Training Epoch: 69 [45056/50048]	Loss: 0.1729
Training Epoch: 69 [45184/50048]	Loss: 0.1515
Training Epoch: 69 [45312/50048]	Loss: 0.1765
Training Epoch: 69 [45440/50048]	Loss: 0.1778
Training Epoch: 69 [45568/50048]	Loss: 0.1810
Training Epoch: 69 [45696/50048]	Loss: 0.1122
2022-12-06 05:22:36,127 [ZeusDataLoader(train)] train epoch 70 done: time=86.43 energy=10497.21
2022-12-06 05:22:36,128 [ZeusDataLoader(eval)] Epoch 70 begin.
Training Epoch: 69 [45824/50048]	Loss: 0.1666
Training Epoch: 69 [45952/50048]	Loss: 0.3470
Training Epoch: 69 [46080/50048]	Loss: 0.1621
Training Epoch: 69 [46208/50048]	Loss: 0.0956
Training Epoch: 69 [46336/50048]	Loss: 0.1846
Training Epoch: 69 [46464/50048]	Loss: 0.1140
Training Epoch: 69 [46592/50048]	Loss: 0.2086
Training Epoch: 69 [46720/50048]	Loss: 0.2812
Training Epoch: 69 [46848/50048]	Loss: 0.1255
Training Epoch: 69 [46976/50048]	Loss: 0.1771
Training Epoch: 69 [47104/50048]	Loss: 0.0726
Training Epoch: 69 [47232/50048]	Loss: 0.2337
Training Epoch: 69 [47360/50048]	Loss: 0.1684
Training Epoch: 69 [47488/50048]	Loss: 0.1653
Training Epoch: 69 [47616/50048]	Loss: 0.1659
Training Epoch: 69 [47744/50048]	Loss: 0.1803
Training Epoch: 69 [47872/50048]	Loss: 0.3510
Training Epoch: 69 [48000/50048]	Loss: 0.1598
Training Epoch: 69 [48128/50048]	Loss: 0.1774
Training Epoch: 69 [48256/50048]	Loss: 0.1448
Training Epoch: 69 [48384/50048]	Loss: 0.1498
Training Epoch: 69 [48512/50048]	Loss: 0.2365
Training Epoch: 69 [48640/50048]	Loss: 0.0950
Training Epoch: 69 [48768/50048]	Loss: 0.1176
Training Epoch: 69 [48896/50048]	Loss: 0.2712
Training Epoch: 69 [49024/50048]	Loss: 0.2160
Training Epoch: 69 [49152/50048]	Loss: 0.1993
Training Epoch: 69 [49280/50048]	Loss: 0.1391
Training Epoch: 69 [49408/50048]	Loss: 0.1517
Training Epoch: 69 [49536/50048]	Loss: 0.0931
Training Epoch: 69 [49664/50048]	Loss: 0.1745
Training Epoch: 69 [49792/50048]	Loss: 0.1687
Training Epoch: 69 [49920/50048]	Loss: 0.1116
Training Epoch: 69 [50048/50048]	Loss: 0.1834
2022-12-06 10:22:39.815 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:22:39,836 [ZeusDataLoader(eval)] eval epoch 70 done: time=3.70 energy=452.36
2022-12-06 05:22:39,836 [ZeusDataLoader(train)] Up to epoch 70: time=6316.08, energy=766707.28, cost=936010.70
2022-12-06 05:22:39,837 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:22:39,837 [ZeusDataLoader(train)] Expected next epoch: time=6405.88, energy=777505.29, cost=949267.08
2022-12-06 05:22:39,838 [ZeusDataLoader(train)] Epoch 71 begin.
Validation Epoch: 69, Average loss: 0.0171, Accuracy: 0.6381
2022-12-06 05:22:40,016 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:22:40,017 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:22:40.018 [ZeusMonitor] Monitor started.
2022-12-06 10:22:40.018 [ZeusMonitor] Running indefinitely. 2022-12-06 10:22:40.019 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:22:40.019 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e71+gpu0.power.log
Training Epoch: 70 [128/50048]	Loss: 0.2302
Training Epoch: 70 [256/50048]	Loss: 0.1173
Training Epoch: 70 [384/50048]	Loss: 0.1982
Training Epoch: 70 [512/50048]	Loss: 0.2013
Training Epoch: 70 [640/50048]	Loss: 0.1616
Training Epoch: 70 [768/50048]	Loss: 0.0601
Training Epoch: 70 [896/50048]	Loss: 0.2116
Training Epoch: 70 [1024/50048]	Loss: 0.0995
Training Epoch: 70 [1152/50048]	Loss: 0.1299
Training Epoch: 70 [1280/50048]	Loss: 0.1610
Training Epoch: 70 [1408/50048]	Loss: 0.1708
Training Epoch: 70 [1536/50048]	Loss: 0.1463
Training Epoch: 70 [1664/50048]	Loss: 0.0765
Training Epoch: 70 [1792/50048]	Loss: 0.1510
Training Epoch: 70 [1920/50048]	Loss: 0.2128
Training Epoch: 70 [2048/50048]	Loss: 0.1136
Training Epoch: 70 [2176/50048]	Loss: 0.1107
Training Epoch: 70 [2304/50048]	Loss: 0.0793
Training Epoch: 70 [2432/50048]	Loss: 0.1624
Training Epoch: 70 [2560/50048]	Loss: 0.0890
Training Epoch: 70 [2688/50048]	Loss: 0.1079
Training Epoch: 70 [2816/50048]	Loss: 0.1012
Training Epoch: 70 [2944/50048]	Loss: 0.1766
Training Epoch: 70 [3072/50048]	Loss: 0.1584
Training Epoch: 70 [3200/50048]	Loss: 0.1901
Training Epoch: 70 [3328/50048]	Loss: 0.1222
Training Epoch: 70 [3456/50048]	Loss: 0.1450
Training Epoch: 70 [3584/50048]	Loss: 0.1348
Training Epoch: 70 [3712/50048]	Loss: 0.1056
Training Epoch: 70 [3840/50048]	Loss: 0.1295
Training Epoch: 70 [3968/50048]	Loss: 0.0900
Training Epoch: 70 [4096/50048]	Loss: 0.0942
Training Epoch: 70 [4224/50048]	Loss: 0.1829
Training Epoch: 70 [4352/50048]	Loss: 0.1470
Training Epoch: 70 [4480/50048]	Loss: 0.3398
Training Epoch: 70 [4608/50048]	Loss: 0.1807
Training Epoch: 70 [4736/50048]	Loss: 0.1879
Training Epoch: 70 [4864/50048]	Loss: 0.1724
Training Epoch: 70 [4992/50048]	Loss: 0.0814
Training Epoch: 70 [5120/50048]	Loss: 0.1354
Training Epoch: 70 [5248/50048]	Loss: 0.1118
Training Epoch: 70 [5376/50048]	Loss: 0.1275
Training Epoch: 70 [5504/50048]	Loss: 0.2325
Training Epoch: 70 [5632/50048]	Loss: 0.1570
Training Epoch: 70 [5760/50048]	Loss: 0.0871
Training Epoch: 70 [5888/50048]	Loss: 0.0827
Training Epoch: 70 [6016/50048]	Loss: 0.1506
Training Epoch: 70 [6144/50048]	Loss: 0.1471
Training Epoch: 70 [6272/50048]	Loss: 0.1221
Training Epoch: 70 [6400/50048]	Loss: 0.1109
Training Epoch: 70 [6528/50048]	Loss: 0.0689
Training Epoch: 70 [6656/50048]	Loss: 0.1073
Training Epoch: 70 [6784/50048]	Loss: 0.0856
Training Epoch: 70 [6912/50048]	Loss: 0.2449
Training Epoch: 70 [7040/50048]	Loss: 0.0548
Training Epoch: 70 [7168/50048]	Loss: 0.1404
Training Epoch: 70 [7296/50048]	Loss: 0.0753
Training Epoch: 70 [7424/50048]	Loss: 0.2208
Training Epoch: 70 [7552/50048]	Loss: 0.1389
Training Epoch: 70 [7680/50048]	Loss: 0.0797
Training Epoch: 70 [7808/50048]	Loss: 0.0895
Training Epoch: 70 [7936/50048]	Loss: 0.1653
Training Epoch: 70 [8064/50048]	Loss: 0.1025
Training Epoch: 70 [8192/50048]	Loss: 0.1130
Training Epoch: 70 [8320/50048]	Loss: 0.1803
Training Epoch: 70 [8448/50048]	Loss: 0.1170
Training Epoch: 70 [8576/50048]	Loss: 0.0941
Training Epoch: 70 [8704/50048]	Loss: 0.1091
Training Epoch: 70 [8832/50048]	Loss: 0.1174
Training Epoch: 70 [8960/50048]	Loss: 0.1677
Training Epoch: 70 [9088/50048]	Loss: 0.1219
Training Epoch: 70 [9216/50048]	Loss: 0.1295
Training Epoch: 70 [9344/50048]	Loss: 0.0752
Training Epoch: 70 [9472/50048]	Loss: 0.1536
Training Epoch: 70 [9600/50048]	Loss: 0.1136
Training Epoch: 70 [9728/50048]	Loss: 0.1833
Training Epoch: 70 [9856/50048]	Loss: 0.2060
Training Epoch: 70 [9984/50048]	Loss: 0.1595
Training Epoch: 70 [10112/50048]	Loss: 0.1218
Training Epoch: 70 [10240/50048]	Loss: 0.1120
Training Epoch: 70 [10368/50048]	Loss: 0.0836
Training Epoch: 70 [10496/50048]	Loss: 0.1910
Training Epoch: 70 [10624/50048]	Loss: 0.0913
Training Epoch: 70 [10752/50048]	Loss: 0.1947
Training Epoch: 70 [10880/50048]	Loss: 0.1598
Training Epoch: 70 [11008/50048]	Loss: 0.1668
Training Epoch: 70 [11136/50048]	Loss: 0.1017
Training Epoch: 70 [11264/50048]	Loss: 0.0862
Training Epoch: 70 [11392/50048]	Loss: 0.0967
Training Epoch: 70 [11520/50048]	Loss: 0.1513
Training Epoch: 70 [11648/50048]	Loss: 0.1778
Training Epoch: 70 [11776/50048]	Loss: 0.1969
Training Epoch: 70 [11904/50048]	Loss: 0.1381
Training Epoch: 70 [12032/50048]	Loss: 0.1062
Training Epoch: 70 [12160/50048]	Loss: 0.1297
Training Epoch: 70 [12288/50048]	Loss: 0.1038
Training Epoch: 70 [12416/50048]	Loss: 0.0937
Training Epoch: 70 [12544/50048]	Loss: 0.1558
Training Epoch: 70 [12672/50048]	Loss: 0.1166
Training Epoch: 70 [12800/50048]	Loss: 0.1793
Training Epoch: 70 [12928/50048]	Loss: 0.1629
Training Epoch: 70 [13056/50048]	Loss: 0.0870
Training Epoch: 70 [13184/50048]	Loss: 0.2688
Training Epoch: 70 [13312/50048]	Loss: 0.1250
Training Epoch: 70 [13440/50048]	Loss: 0.0900
Training Epoch: 70 [13568/50048]	Loss: 0.1172
Training Epoch: 70 [13696/50048]	Loss: 0.1138
Training Epoch: 70 [13824/50048]	Loss: 0.0803
Training Epoch: 70 [13952/50048]	Loss: 0.0835
Training Epoch: 70 [14080/50048]	Loss: 0.1332
Training Epoch: 70 [14208/50048]	Loss: 0.1244
Training Epoch: 70 [14336/50048]	Loss: 0.1578
Training Epoch: 70 [14464/50048]	Loss: 0.1351
Training Epoch: 70 [14592/50048]	Loss: 0.1365
Training Epoch: 70 [14720/50048]	Loss: 0.1186
Training Epoch: 70 [14848/50048]	Loss: 0.0986
Training Epoch: 70 [14976/50048]	Loss: 0.0916
Training Epoch: 70 [15104/50048]	Loss: 0.0846
Training Epoch: 70 [15232/50048]	Loss: 0.1703
Training Epoch: 70 [15360/50048]	Loss: 0.1377
Training Epoch: 70 [15488/50048]	Loss: 0.0912
Training Epoch: 70 [15616/50048]	Loss: 0.1718
Training Epoch: 70 [15744/50048]	Loss: 0.1170
Training Epoch: 70 [15872/50048]	Loss: 0.0802
Training Epoch: 70 [16000/50048]	Loss: 0.0936
Training Epoch: 70 [16128/50048]	Loss: 0.0996
Training Epoch: 70 [16256/50048]	Loss: 0.1212
Training Epoch: 70 [16384/50048]	Loss: 0.1311
Training Epoch: 70 [16512/50048]	Loss: 0.0654
Training Epoch: 70 [16640/50048]	Loss: 0.1651
Training Epoch: 70 [16768/50048]	Loss: 0.1342
Training Epoch: 70 [16896/50048]	Loss: 0.1411
Training Epoch: 70 [17024/50048]	Loss: 0.1540
Training Epoch: 70 [17152/50048]	Loss: 0.1042
Training Epoch: 70 [17280/50048]	Loss: 0.1030
Training Epoch: 70 [17408/50048]	Loss: 0.1616
Training Epoch: 70 [17536/50048]	Loss: 0.1522
Training Epoch: 70 [17664/50048]	Loss: 0.1161
Training Epoch: 70 [17792/50048]	Loss: 0.2260
Training Epoch: 70 [17920/50048]	Loss: 0.1581
Training Epoch: 70 [18048/50048]	Loss: 0.0975
Training Epoch: 70 [18176/50048]	Loss: 0.1587
Training Epoch: 70 [18304/50048]	Loss: 0.1220
Training Epoch: 70 [18432/50048]	Loss: 0.0991
Training Epoch: 70 [18560/50048]	Loss: 0.0884
Training Epoch: 70 [18688/50048]	Loss: 0.2125
Training Epoch: 70 [18816/50048]	Loss: 0.0960
Training Epoch: 70 [18944/50048]	Loss: 0.1422
Training Epoch: 70 [19072/50048]	Loss: 0.1898
Training Epoch: 70 [19200/50048]	Loss: 0.1056
Training Epoch: 70 [19328/50048]	Loss: 0.0928
Training Epoch: 70 [19456/50048]	Loss: 0.0931
Training Epoch: 70 [19584/50048]	Loss: 0.1350
Training Epoch: 70 [19712/50048]	Loss: 0.0602
Training Epoch: 70 [19840/50048]	Loss: 0.1184
Training Epoch: 70 [19968/50048]	Loss: 0.1296
Training Epoch: 70 [20096/50048]	Loss: 0.0902
Training Epoch: 70 [20224/50048]	Loss: 0.1218
Training Epoch: 70 [20352/50048]	Loss: 0.1510
Training Epoch: 70 [20480/50048]	Loss: 0.0652
Training Epoch: 70 [20608/50048]	Loss: 0.1406
Training Epoch: 70 [20736/50048]	Loss: 0.0820
Training Epoch: 70 [20864/50048]	Loss: 0.1729
Training Epoch: 70 [20992/50048]	Loss: 0.1699
Training Epoch: 70 [21120/50048]	Loss: 0.1229
Training Epoch: 70 [21248/50048]	Loss: 0.0807
Training Epoch: 70 [21376/50048]	Loss: 0.1235
Training Epoch: 70 [21504/50048]	Loss: 0.0975
Training Epoch: 70 [21632/50048]	Loss: 0.1018
Training Epoch: 70 [21760/50048]	Loss: 0.1218
Training Epoch: 70 [21888/50048]	Loss: 0.0600
Training Epoch: 70 [22016/50048]	Loss: 0.1629
Training Epoch: 70 [22144/50048]	Loss: 0.1367
Training Epoch: 70 [22272/50048]	Loss: 0.0800
Training Epoch: 70 [22400/50048]	Loss: 0.1502
Training Epoch: 70 [22528/50048]	Loss: 0.1880
Training Epoch: 70 [22656/50048]	Loss: 0.1462
Training Epoch: 70 [22784/50048]	Loss: 0.1605
Training Epoch: 70 [22912/50048]	Loss: 0.1492
Training Epoch: 70 [23040/50048]	Loss: 0.0681
Training Epoch: 70 [23168/50048]	Loss: 0.1922
Training Epoch: 70 [23296/50048]	Loss: 0.1967
Training Epoch: 70 [23424/50048]	Loss: 0.2824
Training Epoch: 70 [23552/50048]	Loss: 0.2687
Training Epoch: 70 [23680/50048]	Loss: 0.1728
Training Epoch: 70 [23808/50048]	Loss: 0.1069
Training Epoch: 70 [23936/50048]	Loss: 0.2008
Training Epoch: 70 [24064/50048]	Loss: 0.1612
Training Epoch: 70 [24192/50048]	Loss: 0.1328
Training Epoch: 70 [24320/50048]	Loss: 0.1712
Training Epoch: 70 [24448/50048]	Loss: 0.1020
Training Epoch: 70 [24576/50048]	Loss: 0.1489
Training Epoch: 70 [24704/50048]	Loss: 0.1638
Training Epoch: 70 [24832/50048]	Loss: 0.1465
Training Epoch: 70 [24960/50048]	Loss: 0.1909
Training Epoch: 70 [25088/50048]	Loss: 0.1240
Training Epoch: 70 [25216/50048]	Loss: 0.2103
Training Epoch: 70 [25344/50048]	Loss: 0.1477
Training Epoch: 70 [25472/50048]	Loss: 0.1281
Training Epoch: 70 [25600/50048]	Loss: 0.0908
Training Epoch: 70 [25728/50048]	Loss: 0.1607
Training Epoch: 70 [25856/50048]	Loss: 0.1304
Training Epoch: 70 [25984/50048]	Loss: 0.1665
Training Epoch: 70 [26112/50048]	Loss: 0.0685
Training Epoch: 70 [26240/50048]	Loss: 0.1354
Training Epoch: 70 [26368/50048]	Loss: 0.1313
Training Epoch: 70 [26496/50048]	Loss: 0.1709
Training Epoch: 70 [26624/50048]	Loss: 0.1542
Training Epoch: 70 [26752/50048]	Loss: 0.1228
Training Epoch: 70 [26880/50048]	Loss: 0.1586
Training Epoch: 70 [27008/50048]	Loss: 0.1076
Training Epoch: 70 [27136/50048]	Loss: 0.0995
Training Epoch: 70 [27264/50048]	Loss: 0.1760
Training Epoch: 70 [27392/50048]	Loss: 0.1681
Training Epoch: 70 [27520/50048]	Loss: 0.1405
Training Epoch: 70 [27648/50048]	Loss: 0.1169
Training Epoch: 70 [27776/50048]	Loss: 0.1495
Training Epoch: 70 [27904/50048]	Loss: 0.1322
Training Epoch: 70 [28032/50048]	Loss: 0.0830
Training Epoch: 70 [28160/50048]	Loss: 0.0816
Training Epoch: 70 [28288/50048]	Loss: 0.1448
Training Epoch: 70 [28416/50048]	Loss: 0.1138
Training Epoch: 70 [28544/50048]	Loss: 0.1021
Training Epoch: 70 [28672/50048]	Loss: 0.1068
Training Epoch: 70 [28800/50048]	Loss: 0.1490
Training Epoch: 70 [28928/50048]	Loss: 0.1434
Training Epoch: 70 [29056/50048]	Loss: 0.1516
Training Epoch: 70 [29184/50048]	Loss: 0.1083
Training Epoch: 70 [29312/50048]	Loss: 0.1338
Training Epoch: 70 [29440/50048]	Loss: 0.2563
Training Epoch: 70 [29568/50048]	Loss: 0.0854
Training Epoch: 70 [29696/50048]	Loss: 0.1848
Training Epoch: 70 [29824/50048]	Loss: 0.1289
Training Epoch: 70 [29952/50048]	Loss: 0.0893
Training Epoch: 70 [30080/50048]	Loss: 0.0853
Training Epoch: 70 [30208/50048]	Loss: 0.1631
Training Epoch: 70 [30336/50048]	Loss: 0.1021
Training Epoch: 70 [30464/50048]	Loss: 0.1653
Training Epoch: 70 [30592/50048]	Loss: 0.1575
Training Epoch: 70 [30720/50048]	Loss: 0.1613
Training Epoch: 70 [30848/50048]	Loss: 0.1532
Training Epoch: 70 [30976/50048]	Loss: 0.1561
Training Epoch: 70 [31104/50048]	Loss: 0.0634
Training Epoch: 70 [31232/50048]	Loss: 0.1082
Training Epoch: 70 [31360/50048]	Loss: 0.1024
Training Epoch: 70 [31488/50048]	Loss: 0.2099
Training Epoch: 70 [31616/50048]	Loss: 0.1314
Training Epoch: 70 [31744/50048]	Loss: 0.1750
Training Epoch: 70 [31872/50048]	Loss: 0.0909
Training Epoch: 70 [32000/50048]	Loss: 0.0678
Training Epoch: 70 [32128/50048]	Loss: 0.2069
Training Epoch: 70 [32256/50048]	Loss: 0.1135
Training Epoch: 70 [32384/50048]	Loss: 0.1264
Training Epoch: 70 [32512/50048]	Loss: 0.2081
Training Epoch: 70 [32640/50048]	Loss: 0.1260
Training Epoch: 70 [32768/50048]	Loss: 0.1517
Training Epoch: 70 [32896/50048]	Loss: 0.2829
Training Epoch: 70 [33024/50048]	Loss: 0.0970
Training Epoch: 70 [33152/50048]	Loss: 0.1657
Training Epoch: 70 [33280/50048]	Loss: 0.1132
Training Epoch: 70 [33408/50048]	Loss: 0.1265
Training Epoch: 70 [33536/50048]	Loss: 0.1098
Training Epoch: 70 [33664/50048]	Loss: 0.1929
Training Epoch: 70 [33792/50048]	Loss: 0.1895
Training Epoch: 70 [33920/50048]	Loss: 0.2368
Training Epoch: 70 [34048/50048]	Loss: 0.1244
Training Epoch: 70 [34176/50048]	Loss: 0.1965
Training Epoch: 70 [34304/50048]	Loss: 0.1221
Training Epoch: 70 [34432/50048]	Loss: 0.0803
Training Epoch: 70 [34560/50048]	Loss: 0.1154
Training Epoch: 70 [34688/50048]	Loss: 0.0683
Training Epoch: 70 [34816/50048]	Loss: 0.1733
Training Epoch: 70 [34944/50048]	Loss: 0.0932
Training Epoch: 70 [35072/50048]	Loss: 0.1748
Training Epoch: 70 [35200/50048]	Loss: 0.1712
Training Epoch: 70 [35328/50048]	Loss: 0.1711
Training Epoch: 70 [35456/50048]	Loss: 0.1435
Training Epoch: 70 [35584/50048]	Loss: 0.1744
Training Epoch: 70 [35712/50048]	Loss: 0.2523
Training Epoch: 70 [35840/50048]	Loss: 0.1661
Training Epoch: 70 [35968/50048]	Loss: 0.2171
Training Epoch: 70 [36096/50048]	Loss: 0.1756
Training Epoch: 70 [36224/50048]	Loss: 0.1569
Training Epoch: 70 [36352/50048]	Loss: 0.1724
Training Epoch: 70 [36480/50048]	Loss: 0.1508
Training Epoch: 70 [36608/50048]	Loss: 0.2620
Training Epoch: 70 [36736/50048]	Loss: 0.1984
Training Epoch: 70 [36864/50048]	Loss: 0.2150
Training Epoch: 70 [36992/50048]	Loss: 0.2048
Training Epoch: 70 [37120/50048]	Loss: 0.1292
Training Epoch: 70 [37248/50048]	Loss: 0.1193
Training Epoch: 70 [37376/50048]	Loss: 0.1222
Training Epoch: 70 [37504/50048]	Loss: 0.2221
Training Epoch: 70 [37632/50048]	Loss: 0.1380
Training Epoch: 70 [37760/50048]	Loss: 0.1071
Training Epoch: 70 [37888/50048]	Loss: 0.2873
Training Epoch: 70 [38016/50048]	Loss: 0.1804
Training Epoch: 70 [38144/50048]	Loss: 0.2893
Training Epoch: 70 [38272/50048]	Loss: 0.1057
Training Epoch: 70 [38400/50048]	Loss: 0.2169
Training Epoch: 70 [38528/50048]	Loss: 0.1093
Training Epoch: 70 [38656/50048]	Loss: 0.2457
Training Epoch: 70 [38784/50048]	Loss: 0.1637
Training Epoch: 70 [38912/50048]	Loss: 0.2114
Training Epoch: 70 [39040/50048]	Loss: 0.2255
Training Epoch: 70 [39168/50048]	Loss: 0.1555
Training Epoch: 70 [39296/50048]	Loss: 0.2123
Training Epoch: 70 [39424/50048]	Loss: 0.2092
Training Epoch: 70 [39552/50048]	Loss: 0.1777
Training Epoch: 70 [39680/50048]	Loss: 0.1640
Training Epoch: 70 [39808/50048]	Loss: 0.1272
Training Epoch: 70 [39936/50048]	Loss: 0.1552
Training Epoch: 70 [40064/50048]	Loss: 0.1813
Training Epoch: 70 [40192/50048]	Loss: 0.2544
Training Epoch: 70 [40320/50048]	Loss: 0.1503
Training Epoch: 70 [40448/50048]	Loss: 0.1483
Training Epoch: 70 [40576/50048]	Loss: 0.1323
Training Epoch: 70 [40704/50048]	Loss: 0.1540
Training Epoch: 70 [40832/50048]	Loss: 0.2398
Training Epoch: 70 [40960/50048]	Loss: 0.1453
Training Epoch: 70 [41088/50048]	Loss: 0.1494
Training Epoch: 70 [41216/50048]	Loss: 0.2240
Training Epoch: 70 [41344/50048]	Loss: 0.1910
Training Epoch: 70 [41472/50048]	Loss: 0.1895
Training Epoch: 70 [41600/50048]	Loss: 0.1510
Training Epoch: 70 [41728/50048]	Loss: 0.1051
Training Epoch: 70 [41856/50048]	Loss: 0.1823
Training Epoch: 70 [41984/50048]	Loss: 0.1498
Training Epoch: 70 [42112/50048]	Loss: 0.1927
Training Epoch: 70 [42240/50048]	Loss: 0.1502
Training Epoch: 70 [42368/50048]	Loss: 0.1755
Training Epoch: 70 [42496/50048]	Loss: 0.1237
Training Epoch: 70 [42624/50048]	Loss: 0.1645
Training Epoch: 70 [42752/50048]	Loss: 0.1880
Training Epoch: 70 [42880/50048]	Loss: 0.1150
Training Epoch: 70 [43008/50048]	Loss: 0.1787
Training Epoch: 70 [43136/50048]	Loss: 0.2142
Training Epoch: 70 [43264/50048]	Loss: 0.2087
Training Epoch: 70 [43392/50048]	Loss: 0.0853
Training Epoch: 70 [43520/50048]	Loss: 0.2047
Training Epoch: 70 [43648/50048]	Loss: 0.1400
Training Epoch: 70 [43776/50048]	Loss: 0.1614
Training Epoch: 70 [43904/50048]	Loss: 0.0658
Training Epoch: 70 [44032/50048]	Loss: 0.1506
Training Epoch: 70 [44160/50048]	Loss: 0.1809
Training Epoch: 70 [44288/50048]	Loss: 0.1645
Training Epoch: 70 [44416/50048]	Loss: 0.1210
Training Epoch: 70 [44544/50048]	Loss: 0.0879
Training Epoch: 70 [44672/50048]	Loss: 0.1925
Training Epoch: 70 [44800/50048]	Loss: 0.2026
Training Epoch: 70 [44928/50048]	Loss: 0.1413
Training Epoch: 70 [45056/50048]	Loss: 0.2000
Training Epoch: 70 [45184/50048]	Loss: 0.1431
Training Epoch: 70 [45312/50048]	Loss: 0.1106
Training Epoch: 70 [45440/50048]	Loss: 0.1226
Training Epoch: 70 [45568/50048]	Loss: 0.1608
Training Epoch: 70 [45696/50048]	Loss: 0.2390
2022-12-06 05:24:06,300 [ZeusDataLoader(train)] train epoch 71 done: time=86.45 energy=10495.71
2022-12-06 05:24:06,301 [ZeusDataLoader(eval)] Epoch 71 begin.
Training Epoch: 70 [45824/50048]	Loss: 0.1184
Training Epoch: 70 [45952/50048]	Loss: 0.2138
Training Epoch: 70 [46080/50048]	Loss: 0.1201
Training Epoch: 70 [46208/50048]	Loss: 0.1824
Training Epoch: 70 [46336/50048]	Loss: 0.1183
Training Epoch: 70 [46464/50048]	Loss: 0.1724
Training Epoch: 70 [46592/50048]	Loss: 0.1172
Training Epoch: 70 [46720/50048]	Loss: 0.1325
Training Epoch: 70 [46848/50048]	Loss: 0.1660
Training Epoch: 70 [46976/50048]	Loss: 0.1128
Training Epoch: 70 [47104/50048]	Loss: 0.0970
Training Epoch: 70 [47232/50048]	Loss: 0.1906
Training Epoch: 70 [47360/50048]	Loss: 0.1544
Training Epoch: 70 [47488/50048]	Loss: 0.1060
Training Epoch: 70 [47616/50048]	Loss: 0.1000
Training Epoch: 70 [47744/50048]	Loss: 0.1762
Training Epoch: 70 [47872/50048]	Loss: 0.1145
Training Epoch: 70 [48000/50048]	Loss: 0.2974
Training Epoch: 70 [48128/50048]	Loss: 0.0733
Training Epoch: 70 [48256/50048]	Loss: 0.1239
Training Epoch: 70 [48384/50048]	Loss: 0.1441
Training Epoch: 70 [48512/50048]	Loss: 0.1026
Training Epoch: 70 [48640/50048]	Loss: 0.2533
Training Epoch: 70 [48768/50048]	Loss: 0.1497
Training Epoch: 70 [48896/50048]	Loss: 0.0661
Training Epoch: 70 [49024/50048]	Loss: 0.2846
Training Epoch: 70 [49152/50048]	Loss: 0.2085
Training Epoch: 70 [49280/50048]	Loss: 0.1766
Training Epoch: 70 [49408/50048]	Loss: 0.1538
Training Epoch: 70 [49536/50048]	Loss: 0.2572
Training Epoch: 70 [49664/50048]	Loss: 0.1780
Training Epoch: 70 [49792/50048]	Loss: 0.2828
Training Epoch: 70 [49920/50048]	Loss: 0.1539
Training Epoch: 70 [50048/50048]	Loss: 0.1592
2022-12-06 10:24:10.014 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:24:10,037 [ZeusDataLoader(eval)] eval epoch 71 done: time=3.73 energy=453.38
2022-12-06 05:24:10,037 [ZeusDataLoader(train)] Up to epoch 71: time=6406.26, energy=777656.37, cost=949375.90
2022-12-06 05:24:10,038 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:24:10,038 [ZeusDataLoader(train)] Expected next epoch: time=6496.06, energy=788454.39, cost=962632.29
2022-12-06 05:24:10,039 [ZeusDataLoader(train)] Epoch 72 begin.
Validation Epoch: 70, Average loss: 0.0176, Accuracy: 0.6375
2022-12-06 05:24:10,178 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:24:10,178 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:24:10.182 [ZeusMonitor] Monitor started.
2022-12-06 10:24:10.182 [ZeusMonitor] Running indefinitely. 2022-12-06 10:24:10.182 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:24:10.182 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e72+gpu0.power.log
Training Epoch: 71 [128/50048]	Loss: 0.0593
Training Epoch: 71 [256/50048]	Loss: 0.0744
Training Epoch: 71 [384/50048]	Loss: 0.1471
Training Epoch: 71 [512/50048]	Loss: 0.1210
Training Epoch: 71 [640/50048]	Loss: 0.2280
Training Epoch: 71 [768/50048]	Loss: 0.0935
Training Epoch: 71 [896/50048]	Loss: 0.1285
Training Epoch: 71 [1024/50048]	Loss: 0.1502
Training Epoch: 71 [1152/50048]	Loss: 0.0801
Training Epoch: 71 [1280/50048]	Loss: 0.1006
Training Epoch: 71 [1408/50048]	Loss: 0.1055
Training Epoch: 71 [1536/50048]	Loss: 0.1170
Training Epoch: 71 [1664/50048]	Loss: 0.1409
Training Epoch: 71 [1792/50048]	Loss: 0.0758
Training Epoch: 71 [1920/50048]	Loss: 0.0539
Training Epoch: 71 [2048/50048]	Loss: 0.2081
Training Epoch: 71 [2176/50048]	Loss: 0.1039
Training Epoch: 71 [2304/50048]	Loss: 0.1325
Training Epoch: 71 [2432/50048]	Loss: 0.1299
Training Epoch: 71 [2560/50048]	Loss: 0.1486
Training Epoch: 71 [2688/50048]	Loss: 0.1590
Training Epoch: 71 [2816/50048]	Loss: 0.0784
Training Epoch: 71 [2944/50048]	Loss: 0.0902
Training Epoch: 71 [3072/50048]	Loss: 0.0937
Training Epoch: 71 [3200/50048]	Loss: 0.1909
Training Epoch: 71 [3328/50048]	Loss: 0.1730
Training Epoch: 71 [3456/50048]	Loss: 0.0818
Training Epoch: 71 [3584/50048]	Loss: 0.1895
Training Epoch: 71 [3712/50048]	Loss: 0.0973
Training Epoch: 71 [3840/50048]	Loss: 0.0981
Training Epoch: 71 [3968/50048]	Loss: 0.1274
Training Epoch: 71 [4096/50048]	Loss: 0.0857
Training Epoch: 71 [4224/50048]	Loss: 0.1188
Training Epoch: 71 [4352/50048]	Loss: 0.1085
Training Epoch: 71 [4480/50048]	Loss: 0.1300
Training Epoch: 71 [4608/50048]	Loss: 0.0845
Training Epoch: 71 [4736/50048]	Loss: 0.1225
Training Epoch: 71 [4864/50048]	Loss: 0.0790
Training Epoch: 71 [4992/50048]	Loss: 0.1518
Training Epoch: 71 [5120/50048]	Loss: 0.1615
Training Epoch: 71 [5248/50048]	Loss: 0.1250
Training Epoch: 71 [5376/50048]	Loss: 0.0827
Training Epoch: 71 [5504/50048]	Loss: 0.2001
Training Epoch: 71 [5632/50048]	Loss: 0.1493
Training Epoch: 71 [5760/50048]	Loss: 0.1091
Training Epoch: 71 [5888/50048]	Loss: 0.1805
Training Epoch: 71 [6016/50048]	Loss: 0.2007
Training Epoch: 71 [6144/50048]	Loss: 0.1924
Training Epoch: 71 [6272/50048]	Loss: 0.1302
Training Epoch: 71 [6400/50048]	Loss: 0.1208
Training Epoch: 71 [6528/50048]	Loss: 0.1396
Training Epoch: 71 [6656/50048]	Loss: 0.0811
Training Epoch: 71 [6784/50048]	Loss: 0.1104
Training Epoch: 71 [6912/50048]	Loss: 0.1512
Training Epoch: 71 [7040/50048]	Loss: 0.1258
Training Epoch: 71 [7168/50048]	Loss: 0.1320
Training Epoch: 71 [7296/50048]	Loss: 0.1215
Training Epoch: 71 [7424/50048]	Loss: 0.1100
Training Epoch: 71 [7552/50048]	Loss: 0.0795
Training Epoch: 71 [7680/50048]	Loss: 0.1999
Training Epoch: 71 [7808/50048]	Loss: 0.1266
Training Epoch: 71 [7936/50048]	Loss: 0.1653
Training Epoch: 71 [8064/50048]	Loss: 0.1560
Training Epoch: 71 [8192/50048]	Loss: 0.1064
Training Epoch: 71 [8320/50048]	Loss: 0.1079
Training Epoch: 71 [8448/50048]	Loss: 0.1464
Training Epoch: 71 [8576/50048]	Loss: 0.1103
Training Epoch: 71 [8704/50048]	Loss: 0.0797
Training Epoch: 71 [8832/50048]	Loss: 0.1260
Training Epoch: 71 [8960/50048]	Loss: 0.0831
Training Epoch: 71 [9088/50048]	Loss: 0.1928
Training Epoch: 71 [9216/50048]	Loss: 0.1716
Training Epoch: 71 [9344/50048]	Loss: 0.1840
Training Epoch: 71 [9472/50048]	Loss: 0.0507
Training Epoch: 71 [9600/50048]	Loss: 0.1233
Training Epoch: 71 [9728/50048]	Loss: 0.1053
Training Epoch: 71 [9856/50048]	Loss: 0.2290
Training Epoch: 71 [9984/50048]	Loss: 0.0982
Training Epoch: 71 [10112/50048]	Loss: 0.0655
Training Epoch: 71 [10240/50048]	Loss: 0.0750
Training Epoch: 71 [10368/50048]	Loss: 0.1088
Training Epoch: 71 [10496/50048]	Loss: 0.1301
Training Epoch: 71 [10624/50048]	Loss: 0.1268
Training Epoch: 71 [10752/50048]	Loss: 0.1675
Training Epoch: 71 [10880/50048]	Loss: 0.0996
Training Epoch: 71 [11008/50048]	Loss: 0.1217
Training Epoch: 71 [11136/50048]	Loss: 0.2132
Training Epoch: 71 [11264/50048]	Loss: 0.1520
Training Epoch: 71 [11392/50048]	Loss: 0.0551
Training Epoch: 71 [11520/50048]	Loss: 0.0429
Training Epoch: 71 [11648/50048]	Loss: 0.0960
Training Epoch: 71 [11776/50048]	Loss: 0.1062
Training Epoch: 71 [11904/50048]	Loss: 0.1118
Training Epoch: 71 [12032/50048]	Loss: 0.0947
Training Epoch: 71 [12160/50048]	Loss: 0.1565
Training Epoch: 71 [12288/50048]	Loss: 0.1385
Training Epoch: 71 [12416/50048]	Loss: 0.1186
Training Epoch: 71 [12544/50048]	Loss: 0.1118
Training Epoch: 71 [12672/50048]	Loss: 0.1171
Training Epoch: 71 [12800/50048]	Loss: 0.0869
Training Epoch: 71 [12928/50048]	Loss: 0.0755
Training Epoch: 71 [13056/50048]	Loss: 0.1366
Training Epoch: 71 [13184/50048]	Loss: 0.1110
Training Epoch: 71 [13312/50048]	Loss: 0.1189
Training Epoch: 71 [13440/50048]	Loss: 0.1895
Training Epoch: 71 [13568/50048]	Loss: 0.1357
Training Epoch: 71 [13696/50048]	Loss: 0.1102
Training Epoch: 71 [13824/50048]	Loss: 0.0891
Training Epoch: 71 [13952/50048]	Loss: 0.1714
Training Epoch: 71 [14080/50048]	Loss: 0.0670
Training Epoch: 71 [14208/50048]	Loss: 0.1497
Training Epoch: 71 [14336/50048]	Loss: 0.1107
Training Epoch: 71 [14464/50048]	Loss: 0.1051
Training Epoch: 71 [14592/50048]	Loss: 0.1572
Training Epoch: 71 [14720/50048]	Loss: 0.1397
Training Epoch: 71 [14848/50048]	Loss: 0.1352
Training Epoch: 71 [14976/50048]	Loss: 0.1287
Training Epoch: 71 [15104/50048]	Loss: 0.1312
Training Epoch: 71 [15232/50048]	Loss: 0.1014
Training Epoch: 71 [15360/50048]	Loss: 0.1041
Training Epoch: 71 [15488/50048]	Loss: 0.1028
Training Epoch: 71 [15616/50048]	Loss: 0.0807
Training Epoch: 71 [15744/50048]	Loss: 0.1461
Training Epoch: 71 [15872/50048]	Loss: 0.1880
Training Epoch: 71 [16000/50048]	Loss: 0.1340
Training Epoch: 71 [16128/50048]	Loss: 0.1327
Training Epoch: 71 [16256/50048]	Loss: 0.0773
Training Epoch: 71 [16384/50048]	Loss: 0.1015
Training Epoch: 71 [16512/50048]	Loss: 0.1938
Training Epoch: 71 [16640/50048]	Loss: 0.1407
Training Epoch: 71 [16768/50048]	Loss: 0.1277
Training Epoch: 71 [16896/50048]	Loss: 0.1386
Training Epoch: 71 [17024/50048]	Loss: 0.0925
Training Epoch: 71 [17152/50048]	Loss: 0.1953
Training Epoch: 71 [17280/50048]	Loss: 0.0712
Training Epoch: 71 [17408/50048]	Loss: 0.1567
Training Epoch: 71 [17536/50048]	Loss: 0.0835
Training Epoch: 71 [17664/50048]	Loss: 0.1388
Training Epoch: 71 [17792/50048]	Loss: 0.1227
Training Epoch: 71 [17920/50048]	Loss: 0.1644
Training Epoch: 71 [18048/50048]	Loss: 0.0870
Training Epoch: 71 [18176/50048]	Loss: 0.1485
Training Epoch: 71 [18304/50048]	Loss: 0.1058
Training Epoch: 71 [18432/50048]	Loss: 0.1433
Training Epoch: 71 [18560/50048]	Loss: 0.1011
Training Epoch: 71 [18688/50048]	Loss: 0.0586
Training Epoch: 71 [18816/50048]	Loss: 0.1322
Training Epoch: 71 [18944/50048]	Loss: 0.1559
Training Epoch: 71 [19072/50048]	Loss: 0.2563
Training Epoch: 71 [19200/50048]	Loss: 0.1717
Training Epoch: 71 [19328/50048]	Loss: 0.1077
Training Epoch: 71 [19456/50048]	Loss: 0.0984
Training Epoch: 71 [19584/50048]	Loss: 0.1526
Training Epoch: 71 [19712/50048]	Loss: 0.1642
Training Epoch: 71 [19840/50048]	Loss: 0.1567
Training Epoch: 71 [19968/50048]	Loss: 0.1343
Training Epoch: 71 [20096/50048]	Loss: 0.1960
Training Epoch: 71 [20224/50048]	Loss: 0.1135
Training Epoch: 71 [20352/50048]	Loss: 0.1069
Training Epoch: 71 [20480/50048]	Loss: 0.2146
Training Epoch: 71 [20608/50048]	Loss: 0.1565
Training Epoch: 71 [20736/50048]	Loss: 0.1204
Training Epoch: 71 [20864/50048]	Loss: 0.1325
Training Epoch: 71 [20992/50048]	Loss: 0.1260
Training Epoch: 71 [21120/50048]	Loss: 0.1123
Training Epoch: 71 [21248/50048]	Loss: 0.0975
Training Epoch: 71 [21376/50048]	Loss: 0.1241
Training Epoch: 71 [21504/50048]	Loss: 0.1386
Training Epoch: 71 [21632/50048]	Loss: 0.1591
Training Epoch: 71 [21760/50048]	Loss: 0.1273
Training Epoch: 71 [21888/50048]	Loss: 0.1247
Training Epoch: 71 [22016/50048]	Loss: 0.0865
Training Epoch: 71 [22144/50048]	Loss: 0.1628
Training Epoch: 71 [22272/50048]	Loss: 0.1628
Training Epoch: 71 [22400/50048]	Loss: 0.1973
Training Epoch: 71 [22528/50048]	Loss: 0.1801
Training Epoch: 71 [22656/50048]	Loss: 0.2355
Training Epoch: 71 [22784/50048]	Loss: 0.0838
Training Epoch: 71 [22912/50048]	Loss: 0.2209
Training Epoch: 71 [23040/50048]	Loss: 0.1040
Training Epoch: 71 [23168/50048]	Loss: 0.1570
Training Epoch: 71 [23296/50048]	Loss: 0.1614
Training Epoch: 71 [23424/50048]	Loss: 0.2700
Training Epoch: 71 [23552/50048]	Loss: 0.0984
Training Epoch: 71 [23680/50048]	Loss: 0.1902
Training Epoch: 71 [23808/50048]	Loss: 0.1821
Training Epoch: 71 [23936/50048]	Loss: 0.0685
Training Epoch: 71 [24064/50048]	Loss: 0.1114
Training Epoch: 71 [24192/50048]	Loss: 0.2177
Training Epoch: 71 [24320/50048]	Loss: 0.1659
Training Epoch: 71 [24448/50048]	Loss: 0.1377
Training Epoch: 71 [24576/50048]	Loss: 0.1399
Training Epoch: 71 [24704/50048]	Loss: 0.1207
Training Epoch: 71 [24832/50048]	Loss: 0.1140
Training Epoch: 71 [24960/50048]	Loss: 0.1702
Training Epoch: 71 [25088/50048]	Loss: 0.1154
Training Epoch: 71 [25216/50048]	Loss: 0.0874
Training Epoch: 71 [25344/50048]	Loss: 0.1293
Training Epoch: 71 [25472/50048]	Loss: 0.2177
Training Epoch: 71 [25600/50048]	Loss: 0.0990
Training Epoch: 71 [25728/50048]	Loss: 0.1362
Training Epoch: 71 [25856/50048]	Loss: 0.2039
Training Epoch: 71 [25984/50048]	Loss: 0.1693
Training Epoch: 71 [26112/50048]	Loss: 0.1944
Training Epoch: 71 [26240/50048]	Loss: 0.0819
Training Epoch: 71 [26368/50048]	Loss: 0.1960
Training Epoch: 71 [26496/50048]	Loss: 0.1097
Training Epoch: 71 [26624/50048]	Loss: 0.0779
Training Epoch: 71 [26752/50048]	Loss: 0.1476
Training Epoch: 71 [26880/50048]	Loss: 0.1396
Training Epoch: 71 [27008/50048]	Loss: 0.1095
Training Epoch: 71 [27136/50048]	Loss: 0.1434
Training Epoch: 71 [27264/50048]	Loss: 0.1301
Training Epoch: 71 [27392/50048]	Loss: 0.1972
Training Epoch: 71 [27520/50048]	Loss: 0.1029
Training Epoch: 71 [27648/50048]	Loss: 0.1273
Training Epoch: 71 [27776/50048]	Loss: 0.1732
Training Epoch: 71 [27904/50048]	Loss: 0.0613
Training Epoch: 71 [28032/50048]	Loss: 0.2350
Training Epoch: 71 [28160/50048]	Loss: 0.0995
Training Epoch: 71 [28288/50048]	Loss: 0.1374
Training Epoch: 71 [28416/50048]	Loss: 0.1183
Training Epoch: 71 [28544/50048]	Loss: 0.0837
Training Epoch: 71 [28672/50048]	Loss: 0.0984
Training Epoch: 71 [28800/50048]	Loss: 0.1015
Training Epoch: 71 [28928/50048]	Loss: 0.1334
Training Epoch: 71 [29056/50048]	Loss: 0.1331
Training Epoch: 71 [29184/50048]	Loss: 0.1104
Training Epoch: 71 [29312/50048]	Loss: 0.1026
Training Epoch: 71 [29440/50048]	Loss: 0.1230
Training Epoch: 71 [29568/50048]	Loss: 0.0701
Training Epoch: 71 [29696/50048]	Loss: 0.1560
Training Epoch: 71 [29824/50048]	Loss: 0.1083
Training Epoch: 71 [29952/50048]	Loss: 0.1154
Training Epoch: 71 [30080/50048]	Loss: 0.1931
Training Epoch: 71 [30208/50048]	Loss: 0.1473
Training Epoch: 71 [30336/50048]	Loss: 0.1278
Training Epoch: 71 [30464/50048]	Loss: 0.1466
Training Epoch: 71 [30592/50048]	Loss: 0.2710
Training Epoch: 71 [30720/50048]	Loss: 0.0951
Training Epoch: 71 [30848/50048]	Loss: 0.1370
Training Epoch: 71 [30976/50048]	Loss: 0.1553
Training Epoch: 71 [31104/50048]	Loss: 0.0711
Training Epoch: 71 [31232/50048]	Loss: 0.1507
Training Epoch: 71 [31360/50048]	Loss: 0.1602
Training Epoch: 71 [31488/50048]	Loss: 0.1485
Training Epoch: 71 [31616/50048]	Loss: 0.1538
Training Epoch: 71 [31744/50048]	Loss: 0.1535
Training Epoch: 71 [31872/50048]	Loss: 0.1455
Training Epoch: 71 [32000/50048]	Loss: 0.2124
Training Epoch: 71 [32128/50048]	Loss: 0.1042
Training Epoch: 71 [32256/50048]	Loss: 0.1427
Training Epoch: 71 [32384/50048]	Loss: 0.1144
Training Epoch: 71 [32512/50048]	Loss: 0.0715
Training Epoch: 71 [32640/50048]	Loss: 0.1597
Training Epoch: 71 [32768/50048]	Loss: 0.1846
Training Epoch: 71 [32896/50048]	Loss: 0.2062
Training Epoch: 71 [33024/50048]	Loss: 0.1619
Training Epoch: 71 [33152/50048]	Loss: 0.1235
Training Epoch: 71 [33280/50048]	Loss: 0.1647
Training Epoch: 71 [33408/50048]	Loss: 0.1021
Training Epoch: 71 [33536/50048]	Loss: 0.1549
Training Epoch: 71 [33664/50048]	Loss: 0.1326
Training Epoch: 71 [33792/50048]	Loss: 0.1065
Training Epoch: 71 [33920/50048]	Loss: 0.1698
Training Epoch: 71 [34048/50048]	Loss: 0.0891
Training Epoch: 71 [34176/50048]	Loss: 0.0789
Training Epoch: 71 [34304/50048]	Loss: 0.0806
Training Epoch: 71 [34432/50048]	Loss: 0.1403
Training Epoch: 71 [34560/50048]	Loss: 0.1024
Training Epoch: 71 [34688/50048]	Loss: 0.1225
Training Epoch: 71 [34816/50048]	Loss: 0.1244
Training Epoch: 71 [34944/50048]	Loss: 0.1982
Training Epoch: 71 [35072/50048]	Loss: 0.2009
Training Epoch: 71 [35200/50048]	Loss: 0.2003
Training Epoch: 71 [35328/50048]	Loss: 0.0929
Training Epoch: 71 [35456/50048]	Loss: 0.1219
Training Epoch: 71 [35584/50048]	Loss: 0.1787
Training Epoch: 71 [35712/50048]	Loss: 0.1193
Training Epoch: 71 [35840/50048]	Loss: 0.1637
Training Epoch: 71 [35968/50048]	Loss: 0.1325
Training Epoch: 71 [36096/50048]	Loss: 0.1329
Training Epoch: 71 [36224/50048]	Loss: 0.1699
Training Epoch: 71 [36352/50048]	Loss: 0.1456
Training Epoch: 71 [36480/50048]	Loss: 0.1752
Training Epoch: 71 [36608/50048]	Loss: 0.1748
Training Epoch: 71 [36736/50048]	Loss: 0.1478
Training Epoch: 71 [36864/50048]	Loss: 0.1398
Training Epoch: 71 [36992/50048]	Loss: 0.1092
Training Epoch: 71 [37120/50048]	Loss: 0.0601
Training Epoch: 71 [37248/50048]	Loss: 0.1651
Training Epoch: 71 [37376/50048]	Loss: 0.1399
Training Epoch: 71 [37504/50048]	Loss: 0.1079
Training Epoch: 71 [37632/50048]	Loss: 0.1930
Training Epoch: 71 [37760/50048]	Loss: 0.2192
Training Epoch: 71 [37888/50048]	Loss: 0.0774
Training Epoch: 71 [38016/50048]	Loss: 0.1964
Training Epoch: 71 [38144/50048]	Loss: 0.1207
Training Epoch: 71 [38272/50048]	Loss: 0.1859
Training Epoch: 71 [38400/50048]	Loss: 0.2733
Training Epoch: 71 [38528/50048]	Loss: 0.2112
Training Epoch: 71 [38656/50048]	Loss: 0.0740
Training Epoch: 71 [38784/50048]	Loss: 0.0974
Training Epoch: 71 [38912/50048]	Loss: 0.1159
Training Epoch: 71 [39040/50048]	Loss: 0.1204
Training Epoch: 71 [39168/50048]	Loss: 0.1465
Training Epoch: 71 [39296/50048]	Loss: 0.1222
Training Epoch: 71 [39424/50048]	Loss: 0.1211
Training Epoch: 71 [39552/50048]	Loss: 0.1242
Training Epoch: 71 [39680/50048]	Loss: 0.2398
Training Epoch: 71 [39808/50048]	Loss: 0.1492
Training Epoch: 71 [39936/50048]	Loss: 0.1273
Training Epoch: 71 [40064/50048]	Loss: 0.1151
Training Epoch: 71 [40192/50048]	Loss: 0.1928
Training Epoch: 71 [40320/50048]	Loss: 0.1744
Training Epoch: 71 [40448/50048]	Loss: 0.0959
Training Epoch: 71 [40576/50048]	Loss: 0.1311
Training Epoch: 71 [40704/50048]	Loss: 0.1091
Training Epoch: 71 [40832/50048]	Loss: 0.0961
Training Epoch: 71 [40960/50048]	Loss: 0.0946
Training Epoch: 71 [41088/50048]	Loss: 0.1153
Training Epoch: 71 [41216/50048]	Loss: 0.1384
Training Epoch: 71 [41344/50048]	Loss: 0.1657
Training Epoch: 71 [41472/50048]	Loss: 0.1709
Training Epoch: 71 [41600/50048]	Loss: 0.1479
Training Epoch: 71 [41728/50048]	Loss: 0.1624
Training Epoch: 71 [41856/50048]	Loss: 0.1215
Training Epoch: 71 [41984/50048]	Loss: 0.1595
Training Epoch: 71 [42112/50048]	Loss: 0.2202
Training Epoch: 71 [42240/50048]	Loss: 0.1291
Training Epoch: 71 [42368/50048]	Loss: 0.1370
Training Epoch: 71 [42496/50048]	Loss: 0.1334
Training Epoch: 71 [42624/50048]	Loss: 0.1767
Training Epoch: 71 [42752/50048]	Loss: 0.1533
Training Epoch: 71 [42880/50048]	Loss: 0.2384
Training Epoch: 71 [43008/50048]	Loss: 0.1644
Training Epoch: 71 [43136/50048]	Loss: 0.1742
Training Epoch: 71 [43264/50048]	Loss: 0.2024
Training Epoch: 71 [43392/50048]	Loss: 0.1003
Training Epoch: 71 [43520/50048]	Loss: 0.0996
Training Epoch: 71 [43648/50048]	Loss: 0.1090
Training Epoch: 71 [43776/50048]	Loss: 0.1665
Training Epoch: 71 [43904/50048]	Loss: 0.1289
Training Epoch: 71 [44032/50048]	Loss: 0.1664
Training Epoch: 71 [44160/50048]	Loss: 0.1049
Training Epoch: 71 [44288/50048]	Loss: 0.1600
Training Epoch: 71 [44416/50048]	Loss: 0.1964
Training Epoch: 71 [44544/50048]	Loss: 0.1058
Training Epoch: 71 [44672/50048]	Loss: 0.1423
Training Epoch: 71 [44800/50048]	Loss: 0.1924
Training Epoch: 71 [44928/50048]	Loss: 0.0799
Training Epoch: 71 [45056/50048]	Loss: 0.1336
Training Epoch: 71 [45184/50048]	Loss: 0.0833
Training Epoch: 71 [45312/50048]	Loss: 0.1404
Training Epoch: 71 [45440/50048]	Loss: 0.1266
Training Epoch: 71 [45568/50048]	Loss: 0.0955
Training Epoch: 71 [45696/50048]	Loss: 0.1446
2022-12-06 05:25:36,542 [ZeusDataLoader(train)] train epoch 72 done: time=86.45 energy=10513.86
2022-12-06 05:25:36,544 [ZeusDataLoader(eval)] Epoch 72 begin.
Training Epoch: 71 [45824/50048]	Loss: 0.2223
Training Epoch: 71 [45952/50048]	Loss: 0.0960
Training Epoch: 71 [46080/50048]	Loss: 0.1453
Training Epoch: 71 [46208/50048]	Loss: 0.0568
Training Epoch: 71 [46336/50048]	Loss: 0.1221
Training Epoch: 71 [46464/50048]	Loss: 0.1736
Training Epoch: 71 [46592/50048]	Loss: 0.1100
Training Epoch: 71 [46720/50048]	Loss: 0.1278
Training Epoch: 71 [46848/50048]	Loss: 0.1054
Training Epoch: 71 [46976/50048]	Loss: 0.2337
Training Epoch: 71 [47104/50048]	Loss: 0.0842
Training Epoch: 71 [47232/50048]	Loss: 0.1454
Training Epoch: 71 [47360/50048]	Loss: 0.1200
Training Epoch: 71 [47488/50048]	Loss: 0.1715
Training Epoch: 71 [47616/50048]	Loss: 0.0825
Training Epoch: 71 [47744/50048]	Loss: 0.1809
Training Epoch: 71 [47872/50048]	Loss: 0.1385
Training Epoch: 71 [48000/50048]	Loss: 0.1868
Training Epoch: 71 [48128/50048]	Loss: 0.1233
Training Epoch: 71 [48256/50048]	Loss: 0.1358
Training Epoch: 71 [48384/50048]	Loss: 0.2834
Training Epoch: 71 [48512/50048]	Loss: 0.0928
Training Epoch: 71 [48640/50048]	Loss: 0.1475
Training Epoch: 71 [48768/50048]	Loss: 0.1278
Training Epoch: 71 [48896/50048]	Loss: 0.0956
Training Epoch: 71 [49024/50048]	Loss: 0.2111
Training Epoch: 71 [49152/50048]	Loss: 0.2288
Training Epoch: 71 [49280/50048]	Loss: 0.1173
Training Epoch: 71 [49408/50048]	Loss: 0.2191
Training Epoch: 71 [49536/50048]	Loss: 0.1386
Training Epoch: 71 [49664/50048]	Loss: 0.2127
Training Epoch: 71 [49792/50048]	Loss: 0.2461
Training Epoch: 71 [49920/50048]	Loss: 0.2177
Training Epoch: 71 [50048/50048]	Loss: 0.2590
2022-12-06 10:25:40.250 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:25:40,290 [ZeusDataLoader(eval)] eval epoch 72 done: time=3.74 energy=450.78
2022-12-06 05:25:40,290 [ZeusDataLoader(train)] Up to epoch 72: time=6496.45, energy=788621.01, cost=962749.53
2022-12-06 05:25:40,290 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:25:40,290 [ZeusDataLoader(train)] Expected next epoch: time=6586.24, energy=799419.03, cost=976005.91
2022-12-06 05:25:40,291 [ZeusDataLoader(train)] Epoch 73 begin.
Validation Epoch: 71, Average loss: 0.0173, Accuracy: 0.6419
2022-12-06 05:25:40,484 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:25:40,485 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:25:40.486 [ZeusMonitor] Monitor started.
2022-12-06 10:25:40.486 [ZeusMonitor] Running indefinitely. 2022-12-06 10:25:40.486 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:25:40.486 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e73+gpu0.power.log
Training Epoch: 72 [128/50048]	Loss: 0.2051
Training Epoch: 72 [256/50048]	Loss: 0.0997
Training Epoch: 72 [384/50048]	Loss: 0.2095
Training Epoch: 72 [512/50048]	Loss: 0.1047
Training Epoch: 72 [640/50048]	Loss: 0.1424
Training Epoch: 72 [768/50048]	Loss: 0.1213
Training Epoch: 72 [896/50048]	Loss: 0.1506
Training Epoch: 72 [1024/50048]	Loss: 0.1155
Training Epoch: 72 [1152/50048]	Loss: 0.1888
Training Epoch: 72 [1280/50048]	Loss: 0.1166
Training Epoch: 72 [1408/50048]	Loss: 0.1454
Training Epoch: 72 [1536/50048]	Loss: 0.1929
Training Epoch: 72 [1664/50048]	Loss: 0.1963
Training Epoch: 72 [1792/50048]	Loss: 0.1897
Training Epoch: 72 [1920/50048]	Loss: 0.1660
Training Epoch: 72 [2048/50048]	Loss: 0.1034
Training Epoch: 72 [2176/50048]	Loss: 0.0767
Training Epoch: 72 [2304/50048]	Loss: 0.1444
Training Epoch: 72 [2432/50048]	Loss: 0.1360
Training Epoch: 72 [2560/50048]	Loss: 0.1138
Training Epoch: 72 [2688/50048]	Loss: 0.1178
Training Epoch: 72 [2816/50048]	Loss: 0.1625
Training Epoch: 72 [2944/50048]	Loss: 0.1060
Training Epoch: 72 [3072/50048]	Loss: 0.0677
Training Epoch: 72 [3200/50048]	Loss: 0.2044
Training Epoch: 72 [3328/50048]	Loss: 0.0926
Training Epoch: 72 [3456/50048]	Loss: 0.0945
Training Epoch: 72 [3584/50048]	Loss: 0.1142
Training Epoch: 72 [3712/50048]	Loss: 0.0970
Training Epoch: 72 [3840/50048]	Loss: 0.0973
Training Epoch: 72 [3968/50048]	Loss: 0.1012
Training Epoch: 72 [4096/50048]	Loss: 0.1309
Training Epoch: 72 [4224/50048]	Loss: 0.1187
Training Epoch: 72 [4352/50048]	Loss: 0.1374
Training Epoch: 72 [4480/50048]	Loss: 0.0967
Training Epoch: 72 [4608/50048]	Loss: 0.1523
Training Epoch: 72 [4736/50048]	Loss: 0.0425
Training Epoch: 72 [4864/50048]	Loss: 0.2024
Training Epoch: 72 [4992/50048]	Loss: 0.1589
Training Epoch: 72 [5120/50048]	Loss: 0.0928
Training Epoch: 72 [5248/50048]	Loss: 0.1466
Training Epoch: 72 [5376/50048]	Loss: 0.0984
Training Epoch: 72 [5504/50048]	Loss: 0.1432
Training Epoch: 72 [5632/50048]	Loss: 0.1392
Training Epoch: 72 [5760/50048]	Loss: 0.0936
Training Epoch: 72 [5888/50048]	Loss: 0.1517
Training Epoch: 72 [6016/50048]	Loss: 0.1424
Training Epoch: 72 [6144/50048]	Loss: 0.0787
Training Epoch: 72 [6272/50048]	Loss: 0.1777
Training Epoch: 72 [6400/50048]	Loss: 0.1727
Training Epoch: 72 [6528/50048]	Loss: 0.1404
Training Epoch: 72 [6656/50048]	Loss: 0.1167
Training Epoch: 72 [6784/50048]	Loss: 0.1981
Training Epoch: 72 [6912/50048]	Loss: 0.0854
Training Epoch: 72 [7040/50048]	Loss: 0.0782
Training Epoch: 72 [7168/50048]	Loss: 0.1549
Training Epoch: 72 [7296/50048]	Loss: 0.1397
Training Epoch: 72 [7424/50048]	Loss: 0.1308
Training Epoch: 72 [7552/50048]	Loss: 0.1230
Training Epoch: 72 [7680/50048]	Loss: 0.1489
Training Epoch: 72 [7808/50048]	Loss: 0.1016
Training Epoch: 72 [7936/50048]	Loss: 0.1731
Training Epoch: 72 [8064/50048]	Loss: 0.0891
Training Epoch: 72 [8192/50048]	Loss: 0.0495
Training Epoch: 72 [8320/50048]	Loss: 0.1606
Training Epoch: 72 [8448/50048]	Loss: 0.1109
Training Epoch: 72 [8576/50048]	Loss: 0.0895
Training Epoch: 72 [8704/50048]	Loss: 0.0733
Training Epoch: 72 [8832/50048]	Loss: 0.2047
Training Epoch: 72 [8960/50048]	Loss: 0.0921
Training Epoch: 72 [9088/50048]	Loss: 0.1296
Training Epoch: 72 [9216/50048]	Loss: 0.1221
Training Epoch: 72 [9344/50048]	Loss: 0.1280
Training Epoch: 72 [9472/50048]	Loss: 0.0465
Training Epoch: 72 [9600/50048]	Loss: 0.1869
Training Epoch: 72 [9728/50048]	Loss: 0.1746
Training Epoch: 72 [9856/50048]	Loss: 0.0736
Training Epoch: 72 [9984/50048]	Loss: 0.1265
Training Epoch: 72 [10112/50048]	Loss: 0.1787
Training Epoch: 72 [10240/50048]	Loss: 0.0836
Training Epoch: 72 [10368/50048]	Loss: 0.1075
Training Epoch: 72 [10496/50048]	Loss: 0.2072
Training Epoch: 72 [10624/50048]	Loss: 0.1632
Training Epoch: 72 [10752/50048]	Loss: 0.0928
Training Epoch: 72 [10880/50048]	Loss: 0.1455
Training Epoch: 72 [11008/50048]	Loss: 0.1550
Training Epoch: 72 [11136/50048]	Loss: 0.1221
Training Epoch: 72 [11264/50048]	Loss: 0.1612
Training Epoch: 72 [11392/50048]	Loss: 0.0690
Training Epoch: 72 [11520/50048]	Loss: 0.1215
Training Epoch: 72 [11648/50048]	Loss: 0.0951
Training Epoch: 72 [11776/50048]	Loss: 0.1114
Training Epoch: 72 [11904/50048]	Loss: 0.0941
Training Epoch: 72 [12032/50048]	Loss: 0.1556
Training Epoch: 72 [12160/50048]	Loss: 0.1189
Training Epoch: 72 [12288/50048]	Loss: 0.1924
Training Epoch: 72 [12416/50048]	Loss: 0.1490
Training Epoch: 72 [12544/50048]	Loss: 0.0651
Training Epoch: 72 [12672/50048]	Loss: 0.1033
Training Epoch: 72 [12800/50048]	Loss: 0.1021
Training Epoch: 72 [12928/50048]	Loss: 0.1810
Training Epoch: 72 [13056/50048]	Loss: 0.1477
Training Epoch: 72 [13184/50048]	Loss: 0.0942
Training Epoch: 72 [13312/50048]	Loss: 0.1460
Training Epoch: 72 [13440/50048]	Loss: 0.0842
Training Epoch: 72 [13568/50048]	Loss: 0.1146
Training Epoch: 72 [13696/50048]	Loss: 0.0402
Training Epoch: 72 [13824/50048]	Loss: 0.0617
Training Epoch: 72 [13952/50048]	Loss: 0.1082
Training Epoch: 72 [14080/50048]	Loss: 0.1554
Training Epoch: 72 [14208/50048]	Loss: 0.0988
Training Epoch: 72 [14336/50048]	Loss: 0.1708
Training Epoch: 72 [14464/50048]	Loss: 0.0588
Training Epoch: 72 [14592/50048]	Loss: 0.1092
Training Epoch: 72 [14720/50048]	Loss: 0.1101
Training Epoch: 72 [14848/50048]	Loss: 0.0857
Training Epoch: 72 [14976/50048]	Loss: 0.1450
Training Epoch: 72 [15104/50048]	Loss: 0.1326
Training Epoch: 72 [15232/50048]	Loss: 0.0918
Training Epoch: 72 [15360/50048]	Loss: 0.1187
Training Epoch: 72 [15488/50048]	Loss: 0.1206
Training Epoch: 72 [15616/50048]	Loss: 0.0603
Training Epoch: 72 [15744/50048]	Loss: 0.1207
Training Epoch: 72 [15872/50048]	Loss: 0.1192
Training Epoch: 72 [16000/50048]	Loss: 0.0868
Training Epoch: 72 [16128/50048]	Loss: 0.2173
Training Epoch: 72 [16256/50048]	Loss: 0.1361
Training Epoch: 72 [16384/50048]	Loss: 0.1701
Training Epoch: 72 [16512/50048]	Loss: 0.1380
Training Epoch: 72 [16640/50048]	Loss: 0.1112
Training Epoch: 72 [16768/50048]	Loss: 0.0902
Training Epoch: 72 [16896/50048]	Loss: 0.0952
Training Epoch: 72 [17024/50048]	Loss: 0.1455
Training Epoch: 72 [17152/50048]	Loss: 0.0974
Training Epoch: 72 [17280/50048]	Loss: 0.1194
Training Epoch: 72 [17408/50048]	Loss: 0.1204
Training Epoch: 72 [17536/50048]	Loss: 0.1127
Training Epoch: 72 [17664/50048]	Loss: 0.0948
Training Epoch: 72 [17792/50048]	Loss: 0.1252
Training Epoch: 72 [17920/50048]	Loss: 0.1872
Training Epoch: 72 [18048/50048]	Loss: 0.0887
Training Epoch: 72 [18176/50048]	Loss: 0.1492
Training Epoch: 72 [18304/50048]	Loss: 0.1951
Training Epoch: 72 [18432/50048]	Loss: 0.2115
Training Epoch: 72 [18560/50048]	Loss: 0.0955
Training Epoch: 72 [18688/50048]	Loss: 0.2217
Training Epoch: 72 [18816/50048]	Loss: 0.0878
Training Epoch: 72 [18944/50048]	Loss: 0.0864
Training Epoch: 72 [19072/50048]	Loss: 0.1694
Training Epoch: 72 [19200/50048]	Loss: 0.0594
Training Epoch: 72 [19328/50048]	Loss: 0.1300
Training Epoch: 72 [19456/50048]	Loss: 0.1298
Training Epoch: 72 [19584/50048]	Loss: 0.1753
Training Epoch: 72 [19712/50048]	Loss: 0.1071
Training Epoch: 72 [19840/50048]	Loss: 0.1106
Training Epoch: 72 [19968/50048]	Loss: 0.1246
Training Epoch: 72 [20096/50048]	Loss: 0.1321
Training Epoch: 72 [20224/50048]	Loss: 0.1641
Training Epoch: 72 [20352/50048]	Loss: 0.0907
Training Epoch: 72 [20480/50048]	Loss: 0.1367
Training Epoch: 72 [20608/50048]	Loss: 0.1082
Training Epoch: 72 [20736/50048]	Loss: 0.1366
Training Epoch: 72 [20864/50048]	Loss: 0.1475
Training Epoch: 72 [20992/50048]	Loss: 0.0893
Training Epoch: 72 [21120/50048]	Loss: 0.2525
Training Epoch: 72 [21248/50048]	Loss: 0.0946
Training Epoch: 72 [21376/50048]	Loss: 0.1342
Training Epoch: 72 [21504/50048]	Loss: 0.1018
Training Epoch: 72 [21632/50048]	Loss: 0.1436
Training Epoch: 72 [21760/50048]	Loss: 0.0996
Training Epoch: 72 [21888/50048]	Loss: 0.1176
Training Epoch: 72 [22016/50048]	Loss: 0.2100
Training Epoch: 72 [22144/50048]	Loss: 0.1061
Training Epoch: 72 [22272/50048]	Loss: 0.1831
Training Epoch: 72 [22400/50048]	Loss: 0.1633
Training Epoch: 72 [22528/50048]	Loss: 0.1959
Training Epoch: 72 [22656/50048]	Loss: 0.1384
Training Epoch: 72 [22784/50048]	Loss: 0.1517
Training Epoch: 72 [22912/50048]	Loss: 0.2137
Training Epoch: 72 [23040/50048]	Loss: 0.0877
Training Epoch: 72 [23168/50048]	Loss: 0.1761
Training Epoch: 72 [23296/50048]	Loss: 0.0895
Training Epoch: 72 [23424/50048]	Loss: 0.1101
Training Epoch: 72 [23552/50048]	Loss: 0.0810
Training Epoch: 72 [23680/50048]	Loss: 0.1268
Training Epoch: 72 [23808/50048]	Loss: 0.1896
Training Epoch: 72 [23936/50048]	Loss: 0.0869
Training Epoch: 72 [24064/50048]	Loss: 0.1011
Training Epoch: 72 [24192/50048]	Loss: 0.1061
Training Epoch: 72 [24320/50048]	Loss: 0.1209
Training Epoch: 72 [24448/50048]	Loss: 0.1939
Training Epoch: 72 [24576/50048]	Loss: 0.1356
Training Epoch: 72 [24704/50048]	Loss: 0.0579
Training Epoch: 72 [24832/50048]	Loss: 0.1095
Training Epoch: 72 [24960/50048]	Loss: 0.1066
Training Epoch: 72 [25088/50048]	Loss: 0.1225
Training Epoch: 72 [25216/50048]	Loss: 0.1325
Training Epoch: 72 [25344/50048]	Loss: 0.0688
Training Epoch: 72 [25472/50048]	Loss: 0.1816
Training Epoch: 72 [25600/50048]	Loss: 0.1627
Training Epoch: 72 [25728/50048]	Loss: 0.0895
Training Epoch: 72 [25856/50048]	Loss: 0.0646
Training Epoch: 72 [25984/50048]	Loss: 0.1367
Training Epoch: 72 [26112/50048]	Loss: 0.1386
Training Epoch: 72 [26240/50048]	Loss: 0.1795
Training Epoch: 72 [26368/50048]	Loss: 0.1102
Training Epoch: 72 [26496/50048]	Loss: 0.0748
Training Epoch: 72 [26624/50048]	Loss: 0.2713
Training Epoch: 72 [26752/50048]	Loss: 0.1040
Training Epoch: 72 [26880/50048]	Loss: 0.1552
Training Epoch: 72 [27008/50048]	Loss: 0.2066
Training Epoch: 72 [27136/50048]	Loss: 0.0746
Training Epoch: 72 [27264/50048]	Loss: 0.0892
Training Epoch: 72 [27392/50048]	Loss: 0.1853
Training Epoch: 72 [27520/50048]	Loss: 0.0955
Training Epoch: 72 [27648/50048]	Loss: 0.1455
Training Epoch: 72 [27776/50048]	Loss: 0.1830
Training Epoch: 72 [27904/50048]	Loss: 0.1237
Training Epoch: 72 [28032/50048]	Loss: 0.1019
Training Epoch: 72 [28160/50048]	Loss: 0.0562
Training Epoch: 72 [28288/50048]	Loss: 0.1028
Training Epoch: 72 [28416/50048]	Loss: 0.0459
Training Epoch: 72 [28544/50048]	Loss: 0.2007
Training Epoch: 72 [28672/50048]	Loss: 0.2120
Training Epoch: 72 [28800/50048]	Loss: 0.0919
Training Epoch: 72 [28928/50048]	Loss: 0.0899
Training Epoch: 72 [29056/50048]	Loss: 0.1630
Training Epoch: 72 [29184/50048]	Loss: 0.1240
Training Epoch: 72 [29312/50048]	Loss: 0.0900
Training Epoch: 72 [29440/50048]	Loss: 0.1128
Training Epoch: 72 [29568/50048]	Loss: 0.0810
Training Epoch: 72 [29696/50048]	Loss: 0.1017
Training Epoch: 72 [29824/50048]	Loss: 0.0806
Training Epoch: 72 [29952/50048]	Loss: 0.1294
Training Epoch: 72 [30080/50048]	Loss: 0.0449
Training Epoch: 72 [30208/50048]	Loss: 0.1878
Training Epoch: 72 [30336/50048]	Loss: 0.1456
Training Epoch: 72 [30464/50048]	Loss: 0.1635
Training Epoch: 72 [30592/50048]	Loss: 0.1078
Training Epoch: 72 [30720/50048]	Loss: 0.1921
Training Epoch: 72 [30848/50048]	Loss: 0.1233
Training Epoch: 72 [30976/50048]	Loss: 0.1844
Training Epoch: 72 [31104/50048]	Loss: 0.1241
Training Epoch: 72 [31232/50048]	Loss: 0.0902
Training Epoch: 72 [31360/50048]	Loss: 0.2303
Training Epoch: 72 [31488/50048]	Loss: 0.1215
Training Epoch: 72 [31616/50048]	Loss: 0.0389
Training Epoch: 72 [31744/50048]	Loss: 0.1195
Training Epoch: 72 [31872/50048]	Loss: 0.1345
Training Epoch: 72 [32000/50048]	Loss: 0.1286
Training Epoch: 72 [32128/50048]	Loss: 0.0811
Training Epoch: 72 [32256/50048]	Loss: 0.1908
Training Epoch: 72 [32384/50048]	Loss: 0.1707
Training Epoch: 72 [32512/50048]	Loss: 0.0471
Training Epoch: 72 [32640/50048]	Loss: 0.1317
Training Epoch: 72 [32768/50048]	Loss: 0.1713
Training Epoch: 72 [32896/50048]	Loss: 0.2019
Training Epoch: 72 [33024/50048]	Loss: 0.1182
Training Epoch: 72 [33152/50048]	Loss: 0.0905
Training Epoch: 72 [33280/50048]	Loss: 0.0586
Training Epoch: 72 [33408/50048]	Loss: 0.1270
Training Epoch: 72 [33536/50048]	Loss: 0.1251
Training Epoch: 72 [33664/50048]	Loss: 0.1582
Training Epoch: 72 [33792/50048]	Loss: 0.1929
Training Epoch: 72 [33920/50048]	Loss: 0.2189
Training Epoch: 72 [34048/50048]	Loss: 0.2005
Training Epoch: 72 [34176/50048]	Loss: 0.1117
Training Epoch: 72 [34304/50048]	Loss: 0.1263
Training Epoch: 72 [34432/50048]	Loss: 0.1339
Training Epoch: 72 [34560/50048]	Loss: 0.0823
Training Epoch: 72 [34688/50048]	Loss: 0.1341
Training Epoch: 72 [34816/50048]	Loss: 0.1462
Training Epoch: 72 [34944/50048]	Loss: 0.1160
Training Epoch: 72 [35072/50048]	Loss: 0.1825
Training Epoch: 72 [35200/50048]	Loss: 0.1889
Training Epoch: 72 [35328/50048]	Loss: 0.2310
Training Epoch: 72 [35456/50048]	Loss: 0.1307
Training Epoch: 72 [35584/50048]	Loss: 0.1458
Training Epoch: 72 [35712/50048]	Loss: 0.1064
Training Epoch: 72 [35840/50048]	Loss: 0.1744
Training Epoch: 72 [35968/50048]	Loss: 0.1803
Training Epoch: 72 [36096/50048]	Loss: 0.1679
Training Epoch: 72 [36224/50048]	Loss: 0.1748
Training Epoch: 72 [36352/50048]	Loss: 0.0934
Training Epoch: 72 [36480/50048]	Loss: 0.1528
Training Epoch: 72 [36608/50048]	Loss: 0.0668
Training Epoch: 72 [36736/50048]	Loss: 0.2095
Training Epoch: 72 [36864/50048]	Loss: 0.1758
Training Epoch: 72 [36992/50048]	Loss: 0.0879
Training Epoch: 72 [37120/50048]	Loss: 0.1800
Training Epoch: 72 [37248/50048]	Loss: 0.1380
Training Epoch: 72 [37376/50048]	Loss: 0.1087
Training Epoch: 72 [37504/50048]	Loss: 0.0718
Training Epoch: 72 [37632/50048]	Loss: 0.1299
Training Epoch: 72 [37760/50048]	Loss: 0.1015
Training Epoch: 72 [37888/50048]	Loss: 0.1943
Training Epoch: 72 [38016/50048]	Loss: 0.1557
Training Epoch: 72 [38144/50048]	Loss: 0.2010
Training Epoch: 72 [38272/50048]	Loss: 0.1387
Training Epoch: 72 [38400/50048]	Loss: 0.1305
Training Epoch: 72 [38528/50048]	Loss: 0.1136
Training Epoch: 72 [38656/50048]	Loss: 0.2028
Training Epoch: 72 [38784/50048]	Loss: 0.1671
Training Epoch: 72 [38912/50048]	Loss: 0.1810
Training Epoch: 72 [39040/50048]	Loss: 0.0775
Training Epoch: 72 [39168/50048]	Loss: 0.1360
Training Epoch: 72 [39296/50048]	Loss: 0.1841
Training Epoch: 72 [39424/50048]	Loss: 0.0938
Training Epoch: 72 [39552/50048]	Loss: 0.1134
Training Epoch: 72 [39680/50048]	Loss: 0.2136
Training Epoch: 72 [39808/50048]	Loss: 0.1236
Training Epoch: 72 [39936/50048]	Loss: 0.1913
Training Epoch: 72 [40064/50048]	Loss: 0.1676
Training Epoch: 72 [40192/50048]	Loss: 0.0671
Training Epoch: 72 [40320/50048]	Loss: 0.1663
Training Epoch: 72 [40448/50048]	Loss: 0.1681
Training Epoch: 72 [40576/50048]	Loss: 0.1913
Training Epoch: 72 [40704/50048]	Loss: 0.0923
Training Epoch: 72 [40832/50048]	Loss: 0.2579
Training Epoch: 72 [40960/50048]	Loss: 0.1350
Training Epoch: 72 [41088/50048]	Loss: 0.1415
Training Epoch: 72 [41216/50048]	Loss: 0.1903
Training Epoch: 72 [41344/50048]	Loss: 0.0968
Training Epoch: 72 [41472/50048]	Loss: 0.0989
Training Epoch: 72 [41600/50048]	Loss: 0.1296
Training Epoch: 72 [41728/50048]	Loss: 0.1022
Training Epoch: 72 [41856/50048]	Loss: 0.0879
Training Epoch: 72 [41984/50048]	Loss: 0.1092
Training Epoch: 72 [42112/50048]	Loss: 0.1591
Training Epoch: 72 [42240/50048]	Loss: 0.1286
Training Epoch: 72 [42368/50048]	Loss: 0.0930
Training Epoch: 72 [42496/50048]	Loss: 0.1316
Training Epoch: 72 [42624/50048]	Loss: 0.1488
Training Epoch: 72 [42752/50048]	Loss: 0.1438
Training Epoch: 72 [42880/50048]	Loss: 0.1642
Training Epoch: 72 [43008/50048]	Loss: 0.0953
Training Epoch: 72 [43136/50048]	Loss: 0.1640
Training Epoch: 72 [43264/50048]	Loss: 0.1703
Training Epoch: 72 [43392/50048]	Loss: 0.1545
Training Epoch: 72 [43520/50048]	Loss: 0.0855
Training Epoch: 72 [43648/50048]	Loss: 0.1610
Training Epoch: 72 [43776/50048]	Loss: 0.1018
Training Epoch: 72 [43904/50048]	Loss: 0.1067
Training Epoch: 72 [44032/50048]	Loss: 0.2097
Training Epoch: 72 [44160/50048]	Loss: 0.1217
Training Epoch: 72 [44288/50048]	Loss: 0.1396
Training Epoch: 72 [44416/50048]	Loss: 0.0949
Training Epoch: 72 [44544/50048]	Loss: 0.2551
Training Epoch: 72 [44672/50048]	Loss: 0.1382
Training Epoch: 72 [44800/50048]	Loss: 0.0964
Training Epoch: 72 [44928/50048]	Loss: 0.2437
Training Epoch: 72 [45056/50048]	Loss: 0.1615
Training Epoch: 72 [45184/50048]	Loss: 0.1682
Training Epoch: 72 [45312/50048]	Loss: 0.2007
Training Epoch: 72 [45440/50048]	Loss: 0.1402
Training Epoch: 72 [45568/50048]	Loss: 0.0974
Training Epoch: 72 [45696/50048]	Loss: 0.1083
2022-12-06 05:27:06,746 [ZeusDataLoader(train)] train epoch 73 done: time=86.45 energy=10497.93
2022-12-06 05:27:06,748 [ZeusDataLoader(eval)] Epoch 73 begin.
Training Epoch: 72 [45824/50048]	Loss: 0.1175
Training Epoch: 72 [45952/50048]	Loss: 0.1664
Training Epoch: 72 [46080/50048]	Loss: 0.1870
Training Epoch: 72 [46208/50048]	Loss: 0.1314
Training Epoch: 72 [46336/50048]	Loss: 0.1180
Training Epoch: 72 [46464/50048]	Loss: 0.2424
Training Epoch: 72 [46592/50048]	Loss: 0.1848
Training Epoch: 72 [46720/50048]	Loss: 0.2162
Training Epoch: 72 [46848/50048]	Loss: 0.1553
Training Epoch: 72 [46976/50048]	Loss: 0.1574
Training Epoch: 72 [47104/50048]	Loss: 0.1246
Training Epoch: 72 [47232/50048]	Loss: 0.1255
Training Epoch: 72 [47360/50048]	Loss: 0.1217
Training Epoch: 72 [47488/50048]	Loss: 0.1394
Training Epoch: 72 [47616/50048]	Loss: 0.1421
Training Epoch: 72 [47744/50048]	Loss: 0.1714
Training Epoch: 72 [47872/50048]	Loss: 0.1362
Training Epoch: 72 [48000/50048]	Loss: 0.1793
Training Epoch: 72 [48128/50048]	Loss: 0.0788
Training Epoch: 72 [48256/50048]	Loss: 0.1254
Training Epoch: 72 [48384/50048]	Loss: 0.1105
Training Epoch: 72 [48512/50048]	Loss: 0.1577
Training Epoch: 72 [48640/50048]	Loss: 0.2491
Training Epoch: 72 [48768/50048]	Loss: 0.1634
Training Epoch: 72 [48896/50048]	Loss: 0.1368
Training Epoch: 72 [49024/50048]	Loss: 0.1822
Training Epoch: 72 [49152/50048]	Loss: 0.1545
Training Epoch: 72 [49280/50048]	Loss: 0.1310
Training Epoch: 72 [49408/50048]	Loss: 0.2035
Training Epoch: 72 [49536/50048]	Loss: 0.1732
Training Epoch: 72 [49664/50048]	Loss: 0.1131
Training Epoch: 72 [49792/50048]	Loss: 0.1408
Training Epoch: 72 [49920/50048]	Loss: 0.1543
Training Epoch: 72 [50048/50048]	Loss: 0.1011
2022-12-06 10:27:10.392 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:27:10,402 [ZeusDataLoader(eval)] eval epoch 73 done: time=3.65 energy=442.58
2022-12-06 05:27:10,402 [ZeusDataLoader(train)] Up to epoch 73: time=6586.54, energy=799561.53, cost=976102.69
2022-12-06 05:27:10,402 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:27:10,402 [ZeusDataLoader(train)] Expected next epoch: time=6676.33, energy=810359.54, cost=989359.08
2022-12-06 05:27:10,403 [ZeusDataLoader(train)] Epoch 74 begin.
Validation Epoch: 72, Average loss: 0.0176, Accuracy: 0.6381
2022-12-06 05:27:10,580 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:27:10,580 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:27:10.582 [ZeusMonitor] Monitor started.
2022-12-06 10:27:10.582 [ZeusMonitor] Running indefinitely. 2022-12-06 10:27:10.582 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:27:10.582 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e74+gpu0.power.log
Training Epoch: 73 [128/50048]	Loss: 0.1247
Training Epoch: 73 [256/50048]	Loss: 0.1628
Training Epoch: 73 [384/50048]	Loss: 0.1526
Training Epoch: 73 [512/50048]	Loss: 0.0611
Training Epoch: 73 [640/50048]	Loss: 0.1128
Training Epoch: 73 [768/50048]	Loss: 0.0756
Training Epoch: 73 [896/50048]	Loss: 0.0903
Training Epoch: 73 [1024/50048]	Loss: 0.0888
Training Epoch: 73 [1152/50048]	Loss: 0.0663
Training Epoch: 73 [1280/50048]	Loss: 0.1252
Training Epoch: 73 [1408/50048]	Loss: 0.1477
Training Epoch: 73 [1536/50048]	Loss: 0.1487
Training Epoch: 73 [1664/50048]	Loss: 0.0970
Training Epoch: 73 [1792/50048]	Loss: 0.1102
Training Epoch: 73 [1920/50048]	Loss: 0.1438
Training Epoch: 73 [2048/50048]	Loss: 0.1208
Training Epoch: 73 [2176/50048]	Loss: 0.1096
Training Epoch: 73 [2304/50048]	Loss: 0.2161
Training Epoch: 73 [2432/50048]	Loss: 0.1490
Training Epoch: 73 [2560/50048]	Loss: 0.1389
Training Epoch: 73 [2688/50048]	Loss: 0.1428
Training Epoch: 73 [2816/50048]	Loss: 0.0811
Training Epoch: 73 [2944/50048]	Loss: 0.0847
Training Epoch: 73 [3072/50048]	Loss: 0.0824
Training Epoch: 73 [3200/50048]	Loss: 0.0787
Training Epoch: 73 [3328/50048]	Loss: 0.1218
Training Epoch: 73 [3456/50048]	Loss: 0.0932
Training Epoch: 73 [3584/50048]	Loss: 0.1753
Training Epoch: 73 [3712/50048]	Loss: 0.1325
Training Epoch: 73 [3840/50048]	Loss: 0.0720
Training Epoch: 73 [3968/50048]	Loss: 0.0993
Training Epoch: 73 [4096/50048]	Loss: 0.0976
Training Epoch: 73 [4224/50048]	Loss: 0.0963
Training Epoch: 73 [4352/50048]	Loss: 0.1410
Training Epoch: 73 [4480/50048]	Loss: 0.1167
Training Epoch: 73 [4608/50048]	Loss: 0.1426
Training Epoch: 73 [4736/50048]	Loss: 0.1160
Training Epoch: 73 [4864/50048]	Loss: 0.0949
Training Epoch: 73 [4992/50048]	Loss: 0.0932
Training Epoch: 73 [5120/50048]	Loss: 0.1239
Training Epoch: 73 [5248/50048]	Loss: 0.0663
Training Epoch: 73 [5376/50048]	Loss: 0.0945
Training Epoch: 73 [5504/50048]	Loss: 0.1217
Training Epoch: 73 [5632/50048]	Loss: 0.0593
Training Epoch: 73 [5760/50048]	Loss: 0.1106
Training Epoch: 73 [5888/50048]	Loss: 0.0681
Training Epoch: 73 [6016/50048]	Loss: 0.0963
Training Epoch: 73 [6144/50048]	Loss: 0.1316
Training Epoch: 73 [6272/50048]	Loss: 0.0987
Training Epoch: 73 [6400/50048]	Loss: 0.1353
Training Epoch: 73 [6528/50048]	Loss: 0.1242
Training Epoch: 73 [6656/50048]	Loss: 0.1091
Training Epoch: 73 [6784/50048]	Loss: 0.0566
Training Epoch: 73 [6912/50048]	Loss: 0.0975
Training Epoch: 73 [7040/50048]	Loss: 0.0935
Training Epoch: 73 [7168/50048]	Loss: 0.0956
Training Epoch: 73 [7296/50048]	Loss: 0.0958
Training Epoch: 73 [7424/50048]	Loss: 0.0570
Training Epoch: 73 [7552/50048]	Loss: 0.0455
Training Epoch: 73 [7680/50048]	Loss: 0.0544
Training Epoch: 73 [7808/50048]	Loss: 0.1084
Training Epoch: 73 [7936/50048]	Loss: 0.1576
Training Epoch: 73 [8064/50048]	Loss: 0.1429
Training Epoch: 73 [8192/50048]	Loss: 0.0712
Training Epoch: 73 [8320/50048]	Loss: 0.1360
Training Epoch: 73 [8448/50048]	Loss: 0.1246
Training Epoch: 73 [8576/50048]	Loss: 0.0479
Training Epoch: 73 [8704/50048]	Loss: 0.0810
Training Epoch: 73 [8832/50048]	Loss: 0.1103
Training Epoch: 73 [8960/50048]	Loss: 0.0986
Training Epoch: 73 [9088/50048]	Loss: 0.1620
Training Epoch: 73 [9216/50048]	Loss: 0.0690
Training Epoch: 73 [9344/50048]	Loss: 0.1839
Training Epoch: 73 [9472/50048]	Loss: 0.1272
Training Epoch: 73 [9600/50048]	Loss: 0.1265
Training Epoch: 73 [9728/50048]	Loss: 0.1568
Training Epoch: 73 [9856/50048]	Loss: 0.0959
Training Epoch: 73 [9984/50048]	Loss: 0.1492
Training Epoch: 73 [10112/50048]	Loss: 0.0889
Training Epoch: 73 [10240/50048]	Loss: 0.2069
Training Epoch: 73 [10368/50048]	Loss: 0.1870
Training Epoch: 73 [10496/50048]	Loss: 0.1602
Training Epoch: 73 [10624/50048]	Loss: 0.0421
Training Epoch: 73 [10752/50048]	Loss: 0.1046
Training Epoch: 73 [10880/50048]	Loss: 0.1334
Training Epoch: 73 [11008/50048]	Loss: 0.0751
Training Epoch: 73 [11136/50048]	Loss: 0.1185
Training Epoch: 73 [11264/50048]	Loss: 0.1094
Training Epoch: 73 [11392/50048]	Loss: 0.1241
Training Epoch: 73 [11520/50048]	Loss: 0.1808
Training Epoch: 73 [11648/50048]	Loss: 0.0870
Training Epoch: 73 [11776/50048]	Loss: 0.1996
Training Epoch: 73 [11904/50048]	Loss: 0.0989
Training Epoch: 73 [12032/50048]	Loss: 0.1787
Training Epoch: 73 [12160/50048]	Loss: 0.1148
Training Epoch: 73 [12288/50048]	Loss: 0.1733
Training Epoch: 73 [12416/50048]	Loss: 0.0522
Training Epoch: 73 [12544/50048]	Loss: 0.1255
Training Epoch: 73 [12672/50048]	Loss: 0.1316
Training Epoch: 73 [12800/50048]	Loss: 0.1566
Training Epoch: 73 [12928/50048]	Loss: 0.1198
Training Epoch: 73 [13056/50048]	Loss: 0.1082
Training Epoch: 73 [13184/50048]	Loss: 0.2014
Training Epoch: 73 [13312/50048]	Loss: 0.2247
Training Epoch: 73 [13440/50048]	Loss: 0.2994
Training Epoch: 73 [13568/50048]	Loss: 0.1745
Training Epoch: 73 [13696/50048]	Loss: 0.1966
Training Epoch: 73 [13824/50048]	Loss: 0.1831
Training Epoch: 73 [13952/50048]	Loss: 0.1122
Training Epoch: 73 [14080/50048]	Loss: 0.0700
Training Epoch: 73 [14208/50048]	Loss: 0.1179
Training Epoch: 73 [14336/50048]	Loss: 0.2121
Training Epoch: 73 [14464/50048]	Loss: 0.0835
Training Epoch: 73 [14592/50048]	Loss: 0.0957
Training Epoch: 73 [14720/50048]	Loss: 0.0804
Training Epoch: 73 [14848/50048]	Loss: 0.0901
Training Epoch: 73 [14976/50048]	Loss: 0.1466
Training Epoch: 73 [15104/50048]	Loss: 0.1127
Training Epoch: 73 [15232/50048]	Loss: 0.0573
Training Epoch: 73 [15360/50048]	Loss: 0.1057
Training Epoch: 73 [15488/50048]	Loss: 0.0930
Training Epoch: 73 [15616/50048]	Loss: 0.1793
Training Epoch: 73 [15744/50048]	Loss: 0.1957
Training Epoch: 73 [15872/50048]	Loss: 0.1622
Training Epoch: 73 [16000/50048]	Loss: 0.1474
Training Epoch: 73 [16128/50048]	Loss: 0.1444
Training Epoch: 73 [16256/50048]	Loss: 0.0942
Training Epoch: 73 [16384/50048]	Loss: 0.1132
Training Epoch: 73 [16512/50048]	Loss: 0.0727
Training Epoch: 73 [16640/50048]	Loss: 0.1046
Training Epoch: 73 [16768/50048]	Loss: 0.0631
Training Epoch: 73 [16896/50048]	Loss: 0.1370
Training Epoch: 73 [17024/50048]	Loss: 0.1124
Training Epoch: 73 [17152/50048]	Loss: 0.1011
Training Epoch: 73 [17280/50048]	Loss: 0.1593
Training Epoch: 73 [17408/50048]	Loss: 0.1145
Training Epoch: 73 [17536/50048]	Loss: 0.0806
Training Epoch: 73 [17664/50048]	Loss: 0.0853
Training Epoch: 73 [17792/50048]	Loss: 0.1312
Training Epoch: 73 [17920/50048]	Loss: 0.1346
Training Epoch: 73 [18048/50048]	Loss: 0.1157
Training Epoch: 73 [18176/50048]	Loss: 0.0733
Training Epoch: 73 [18304/50048]	Loss: 0.0737
Training Epoch: 73 [18432/50048]	Loss: 0.1210
Training Epoch: 73 [18560/50048]	Loss: 0.1585
Training Epoch: 73 [18688/50048]	Loss: 0.1164
Training Epoch: 73 [18816/50048]	Loss: 0.0887
Training Epoch: 73 [18944/50048]	Loss: 0.1179
Training Epoch: 73 [19072/50048]	Loss: 0.0661
Training Epoch: 73 [19200/50048]	Loss: 0.1091
Training Epoch: 73 [19328/50048]	Loss: 0.1114
Training Epoch: 73 [19456/50048]	Loss: 0.1277
Training Epoch: 73 [19584/50048]	Loss: 0.0472
Training Epoch: 73 [19712/50048]	Loss: 0.1172
Training Epoch: 73 [19840/50048]	Loss: 0.1361
Training Epoch: 73 [19968/50048]	Loss: 0.0708
Training Epoch: 73 [20096/50048]	Loss: 0.1228
Training Epoch: 73 [20224/50048]	Loss: 0.1981
Training Epoch: 73 [20352/50048]	Loss: 0.1522
Training Epoch: 73 [20480/50048]	Loss: 0.1094
Training Epoch: 73 [20608/50048]	Loss: 0.1361
Training Epoch: 73 [20736/50048]	Loss: 0.2107
Training Epoch: 73 [20864/50048]	Loss: 0.1167
Training Epoch: 73 [20992/50048]	Loss: 0.1509
Training Epoch: 73 [21120/50048]	Loss: 0.2149
Training Epoch: 73 [21248/50048]	Loss: 0.1506
Training Epoch: 73 [21376/50048]	Loss: 0.1801
Training Epoch: 73 [21504/50048]	Loss: 0.0912
Training Epoch: 73 [21632/50048]	Loss: 0.1162
Training Epoch: 73 [21760/50048]	Loss: 0.0789
Training Epoch: 73 [21888/50048]	Loss: 0.1445
Training Epoch: 73 [22016/50048]	Loss: 0.0781
Training Epoch: 73 [22144/50048]	Loss: 0.1912
Training Epoch: 73 [22272/50048]	Loss: 0.1667
Training Epoch: 73 [22400/50048]	Loss: 0.0969
Training Epoch: 73 [22528/50048]	Loss: 0.0614
Training Epoch: 73 [22656/50048]	Loss: 0.0971
Training Epoch: 73 [22784/50048]	Loss: 0.0932
Training Epoch: 73 [22912/50048]	Loss: 0.0829
Training Epoch: 73 [23040/50048]	Loss: 0.0819
Training Epoch: 73 [23168/50048]	Loss: 0.1164
Training Epoch: 73 [23296/50048]	Loss: 0.1537
Training Epoch: 73 [23424/50048]	Loss: 0.1731
Training Epoch: 73 [23552/50048]	Loss: 0.0791
Training Epoch: 73 [23680/50048]	Loss: 0.1702
Training Epoch: 73 [23808/50048]	Loss: 0.0964
Training Epoch: 73 [23936/50048]	Loss: 0.1564
Training Epoch: 73 [24064/50048]	Loss: 0.0941
Training Epoch: 73 [24192/50048]	Loss: 0.1750
Training Epoch: 73 [24320/50048]	Loss: 0.1594
Training Epoch: 73 [24448/50048]	Loss: 0.0972
Training Epoch: 73 [24576/50048]	Loss: 0.1189
Training Epoch: 73 [24704/50048]	Loss: 0.1286
Training Epoch: 73 [24832/50048]	Loss: 0.1579
Training Epoch: 73 [24960/50048]	Loss: 0.1882
Training Epoch: 73 [25088/50048]	Loss: 0.2265
Training Epoch: 73 [25216/50048]	Loss: 0.1014
Training Epoch: 73 [25344/50048]	Loss: 0.1056
Training Epoch: 73 [25472/50048]	Loss: 0.0967
Training Epoch: 73 [25600/50048]	Loss: 0.1704
Training Epoch: 73 [25728/50048]	Loss: 0.2052
Training Epoch: 73 [25856/50048]	Loss: 0.1391
Training Epoch: 73 [25984/50048]	Loss: 0.0856
Training Epoch: 73 [26112/50048]	Loss: 0.1938
Training Epoch: 73 [26240/50048]	Loss: 0.1494
Training Epoch: 73 [26368/50048]	Loss: 0.2249
Training Epoch: 73 [26496/50048]	Loss: 0.1873
Training Epoch: 73 [26624/50048]	Loss: 0.1200
Training Epoch: 73 [26752/50048]	Loss: 0.1505
Training Epoch: 73 [26880/50048]	Loss: 0.1254
Training Epoch: 73 [27008/50048]	Loss: 0.1456
Training Epoch: 73 [27136/50048]	Loss: 0.1039
Training Epoch: 73 [27264/50048]	Loss: 0.1829
Training Epoch: 73 [27392/50048]	Loss: 0.0770
Training Epoch: 73 [27520/50048]	Loss: 0.1378
Training Epoch: 73 [27648/50048]	Loss: 0.1842
Training Epoch: 73 [27776/50048]	Loss: 0.1045
Training Epoch: 73 [27904/50048]	Loss: 0.1068
Training Epoch: 73 [28032/50048]	Loss: 0.2373
Training Epoch: 73 [28160/50048]	Loss: 0.0991
Training Epoch: 73 [28288/50048]	Loss: 0.2306
Training Epoch: 73 [28416/50048]	Loss: 0.1915
Training Epoch: 73 [28544/50048]	Loss: 0.1080
Training Epoch: 73 [28672/50048]	Loss: 0.0977
Training Epoch: 73 [28800/50048]	Loss: 0.1072
Training Epoch: 73 [28928/50048]	Loss: 0.1772
Training Epoch: 73 [29056/50048]	Loss: 0.1987
Training Epoch: 73 [29184/50048]	Loss: 0.1181
Training Epoch: 73 [29312/50048]	Loss: 0.1058
Training Epoch: 73 [29440/50048]	Loss: 0.0474
Training Epoch: 73 [29568/50048]	Loss: 0.1559
Training Epoch: 73 [29696/50048]	Loss: 0.1613
Training Epoch: 73 [29824/50048]	Loss: 0.1060
Training Epoch: 73 [29952/50048]	Loss: 0.1164
Training Epoch: 73 [30080/50048]	Loss: 0.1545
Training Epoch: 73 [30208/50048]	Loss: 0.1059
Training Epoch: 73 [30336/50048]	Loss: 0.1171
Training Epoch: 73 [30464/50048]	Loss: 0.0759
Training Epoch: 73 [30592/50048]	Loss: 0.0888
Training Epoch: 73 [30720/50048]	Loss: 0.0976
Training Epoch: 73 [30848/50048]	Loss: 0.1201
Training Epoch: 73 [30976/50048]	Loss: 0.1996
Training Epoch: 73 [31104/50048]	Loss: 0.0932
Training Epoch: 73 [31232/50048]	Loss: 0.0633
Training Epoch: 73 [31360/50048]	Loss: 0.1432
Training Epoch: 73 [31488/50048]	Loss: 0.1423
Training Epoch: 73 [31616/50048]	Loss: 0.0918
Training Epoch: 73 [31744/50048]	Loss: 0.0739
Training Epoch: 73 [31872/50048]	Loss: 0.1372
Training Epoch: 73 [32000/50048]	Loss: 0.1177
Training Epoch: 73 [32128/50048]	Loss: 0.0773
Training Epoch: 73 [32256/50048]	Loss: 0.0938
Training Epoch: 73 [32384/50048]	Loss: 0.1820
Training Epoch: 73 [32512/50048]	Loss: 0.1461
Training Epoch: 73 [32640/50048]	Loss: 0.1585
Training Epoch: 73 [32768/50048]	Loss: 0.1298
Training Epoch: 73 [32896/50048]	Loss: 0.1782
Training Epoch: 73 [33024/50048]	Loss: 0.1942
Training Epoch: 73 [33152/50048]	Loss: 0.0451
Training Epoch: 73 [33280/50048]	Loss: 0.0663
Training Epoch: 73 [33408/50048]	Loss: 0.1295
Training Epoch: 73 [33536/50048]	Loss: 0.1549
Training Epoch: 73 [33664/50048]	Loss: 0.0928
Training Epoch: 73 [33792/50048]	Loss: 0.1050
Training Epoch: 73 [33920/50048]	Loss: 0.1773
Training Epoch: 73 [34048/50048]	Loss: 0.0677
Training Epoch: 73 [34176/50048]	Loss: 0.3006
Training Epoch: 73 [34304/50048]	Loss: 0.1330
Training Epoch: 73 [34432/50048]	Loss: 0.1292
Training Epoch: 73 [34560/50048]	Loss: 0.1234
Training Epoch: 73 [34688/50048]	Loss: 0.1093
Training Epoch: 73 [34816/50048]	Loss: 0.1422
Training Epoch: 73 [34944/50048]	Loss: 0.1173
Training Epoch: 73 [35072/50048]	Loss: 0.1180
Training Epoch: 73 [35200/50048]	Loss: 0.1635
Training Epoch: 73 [35328/50048]	Loss: 0.1758
Training Epoch: 73 [35456/50048]	Loss: 0.0892
Training Epoch: 73 [35584/50048]	Loss: 0.0664
Training Epoch: 73 [35712/50048]	Loss: 0.2157
Training Epoch: 73 [35840/50048]	Loss: 0.0686
Training Epoch: 73 [35968/50048]	Loss: 0.0899
Training Epoch: 73 [36096/50048]	Loss: 0.1131
Training Epoch: 73 [36224/50048]	Loss: 0.1343
Training Epoch: 73 [36352/50048]	Loss: 0.1259
Training Epoch: 73 [36480/50048]	Loss: 0.1549
Training Epoch: 73 [36608/50048]	Loss: 0.0916
Training Epoch: 73 [36736/50048]	Loss: 0.1212
Training Epoch: 73 [36864/50048]	Loss: 0.1727
Training Epoch: 73 [36992/50048]	Loss: 0.1505
Training Epoch: 73 [37120/50048]	Loss: 0.1563
Training Epoch: 73 [37248/50048]	Loss: 0.1312
Training Epoch: 73 [37376/50048]	Loss: 0.1748
Training Epoch: 73 [37504/50048]	Loss: 0.1434
Training Epoch: 73 [37632/50048]	Loss: 0.2102
Training Epoch: 73 [37760/50048]	Loss: 0.1613
Training Epoch: 73 [37888/50048]	Loss: 0.2003
Training Epoch: 73 [38016/50048]	Loss: 0.2119
Training Epoch: 73 [38144/50048]	Loss: 0.1254
Training Epoch: 73 [38272/50048]	Loss: 0.1021
Training Epoch: 73 [38400/50048]	Loss: 0.1748
Training Epoch: 73 [38528/50048]	Loss: 0.1934
Training Epoch: 73 [38656/50048]	Loss: 0.1653
Training Epoch: 73 [38784/50048]	Loss: 0.1170
Training Epoch: 73 [38912/50048]	Loss: 0.1784
Training Epoch: 73 [39040/50048]	Loss: 0.1376
Training Epoch: 73 [39168/50048]	Loss: 0.0913
Training Epoch: 73 [39296/50048]	Loss: 0.1183
Training Epoch: 73 [39424/50048]	Loss: 0.1398
Training Epoch: 73 [39552/50048]	Loss: 0.1711
Training Epoch: 73 [39680/50048]	Loss: 0.1338
Training Epoch: 73 [39808/50048]	Loss: 0.2832
Training Epoch: 73 [39936/50048]	Loss: 0.1255
Training Epoch: 73 [40064/50048]	Loss: 0.1152
Training Epoch: 73 [40192/50048]	Loss: 0.0936
Training Epoch: 73 [40320/50048]	Loss: 0.1198
Training Epoch: 73 [40448/50048]	Loss: 0.1056
Training Epoch: 73 [40576/50048]	Loss: 0.1549
Training Epoch: 73 [40704/50048]	Loss: 0.1871
Training Epoch: 73 [40832/50048]	Loss: 0.0936
Training Epoch: 73 [40960/50048]	Loss: 0.1550
Training Epoch: 73 [41088/50048]	Loss: 0.1838
Training Epoch: 73 [41216/50048]	Loss: 0.1940
Training Epoch: 73 [41344/50048]	Loss: 0.2394
Training Epoch: 73 [41472/50048]	Loss: 0.1986
Training Epoch: 73 [41600/50048]	Loss: 0.0958
Training Epoch: 73 [41728/50048]	Loss: 0.2138
Training Epoch: 73 [41856/50048]	Loss: 0.1455
Training Epoch: 73 [41984/50048]	Loss: 0.0595
Training Epoch: 73 [42112/50048]	Loss: 0.1436
Training Epoch: 73 [42240/50048]	Loss: 0.1397
Training Epoch: 73 [42368/50048]	Loss: 0.0812
Training Epoch: 73 [42496/50048]	Loss: 0.1206
Training Epoch: 73 [42624/50048]	Loss: 0.1325
Training Epoch: 73 [42752/50048]	Loss: 0.1489
Training Epoch: 73 [42880/50048]	Loss: 0.1042
Training Epoch: 73 [43008/50048]	Loss: 0.1431
Training Epoch: 73 [43136/50048]	Loss: 0.0923
Training Epoch: 73 [43264/50048]	Loss: 0.0914
Training Epoch: 73 [43392/50048]	Loss: 0.1173
Training Epoch: 73 [43520/50048]	Loss: 0.1446
Training Epoch: 73 [43648/50048]	Loss: 0.0894
Training Epoch: 73 [43776/50048]	Loss: 0.1297
Training Epoch: 73 [43904/50048]	Loss: 0.2374
Training Epoch: 73 [44032/50048]	Loss: 0.1539
Training Epoch: 73 [44160/50048]	Loss: 0.1093
Training Epoch: 73 [44288/50048]	Loss: 0.1123
Training Epoch: 73 [44416/50048]	Loss: 0.1684
Training Epoch: 73 [44544/50048]	Loss: 0.1126
Training Epoch: 73 [44672/50048]	Loss: 0.1242
Training Epoch: 73 [44800/50048]	Loss: 0.1147
Training Epoch: 73 [44928/50048]	Loss: 0.1360
Training Epoch: 73 [45056/50048]	Loss: 0.0678
Training Epoch: 73 [45184/50048]	Loss: 0.1290
Training Epoch: 73 [45312/50048]	Loss: 0.0698
Training Epoch: 73 [45440/50048]	Loss: 0.1074
Training Epoch: 73 [45568/50048]	Loss: 0.1443
Training Epoch: 73 [45696/50048]	Loss: 0.1220
2022-12-06 05:28:36,911 [ZeusDataLoader(train)] train epoch 74 done: time=86.50 energy=10494.02
2022-12-06 05:28:36,913 [ZeusDataLoader(eval)] Epoch 74 begin.
Training Epoch: 73 [45824/50048]	Loss: 0.2454
Training Epoch: 73 [45952/50048]	Loss: 0.1517
Training Epoch: 73 [46080/50048]	Loss: 0.1301
Training Epoch: 73 [46208/50048]	Loss: 0.1728
Training Epoch: 73 [46336/50048]	Loss: 0.2765
Training Epoch: 73 [46464/50048]	Loss: 0.1571
Training Epoch: 73 [46592/50048]	Loss: 0.0852
Training Epoch: 73 [46720/50048]	Loss: 0.2111
Training Epoch: 73 [46848/50048]	Loss: 0.0978
Training Epoch: 73 [46976/50048]	Loss: 0.1533
Training Epoch: 73 [47104/50048]	Loss: 0.0882
Training Epoch: 73 [47232/50048]	Loss: 0.0900
Training Epoch: 73 [47360/50048]	Loss: 0.1833
Training Epoch: 73 [47488/50048]	Loss: 0.1405
Training Epoch: 73 [47616/50048]	Loss: 0.0879
Training Epoch: 73 [47744/50048]	Loss: 0.2321
Training Epoch: 73 [47872/50048]	Loss: 0.1508
Training Epoch: 73 [48000/50048]	Loss: 0.0980
Training Epoch: 73 [48128/50048]	Loss: 0.0807
Training Epoch: 73 [48256/50048]	Loss: 0.0811
Training Epoch: 73 [48384/50048]	Loss: 0.1304
Training Epoch: 73 [48512/50048]	Loss: 0.1018
Training Epoch: 73 [48640/50048]	Loss: 0.1157
Training Epoch: 73 [48768/50048]	Loss: 0.1731
Training Epoch: 73 [48896/50048]	Loss: 0.0865
Training Epoch: 73 [49024/50048]	Loss: 0.2098
Training Epoch: 73 [49152/50048]	Loss: 0.2526
Training Epoch: 73 [49280/50048]	Loss: 0.1612
Training Epoch: 73 [49408/50048]	Loss: 0.2491
Training Epoch: 73 [49536/50048]	Loss: 0.1466
Training Epoch: 73 [49664/50048]	Loss: 0.1287
Training Epoch: 73 [49792/50048]	Loss: 0.1754
Training Epoch: 73 [49920/50048]	Loss: 0.1156
Training Epoch: 73 [50048/50048]	Loss: 0.1498
2022-12-06 10:28:40.608 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:28:40,618 [ZeusDataLoader(eval)] eval epoch 74 done: time=3.70 energy=453.10
2022-12-06 05:28:40,619 [ZeusDataLoader(train)] Up to epoch 74: time=6676.73, energy=810508.64, cost=989468.25
2022-12-06 05:28:40,619 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:28:40,619 [ZeusDataLoader(train)] Expected next epoch: time=6766.53, energy=821306.65, cost=1002724.63
2022-12-06 05:28:40,620 [ZeusDataLoader(train)] Epoch 75 begin.
Validation Epoch: 73, Average loss: 0.0178, Accuracy: 0.6330
2022-12-06 05:28:40,801 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:28:40,801 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:28:40.803 [ZeusMonitor] Monitor started.
2022-12-06 10:28:40.803 [ZeusMonitor] Running indefinitely. 2022-12-06 10:28:40.803 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:28:40.803 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e75+gpu0.power.log
Training Epoch: 74 [128/50048]	Loss: 0.1752
Training Epoch: 74 [256/50048]	Loss: 0.0960
Training Epoch: 74 [384/50048]	Loss: 0.0943
Training Epoch: 74 [512/50048]	Loss: 0.0979
Training Epoch: 74 [640/50048]	Loss: 0.1547
Training Epoch: 74 [768/50048]	Loss: 0.1451
Training Epoch: 74 [896/50048]	Loss: 0.1356
Training Epoch: 74 [1024/50048]	Loss: 0.1263
Training Epoch: 74 [1152/50048]	Loss: 0.1203
Training Epoch: 74 [1280/50048]	Loss: 0.1285
Training Epoch: 74 [1408/50048]	Loss: 0.0934
Training Epoch: 74 [1536/50048]	Loss: 0.1248
Training Epoch: 74 [1664/50048]	Loss: 0.1264
Training Epoch: 74 [1792/50048]	Loss: 0.0383
Training Epoch: 74 [1920/50048]	Loss: 0.0675
Training Epoch: 74 [2048/50048]	Loss: 0.0623
Training Epoch: 74 [2176/50048]	Loss: 0.0885
Training Epoch: 74 [2304/50048]	Loss: 0.1044
Training Epoch: 74 [2432/50048]	Loss: 0.1597
Training Epoch: 74 [2560/50048]	Loss: 0.1160
Training Epoch: 74 [2688/50048]	Loss: 0.1759
Training Epoch: 74 [2816/50048]	Loss: 0.0946
Training Epoch: 74 [2944/50048]	Loss: 0.1129
Training Epoch: 74 [3072/50048]	Loss: 0.0909
Training Epoch: 74 [3200/50048]	Loss: 0.2014
Training Epoch: 74 [3328/50048]	Loss: 0.0659
Training Epoch: 74 [3456/50048]	Loss: 0.1205
Training Epoch: 74 [3584/50048]	Loss: 0.0732
Training Epoch: 74 [3712/50048]	Loss: 0.1037
Training Epoch: 74 [3840/50048]	Loss: 0.1578
Training Epoch: 74 [3968/50048]	Loss: 0.0971
Training Epoch: 74 [4096/50048]	Loss: 0.1126
Training Epoch: 74 [4224/50048]	Loss: 0.1449
Training Epoch: 74 [4352/50048]	Loss: 0.0807
Training Epoch: 74 [4480/50048]	Loss: 0.1174
Training Epoch: 74 [4608/50048]	Loss: 0.1315
Training Epoch: 74 [4736/50048]	Loss: 0.1036
Training Epoch: 74 [4864/50048]	Loss: 0.0848
Training Epoch: 74 [4992/50048]	Loss: 0.0423
Training Epoch: 74 [5120/50048]	Loss: 0.1484
Training Epoch: 74 [5248/50048]	Loss: 0.0537
Training Epoch: 74 [5376/50048]	Loss: 0.1040
Training Epoch: 74 [5504/50048]	Loss: 0.0666
Training Epoch: 74 [5632/50048]	Loss: 0.0983
Training Epoch: 74 [5760/50048]	Loss: 0.1113
Training Epoch: 74 [5888/50048]	Loss: 0.0882
Training Epoch: 74 [6016/50048]	Loss: 0.2089
Training Epoch: 74 [6144/50048]	Loss: 0.0895
Training Epoch: 74 [6272/50048]	Loss: 0.1342
Training Epoch: 74 [6400/50048]	Loss: 0.0811
Training Epoch: 74 [6528/50048]	Loss: 0.0752
Training Epoch: 74 [6656/50048]	Loss: 0.1316
Training Epoch: 74 [6784/50048]	Loss: 0.1507
Training Epoch: 74 [6912/50048]	Loss: 0.1500
Training Epoch: 74 [7040/50048]	Loss: 0.1412
Training Epoch: 74 [7168/50048]	Loss: 0.1395
Training Epoch: 74 [7296/50048]	Loss: 0.1254
Training Epoch: 74 [7424/50048]	Loss: 0.0626
Training Epoch: 74 [7552/50048]	Loss: 0.1002
Training Epoch: 74 [7680/50048]	Loss: 0.1028
Training Epoch: 74 [7808/50048]	Loss: 0.0528
Training Epoch: 74 [7936/50048]	Loss: 0.1387
Training Epoch: 74 [8064/50048]	Loss: 0.1697
Training Epoch: 74 [8192/50048]	Loss: 0.1206
Training Epoch: 74 [8320/50048]	Loss: 0.1376
Training Epoch: 74 [8448/50048]	Loss: 0.1731
Training Epoch: 74 [8576/50048]	Loss: 0.2369
Training Epoch: 74 [8704/50048]	Loss: 0.0942
Training Epoch: 74 [8832/50048]	Loss: 0.0649
Training Epoch: 74 [8960/50048]	Loss: 0.1313
Training Epoch: 74 [9088/50048]	Loss: 0.1320
Training Epoch: 74 [9216/50048]	Loss: 0.1297
Training Epoch: 74 [9344/50048]	Loss: 0.1085
Training Epoch: 74 [9472/50048]	Loss: 0.2682
Training Epoch: 74 [9600/50048]	Loss: 0.1509
Training Epoch: 74 [9728/50048]	Loss: 0.1339
Training Epoch: 74 [9856/50048]	Loss: 0.0967
Training Epoch: 74 [9984/50048]	Loss: 0.1227
Training Epoch: 74 [10112/50048]	Loss: 0.1091
Training Epoch: 74 [10240/50048]	Loss: 0.0887
Training Epoch: 74 [10368/50048]	Loss: 0.1039
Training Epoch: 74 [10496/50048]	Loss: 0.0548
Training Epoch: 74 [10624/50048]	Loss: 0.1302
Training Epoch: 74 [10752/50048]	Loss: 0.0726
Training Epoch: 74 [10880/50048]	Loss: 0.1539
Training Epoch: 74 [11008/50048]	Loss: 0.0815
Training Epoch: 74 [11136/50048]	Loss: 0.1011
Training Epoch: 74 [11264/50048]	Loss: 0.1325
Training Epoch: 74 [11392/50048]	Loss: 0.1240
Training Epoch: 74 [11520/50048]	Loss: 0.1304
Training Epoch: 74 [11648/50048]	Loss: 0.1748
Training Epoch: 74 [11776/50048]	Loss: 0.0748
Training Epoch: 74 [11904/50048]	Loss: 0.0610
Training Epoch: 74 [12032/50048]	Loss: 0.1327
Training Epoch: 74 [12160/50048]	Loss: 0.1530
Training Epoch: 74 [12288/50048]	Loss: 0.1742
Training Epoch: 74 [12416/50048]	Loss: 0.1358
Training Epoch: 74 [12544/50048]	Loss: 0.1108
Training Epoch: 74 [12672/50048]	Loss: 0.1682
Training Epoch: 74 [12800/50048]	Loss: 0.1053
Training Epoch: 74 [12928/50048]	Loss: 0.1075
Training Epoch: 74 [13056/50048]	Loss: 0.1241
Training Epoch: 74 [13184/50048]	Loss: 0.1061
Training Epoch: 74 [13312/50048]	Loss: 0.1060
Training Epoch: 74 [13440/50048]	Loss: 0.0782
Training Epoch: 74 [13568/50048]	Loss: 0.1409
Training Epoch: 74 [13696/50048]	Loss: 0.1468
Training Epoch: 74 [13824/50048]	Loss: 0.0597
Training Epoch: 74 [13952/50048]	Loss: 0.1257
Training Epoch: 74 [14080/50048]	Loss: 0.1102
Training Epoch: 74 [14208/50048]	Loss: 0.1624
Training Epoch: 74 [14336/50048]	Loss: 0.0553
Training Epoch: 74 [14464/50048]	Loss: 0.0670
Training Epoch: 74 [14592/50048]	Loss: 0.0813
Training Epoch: 74 [14720/50048]	Loss: 0.1199
Training Epoch: 74 [14848/50048]	Loss: 0.1413
Training Epoch: 74 [14976/50048]	Loss: 0.0911
Training Epoch: 74 [15104/50048]	Loss: 0.0843
Training Epoch: 74 [15232/50048]	Loss: 0.1500
Training Epoch: 74 [15360/50048]	Loss: 0.1056
Training Epoch: 74 [15488/50048]	Loss: 0.0735
Training Epoch: 74 [15616/50048]	Loss: 0.1688
Training Epoch: 74 [15744/50048]	Loss: 0.1224
Training Epoch: 74 [15872/50048]	Loss: 0.1263
Training Epoch: 74 [16000/50048]	Loss: 0.1094
Training Epoch: 74 [16128/50048]	Loss: 0.1496
Training Epoch: 74 [16256/50048]	Loss: 0.1488
Training Epoch: 74 [16384/50048]	Loss: 0.0728
Training Epoch: 74 [16512/50048]	Loss: 0.1031
Training Epoch: 74 [16640/50048]	Loss: 0.2292
Training Epoch: 74 [16768/50048]	Loss: 0.1021
Training Epoch: 74 [16896/50048]	Loss: 0.0876
Training Epoch: 74 [17024/50048]	Loss: 0.1096
Training Epoch: 74 [17152/50048]	Loss: 0.0546
Training Epoch: 74 [17280/50048]	Loss: 0.0849
Training Epoch: 74 [17408/50048]	Loss: 0.1075
Training Epoch: 74 [17536/50048]	Loss: 0.1997
Training Epoch: 74 [17664/50048]	Loss: 0.1049
Training Epoch: 74 [17792/50048]	Loss: 0.1201
Training Epoch: 74 [17920/50048]	Loss: 0.0673
Training Epoch: 74 [18048/50048]	Loss: 0.1205
Training Epoch: 74 [18176/50048]	Loss: 0.0965
Training Epoch: 74 [18304/50048]	Loss: 0.2028
Training Epoch: 74 [18432/50048]	Loss: 0.2192
Training Epoch: 74 [18560/50048]	Loss: 0.1216
Training Epoch: 74 [18688/50048]	Loss: 0.0708
Training Epoch: 74 [18816/50048]	Loss: 0.1762
Training Epoch: 74 [18944/50048]	Loss: 0.0616
Training Epoch: 74 [19072/50048]	Loss: 0.0717
Training Epoch: 74 [19200/50048]	Loss: 0.1452
Training Epoch: 74 [19328/50048]	Loss: 0.1816
Training Epoch: 74 [19456/50048]	Loss: 0.0695
Training Epoch: 74 [19584/50048]	Loss: 0.1064
Training Epoch: 74 [19712/50048]	Loss: 0.1594
Training Epoch: 74 [19840/50048]	Loss: 0.1590
Training Epoch: 74 [19968/50048]	Loss: 0.1148
Training Epoch: 74 [20096/50048]	Loss: 0.0424
Training Epoch: 74 [20224/50048]	Loss: 0.1571
Training Epoch: 74 [20352/50048]	Loss: 0.2415
Training Epoch: 74 [20480/50048]	Loss: 0.1657
Training Epoch: 74 [20608/50048]	Loss: 0.0871
Training Epoch: 74 [20736/50048]	Loss: 0.1123
Training Epoch: 74 [20864/50048]	Loss: 0.1176
Training Epoch: 74 [20992/50048]	Loss: 0.0757
Training Epoch: 74 [21120/50048]	Loss: 0.1180
Training Epoch: 74 [21248/50048]	Loss: 0.1569
Training Epoch: 74 [21376/50048]	Loss: 0.1884
Training Epoch: 74 [21504/50048]	Loss: 0.0677
Training Epoch: 74 [21632/50048]	Loss: 0.1675
Training Epoch: 74 [21760/50048]	Loss: 0.1259
Training Epoch: 74 [21888/50048]	Loss: 0.0993
Training Epoch: 74 [22016/50048]	Loss: 0.2057
Training Epoch: 74 [22144/50048]	Loss: 0.1287
Training Epoch: 74 [22272/50048]	Loss: 0.0976
Training Epoch: 74 [22400/50048]	Loss: 0.2036
Training Epoch: 74 [22528/50048]	Loss: 0.1715
Training Epoch: 74 [22656/50048]	Loss: 0.1638
Training Epoch: 74 [22784/50048]	Loss: 0.2070
Training Epoch: 74 [22912/50048]	Loss: 0.1407
Training Epoch: 74 [23040/50048]	Loss: 0.0408
Training Epoch: 74 [23168/50048]	Loss: 0.1232
Training Epoch: 74 [23296/50048]	Loss: 0.2377
Training Epoch: 74 [23424/50048]	Loss: 0.0969
Training Epoch: 74 [23552/50048]	Loss: 0.0972
Training Epoch: 74 [23680/50048]	Loss: 0.1873
Training Epoch: 74 [23808/50048]	Loss: 0.1097
Training Epoch: 74 [23936/50048]	Loss: 0.1998
Training Epoch: 74 [24064/50048]	Loss: 0.1200
Training Epoch: 74 [24192/50048]	Loss: 0.1190
Training Epoch: 74 [24320/50048]	Loss: 0.0820
Training Epoch: 74 [24448/50048]	Loss: 0.1506
Training Epoch: 74 [24576/50048]	Loss: 0.0994
Training Epoch: 74 [24704/50048]	Loss: 0.1374
Training Epoch: 74 [24832/50048]	Loss: 0.0765
Training Epoch: 74 [24960/50048]	Loss: 0.0823
Training Epoch: 74 [25088/50048]	Loss: 0.0827
Training Epoch: 74 [25216/50048]	Loss: 0.1628
Training Epoch: 74 [25344/50048]	Loss: 0.1146
Training Epoch: 74 [25472/50048]	Loss: 0.0888
Training Epoch: 74 [25600/50048]	Loss: 0.1402
Training Epoch: 74 [25728/50048]	Loss: 0.1751
Training Epoch: 74 [25856/50048]	Loss: 0.0548
Training Epoch: 74 [25984/50048]	Loss: 0.1749
Training Epoch: 74 [26112/50048]	Loss: 0.0842
Training Epoch: 74 [26240/50048]	Loss: 0.1686
Training Epoch: 74 [26368/50048]	Loss: 0.1290
Training Epoch: 74 [26496/50048]	Loss: 0.1477
Training Epoch: 74 [26624/50048]	Loss: 0.1014
Training Epoch: 74 [26752/50048]	Loss: 0.1014
Training Epoch: 74 [26880/50048]	Loss: 0.1819
Training Epoch: 74 [27008/50048]	Loss: 0.1280
Training Epoch: 74 [27136/50048]	Loss: 0.0662
Training Epoch: 74 [27264/50048]	Loss: 0.0908
Training Epoch: 74 [27392/50048]	Loss: 0.1351
Training Epoch: 74 [27520/50048]	Loss: 0.1187
Training Epoch: 74 [27648/50048]	Loss: 0.1386
Training Epoch: 74 [27776/50048]	Loss: 0.1327
Training Epoch: 74 [27904/50048]	Loss: 0.2117
Training Epoch: 74 [28032/50048]	Loss: 0.1759
Training Epoch: 74 [28160/50048]	Loss: 0.1266
Training Epoch: 74 [28288/50048]	Loss: 0.1024
Training Epoch: 74 [28416/50048]	Loss: 0.1006
Training Epoch: 74 [28544/50048]	Loss: 0.0915
Training Epoch: 74 [28672/50048]	Loss: 0.0539
Training Epoch: 74 [28800/50048]	Loss: 0.1188
Training Epoch: 74 [28928/50048]	Loss: 0.1786
Training Epoch: 74 [29056/50048]	Loss: 0.1658
Training Epoch: 74 [29184/50048]	Loss: 0.1756
Training Epoch: 74 [29312/50048]	Loss: 0.1177
Training Epoch: 74 [29440/50048]	Loss: 0.1447
Training Epoch: 74 [29568/50048]	Loss: 0.0901
Training Epoch: 74 [29696/50048]	Loss: 0.1270
Training Epoch: 74 [29824/50048]	Loss: 0.0926
Training Epoch: 74 [29952/50048]	Loss: 0.1068
Training Epoch: 74 [30080/50048]	Loss: 0.0804
Training Epoch: 74 [30208/50048]	Loss: 0.1338
Training Epoch: 74 [30336/50048]	Loss: 0.0724
Training Epoch: 74 [30464/50048]	Loss: 0.1353
Training Epoch: 74 [30592/50048]	Loss: 0.0669
Training Epoch: 74 [30720/50048]	Loss: 0.0831
Training Epoch: 74 [30848/50048]	Loss: 0.1634
Training Epoch: 74 [30976/50048]	Loss: 0.2299
Training Epoch: 74 [31104/50048]	Loss: 0.0813
Training Epoch: 74 [31232/50048]	Loss: 0.2375
Training Epoch: 74 [31360/50048]	Loss: 0.1304
Training Epoch: 74 [31488/50048]	Loss: 0.0701
Training Epoch: 74 [31616/50048]	Loss: 0.1283
Training Epoch: 74 [31744/50048]	Loss: 0.1424
Training Epoch: 74 [31872/50048]	Loss: 0.1563
Training Epoch: 74 [32000/50048]	Loss: 0.1390
Training Epoch: 74 [32128/50048]	Loss: 0.0881
Training Epoch: 74 [32256/50048]	Loss: 0.2001
Training Epoch: 74 [32384/50048]	Loss: 0.1626
Training Epoch: 74 [32512/50048]	Loss: 0.0938
Training Epoch: 74 [32640/50048]	Loss: 0.0716
Training Epoch: 74 [32768/50048]	Loss: 0.1978
Training Epoch: 74 [32896/50048]	Loss: 0.2182
Training Epoch: 74 [33024/50048]	Loss: 0.1168
Training Epoch: 74 [33152/50048]	Loss: 0.1118
Training Epoch: 74 [33280/50048]	Loss: 0.1384
Training Epoch: 74 [33408/50048]	Loss: 0.1085
Training Epoch: 74 [33536/50048]	Loss: 0.1792
Training Epoch: 74 [33664/50048]	Loss: 0.0850
Training Epoch: 74 [33792/50048]	Loss: 0.1343
Training Epoch: 74 [33920/50048]	Loss: 0.0863
Training Epoch: 74 [34048/50048]	Loss: 0.1358
Training Epoch: 74 [34176/50048]	Loss: 0.1489
Training Epoch: 74 [34304/50048]	Loss: 0.1377
Training Epoch: 74 [34432/50048]	Loss: 0.1386
Training Epoch: 74 [34560/50048]	Loss: 0.2496
Training Epoch: 74 [34688/50048]	Loss: 0.0937
Training Epoch: 74 [34816/50048]	Loss: 0.1396
Training Epoch: 74 [34944/50048]	Loss: 0.1807
Training Epoch: 74 [35072/50048]	Loss: 0.1187
Training Epoch: 74 [35200/50048]	Loss: 0.1104
Training Epoch: 74 [35328/50048]	Loss: 0.1083
Training Epoch: 74 [35456/50048]	Loss: 0.0751
Training Epoch: 74 [35584/50048]	Loss: 0.1480
Training Epoch: 74 [35712/50048]	Loss: 0.1420
Training Epoch: 74 [35840/50048]	Loss: 0.1234
Training Epoch: 74 [35968/50048]	Loss: 0.1668
Training Epoch: 74 [36096/50048]	Loss: 0.2215
Training Epoch: 74 [36224/50048]	Loss: 0.1212
Training Epoch: 74 [36352/50048]	Loss: 0.0744
Training Epoch: 74 [36480/50048]	Loss: 0.0837
Training Epoch: 74 [36608/50048]	Loss: 0.1211
Training Epoch: 74 [36736/50048]	Loss: 0.1734
Training Epoch: 74 [36864/50048]	Loss: 0.1354
Training Epoch: 74 [36992/50048]	Loss: 0.1725
Training Epoch: 74 [37120/50048]	Loss: 0.0826
Training Epoch: 74 [37248/50048]	Loss: 0.2250
Training Epoch: 74 [37376/50048]	Loss: 0.1228
Training Epoch: 74 [37504/50048]	Loss: 0.1021
Training Epoch: 74 [37632/50048]	Loss: 0.0919
Training Epoch: 74 [37760/50048]	Loss: 0.2030
Training Epoch: 74 [37888/50048]	Loss: 0.1062
Training Epoch: 74 [38016/50048]	Loss: 0.0725
Training Epoch: 74 [38144/50048]	Loss: 0.1116
Training Epoch: 74 [38272/50048]	Loss: 0.1448
Training Epoch: 74 [38400/50048]	Loss: 0.0726
Training Epoch: 74 [38528/50048]	Loss: 0.1282
Training Epoch: 74 [38656/50048]	Loss: 0.1434
Training Epoch: 74 [38784/50048]	Loss: 0.0405
Training Epoch: 74 [38912/50048]	Loss: 0.2430
Training Epoch: 74 [39040/50048]	Loss: 0.1148
Training Epoch: 74 [39168/50048]	Loss: 0.1158
Training Epoch: 74 [39296/50048]	Loss: 0.1253
Training Epoch: 74 [39424/50048]	Loss: 0.0989
Training Epoch: 74 [39552/50048]	Loss: 0.1329
Training Epoch: 74 [39680/50048]	Loss: 0.1125
Training Epoch: 74 [39808/50048]	Loss: 0.0809
Training Epoch: 74 [39936/50048]	Loss: 0.1303
Training Epoch: 74 [40064/50048]	Loss: 0.0900
Training Epoch: 74 [40192/50048]	Loss: 0.1608
Training Epoch: 74 [40320/50048]	Loss: 0.1059
Training Epoch: 74 [40448/50048]	Loss: 0.0639
Training Epoch: 74 [40576/50048]	Loss: 0.0853
Training Epoch: 74 [40704/50048]	Loss: 0.0966
Training Epoch: 74 [40832/50048]	Loss: 0.1120
Training Epoch: 74 [40960/50048]	Loss: 0.1246
Training Epoch: 74 [41088/50048]	Loss: 0.0541
Training Epoch: 74 [41216/50048]	Loss: 0.1052
Training Epoch: 74 [41344/50048]	Loss: 0.0681
Training Epoch: 74 [41472/50048]	Loss: 0.0927
Training Epoch: 74 [41600/50048]	Loss: 0.1422
Training Epoch: 74 [41728/50048]	Loss: 0.0865
Training Epoch: 74 [41856/50048]	Loss: 0.1119
Training Epoch: 74 [41984/50048]	Loss: 0.1435
Training Epoch: 74 [42112/50048]	Loss: 0.1358
Training Epoch: 74 [42240/50048]	Loss: 0.0893
Training Epoch: 74 [42368/50048]	Loss: 0.1472
Training Epoch: 74 [42496/50048]	Loss: 0.0952
Training Epoch: 74 [42624/50048]	Loss: 0.1629
Training Epoch: 74 [42752/50048]	Loss: 0.1168
Training Epoch: 74 [42880/50048]	Loss: 0.0847
Training Epoch: 74 [43008/50048]	Loss: 0.1473
Training Epoch: 74 [43136/50048]	Loss: 0.1367
Training Epoch: 74 [43264/50048]	Loss: 0.1235
Training Epoch: 74 [43392/50048]	Loss: 0.1877
Training Epoch: 74 [43520/50048]	Loss: 0.1151
Training Epoch: 74 [43648/50048]	Loss: 0.1749
Training Epoch: 74 [43776/50048]	Loss: 0.1385
Training Epoch: 74 [43904/50048]	Loss: 0.0811
Training Epoch: 74 [44032/50048]	Loss: 0.1395
Training Epoch: 74 [44160/50048]	Loss: 0.1433
Training Epoch: 74 [44288/50048]	Loss: 0.1329
Training Epoch: 74 [44416/50048]	Loss: 0.2042
Training Epoch: 74 [44544/50048]	Loss: 0.0674
Training Epoch: 74 [44672/50048]	Loss: 0.1441
Training Epoch: 74 [44800/50048]	Loss: 0.1320
Training Epoch: 74 [44928/50048]	Loss: 0.1346
Training Epoch: 74 [45056/50048]	Loss: 0.0790
Training Epoch: 74 [45184/50048]	Loss: 0.1510
Training Epoch: 74 [45312/50048]	Loss: 0.1362
Training Epoch: 74 [45440/50048]	Loss: 0.0871
Training Epoch: 74 [45568/50048]	Loss: 0.1540
Training Epoch: 74 [45696/50048]	Loss: 0.1367
2022-12-06 05:30:07,069 [ZeusDataLoader(train)] train epoch 75 done: time=86.44 energy=10496.24
2022-12-06 05:30:07,070 [ZeusDataLoader(eval)] Epoch 75 begin.
Training Epoch: 74 [45824/50048]	Loss: 0.1174
Training Epoch: 74 [45952/50048]	Loss: 0.1279
Training Epoch: 74 [46080/50048]	Loss: 0.1448
Training Epoch: 74 [46208/50048]	Loss: 0.1616
Training Epoch: 74 [46336/50048]	Loss: 0.1015
Training Epoch: 74 [46464/50048]	Loss: 0.1474
Training Epoch: 74 [46592/50048]	Loss: 0.1396
Training Epoch: 74 [46720/50048]	Loss: 0.1496
Training Epoch: 74 [46848/50048]	Loss: 0.0677
Training Epoch: 74 [46976/50048]	Loss: 0.1041
Training Epoch: 74 [47104/50048]	Loss: 0.1510
Training Epoch: 74 [47232/50048]	Loss: 0.1902
Training Epoch: 74 [47360/50048]	Loss: 0.1566
Training Epoch: 74 [47488/50048]	Loss: 0.1728
Training Epoch: 74 [47616/50048]	Loss: 0.1698
Training Epoch: 74 [47744/50048]	Loss: 0.1184
Training Epoch: 74 [47872/50048]	Loss: 0.0959
Training Epoch: 74 [48000/50048]	Loss: 0.1966
Training Epoch: 74 [48128/50048]	Loss: 0.1848
Training Epoch: 74 [48256/50048]	Loss: 0.1009
Training Epoch: 74 [48384/50048]	Loss: 0.1006
Training Epoch: 74 [48512/50048]	Loss: 0.1005
Training Epoch: 74 [48640/50048]	Loss: 0.1003
Training Epoch: 74 [48768/50048]	Loss: 0.0852
Training Epoch: 74 [48896/50048]	Loss: 0.1334
Training Epoch: 74 [49024/50048]	Loss: 0.1204
Training Epoch: 74 [49152/50048]	Loss: 0.1396
Training Epoch: 74 [49280/50048]	Loss: 0.1417
Training Epoch: 74 [49408/50048]	Loss: 0.1947
Training Epoch: 74 [49536/50048]	Loss: 0.2083
Training Epoch: 74 [49664/50048]	Loss: 0.1404
Training Epoch: 74 [49792/50048]	Loss: 0.2019
Training Epoch: 74 [49920/50048]	Loss: 0.1961
Training Epoch: 74 [50048/50048]	Loss: 0.1076
2022-12-06 10:30:10.799 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:30:10,838 [ZeusDataLoader(eval)] eval epoch 75 done: time=3.76 energy=452.53
2022-12-06 05:30:10,838 [ZeusDataLoader(train)] Up to epoch 75: time=6766.93, energy=821457.40, cost=1002834.91
2022-12-06 05:30:10,838 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:30:10,838 [ZeusDataLoader(train)] Expected next epoch: time=6856.73, energy=832255.42, cost=1016091.30
2022-12-06 05:30:10,839 [ZeusDataLoader(train)] Epoch 76 begin.
Validation Epoch: 74, Average loss: 0.0178, Accuracy: 0.6379
2022-12-06 05:30:11,019 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:30:11,020 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:30:11.022 [ZeusMonitor] Monitor started.
2022-12-06 10:30:11.022 [ZeusMonitor] Running indefinitely. 2022-12-06 10:30:11.022 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:30:11.022 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e76+gpu0.power.log
Training Epoch: 75 [128/50048]	Loss: 0.1441
Training Epoch: 75 [256/50048]	Loss: 0.0996
Training Epoch: 75 [384/50048]	Loss: 0.0451
Training Epoch: 75 [512/50048]	Loss: 0.0686
Training Epoch: 75 [640/50048]	Loss: 0.2114
Training Epoch: 75 [768/50048]	Loss: 0.0977
Training Epoch: 75 [896/50048]	Loss: 0.0494
Training Epoch: 75 [1024/50048]	Loss: 0.0958
Training Epoch: 75 [1152/50048]	Loss: 0.0866
Training Epoch: 75 [1280/50048]	Loss: 0.1415
Training Epoch: 75 [1408/50048]	Loss: 0.0778
Training Epoch: 75 [1536/50048]	Loss: 0.1027
Training Epoch: 75 [1664/50048]	Loss: 0.0998
Training Epoch: 75 [1792/50048]	Loss: 0.0719
Training Epoch: 75 [1920/50048]	Loss: 0.1374
Training Epoch: 75 [2048/50048]	Loss: 0.0865
Training Epoch: 75 [2176/50048]	Loss: 0.0940
Training Epoch: 75 [2304/50048]	Loss: 0.0952
Training Epoch: 75 [2432/50048]	Loss: 0.0942
Training Epoch: 75 [2560/50048]	Loss: 0.1221
Training Epoch: 75 [2688/50048]	Loss: 0.1148
Training Epoch: 75 [2816/50048]	Loss: 0.1505
Training Epoch: 75 [2944/50048]	Loss: 0.0506
Training Epoch: 75 [3072/50048]	Loss: 0.0662
Training Epoch: 75 [3200/50048]	Loss: 0.1198
Training Epoch: 75 [3328/50048]	Loss: 0.0844
Training Epoch: 75 [3456/50048]	Loss: 0.1063
Training Epoch: 75 [3584/50048]	Loss: 0.0668
Training Epoch: 75 [3712/50048]	Loss: 0.0731
Training Epoch: 75 [3840/50048]	Loss: 0.0844
Training Epoch: 75 [3968/50048]	Loss: 0.1451
Training Epoch: 75 [4096/50048]	Loss: 0.1503
Training Epoch: 75 [4224/50048]	Loss: 0.2295
Training Epoch: 75 [4352/50048]	Loss: 0.1235
Training Epoch: 75 [4480/50048]	Loss: 0.1187
Training Epoch: 75 [4608/50048]	Loss: 0.0811
Training Epoch: 75 [4736/50048]	Loss: 0.0865
Training Epoch: 75 [4864/50048]	Loss: 0.0863
Training Epoch: 75 [4992/50048]	Loss: 0.0913
Training Epoch: 75 [5120/50048]	Loss: 0.0758
Training Epoch: 75 [5248/50048]	Loss: 0.0550
Training Epoch: 75 [5376/50048]	Loss: 0.1092
Training Epoch: 75 [5504/50048]	Loss: 0.1782
Training Epoch: 75 [5632/50048]	Loss: 0.0826
Training Epoch: 75 [5760/50048]	Loss: 0.0793
Training Epoch: 75 [5888/50048]	Loss: 0.0717
Training Epoch: 75 [6016/50048]	Loss: 0.0875
Training Epoch: 75 [6144/50048]	Loss: 0.0917
Training Epoch: 75 [6272/50048]	Loss: 0.0988
Training Epoch: 75 [6400/50048]	Loss: 0.1359
Training Epoch: 75 [6528/50048]	Loss: 0.1077
Training Epoch: 75 [6656/50048]	Loss: 0.0865
Training Epoch: 75 [6784/50048]	Loss: 0.1804
Training Epoch: 75 [6912/50048]	Loss: 0.1427
Training Epoch: 75 [7040/50048]	Loss: 0.0347
Training Epoch: 75 [7168/50048]	Loss: 0.0527
Training Epoch: 75 [7296/50048]	Loss: 0.0936
Training Epoch: 75 [7424/50048]	Loss: 0.1093
Training Epoch: 75 [7552/50048]	Loss: 0.1363
Training Epoch: 75 [7680/50048]	Loss: 0.0762
Training Epoch: 75 [7808/50048]	Loss: 0.2177
Training Epoch: 75 [7936/50048]	Loss: 0.1517
Training Epoch: 75 [8064/50048]	Loss: 0.1477
Training Epoch: 75 [8192/50048]	Loss: 0.0766
Training Epoch: 75 [8320/50048]	Loss: 0.0786
Training Epoch: 75 [8448/50048]	Loss: 0.0691
Training Epoch: 75 [8576/50048]	Loss: 0.0649
Training Epoch: 75 [8704/50048]	Loss: 0.0687
Training Epoch: 75 [8832/50048]	Loss: 0.1645
Training Epoch: 75 [8960/50048]	Loss: 0.1652
Training Epoch: 75 [9088/50048]	Loss: 0.2007
Training Epoch: 75 [9216/50048]	Loss: 0.1391
Training Epoch: 75 [9344/50048]	Loss: 0.1100
Training Epoch: 75 [9472/50048]	Loss: 0.0721
Training Epoch: 75 [9600/50048]	Loss: 0.1433
Training Epoch: 75 [9728/50048]	Loss: 0.1716
Training Epoch: 75 [9856/50048]	Loss: 0.1985
Training Epoch: 75 [9984/50048]	Loss: 0.1561
Training Epoch: 75 [10112/50048]	Loss: 0.1042
Training Epoch: 75 [10240/50048]	Loss: 0.0712
Training Epoch: 75 [10368/50048]	Loss: 0.1547
Training Epoch: 75 [10496/50048]	Loss: 0.1474
Training Epoch: 75 [10624/50048]	Loss: 0.0881
Training Epoch: 75 [10752/50048]	Loss: 0.0649
Training Epoch: 75 [10880/50048]	Loss: 0.0771
Training Epoch: 75 [11008/50048]	Loss: 0.0672
Training Epoch: 75 [11136/50048]	Loss: 0.1331
Training Epoch: 75 [11264/50048]	Loss: 0.1289
Training Epoch: 75 [11392/50048]	Loss: 0.0840
Training Epoch: 75 [11520/50048]	Loss: 0.0605
Training Epoch: 75 [11648/50048]	Loss: 0.1824
Training Epoch: 75 [11776/50048]	Loss: 0.0668
Training Epoch: 75 [11904/50048]	Loss: 0.0944
Training Epoch: 75 [12032/50048]	Loss: 0.1500
Training Epoch: 75 [12160/50048]	Loss: 0.1099
Training Epoch: 75 [12288/50048]	Loss: 0.0642
Training Epoch: 75 [12416/50048]	Loss: 0.1346
Training Epoch: 75 [12544/50048]	Loss: 0.1391
Training Epoch: 75 [12672/50048]	Loss: 0.1497
Training Epoch: 75 [12800/50048]	Loss: 0.0656
Training Epoch: 75 [12928/50048]	Loss: 0.1314
Training Epoch: 75 [13056/50048]	Loss: 0.1932
Training Epoch: 75 [13184/50048]	Loss: 0.1200
Training Epoch: 75 [13312/50048]	Loss: 0.0999
Training Epoch: 75 [13440/50048]	Loss: 0.1463
Training Epoch: 75 [13568/50048]	Loss: 0.0722
Training Epoch: 75 [13696/50048]	Loss: 0.0765
Training Epoch: 75 [13824/50048]	Loss: 0.1401
Training Epoch: 75 [13952/50048]	Loss: 0.1788
Training Epoch: 75 [14080/50048]	Loss: 0.1244
Training Epoch: 75 [14208/50048]	Loss: 0.1227
Training Epoch: 75 [14336/50048]	Loss: 0.1209
Training Epoch: 75 [14464/50048]	Loss: 0.1680
Training Epoch: 75 [14592/50048]	Loss: 0.0649
Training Epoch: 75 [14720/50048]	Loss: 0.0757
Training Epoch: 75 [14848/50048]	Loss: 0.0649
Training Epoch: 75 [14976/50048]	Loss: 0.0699
Training Epoch: 75 [15104/50048]	Loss: 0.1139
Training Epoch: 75 [15232/50048]	Loss: 0.1100
Training Epoch: 75 [15360/50048]	Loss: 0.1500
Training Epoch: 75 [15488/50048]	Loss: 0.1838
Training Epoch: 75 [15616/50048]	Loss: 0.1061
Training Epoch: 75 [15744/50048]	Loss: 0.0516
Training Epoch: 75 [15872/50048]	Loss: 0.0824
Training Epoch: 75 [16000/50048]	Loss: 0.0700
Training Epoch: 75 [16128/50048]	Loss: 0.0840
Training Epoch: 75 [16256/50048]	Loss: 0.1742
Training Epoch: 75 [16384/50048]	Loss: 0.1122
Training Epoch: 75 [16512/50048]	Loss: 0.1263
Training Epoch: 75 [16640/50048]	Loss: 0.0690
Training Epoch: 75 [16768/50048]	Loss: 0.0639
Training Epoch: 75 [16896/50048]	Loss: 0.1001
Training Epoch: 75 [17024/50048]	Loss: 0.1285
Training Epoch: 75 [17152/50048]	Loss: 0.1430
Training Epoch: 75 [17280/50048]	Loss: 0.2322
Training Epoch: 75 [17408/50048]	Loss: 0.1128
Training Epoch: 75 [17536/50048]	Loss: 0.1718
Training Epoch: 75 [17664/50048]	Loss: 0.1592
Training Epoch: 75 [17792/50048]	Loss: 0.1486
Training Epoch: 75 [17920/50048]	Loss: 0.0951
Training Epoch: 75 [18048/50048]	Loss: 0.1199
Training Epoch: 75 [18176/50048]	Loss: 0.0693
Training Epoch: 75 [18304/50048]	Loss: 0.1252
Training Epoch: 75 [18432/50048]	Loss: 0.0810
Training Epoch: 75 [18560/50048]	Loss: 0.0458
Training Epoch: 75 [18688/50048]	Loss: 0.0888
Training Epoch: 75 [18816/50048]	Loss: 0.1671
Training Epoch: 75 [18944/50048]	Loss: 0.1418
Training Epoch: 75 [19072/50048]	Loss: 0.0828
Training Epoch: 75 [19200/50048]	Loss: 0.1151
Training Epoch: 75 [19328/50048]	Loss: 0.0874
Training Epoch: 75 [19456/50048]	Loss: 0.2010
Training Epoch: 75 [19584/50048]	Loss: 0.0951
Training Epoch: 75 [19712/50048]	Loss: 0.1361
Training Epoch: 75 [19840/50048]	Loss: 0.2322
Training Epoch: 75 [19968/50048]	Loss: 0.0437
Training Epoch: 75 [20096/50048]	Loss: 0.1033
Training Epoch: 75 [20224/50048]	Loss: 0.0625
Training Epoch: 75 [20352/50048]	Loss: 0.1687
Training Epoch: 75 [20480/50048]	Loss: 0.1347
Training Epoch: 75 [20608/50048]	Loss: 0.1384
Training Epoch: 75 [20736/50048]	Loss: 0.1155
Training Epoch: 75 [20864/50048]	Loss: 0.0901
Training Epoch: 75 [20992/50048]	Loss: 0.1560
Training Epoch: 75 [21120/50048]	Loss: 0.1089
Training Epoch: 75 [21248/50048]	Loss: 0.0759
Training Epoch: 75 [21376/50048]	Loss: 0.1489
Training Epoch: 75 [21504/50048]	Loss: 0.1496
Training Epoch: 75 [21632/50048]	Loss: 0.2249
Training Epoch: 75 [21760/50048]	Loss: 0.1241
Training Epoch: 75 [21888/50048]	Loss: 0.1666
Training Epoch: 75 [22016/50048]	Loss: 0.1867
Training Epoch: 75 [22144/50048]	Loss: 0.1517
Training Epoch: 75 [22272/50048]	Loss: 0.1270
Training Epoch: 75 [22400/50048]	Loss: 0.1121
Training Epoch: 75 [22528/50048]	Loss: 0.1425
Training Epoch: 75 [22656/50048]	Loss: 0.0977
Training Epoch: 75 [22784/50048]	Loss: 0.0871
Training Epoch: 75 [22912/50048]	Loss: 0.1466
Training Epoch: 75 [23040/50048]	Loss: 0.0850
Training Epoch: 75 [23168/50048]	Loss: 0.0782
Training Epoch: 75 [23296/50048]	Loss: 0.0745
Training Epoch: 75 [23424/50048]	Loss: 0.1184
Training Epoch: 75 [23552/50048]	Loss: 0.0662
Training Epoch: 75 [23680/50048]	Loss: 0.1193
Training Epoch: 75 [23808/50048]	Loss: 0.1603
Training Epoch: 75 [23936/50048]	Loss: 0.1054
Training Epoch: 75 [24064/50048]	Loss: 0.1227
Training Epoch: 75 [24192/50048]	Loss: 0.1412
Training Epoch: 75 [24320/50048]	Loss: 0.0618
Training Epoch: 75 [24448/50048]	Loss: 0.1432
Training Epoch: 75 [24576/50048]	Loss: 0.0768
Training Epoch: 75 [24704/50048]	Loss: 0.1240
Training Epoch: 75 [24832/50048]	Loss: 0.0926
Training Epoch: 75 [24960/50048]	Loss: 0.0844
Training Epoch: 75 [25088/50048]	Loss: 0.1271
Training Epoch: 75 [25216/50048]	Loss: 0.1597
Training Epoch: 75 [25344/50048]	Loss: 0.1706
Training Epoch: 75 [25472/50048]	Loss: 0.1367
Training Epoch: 75 [25600/50048]	Loss: 0.1131
Training Epoch: 75 [25728/50048]	Loss: 0.2024
Training Epoch: 75 [25856/50048]	Loss: 0.1794
Training Epoch: 75 [25984/50048]	Loss: 0.1609
Training Epoch: 75 [26112/50048]	Loss: 0.0669
Training Epoch: 75 [26240/50048]	Loss: 0.1393
Training Epoch: 75 [26368/50048]	Loss: 0.0839
Training Epoch: 75 [26496/50048]	Loss: 0.1199
Training Epoch: 75 [26624/50048]	Loss: 0.0988
Training Epoch: 75 [26752/50048]	Loss: 0.1130
Training Epoch: 75 [26880/50048]	Loss: 0.0894
Training Epoch: 75 [27008/50048]	Loss: 0.1329
Training Epoch: 75 [27136/50048]	Loss: 0.1064
Training Epoch: 75 [27264/50048]	Loss: 0.1248
Training Epoch: 75 [27392/50048]	Loss: 0.1029
Training Epoch: 75 [27520/50048]	Loss: 0.1271
Training Epoch: 75 [27648/50048]	Loss: 0.1636
Training Epoch: 75 [27776/50048]	Loss: 0.1096
Training Epoch: 75 [27904/50048]	Loss: 0.2007
Training Epoch: 75 [28032/50048]	Loss: 0.1152
Training Epoch: 75 [28160/50048]	Loss: 0.1684
Training Epoch: 75 [28288/50048]	Loss: 0.1631
Training Epoch: 75 [28416/50048]	Loss: 0.1525
Training Epoch: 75 [28544/50048]	Loss: 0.1347
Training Epoch: 75 [28672/50048]	Loss: 0.2559
Training Epoch: 75 [28800/50048]	Loss: 0.1468
Training Epoch: 75 [28928/50048]	Loss: 0.1474
Training Epoch: 75 [29056/50048]	Loss: 0.1749
Training Epoch: 75 [29184/50048]	Loss: 0.1455
Training Epoch: 75 [29312/50048]	Loss: 0.1077
Training Epoch: 75 [29440/50048]	Loss: 0.1093
Training Epoch: 75 [29568/50048]	Loss: 0.1339
Training Epoch: 75 [29696/50048]	Loss: 0.0729
Training Epoch: 75 [29824/50048]	Loss: 0.0802
Training Epoch: 75 [29952/50048]	Loss: 0.0979
Training Epoch: 75 [30080/50048]	Loss: 0.1112
Training Epoch: 75 [30208/50048]	Loss: 0.0801
Training Epoch: 75 [30336/50048]	Loss: 0.1426
Training Epoch: 75 [30464/50048]	Loss: 0.1983
Training Epoch: 75 [30592/50048]	Loss: 0.1253
Training Epoch: 75 [30720/50048]	Loss: 0.0931
Training Epoch: 75 [30848/50048]	Loss: 0.1176
Training Epoch: 75 [30976/50048]	Loss: 0.2029
Training Epoch: 75 [31104/50048]	Loss: 0.0870
Training Epoch: 75 [31232/50048]	Loss: 0.0721
Training Epoch: 75 [31360/50048]	Loss: 0.1008
Training Epoch: 75 [31488/50048]	Loss: 0.1345
Training Epoch: 75 [31616/50048]	Loss: 0.2831
Training Epoch: 75 [31744/50048]	Loss: 0.2165
Training Epoch: 75 [31872/50048]	Loss: 0.0653
Training Epoch: 75 [32000/50048]	Loss: 0.1339
Training Epoch: 75 [32128/50048]	Loss: 0.1502
Training Epoch: 75 [32256/50048]	Loss: 0.1820
Training Epoch: 75 [32384/50048]	Loss: 0.1784
Training Epoch: 75 [32512/50048]	Loss: 0.1348
Training Epoch: 75 [32640/50048]	Loss: 0.1234
Training Epoch: 75 [32768/50048]	Loss: 0.1167
Training Epoch: 75 [32896/50048]	Loss: 0.0502
Training Epoch: 75 [33024/50048]	Loss: 0.1106
Training Epoch: 75 [33152/50048]	Loss: 0.1548
Training Epoch: 75 [33280/50048]	Loss: 0.0949
Training Epoch: 75 [33408/50048]	Loss: 0.1067
Training Epoch: 75 [33536/50048]	Loss: 0.1605
Training Epoch: 75 [33664/50048]	Loss: 0.1215
Training Epoch: 75 [33792/50048]	Loss: 0.2369
Training Epoch: 75 [33920/50048]	Loss: 0.1198
Training Epoch: 75 [34048/50048]	Loss: 0.1171
Training Epoch: 75 [34176/50048]	Loss: 0.0701
Training Epoch: 75 [34304/50048]	Loss: 0.1321
Training Epoch: 75 [34432/50048]	Loss: 0.1457
Training Epoch: 75 [34560/50048]	Loss: 0.1669
Training Epoch: 75 [34688/50048]	Loss: 0.1017
Training Epoch: 75 [34816/50048]	Loss: 0.1981
Training Epoch: 75 [34944/50048]	Loss: 0.1671
Training Epoch: 75 [35072/50048]	Loss: 0.0886
Training Epoch: 75 [35200/50048]	Loss: 0.0572
Training Epoch: 75 [35328/50048]	Loss: 0.1665
Training Epoch: 75 [35456/50048]	Loss: 0.0927
Training Epoch: 75 [35584/50048]	Loss: 0.1887
Training Epoch: 75 [35712/50048]	Loss: 0.0967
Training Epoch: 75 [35840/50048]	Loss: 0.1448
Training Epoch: 75 [35968/50048]	Loss: 0.1632
Training Epoch: 75 [36096/50048]	Loss: 0.0899
Training Epoch: 75 [36224/50048]	Loss: 0.1409
Training Epoch: 75 [36352/50048]	Loss: 0.1720
Training Epoch: 75 [36480/50048]	Loss: 0.1198
Training Epoch: 75 [36608/50048]	Loss: 0.1299
Training Epoch: 75 [36736/50048]	Loss: 0.1260
Training Epoch: 75 [36864/50048]	Loss: 0.2095
Training Epoch: 75 [36992/50048]	Loss: 0.1073
Training Epoch: 75 [37120/50048]	Loss: 0.1071
Training Epoch: 75 [37248/50048]	Loss: 0.0870
Training Epoch: 75 [37376/50048]	Loss: 0.1443
Training Epoch: 75 [37504/50048]	Loss: 0.0701
Training Epoch: 75 [37632/50048]	Loss: 0.1392
Training Epoch: 75 [37760/50048]	Loss: 0.1304
Training Epoch: 75 [37888/50048]	Loss: 0.0547
Training Epoch: 75 [38016/50048]	Loss: 0.1590
Training Epoch: 75 [38144/50048]	Loss: 0.1090
Training Epoch: 75 [38272/50048]	Loss: 0.0901
Training Epoch: 75 [38400/50048]	Loss: 0.1293
Training Epoch: 75 [38528/50048]	Loss: 0.0443
Training Epoch: 75 [38656/50048]	Loss: 0.1132
Training Epoch: 75 [38784/50048]	Loss: 0.1132
Training Epoch: 75 [38912/50048]	Loss: 0.1651
Training Epoch: 75 [39040/50048]	Loss: 0.2183
Training Epoch: 75 [39168/50048]	Loss: 0.1792
Training Epoch: 75 [39296/50048]	Loss: 0.1476
Training Epoch: 75 [39424/50048]	Loss: 0.2305
Training Epoch: 75 [39552/50048]	Loss: 0.1451
Training Epoch: 75 [39680/50048]	Loss: 0.1660
Training Epoch: 75 [39808/50048]	Loss: 0.1467
Training Epoch: 75 [39936/50048]	Loss: 0.1049
Training Epoch: 75 [40064/50048]	Loss: 0.1253
Training Epoch: 75 [40192/50048]	Loss: 0.2150
Training Epoch: 75 [40320/50048]	Loss: 0.1959
Training Epoch: 75 [40448/50048]	Loss: 0.0897
Training Epoch: 75 [40576/50048]	Loss: 0.0927
Training Epoch: 75 [40704/50048]	Loss: 0.0986
Training Epoch: 75 [40832/50048]	Loss: 0.1052
Training Epoch: 75 [40960/50048]	Loss: 0.1143
Training Epoch: 75 [41088/50048]	Loss: 0.1449
Training Epoch: 75 [41216/50048]	Loss: 0.0822
Training Epoch: 75 [41344/50048]	Loss: 0.1565
Training Epoch: 75 [41472/50048]	Loss: 0.1565
Training Epoch: 75 [41600/50048]	Loss: 0.1378
Training Epoch: 75 [41728/50048]	Loss: 0.1002
Training Epoch: 75 [41856/50048]	Loss: 0.1791
Training Epoch: 75 [41984/50048]	Loss: 0.1926
Training Epoch: 75 [42112/50048]	Loss: 0.1030
Training Epoch: 75 [42240/50048]	Loss: 0.2085
Training Epoch: 75 [42368/50048]	Loss: 0.1209
Training Epoch: 75 [42496/50048]	Loss: 0.1171
Training Epoch: 75 [42624/50048]	Loss: 0.0759
Training Epoch: 75 [42752/50048]	Loss: 0.0671
Training Epoch: 75 [42880/50048]	Loss: 0.1415
Training Epoch: 75 [43008/50048]	Loss: 0.2161
Training Epoch: 75 [43136/50048]	Loss: 0.0618
Training Epoch: 75 [43264/50048]	Loss: 0.0994
Training Epoch: 75 [43392/50048]	Loss: 0.1338
Training Epoch: 75 [43520/50048]	Loss: 0.0987
Training Epoch: 75 [43648/50048]	Loss: 0.1291
Training Epoch: 75 [43776/50048]	Loss: 0.1415
Training Epoch: 75 [43904/50048]	Loss: 0.1999
Training Epoch: 75 [44032/50048]	Loss: 0.2101
Training Epoch: 75 [44160/50048]	Loss: 0.1480
Training Epoch: 75 [44288/50048]	Loss: 0.0887
Training Epoch: 75 [44416/50048]	Loss: 0.1174
Training Epoch: 75 [44544/50048]	Loss: 0.1074
Training Epoch: 75 [44672/50048]	Loss: 0.0655
Training Epoch: 75 [44800/50048]	Loss: 0.1072
Training Epoch: 75 [44928/50048]	Loss: 0.2562
Training Epoch: 75 [45056/50048]	Loss: 0.1460
Training Epoch: 75 [45184/50048]	Loss: 0.0961
Training Epoch: 75 [45312/50048]	Loss: 0.1096
Training Epoch: 75 [45440/50048]	Loss: 0.1294
Training Epoch: 75 [45568/50048]	Loss: 0.1726
Training Epoch: 75 [45696/50048]	Loss: 0.1481
2022-12-06 05:31:37,347 [ZeusDataLoader(train)] train epoch 76 done: time=86.50 energy=10497.52
2022-12-06 05:31:37,349 [ZeusDataLoader(eval)] Epoch 76 begin.
Training Epoch: 75 [45824/50048]	Loss: 0.1620
Training Epoch: 75 [45952/50048]	Loss: 0.0944
Training Epoch: 75 [46080/50048]	Loss: 0.0680
Training Epoch: 75 [46208/50048]	Loss: 0.1532
Training Epoch: 75 [46336/50048]	Loss: 0.1645
Training Epoch: 75 [46464/50048]	Loss: 0.1281
Training Epoch: 75 [46592/50048]	Loss: 0.1325
Training Epoch: 75 [46720/50048]	Loss: 0.1099
Training Epoch: 75 [46848/50048]	Loss: 0.1968
Training Epoch: 75 [46976/50048]	Loss: 0.1048
Training Epoch: 75 [47104/50048]	Loss: 0.2005
Training Epoch: 75 [47232/50048]	Loss: 0.1683
Training Epoch: 75 [47360/50048]	Loss: 0.1199
Training Epoch: 75 [47488/50048]	Loss: 0.1618
Training Epoch: 75 [47616/50048]	Loss: 0.1316
Training Epoch: 75 [47744/50048]	Loss: 0.1134
Training Epoch: 75 [47872/50048]	Loss: 0.1178
Training Epoch: 75 [48000/50048]	Loss: 0.0924
Training Epoch: 75 [48128/50048]	Loss: 0.1027
Training Epoch: 75 [48256/50048]	Loss: 0.1558
Training Epoch: 75 [48384/50048]	Loss: 0.1683
Training Epoch: 75 [48512/50048]	Loss: 0.1135
Training Epoch: 75 [48640/50048]	Loss: 0.1170
Training Epoch: 75 [48768/50048]	Loss: 0.1391
Training Epoch: 75 [48896/50048]	Loss: 0.1479
Training Epoch: 75 [49024/50048]	Loss: 0.2337
Training Epoch: 75 [49152/50048]	Loss: 0.0634
Training Epoch: 75 [49280/50048]	Loss: 0.1114
Training Epoch: 75 [49408/50048]	Loss: 0.1778
Training Epoch: 75 [49536/50048]	Loss: 0.1429
Training Epoch: 75 [49664/50048]	Loss: 0.1743
Training Epoch: 75 [49792/50048]	Loss: 0.1396
Training Epoch: 75 [49920/50048]	Loss: 0.1028
Training Epoch: 75 [50048/50048]	Loss: 0.1081
2022-12-06 10:31:41.059 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:31:41,098 [ZeusDataLoader(eval)] eval epoch 76 done: time=3.74 energy=455.94
2022-12-06 05:31:41,098 [ZeusDataLoader(train)] Up to epoch 76: time=6857.17, energy=832410.86, cost=1016207.52
2022-12-06 05:31:41,099 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:31:41,099 [ZeusDataLoader(train)] Expected next epoch: time=6946.97, energy=843208.87, cost=1029463.90
2022-12-06 05:31:41,100 [ZeusDataLoader(train)] Epoch 77 begin.
Validation Epoch: 75, Average loss: 0.0176, Accuracy: 0.6338
2022-12-06 05:31:41,271 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:31:41,272 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:31:41.289 [ZeusMonitor] Monitor started.
2022-12-06 10:31:41.290 [ZeusMonitor] Running indefinitely. 2022-12-06 10:31:41.290 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:31:41.290 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e77+gpu0.power.log
Training Epoch: 76 [128/50048]	Loss: 0.1032
Training Epoch: 76 [256/50048]	Loss: 0.0938
Training Epoch: 76 [384/50048]	Loss: 0.1579
Training Epoch: 76 [512/50048]	Loss: 0.1053
Training Epoch: 76 [640/50048]	Loss: 0.1043
Training Epoch: 76 [768/50048]	Loss: 0.1359
Training Epoch: 76 [896/50048]	Loss: 0.1246
Training Epoch: 76 [1024/50048]	Loss: 0.0899
Training Epoch: 76 [1152/50048]	Loss: 0.1455
Training Epoch: 76 [1280/50048]	Loss: 0.0968
Training Epoch: 76 [1408/50048]	Loss: 0.0728
Training Epoch: 76 [1536/50048]	Loss: 0.1460
Training Epoch: 76 [1664/50048]	Loss: 0.0869
Training Epoch: 76 [1792/50048]	Loss: 0.0624
Training Epoch: 76 [1920/50048]	Loss: 0.0696
Training Epoch: 76 [2048/50048]	Loss: 0.0807
Training Epoch: 76 [2176/50048]	Loss: 0.1387
Training Epoch: 76 [2304/50048]	Loss: 0.1109
Training Epoch: 76 [2432/50048]	Loss: 0.1368
Training Epoch: 76 [2560/50048]	Loss: 0.0908
Training Epoch: 76 [2688/50048]	Loss: 0.1164
Training Epoch: 76 [2816/50048]	Loss: 0.0885
Training Epoch: 76 [2944/50048]	Loss: 0.1564
Training Epoch: 76 [3072/50048]	Loss: 0.1452
Training Epoch: 76 [3200/50048]	Loss: 0.0646
Training Epoch: 76 [3328/50048]	Loss: 0.0893
Training Epoch: 76 [3456/50048]	Loss: 0.0928
Training Epoch: 76 [3584/50048]	Loss: 0.1004
Training Epoch: 76 [3712/50048]	Loss: 0.1208
Training Epoch: 76 [3840/50048]	Loss: 0.0872
Training Epoch: 76 [3968/50048]	Loss: 0.1330
Training Epoch: 76 [4096/50048]	Loss: 0.1371
Training Epoch: 76 [4224/50048]	Loss: 0.0568
Training Epoch: 76 [4352/50048]	Loss: 0.1110
Training Epoch: 76 [4480/50048]	Loss: 0.0956
Training Epoch: 76 [4608/50048]	Loss: 0.1265
Training Epoch: 76 [4736/50048]	Loss: 0.0975
Training Epoch: 76 [4864/50048]	Loss: 0.1262
Training Epoch: 76 [4992/50048]	Loss: 0.0832
Training Epoch: 76 [5120/50048]	Loss: 0.1305
Training Epoch: 76 [5248/50048]	Loss: 0.0781
Training Epoch: 76 [5376/50048]	Loss: 0.1171
Training Epoch: 76 [5504/50048]	Loss: 0.1261
Training Epoch: 76 [5632/50048]	Loss: 0.1860
Training Epoch: 76 [5760/50048]	Loss: 0.1486
Training Epoch: 76 [5888/50048]	Loss: 0.1093
Training Epoch: 76 [6016/50048]	Loss: 0.0828
Training Epoch: 76 [6144/50048]	Loss: 0.0695
Training Epoch: 76 [6272/50048]	Loss: 0.1225
Training Epoch: 76 [6400/50048]	Loss: 0.1068
Training Epoch: 76 [6528/50048]	Loss: 0.0836
Training Epoch: 76 [6656/50048]	Loss: 0.1032
Training Epoch: 76 [6784/50048]	Loss: 0.0671
Training Epoch: 76 [6912/50048]	Loss: 0.0897
Training Epoch: 76 [7040/50048]	Loss: 0.1046
Training Epoch: 76 [7168/50048]	Loss: 0.1286
Training Epoch: 76 [7296/50048]	Loss: 0.0762
Training Epoch: 76 [7424/50048]	Loss: 0.0854
Training Epoch: 76 [7552/50048]	Loss: 0.0622
Training Epoch: 76 [7680/50048]	Loss: 0.0850
Training Epoch: 76 [7808/50048]	Loss: 0.0678
Training Epoch: 76 [7936/50048]	Loss: 0.0656
Training Epoch: 76 [8064/50048]	Loss: 0.1474
Training Epoch: 76 [8192/50048]	Loss: 0.1344
Training Epoch: 76 [8320/50048]	Loss: 0.0900
Training Epoch: 76 [8448/50048]	Loss: 0.0658
Training Epoch: 76 [8576/50048]	Loss: 0.1317
Training Epoch: 76 [8704/50048]	Loss: 0.0570
Training Epoch: 76 [8832/50048]	Loss: 0.1335
Training Epoch: 76 [8960/50048]	Loss: 0.1661
Training Epoch: 76 [9088/50048]	Loss: 0.1088
Training Epoch: 76 [9216/50048]	Loss: 0.1252
Training Epoch: 76 [9344/50048]	Loss: 0.0796
Training Epoch: 76 [9472/50048]	Loss: 0.0974
Training Epoch: 76 [9600/50048]	Loss: 0.0960
Training Epoch: 76 [9728/50048]	Loss: 0.1006
Training Epoch: 76 [9856/50048]	Loss: 0.0790
Training Epoch: 76 [9984/50048]	Loss: 0.1020
Training Epoch: 76 [10112/50048]	Loss: 0.1287
Training Epoch: 76 [10240/50048]	Loss: 0.0930
Training Epoch: 76 [10368/50048]	Loss: 0.1321
Training Epoch: 76 [10496/50048]	Loss: 0.0825
Training Epoch: 76 [10624/50048]	Loss: 0.1959
Training Epoch: 76 [10752/50048]	Loss: 0.1494
Training Epoch: 76 [10880/50048]	Loss: 0.1393
Training Epoch: 76 [11008/50048]	Loss: 0.1314
Training Epoch: 76 [11136/50048]	Loss: 0.1545
Training Epoch: 76 [11264/50048]	Loss: 0.0697
Training Epoch: 76 [11392/50048]	Loss: 0.0707
Training Epoch: 76 [11520/50048]	Loss: 0.0831
Training Epoch: 76 [11648/50048]	Loss: 0.0819
Training Epoch: 76 [11776/50048]	Loss: 0.0621
Training Epoch: 76 [11904/50048]	Loss: 0.1486
Training Epoch: 76 [12032/50048]	Loss: 0.0531
Training Epoch: 76 [12160/50048]	Loss: 0.1220
Training Epoch: 76 [12288/50048]	Loss: 0.0906
Training Epoch: 76 [12416/50048]	Loss: 0.0816
Training Epoch: 76 [12544/50048]	Loss: 0.1036
Training Epoch: 76 [12672/50048]	Loss: 0.1314
Training Epoch: 76 [12800/50048]	Loss: 0.0814
Training Epoch: 76 [12928/50048]	Loss: 0.0633
Training Epoch: 76 [13056/50048]	Loss: 0.0888
Training Epoch: 76 [13184/50048]	Loss: 0.0894
Training Epoch: 76 [13312/50048]	Loss: 0.1063
Training Epoch: 76 [13440/50048]	Loss: 0.0528
Training Epoch: 76 [13568/50048]	Loss: 0.0971
Training Epoch: 76 [13696/50048]	Loss: 0.1093
Training Epoch: 76 [13824/50048]	Loss: 0.0535
Training Epoch: 76 [13952/50048]	Loss: 0.1042
Training Epoch: 76 [14080/50048]	Loss: 0.0625
Training Epoch: 76 [14208/50048]	Loss: 0.1020
Training Epoch: 76 [14336/50048]	Loss: 0.1554
Training Epoch: 76 [14464/50048]	Loss: 0.1390
Training Epoch: 76 [14592/50048]	Loss: 0.1152
Training Epoch: 76 [14720/50048]	Loss: 0.1116
Training Epoch: 76 [14848/50048]	Loss: 0.1045
Training Epoch: 76 [14976/50048]	Loss: 0.1572
Training Epoch: 76 [15104/50048]	Loss: 0.1075
Training Epoch: 76 [15232/50048]	Loss: 0.1307
Training Epoch: 76 [15360/50048]	Loss: 0.0846
Training Epoch: 76 [15488/50048]	Loss: 0.1162
Training Epoch: 76 [15616/50048]	Loss: 0.0641
Training Epoch: 76 [15744/50048]	Loss: 0.1028
Training Epoch: 76 [15872/50048]	Loss: 0.1533
Training Epoch: 76 [16000/50048]	Loss: 0.1021
Training Epoch: 76 [16128/50048]	Loss: 0.1246
Training Epoch: 76 [16256/50048]	Loss: 0.0707
Training Epoch: 76 [16384/50048]	Loss: 0.1070
Training Epoch: 76 [16512/50048]	Loss: 0.0545
Training Epoch: 76 [16640/50048]	Loss: 0.0388
Training Epoch: 76 [16768/50048]	Loss: 0.1229
Training Epoch: 76 [16896/50048]	Loss: 0.1615
Training Epoch: 76 [17024/50048]	Loss: 0.0976
Training Epoch: 76 [17152/50048]	Loss: 0.1258
Training Epoch: 76 [17280/50048]	Loss: 0.0883
Training Epoch: 76 [17408/50048]	Loss: 0.1349
Training Epoch: 76 [17536/50048]	Loss: 0.2480
Training Epoch: 76 [17664/50048]	Loss: 0.1436
Training Epoch: 76 [17792/50048]	Loss: 0.1313
Training Epoch: 76 [17920/50048]	Loss: 0.0791
Training Epoch: 76 [18048/50048]	Loss: 0.0926
Training Epoch: 76 [18176/50048]	Loss: 0.0654
Training Epoch: 76 [18304/50048]	Loss: 0.1458
Training Epoch: 76 [18432/50048]	Loss: 0.1016
Training Epoch: 76 [18560/50048]	Loss: 0.1862
Training Epoch: 76 [18688/50048]	Loss: 0.0779
Training Epoch: 76 [18816/50048]	Loss: 0.0837
Training Epoch: 76 [18944/50048]	Loss: 0.1191
Training Epoch: 76 [19072/50048]	Loss: 0.1015
Training Epoch: 76 [19200/50048]	Loss: 0.1914
Training Epoch: 76 [19328/50048]	Loss: 0.1150
Training Epoch: 76 [19456/50048]	Loss: 0.0987
Training Epoch: 76 [19584/50048]	Loss: 0.1423
Training Epoch: 76 [19712/50048]	Loss: 0.1293
Training Epoch: 76 [19840/50048]	Loss: 0.1453
Training Epoch: 76 [19968/50048]	Loss: 0.0807
Training Epoch: 76 [20096/50048]	Loss: 0.0770
Training Epoch: 76 [20224/50048]	Loss: 0.1623
Training Epoch: 76 [20352/50048]	Loss: 0.1512
Training Epoch: 76 [20480/50048]	Loss: 0.1177
Training Epoch: 76 [20608/50048]	Loss: 0.1651
Training Epoch: 76 [20736/50048]	Loss: 0.1352
Training Epoch: 76 [20864/50048]	Loss: 0.0777
Training Epoch: 76 [20992/50048]	Loss: 0.1171
Training Epoch: 76 [21120/50048]	Loss: 0.1859
Training Epoch: 76 [21248/50048]	Loss: 0.1726
Training Epoch: 76 [21376/50048]	Loss: 0.0690
Training Epoch: 76 [21504/50048]	Loss: 0.0699
Training Epoch: 76 [21632/50048]	Loss: 0.1257
Training Epoch: 76 [21760/50048]	Loss: 0.0527
Training Epoch: 76 [21888/50048]	Loss: 0.1834
Training Epoch: 76 [22016/50048]	Loss: 0.1598
Training Epoch: 76 [22144/50048]	Loss: 0.1222
Training Epoch: 76 [22272/50048]	Loss: 0.0829
Training Epoch: 76 [22400/50048]	Loss: 0.0837
Training Epoch: 76 [22528/50048]	Loss: 0.1091
Training Epoch: 76 [22656/50048]	Loss: 0.0933
Training Epoch: 76 [22784/50048]	Loss: 0.0549
Training Epoch: 76 [22912/50048]	Loss: 0.1145
Training Epoch: 76 [23040/50048]	Loss: 0.1255
Training Epoch: 76 [23168/50048]	Loss: 0.1641
Training Epoch: 76 [23296/50048]	Loss: 0.0605
Training Epoch: 76 [23424/50048]	Loss: 0.0567
Training Epoch: 76 [23552/50048]	Loss: 0.0808
Training Epoch: 76 [23680/50048]	Loss: 0.1262
Training Epoch: 76 [23808/50048]	Loss: 0.1417
Training Epoch: 76 [23936/50048]	Loss: 0.0940
Training Epoch: 76 [24064/50048]	Loss: 0.1482
Training Epoch: 76 [24192/50048]	Loss: 0.2057
Training Epoch: 76 [24320/50048]	Loss: 0.1734
Training Epoch: 76 [24448/50048]	Loss: 0.1012
Training Epoch: 76 [24576/50048]	Loss: 0.1271
Training Epoch: 76 [24704/50048]	Loss: 0.0997
Training Epoch: 76 [24832/50048]	Loss: 0.1059
Training Epoch: 76 [24960/50048]	Loss: 0.0988
Training Epoch: 76 [25088/50048]	Loss: 0.1232
Training Epoch: 76 [25216/50048]	Loss: 0.1370
Training Epoch: 76 [25344/50048]	Loss: 0.0914
Training Epoch: 76 [25472/50048]	Loss: 0.0988
Training Epoch: 76 [25600/50048]	Loss: 0.0649
Training Epoch: 76 [25728/50048]	Loss: 0.0962
Training Epoch: 76 [25856/50048]	Loss: 0.1232
Training Epoch: 76 [25984/50048]	Loss: 0.1380
Training Epoch: 76 [26112/50048]	Loss: 0.0813
Training Epoch: 76 [26240/50048]	Loss: 0.1848
Training Epoch: 76 [26368/50048]	Loss: 0.0744
Training Epoch: 76 [26496/50048]	Loss: 0.1361
Training Epoch: 76 [26624/50048]	Loss: 0.1158
Training Epoch: 76 [26752/50048]	Loss: 0.1018
Training Epoch: 76 [26880/50048]	Loss: 0.1163
Training Epoch: 76 [27008/50048]	Loss: 0.1516
Training Epoch: 76 [27136/50048]	Loss: 0.1448
Training Epoch: 76 [27264/50048]	Loss: 0.1020
Training Epoch: 76 [27392/50048]	Loss: 0.1406
Training Epoch: 76 [27520/50048]	Loss: 0.1255
Training Epoch: 76 [27648/50048]	Loss: 0.0916
Training Epoch: 76 [27776/50048]	Loss: 0.1010
Training Epoch: 76 [27904/50048]	Loss: 0.1461
Training Epoch: 76 [28032/50048]	Loss: 0.1196
Training Epoch: 76 [28160/50048]	Loss: 0.0916
Training Epoch: 76 [28288/50048]	Loss: 0.1566
Training Epoch: 76 [28416/50048]	Loss: 0.0946
Training Epoch: 76 [28544/50048]	Loss: 0.1569
Training Epoch: 76 [28672/50048]	Loss: 0.1124
Training Epoch: 76 [28800/50048]	Loss: 0.1451
Training Epoch: 76 [28928/50048]	Loss: 0.0997
Training Epoch: 76 [29056/50048]	Loss: 0.1044
Training Epoch: 76 [29184/50048]	Loss: 0.0702
Training Epoch: 76 [29312/50048]	Loss: 0.1619
Training Epoch: 76 [29440/50048]	Loss: 0.0798
Training Epoch: 76 [29568/50048]	Loss: 0.0634
Training Epoch: 76 [29696/50048]	Loss: 0.0971
Training Epoch: 76 [29824/50048]	Loss: 0.1063
Training Epoch: 76 [29952/50048]	Loss: 0.0819
Training Epoch: 76 [30080/50048]	Loss: 0.1132
Training Epoch: 76 [30208/50048]	Loss: 0.2047
Training Epoch: 76 [30336/50048]	Loss: 0.1669
Training Epoch: 76 [30464/50048]	Loss: 0.0482
Training Epoch: 76 [30592/50048]	Loss: 0.1450
Training Epoch: 76 [30720/50048]	Loss: 0.0810
Training Epoch: 76 [30848/50048]	Loss: 0.0665
Training Epoch: 76 [30976/50048]	Loss: 0.1203
Training Epoch: 76 [31104/50048]	Loss: 0.0945
Training Epoch: 76 [31232/50048]	Loss: 0.1439
Training Epoch: 76 [31360/50048]	Loss: 0.0688
Training Epoch: 76 [31488/50048]	Loss: 0.0947
Training Epoch: 76 [31616/50048]	Loss: 0.0971
Training Epoch: 76 [31744/50048]	Loss: 0.1177
Training Epoch: 76 [31872/50048]	Loss: 0.1294
Training Epoch: 76 [32000/50048]	Loss: 0.1064
Training Epoch: 76 [32128/50048]	Loss: 0.0840
Training Epoch: 76 [32256/50048]	Loss: 0.0979
Training Epoch: 76 [32384/50048]	Loss: 0.1112
Training Epoch: 76 [32512/50048]	Loss: 0.1170
Training Epoch: 76 [32640/50048]	Loss: 0.1255
Training Epoch: 76 [32768/50048]	Loss: 0.1333
Training Epoch: 76 [32896/50048]	Loss: 0.1138
Training Epoch: 76 [33024/50048]	Loss: 0.1370
Training Epoch: 76 [33152/50048]	Loss: 0.1096
Training Epoch: 76 [33280/50048]	Loss: 0.1611
Training Epoch: 76 [33408/50048]	Loss: 0.2180
Training Epoch: 76 [33536/50048]	Loss: 0.1469
Training Epoch: 76 [33664/50048]	Loss: 0.1014
Training Epoch: 76 [33792/50048]	Loss: 0.1079
Training Epoch: 76 [33920/50048]	Loss: 0.0914
Training Epoch: 76 [34048/50048]	Loss: 0.1702
Training Epoch: 76 [34176/50048]	Loss: 0.0771
Training Epoch: 76 [34304/50048]	Loss: 0.1507
Training Epoch: 76 [34432/50048]	Loss: 0.1489
Training Epoch: 76 [34560/50048]	Loss: 0.1576
Training Epoch: 76 [34688/50048]	Loss: 0.1386
Training Epoch: 76 [34816/50048]	Loss: 0.1475
Training Epoch: 76 [34944/50048]	Loss: 0.1385
Training Epoch: 76 [35072/50048]	Loss: 0.1805
Training Epoch: 76 [35200/50048]	Loss: 0.1614
Training Epoch: 76 [35328/50048]	Loss: 0.1414
Training Epoch: 76 [35456/50048]	Loss: 0.0970
Training Epoch: 76 [35584/50048]	Loss: 0.0698
Training Epoch: 76 [35712/50048]	Loss: 0.2049
Training Epoch: 76 [35840/50048]	Loss: 0.0959
Training Epoch: 76 [35968/50048]	Loss: 0.1511
Training Epoch: 76 [36096/50048]	Loss: 0.1500
Training Epoch: 76 [36224/50048]	Loss: 0.1240
Training Epoch: 76 [36352/50048]	Loss: 0.1504
Training Epoch: 76 [36480/50048]	Loss: 0.1360
Training Epoch: 76 [36608/50048]	Loss: 0.1512
Training Epoch: 76 [36736/50048]	Loss: 0.1001
Training Epoch: 76 [36864/50048]	Loss: 0.1316
Training Epoch: 76 [36992/50048]	Loss: 0.1834
Training Epoch: 76 [37120/50048]	Loss: 0.1174
Training Epoch: 76 [37248/50048]	Loss: 0.1847
Training Epoch: 76 [37376/50048]	Loss: 0.0616
Training Epoch: 76 [37504/50048]	Loss: 0.1042
Training Epoch: 76 [37632/50048]	Loss: 0.1314
Training Epoch: 76 [37760/50048]	Loss: 0.0790
Training Epoch: 76 [37888/50048]	Loss: 0.0884
Training Epoch: 76 [38016/50048]	Loss: 0.1203
Training Epoch: 76 [38144/50048]	Loss: 0.1520
Training Epoch: 76 [38272/50048]	Loss: 0.2234
Training Epoch: 76 [38400/50048]	Loss: 0.1170
Training Epoch: 76 [38528/50048]	Loss: 0.2201
Training Epoch: 76 [38656/50048]	Loss: 0.0899
Training Epoch: 76 [38784/50048]	Loss: 0.1850
Training Epoch: 76 [38912/50048]	Loss: 0.1184
Training Epoch: 76 [39040/50048]	Loss: 0.1188
Training Epoch: 76 [39168/50048]	Loss: 0.1693
Training Epoch: 76 [39296/50048]	Loss: 0.0987
Training Epoch: 76 [39424/50048]	Loss: 0.1116
Training Epoch: 76 [39552/50048]	Loss: 0.2388
Training Epoch: 76 [39680/50048]	Loss: 0.0641
Training Epoch: 76 [39808/50048]	Loss: 0.1403
Training Epoch: 76 [39936/50048]	Loss: 0.2804
Training Epoch: 76 [40064/50048]	Loss: 0.2054
Training Epoch: 76 [40192/50048]	Loss: 0.1624
Training Epoch: 76 [40320/50048]	Loss: 0.1138
Training Epoch: 76 [40448/50048]	Loss: 0.2080
Training Epoch: 76 [40576/50048]	Loss: 0.2166
Training Epoch: 76 [40704/50048]	Loss: 0.1274
Training Epoch: 76 [40832/50048]	Loss: 0.0913
Training Epoch: 76 [40960/50048]	Loss: 0.1534
Training Epoch: 76 [41088/50048]	Loss: 0.1619
Training Epoch: 76 [41216/50048]	Loss: 0.1443
Training Epoch: 76 [41344/50048]	Loss: 0.1926
Training Epoch: 76 [41472/50048]	Loss: 0.0639
Training Epoch: 76 [41600/50048]	Loss: 0.0650
Training Epoch: 76 [41728/50048]	Loss: 0.0870
Training Epoch: 76 [41856/50048]	Loss: 0.1231
Training Epoch: 76 [41984/50048]	Loss: 0.1047
Training Epoch: 76 [42112/50048]	Loss: 0.1881
Training Epoch: 76 [42240/50048]	Loss: 0.0983
Training Epoch: 76 [42368/50048]	Loss: 0.1021
Training Epoch: 76 [42496/50048]	Loss: 0.1018
Training Epoch: 76 [42624/50048]	Loss: 0.1312
Training Epoch: 76 [42752/50048]	Loss: 0.1452
Training Epoch: 76 [42880/50048]	Loss: 0.0922
Training Epoch: 76 [43008/50048]	Loss: 0.1133
Training Epoch: 76 [43136/50048]	Loss: 0.1748
Training Epoch: 76 [43264/50048]	Loss: 0.1187
Training Epoch: 76 [43392/50048]	Loss: 0.1450
Training Epoch: 76 [43520/50048]	Loss: 0.1098
Training Epoch: 76 [43648/50048]	Loss: 0.2154
Training Epoch: 76 [43776/50048]	Loss: 0.0843
Training Epoch: 76 [43904/50048]	Loss: 0.2369
Training Epoch: 76 [44032/50048]	Loss: 0.1036
Training Epoch: 76 [44160/50048]	Loss: 0.1158
Training Epoch: 76 [44288/50048]	Loss: 0.1299
Training Epoch: 76 [44416/50048]	Loss: 0.0761
Training Epoch: 76 [44544/50048]	Loss: 0.1406
Training Epoch: 76 [44672/50048]	Loss: 0.1658
Training Epoch: 76 [44800/50048]	Loss: 0.1349
Training Epoch: 76 [44928/50048]	Loss: 0.1539
Training Epoch: 76 [45056/50048]	Loss: 0.1957
Training Epoch: 76 [45184/50048]	Loss: 0.1481
Training Epoch: 76 [45312/50048]	Loss: 0.1757
Training Epoch: 76 [45440/50048]	Loss: 0.1237
Training Epoch: 76 [45568/50048]	Loss: 0.1531
Training Epoch: 76 [45696/50048]	Loss: 0.1501
2022-12-06 05:33:07,486 [ZeusDataLoader(train)] train epoch 77 done: time=86.38 energy=10487.91
2022-12-06 05:33:07,488 [ZeusDataLoader(eval)] Epoch 77 begin.
Training Epoch: 76 [45824/50048]	Loss: 0.1505
Training Epoch: 76 [45952/50048]	Loss: 0.0741
Training Epoch: 76 [46080/50048]	Loss: 0.1682
Training Epoch: 76 [46208/50048]	Loss: 0.1337
Training Epoch: 76 [46336/50048]	Loss: 0.0884
Training Epoch: 76 [46464/50048]	Loss: 0.1654
Training Epoch: 76 [46592/50048]	Loss: 0.1296
Training Epoch: 76 [46720/50048]	Loss: 0.1284
Training Epoch: 76 [46848/50048]	Loss: 0.1534
Training Epoch: 76 [46976/50048]	Loss: 0.0941
Training Epoch: 76 [47104/50048]	Loss: 0.0556
Training Epoch: 76 [47232/50048]	Loss: 0.1455
Training Epoch: 76 [47360/50048]	Loss: 0.1838
Training Epoch: 76 [47488/50048]	Loss: 0.2167
Training Epoch: 76 [47616/50048]	Loss: 0.1254
Training Epoch: 76 [47744/50048]	Loss: 0.1103
Training Epoch: 76 [47872/50048]	Loss: 0.1048
Training Epoch: 76 [48000/50048]	Loss: 0.0952
Training Epoch: 76 [48128/50048]	Loss: 0.2255
Training Epoch: 76 [48256/50048]	Loss: 0.1268
Training Epoch: 76 [48384/50048]	Loss: 0.0979
Training Epoch: 76 [48512/50048]	Loss: 0.2082
Training Epoch: 76 [48640/50048]	Loss: 0.1674
Training Epoch: 76 [48768/50048]	Loss: 0.1690
Training Epoch: 76 [48896/50048]	Loss: 0.1645
Training Epoch: 76 [49024/50048]	Loss: 0.1775
Training Epoch: 76 [49152/50048]	Loss: 0.1833
Training Epoch: 76 [49280/50048]	Loss: 0.2177
Training Epoch: 76 [49408/50048]	Loss: 0.1086
Training Epoch: 76 [49536/50048]	Loss: 0.1260
Training Epoch: 76 [49664/50048]	Loss: 0.1597
Training Epoch: 76 [49792/50048]	Loss: 0.1111
Training Epoch: 76 [49920/50048]	Loss: 0.1207
Training Epoch: 76 [50048/50048]	Loss: 0.2253
2022-12-06 10:33:11.214 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:33:11,266 [ZeusDataLoader(eval)] eval epoch 77 done: time=3.77 energy=451.98
2022-12-06 05:33:11,267 [ZeusDataLoader(train)] Up to epoch 77: time=6947.31, energy=843350.75, cost=1029565.20
2022-12-06 05:33:11,267 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:33:11,267 [ZeusDataLoader(train)] Expected next epoch: time=7037.11, energy=854148.76, cost=1042821.58
2022-12-06 05:33:11,268 [ZeusDataLoader(train)] Epoch 78 begin.
Validation Epoch: 76, Average loss: 0.0181, Accuracy: 0.6357
2022-12-06 05:33:11,457 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:33:11,458 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:33:11.460 [ZeusMonitor] Monitor started.
2022-12-06 10:33:11.460 [ZeusMonitor] Running indefinitely. 2022-12-06 10:33:11.460 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:33:11.460 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e78+gpu0.power.log
Training Epoch: 77 [128/50048]	Loss: 0.1385
Training Epoch: 77 [256/50048]	Loss: 0.0713
Training Epoch: 77 [384/50048]	Loss: 0.0536
Training Epoch: 77 [512/50048]	Loss: 0.2229
Training Epoch: 77 [640/50048]	Loss: 0.0966
Training Epoch: 77 [768/50048]	Loss: 0.1160
Training Epoch: 77 [896/50048]	Loss: 0.0953
Training Epoch: 77 [1024/50048]	Loss: 0.1279
Training Epoch: 77 [1152/50048]	Loss: 0.1545
Training Epoch: 77 [1280/50048]	Loss: 0.0933
Training Epoch: 77 [1408/50048]	Loss: 0.0809
Training Epoch: 77 [1536/50048]	Loss: 0.1657
Training Epoch: 77 [1664/50048]	Loss: 0.0998
Training Epoch: 77 [1792/50048]	Loss: 0.1265
Training Epoch: 77 [1920/50048]	Loss: 0.0975
Training Epoch: 77 [2048/50048]	Loss: 0.1052
Training Epoch: 77 [2176/50048]	Loss: 0.1986
Training Epoch: 77 [2304/50048]	Loss: 0.1066
Training Epoch: 77 [2432/50048]	Loss: 0.1005
Training Epoch: 77 [2560/50048]	Loss: 0.0743
Training Epoch: 77 [2688/50048]	Loss: 0.0629
Training Epoch: 77 [2816/50048]	Loss: 0.1184
Training Epoch: 77 [2944/50048]	Loss: 0.0972
Training Epoch: 77 [3072/50048]	Loss: 0.1349
Training Epoch: 77 [3200/50048]	Loss: 0.1145
Training Epoch: 77 [3328/50048]	Loss: 0.1257
Training Epoch: 77 [3456/50048]	Loss: 0.1496
Training Epoch: 77 [3584/50048]	Loss: 0.1246
Training Epoch: 77 [3712/50048]	Loss: 0.1224
Training Epoch: 77 [3840/50048]	Loss: 0.0889
Training Epoch: 77 [3968/50048]	Loss: 0.1593
Training Epoch: 77 [4096/50048]	Loss: 0.1281
Training Epoch: 77 [4224/50048]	Loss: 0.0788
Training Epoch: 77 [4352/50048]	Loss: 0.0907
Training Epoch: 77 [4480/50048]	Loss: 0.1757
Training Epoch: 77 [4608/50048]	Loss: 0.1418
Training Epoch: 77 [4736/50048]	Loss: 0.1812
Training Epoch: 77 [4864/50048]	Loss: 0.1685
Training Epoch: 77 [4992/50048]	Loss: 0.0515
Training Epoch: 77 [5120/50048]	Loss: 0.0732
Training Epoch: 77 [5248/50048]	Loss: 0.0966
Training Epoch: 77 [5376/50048]	Loss: 0.0517
Training Epoch: 77 [5504/50048]	Loss: 0.1690
Training Epoch: 77 [5632/50048]	Loss: 0.1202
Training Epoch: 77 [5760/50048]	Loss: 0.1358
Training Epoch: 77 [5888/50048]	Loss: 0.1152
Training Epoch: 77 [6016/50048]	Loss: 0.1052
Training Epoch: 77 [6144/50048]	Loss: 0.0688
Training Epoch: 77 [6272/50048]	Loss: 0.1267
Training Epoch: 77 [6400/50048]	Loss: 0.1159
Training Epoch: 77 [6528/50048]	Loss: 0.0881
Training Epoch: 77 [6656/50048]	Loss: 0.1087
Training Epoch: 77 [6784/50048]	Loss: 0.0986
Training Epoch: 77 [6912/50048]	Loss: 0.0953
Training Epoch: 77 [7040/50048]	Loss: 0.1287
Training Epoch: 77 [7168/50048]	Loss: 0.0994
Training Epoch: 77 [7296/50048]	Loss: 0.0877
Training Epoch: 77 [7424/50048]	Loss: 0.1360
Training Epoch: 77 [7552/50048]	Loss: 0.1470
Training Epoch: 77 [7680/50048]	Loss: 0.1007
Training Epoch: 77 [7808/50048]	Loss: 0.0882
Training Epoch: 77 [7936/50048]	Loss: 0.1459
Training Epoch: 77 [8064/50048]	Loss: 0.1926
Training Epoch: 77 [8192/50048]	Loss: 0.0879
Training Epoch: 77 [8320/50048]	Loss: 0.0894
Training Epoch: 77 [8448/50048]	Loss: 0.0989
Training Epoch: 77 [8576/50048]	Loss: 0.1337
Training Epoch: 77 [8704/50048]	Loss: 0.1034
Training Epoch: 77 [8832/50048]	Loss: 0.1350
Training Epoch: 77 [8960/50048]	Loss: 0.0881
Training Epoch: 77 [9088/50048]	Loss: 0.2381
Training Epoch: 77 [9216/50048]	Loss: 0.0427
Training Epoch: 77 [9344/50048]	Loss: 0.1254
Training Epoch: 77 [9472/50048]	Loss: 0.0873
Training Epoch: 77 [9600/50048]	Loss: 0.1153
Training Epoch: 77 [9728/50048]	Loss: 0.1627
Training Epoch: 77 [9856/50048]	Loss: 0.1660
Training Epoch: 77 [9984/50048]	Loss: 0.1160
Training Epoch: 77 [10112/50048]	Loss: 0.1247
Training Epoch: 77 [10240/50048]	Loss: 0.0996
Training Epoch: 77 [10368/50048]	Loss: 0.1025
Training Epoch: 77 [10496/50048]	Loss: 0.0857
Training Epoch: 77 [10624/50048]	Loss: 0.0572
Training Epoch: 77 [10752/50048]	Loss: 0.0661
Training Epoch: 77 [10880/50048]	Loss: 0.0983
Training Epoch: 77 [11008/50048]	Loss: 0.1923
Training Epoch: 77 [11136/50048]	Loss: 0.1393
Training Epoch: 77 [11264/50048]	Loss: 0.0671
Training Epoch: 77 [11392/50048]	Loss: 0.0737
Training Epoch: 77 [11520/50048]	Loss: 0.1370
Training Epoch: 77 [11648/50048]	Loss: 0.1286
Training Epoch: 77 [11776/50048]	Loss: 0.2132
Training Epoch: 77 [11904/50048]	Loss: 0.0491
Training Epoch: 77 [12032/50048]	Loss: 0.1072
Training Epoch: 77 [12160/50048]	Loss: 0.2407
Training Epoch: 77 [12288/50048]	Loss: 0.0662
Training Epoch: 77 [12416/50048]	Loss: 0.0829
Training Epoch: 77 [12544/50048]	Loss: 0.0963
Training Epoch: 77 [12672/50048]	Loss: 0.0931
Training Epoch: 77 [12800/50048]	Loss: 0.1047
Training Epoch: 77 [12928/50048]	Loss: 0.0690
Training Epoch: 77 [13056/50048]	Loss: 0.1880
Training Epoch: 77 [13184/50048]	Loss: 0.0812
Training Epoch: 77 [13312/50048]	Loss: 0.1206
Training Epoch: 77 [13440/50048]	Loss: 0.0683
Training Epoch: 77 [13568/50048]	Loss: 0.0851
Training Epoch: 77 [13696/50048]	Loss: 0.0890
Training Epoch: 77 [13824/50048]	Loss: 0.1168
Training Epoch: 77 [13952/50048]	Loss: 0.1340
Training Epoch: 77 [14080/50048]	Loss: 0.1164
Training Epoch: 77 [14208/50048]	Loss: 0.0919
Training Epoch: 77 [14336/50048]	Loss: 0.1412
Training Epoch: 77 [14464/50048]	Loss: 0.1208
Training Epoch: 77 [14592/50048]	Loss: 0.0934
Training Epoch: 77 [14720/50048]	Loss: 0.1254
Training Epoch: 77 [14848/50048]	Loss: 0.1422
Training Epoch: 77 [14976/50048]	Loss: 0.1549
Training Epoch: 77 [15104/50048]	Loss: 0.1074
Training Epoch: 77 [15232/50048]	Loss: 0.1605
Training Epoch: 77 [15360/50048]	Loss: 0.0642
Training Epoch: 77 [15488/50048]	Loss: 0.1693
Training Epoch: 77 [15616/50048]	Loss: 0.0812
Training Epoch: 77 [15744/50048]	Loss: 0.0881
Training Epoch: 77 [15872/50048]	Loss: 0.1316
Training Epoch: 77 [16000/50048]	Loss: 0.0660
Training Epoch: 77 [16128/50048]	Loss: 0.0756
Training Epoch: 77 [16256/50048]	Loss: 0.0684
Training Epoch: 77 [16384/50048]	Loss: 0.1108
Training Epoch: 77 [16512/50048]	Loss: 0.1690
Training Epoch: 77 [16640/50048]	Loss: 0.0974
Training Epoch: 77 [16768/50048]	Loss: 0.1114
Training Epoch: 77 [16896/50048]	Loss: 0.1095
Training Epoch: 77 [17024/50048]	Loss: 0.1029
Training Epoch: 77 [17152/50048]	Loss: 0.1281
Training Epoch: 77 [17280/50048]	Loss: 0.1358
Training Epoch: 77 [17408/50048]	Loss: 0.1110
Training Epoch: 77 [17536/50048]	Loss: 0.1302
Training Epoch: 77 [17664/50048]	Loss: 0.1717
Training Epoch: 77 [17792/50048]	Loss: 0.1433
Training Epoch: 77 [17920/50048]	Loss: 0.1999
Training Epoch: 77 [18048/50048]	Loss: 0.1022
Training Epoch: 77 [18176/50048]	Loss: 0.0853
Training Epoch: 77 [18304/50048]	Loss: 0.0974
Training Epoch: 77 [18432/50048]	Loss: 0.1354
Training Epoch: 77 [18560/50048]	Loss: 0.0736
Training Epoch: 77 [18688/50048]	Loss: 0.1121
Training Epoch: 77 [18816/50048]	Loss: 0.0981
Training Epoch: 77 [18944/50048]	Loss: 0.0856
Training Epoch: 77 [19072/50048]	Loss: 0.0868
Training Epoch: 77 [19200/50048]	Loss: 0.1052
Training Epoch: 77 [19328/50048]	Loss: 0.1481
Training Epoch: 77 [19456/50048]	Loss: 0.0725
Training Epoch: 77 [19584/50048]	Loss: 0.1439
Training Epoch: 77 [19712/50048]	Loss: 0.0852
Training Epoch: 77 [19840/50048]	Loss: 0.1182
Training Epoch: 77 [19968/50048]	Loss: 0.0684
Training Epoch: 77 [20096/50048]	Loss: 0.1718
Training Epoch: 77 [20224/50048]	Loss: 0.1429
Training Epoch: 77 [20352/50048]	Loss: 0.0633
Training Epoch: 77 [20480/50048]	Loss: 0.1090
Training Epoch: 77 [20608/50048]	Loss: 0.1076
Training Epoch: 77 [20736/50048]	Loss: 0.0771
Training Epoch: 77 [20864/50048]	Loss: 0.0653
Training Epoch: 77 [20992/50048]	Loss: 0.1957
Training Epoch: 77 [21120/50048]	Loss: 0.1829
Training Epoch: 77 [21248/50048]	Loss: 0.1361
Training Epoch: 77 [21376/50048]	Loss: 0.1347
Training Epoch: 77 [21504/50048]	Loss: 0.1052
Training Epoch: 77 [21632/50048]	Loss: 0.0796
Training Epoch: 77 [21760/50048]	Loss: 0.0734
Training Epoch: 77 [21888/50048]	Loss: 0.0525
Training Epoch: 77 [22016/50048]	Loss: 0.1068
Training Epoch: 77 [22144/50048]	Loss: 0.1534
Training Epoch: 77 [22272/50048]	Loss: 0.1409
Training Epoch: 77 [22400/50048]	Loss: 0.1337
Training Epoch: 77 [22528/50048]	Loss: 0.0868
Training Epoch: 77 [22656/50048]	Loss: 0.1650
Training Epoch: 77 [22784/50048]	Loss: 0.1220
Training Epoch: 77 [22912/50048]	Loss: 0.1343
Training Epoch: 77 [23040/50048]	Loss: 0.1327
Training Epoch: 77 [23168/50048]	Loss: 0.0553
Training Epoch: 77 [23296/50048]	Loss: 0.1088
Training Epoch: 77 [23424/50048]	Loss: 0.0503
Training Epoch: 77 [23552/50048]	Loss: 0.1352
Training Epoch: 77 [23680/50048]	Loss: 0.1511
Training Epoch: 77 [23808/50048]	Loss: 0.1132
Training Epoch: 77 [23936/50048]	Loss: 0.2050
Training Epoch: 77 [24064/50048]	Loss: 0.0939
Training Epoch: 77 [24192/50048]	Loss: 0.1203
Training Epoch: 77 [24320/50048]	Loss: 0.1022
Training Epoch: 77 [24448/50048]	Loss: 0.1625
Training Epoch: 77 [24576/50048]	Loss: 0.1354
Training Epoch: 77 [24704/50048]	Loss: 0.0914
Training Epoch: 77 [24832/50048]	Loss: 0.1240
Training Epoch: 77 [24960/50048]	Loss: 0.1195
Training Epoch: 77 [25088/50048]	Loss: 0.1738
Training Epoch: 77 [25216/50048]	Loss: 0.1118
Training Epoch: 77 [25344/50048]	Loss: 0.0986
Training Epoch: 77 [25472/50048]	Loss: 0.1588
Training Epoch: 77 [25600/50048]	Loss: 0.0835
Training Epoch: 77 [25728/50048]	Loss: 0.0796
Training Epoch: 77 [25856/50048]	Loss: 0.0757
Training Epoch: 77 [25984/50048]	Loss: 0.0687
Training Epoch: 77 [26112/50048]	Loss: 0.0540
Training Epoch: 77 [26240/50048]	Loss: 0.0962
Training Epoch: 77 [26368/50048]	Loss: 0.1520
Training Epoch: 77 [26496/50048]	Loss: 0.1769
Training Epoch: 77 [26624/50048]	Loss: 0.0851
Training Epoch: 77 [26752/50048]	Loss: 0.1560
Training Epoch: 77 [26880/50048]	Loss: 0.1078
Training Epoch: 77 [27008/50048]	Loss: 0.0985
Training Epoch: 77 [27136/50048]	Loss: 0.1206
Training Epoch: 77 [27264/50048]	Loss: 0.0983
Training Epoch: 77 [27392/50048]	Loss: 0.1028
Training Epoch: 77 [27520/50048]	Loss: 0.2188
Training Epoch: 77 [27648/50048]	Loss: 0.1053
Training Epoch: 77 [27776/50048]	Loss: 0.0729
Training Epoch: 77 [27904/50048]	Loss: 0.1189
Training Epoch: 77 [28032/50048]	Loss: 0.1504
Training Epoch: 77 [28160/50048]	Loss: 0.0743
Training Epoch: 77 [28288/50048]	Loss: 0.0779
Training Epoch: 77 [28416/50048]	Loss: 0.0939
Training Epoch: 77 [28544/50048]	Loss: 0.1178
Training Epoch: 77 [28672/50048]	Loss: 0.2033
Training Epoch: 77 [28800/50048]	Loss: 0.0911
Training Epoch: 77 [28928/50048]	Loss: 0.1414
Training Epoch: 77 [29056/50048]	Loss: 0.1048
Training Epoch: 77 [29184/50048]	Loss: 0.2314
Training Epoch: 77 [29312/50048]	Loss: 0.1196
Training Epoch: 77 [29440/50048]	Loss: 0.1522
Training Epoch: 77 [29568/50048]	Loss: 0.1233
Training Epoch: 77 [29696/50048]	Loss: 0.1397
Training Epoch: 77 [29824/50048]	Loss: 0.1201
Training Epoch: 77 [29952/50048]	Loss: 0.1375
Training Epoch: 77 [30080/50048]	Loss: 0.1512
Training Epoch: 77 [30208/50048]	Loss: 0.1047
Training Epoch: 77 [30336/50048]	Loss: 0.1496
Training Epoch: 77 [30464/50048]	Loss: 0.0849
Training Epoch: 77 [30592/50048]	Loss: 0.1582
Training Epoch: 77 [30720/50048]	Loss: 0.0727
Training Epoch: 77 [30848/50048]	Loss: 0.1176
Training Epoch: 77 [30976/50048]	Loss: 0.1062
Training Epoch: 77 [31104/50048]	Loss: 0.1366
Training Epoch: 77 [31232/50048]	Loss: 0.2023
Training Epoch: 77 [31360/50048]	Loss: 0.2091
Training Epoch: 77 [31488/50048]	Loss: 0.0722
Training Epoch: 77 [31616/50048]	Loss: 0.0944
Training Epoch: 77 [31744/50048]	Loss: 0.0810
Training Epoch: 77 [31872/50048]	Loss: 0.1208
Training Epoch: 77 [32000/50048]	Loss: 0.0824
Training Epoch: 77 [32128/50048]	Loss: 0.1124
Training Epoch: 77 [32256/50048]	Loss: 0.0836
Training Epoch: 77 [32384/50048]	Loss: 0.0811
Training Epoch: 77 [32512/50048]	Loss: 0.0990
Training Epoch: 77 [32640/50048]	Loss: 0.1659
Training Epoch: 77 [32768/50048]	Loss: 0.1419
Training Epoch: 77 [32896/50048]	Loss: 0.1414
Training Epoch: 77 [33024/50048]	Loss: 0.1306
Training Epoch: 77 [33152/50048]	Loss: 0.1509
Training Epoch: 77 [33280/50048]	Loss: 0.1442
Training Epoch: 77 [33408/50048]	Loss: 0.1294
Training Epoch: 77 [33536/50048]	Loss: 0.1064
Training Epoch: 77 [33664/50048]	Loss: 0.0993
Training Epoch: 77 [33792/50048]	Loss: 0.1160
Training Epoch: 77 [33920/50048]	Loss: 0.1044
Training Epoch: 77 [34048/50048]	Loss: 0.2052
Training Epoch: 77 [34176/50048]	Loss: 0.1341
Training Epoch: 77 [34304/50048]	Loss: 0.2240
Training Epoch: 77 [34432/50048]	Loss: 0.0844
Training Epoch: 77 [34560/50048]	Loss: 0.1049
Training Epoch: 77 [34688/50048]	Loss: 0.1501
Training Epoch: 77 [34816/50048]	Loss: 0.1412
Training Epoch: 77 [34944/50048]	Loss: 0.0969
Training Epoch: 77 [35072/50048]	Loss: 0.1080
Training Epoch: 77 [35200/50048]	Loss: 0.1824
Training Epoch: 77 [35328/50048]	Loss: 0.1775
Training Epoch: 77 [35456/50048]	Loss: 0.1499
Training Epoch: 77 [35584/50048]	Loss: 0.1338
Training Epoch: 77 [35712/50048]	Loss: 0.0735
Training Epoch: 77 [35840/50048]	Loss: 0.1570
Training Epoch: 77 [35968/50048]	Loss: 0.1535
Training Epoch: 77 [36096/50048]	Loss: 0.1623
Training Epoch: 77 [36224/50048]	Loss: 0.1101
Training Epoch: 77 [36352/50048]	Loss: 0.0794
Training Epoch: 77 [36480/50048]	Loss: 0.0908
Training Epoch: 77 [36608/50048]	Loss: 0.1039
Training Epoch: 77 [36736/50048]	Loss: 0.1129
Training Epoch: 77 [36864/50048]	Loss: 0.1607
Training Epoch: 77 [36992/50048]	Loss: 0.0935
Training Epoch: 77 [37120/50048]	Loss: 0.1737
Training Epoch: 77 [37248/50048]	Loss: 0.1714
Training Epoch: 77 [37376/50048]	Loss: 0.1739
Training Epoch: 77 [37504/50048]	Loss: 0.1244
Training Epoch: 77 [37632/50048]	Loss: 0.1323
Training Epoch: 77 [37760/50048]	Loss: 0.0906
Training Epoch: 77 [37888/50048]	Loss: 0.0871
Training Epoch: 77 [38016/50048]	Loss: 0.1714
Training Epoch: 77 [38144/50048]	Loss: 0.1022
Training Epoch: 77 [38272/50048]	Loss: 0.1489
Training Epoch: 77 [38400/50048]	Loss: 0.1915
Training Epoch: 77 [38528/50048]	Loss: 0.1062
Training Epoch: 77 [38656/50048]	Loss: 0.1385
Training Epoch: 77 [38784/50048]	Loss: 0.1698
Training Epoch: 77 [38912/50048]	Loss: 0.1304
Training Epoch: 77 [39040/50048]	Loss: 0.1267
Training Epoch: 77 [39168/50048]	Loss: 0.0503
Training Epoch: 77 [39296/50048]	Loss: 0.1155
Training Epoch: 77 [39424/50048]	Loss: 0.0878
Training Epoch: 77 [39552/50048]	Loss: 0.1720
Training Epoch: 77 [39680/50048]	Loss: 0.1150
Training Epoch: 77 [39808/50048]	Loss: 0.1754
Training Epoch: 77 [39936/50048]	Loss: 0.1314
Training Epoch: 77 [40064/50048]	Loss: 0.0815
Training Epoch: 77 [40192/50048]	Loss: 0.1370
Training Epoch: 77 [40320/50048]	Loss: 0.1208
Training Epoch: 77 [40448/50048]	Loss: 0.0653
Training Epoch: 77 [40576/50048]	Loss: 0.1157
Training Epoch: 77 [40704/50048]	Loss: 0.0830
Training Epoch: 77 [40832/50048]	Loss: 0.1394
Training Epoch: 77 [40960/50048]	Loss: 0.1553
Training Epoch: 77 [41088/50048]	Loss: 0.1409
Training Epoch: 77 [41216/50048]	Loss: 0.1486
Training Epoch: 77 [41344/50048]	Loss: 0.0957
Training Epoch: 77 [41472/50048]	Loss: 0.1304
Training Epoch: 77 [41600/50048]	Loss: 0.1203
Training Epoch: 77 [41728/50048]	Loss: 0.1270
Training Epoch: 77 [41856/50048]	Loss: 0.1528
Training Epoch: 77 [41984/50048]	Loss: 0.1710
Training Epoch: 77 [42112/50048]	Loss: 0.1998
Training Epoch: 77 [42240/50048]	Loss: 0.0998
Training Epoch: 77 [42368/50048]	Loss: 0.0998
Training Epoch: 77 [42496/50048]	Loss: 0.1611
Training Epoch: 77 [42624/50048]	Loss: 0.1094
Training Epoch: 77 [42752/50048]	Loss: 0.2026
Training Epoch: 77 [42880/50048]	Loss: 0.1568
Training Epoch: 77 [43008/50048]	Loss: 0.1376
Training Epoch: 77 [43136/50048]	Loss: 0.0762
Training Epoch: 77 [43264/50048]	Loss: 0.0693
Training Epoch: 77 [43392/50048]	Loss: 0.1405
Training Epoch: 77 [43520/50048]	Loss: 0.1268
Training Epoch: 77 [43648/50048]	Loss: 0.1186
Training Epoch: 77 [43776/50048]	Loss: 0.1725
Training Epoch: 77 [43904/50048]	Loss: 0.1181
Training Epoch: 77 [44032/50048]	Loss: 0.1382
Training Epoch: 77 [44160/50048]	Loss: 0.0899
Training Epoch: 77 [44288/50048]	Loss: 0.1285
Training Epoch: 77 [44416/50048]	Loss: 0.1710
Training Epoch: 77 [44544/50048]	Loss: 0.0834
Training Epoch: 77 [44672/50048]	Loss: 0.0882
Training Epoch: 77 [44800/50048]	Loss: 0.0572
Training Epoch: 77 [44928/50048]	Loss: 0.1424
Training Epoch: 77 [45056/50048]	Loss: 0.1027
Training Epoch: 77 [45184/50048]	Loss: 0.0502
Training Epoch: 77 [45312/50048]	Loss: 0.1859
Training Epoch: 77 [45440/50048]	Loss: 0.0952
Training Epoch: 77 [45568/50048]	Loss: 0.1879
Training Epoch: 77 [45696/50048]	Loss: 0.1267
2022-12-06 05:34:37,698 [ZeusDataLoader(train)] train epoch 78 done: time=86.42 energy=10500.77
2022-12-06 05:34:37,699 [ZeusDataLoader(eval)] Epoch 78 begin.
Training Epoch: 77 [45824/50048]	Loss: 0.2267
Training Epoch: 77 [45952/50048]	Loss: 0.1158
Training Epoch: 77 [46080/50048]	Loss: 0.1310
Training Epoch: 77 [46208/50048]	Loss: 0.1215
Training Epoch: 77 [46336/50048]	Loss: 0.1561
Training Epoch: 77 [46464/50048]	Loss: 0.1002
Training Epoch: 77 [46592/50048]	Loss: 0.2086
Training Epoch: 77 [46720/50048]	Loss: 0.1391
Training Epoch: 77 [46848/50048]	Loss: 0.1096
Training Epoch: 77 [46976/50048]	Loss: 0.1844
Training Epoch: 77 [47104/50048]	Loss: 0.1102
Training Epoch: 77 [47232/50048]	Loss: 0.1244
Training Epoch: 77 [47360/50048]	Loss: 0.1353
Training Epoch: 77 [47488/50048]	Loss: 0.2256
Training Epoch: 77 [47616/50048]	Loss: 0.0467
Training Epoch: 77 [47744/50048]	Loss: 0.0996
Training Epoch: 77 [47872/50048]	Loss: 0.1166
Training Epoch: 77 [48000/50048]	Loss: 0.0887
Training Epoch: 77 [48128/50048]	Loss: 0.1964
Training Epoch: 77 [48256/50048]	Loss: 0.1247
Training Epoch: 77 [48384/50048]	Loss: 0.0742
Training Epoch: 77 [48512/50048]	Loss: 0.1773
Training Epoch: 77 [48640/50048]	Loss: 0.0952
Training Epoch: 77 [48768/50048]	Loss: 0.0973
Training Epoch: 77 [48896/50048]	Loss: 0.0823
Training Epoch: 77 [49024/50048]	Loss: 0.1234
Training Epoch: 77 [49152/50048]	Loss: 0.0991
Training Epoch: 77 [49280/50048]	Loss: 0.1131
Training Epoch: 77 [49408/50048]	Loss: 0.1454
Training Epoch: 77 [49536/50048]	Loss: 0.0906
Training Epoch: 77 [49664/50048]	Loss: 0.0719
Training Epoch: 77 [49792/50048]	Loss: 0.0840
Training Epoch: 77 [49920/50048]	Loss: 0.0719
Training Epoch: 77 [50048/50048]	Loss: 0.2198
2022-12-06 10:34:41.421 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:34:41,455 [ZeusDataLoader(eval)] eval epoch 78 done: time=3.75 energy=454.60
2022-12-06 05:34:41,455 [ZeusDataLoader(train)] Up to epoch 78: time=7037.48, energy=854306.12, cost=1042932.44
2022-12-06 05:34:41,455 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:34:41,455 [ZeusDataLoader(train)] Expected next epoch: time=7127.28, energy=865104.14, cost=1056188.83
2022-12-06 05:34:41,456 [ZeusDataLoader(train)] Epoch 79 begin.
Validation Epoch: 77, Average loss: 0.0180, Accuracy: 0.6395
2022-12-06 05:34:41,629 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:34:41,630 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:34:41.632 [ZeusMonitor] Monitor started.
2022-12-06 10:34:41.632 [ZeusMonitor] Running indefinitely. 2022-12-06 10:34:41.632 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:34:41.632 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e79+gpu0.power.log
Training Epoch: 78 [128/50048]	Loss: 0.1033
Training Epoch: 78 [256/50048]	Loss: 0.0865
Training Epoch: 78 [384/50048]	Loss: 0.0855
Training Epoch: 78 [512/50048]	Loss: 0.0600
Training Epoch: 78 [640/50048]	Loss: 0.0622
Training Epoch: 78 [768/50048]	Loss: 0.1019
Training Epoch: 78 [896/50048]	Loss: 0.0937
Training Epoch: 78 [1024/50048]	Loss: 0.0647
Training Epoch: 78 [1152/50048]	Loss: 0.1532
Training Epoch: 78 [1280/50048]	Loss: 0.1111
Training Epoch: 78 [1408/50048]	Loss: 0.2208
Training Epoch: 78 [1536/50048]	Loss: 0.0953
Training Epoch: 78 [1664/50048]	Loss: 0.1165
Training Epoch: 78 [1792/50048]	Loss: 0.0830
Training Epoch: 78 [1920/50048]	Loss: 0.0412
Training Epoch: 78 [2048/50048]	Loss: 0.0573
Training Epoch: 78 [2176/50048]	Loss: 0.1637
Training Epoch: 78 [2304/50048]	Loss: 0.0838
Training Epoch: 78 [2432/50048]	Loss: 0.1091
Training Epoch: 78 [2560/50048]	Loss: 0.0742
Training Epoch: 78 [2688/50048]	Loss: 0.1920
Training Epoch: 78 [2816/50048]	Loss: 0.1508
Training Epoch: 78 [2944/50048]	Loss: 0.1158
Training Epoch: 78 [3072/50048]	Loss: 0.1219
Training Epoch: 78 [3200/50048]	Loss: 0.0509
Training Epoch: 78 [3328/50048]	Loss: 0.0902
Training Epoch: 78 [3456/50048]	Loss: 0.0811
Training Epoch: 78 [3584/50048]	Loss: 0.1221
Training Epoch: 78 [3712/50048]	Loss: 0.0766
Training Epoch: 78 [3840/50048]	Loss: 0.0938
Training Epoch: 78 [3968/50048]	Loss: 0.0815
Training Epoch: 78 [4096/50048]	Loss: 0.0759
Training Epoch: 78 [4224/50048]	Loss: 0.1747
Training Epoch: 78 [4352/50048]	Loss: 0.1341
Training Epoch: 78 [4480/50048]	Loss: 0.0637
Training Epoch: 78 [4608/50048]	Loss: 0.0813
Training Epoch: 78 [4736/50048]	Loss: 0.0509
Training Epoch: 78 [4864/50048]	Loss: 0.1514
Training Epoch: 78 [4992/50048]	Loss: 0.0939
Training Epoch: 78 [5120/50048]	Loss: 0.1130
Training Epoch: 78 [5248/50048]	Loss: 0.0685
Training Epoch: 78 [5376/50048]	Loss: 0.0765
Training Epoch: 78 [5504/50048]	Loss: 0.1465
Training Epoch: 78 [5632/50048]	Loss: 0.1985
Training Epoch: 78 [5760/50048]	Loss: 0.0900
Training Epoch: 78 [5888/50048]	Loss: 0.0843
Training Epoch: 78 [6016/50048]	Loss: 0.1364
Training Epoch: 78 [6144/50048]	Loss: 0.1007
Training Epoch: 78 [6272/50048]	Loss: 0.0800
Training Epoch: 78 [6400/50048]	Loss: 0.1008
Training Epoch: 78 [6528/50048]	Loss: 0.1039
Training Epoch: 78 [6656/50048]	Loss: 0.1061
Training Epoch: 78 [6784/50048]	Loss: 0.1303
Training Epoch: 78 [6912/50048]	Loss: 0.1659
Training Epoch: 78 [7040/50048]	Loss: 0.0435
Training Epoch: 78 [7168/50048]	Loss: 0.0899
Training Epoch: 78 [7296/50048]	Loss: 0.1493
Training Epoch: 78 [7424/50048]	Loss: 0.1042
Training Epoch: 78 [7552/50048]	Loss: 0.1293
Training Epoch: 78 [7680/50048]	Loss: 0.0549
Training Epoch: 78 [7808/50048]	Loss: 0.1584
Training Epoch: 78 [7936/50048]	Loss: 0.0959
Training Epoch: 78 [8064/50048]	Loss: 0.1180
Training Epoch: 78 [8192/50048]	Loss: 0.0785
Training Epoch: 78 [8320/50048]	Loss: 0.1599
Training Epoch: 78 [8448/50048]	Loss: 0.1027
Training Epoch: 78 [8576/50048]	Loss: 0.1001
Training Epoch: 78 [8704/50048]	Loss: 0.1453
Training Epoch: 78 [8832/50048]	Loss: 0.1311
Training Epoch: 78 [8960/50048]	Loss: 0.1215
Training Epoch: 78 [9088/50048]	Loss: 0.1147
Training Epoch: 78 [9216/50048]	Loss: 0.1070
Training Epoch: 78 [9344/50048]	Loss: 0.1542
Training Epoch: 78 [9472/50048]	Loss: 0.0518
Training Epoch: 78 [9600/50048]	Loss: 0.1359
Training Epoch: 78 [9728/50048]	Loss: 0.0999
Training Epoch: 78 [9856/50048]	Loss: 0.0891
Training Epoch: 78 [9984/50048]	Loss: 0.1257
Training Epoch: 78 [10112/50048]	Loss: 0.0647
Training Epoch: 78 [10240/50048]	Loss: 0.0742
Training Epoch: 78 [10368/50048]	Loss: 0.0790
Training Epoch: 78 [10496/50048]	Loss: 0.1142
Training Epoch: 78 [10624/50048]	Loss: 0.0850
Training Epoch: 78 [10752/50048]	Loss: 0.1060
Training Epoch: 78 [10880/50048]	Loss: 0.0757
Training Epoch: 78 [11008/50048]	Loss: 0.0678
Training Epoch: 78 [11136/50048]	Loss: 0.0477
Training Epoch: 78 [11264/50048]	Loss: 0.0485
Training Epoch: 78 [11392/50048]	Loss: 0.0791
Training Epoch: 78 [11520/50048]	Loss: 0.1527
Training Epoch: 78 [11648/50048]	Loss: 0.1212
Training Epoch: 78 [11776/50048]	Loss: 0.0510
Training Epoch: 78 [11904/50048]	Loss: 0.1125
Training Epoch: 78 [12032/50048]	Loss: 0.0612
Training Epoch: 78 [12160/50048]	Loss: 0.1413
Training Epoch: 78 [12288/50048]	Loss: 0.0533
Training Epoch: 78 [12416/50048]	Loss: 0.1153
Training Epoch: 78 [12544/50048]	Loss: 0.1047
Training Epoch: 78 [12672/50048]	Loss: 0.2491
Training Epoch: 78 [12800/50048]	Loss: 0.0958
Training Epoch: 78 [12928/50048]	Loss: 0.1938
Training Epoch: 78 [13056/50048]	Loss: 0.0935
Training Epoch: 78 [13184/50048]	Loss: 0.2153
Training Epoch: 78 [13312/50048]	Loss: 0.1194
Training Epoch: 78 [13440/50048]	Loss: 0.1059
Training Epoch: 78 [13568/50048]	Loss: 0.1153
Training Epoch: 78 [13696/50048]	Loss: 0.1216
Training Epoch: 78 [13824/50048]	Loss: 0.1348
Training Epoch: 78 [13952/50048]	Loss: 0.1209
Training Epoch: 78 [14080/50048]	Loss: 0.0951
Training Epoch: 78 [14208/50048]	Loss: 0.1199
Training Epoch: 78 [14336/50048]	Loss: 0.0712
Training Epoch: 78 [14464/50048]	Loss: 0.1336
Training Epoch: 78 [14592/50048]	Loss: 0.0815
Training Epoch: 78 [14720/50048]	Loss: 0.1370
Training Epoch: 78 [14848/50048]	Loss: 0.0828
Training Epoch: 78 [14976/50048]	Loss: 0.0972
Training Epoch: 78 [15104/50048]	Loss: 0.1388
Training Epoch: 78 [15232/50048]	Loss: 0.1560
Training Epoch: 78 [15360/50048]	Loss: 0.1051
Training Epoch: 78 [15488/50048]	Loss: 0.0946
Training Epoch: 78 [15616/50048]	Loss: 0.1002
Training Epoch: 78 [15744/50048]	Loss: 0.1321
Training Epoch: 78 [15872/50048]	Loss: 0.1772
Training Epoch: 78 [16000/50048]	Loss: 0.1219
Training Epoch: 78 [16128/50048]	Loss: 0.1082
Training Epoch: 78 [16256/50048]	Loss: 0.0992
Training Epoch: 78 [16384/50048]	Loss: 0.0961
Training Epoch: 78 [16512/50048]	Loss: 0.0725
Training Epoch: 78 [16640/50048]	Loss: 0.1087
Training Epoch: 78 [16768/50048]	Loss: 0.0954
Training Epoch: 78 [16896/50048]	Loss: 0.0586
Training Epoch: 78 [17024/50048]	Loss: 0.0959
Training Epoch: 78 [17152/50048]	Loss: 0.1314
Training Epoch: 78 [17280/50048]	Loss: 0.0565
Training Epoch: 78 [17408/50048]	Loss: 0.0675
Training Epoch: 78 [17536/50048]	Loss: 0.1452
Training Epoch: 78 [17664/50048]	Loss: 0.1669
Training Epoch: 78 [17792/50048]	Loss: 0.0917
Training Epoch: 78 [17920/50048]	Loss: 0.1247
Training Epoch: 78 [18048/50048]	Loss: 0.1305
Training Epoch: 78 [18176/50048]	Loss: 0.0820
Training Epoch: 78 [18304/50048]	Loss: 0.0873
Training Epoch: 78 [18432/50048]	Loss: 0.0900
Training Epoch: 78 [18560/50048]	Loss: 0.1167
Training Epoch: 78 [18688/50048]	Loss: 0.1060
Training Epoch: 78 [18816/50048]	Loss: 0.1745
Training Epoch: 78 [18944/50048]	Loss: 0.1085
Training Epoch: 78 [19072/50048]	Loss: 0.1327
Training Epoch: 78 [19200/50048]	Loss: 0.1773
Training Epoch: 78 [19328/50048]	Loss: 0.1306
Training Epoch: 78 [19456/50048]	Loss: 0.1145
Training Epoch: 78 [19584/50048]	Loss: 0.0628
Training Epoch: 78 [19712/50048]	Loss: 0.1493
Training Epoch: 78 [19840/50048]	Loss: 0.0457
Training Epoch: 78 [19968/50048]	Loss: 0.1222
Training Epoch: 78 [20096/50048]	Loss: 0.1178
Training Epoch: 78 [20224/50048]	Loss: 0.1228
Training Epoch: 78 [20352/50048]	Loss: 0.1187
Training Epoch: 78 [20480/50048]	Loss: 0.1020
Training Epoch: 78 [20608/50048]	Loss: 0.2244
Training Epoch: 78 [20736/50048]	Loss: 0.1832
Training Epoch: 78 [20864/50048]	Loss: 0.1773
Training Epoch: 78 [20992/50048]	Loss: 0.0768
Training Epoch: 78 [21120/50048]	Loss: 0.0706
Training Epoch: 78 [21248/50048]	Loss: 0.1221
Training Epoch: 78 [21376/50048]	Loss: 0.1682
Training Epoch: 78 [21504/50048]	Loss: 0.1302
Training Epoch: 78 [21632/50048]	Loss: 0.1250
Training Epoch: 78 [21760/50048]	Loss: 0.0725
Training Epoch: 78 [21888/50048]	Loss: 0.1133
Training Epoch: 78 [22016/50048]	Loss: 0.1832
Training Epoch: 78 [22144/50048]	Loss: 0.1184
Training Epoch: 78 [22272/50048]	Loss: 0.1663
Training Epoch: 78 [22400/50048]	Loss: 0.1313
Training Epoch: 78 [22528/50048]	Loss: 0.0810
Training Epoch: 78 [22656/50048]	Loss: 0.1234
Training Epoch: 78 [22784/50048]	Loss: 0.1047
Training Epoch: 78 [22912/50048]	Loss: 0.1441
Training Epoch: 78 [23040/50048]	Loss: 0.0793
Training Epoch: 78 [23168/50048]	Loss: 0.0934
Training Epoch: 78 [23296/50048]	Loss: 0.1872
Training Epoch: 78 [23424/50048]	Loss: 0.0816
Training Epoch: 78 [23552/50048]	Loss: 0.1418
Training Epoch: 78 [23680/50048]	Loss: 0.0965
Training Epoch: 78 [23808/50048]	Loss: 0.1010
Training Epoch: 78 [23936/50048]	Loss: 0.0808
Training Epoch: 78 [24064/50048]	Loss: 0.1499
Training Epoch: 78 [24192/50048]	Loss: 0.0977
Training Epoch: 78 [24320/50048]	Loss: 0.0933
Training Epoch: 78 [24448/50048]	Loss: 0.0581
Training Epoch: 78 [24576/50048]	Loss: 0.0876
Training Epoch: 78 [24704/50048]	Loss: 0.0879
Training Epoch: 78 [24832/50048]	Loss: 0.1285
Training Epoch: 78 [24960/50048]	Loss: 0.1357
Training Epoch: 78 [25088/50048]	Loss: 0.0472
Training Epoch: 78 [25216/50048]	Loss: 0.1911
Training Epoch: 78 [25344/50048]	Loss: 0.1667
Training Epoch: 78 [25472/50048]	Loss: 0.0592
Training Epoch: 78 [25600/50048]	Loss: 0.1745
Training Epoch: 78 [25728/50048]	Loss: 0.0974
Training Epoch: 78 [25856/50048]	Loss: 0.0703
Training Epoch: 78 [25984/50048]	Loss: 0.0610
Training Epoch: 78 [26112/50048]	Loss: 0.1019
Training Epoch: 78 [26240/50048]	Loss: 0.1103
Training Epoch: 78 [26368/50048]	Loss: 0.0548
Training Epoch: 78 [26496/50048]	Loss: 0.1160
Training Epoch: 78 [26624/50048]	Loss: 0.1006
Training Epoch: 78 [26752/50048]	Loss: 0.1257
Training Epoch: 78 [26880/50048]	Loss: 0.0606
Training Epoch: 78 [27008/50048]	Loss: 0.1575
Training Epoch: 78 [27136/50048]	Loss: 0.1572
Training Epoch: 78 [27264/50048]	Loss: 0.1212
Training Epoch: 78 [27392/50048]	Loss: 0.1401
Training Epoch: 78 [27520/50048]	Loss: 0.1104
Training Epoch: 78 [27648/50048]	Loss: 0.1103
Training Epoch: 78 [27776/50048]	Loss: 0.1820
Training Epoch: 78 [27904/50048]	Loss: 0.1233
Training Epoch: 78 [28032/50048]	Loss: 0.0764
Training Epoch: 78 [28160/50048]	Loss: 0.1317
Training Epoch: 78 [28288/50048]	Loss: 0.1172
Training Epoch: 78 [28416/50048]	Loss: 0.1188
Training Epoch: 78 [28544/50048]	Loss: 0.1396
Training Epoch: 78 [28672/50048]	Loss: 0.0468
Training Epoch: 78 [28800/50048]	Loss: 0.2353
Training Epoch: 78 [28928/50048]	Loss: 0.1006
Training Epoch: 78 [29056/50048]	Loss: 0.2312
Training Epoch: 78 [29184/50048]	Loss: 0.1672
Training Epoch: 78 [29312/50048]	Loss: 0.1896
Training Epoch: 78 [29440/50048]	Loss: 0.1330
Training Epoch: 78 [29568/50048]	Loss: 0.1577
Training Epoch: 78 [29696/50048]	Loss: 0.1080
Training Epoch: 78 [29824/50048]	Loss: 0.1683
Training Epoch: 78 [29952/50048]	Loss: 0.2280
Training Epoch: 78 [30080/50048]	Loss: 0.1517
Training Epoch: 78 [30208/50048]	Loss: 0.1276
Training Epoch: 78 [30336/50048]	Loss: 0.2378
Training Epoch: 78 [30464/50048]	Loss: 0.1565
Training Epoch: 78 [30592/50048]	Loss: 0.1096
Training Epoch: 78 [30720/50048]	Loss: 0.1539
Training Epoch: 78 [30848/50048]	Loss: 0.1222
Training Epoch: 78 [30976/50048]	Loss: 0.0638
Training Epoch: 78 [31104/50048]	Loss: 0.0871
Training Epoch: 78 [31232/50048]	Loss: 0.1105
Training Epoch: 78 [31360/50048]	Loss: 0.1699
Training Epoch: 78 [31488/50048]	Loss: 0.1701
Training Epoch: 78 [31616/50048]	Loss: 0.0807
Training Epoch: 78 [31744/50048]	Loss: 0.1353
Training Epoch: 78 [31872/50048]	Loss: 0.0631
Training Epoch: 78 [32000/50048]	Loss: 0.1613
Training Epoch: 78 [32128/50048]	Loss: 0.1193
Training Epoch: 78 [32256/50048]	Loss: 0.1524
Training Epoch: 78 [32384/50048]	Loss: 0.1595
Training Epoch: 78 [32512/50048]	Loss: 0.1504
Training Epoch: 78 [32640/50048]	Loss: 0.0903
Training Epoch: 78 [32768/50048]	Loss: 0.1270
Training Epoch: 78 [32896/50048]	Loss: 0.1578
Training Epoch: 78 [33024/50048]	Loss: 0.0943
Training Epoch: 78 [33152/50048]	Loss: 0.1071
Training Epoch: 78 [33280/50048]	Loss: 0.1036
Training Epoch: 78 [33408/50048]	Loss: 0.1849
Training Epoch: 78 [33536/50048]	Loss: 0.1494
Training Epoch: 78 [33664/50048]	Loss: 0.0931
Training Epoch: 78 [33792/50048]	Loss: 0.1217
Training Epoch: 78 [33920/50048]	Loss: 0.1600
Training Epoch: 78 [34048/50048]	Loss: 0.1291
Training Epoch: 78 [34176/50048]	Loss: 0.0668
Training Epoch: 78 [34304/50048]	Loss: 0.1937
Training Epoch: 78 [34432/50048]	Loss: 0.1364
Training Epoch: 78 [34560/50048]	Loss: 0.2233
Training Epoch: 78 [34688/50048]	Loss: 0.1538
Training Epoch: 78 [34816/50048]	Loss: 0.1226
Training Epoch: 78 [34944/50048]	Loss: 0.1218
Training Epoch: 78 [35072/50048]	Loss: 0.1010
Training Epoch: 78 [35200/50048]	Loss: 0.1286
Training Epoch: 78 [35328/50048]	Loss: 0.0956
Training Epoch: 78 [35456/50048]	Loss: 0.1007
Training Epoch: 78 [35584/50048]	Loss: 0.0967
Training Epoch: 78 [35712/50048]	Loss: 0.1281
Training Epoch: 78 [35840/50048]	Loss: 0.1255
Training Epoch: 78 [35968/50048]	Loss: 0.1699
Training Epoch: 78 [36096/50048]	Loss: 0.0663
Training Epoch: 78 [36224/50048]	Loss: 0.1822
Training Epoch: 78 [36352/50048]	Loss: 0.1504
Training Epoch: 78 [36480/50048]	Loss: 0.0722
Training Epoch: 78 [36608/50048]	Loss: 0.1313
Training Epoch: 78 [36736/50048]	Loss: 0.1461
Training Epoch: 78 [36864/50048]	Loss: 0.1806
Training Epoch: 78 [36992/50048]	Loss: 0.1527
Training Epoch: 78 [37120/50048]	Loss: 0.1500
Training Epoch: 78 [37248/50048]	Loss: 0.0883
Training Epoch: 78 [37376/50048]	Loss: 0.1590
Training Epoch: 78 [37504/50048]	Loss: 0.1649
Training Epoch: 78 [37632/50048]	Loss: 0.1365
Training Epoch: 78 [37760/50048]	Loss: 0.0289
Training Epoch: 78 [37888/50048]	Loss: 0.1630
Training Epoch: 78 [38016/50048]	Loss: 0.0995
Training Epoch: 78 [38144/50048]	Loss: 0.0772
Training Epoch: 78 [38272/50048]	Loss: 0.1799
Training Epoch: 78 [38400/50048]	Loss: 0.1295
Training Epoch: 78 [38528/50048]	Loss: 0.1662
Training Epoch: 78 [38656/50048]	Loss: 0.1204
Training Epoch: 78 [38784/50048]	Loss: 0.1173
Training Epoch: 78 [38912/50048]	Loss: 0.1296
Training Epoch: 78 [39040/50048]	Loss: 0.1021
Training Epoch: 78 [39168/50048]	Loss: 0.1122
Training Epoch: 78 [39296/50048]	Loss: 0.0898
Training Epoch: 78 [39424/50048]	Loss: 0.0661
Training Epoch: 78 [39552/50048]	Loss: 0.0464
Training Epoch: 78 [39680/50048]	Loss: 0.1184
Training Epoch: 78 [39808/50048]	Loss: 0.1831
Training Epoch: 78 [39936/50048]	Loss: 0.1397
Training Epoch: 78 [40064/50048]	Loss: 0.0598
Training Epoch: 78 [40192/50048]	Loss: 0.0385
Training Epoch: 78 [40320/50048]	Loss: 0.1607
Training Epoch: 78 [40448/50048]	Loss: 0.1369
Training Epoch: 78 [40576/50048]	Loss: 0.1724
Training Epoch: 78 [40704/50048]	Loss: 0.1040
Training Epoch: 78 [40832/50048]	Loss: 0.1082
Training Epoch: 78 [40960/50048]	Loss: 0.0754
Training Epoch: 78 [41088/50048]	Loss: 0.1005
Training Epoch: 78 [41216/50048]	Loss: 0.1440
Training Epoch: 78 [41344/50048]	Loss: 0.1198
Training Epoch: 78 [41472/50048]	Loss: 0.0945
Training Epoch: 78 [41600/50048]	Loss: 0.1323
Training Epoch: 78 [41728/50048]	Loss: 0.1012
Training Epoch: 78 [41856/50048]	Loss: 0.0502
Training Epoch: 78 [41984/50048]	Loss: 0.0620
Training Epoch: 78 [42112/50048]	Loss: 0.1601
Training Epoch: 78 [42240/50048]	Loss: 0.1315
Training Epoch: 78 [42368/50048]	Loss: 0.1250
Training Epoch: 78 [42496/50048]	Loss: 0.0368
Training Epoch: 78 [42624/50048]	Loss: 0.1294
Training Epoch: 78 [42752/50048]	Loss: 0.1532
Training Epoch: 78 [42880/50048]	Loss: 0.1274
Training Epoch: 78 [43008/50048]	Loss: 0.0847
Training Epoch: 78 [43136/50048]	Loss: 0.1232
Training Epoch: 78 [43264/50048]	Loss: 0.1843
Training Epoch: 78 [43392/50048]	Loss: 0.1099
Training Epoch: 78 [43520/50048]	Loss: 0.1422
Training Epoch: 78 [43648/50048]	Loss: 0.0654
Training Epoch: 78 [43776/50048]	Loss: 0.0825
Training Epoch: 78 [43904/50048]	Loss: 0.0822
Training Epoch: 78 [44032/50048]	Loss: 0.0458
Training Epoch: 78 [44160/50048]	Loss: 0.2126
Training Epoch: 78 [44288/50048]	Loss: 0.1249
Training Epoch: 78 [44416/50048]	Loss: 0.2433
Training Epoch: 78 [44544/50048]	Loss: 0.0972
Training Epoch: 78 [44672/50048]	Loss: 0.1171
Training Epoch: 78 [44800/50048]	Loss: 0.1163
Training Epoch: 78 [44928/50048]	Loss: 0.1468
Training Epoch: 78 [45056/50048]	Loss: 0.1227
Training Epoch: 78 [45184/50048]	Loss: 0.1305
Training Epoch: 78 [45312/50048]	Loss: 0.0905
Training Epoch: 78 [45440/50048]	Loss: 0.0755
Training Epoch: 78 [45568/50048]	Loss: 0.1772
Training Epoch: 78 [45696/50048]	Loss: 0.1419
2022-12-06 05:36:07,839 [ZeusDataLoader(train)] train epoch 79 done: time=86.37 energy=10499.87
2022-12-06 05:36:07,840 [ZeusDataLoader(eval)] Epoch 79 begin.
Training Epoch: 78 [45824/50048]	Loss: 0.0881
Training Epoch: 78 [45952/50048]	Loss: 0.1693
Training Epoch: 78 [46080/50048]	Loss: 0.1505
Training Epoch: 78 [46208/50048]	Loss: 0.1325
Training Epoch: 78 [46336/50048]	Loss: 0.0787
Training Epoch: 78 [46464/50048]	Loss: 0.1552
Training Epoch: 78 [46592/50048]	Loss: 0.0913
Training Epoch: 78 [46720/50048]	Loss: 0.1361
Training Epoch: 78 [46848/50048]	Loss: 0.2196
Training Epoch: 78 [46976/50048]	Loss: 0.1392
Training Epoch: 78 [47104/50048]	Loss: 0.1170
Training Epoch: 78 [47232/50048]	Loss: 0.0586
Training Epoch: 78 [47360/50048]	Loss: 0.2089
Training Epoch: 78 [47488/50048]	Loss: 0.1181
Training Epoch: 78 [47616/50048]	Loss: 0.0960
Training Epoch: 78 [47744/50048]	Loss: 0.1267
Training Epoch: 78 [47872/50048]	Loss: 0.1046
Training Epoch: 78 [48000/50048]	Loss: 0.1337
Training Epoch: 78 [48128/50048]	Loss: 0.0647
Training Epoch: 78 [48256/50048]	Loss: 0.1282
Training Epoch: 78 [48384/50048]	Loss: 0.0664
Training Epoch: 78 [48512/50048]	Loss: 0.0844
Training Epoch: 78 [48640/50048]	Loss: 0.1262
Training Epoch: 78 [48768/50048]	Loss: 0.0566
Training Epoch: 78 [48896/50048]	Loss: 0.2145
Training Epoch: 78 [49024/50048]	Loss: 0.1131
Training Epoch: 78 [49152/50048]	Loss: 0.1118
Training Epoch: 78 [49280/50048]	Loss: 0.1285
Training Epoch: 78 [49408/50048]	Loss: 0.1637
Training Epoch: 78 [49536/50048]	Loss: 0.0782
Training Epoch: 78 [49664/50048]	Loss: 0.1462
Training Epoch: 78 [49792/50048]	Loss: 0.1938
Training Epoch: 78 [49920/50048]	Loss: 0.2028
Training Epoch: 78 [50048/50048]	Loss: 0.0473
2022-12-06 10:36:11.508 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:36:11,520 [ZeusDataLoader(eval)] eval epoch 79 done: time=3.67 energy=440.47
2022-12-06 05:36:11,521 [ZeusDataLoader(train)] Up to epoch 79: time=7127.52, energy=865246.46, cost=1056281.47
2022-12-06 05:36:11,521 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:36:11,521 [ZeusDataLoader(train)] Expected next epoch: time=7217.32, energy=876044.48, cost=1069537.85
2022-12-06 05:36:11,522 [ZeusDataLoader(train)] Epoch 80 begin.
Validation Epoch: 78, Average loss: 0.0180, Accuracy: 0.6411
2022-12-06 05:36:11,699 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:36:11,700 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:36:11.702 [ZeusMonitor] Monitor started.
2022-12-06 10:36:11.702 [ZeusMonitor] Running indefinitely. 2022-12-06 10:36:11.702 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:36:11.702 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e80+gpu0.power.log
Training Epoch: 79 [128/50048]	Loss: 0.1529
Training Epoch: 79 [256/50048]	Loss: 0.0656
Training Epoch: 79 [384/50048]	Loss: 0.1088
Training Epoch: 79 [512/50048]	Loss: 0.1988
Training Epoch: 79 [640/50048]	Loss: 0.1160
Training Epoch: 79 [768/50048]	Loss: 0.1335
Training Epoch: 79 [896/50048]	Loss: 0.1386
Training Epoch: 79 [1024/50048]	Loss: 0.0991
Training Epoch: 79 [1152/50048]	Loss: 0.0388
Training Epoch: 79 [1280/50048]	Loss: 0.1691
Training Epoch: 79 [1408/50048]	Loss: 0.2211
Training Epoch: 79 [1536/50048]	Loss: 0.0603
Training Epoch: 79 [1664/50048]	Loss: 0.0722
Training Epoch: 79 [1792/50048]	Loss: 0.1130
Training Epoch: 79 [1920/50048]	Loss: 0.0739
Training Epoch: 79 [2048/50048]	Loss: 0.0567
Training Epoch: 79 [2176/50048]	Loss: 0.1050
Training Epoch: 79 [2304/50048]	Loss: 0.1051
Training Epoch: 79 [2432/50048]	Loss: 0.1028
Training Epoch: 79 [2560/50048]	Loss: 0.0695
Training Epoch: 79 [2688/50048]	Loss: 0.1075
Training Epoch: 79 [2816/50048]	Loss: 0.1118
Training Epoch: 79 [2944/50048]	Loss: 0.0682
Training Epoch: 79 [3072/50048]	Loss: 0.0748
Training Epoch: 79 [3200/50048]	Loss: 0.1350
Training Epoch: 79 [3328/50048]	Loss: 0.1276
Training Epoch: 79 [3456/50048]	Loss: 0.1144
Training Epoch: 79 [3584/50048]	Loss: 0.1153
Training Epoch: 79 [3712/50048]	Loss: 0.1337
Training Epoch: 79 [3840/50048]	Loss: 0.0532
Training Epoch: 79 [3968/50048]	Loss: 0.1144
Training Epoch: 79 [4096/50048]	Loss: 0.0511
Training Epoch: 79 [4224/50048]	Loss: 0.1524
Training Epoch: 79 [4352/50048]	Loss: 0.1610
Training Epoch: 79 [4480/50048]	Loss: 0.0913
Training Epoch: 79 [4608/50048]	Loss: 0.1255
Training Epoch: 79 [4736/50048]	Loss: 0.1062
Training Epoch: 79 [4864/50048]	Loss: 0.0789
Training Epoch: 79 [4992/50048]	Loss: 0.0966
Training Epoch: 79 [5120/50048]	Loss: 0.0797
Training Epoch: 79 [5248/50048]	Loss: 0.0614
Training Epoch: 79 [5376/50048]	Loss: 0.0537
Training Epoch: 79 [5504/50048]	Loss: 0.1017
Training Epoch: 79 [5632/50048]	Loss: 0.0600
Training Epoch: 79 [5760/50048]	Loss: 0.1459
Training Epoch: 79 [5888/50048]	Loss: 0.1527
Training Epoch: 79 [6016/50048]	Loss: 0.1222
Training Epoch: 79 [6144/50048]	Loss: 0.0747
Training Epoch: 79 [6272/50048]	Loss: 0.1300
Training Epoch: 79 [6400/50048]	Loss: 0.1090
Training Epoch: 79 [6528/50048]	Loss: 0.0652
Training Epoch: 79 [6656/50048]	Loss: 0.0858
Training Epoch: 79 [6784/50048]	Loss: 0.1846
Training Epoch: 79 [6912/50048]	Loss: 0.0513
Training Epoch: 79 [7040/50048]	Loss: 0.0786
Training Epoch: 79 [7168/50048]	Loss: 0.0763
Training Epoch: 79 [7296/50048]	Loss: 0.0788
Training Epoch: 79 [7424/50048]	Loss: 0.1166
Training Epoch: 79 [7552/50048]	Loss: 0.1727
Training Epoch: 79 [7680/50048]	Loss: 0.1807
Training Epoch: 79 [7808/50048]	Loss: 0.1674
Training Epoch: 79 [7936/50048]	Loss: 0.0770
Training Epoch: 79 [8064/50048]	Loss: 0.0936
Training Epoch: 79 [8192/50048]	Loss: 0.1369
Training Epoch: 79 [8320/50048]	Loss: 0.0834
Training Epoch: 79 [8448/50048]	Loss: 0.1192
Training Epoch: 79 [8576/50048]	Loss: 0.1072
Training Epoch: 79 [8704/50048]	Loss: 0.0554
Training Epoch: 79 [8832/50048]	Loss: 0.0802
Training Epoch: 79 [8960/50048]	Loss: 0.0770
Training Epoch: 79 [9088/50048]	Loss: 0.1057
Training Epoch: 79 [9216/50048]	Loss: 0.0494
Training Epoch: 79 [9344/50048]	Loss: 0.1441
Training Epoch: 79 [9472/50048]	Loss: 0.0738
Training Epoch: 79 [9600/50048]	Loss: 0.1065
Training Epoch: 79 [9728/50048]	Loss: 0.0832
Training Epoch: 79 [9856/50048]	Loss: 0.1018
Training Epoch: 79 [9984/50048]	Loss: 0.0812
Training Epoch: 79 [10112/50048]	Loss: 0.1023
Training Epoch: 79 [10240/50048]	Loss: 0.1092
Training Epoch: 79 [10368/50048]	Loss: 0.0973
Training Epoch: 79 [10496/50048]	Loss: 0.0672
Training Epoch: 79 [10624/50048]	Loss: 0.1114
Training Epoch: 79 [10752/50048]	Loss: 0.0633
Training Epoch: 79 [10880/50048]	Loss: 0.0860
Training Epoch: 79 [11008/50048]	Loss: 0.1750
Training Epoch: 79 [11136/50048]	Loss: 0.0548
Training Epoch: 79 [11264/50048]	Loss: 0.1656
Training Epoch: 79 [11392/50048]	Loss: 0.1422
Training Epoch: 79 [11520/50048]	Loss: 0.0498
Training Epoch: 79 [11648/50048]	Loss: 0.0772
Training Epoch: 79 [11776/50048]	Loss: 0.1006
Training Epoch: 79 [11904/50048]	Loss: 0.1261
Training Epoch: 79 [12032/50048]	Loss: 0.1413
Training Epoch: 79 [12160/50048]	Loss: 0.0590
Training Epoch: 79 [12288/50048]	Loss: 0.1197
Training Epoch: 79 [12416/50048]	Loss: 0.0865
Training Epoch: 79 [12544/50048]	Loss: 0.0856
Training Epoch: 79 [12672/50048]	Loss: 0.1148
Training Epoch: 79 [12800/50048]	Loss: 0.0596
Training Epoch: 79 [12928/50048]	Loss: 0.1595
Training Epoch: 79 [13056/50048]	Loss: 0.0850
Training Epoch: 79 [13184/50048]	Loss: 0.0550
Training Epoch: 79 [13312/50048]	Loss: 0.1157
Training Epoch: 79 [13440/50048]	Loss: 0.1065
Training Epoch: 79 [13568/50048]	Loss: 0.1770
Training Epoch: 79 [13696/50048]	Loss: 0.1876
Training Epoch: 79 [13824/50048]	Loss: 0.1475
Training Epoch: 79 [13952/50048]	Loss: 0.0824
Training Epoch: 79 [14080/50048]	Loss: 0.0521
Training Epoch: 79 [14208/50048]	Loss: 0.1087
Training Epoch: 79 [14336/50048]	Loss: 0.0551
Training Epoch: 79 [14464/50048]	Loss: 0.0904
Training Epoch: 79 [14592/50048]	Loss: 0.1167
Training Epoch: 79 [14720/50048]	Loss: 0.1415
Training Epoch: 79 [14848/50048]	Loss: 0.1324
Training Epoch: 79 [14976/50048]	Loss: 0.0858
Training Epoch: 79 [15104/50048]	Loss: 0.0656
Training Epoch: 79 [15232/50048]	Loss: 0.0721
Training Epoch: 79 [15360/50048]	Loss: 0.0982
Training Epoch: 79 [15488/50048]	Loss: 0.1140
Training Epoch: 79 [15616/50048]	Loss: 0.1011
Training Epoch: 79 [15744/50048]	Loss: 0.1768
Training Epoch: 79 [15872/50048]	Loss: 0.2240
Training Epoch: 79 [16000/50048]	Loss: 0.0910
Training Epoch: 79 [16128/50048]	Loss: 0.0985
Training Epoch: 79 [16256/50048]	Loss: 0.0829
Training Epoch: 79 [16384/50048]	Loss: 0.1090
Training Epoch: 79 [16512/50048]	Loss: 0.0981
Training Epoch: 79 [16640/50048]	Loss: 0.1241
Training Epoch: 79 [16768/50048]	Loss: 0.0768
Training Epoch: 79 [16896/50048]	Loss: 0.1363
Training Epoch: 79 [17024/50048]	Loss: 0.0827
Training Epoch: 79 [17152/50048]	Loss: 0.0759
Training Epoch: 79 [17280/50048]	Loss: 0.1057
Training Epoch: 79 [17408/50048]	Loss: 0.0968
Training Epoch: 79 [17536/50048]	Loss: 0.1034
Training Epoch: 79 [17664/50048]	Loss: 0.0526
Training Epoch: 79 [17792/50048]	Loss: 0.0806
Training Epoch: 79 [17920/50048]	Loss: 0.0703
Training Epoch: 79 [18048/50048]	Loss: 0.1373
Training Epoch: 79 [18176/50048]	Loss: 0.1691
Training Epoch: 79 [18304/50048]	Loss: 0.1081
Training Epoch: 79 [18432/50048]	Loss: 0.0604
Training Epoch: 79 [18560/50048]	Loss: 0.0641
Training Epoch: 79 [18688/50048]	Loss: 0.0771
Training Epoch: 79 [18816/50048]	Loss: 0.0755
Training Epoch: 79 [18944/50048]	Loss: 0.1661
Training Epoch: 79 [19072/50048]	Loss: 0.0985
Training Epoch: 79 [19200/50048]	Loss: 0.0552
Training Epoch: 79 [19328/50048]	Loss: 0.1174
Training Epoch: 79 [19456/50048]	Loss: 0.1491
Training Epoch: 79 [19584/50048]	Loss: 0.1128
Training Epoch: 79 [19712/50048]	Loss: 0.0679
Training Epoch: 79 [19840/50048]	Loss: 0.0735
Training Epoch: 79 [19968/50048]	Loss: 0.0821
Training Epoch: 79 [20096/50048]	Loss: 0.0892
Training Epoch: 79 [20224/50048]	Loss: 0.1049
Training Epoch: 79 [20352/50048]	Loss: 0.1315
Training Epoch: 79 [20480/50048]	Loss: 0.1236
Training Epoch: 79 [20608/50048]	Loss: 0.1583
Training Epoch: 79 [20736/50048]	Loss: 0.1308
Training Epoch: 79 [20864/50048]	Loss: 0.1579
Training Epoch: 79 [20992/50048]	Loss: 0.1132
Training Epoch: 79 [21120/50048]	Loss: 0.0826
Training Epoch: 79 [21248/50048]	Loss: 0.0963
Training Epoch: 79 [21376/50048]	Loss: 0.1139
Training Epoch: 79 [21504/50048]	Loss: 0.0986
Training Epoch: 79 [21632/50048]	Loss: 0.0902
Training Epoch: 79 [21760/50048]	Loss: 0.1273
Training Epoch: 79 [21888/50048]	Loss: 0.0647
Training Epoch: 79 [22016/50048]	Loss: 0.1195
Training Epoch: 79 [22144/50048]	Loss: 0.0954
Training Epoch: 79 [22272/50048]	Loss: 0.1084
Training Epoch: 79 [22400/50048]	Loss: 0.1274
Training Epoch: 79 [22528/50048]	Loss: 0.1045
Training Epoch: 79 [22656/50048]	Loss: 0.1645
Training Epoch: 79 [22784/50048]	Loss: 0.1063
Training Epoch: 79 [22912/50048]	Loss: 0.1598
Training Epoch: 79 [23040/50048]	Loss: 0.0932
Training Epoch: 79 [23168/50048]	Loss: 0.1004
Training Epoch: 79 [23296/50048]	Loss: 0.1312
Training Epoch: 79 [23424/50048]	Loss: 0.1230
Training Epoch: 79 [23552/50048]	Loss: 0.1623
Training Epoch: 79 [23680/50048]	Loss: 0.1315
Training Epoch: 79 [23808/50048]	Loss: 0.1320
Training Epoch: 79 [23936/50048]	Loss: 0.0553
Training Epoch: 79 [24064/50048]	Loss: 0.0705
Training Epoch: 79 [24192/50048]	Loss: 0.1400
Training Epoch: 79 [24320/50048]	Loss: 0.1374
Training Epoch: 79 [24448/50048]	Loss: 0.1268
Training Epoch: 79 [24576/50048]	Loss: 0.1207
Training Epoch: 79 [24704/50048]	Loss: 0.1025
Training Epoch: 79 [24832/50048]	Loss: 0.1103
Training Epoch: 79 [24960/50048]	Loss: 0.0978
Training Epoch: 79 [25088/50048]	Loss: 0.0774
Training Epoch: 79 [25216/50048]	Loss: 0.0832
Training Epoch: 79 [25344/50048]	Loss: 0.1208
Training Epoch: 79 [25472/50048]	Loss: 0.0629
Training Epoch: 79 [25600/50048]	Loss: 0.1506
Training Epoch: 79 [25728/50048]	Loss: 0.0481
Training Epoch: 79 [25856/50048]	Loss: 0.1229
Training Epoch: 79 [25984/50048]	Loss: 0.0723
Training Epoch: 79 [26112/50048]	Loss: 0.1639
Training Epoch: 79 [26240/50048]	Loss: 0.0917
Training Epoch: 79 [26368/50048]	Loss: 0.1101
Training Epoch: 79 [26496/50048]	Loss: 0.1408
Training Epoch: 79 [26624/50048]	Loss: 0.1851
Training Epoch: 79 [26752/50048]	Loss: 0.0992
Training Epoch: 79 [26880/50048]	Loss: 0.1064
Training Epoch: 79 [27008/50048]	Loss: 0.1171
Training Epoch: 79 [27136/50048]	Loss: 0.0744
Training Epoch: 79 [27264/50048]	Loss: 0.0960
Training Epoch: 79 [27392/50048]	Loss: 0.1236
Training Epoch: 79 [27520/50048]	Loss: 0.1255
Training Epoch: 79 [27648/50048]	Loss: 0.0711
Training Epoch: 79 [27776/50048]	Loss: 0.1173
Training Epoch: 79 [27904/50048]	Loss: 0.0851
Training Epoch: 79 [28032/50048]	Loss: 0.0958
Training Epoch: 79 [28160/50048]	Loss: 0.1399
Training Epoch: 79 [28288/50048]	Loss: 0.1779
Training Epoch: 79 [28416/50048]	Loss: 0.0654
Training Epoch: 79 [28544/50048]	Loss: 0.0614
Training Epoch: 79 [28672/50048]	Loss: 0.1035
Training Epoch: 79 [28800/50048]	Loss: 0.1080
Training Epoch: 79 [28928/50048]	Loss: 0.1101
Training Epoch: 79 [29056/50048]	Loss: 0.1468
Training Epoch: 79 [29184/50048]	Loss: 0.0969
Training Epoch: 79 [29312/50048]	Loss: 0.0806
Training Epoch: 79 [29440/50048]	Loss: 0.0936
Training Epoch: 79 [29568/50048]	Loss: 0.0970
Training Epoch: 79 [29696/50048]	Loss: 0.1669
Training Epoch: 79 [29824/50048]	Loss: 0.1350
Training Epoch: 79 [29952/50048]	Loss: 0.1655
Training Epoch: 79 [30080/50048]	Loss: 0.0911
Training Epoch: 79 [30208/50048]	Loss: 0.1263
Training Epoch: 79 [30336/50048]	Loss: 0.1051
Training Epoch: 79 [30464/50048]	Loss: 0.1174
Training Epoch: 79 [30592/50048]	Loss: 0.1384
Training Epoch: 79 [30720/50048]	Loss: 0.1183
Training Epoch: 79 [30848/50048]	Loss: 0.2028
Training Epoch: 79 [30976/50048]	Loss: 0.1047
Training Epoch: 79 [31104/50048]	Loss: 0.1820
Training Epoch: 79 [31232/50048]	Loss: 0.1638
Training Epoch: 79 [31360/50048]	Loss: 0.1278
Training Epoch: 79 [31488/50048]	Loss: 0.1619
Training Epoch: 79 [31616/50048]	Loss: 0.1949
Training Epoch: 79 [31744/50048]	Loss: 0.0988
Training Epoch: 79 [31872/50048]	Loss: 0.1662
Training Epoch: 79 [32000/50048]	Loss: 0.0545
Training Epoch: 79 [32128/50048]	Loss: 0.1909
Training Epoch: 79 [32256/50048]	Loss: 0.1443
Training Epoch: 79 [32384/50048]	Loss: 0.1679
Training Epoch: 79 [32512/50048]	Loss: 0.1128
Training Epoch: 79 [32640/50048]	Loss: 0.1781
Training Epoch: 79 [32768/50048]	Loss: 0.0987
Training Epoch: 79 [32896/50048]	Loss: 0.1153
Training Epoch: 79 [33024/50048]	Loss: 0.1286
Training Epoch: 79 [33152/50048]	Loss: 0.0685
Training Epoch: 79 [33280/50048]	Loss: 0.0695
Training Epoch: 79 [33408/50048]	Loss: 0.1411
Training Epoch: 79 [33536/50048]	Loss: 0.1739
Training Epoch: 79 [33664/50048]	Loss: 0.0763
Training Epoch: 79 [33792/50048]	Loss: 0.1757
Training Epoch: 79 [33920/50048]	Loss: 0.1416
Training Epoch: 79 [34048/50048]	Loss: 0.1527
Training Epoch: 79 [34176/50048]	Loss: 0.0888
Training Epoch: 79 [34304/50048]	Loss: 0.0392
Training Epoch: 79 [34432/50048]	Loss: 0.0750
Training Epoch: 79 [34560/50048]	Loss: 0.1428
Training Epoch: 79 [34688/50048]	Loss: 0.1212
Training Epoch: 79 [34816/50048]	Loss: 0.1337
Training Epoch: 79 [34944/50048]	Loss: 0.1187
Training Epoch: 79 [35072/50048]	Loss: 0.0588
Training Epoch: 79 [35200/50048]	Loss: 0.1142
Training Epoch: 79 [35328/50048]	Loss: 0.0981
Training Epoch: 79 [35456/50048]	Loss: 0.1128
Training Epoch: 79 [35584/50048]	Loss: 0.0777
Training Epoch: 79 [35712/50048]	Loss: 0.0707
Training Epoch: 79 [35840/50048]	Loss: 0.1002
Training Epoch: 79 [35968/50048]	Loss: 0.1269
Training Epoch: 79 [36096/50048]	Loss: 0.1707
Training Epoch: 79 [36224/50048]	Loss: 0.1354
Training Epoch: 79 [36352/50048]	Loss: 0.0848
Training Epoch: 79 [36480/50048]	Loss: 0.0546
Training Epoch: 79 [36608/50048]	Loss: 0.1462
Training Epoch: 79 [36736/50048]	Loss: 0.1292
Training Epoch: 79 [36864/50048]	Loss: 0.1099
Training Epoch: 79 [36992/50048]	Loss: 0.0818
Training Epoch: 79 [37120/50048]	Loss: 0.0579
Training Epoch: 79 [37248/50048]	Loss: 0.1159
Training Epoch: 79 [37376/50048]	Loss: 0.1257
Training Epoch: 79 [37504/50048]	Loss: 0.1574
Training Epoch: 79 [37632/50048]	Loss: 0.1094
Training Epoch: 79 [37760/50048]	Loss: 0.0749
Training Epoch: 79 [37888/50048]	Loss: 0.1157
Training Epoch: 79 [38016/50048]	Loss: 0.1394
Training Epoch: 79 [38144/50048]	Loss: 0.1194
Training Epoch: 79 [38272/50048]	Loss: 0.1333
Training Epoch: 79 [38400/50048]	Loss: 0.0945
Training Epoch: 79 [38528/50048]	Loss: 0.0974
Training Epoch: 79 [38656/50048]	Loss: 0.0664
Training Epoch: 79 [38784/50048]	Loss: 0.1565
Training Epoch: 79 [38912/50048]	Loss: 0.1083
Training Epoch: 79 [39040/50048]	Loss: 0.1038
Training Epoch: 79 [39168/50048]	Loss: 0.0881
Training Epoch: 79 [39296/50048]	Loss: 0.0523
Training Epoch: 79 [39424/50048]	Loss: 0.1653
Training Epoch: 79 [39552/50048]	Loss: 0.1448
Training Epoch: 79 [39680/50048]	Loss: 0.1678
Training Epoch: 79 [39808/50048]	Loss: 0.1498
Training Epoch: 79 [39936/50048]	Loss: 0.1229
Training Epoch: 79 [40064/50048]	Loss: 0.1028
Training Epoch: 79 [40192/50048]	Loss: 0.1729
Training Epoch: 79 [40320/50048]	Loss: 0.2463
Training Epoch: 79 [40448/50048]	Loss: 0.1052
Training Epoch: 79 [40576/50048]	Loss: 0.2165
Training Epoch: 79 [40704/50048]	Loss: 0.2280
Training Epoch: 79 [40832/50048]	Loss: 0.1648
Training Epoch: 79 [40960/50048]	Loss: 0.0844
Training Epoch: 79 [41088/50048]	Loss: 0.1029
Training Epoch: 79 [41216/50048]	Loss: 0.0976
Training Epoch: 79 [41344/50048]	Loss: 0.0944
Training Epoch: 79 [41472/50048]	Loss: 0.1204
Training Epoch: 79 [41600/50048]	Loss: 0.1489
Training Epoch: 79 [41728/50048]	Loss: 0.1758
Training Epoch: 79 [41856/50048]	Loss: 0.1455
Training Epoch: 79 [41984/50048]	Loss: 0.0877
Training Epoch: 79 [42112/50048]	Loss: 0.1287
Training Epoch: 79 [42240/50048]	Loss: 0.0836
Training Epoch: 79 [42368/50048]	Loss: 0.1234
Training Epoch: 79 [42496/50048]	Loss: 0.1366
Training Epoch: 79 [42624/50048]	Loss: 0.1339
Training Epoch: 79 [42752/50048]	Loss: 0.1368
Training Epoch: 79 [42880/50048]	Loss: 0.1805
Training Epoch: 79 [43008/50048]	Loss: 0.1041
Training Epoch: 79 [43136/50048]	Loss: 0.0989
Training Epoch: 79 [43264/50048]	Loss: 0.0971
Training Epoch: 79 [43392/50048]	Loss: 0.0952
Training Epoch: 79 [43520/50048]	Loss: 0.1202
Training Epoch: 79 [43648/50048]	Loss: 0.1128
Training Epoch: 79 [43776/50048]	Loss: 0.0917
Training Epoch: 79 [43904/50048]	Loss: 0.0844
Training Epoch: 79 [44032/50048]	Loss: 0.0822
Training Epoch: 79 [44160/50048]	Loss: 0.0732
Training Epoch: 79 [44288/50048]	Loss: 0.1019
Training Epoch: 79 [44416/50048]	Loss: 0.1028
Training Epoch: 79 [44544/50048]	Loss: 0.0850
Training Epoch: 79 [44672/50048]	Loss: 0.2144
Training Epoch: 79 [44800/50048]	Loss: 0.0974
Training Epoch: 79 [44928/50048]	Loss: 0.0791
Training Epoch: 79 [45056/50048]	Loss: 0.0697
Training Epoch: 79 [45184/50048]	Loss: 0.1313
Training Epoch: 79 [45312/50048]	Loss: 0.1619
Training Epoch: 79 [45440/50048]	Loss: 0.1204
Training Epoch: 79 [45568/50048]	Loss: 0.1313
Training Epoch: 79 [45696/50048]	Loss: 0.1374
2022-12-06 05:37:37,965 [ZeusDataLoader(train)] train epoch 80 done: time=86.43 energy=10494.58
2022-12-06 05:37:37,967 [ZeusDataLoader(eval)] Epoch 80 begin.
Training Epoch: 79 [45824/50048]	Loss: 0.0915
Training Epoch: 79 [45952/50048]	Loss: 0.1540
Training Epoch: 79 [46080/50048]	Loss: 0.1026
Training Epoch: 79 [46208/50048]	Loss: 0.1521
Training Epoch: 79 [46336/50048]	Loss: 0.1453
Training Epoch: 79 [46464/50048]	Loss: 0.1066
Training Epoch: 79 [46592/50048]	Loss: 0.0830
Training Epoch: 79 [46720/50048]	Loss: 0.1085
Training Epoch: 79 [46848/50048]	Loss: 0.1331
Training Epoch: 79 [46976/50048]	Loss: 0.1544
Training Epoch: 79 [47104/50048]	Loss: 0.1196
Training Epoch: 79 [47232/50048]	Loss: 0.1823
Training Epoch: 79 [47360/50048]	Loss: 0.1329
Training Epoch: 79 [47488/50048]	Loss: 0.0883
Training Epoch: 79 [47616/50048]	Loss: 0.0611
Training Epoch: 79 [47744/50048]	Loss: 0.1975
Training Epoch: 79 [47872/50048]	Loss: 0.0927
Training Epoch: 79 [48000/50048]	Loss: 0.1571
Training Epoch: 79 [48128/50048]	Loss: 0.1276
Training Epoch: 79 [48256/50048]	Loss: 0.1041
Training Epoch: 79 [48384/50048]	Loss: 0.1471
Training Epoch: 79 [48512/50048]	Loss: 0.1072
Training Epoch: 79 [48640/50048]	Loss: 0.1622
Training Epoch: 79 [48768/50048]	Loss: 0.1205
Training Epoch: 79 [48896/50048]	Loss: 0.2241
Training Epoch: 79 [49024/50048]	Loss: 0.0745
Training Epoch: 79 [49152/50048]	Loss: 0.1287
Training Epoch: 79 [49280/50048]	Loss: 0.1584
Training Epoch: 79 [49408/50048]	Loss: 0.0864
Training Epoch: 79 [49536/50048]	Loss: 0.0896
Training Epoch: 79 [49664/50048]	Loss: 0.0903
Training Epoch: 79 [49792/50048]	Loss: 0.0910
Training Epoch: 79 [49920/50048]	Loss: 0.0953
Training Epoch: 79 [50048/50048]	Loss: 0.0934
2022-12-06 10:37:41.628 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:37:41,639 [ZeusDataLoader(eval)] eval epoch 80 done: time=3.66 energy=440.15
2022-12-06 05:37:41,639 [ZeusDataLoader(train)] Up to epoch 80: time=7217.62, energy=876181.20, cost=1069632.26
2022-12-06 05:37:41,639 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:37:41,639 [ZeusDataLoader(train)] Expected next epoch: time=7307.42, energy=886979.21, cost=1082888.65
2022-12-06 05:37:41,640 [ZeusDataLoader(train)] Epoch 81 begin.
Validation Epoch: 79, Average loss: 0.0176, Accuracy: 0.6460
2022-12-06 05:37:41,817 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:37:41,817 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:37:41.831 [ZeusMonitor] Monitor started.
2022-12-06 10:37:41.831 [ZeusMonitor] Running indefinitely. 2022-12-06 10:37:41.831 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:37:41.831 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e81+gpu0.power.log
Training Epoch: 80 [128/50048]	Loss: 0.0526
Training Epoch: 80 [256/50048]	Loss: 0.1321
Training Epoch: 80 [384/50048]	Loss: 0.0550
Training Epoch: 80 [512/50048]	Loss: 0.0994
Training Epoch: 80 [640/50048]	Loss: 0.0958
Training Epoch: 80 [768/50048]	Loss: 0.0627
Training Epoch: 80 [896/50048]	Loss: 0.0689
Training Epoch: 80 [1024/50048]	Loss: 0.1517
Training Epoch: 80 [1152/50048]	Loss: 0.1178
Training Epoch: 80 [1280/50048]	Loss: 0.1334
Training Epoch: 80 [1408/50048]	Loss: 0.0667
Training Epoch: 80 [1536/50048]	Loss: 0.1311
Training Epoch: 80 [1664/50048]	Loss: 0.1154
Training Epoch: 80 [1792/50048]	Loss: 0.0982
Training Epoch: 80 [1920/50048]	Loss: 0.1626
Training Epoch: 80 [2048/50048]	Loss: 0.0805
Training Epoch: 80 [2176/50048]	Loss: 0.0763
Training Epoch: 80 [2304/50048]	Loss: 0.0842
Training Epoch: 80 [2432/50048]	Loss: 0.1601
Training Epoch: 80 [2560/50048]	Loss: 0.1191
Training Epoch: 80 [2688/50048]	Loss: 0.1066
Training Epoch: 80 [2816/50048]	Loss: 0.0478
Training Epoch: 80 [2944/50048]	Loss: 0.0807
Training Epoch: 80 [3072/50048]	Loss: 0.1179
Training Epoch: 80 [3200/50048]	Loss: 0.0979
Training Epoch: 80 [3328/50048]	Loss: 0.1385
Training Epoch: 80 [3456/50048]	Loss: 0.1053
Training Epoch: 80 [3584/50048]	Loss: 0.0861
Training Epoch: 80 [3712/50048]	Loss: 0.0973
Training Epoch: 80 [3840/50048]	Loss: 0.0988
Training Epoch: 80 [3968/50048]	Loss: 0.1438
Training Epoch: 80 [4096/50048]	Loss: 0.0492
Training Epoch: 80 [4224/50048]	Loss: 0.0599
Training Epoch: 80 [4352/50048]	Loss: 0.1043
Training Epoch: 80 [4480/50048]	Loss: 0.0616
Training Epoch: 80 [4608/50048]	Loss: 0.1238
Training Epoch: 80 [4736/50048]	Loss: 0.1696
Training Epoch: 80 [4864/50048]	Loss: 0.0443
Training Epoch: 80 [4992/50048]	Loss: 0.0868
Training Epoch: 80 [5120/50048]	Loss: 0.1048
Training Epoch: 80 [5248/50048]	Loss: 0.0829
Training Epoch: 80 [5376/50048]	Loss: 0.0505
Training Epoch: 80 [5504/50048]	Loss: 0.0780
Training Epoch: 80 [5632/50048]	Loss: 0.0939
Training Epoch: 80 [5760/50048]	Loss: 0.0485
Training Epoch: 80 [5888/50048]	Loss: 0.1379
Training Epoch: 80 [6016/50048]	Loss: 0.1077
Training Epoch: 80 [6144/50048]	Loss: 0.0432
Training Epoch: 80 [6272/50048]	Loss: 0.0844
Training Epoch: 80 [6400/50048]	Loss: 0.0998
Training Epoch: 80 [6528/50048]	Loss: 0.1120
Training Epoch: 80 [6656/50048]	Loss: 0.0657
Training Epoch: 80 [6784/50048]	Loss: 0.1024
Training Epoch: 80 [6912/50048]	Loss: 0.0991
Training Epoch: 80 [7040/50048]	Loss: 0.1224
Training Epoch: 80 [7168/50048]	Loss: 0.0352
Training Epoch: 80 [7296/50048]	Loss: 0.0845
Training Epoch: 80 [7424/50048]	Loss: 0.0591
Training Epoch: 80 [7552/50048]	Loss: 0.0893
Training Epoch: 80 [7680/50048]	Loss: 0.0691
Training Epoch: 80 [7808/50048]	Loss: 0.0779
Training Epoch: 80 [7936/50048]	Loss: 0.1109
Training Epoch: 80 [8064/50048]	Loss: 0.0696
Training Epoch: 80 [8192/50048]	Loss: 0.0283
Training Epoch: 80 [8320/50048]	Loss: 0.1192
Training Epoch: 80 [8448/50048]	Loss: 0.1791
Training Epoch: 80 [8576/50048]	Loss: 0.1150
Training Epoch: 80 [8704/50048]	Loss: 0.0704
Training Epoch: 80 [8832/50048]	Loss: 0.1110
Training Epoch: 80 [8960/50048]	Loss: 0.1498
Training Epoch: 80 [9088/50048]	Loss: 0.0688
Training Epoch: 80 [9216/50048]	Loss: 0.0665
Training Epoch: 80 [9344/50048]	Loss: 0.1534
Training Epoch: 80 [9472/50048]	Loss: 0.0664
Training Epoch: 80 [9600/50048]	Loss: 0.1354
Training Epoch: 80 [9728/50048]	Loss: 0.1177
Training Epoch: 80 [9856/50048]	Loss: 0.0914
Training Epoch: 80 [9984/50048]	Loss: 0.1459
Training Epoch: 80 [10112/50048]	Loss: 0.0589
Training Epoch: 80 [10240/50048]	Loss: 0.0992
Training Epoch: 80 [10368/50048]	Loss: 0.1641
Training Epoch: 80 [10496/50048]	Loss: 0.1507
Training Epoch: 80 [10624/50048]	Loss: 0.0745
Training Epoch: 80 [10752/50048]	Loss: 0.0876
Training Epoch: 80 [10880/50048]	Loss: 0.1383
Training Epoch: 80 [11008/50048]	Loss: 0.1225
Training Epoch: 80 [11136/50048]	Loss: 0.0905
Training Epoch: 80 [11264/50048]	Loss: 0.0884
Training Epoch: 80 [11392/50048]	Loss: 0.0938
Training Epoch: 80 [11520/50048]	Loss: 0.1777
Training Epoch: 80 [11648/50048]	Loss: 0.1461
Training Epoch: 80 [11776/50048]	Loss: 0.1079
Training Epoch: 80 [11904/50048]	Loss: 0.0786
Training Epoch: 80 [12032/50048]	Loss: 0.1304
Training Epoch: 80 [12160/50048]	Loss: 0.0834
Training Epoch: 80 [12288/50048]	Loss: 0.0946
Training Epoch: 80 [12416/50048]	Loss: 0.0695
Training Epoch: 80 [12544/50048]	Loss: 0.1092
Training Epoch: 80 [12672/50048]	Loss: 0.0967
Training Epoch: 80 [12800/50048]	Loss: 0.0506
Training Epoch: 80 [12928/50048]	Loss: 0.0705
Training Epoch: 80 [13056/50048]	Loss: 0.0529
Training Epoch: 80 [13184/50048]	Loss: 0.0644
Training Epoch: 80 [13312/50048]	Loss: 0.1921
Training Epoch: 80 [13440/50048]	Loss: 0.1219
Training Epoch: 80 [13568/50048]	Loss: 0.1514
Training Epoch: 80 [13696/50048]	Loss: 0.0827
Training Epoch: 80 [13824/50048]	Loss: 0.2127
Training Epoch: 80 [13952/50048]	Loss: 0.0858
Training Epoch: 80 [14080/50048]	Loss: 0.1337
Training Epoch: 80 [14208/50048]	Loss: 0.0480
Training Epoch: 80 [14336/50048]	Loss: 0.1535
Training Epoch: 80 [14464/50048]	Loss: 0.1366
Training Epoch: 80 [14592/50048]	Loss: 0.0981
Training Epoch: 80 [14720/50048]	Loss: 0.1451
Training Epoch: 80 [14848/50048]	Loss: 0.0850
Training Epoch: 80 [14976/50048]	Loss: 0.1991
Training Epoch: 80 [15104/50048]	Loss: 0.1426
Training Epoch: 80 [15232/50048]	Loss: 0.0902
Training Epoch: 80 [15360/50048]	Loss: 0.0688
Training Epoch: 80 [15488/50048]	Loss: 0.0860
Training Epoch: 80 [15616/50048]	Loss: 0.0720
Training Epoch: 80 [15744/50048]	Loss: 0.0894
Training Epoch: 80 [15872/50048]	Loss: 0.0919
Training Epoch: 80 [16000/50048]	Loss: 0.1274
Training Epoch: 80 [16128/50048]	Loss: 0.1430
Training Epoch: 80 [16256/50048]	Loss: 0.0695
Training Epoch: 80 [16384/50048]	Loss: 0.1045
Training Epoch: 80 [16512/50048]	Loss: 0.1237
Training Epoch: 80 [16640/50048]	Loss: 0.0963
Training Epoch: 80 [16768/50048]	Loss: 0.0946
Training Epoch: 80 [16896/50048]	Loss: 0.1439
Training Epoch: 80 [17024/50048]	Loss: 0.0623
Training Epoch: 80 [17152/50048]	Loss: 0.0892
Training Epoch: 80 [17280/50048]	Loss: 0.1513
Training Epoch: 80 [17408/50048]	Loss: 0.0972
Training Epoch: 80 [17536/50048]	Loss: 0.1242
Training Epoch: 80 [17664/50048]	Loss: 0.1832
Training Epoch: 80 [17792/50048]	Loss: 0.0890
Training Epoch: 80 [17920/50048]	Loss: 0.0970
Training Epoch: 80 [18048/50048]	Loss: 0.0720
Training Epoch: 80 [18176/50048]	Loss: 0.1290
Training Epoch: 80 [18304/50048]	Loss: 0.1468
Training Epoch: 80 [18432/50048]	Loss: 0.1194
Training Epoch: 80 [18560/50048]	Loss: 0.0979
Training Epoch: 80 [18688/50048]	Loss: 0.0979
Training Epoch: 80 [18816/50048]	Loss: 0.1926
Training Epoch: 80 [18944/50048]	Loss: 0.1604
Training Epoch: 80 [19072/50048]	Loss: 0.0617
Training Epoch: 80 [19200/50048]	Loss: 0.1032
Training Epoch: 80 [19328/50048]	Loss: 0.1463
Training Epoch: 80 [19456/50048]	Loss: 0.1656
Training Epoch: 80 [19584/50048]	Loss: 0.1276
Training Epoch: 80 [19712/50048]	Loss: 0.0807
Training Epoch: 80 [19840/50048]	Loss: 0.0383
Training Epoch: 80 [19968/50048]	Loss: 0.1117
Training Epoch: 80 [20096/50048]	Loss: 0.1544
Training Epoch: 80 [20224/50048]	Loss: 0.1622
Training Epoch: 80 [20352/50048]	Loss: 0.0979
Training Epoch: 80 [20480/50048]	Loss: 0.1147
Training Epoch: 80 [20608/50048]	Loss: 0.1174
Training Epoch: 80 [20736/50048]	Loss: 0.0969
Training Epoch: 80 [20864/50048]	Loss: 0.0899
Training Epoch: 80 [20992/50048]	Loss: 0.0709
Training Epoch: 80 [21120/50048]	Loss: 0.0974
Training Epoch: 80 [21248/50048]	Loss: 0.1155
Training Epoch: 80 [21376/50048]	Loss: 0.1254
Training Epoch: 80 [21504/50048]	Loss: 0.0628
Training Epoch: 80 [21632/50048]	Loss: 0.0715
Training Epoch: 80 [21760/50048]	Loss: 0.0821
Training Epoch: 80 [21888/50048]	Loss: 0.0789
Training Epoch: 80 [22016/50048]	Loss: 0.0901
Training Epoch: 80 [22144/50048]	Loss: 0.0441
Training Epoch: 80 [22272/50048]	Loss: 0.1073
Training Epoch: 80 [22400/50048]	Loss: 0.1635
Training Epoch: 80 [22528/50048]	Loss: 0.1202
Training Epoch: 80 [22656/50048]	Loss: 0.0643
Training Epoch: 80 [22784/50048]	Loss: 0.0667
Training Epoch: 80 [22912/50048]	Loss: 0.0657
Training Epoch: 80 [23040/50048]	Loss: 0.1104
Training Epoch: 80 [23168/50048]	Loss: 0.0405
Training Epoch: 80 [23296/50048]	Loss: 0.1851
Training Epoch: 80 [23424/50048]	Loss: 0.1083
Training Epoch: 80 [23552/50048]	Loss: 0.0662
Training Epoch: 80 [23680/50048]	Loss: 0.0744
Training Epoch: 80 [23808/50048]	Loss: 0.1774
Training Epoch: 80 [23936/50048]	Loss: 0.1428
Training Epoch: 80 [24064/50048]	Loss: 0.1883
Training Epoch: 80 [24192/50048]	Loss: 0.0680
Training Epoch: 80 [24320/50048]	Loss: 0.0732
Training Epoch: 80 [24448/50048]	Loss: 0.0839
Training Epoch: 80 [24576/50048]	Loss: 0.0811
Training Epoch: 80 [24704/50048]	Loss: 0.0974
Training Epoch: 80 [24832/50048]	Loss: 0.1327
Training Epoch: 80 [24960/50048]	Loss: 0.1437
Training Epoch: 80 [25088/50048]	Loss: 0.1193
Training Epoch: 80 [25216/50048]	Loss: 0.1283
Training Epoch: 80 [25344/50048]	Loss: 0.1222
Training Epoch: 80 [25472/50048]	Loss: 0.0763
Training Epoch: 80 [25600/50048]	Loss: 0.1302
Training Epoch: 80 [25728/50048]	Loss: 0.0494
Training Epoch: 80 [25856/50048]	Loss: 0.0777
Training Epoch: 80 [25984/50048]	Loss: 0.0794
Training Epoch: 80 [26112/50048]	Loss: 0.1114
Training Epoch: 80 [26240/50048]	Loss: 0.0479
Training Epoch: 80 [26368/50048]	Loss: 0.1072
Training Epoch: 80 [26496/50048]	Loss: 0.1372
Training Epoch: 80 [26624/50048]	Loss: 0.0739
Training Epoch: 80 [26752/50048]	Loss: 0.1140
Training Epoch: 80 [26880/50048]	Loss: 0.0928
Training Epoch: 80 [27008/50048]	Loss: 0.0974
Training Epoch: 80 [27136/50048]	Loss: 0.1210
Training Epoch: 80 [27264/50048]	Loss: 0.1590
Training Epoch: 80 [27392/50048]	Loss: 0.1180
Training Epoch: 80 [27520/50048]	Loss: 0.2754
Training Epoch: 80 [27648/50048]	Loss: 0.1368
Training Epoch: 80 [27776/50048]	Loss: 0.0872
Training Epoch: 80 [27904/50048]	Loss: 0.1025
Training Epoch: 80 [28032/50048]	Loss: 0.1577
Training Epoch: 80 [28160/50048]	Loss: 0.1254
Training Epoch: 80 [28288/50048]	Loss: 0.1297
Training Epoch: 80 [28416/50048]	Loss: 0.0554
Training Epoch: 80 [28544/50048]	Loss: 0.1359
Training Epoch: 80 [28672/50048]	Loss: 0.1366
Training Epoch: 80 [28800/50048]	Loss: 0.1332
Training Epoch: 80 [28928/50048]	Loss: 0.1276
Training Epoch: 80 [29056/50048]	Loss: 0.1163
Training Epoch: 80 [29184/50048]	Loss: 0.1346
Training Epoch: 80 [29312/50048]	Loss: 0.1568
Training Epoch: 80 [29440/50048]	Loss: 0.2034
Training Epoch: 80 [29568/50048]	Loss: 0.1543
Training Epoch: 80 [29696/50048]	Loss: 0.0799
Training Epoch: 80 [29824/50048]	Loss: 0.1897
Training Epoch: 80 [29952/50048]	Loss: 0.1050
Training Epoch: 80 [30080/50048]	Loss: 0.1208
Training Epoch: 80 [30208/50048]	Loss: 0.0645
Training Epoch: 80 [30336/50048]	Loss: 0.1179
Training Epoch: 80 [30464/50048]	Loss: 0.0741
Training Epoch: 80 [30592/50048]	Loss: 0.0676
Training Epoch: 80 [30720/50048]	Loss: 0.1134
Training Epoch: 80 [30848/50048]	Loss: 0.2429
Training Epoch: 80 [30976/50048]	Loss: 0.0664
Training Epoch: 80 [31104/50048]	Loss: 0.0786
Training Epoch: 80 [31232/50048]	Loss: 0.1659
Training Epoch: 80 [31360/50048]	Loss: 0.1186
Training Epoch: 80 [31488/50048]	Loss: 0.2352
Training Epoch: 80 [31616/50048]	Loss: 0.1080
Training Epoch: 80 [31744/50048]	Loss: 0.1209
Training Epoch: 80 [31872/50048]	Loss: 0.0651
Training Epoch: 80 [32000/50048]	Loss: 0.0729
Training Epoch: 80 [32128/50048]	Loss: 0.1316
Training Epoch: 80 [32256/50048]	Loss: 0.1703
Training Epoch: 80 [32384/50048]	Loss: 0.0884
Training Epoch: 80 [32512/50048]	Loss: 0.0802
Training Epoch: 80 [32640/50048]	Loss: 0.0910
Training Epoch: 80 [32768/50048]	Loss: 0.0559
Training Epoch: 80 [32896/50048]	Loss: 0.1009
Training Epoch: 80 [33024/50048]	Loss: 0.1051
Training Epoch: 80 [33152/50048]	Loss: 0.1413
Training Epoch: 80 [33280/50048]	Loss: 0.0774
Training Epoch: 80 [33408/50048]	Loss: 0.1782
Training Epoch: 80 [33536/50048]	Loss: 0.0907
Training Epoch: 80 [33664/50048]	Loss: 0.0830
Training Epoch: 80 [33792/50048]	Loss: 0.1071
Training Epoch: 80 [33920/50048]	Loss: 0.1094
Training Epoch: 80 [34048/50048]	Loss: 0.1950
Training Epoch: 80 [34176/50048]	Loss: 0.0654
Training Epoch: 80 [34304/50048]	Loss: 0.0875
Training Epoch: 80 [34432/50048]	Loss: 0.0926
Training Epoch: 80 [34560/50048]	Loss: 0.1572
Training Epoch: 80 [34688/50048]	Loss: 0.1267
Training Epoch: 80 [34816/50048]	Loss: 0.1151
Training Epoch: 80 [34944/50048]	Loss: 0.0920
Training Epoch: 80 [35072/50048]	Loss: 0.1107
Training Epoch: 80 [35200/50048]	Loss: 0.0971
Training Epoch: 80 [35328/50048]	Loss: 0.1267
Training Epoch: 80 [35456/50048]	Loss: 0.1093
Training Epoch: 80 [35584/50048]	Loss: 0.0838
Training Epoch: 80 [35712/50048]	Loss: 0.1410
Training Epoch: 80 [35840/50048]	Loss: 0.0864
Training Epoch: 80 [35968/50048]	Loss: 0.1476
Training Epoch: 80 [36096/50048]	Loss: 0.1419
Training Epoch: 80 [36224/50048]	Loss: 0.1462
Training Epoch: 80 [36352/50048]	Loss: 0.0784
Training Epoch: 80 [36480/50048]	Loss: 0.1299
Training Epoch: 80 [36608/50048]	Loss: 0.1622
Training Epoch: 80 [36736/50048]	Loss: 0.2394
Training Epoch: 80 [36864/50048]	Loss: 0.1587
Training Epoch: 80 [36992/50048]	Loss: 0.0702
Training Epoch: 80 [37120/50048]	Loss: 0.1001
Training Epoch: 80 [37248/50048]	Loss: 0.1780
Training Epoch: 80 [37376/50048]	Loss: 0.0906
Training Epoch: 80 [37504/50048]	Loss: 0.1326
Training Epoch: 80 [37632/50048]	Loss: 0.0623
Training Epoch: 80 [37760/50048]	Loss: 0.0822
Training Epoch: 80 [37888/50048]	Loss: 0.1202
Training Epoch: 80 [38016/50048]	Loss: 0.0554
Training Epoch: 80 [38144/50048]	Loss: 0.1268
Training Epoch: 80 [38272/50048]	Loss: 0.1484
Training Epoch: 80 [38400/50048]	Loss: 0.1491
Training Epoch: 80 [38528/50048]	Loss: 0.0676
Training Epoch: 80 [38656/50048]	Loss: 0.1238
Training Epoch: 80 [38784/50048]	Loss: 0.1170
Training Epoch: 80 [38912/50048]	Loss: 0.0855
Training Epoch: 80 [39040/50048]	Loss: 0.1344
Training Epoch: 80 [39168/50048]	Loss: 0.1286
Training Epoch: 80 [39296/50048]	Loss: 0.1048
Training Epoch: 80 [39424/50048]	Loss: 0.1268
Training Epoch: 80 [39552/50048]	Loss: 0.0822
Training Epoch: 80 [39680/50048]	Loss: 0.1044
Training Epoch: 80 [39808/50048]	Loss: 0.1230
Training Epoch: 80 [39936/50048]	Loss: 0.1072
Training Epoch: 80 [40064/50048]	Loss: 0.2126
Training Epoch: 80 [40192/50048]	Loss: 0.1569
Training Epoch: 80 [40320/50048]	Loss: 0.1060
Training Epoch: 80 [40448/50048]	Loss: 0.1185
Training Epoch: 80 [40576/50048]	Loss: 0.0990
Training Epoch: 80 [40704/50048]	Loss: 0.1429
Training Epoch: 80 [40832/50048]	Loss: 0.1185
Training Epoch: 80 [40960/50048]	Loss: 0.0539
Training Epoch: 80 [41088/50048]	Loss: 0.2216
Training Epoch: 80 [41216/50048]	Loss: 0.0918
Training Epoch: 80 [41344/50048]	Loss: 0.0740
Training Epoch: 80 [41472/50048]	Loss: 0.0706
Training Epoch: 80 [41600/50048]	Loss: 0.1304
Training Epoch: 80 [41728/50048]	Loss: 0.1414
Training Epoch: 80 [41856/50048]	Loss: 0.0887
Training Epoch: 80 [41984/50048]	Loss: 0.0806
Training Epoch: 80 [42112/50048]	Loss: 0.1231
Training Epoch: 80 [42240/50048]	Loss: 0.1427
Training Epoch: 80 [42368/50048]	Loss: 0.1171
Training Epoch: 80 [42496/50048]	Loss: 0.1364
Training Epoch: 80 [42624/50048]	Loss: 0.1426
Training Epoch: 80 [42752/50048]	Loss: 0.1663
Training Epoch: 80 [42880/50048]	Loss: 0.0696
Training Epoch: 80 [43008/50048]	Loss: 0.1177
Training Epoch: 80 [43136/50048]	Loss: 0.1206
Training Epoch: 80 [43264/50048]	Loss: 0.1496
Training Epoch: 80 [43392/50048]	Loss: 0.1552
Training Epoch: 80 [43520/50048]	Loss: 0.1488
Training Epoch: 80 [43648/50048]	Loss: 0.1155
Training Epoch: 80 [43776/50048]	Loss: 0.0972
Training Epoch: 80 [43904/50048]	Loss: 0.1334
Training Epoch: 80 [44032/50048]	Loss: 0.1387
Training Epoch: 80 [44160/50048]	Loss: 0.1888
Training Epoch: 80 [44288/50048]	Loss: 0.1610
Training Epoch: 80 [44416/50048]	Loss: 0.0748
Training Epoch: 80 [44544/50048]	Loss: 0.1569
Training Epoch: 80 [44672/50048]	Loss: 0.0858
Training Epoch: 80 [44800/50048]	Loss: 0.1067
Training Epoch: 80 [44928/50048]	Loss: 0.1290
Training Epoch: 80 [45056/50048]	Loss: 0.0692
Training Epoch: 80 [45184/50048]	Loss: 0.0765
Training Epoch: 80 [45312/50048]	Loss: 0.0676
Training Epoch: 80 [45440/50048]	Loss: 0.1108
Training Epoch: 80 [45568/50048]	Loss: 0.0734
Training Epoch: 80 [45696/50048]	Loss: 0.1054
2022-12-06 05:39:08,410 [ZeusDataLoader(train)] train epoch 81 done: time=86.76 energy=10517.65
2022-12-06 05:39:08,411 [ZeusDataLoader(eval)] Epoch 81 begin.
Training Epoch: 80 [45824/50048]	Loss: 0.1146
Training Epoch: 80 [45952/50048]	Loss: 0.0928
Training Epoch: 80 [46080/50048]	Loss: 0.1248
Training Epoch: 80 [46208/50048]	Loss: 0.2945
Training Epoch: 80 [46336/50048]	Loss: 0.1043
Training Epoch: 80 [46464/50048]	Loss: 0.0850
Training Epoch: 80 [46592/50048]	Loss: 0.0523
Training Epoch: 80 [46720/50048]	Loss: 0.1745
Training Epoch: 80 [46848/50048]	Loss: 0.0784
Training Epoch: 80 [46976/50048]	Loss: 0.1541
Training Epoch: 80 [47104/50048]	Loss: 0.1033
Training Epoch: 80 [47232/50048]	Loss: 0.1120
Training Epoch: 80 [47360/50048]	Loss: 0.1174
Training Epoch: 80 [47488/50048]	Loss: 0.0813
Training Epoch: 80 [47616/50048]	Loss: 0.0815
Training Epoch: 80 [47744/50048]	Loss: 0.1175
Training Epoch: 80 [47872/50048]	Loss: 0.1027
Training Epoch: 80 [48000/50048]	Loss: 0.1079
Training Epoch: 80 [48128/50048]	Loss: 0.1035
Training Epoch: 80 [48256/50048]	Loss: 0.1569
Training Epoch: 80 [48384/50048]	Loss: 0.2126
Training Epoch: 80 [48512/50048]	Loss: 0.1047
Training Epoch: 80 [48640/50048]	Loss: 0.1184
Training Epoch: 80 [48768/50048]	Loss: 0.1061
Training Epoch: 80 [48896/50048]	Loss: 0.1285
Training Epoch: 80 [49024/50048]	Loss: 0.0860
Training Epoch: 80 [49152/50048]	Loss: 0.0446
Training Epoch: 80 [49280/50048]	Loss: 0.0942
Training Epoch: 80 [49408/50048]	Loss: 0.1406
Training Epoch: 80 [49536/50048]	Loss: 0.1718
Training Epoch: 80 [49664/50048]	Loss: 0.0935
Training Epoch: 80 [49792/50048]	Loss: 0.1284
Training Epoch: 80 [49920/50048]	Loss: 0.0941
Training Epoch: 80 [50048/50048]	Loss: 0.1777
2022-12-06 10:39:12.075 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:39:12,093 [ZeusDataLoader(eval)] eval epoch 81 done: time=3.67 energy=440.64
2022-12-06 05:39:12,093 [ZeusDataLoader(train)] Up to epoch 81: time=7308.05, energy=887139.48, cost=1083024.22
2022-12-06 05:39:12,093 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:39:12,093 [ZeusDataLoader(train)] Expected next epoch: time=7397.85, energy=897937.50, cost=1096280.61
2022-12-06 05:39:12,094 [ZeusDataLoader(train)] Epoch 82 begin.
Validation Epoch: 80, Average loss: 0.0183, Accuracy: 0.6393
2022-12-06 05:39:12,276 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:39:12,276 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:39:12.278 [ZeusMonitor] Monitor started.
2022-12-06 10:39:12.278 [ZeusMonitor] Running indefinitely. 2022-12-06 10:39:12.278 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:39:12.278 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e82+gpu0.power.log
Training Epoch: 81 [128/50048]	Loss: 0.1539
Training Epoch: 81 [256/50048]	Loss: 0.0727
Training Epoch: 81 [384/50048]	Loss: 0.1116
Training Epoch: 81 [512/50048]	Loss: 0.1075
Training Epoch: 81 [640/50048]	Loss: 0.0770
Training Epoch: 81 [768/50048]	Loss: 0.1135
Training Epoch: 81 [896/50048]	Loss: 0.1622
Training Epoch: 81 [1024/50048]	Loss: 0.1097
Training Epoch: 81 [1152/50048]	Loss: 0.0578
Training Epoch: 81 [1280/50048]	Loss: 0.0671
Training Epoch: 81 [1408/50048]	Loss: 0.1006
Training Epoch: 81 [1536/50048]	Loss: 0.0497
Training Epoch: 81 [1664/50048]	Loss: 0.1248
Training Epoch: 81 [1792/50048]	Loss: 0.0889
Training Epoch: 81 [1920/50048]	Loss: 0.0633
Training Epoch: 81 [2048/50048]	Loss: 0.0638
Training Epoch: 81 [2176/50048]	Loss: 0.0960
Training Epoch: 81 [2304/50048]	Loss: 0.0507
Training Epoch: 81 [2432/50048]	Loss: 0.1130
Training Epoch: 81 [2560/50048]	Loss: 0.0794
Training Epoch: 81 [2688/50048]	Loss: 0.0671
Training Epoch: 81 [2816/50048]	Loss: 0.1061
Training Epoch: 81 [2944/50048]	Loss: 0.1065
Training Epoch: 81 [3072/50048]	Loss: 0.1321
Training Epoch: 81 [3200/50048]	Loss: 0.0728
Training Epoch: 81 [3328/50048]	Loss: 0.1171
Training Epoch: 81 [3456/50048]	Loss: 0.0459
Training Epoch: 81 [3584/50048]	Loss: 0.0837
Training Epoch: 81 [3712/50048]	Loss: 0.1862
Training Epoch: 81 [3840/50048]	Loss: 0.0869
Training Epoch: 81 [3968/50048]	Loss: 0.1936
Training Epoch: 81 [4096/50048]	Loss: 0.1114
Training Epoch: 81 [4224/50048]	Loss: 0.0927
Training Epoch: 81 [4352/50048]	Loss: 0.0789
Training Epoch: 81 [4480/50048]	Loss: 0.1313
Training Epoch: 81 [4608/50048]	Loss: 0.1493
Training Epoch: 81 [4736/50048]	Loss: 0.0579
Training Epoch: 81 [4864/50048]	Loss: 0.1075
Training Epoch: 81 [4992/50048]	Loss: 0.1344
Training Epoch: 81 [5120/50048]	Loss: 0.1091
Training Epoch: 81 [5248/50048]	Loss: 0.0654
Training Epoch: 81 [5376/50048]	Loss: 0.1327
Training Epoch: 81 [5504/50048]	Loss: 0.0646
Training Epoch: 81 [5632/50048]	Loss: 0.1395
Training Epoch: 81 [5760/50048]	Loss: 0.2288
Training Epoch: 81 [5888/50048]	Loss: 0.0928
Training Epoch: 81 [6016/50048]	Loss: 0.1082
Training Epoch: 81 [6144/50048]	Loss: 0.0641
Training Epoch: 81 [6272/50048]	Loss: 0.0805
Training Epoch: 81 [6400/50048]	Loss: 0.1049
Training Epoch: 81 [6528/50048]	Loss: 0.0653
Training Epoch: 81 [6656/50048]	Loss: 0.1384
Training Epoch: 81 [6784/50048]	Loss: 0.0975
Training Epoch: 81 [6912/50048]	Loss: 0.0784
Training Epoch: 81 [7040/50048]	Loss: 0.1809
Training Epoch: 81 [7168/50048]	Loss: 0.1434
Training Epoch: 81 [7296/50048]	Loss: 0.0857
Training Epoch: 81 [7424/50048]	Loss: 0.1577
Training Epoch: 81 [7552/50048]	Loss: 0.0777
Training Epoch: 81 [7680/50048]	Loss: 0.1223
Training Epoch: 81 [7808/50048]	Loss: 0.0895
Training Epoch: 81 [7936/50048]	Loss: 0.1064
Training Epoch: 81 [8064/50048]	Loss: 0.1637
Training Epoch: 81 [8192/50048]	Loss: 0.1086
Training Epoch: 81 [8320/50048]	Loss: 0.0932
Training Epoch: 81 [8448/50048]	Loss: 0.1228
Training Epoch: 81 [8576/50048]	Loss: 0.1481
Training Epoch: 81 [8704/50048]	Loss: 0.1293
Training Epoch: 81 [8832/50048]	Loss: 0.1712
Training Epoch: 81 [8960/50048]	Loss: 0.0873
Training Epoch: 81 [9088/50048]	Loss: 0.0774
Training Epoch: 81 [9216/50048]	Loss: 0.0904
Training Epoch: 81 [9344/50048]	Loss: 0.0726
Training Epoch: 81 [9472/50048]	Loss: 0.1254
Training Epoch: 81 [9600/50048]	Loss: 0.0875
Training Epoch: 81 [9728/50048]	Loss: 0.1263
Training Epoch: 81 [9856/50048]	Loss: 0.1580
Training Epoch: 81 [9984/50048]	Loss: 0.0800
Training Epoch: 81 [10112/50048]	Loss: 0.1131
Training Epoch: 81 [10240/50048]	Loss: 0.0559
Training Epoch: 81 [10368/50048]	Loss: 0.1007
Training Epoch: 81 [10496/50048]	Loss: 0.0928
Training Epoch: 81 [10624/50048]	Loss: 0.1196
Training Epoch: 81 [10752/50048]	Loss: 0.1553
Training Epoch: 81 [10880/50048]	Loss: 0.0463
Training Epoch: 81 [11008/50048]	Loss: 0.0738
Training Epoch: 81 [11136/50048]	Loss: 0.0713
Training Epoch: 81 [11264/50048]	Loss: 0.1218
Training Epoch: 81 [11392/50048]	Loss: 0.1095
Training Epoch: 81 [11520/50048]	Loss: 0.0653
Training Epoch: 81 [11648/50048]	Loss: 0.0885
Training Epoch: 81 [11776/50048]	Loss: 0.0560
Training Epoch: 81 [11904/50048]	Loss: 0.1377
Training Epoch: 81 [12032/50048]	Loss: 0.1048
Training Epoch: 81 [12160/50048]	Loss: 0.1350
Training Epoch: 81 [12288/50048]	Loss: 0.0704
Training Epoch: 81 [12416/50048]	Loss: 0.0994
Training Epoch: 81 [12544/50048]	Loss: 0.1532
Training Epoch: 81 [12672/50048]	Loss: 0.1201
Training Epoch: 81 [12800/50048]	Loss: 0.0495
Training Epoch: 81 [12928/50048]	Loss: 0.1617
Training Epoch: 81 [13056/50048]	Loss: 0.0714
Training Epoch: 81 [13184/50048]	Loss: 0.1325
Training Epoch: 81 [13312/50048]	Loss: 0.1018
Training Epoch: 81 [13440/50048]	Loss: 0.2165
Training Epoch: 81 [13568/50048]	Loss: 0.1258
Training Epoch: 81 [13696/50048]	Loss: 0.1010
Training Epoch: 81 [13824/50048]	Loss: 0.2333
Training Epoch: 81 [13952/50048]	Loss: 0.1793
Training Epoch: 81 [14080/50048]	Loss: 0.1647
Training Epoch: 81 [14208/50048]	Loss: 0.0870
Training Epoch: 81 [14336/50048]	Loss: 0.0636
Training Epoch: 81 [14464/50048]	Loss: 0.0779
Training Epoch: 81 [14592/50048]	Loss: 0.1977
Training Epoch: 81 [14720/50048]	Loss: 0.1598
Training Epoch: 81 [14848/50048]	Loss: 0.1370
Training Epoch: 81 [14976/50048]	Loss: 0.0749
Training Epoch: 81 [15104/50048]	Loss: 0.0902
Training Epoch: 81 [15232/50048]	Loss: 0.1326
Training Epoch: 81 [15360/50048]	Loss: 0.0457
Training Epoch: 81 [15488/50048]	Loss: 0.1057
Training Epoch: 81 [15616/50048]	Loss: 0.1269
Training Epoch: 81 [15744/50048]	Loss: 0.0954
Training Epoch: 81 [15872/50048]	Loss: 0.1265
Training Epoch: 81 [16000/50048]	Loss: 0.1052
Training Epoch: 81 [16128/50048]	Loss: 0.1195
Training Epoch: 81 [16256/50048]	Loss: 0.1124
Training Epoch: 81 [16384/50048]	Loss: 0.1026
Training Epoch: 81 [16512/50048]	Loss: 0.0894
Training Epoch: 81 [16640/50048]	Loss: 0.0465
Training Epoch: 81 [16768/50048]	Loss: 0.1079
Training Epoch: 81 [16896/50048]	Loss: 0.1383
Training Epoch: 81 [17024/50048]	Loss: 0.1206
Training Epoch: 81 [17152/50048]	Loss: 0.0668
Training Epoch: 81 [17280/50048]	Loss: 0.0740
Training Epoch: 81 [17408/50048]	Loss: 0.1076
Training Epoch: 81 [17536/50048]	Loss: 0.0922
Training Epoch: 81 [17664/50048]	Loss: 0.1162
Training Epoch: 81 [17792/50048]	Loss: 0.1177
Training Epoch: 81 [17920/50048]	Loss: 0.1380
Training Epoch: 81 [18048/50048]	Loss: 0.1261
Training Epoch: 81 [18176/50048]	Loss: 0.0690
Training Epoch: 81 [18304/50048]	Loss: 0.0714
Training Epoch: 81 [18432/50048]	Loss: 0.1271
Training Epoch: 81 [18560/50048]	Loss: 0.0874
Training Epoch: 81 [18688/50048]	Loss: 0.1424
Training Epoch: 81 [18816/50048]	Loss: 0.0453
Training Epoch: 81 [18944/50048]	Loss: 0.0874
Training Epoch: 81 [19072/50048]	Loss: 0.1246
Training Epoch: 81 [19200/50048]	Loss: 0.0938
Training Epoch: 81 [19328/50048]	Loss: 0.1014
Training Epoch: 81 [19456/50048]	Loss: 0.1025
Training Epoch: 81 [19584/50048]	Loss: 0.0683
Training Epoch: 81 [19712/50048]	Loss: 0.1047
Training Epoch: 81 [19840/50048]	Loss: 0.0585
Training Epoch: 81 [19968/50048]	Loss: 0.0579
Training Epoch: 81 [20096/50048]	Loss: 0.1430
Training Epoch: 81 [20224/50048]	Loss: 0.0787
Training Epoch: 81 [20352/50048]	Loss: 0.1002
Training Epoch: 81 [20480/50048]	Loss: 0.0984
Training Epoch: 81 [20608/50048]	Loss: 0.0647
Training Epoch: 81 [20736/50048]	Loss: 0.1338
Training Epoch: 81 [20864/50048]	Loss: 0.0921
Training Epoch: 81 [20992/50048]	Loss: 0.0955
Training Epoch: 81 [21120/50048]	Loss: 0.0637
Training Epoch: 81 [21248/50048]	Loss: 0.0922
Training Epoch: 81 [21376/50048]	Loss: 0.0998
Training Epoch: 81 [21504/50048]	Loss: 0.0998
Training Epoch: 81 [21632/50048]	Loss: 0.1623
Training Epoch: 81 [21760/50048]	Loss: 0.1469
Training Epoch: 81 [21888/50048]	Loss: 0.1520
Training Epoch: 81 [22016/50048]	Loss: 0.0695
Training Epoch: 81 [22144/50048]	Loss: 0.0474
Training Epoch: 81 [22272/50048]	Loss: 0.0611
Training Epoch: 81 [22400/50048]	Loss: 0.0778
Training Epoch: 81 [22528/50048]	Loss: 0.1794
Training Epoch: 81 [22656/50048]	Loss: 0.0790
Training Epoch: 81 [22784/50048]	Loss: 0.1052
Training Epoch: 81 [22912/50048]	Loss: 0.1741
Training Epoch: 81 [23040/50048]	Loss: 0.1093
Training Epoch: 81 [23168/50048]	Loss: 0.0966
Training Epoch: 81 [23296/50048]	Loss: 0.1620
Training Epoch: 81 [23424/50048]	Loss: 0.0774
Training Epoch: 81 [23552/50048]	Loss: 0.1327
Training Epoch: 81 [23680/50048]	Loss: 0.0821
Training Epoch: 81 [23808/50048]	Loss: 0.0810
Training Epoch: 81 [23936/50048]	Loss: 0.0642
Training Epoch: 81 [24064/50048]	Loss: 0.1272
Training Epoch: 81 [24192/50048]	Loss: 0.1165
Training Epoch: 81 [24320/50048]	Loss: 0.0779
Training Epoch: 81 [24448/50048]	Loss: 0.1634
Training Epoch: 81 [24576/50048]	Loss: 0.0593
Training Epoch: 81 [24704/50048]	Loss: 0.1138
Training Epoch: 81 [24832/50048]	Loss: 0.0811
Training Epoch: 81 [24960/50048]	Loss: 0.0856
Training Epoch: 81 [25088/50048]	Loss: 0.1156
Training Epoch: 81 [25216/50048]	Loss: 0.1195
Training Epoch: 81 [25344/50048]	Loss: 0.0787
Training Epoch: 81 [25472/50048]	Loss: 0.1475
Training Epoch: 81 [25600/50048]	Loss: 0.0591
Training Epoch: 81 [25728/50048]	Loss: 0.1431
Training Epoch: 81 [25856/50048]	Loss: 0.0419
Training Epoch: 81 [25984/50048]	Loss: 0.1111
Training Epoch: 81 [26112/50048]	Loss: 0.1342
Training Epoch: 81 [26240/50048]	Loss: 0.1376
Training Epoch: 81 [26368/50048]	Loss: 0.0909
Training Epoch: 81 [26496/50048]	Loss: 0.1379
Training Epoch: 81 [26624/50048]	Loss: 0.1731
Training Epoch: 81 [26752/50048]	Loss: 0.0488
Training Epoch: 81 [26880/50048]	Loss: 0.2002
Training Epoch: 81 [27008/50048]	Loss: 0.1447
Training Epoch: 81 [27136/50048]	Loss: 0.0885
Training Epoch: 81 [27264/50048]	Loss: 0.0842
Training Epoch: 81 [27392/50048]	Loss: 0.1535
Training Epoch: 81 [27520/50048]	Loss: 0.0937
Training Epoch: 81 [27648/50048]	Loss: 0.1479
Training Epoch: 81 [27776/50048]	Loss: 0.1320
Training Epoch: 81 [27904/50048]	Loss: 0.1111
Training Epoch: 81 [28032/50048]	Loss: 0.1132
Training Epoch: 81 [28160/50048]	Loss: 0.1195
Training Epoch: 81 [28288/50048]	Loss: 0.1952
Training Epoch: 81 [28416/50048]	Loss: 0.1290
Training Epoch: 81 [28544/50048]	Loss: 0.1024
Training Epoch: 81 [28672/50048]	Loss: 0.1043
Training Epoch: 81 [28800/50048]	Loss: 0.0758
Training Epoch: 81 [28928/50048]	Loss: 0.1070
Training Epoch: 81 [29056/50048]	Loss: 0.1232
Training Epoch: 81 [29184/50048]	Loss: 0.0921
Training Epoch: 81 [29312/50048]	Loss: 0.0692
Training Epoch: 81 [29440/50048]	Loss: 0.0947
Training Epoch: 81 [29568/50048]	Loss: 0.1286
Training Epoch: 81 [29696/50048]	Loss: 0.1138
Training Epoch: 81 [29824/50048]	Loss: 0.0788
Training Epoch: 81 [29952/50048]	Loss: 0.0884
Training Epoch: 81 [30080/50048]	Loss: 0.0590
Training Epoch: 81 [30208/50048]	Loss: 0.1393
Training Epoch: 81 [30336/50048]	Loss: 0.0622
Training Epoch: 81 [30464/50048]	Loss: 0.0849
Training Epoch: 81 [30592/50048]	Loss: 0.1162
Training Epoch: 81 [30720/50048]	Loss: 0.0696
Training Epoch: 81 [30848/50048]	Loss: 0.1147
Training Epoch: 81 [30976/50048]	Loss: 0.1255
Training Epoch: 81 [31104/50048]	Loss: 0.0997
Training Epoch: 81 [31232/50048]	Loss: 0.0847
Training Epoch: 81 [31360/50048]	Loss: 0.0641
Training Epoch: 81 [31488/50048]	Loss: 0.0959
Training Epoch: 81 [31616/50048]	Loss: 0.1789
Training Epoch: 81 [31744/50048]	Loss: 0.1216
Training Epoch: 81 [31872/50048]	Loss: 0.0347
Training Epoch: 81 [32000/50048]	Loss: 0.0560
Training Epoch: 81 [32128/50048]	Loss: 0.1011
Training Epoch: 81 [32256/50048]	Loss: 0.0901
Training Epoch: 81 [32384/50048]	Loss: 0.0719
Training Epoch: 81 [32512/50048]	Loss: 0.1809
Training Epoch: 81 [32640/50048]	Loss: 0.1613
Training Epoch: 81 [32768/50048]	Loss: 0.1739
Training Epoch: 81 [32896/50048]	Loss: 0.0656
Training Epoch: 81 [33024/50048]	Loss: 0.1192
Training Epoch: 81 [33152/50048]	Loss: 0.1183
Training Epoch: 81 [33280/50048]	Loss: 0.1160
Training Epoch: 81 [33408/50048]	Loss: 0.0689
Training Epoch: 81 [33536/50048]	Loss: 0.1051
Training Epoch: 81 [33664/50048]	Loss: 0.0945
Training Epoch: 81 [33792/50048]	Loss: 0.1304
Training Epoch: 81 [33920/50048]	Loss: 0.0769
Training Epoch: 81 [34048/50048]	Loss: 0.0705
Training Epoch: 81 [34176/50048]	Loss: 0.0503
Training Epoch: 81 [34304/50048]	Loss: 0.1338
Training Epoch: 81 [34432/50048]	Loss: 0.0676
Training Epoch: 81 [34560/50048]	Loss: 0.0675
Training Epoch: 81 [34688/50048]	Loss: 0.0594
Training Epoch: 81 [34816/50048]	Loss: 0.0632
Training Epoch: 81 [34944/50048]	Loss: 0.0500
Training Epoch: 81 [35072/50048]	Loss: 0.1764
Training Epoch: 81 [35200/50048]	Loss: 0.0909
Training Epoch: 81 [35328/50048]	Loss: 0.0849
Training Epoch: 81 [35456/50048]	Loss: 0.1164
Training Epoch: 81 [35584/50048]	Loss: 0.0999
Training Epoch: 81 [35712/50048]	Loss: 0.2008
Training Epoch: 81 [35840/50048]	Loss: 0.1381
Training Epoch: 81 [35968/50048]	Loss: 0.1433
Training Epoch: 81 [36096/50048]	Loss: 0.1322
Training Epoch: 81 [36224/50048]	Loss: 0.1152
Training Epoch: 81 [36352/50048]	Loss: 0.0897
Training Epoch: 81 [36480/50048]	Loss: 0.0564
Training Epoch: 81 [36608/50048]	Loss: 0.0723
Training Epoch: 81 [36736/50048]	Loss: 0.0983
Training Epoch: 81 [36864/50048]	Loss: 0.0704
Training Epoch: 81 [36992/50048]	Loss: 0.0528
Training Epoch: 81 [37120/50048]	Loss: 0.1651
Training Epoch: 81 [37248/50048]	Loss: 0.0577
Training Epoch: 81 [37376/50048]	Loss: 0.1667
Training Epoch: 81 [37504/50048]	Loss: 0.0743
Training Epoch: 81 [37632/50048]	Loss: 0.1121
Training Epoch: 81 [37760/50048]	Loss: 0.0957
Training Epoch: 81 [37888/50048]	Loss: 0.0882
Training Epoch: 81 [38016/50048]	Loss: 0.0552
Training Epoch: 81 [38144/50048]	Loss: 0.0960
Training Epoch: 81 [38272/50048]	Loss: 0.0799
Training Epoch: 81 [38400/50048]	Loss: 0.0892
Training Epoch: 81 [38528/50048]	Loss: 0.0904
Training Epoch: 81 [38656/50048]	Loss: 0.1527
Training Epoch: 81 [38784/50048]	Loss: 0.1040
Training Epoch: 81 [38912/50048]	Loss: 0.1030
Training Epoch: 81 [39040/50048]	Loss: 0.1003
Training Epoch: 81 [39168/50048]	Loss: 0.2131
Training Epoch: 81 [39296/50048]	Loss: 0.0951
Training Epoch: 81 [39424/50048]	Loss: 0.0820
Training Epoch: 81 [39552/50048]	Loss: 0.0725
Training Epoch: 81 [39680/50048]	Loss: 0.1945
Training Epoch: 81 [39808/50048]	Loss: 0.0440
Training Epoch: 81 [39936/50048]	Loss: 0.0846
Training Epoch: 81 [40064/50048]	Loss: 0.0970
Training Epoch: 81 [40192/50048]	Loss: 0.1342
Training Epoch: 81 [40320/50048]	Loss: 0.0505
Training Epoch: 81 [40448/50048]	Loss: 0.0927
Training Epoch: 81 [40576/50048]	Loss: 0.0575
Training Epoch: 81 [40704/50048]	Loss: 0.0777
Training Epoch: 81 [40832/50048]	Loss: 0.0542
Training Epoch: 81 [40960/50048]	Loss: 0.1362
Training Epoch: 81 [41088/50048]	Loss: 0.0831
Training Epoch: 81 [41216/50048]	Loss: 0.0664
Training Epoch: 81 [41344/50048]	Loss: 0.0800
Training Epoch: 81 [41472/50048]	Loss: 0.1004
Training Epoch: 81 [41600/50048]	Loss: 0.1228
Training Epoch: 81 [41728/50048]	Loss: 0.1056
Training Epoch: 81 [41856/50048]	Loss: 0.1135
Training Epoch: 81 [41984/50048]	Loss: 0.1062
Training Epoch: 81 [42112/50048]	Loss: 0.0828
Training Epoch: 81 [42240/50048]	Loss: 0.1771
Training Epoch: 81 [42368/50048]	Loss: 0.0346
Training Epoch: 81 [42496/50048]	Loss: 0.0981
Training Epoch: 81 [42624/50048]	Loss: 0.1498
Training Epoch: 81 [42752/50048]	Loss: 0.1618
Training Epoch: 81 [42880/50048]	Loss: 0.1668
Training Epoch: 81 [43008/50048]	Loss: 0.0862
Training Epoch: 81 [43136/50048]	Loss: 0.1012
Training Epoch: 81 [43264/50048]	Loss: 0.0983
Training Epoch: 81 [43392/50048]	Loss: 0.1423
Training Epoch: 81 [43520/50048]	Loss: 0.0909
Training Epoch: 81 [43648/50048]	Loss: 0.1009
Training Epoch: 81 [43776/50048]	Loss: 0.0746
Training Epoch: 81 [43904/50048]	Loss: 0.1150
Training Epoch: 81 [44032/50048]	Loss: 0.1587
Training Epoch: 81 [44160/50048]	Loss: 0.1472
Training Epoch: 81 [44288/50048]	Loss: 0.0617
Training Epoch: 81 [44416/50048]	Loss: 0.1696
Training Epoch: 81 [44544/50048]	Loss: 0.1874
Training Epoch: 81 [44672/50048]	Loss: 0.1327
Training Epoch: 81 [44800/50048]	Loss: 0.1388
Training Epoch: 81 [44928/50048]	Loss: 0.1691
Training Epoch: 81 [45056/50048]	Loss: 0.0822
Training Epoch: 81 [45184/50048]	Loss: 0.0785
Training Epoch: 81 [45312/50048]	Loss: 0.1149
Training Epoch: 81 [45440/50048]	Loss: 0.1148
Training Epoch: 81 [45568/50048]	Loss: 0.2218
Training Epoch: 81 [45696/50048]	Loss: 0.0925
2022-12-06 05:40:38,620 [ZeusDataLoader(train)] train epoch 82 done: time=86.52 energy=10508.86
2022-12-06 05:40:38,621 [ZeusDataLoader(eval)] Epoch 82 begin.
Training Epoch: 81 [45824/50048]	Loss: 0.1388
Training Epoch: 81 [45952/50048]	Loss: 0.1093
Training Epoch: 81 [46080/50048]	Loss: 0.1733
Training Epoch: 81 [46208/50048]	Loss: 0.0831
Training Epoch: 81 [46336/50048]	Loss: 0.1884
Training Epoch: 81 [46464/50048]	Loss: 0.2016
Training Epoch: 81 [46592/50048]	Loss: 0.0883
Training Epoch: 81 [46720/50048]	Loss: 0.0656
Training Epoch: 81 [46848/50048]	Loss: 0.1524
Training Epoch: 81 [46976/50048]	Loss: 0.1264
Training Epoch: 81 [47104/50048]	Loss: 0.1650
Training Epoch: 81 [47232/50048]	Loss: 0.0696
Training Epoch: 81 [47360/50048]	Loss: 0.1430
Training Epoch: 81 [47488/50048]	Loss: 0.1115
Training Epoch: 81 [47616/50048]	Loss: 0.1209
Training Epoch: 81 [47744/50048]	Loss: 0.0662
Training Epoch: 81 [47872/50048]	Loss: 0.1090
Training Epoch: 81 [48000/50048]	Loss: 0.0868
Training Epoch: 81 [48128/50048]	Loss: 0.0303
Training Epoch: 81 [48256/50048]	Loss: 0.0592
Training Epoch: 81 [48384/50048]	Loss: 0.1107
Training Epoch: 81 [48512/50048]	Loss: 0.0921
Training Epoch: 81 [48640/50048]	Loss: 0.1043
Training Epoch: 81 [48768/50048]	Loss: 0.0625
Training Epoch: 81 [48896/50048]	Loss: 0.1518
Training Epoch: 81 [49024/50048]	Loss: 0.0755
Training Epoch: 81 [49152/50048]	Loss: 0.1499
Training Epoch: 81 [49280/50048]	Loss: 0.0368
Training Epoch: 81 [49408/50048]	Loss: 0.0983
Training Epoch: 81 [49536/50048]	Loss: 0.1162
Training Epoch: 81 [49664/50048]	Loss: 0.0453
Training Epoch: 81 [49792/50048]	Loss: 0.0768
Training Epoch: 81 [49920/50048]	Loss: 0.1119
Training Epoch: 81 [50048/50048]	Loss: 0.2213
2022-12-06 10:40:42.295 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:40:42,316 [ZeusDataLoader(eval)] eval epoch 82 done: time=3.69 energy=439.24
2022-12-06 05:40:42,316 [ZeusDataLoader(train)] Up to epoch 82: time=7398.25, energy=898087.59, cost=1096390.88
2022-12-06 05:40:42,316 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:40:42,316 [ZeusDataLoader(train)] Expected next epoch: time=7488.05, energy=908885.60, cost=1109647.26
2022-12-06 05:40:42,317 [ZeusDataLoader(train)] Epoch 83 begin.
Validation Epoch: 81, Average loss: 0.0180, Accuracy: 0.6385
2022-12-06 05:40:42,504 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:40:42,505 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:40:42.506 [ZeusMonitor] Monitor started.
2022-12-06 10:40:42.506 [ZeusMonitor] Running indefinitely. 2022-12-06 10:40:42.506 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:40:42.506 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e83+gpu0.power.log
Training Epoch: 82 [128/50048]	Loss: 0.1260
Training Epoch: 82 [256/50048]	Loss: 0.0665
Training Epoch: 82 [384/50048]	Loss: 0.0672
Training Epoch: 82 [512/50048]	Loss: 0.0705
Training Epoch: 82 [640/50048]	Loss: 0.1345
Training Epoch: 82 [768/50048]	Loss: 0.0707
Training Epoch: 82 [896/50048]	Loss: 0.1006
Training Epoch: 82 [1024/50048]	Loss: 0.0681
Training Epoch: 82 [1152/50048]	Loss: 0.0503
Training Epoch: 82 [1280/50048]	Loss: 0.1694
Training Epoch: 82 [1408/50048]	Loss: 0.1482
Training Epoch: 82 [1536/50048]	Loss: 0.0976
Training Epoch: 82 [1664/50048]	Loss: 0.1164
Training Epoch: 82 [1792/50048]	Loss: 0.1233
Training Epoch: 82 [1920/50048]	Loss: 0.0709
Training Epoch: 82 [2048/50048]	Loss: 0.0549
Training Epoch: 82 [2176/50048]	Loss: 0.0560
Training Epoch: 82 [2304/50048]	Loss: 0.0852
Training Epoch: 82 [2432/50048]	Loss: 0.0641
Training Epoch: 82 [2560/50048]	Loss: 0.1755
Training Epoch: 82 [2688/50048]	Loss: 0.0848
Training Epoch: 82 [2816/50048]	Loss: 0.0886
Training Epoch: 82 [2944/50048]	Loss: 0.1238
Training Epoch: 82 [3072/50048]	Loss: 0.0529
Training Epoch: 82 [3200/50048]	Loss: 0.0492
Training Epoch: 82 [3328/50048]	Loss: 0.0961
Training Epoch: 82 [3456/50048]	Loss: 0.0646
Training Epoch: 82 [3584/50048]	Loss: 0.0708
Training Epoch: 82 [3712/50048]	Loss: 0.0871
Training Epoch: 82 [3840/50048]	Loss: 0.0300
Training Epoch: 82 [3968/50048]	Loss: 0.1510
Training Epoch: 82 [4096/50048]	Loss: 0.0796
Training Epoch: 82 [4224/50048]	Loss: 0.0978
Training Epoch: 82 [4352/50048]	Loss: 0.0848
Training Epoch: 82 [4480/50048]	Loss: 0.0980
Training Epoch: 82 [4608/50048]	Loss: 0.0925
Training Epoch: 82 [4736/50048]	Loss: 0.0791
Training Epoch: 82 [4864/50048]	Loss: 0.0809
Training Epoch: 82 [4992/50048]	Loss: 0.0597
Training Epoch: 82 [5120/50048]	Loss: 0.1080
Training Epoch: 82 [5248/50048]	Loss: 0.1387
Training Epoch: 82 [5376/50048]	Loss: 0.0480
Training Epoch: 82 [5504/50048]	Loss: 0.1566
Training Epoch: 82 [5632/50048]	Loss: 0.1007
Training Epoch: 82 [5760/50048]	Loss: 0.0906
Training Epoch: 82 [5888/50048]	Loss: 0.0863
Training Epoch: 82 [6016/50048]	Loss: 0.1188
Training Epoch: 82 [6144/50048]	Loss: 0.1171
Training Epoch: 82 [6272/50048]	Loss: 0.1568
Training Epoch: 82 [6400/50048]	Loss: 0.0588
Training Epoch: 82 [6528/50048]	Loss: 0.0592
Training Epoch: 82 [6656/50048]	Loss: 0.0839
Training Epoch: 82 [6784/50048]	Loss: 0.1277
Training Epoch: 82 [6912/50048]	Loss: 0.0719
Training Epoch: 82 [7040/50048]	Loss: 0.1046
Training Epoch: 82 [7168/50048]	Loss: 0.0635
Training Epoch: 82 [7296/50048]	Loss: 0.1352
Training Epoch: 82 [7424/50048]	Loss: 0.0959
Training Epoch: 82 [7552/50048]	Loss: 0.0964
Training Epoch: 82 [7680/50048]	Loss: 0.0476
Training Epoch: 82 [7808/50048]	Loss: 0.0775
Training Epoch: 82 [7936/50048]	Loss: 0.1558
Training Epoch: 82 [8064/50048]	Loss: 0.0651
Training Epoch: 82 [8192/50048]	Loss: 0.1215
Training Epoch: 82 [8320/50048]	Loss: 0.1135
Training Epoch: 82 [8448/50048]	Loss: 0.0913
Training Epoch: 82 [8576/50048]	Loss: 0.0805
Training Epoch: 82 [8704/50048]	Loss: 0.0541
Training Epoch: 82 [8832/50048]	Loss: 0.0641
Training Epoch: 82 [8960/50048]	Loss: 0.1130
Training Epoch: 82 [9088/50048]	Loss: 0.0821
Training Epoch: 82 [9216/50048]	Loss: 0.0771
Training Epoch: 82 [9344/50048]	Loss: 0.0724
Training Epoch: 82 [9472/50048]	Loss: 0.1467
Training Epoch: 82 [9600/50048]	Loss: 0.0708
Training Epoch: 82 [9728/50048]	Loss: 0.1772
Training Epoch: 82 [9856/50048]	Loss: 0.0977
Training Epoch: 82 [9984/50048]	Loss: 0.0933
Training Epoch: 82 [10112/50048]	Loss: 0.0853
Training Epoch: 82 [10240/50048]	Loss: 0.0250
Training Epoch: 82 [10368/50048]	Loss: 0.1346
Training Epoch: 82 [10496/50048]	Loss: 0.1078
Training Epoch: 82 [10624/50048]	Loss: 0.0927
Training Epoch: 82 [10752/50048]	Loss: 0.1106
Training Epoch: 82 [10880/50048]	Loss: 0.1220
Training Epoch: 82 [11008/50048]	Loss: 0.0849
Training Epoch: 82 [11136/50048]	Loss: 0.0797
Training Epoch: 82 [11264/50048]	Loss: 0.0474
Training Epoch: 82 [11392/50048]	Loss: 0.0522
Training Epoch: 82 [11520/50048]	Loss: 0.1694
Training Epoch: 82 [11648/50048]	Loss: 0.0736
Training Epoch: 82 [11776/50048]	Loss: 0.1306
Training Epoch: 82 [11904/50048]	Loss: 0.1336
Training Epoch: 82 [12032/50048]	Loss: 0.1317
Training Epoch: 82 [12160/50048]	Loss: 0.0598
Training Epoch: 82 [12288/50048]	Loss: 0.0971
Training Epoch: 82 [12416/50048]	Loss: 0.0972
Training Epoch: 82 [12544/50048]	Loss: 0.0599
Training Epoch: 82 [12672/50048]	Loss: 0.0658
Training Epoch: 82 [12800/50048]	Loss: 0.0814
Training Epoch: 82 [12928/50048]	Loss: 0.0747
Training Epoch: 82 [13056/50048]	Loss: 0.0878
Training Epoch: 82 [13184/50048]	Loss: 0.1617
Training Epoch: 82 [13312/50048]	Loss: 0.1087
Training Epoch: 82 [13440/50048]	Loss: 0.1197
Training Epoch: 82 [13568/50048]	Loss: 0.0871
Training Epoch: 82 [13696/50048]	Loss: 0.0636
Training Epoch: 82 [13824/50048]	Loss: 0.1369
Training Epoch: 82 [13952/50048]	Loss: 0.1335
Training Epoch: 82 [14080/50048]	Loss: 0.0501
Training Epoch: 82 [14208/50048]	Loss: 0.1464
Training Epoch: 82 [14336/50048]	Loss: 0.1165
Training Epoch: 82 [14464/50048]	Loss: 0.0473
Training Epoch: 82 [14592/50048]	Loss: 0.2163
Training Epoch: 82 [14720/50048]	Loss: 0.0716
Training Epoch: 82 [14848/50048]	Loss: 0.1870
Training Epoch: 82 [14976/50048]	Loss: 0.1457
Training Epoch: 82 [15104/50048]	Loss: 0.0991
Training Epoch: 82 [15232/50048]	Loss: 0.1198
Training Epoch: 82 [15360/50048]	Loss: 0.1443
Training Epoch: 82 [15488/50048]	Loss: 0.1370
Training Epoch: 82 [15616/50048]	Loss: 0.0464
Training Epoch: 82 [15744/50048]	Loss: 0.0655
Training Epoch: 82 [15872/50048]	Loss: 0.1451
Training Epoch: 82 [16000/50048]	Loss: 0.1404
Training Epoch: 82 [16128/50048]	Loss: 0.1571
Training Epoch: 82 [16256/50048]	Loss: 0.1348
Training Epoch: 82 [16384/50048]	Loss: 0.0992
Training Epoch: 82 [16512/50048]	Loss: 0.1074
Training Epoch: 82 [16640/50048]	Loss: 0.0940
Training Epoch: 82 [16768/50048]	Loss: 0.0776
Training Epoch: 82 [16896/50048]	Loss: 0.1846
Training Epoch: 82 [17024/50048]	Loss: 0.0921
Training Epoch: 82 [17152/50048]	Loss: 0.0706
Training Epoch: 82 [17280/50048]	Loss: 0.1093
Training Epoch: 82 [17408/50048]	Loss: 0.1054
Training Epoch: 82 [17536/50048]	Loss: 0.1813
Training Epoch: 82 [17664/50048]	Loss: 0.1117
Training Epoch: 82 [17792/50048]	Loss: 0.0961
Training Epoch: 82 [17920/50048]	Loss: 0.0938
Training Epoch: 82 [18048/50048]	Loss: 0.1184
Training Epoch: 82 [18176/50048]	Loss: 0.1944
Training Epoch: 82 [18304/50048]	Loss: 0.0772
Training Epoch: 82 [18432/50048]	Loss: 0.1298
Training Epoch: 82 [18560/50048]	Loss: 0.0932
Training Epoch: 82 [18688/50048]	Loss: 0.1233
Training Epoch: 82 [18816/50048]	Loss: 0.0885
Training Epoch: 82 [18944/50048]	Loss: 0.1234
Training Epoch: 82 [19072/50048]	Loss: 0.0603
Training Epoch: 82 [19200/50048]	Loss: 0.2372
Training Epoch: 82 [19328/50048]	Loss: 0.1210
Training Epoch: 82 [19456/50048]	Loss: 0.1505
Training Epoch: 82 [19584/50048]	Loss: 0.0519
Training Epoch: 82 [19712/50048]	Loss: 0.0853
Training Epoch: 82 [19840/50048]	Loss: 0.1714
Training Epoch: 82 [19968/50048]	Loss: 0.1157
Training Epoch: 82 [20096/50048]	Loss: 0.1083
Training Epoch: 82 [20224/50048]	Loss: 0.1532
Training Epoch: 82 [20352/50048]	Loss: 0.0880
Training Epoch: 82 [20480/50048]	Loss: 0.0780
Training Epoch: 82 [20608/50048]	Loss: 0.0648
Training Epoch: 82 [20736/50048]	Loss: 0.1409
Training Epoch: 82 [20864/50048]	Loss: 0.0380
Training Epoch: 82 [20992/50048]	Loss: 0.1243
Training Epoch: 82 [21120/50048]	Loss: 0.1048
Training Epoch: 82 [21248/50048]	Loss: 0.1062
Training Epoch: 82 [21376/50048]	Loss: 0.0982
Training Epoch: 82 [21504/50048]	Loss: 0.0566
Training Epoch: 82 [21632/50048]	Loss: 0.1235
Training Epoch: 82 [21760/50048]	Loss: 0.0900
Training Epoch: 82 [21888/50048]	Loss: 0.1265
Training Epoch: 82 [22016/50048]	Loss: 0.1127
Training Epoch: 82 [22144/50048]	Loss: 0.0846
Training Epoch: 82 [22272/50048]	Loss: 0.2012
Training Epoch: 82 [22400/50048]	Loss: 0.1474
Training Epoch: 82 [22528/50048]	Loss: 0.1177
Training Epoch: 82 [22656/50048]	Loss: 0.1004
Training Epoch: 82 [22784/50048]	Loss: 0.1287
Training Epoch: 82 [22912/50048]	Loss: 0.1094
Training Epoch: 82 [23040/50048]	Loss: 0.1095
Training Epoch: 82 [23168/50048]	Loss: 0.1053
Training Epoch: 82 [23296/50048]	Loss: 0.1769
Training Epoch: 82 [23424/50048]	Loss: 0.1284
Training Epoch: 82 [23552/50048]	Loss: 0.0553
Training Epoch: 82 [23680/50048]	Loss: 0.1372
Training Epoch: 82 [23808/50048]	Loss: 0.0758
Training Epoch: 82 [23936/50048]	Loss: 0.0915
Training Epoch: 82 [24064/50048]	Loss: 0.1027
Training Epoch: 82 [24192/50048]	Loss: 0.1099
Training Epoch: 82 [24320/50048]	Loss: 0.0756
Training Epoch: 82 [24448/50048]	Loss: 0.0524
Training Epoch: 82 [24576/50048]	Loss: 0.0970
Training Epoch: 82 [24704/50048]	Loss: 0.1144
Training Epoch: 82 [24832/50048]	Loss: 0.1055
Training Epoch: 82 [24960/50048]	Loss: 0.0531
Training Epoch: 82 [25088/50048]	Loss: 0.1577
Training Epoch: 82 [25216/50048]	Loss: 0.0772
Training Epoch: 82 [25344/50048]	Loss: 0.0885
Training Epoch: 82 [25472/50048]	Loss: 0.0373
Training Epoch: 82 [25600/50048]	Loss: 0.1968
Training Epoch: 82 [25728/50048]	Loss: 0.1526
Training Epoch: 82 [25856/50048]	Loss: 0.0751
Training Epoch: 82 [25984/50048]	Loss: 0.0989
Training Epoch: 82 [26112/50048]	Loss: 0.0645
Training Epoch: 82 [26240/50048]	Loss: 0.0839
Training Epoch: 82 [26368/50048]	Loss: 0.0322
Training Epoch: 82 [26496/50048]	Loss: 0.0540
Training Epoch: 82 [26624/50048]	Loss: 0.1175
Training Epoch: 82 [26752/50048]	Loss: 0.0698
Training Epoch: 82 [26880/50048]	Loss: 0.2128
Training Epoch: 82 [27008/50048]	Loss: 0.0912
Training Epoch: 82 [27136/50048]	Loss: 0.1196
Training Epoch: 82 [27264/50048]	Loss: 0.0862
Training Epoch: 82 [27392/50048]	Loss: 0.0888
Training Epoch: 82 [27520/50048]	Loss: 0.0616
Training Epoch: 82 [27648/50048]	Loss: 0.0727
Training Epoch: 82 [27776/50048]	Loss: 0.2037
Training Epoch: 82 [27904/50048]	Loss: 0.0805
Training Epoch: 82 [28032/50048]	Loss: 0.1213
Training Epoch: 82 [28160/50048]	Loss: 0.0553
Training Epoch: 82 [28288/50048]	Loss: 0.0801
Training Epoch: 82 [28416/50048]	Loss: 0.0861
Training Epoch: 82 [28544/50048]	Loss: 0.1504
Training Epoch: 82 [28672/50048]	Loss: 0.1124
Training Epoch: 82 [28800/50048]	Loss: 0.1298
Training Epoch: 82 [28928/50048]	Loss: 0.1968
Training Epoch: 82 [29056/50048]	Loss: 0.1050
Training Epoch: 82 [29184/50048]	Loss: 0.1402
Training Epoch: 82 [29312/50048]	Loss: 0.0665
Training Epoch: 82 [29440/50048]	Loss: 0.0901
Training Epoch: 82 [29568/50048]	Loss: 0.1474
Training Epoch: 82 [29696/50048]	Loss: 0.1424
Training Epoch: 82 [29824/50048]	Loss: 0.1280
Training Epoch: 82 [29952/50048]	Loss: 0.1131
Training Epoch: 82 [30080/50048]	Loss: 0.1129
Training Epoch: 82 [30208/50048]	Loss: 0.1384
Training Epoch: 82 [30336/50048]	Loss: 0.1112
Training Epoch: 82 [30464/50048]	Loss: 0.0977
Training Epoch: 82 [30592/50048]	Loss: 0.0537
Training Epoch: 82 [30720/50048]	Loss: 0.0958
Training Epoch: 82 [30848/50048]	Loss: 0.0912
Training Epoch: 82 [30976/50048]	Loss: 0.0582
Training Epoch: 82 [31104/50048]	Loss: 0.1033
Training Epoch: 82 [31232/50048]	Loss: 0.1165
Training Epoch: 82 [31360/50048]	Loss: 0.0831
Training Epoch: 82 [31488/50048]	Loss: 0.1341
Training Epoch: 82 [31616/50048]	Loss: 0.1214
Training Epoch: 82 [31744/50048]	Loss: 0.0727
Training Epoch: 82 [31872/50048]	Loss: 0.1070
Training Epoch: 82 [32000/50048]	Loss: 0.1338
Training Epoch: 82 [32128/50048]	Loss: 0.0723
Training Epoch: 82 [32256/50048]	Loss: 0.0545
Training Epoch: 82 [32384/50048]	Loss: 0.1191
Training Epoch: 82 [32512/50048]	Loss: 0.0586
Training Epoch: 82 [32640/50048]	Loss: 0.0463
Training Epoch: 82 [32768/50048]	Loss: 0.0546
Training Epoch: 82 [32896/50048]	Loss: 0.0754
Training Epoch: 82 [33024/50048]	Loss: 0.1217
Training Epoch: 82 [33152/50048]	Loss: 0.1087
Training Epoch: 82 [33280/50048]	Loss: 0.0834
Training Epoch: 82 [33408/50048]	Loss: 0.1058
Training Epoch: 82 [33536/50048]	Loss: 0.1459
Training Epoch: 82 [33664/50048]	Loss: 0.0998
Training Epoch: 82 [33792/50048]	Loss: 0.1637
Training Epoch: 82 [33920/50048]	Loss: 0.1025
Training Epoch: 82 [34048/50048]	Loss: 0.0998
Training Epoch: 82 [34176/50048]	Loss: 0.0683
Training Epoch: 82 [34304/50048]	Loss: 0.0889
Training Epoch: 82 [34432/50048]	Loss: 0.1182
Training Epoch: 82 [34560/50048]	Loss: 0.0920
Training Epoch: 82 [34688/50048]	Loss: 0.0914
Training Epoch: 82 [34816/50048]	Loss: 0.0769
Training Epoch: 82 [34944/50048]	Loss: 0.1073
Training Epoch: 82 [35072/50048]	Loss: 0.1218
Training Epoch: 82 [35200/50048]	Loss: 0.1059
Training Epoch: 82 [35328/50048]	Loss: 0.0928
Training Epoch: 82 [35456/50048]	Loss: 0.1649
Training Epoch: 82 [35584/50048]	Loss: 0.1596
Training Epoch: 82 [35712/50048]	Loss: 0.1082
Training Epoch: 82 [35840/50048]	Loss: 0.1093
Training Epoch: 82 [35968/50048]	Loss: 0.1458
Training Epoch: 82 [36096/50048]	Loss: 0.1129
Training Epoch: 82 [36224/50048]	Loss: 0.1417
Training Epoch: 82 [36352/50048]	Loss: 0.1254
Training Epoch: 82 [36480/50048]	Loss: 0.1644
Training Epoch: 82 [36608/50048]	Loss: 0.0846
Training Epoch: 82 [36736/50048]	Loss: 0.0989
Training Epoch: 82 [36864/50048]	Loss: 0.2121
Training Epoch: 82 [36992/50048]	Loss: 0.0836
Training Epoch: 82 [37120/50048]	Loss: 0.2014
Training Epoch: 82 [37248/50048]	Loss: 0.1363
Training Epoch: 82 [37376/50048]	Loss: 0.0531
Training Epoch: 82 [37504/50048]	Loss: 0.1315
Training Epoch: 82 [37632/50048]	Loss: 0.1030
Training Epoch: 82 [37760/50048]	Loss: 0.0925
Training Epoch: 82 [37888/50048]	Loss: 0.1351
Training Epoch: 82 [38016/50048]	Loss: 0.0996
Training Epoch: 82 [38144/50048]	Loss: 0.1518
Training Epoch: 82 [38272/50048]	Loss: 0.0901
Training Epoch: 82 [38400/50048]	Loss: 0.1543
Training Epoch: 82 [38528/50048]	Loss: 0.1557
Training Epoch: 82 [38656/50048]	Loss: 0.0671
Training Epoch: 82 [38784/50048]	Loss: 0.1115
Training Epoch: 82 [38912/50048]	Loss: 0.1100
Training Epoch: 82 [39040/50048]	Loss: 0.1692
Training Epoch: 82 [39168/50048]	Loss: 0.1247
Training Epoch: 82 [39296/50048]	Loss: 0.1376
Training Epoch: 82 [39424/50048]	Loss: 0.1408
Training Epoch: 82 [39552/50048]	Loss: 0.0780
Training Epoch: 82 [39680/50048]	Loss: 0.2021
Training Epoch: 82 [39808/50048]	Loss: 0.0621
Training Epoch: 82 [39936/50048]	Loss: 0.0908
Training Epoch: 82 [40064/50048]	Loss: 0.1063
Training Epoch: 82 [40192/50048]	Loss: 0.0749
Training Epoch: 82 [40320/50048]	Loss: 0.1173
Training Epoch: 82 [40448/50048]	Loss: 0.1215
Training Epoch: 82 [40576/50048]	Loss: 0.2044
Training Epoch: 82 [40704/50048]	Loss: 0.1400
Training Epoch: 82 [40832/50048]	Loss: 0.0820
Training Epoch: 82 [40960/50048]	Loss: 0.1127
Training Epoch: 82 [41088/50048]	Loss: 0.1285
Training Epoch: 82 [41216/50048]	Loss: 0.1184
Training Epoch: 82 [41344/50048]	Loss: 0.1258
Training Epoch: 82 [41472/50048]	Loss: 0.1488
Training Epoch: 82 [41600/50048]	Loss: 0.1141
Training Epoch: 82 [41728/50048]	Loss: 0.1584
Training Epoch: 82 [41856/50048]	Loss: 0.1623
Training Epoch: 82 [41984/50048]	Loss: 0.0879
Training Epoch: 82 [42112/50048]	Loss: 0.0683
Training Epoch: 82 [42240/50048]	Loss: 0.1718
Training Epoch: 82 [42368/50048]	Loss: 0.0885
Training Epoch: 82 [42496/50048]	Loss: 0.0749
Training Epoch: 82 [42624/50048]	Loss: 0.0611
Training Epoch: 82 [42752/50048]	Loss: 0.0761
Training Epoch: 82 [42880/50048]	Loss: 0.0953
Training Epoch: 82 [43008/50048]	Loss: 0.1035
Training Epoch: 82 [43136/50048]	Loss: 0.1006
Training Epoch: 82 [43264/50048]	Loss: 0.1262
Training Epoch: 82 [43392/50048]	Loss: 0.0513
Training Epoch: 82 [43520/50048]	Loss: 0.1168
Training Epoch: 82 [43648/50048]	Loss: 0.0931
Training Epoch: 82 [43776/50048]	Loss: 0.1276
Training Epoch: 82 [43904/50048]	Loss: 0.1026
Training Epoch: 82 [44032/50048]	Loss: 0.1461
Training Epoch: 82 [44160/50048]	Loss: 0.1753
Training Epoch: 82 [44288/50048]	Loss: 0.0861
Training Epoch: 82 [44416/50048]	Loss: 0.1252
Training Epoch: 82 [44544/50048]	Loss: 0.1057
Training Epoch: 82 [44672/50048]	Loss: 0.0962
Training Epoch: 82 [44800/50048]	Loss: 0.0861
Training Epoch: 82 [44928/50048]	Loss: 0.0577
Training Epoch: 82 [45056/50048]	Loss: 0.0909
Training Epoch: 82 [45184/50048]	Loss: 0.0758
Training Epoch: 82 [45312/50048]	Loss: 0.0846
Training Epoch: 82 [45440/50048]	Loss: 0.1437
Training Epoch: 82 [45568/50048]	Loss: 0.1132
Training Epoch: 82 [45696/50048]	Loss: 0.0973
2022-12-06 05:42:08,775 [ZeusDataLoader(train)] train epoch 83 done: time=86.45 energy=10508.34
2022-12-06 05:42:08,776 [ZeusDataLoader(eval)] Epoch 83 begin.
Training Epoch: 82 [45824/50048]	Loss: 0.0993
Training Epoch: 82 [45952/50048]	Loss: 0.0855
Training Epoch: 82 [46080/50048]	Loss: 0.1299
Training Epoch: 82 [46208/50048]	Loss: 0.1215
Training Epoch: 82 [46336/50048]	Loss: 0.0926
Training Epoch: 82 [46464/50048]	Loss: 0.1132
Training Epoch: 82 [46592/50048]	Loss: 0.0801
Training Epoch: 82 [46720/50048]	Loss: 0.0841
Training Epoch: 82 [46848/50048]	Loss: 0.0877
Training Epoch: 82 [46976/50048]	Loss: 0.1115
Training Epoch: 82 [47104/50048]	Loss: 0.1343
Training Epoch: 82 [47232/50048]	Loss: 0.1421
Training Epoch: 82 [47360/50048]	Loss: 0.0502
Training Epoch: 82 [47488/50048]	Loss: 0.1097
Training Epoch: 82 [47616/50048]	Loss: 0.1096
Training Epoch: 82 [47744/50048]	Loss: 0.2081
Training Epoch: 82 [47872/50048]	Loss: 0.1563
Training Epoch: 82 [48000/50048]	Loss: 0.0825
Training Epoch: 82 [48128/50048]	Loss: 0.1783
Training Epoch: 82 [48256/50048]	Loss: 0.1131
Training Epoch: 82 [48384/50048]	Loss: 0.1057
Training Epoch: 82 [48512/50048]	Loss: 0.1320
Training Epoch: 82 [48640/50048]	Loss: 0.0841
Training Epoch: 82 [48768/50048]	Loss: 0.0948
Training Epoch: 82 [48896/50048]	Loss: 0.1486
Training Epoch: 82 [49024/50048]	Loss: 0.1720
Training Epoch: 82 [49152/50048]	Loss: 0.1043
Training Epoch: 82 [49280/50048]	Loss: 0.0915
Training Epoch: 82 [49408/50048]	Loss: 0.2539
Training Epoch: 82 [49536/50048]	Loss: 0.1890
Training Epoch: 82 [49664/50048]	Loss: 0.1257
Training Epoch: 82 [49792/50048]	Loss: 0.0861
Training Epoch: 82 [49920/50048]	Loss: 0.0879
Training Epoch: 82 [50048/50048]	Loss: 0.1848
2022-12-06 10:42:12.432 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:42:12,450 [ZeusDataLoader(eval)] eval epoch 83 done: time=3.67 energy=440.72
2022-12-06 05:42:12,451 [ZeusDataLoader(train)] Up to epoch 83: time=7488.36, energy=909036.64, cost=1109750.23
2022-12-06 05:42:12,451 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:42:12,451 [ZeusDataLoader(train)] Expected next epoch: time=7578.16, energy=919834.66, cost=1123006.61
2022-12-06 05:42:12,452 [ZeusDataLoader(train)] Epoch 84 begin.
Validation Epoch: 82, Average loss: 0.0187, Accuracy: 0.6332
2022-12-06 05:42:12,632 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:42:12,632 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:42:12.634 [ZeusMonitor] Monitor started.
2022-12-06 10:42:12.634 [ZeusMonitor] Running indefinitely. 2022-12-06 10:42:12.634 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:42:12.634 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e84+gpu0.power.log
Training Epoch: 83 [128/50048]	Loss: 0.1371
Training Epoch: 83 [256/50048]	Loss: 0.1101
Training Epoch: 83 [384/50048]	Loss: 0.1686
Training Epoch: 83 [512/50048]	Loss: 0.1129
Training Epoch: 83 [640/50048]	Loss: 0.1727
Training Epoch: 83 [768/50048]	Loss: 0.1068
Training Epoch: 83 [896/50048]	Loss: 0.1564
Training Epoch: 83 [1024/50048]	Loss: 0.0747
Training Epoch: 83 [1152/50048]	Loss: 0.1479
Training Epoch: 83 [1280/50048]	Loss: 0.0942
Training Epoch: 83 [1408/50048]	Loss: 0.1214
Training Epoch: 83 [1536/50048]	Loss: 0.0938
Training Epoch: 83 [1664/50048]	Loss: 0.0922
Training Epoch: 83 [1792/50048]	Loss: 0.0541
Training Epoch: 83 [1920/50048]	Loss: 0.0256
Training Epoch: 83 [2048/50048]	Loss: 0.0904
Training Epoch: 83 [2176/50048]	Loss: 0.0762
Training Epoch: 83 [2304/50048]	Loss: 0.0526
Training Epoch: 83 [2432/50048]	Loss: 0.0948
Training Epoch: 83 [2560/50048]	Loss: 0.1016
Training Epoch: 83 [2688/50048]	Loss: 0.0754
Training Epoch: 83 [2816/50048]	Loss: 0.0816
Training Epoch: 83 [2944/50048]	Loss: 0.1062
Training Epoch: 83 [3072/50048]	Loss: 0.1581
Training Epoch: 83 [3200/50048]	Loss: 0.0793
Training Epoch: 83 [3328/50048]	Loss: 0.1747
Training Epoch: 83 [3456/50048]	Loss: 0.0576
Training Epoch: 83 [3584/50048]	Loss: 0.1187
Training Epoch: 83 [3712/50048]	Loss: 0.0810
Training Epoch: 83 [3840/50048]	Loss: 0.0671
Training Epoch: 83 [3968/50048]	Loss: 0.0807
Training Epoch: 83 [4096/50048]	Loss: 0.0454
Training Epoch: 83 [4224/50048]	Loss: 0.1088
Training Epoch: 83 [4352/50048]	Loss: 0.0638
Training Epoch: 83 [4480/50048]	Loss: 0.1561
Training Epoch: 83 [4608/50048]	Loss: 0.1057
Training Epoch: 83 [4736/50048]	Loss: 0.1085
Training Epoch: 83 [4864/50048]	Loss: 0.1264
Training Epoch: 83 [4992/50048]	Loss: 0.1141
Training Epoch: 83 [5120/50048]	Loss: 0.0603
Training Epoch: 83 [5248/50048]	Loss: 0.0672
Training Epoch: 83 [5376/50048]	Loss: 0.0956
Training Epoch: 83 [5504/50048]	Loss: 0.0564
Training Epoch: 83 [5632/50048]	Loss: 0.1064
Training Epoch: 83 [5760/50048]	Loss: 0.1448
Training Epoch: 83 [5888/50048]	Loss: 0.0833
Training Epoch: 83 [6016/50048]	Loss: 0.1281
Training Epoch: 83 [6144/50048]	Loss: 0.0977
Training Epoch: 83 [6272/50048]	Loss: 0.1108
Training Epoch: 83 [6400/50048]	Loss: 0.0713
Training Epoch: 83 [6528/50048]	Loss: 0.0987
Training Epoch: 83 [6656/50048]	Loss: 0.0684
Training Epoch: 83 [6784/50048]	Loss: 0.0629
Training Epoch: 83 [6912/50048]	Loss: 0.0796
Training Epoch: 83 [7040/50048]	Loss: 0.2126
Training Epoch: 83 [7168/50048]	Loss: 0.0762
Training Epoch: 83 [7296/50048]	Loss: 0.1353
Training Epoch: 83 [7424/50048]	Loss: 0.0422
Training Epoch: 83 [7552/50048]	Loss: 0.0709
Training Epoch: 83 [7680/50048]	Loss: 0.0458
Training Epoch: 83 [7808/50048]	Loss: 0.1007
Training Epoch: 83 [7936/50048]	Loss: 0.0690
Training Epoch: 83 [8064/50048]	Loss: 0.0878
Training Epoch: 83 [8192/50048]	Loss: 0.1907
Training Epoch: 83 [8320/50048]	Loss: 0.0278
Training Epoch: 83 [8448/50048]	Loss: 0.0857
Training Epoch: 83 [8576/50048]	Loss: 0.0759
Training Epoch: 83 [8704/50048]	Loss: 0.0784
Training Epoch: 83 [8832/50048]	Loss: 0.1276
Training Epoch: 83 [8960/50048]	Loss: 0.0997
Training Epoch: 83 [9088/50048]	Loss: 0.0522
Training Epoch: 83 [9216/50048]	Loss: 0.0835
Training Epoch: 83 [9344/50048]	Loss: 0.1606
Training Epoch: 83 [9472/50048]	Loss: 0.0707
Training Epoch: 83 [9600/50048]	Loss: 0.0917
Training Epoch: 83 [9728/50048]	Loss: 0.0954
Training Epoch: 83 [9856/50048]	Loss: 0.0310
Training Epoch: 83 [9984/50048]	Loss: 0.0558
Training Epoch: 83 [10112/50048]	Loss: 0.1028
Training Epoch: 83 [10240/50048]	Loss: 0.0617
Training Epoch: 83 [10368/50048]	Loss: 0.1541
Training Epoch: 83 [10496/50048]	Loss: 0.0534
Training Epoch: 83 [10624/50048]	Loss: 0.0745
Training Epoch: 83 [10752/50048]	Loss: 0.0563
Training Epoch: 83 [10880/50048]	Loss: 0.0611
Training Epoch: 83 [11008/50048]	Loss: 0.0931
Training Epoch: 83 [11136/50048]	Loss: 0.0947
Training Epoch: 83 [11264/50048]	Loss: 0.1019
Training Epoch: 83 [11392/50048]	Loss: 0.1755
Training Epoch: 83 [11520/50048]	Loss: 0.1127
Training Epoch: 83 [11648/50048]	Loss: 0.1232
Training Epoch: 83 [11776/50048]	Loss: 0.0974
Training Epoch: 83 [11904/50048]	Loss: 0.0878
Training Epoch: 83 [12032/50048]	Loss: 0.1146
Training Epoch: 83 [12160/50048]	Loss: 0.1401
Training Epoch: 83 [12288/50048]	Loss: 0.1017
Training Epoch: 83 [12416/50048]	Loss: 0.0711
Training Epoch: 83 [12544/50048]	Loss: 0.0792
Training Epoch: 83 [12672/50048]	Loss: 0.1579
Training Epoch: 83 [12800/50048]	Loss: 0.0941
Training Epoch: 83 [12928/50048]	Loss: 0.1126
Training Epoch: 83 [13056/50048]	Loss: 0.0980
Training Epoch: 83 [13184/50048]	Loss: 0.0948
Training Epoch: 83 [13312/50048]	Loss: 0.1591
Training Epoch: 83 [13440/50048]	Loss: 0.0822
Training Epoch: 83 [13568/50048]	Loss: 0.0588
Training Epoch: 83 [13696/50048]	Loss: 0.1478
Training Epoch: 83 [13824/50048]	Loss: 0.0941
Training Epoch: 83 [13952/50048]	Loss: 0.0745
Training Epoch: 83 [14080/50048]	Loss: 0.0711
Training Epoch: 83 [14208/50048]	Loss: 0.1114
Training Epoch: 83 [14336/50048]	Loss: 0.1119
Training Epoch: 83 [14464/50048]	Loss: 0.0897
Training Epoch: 83 [14592/50048]	Loss: 0.1238
Training Epoch: 83 [14720/50048]	Loss: 0.1542
Training Epoch: 83 [14848/50048]	Loss: 0.0711
Training Epoch: 83 [14976/50048]	Loss: 0.0380
Training Epoch: 83 [15104/50048]	Loss: 0.0740
Training Epoch: 83 [15232/50048]	Loss: 0.1157
Training Epoch: 83 [15360/50048]	Loss: 0.2218
Training Epoch: 83 [15488/50048]	Loss: 0.0532
Training Epoch: 83 [15616/50048]	Loss: 0.0650
Training Epoch: 83 [15744/50048]	Loss: 0.1519
Training Epoch: 83 [15872/50048]	Loss: 0.0930
Training Epoch: 83 [16000/50048]	Loss: 0.0548
Training Epoch: 83 [16128/50048]	Loss: 0.1352
Training Epoch: 83 [16256/50048]	Loss: 0.1096
Training Epoch: 83 [16384/50048]	Loss: 0.1155
Training Epoch: 83 [16512/50048]	Loss: 0.1263
Training Epoch: 83 [16640/50048]	Loss: 0.1087
Training Epoch: 83 [16768/50048]	Loss: 0.0980
Training Epoch: 83 [16896/50048]	Loss: 0.0836
Training Epoch: 83 [17024/50048]	Loss: 0.0933
Training Epoch: 83 [17152/50048]	Loss: 0.1184
Training Epoch: 83 [17280/50048]	Loss: 0.0873
Training Epoch: 83 [17408/50048]	Loss: 0.1168
Training Epoch: 83 [17536/50048]	Loss: 0.1548
Training Epoch: 83 [17664/50048]	Loss: 0.0864
Training Epoch: 83 [17792/50048]	Loss: 0.1157
Training Epoch: 83 [17920/50048]	Loss: 0.1562
Training Epoch: 83 [18048/50048]	Loss: 0.1366
Training Epoch: 83 [18176/50048]	Loss: 0.1936
Training Epoch: 83 [18304/50048]	Loss: 0.0577
Training Epoch: 83 [18432/50048]	Loss: 0.1533
Training Epoch: 83 [18560/50048]	Loss: 0.1042
Training Epoch: 83 [18688/50048]	Loss: 0.1312
Training Epoch: 83 [18816/50048]	Loss: 0.1360
Training Epoch: 83 [18944/50048]	Loss: 0.1038
Training Epoch: 83 [19072/50048]	Loss: 0.1854
Training Epoch: 83 [19200/50048]	Loss: 0.0570
Training Epoch: 83 [19328/50048]	Loss: 0.1522
Training Epoch: 83 [19456/50048]	Loss: 0.2296
Training Epoch: 83 [19584/50048]	Loss: 0.1575
Training Epoch: 83 [19712/50048]	Loss: 0.0970
Training Epoch: 83 [19840/50048]	Loss: 0.0817
Training Epoch: 83 [19968/50048]	Loss: 0.0782
Training Epoch: 83 [20096/50048]	Loss: 0.1090
Training Epoch: 83 [20224/50048]	Loss: 0.1512
Training Epoch: 83 [20352/50048]	Loss: 0.1184
Training Epoch: 83 [20480/50048]	Loss: 0.0701
Training Epoch: 83 [20608/50048]	Loss: 0.1241
Training Epoch: 83 [20736/50048]	Loss: 0.0780
Training Epoch: 83 [20864/50048]	Loss: 0.1042
Training Epoch: 83 [20992/50048]	Loss: 0.0779
Training Epoch: 83 [21120/50048]	Loss: 0.1077
Training Epoch: 83 [21248/50048]	Loss: 0.1602
Training Epoch: 83 [21376/50048]	Loss: 0.0439
Training Epoch: 83 [21504/50048]	Loss: 0.0485
Training Epoch: 83 [21632/50048]	Loss: 0.1029
Training Epoch: 83 [21760/50048]	Loss: 0.1574
Training Epoch: 83 [21888/50048]	Loss: 0.1016
Training Epoch: 83 [22016/50048]	Loss: 0.0931
Training Epoch: 83 [22144/50048]	Loss: 0.0457
Training Epoch: 83 [22272/50048]	Loss: 0.1033
Training Epoch: 83 [22400/50048]	Loss: 0.1517
Training Epoch: 83 [22528/50048]	Loss: 0.0946
Training Epoch: 83 [22656/50048]	Loss: 0.0774
Training Epoch: 83 [22784/50048]	Loss: 0.0848
Training Epoch: 83 [22912/50048]	Loss: 0.0625
Training Epoch: 83 [23040/50048]	Loss: 0.0660
Training Epoch: 83 [23168/50048]	Loss: 0.1090
Training Epoch: 83 [23296/50048]	Loss: 0.0947
Training Epoch: 83 [23424/50048]	Loss: 0.0512
Training Epoch: 83 [23552/50048]	Loss: 0.1539
Training Epoch: 83 [23680/50048]	Loss: 0.0870
Training Epoch: 83 [23808/50048]	Loss: 0.0972
Training Epoch: 83 [23936/50048]	Loss: 0.0900
Training Epoch: 83 [24064/50048]	Loss: 0.1043
Training Epoch: 83 [24192/50048]	Loss: 0.1462
Training Epoch: 83 [24320/50048]	Loss: 0.0721
Training Epoch: 83 [24448/50048]	Loss: 0.0868
Training Epoch: 83 [24576/50048]	Loss: 0.1461
Training Epoch: 83 [24704/50048]	Loss: 0.0579
Training Epoch: 83 [24832/50048]	Loss: 0.1022
Training Epoch: 83 [24960/50048]	Loss: 0.1043
Training Epoch: 83 [25088/50048]	Loss: 0.1130
Training Epoch: 83 [25216/50048]	Loss: 0.0972
Training Epoch: 83 [25344/50048]	Loss: 0.1313
Training Epoch: 83 [25472/50048]	Loss: 0.0738
Training Epoch: 83 [25600/50048]	Loss: 0.0605
Training Epoch: 83 [25728/50048]	Loss: 0.0787
Training Epoch: 83 [25856/50048]	Loss: 0.0697
Training Epoch: 83 [25984/50048]	Loss: 0.1747
Training Epoch: 83 [26112/50048]	Loss: 0.0959
Training Epoch: 83 [26240/50048]	Loss: 0.0713
Training Epoch: 83 [26368/50048]	Loss: 0.1433
Training Epoch: 83 [26496/50048]	Loss: 0.0623
Training Epoch: 83 [26624/50048]	Loss: 0.0615
Training Epoch: 83 [26752/50048]	Loss: 0.1598
Training Epoch: 83 [26880/50048]	Loss: 0.0895
Training Epoch: 83 [27008/50048]	Loss: 0.1466
Training Epoch: 83 [27136/50048]	Loss: 0.0560
Training Epoch: 83 [27264/50048]	Loss: 0.1912
Training Epoch: 83 [27392/50048]	Loss: 0.0599
Training Epoch: 83 [27520/50048]	Loss: 0.0545
Training Epoch: 83 [27648/50048]	Loss: 0.1410
Training Epoch: 83 [27776/50048]	Loss: 0.0562
Training Epoch: 83 [27904/50048]	Loss: 0.1344
Training Epoch: 83 [28032/50048]	Loss: 0.0795
Training Epoch: 83 [28160/50048]	Loss: 0.1240
Training Epoch: 83 [28288/50048]	Loss: 0.1178
Training Epoch: 83 [28416/50048]	Loss: 0.0766
Training Epoch: 83 [28544/50048]	Loss: 0.0463
Training Epoch: 83 [28672/50048]	Loss: 0.0643
Training Epoch: 83 [28800/50048]	Loss: 0.1041
Training Epoch: 83 [28928/50048]	Loss: 0.1183
Training Epoch: 83 [29056/50048]	Loss: 0.1457
Training Epoch: 83 [29184/50048]	Loss: 0.1089
Training Epoch: 83 [29312/50048]	Loss: 0.1408
Training Epoch: 83 [29440/50048]	Loss: 0.1361
Training Epoch: 83 [29568/50048]	Loss: 0.1157
Training Epoch: 83 [29696/50048]	Loss: 0.1588
Training Epoch: 83 [29824/50048]	Loss: 0.0679
Training Epoch: 83 [29952/50048]	Loss: 0.0533
Training Epoch: 83 [30080/50048]	Loss: 0.1204
Training Epoch: 83 [30208/50048]	Loss: 0.1212
Training Epoch: 83 [30336/50048]	Loss: 0.0975
Training Epoch: 83 [30464/50048]	Loss: 0.1763
Training Epoch: 83 [30592/50048]	Loss: 0.0931
Training Epoch: 83 [30720/50048]	Loss: 0.0908
Training Epoch: 83 [30848/50048]	Loss: 0.0513
Training Epoch: 83 [30976/50048]	Loss: 0.1261
Training Epoch: 83 [31104/50048]	Loss: 0.1565
Training Epoch: 83 [31232/50048]	Loss: 0.1190
Training Epoch: 83 [31360/50048]	Loss: 0.1101
Training Epoch: 83 [31488/50048]	Loss: 0.0931
Training Epoch: 83 [31616/50048]	Loss: 0.1438
Training Epoch: 83 [31744/50048]	Loss: 0.0924
Training Epoch: 83 [31872/50048]	Loss: 0.1441
Training Epoch: 83 [32000/50048]	Loss: 0.0993
Training Epoch: 83 [32128/50048]	Loss: 0.0844
Training Epoch: 83 [32256/50048]	Loss: 0.0935
Training Epoch: 83 [32384/50048]	Loss: 0.0699
Training Epoch: 83 [32512/50048]	Loss: 0.1252
Training Epoch: 83 [32640/50048]	Loss: 0.0967
Training Epoch: 83 [32768/50048]	Loss: 0.1348
Training Epoch: 83 [32896/50048]	Loss: 0.1081
Training Epoch: 83 [33024/50048]	Loss: 0.1145
Training Epoch: 83 [33152/50048]	Loss: 0.1821
Training Epoch: 83 [33280/50048]	Loss: 0.1719
Training Epoch: 83 [33408/50048]	Loss: 0.0628
Training Epoch: 83 [33536/50048]	Loss: 0.1277
Training Epoch: 83 [33664/50048]	Loss: 0.0989
Training Epoch: 83 [33792/50048]	Loss: 0.0740
Training Epoch: 83 [33920/50048]	Loss: 0.1173
Training Epoch: 83 [34048/50048]	Loss: 0.0588
Training Epoch: 83 [34176/50048]	Loss: 0.0805
Training Epoch: 83 [34304/50048]	Loss: 0.1696
Training Epoch: 83 [34432/50048]	Loss: 0.0961
Training Epoch: 83 [34560/50048]	Loss: 0.1053
Training Epoch: 83 [34688/50048]	Loss: 0.0715
Training Epoch: 83 [34816/50048]	Loss: 0.1494
Training Epoch: 83 [34944/50048]	Loss: 0.1654
Training Epoch: 83 [35072/50048]	Loss: 0.1002
Training Epoch: 83 [35200/50048]	Loss: 0.0961
Training Epoch: 83 [35328/50048]	Loss: 0.1217
Training Epoch: 83 [35456/50048]	Loss: 0.0838
Training Epoch: 83 [35584/50048]	Loss: 0.2116
Training Epoch: 83 [35712/50048]	Loss: 0.0815
Training Epoch: 83 [35840/50048]	Loss: 0.0906
Training Epoch: 83 [35968/50048]	Loss: 0.0665
Training Epoch: 83 [36096/50048]	Loss: 0.0545
Training Epoch: 83 [36224/50048]	Loss: 0.1830
Training Epoch: 83 [36352/50048]	Loss: 0.1332
Training Epoch: 83 [36480/50048]	Loss: 0.0913
Training Epoch: 83 [36608/50048]	Loss: 0.1092
Training Epoch: 83 [36736/50048]	Loss: 0.0824
Training Epoch: 83 [36864/50048]	Loss: 0.0352
Training Epoch: 83 [36992/50048]	Loss: 0.1118
Training Epoch: 83 [37120/50048]	Loss: 0.0536
Training Epoch: 83 [37248/50048]	Loss: 0.0815
Training Epoch: 83 [37376/50048]	Loss: 0.1431
Training Epoch: 83 [37504/50048]	Loss: 0.1057
Training Epoch: 83 [37632/50048]	Loss: 0.1072
Training Epoch: 83 [37760/50048]	Loss: 0.1289
Training Epoch: 83 [37888/50048]	Loss: 0.0773
Training Epoch: 83 [38016/50048]	Loss: 0.1420
Training Epoch: 83 [38144/50048]	Loss: 0.1271
Training Epoch: 83 [38272/50048]	Loss: 0.0908
Training Epoch: 83 [38400/50048]	Loss: 0.0394
Training Epoch: 83 [38528/50048]	Loss: 0.1015
Training Epoch: 83 [38656/50048]	Loss: 0.1405
Training Epoch: 83 [38784/50048]	Loss: 0.1754
Training Epoch: 83 [38912/50048]	Loss: 0.0264
Training Epoch: 83 [39040/50048]	Loss: 0.1933
Training Epoch: 83 [39168/50048]	Loss: 0.0466
Training Epoch: 83 [39296/50048]	Loss: 0.1169
Training Epoch: 83 [39424/50048]	Loss: 0.1024
Training Epoch: 83 [39552/50048]	Loss: 0.0807
Training Epoch: 83 [39680/50048]	Loss: 0.0628
Training Epoch: 83 [39808/50048]	Loss: 0.1294
Training Epoch: 83 [39936/50048]	Loss: 0.1353
Training Epoch: 83 [40064/50048]	Loss: 0.1286
Training Epoch: 83 [40192/50048]	Loss: 0.1404
Training Epoch: 83 [40320/50048]	Loss: 0.1900
Training Epoch: 83 [40448/50048]	Loss: 0.1395
Training Epoch: 83 [40576/50048]	Loss: 0.0834
Training Epoch: 83 [40704/50048]	Loss: 0.1261
Training Epoch: 83 [40832/50048]	Loss: 0.1448
Training Epoch: 83 [40960/50048]	Loss: 0.1281
Training Epoch: 83 [41088/50048]	Loss: 0.1199
Training Epoch: 83 [41216/50048]	Loss: 0.0979
Training Epoch: 83 [41344/50048]	Loss: 0.0832
Training Epoch: 83 [41472/50048]	Loss: 0.0819
Training Epoch: 83 [41600/50048]	Loss: 0.0641
Training Epoch: 83 [41728/50048]	Loss: 0.0744
Training Epoch: 83 [41856/50048]	Loss: 0.0922
Training Epoch: 83 [41984/50048]	Loss: 0.0898
Training Epoch: 83 [42112/50048]	Loss: 0.0835
Training Epoch: 83 [42240/50048]	Loss: 0.0960
Training Epoch: 83 [42368/50048]	Loss: 0.1018
Training Epoch: 83 [42496/50048]	Loss: 0.0965
Training Epoch: 83 [42624/50048]	Loss: 0.1416
Training Epoch: 83 [42752/50048]	Loss: 0.0865
Training Epoch: 83 [42880/50048]	Loss: 0.1114
Training Epoch: 83 [43008/50048]	Loss: 0.0493
Training Epoch: 83 [43136/50048]	Loss: 0.1831
Training Epoch: 83 [43264/50048]	Loss: 0.0797
Training Epoch: 83 [43392/50048]	Loss: 0.0855
Training Epoch: 83 [43520/50048]	Loss: 0.0776
Training Epoch: 83 [43648/50048]	Loss: 0.1182
Training Epoch: 83 [43776/50048]	Loss: 0.1743
Training Epoch: 83 [43904/50048]	Loss: 0.1117
Training Epoch: 83 [44032/50048]	Loss: 0.0963
Training Epoch: 83 [44160/50048]	Loss: 0.1263
Training Epoch: 83 [44288/50048]	Loss: 0.0777
Training Epoch: 83 [44416/50048]	Loss: 0.1003
Training Epoch: 83 [44544/50048]	Loss: 0.1391
Training Epoch: 83 [44672/50048]	Loss: 0.0704
Training Epoch: 83 [44800/50048]	Loss: 0.1759
Training Epoch: 83 [44928/50048]	Loss: 0.0977
Training Epoch: 83 [45056/50048]	Loss: 0.0357
Training Epoch: 83 [45184/50048]	Loss: 0.1342
Training Epoch: 83 [45312/50048]	Loss: 0.0578
Training Epoch: 83 [45440/50048]	Loss: 0.1604
Training Epoch: 83 [45568/50048]	Loss: 0.1052
Training Epoch: 83 [45696/50048]	Loss: 0.0628
2022-12-06 05:43:39,002 [ZeusDataLoader(train)] train epoch 84 done: time=86.54 energy=10508.32
2022-12-06 05:43:39,003 [ZeusDataLoader(eval)] Epoch 84 begin.
Training Epoch: 83 [45824/50048]	Loss: 0.1442
Training Epoch: 83 [45952/50048]	Loss: 0.0620
Training Epoch: 83 [46080/50048]	Loss: 0.0770
Training Epoch: 83 [46208/50048]	Loss: 0.1143
Training Epoch: 83 [46336/50048]	Loss: 0.0893
Training Epoch: 83 [46464/50048]	Loss: 0.0941
Training Epoch: 83 [46592/50048]	Loss: 0.0863
Training Epoch: 83 [46720/50048]	Loss: 0.0885
Training Epoch: 83 [46848/50048]	Loss: 0.0987
Training Epoch: 83 [46976/50048]	Loss: 0.0682
Training Epoch: 83 [47104/50048]	Loss: 0.0903
Training Epoch: 83 [47232/50048]	Loss: 0.1994
Training Epoch: 83 [47360/50048]	Loss: 0.0293
Training Epoch: 83 [47488/50048]	Loss: 0.1603
Training Epoch: 83 [47616/50048]	Loss: 0.0958
Training Epoch: 83 [47744/50048]	Loss: 0.0991
Training Epoch: 83 [47872/50048]	Loss: 0.1628
Training Epoch: 83 [48000/50048]	Loss: 0.1468
Training Epoch: 83 [48128/50048]	Loss: 0.1468
Training Epoch: 83 [48256/50048]	Loss: 0.1120
Training Epoch: 83 [48384/50048]	Loss: 0.1772
Training Epoch: 83 [48512/50048]	Loss: 0.0665
Training Epoch: 83 [48640/50048]	Loss: 0.0591
Training Epoch: 83 [48768/50048]	Loss: 0.1441
Training Epoch: 83 [48896/50048]	Loss: 0.1691
Training Epoch: 83 [49024/50048]	Loss: 0.0821
Training Epoch: 83 [49152/50048]	Loss: 0.1841
Training Epoch: 83 [49280/50048]	Loss: 0.1226
Training Epoch: 83 [49408/50048]	Loss: 0.0338
Training Epoch: 83 [49536/50048]	Loss: 0.0492
Training Epoch: 83 [49664/50048]	Loss: 0.1588
Training Epoch: 83 [49792/50048]	Loss: 0.1527
Training Epoch: 83 [49920/50048]	Loss: 0.1466
Training Epoch: 83 [50048/50048]	Loss: 0.1713
2022-12-06 10:43:42.665 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:43:42,683 [ZeusDataLoader(eval)] eval epoch 84 done: time=3.67 energy=442.76
2022-12-06 05:43:42,683 [ZeusDataLoader(train)] Up to epoch 84: time=7578.58, energy=919987.73, cost=1123119.19
2022-12-06 05:43:42,683 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:43:42,683 [ZeusDataLoader(train)] Expected next epoch: time=7668.37, energy=930785.74, cost=1136375.57
2022-12-06 05:43:42,684 [ZeusDataLoader(train)] Epoch 85 begin.
Validation Epoch: 83, Average loss: 0.0180, Accuracy: 0.6445
2022-12-06 05:43:42,859 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:43:42,860 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:43:42.862 [ZeusMonitor] Monitor started.
2022-12-06 10:43:42.862 [ZeusMonitor] Running indefinitely. 2022-12-06 10:43:42.862 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:43:42.862 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e85+gpu0.power.log
Training Epoch: 84 [128/50048]	Loss: 0.0938
Training Epoch: 84 [256/50048]	Loss: 0.0543
Training Epoch: 84 [384/50048]	Loss: 0.1225
Training Epoch: 84 [512/50048]	Loss: 0.0669
Training Epoch: 84 [640/50048]	Loss: 0.1124
Training Epoch: 84 [768/50048]	Loss: 0.0669
Training Epoch: 84 [896/50048]	Loss: 0.1406
Training Epoch: 84 [1024/50048]	Loss: 0.0463
Training Epoch: 84 [1152/50048]	Loss: 0.0522
Training Epoch: 84 [1280/50048]	Loss: 0.1441
Training Epoch: 84 [1408/50048]	Loss: 0.1627
Training Epoch: 84 [1536/50048]	Loss: 0.0697
Training Epoch: 84 [1664/50048]	Loss: 0.0454
Training Epoch: 84 [1792/50048]	Loss: 0.1120
Training Epoch: 84 [1920/50048]	Loss: 0.0749
Training Epoch: 84 [2048/50048]	Loss: 0.1111
Training Epoch: 84 [2176/50048]	Loss: 0.1440
Training Epoch: 84 [2304/50048]	Loss: 0.0821
Training Epoch: 84 [2432/50048]	Loss: 0.0760
Training Epoch: 84 [2560/50048]	Loss: 0.0678
Training Epoch: 84 [2688/50048]	Loss: 0.1388
Training Epoch: 84 [2816/50048]	Loss: 0.1817
Training Epoch: 84 [2944/50048]	Loss: 0.0729
Training Epoch: 84 [3072/50048]	Loss: 0.0600
Training Epoch: 84 [3200/50048]	Loss: 0.0386
Training Epoch: 84 [3328/50048]	Loss: 0.0767
Training Epoch: 84 [3456/50048]	Loss: 0.0668
Training Epoch: 84 [3584/50048]	Loss: 0.0460
Training Epoch: 84 [3712/50048]	Loss: 0.0933
Training Epoch: 84 [3840/50048]	Loss: 0.0804
Training Epoch: 84 [3968/50048]	Loss: 0.1290
Training Epoch: 84 [4096/50048]	Loss: 0.0662
Training Epoch: 84 [4224/50048]	Loss: 0.0822
Training Epoch: 84 [4352/50048]	Loss: 0.0684
Training Epoch: 84 [4480/50048]	Loss: 0.0425
Training Epoch: 84 [4608/50048]	Loss: 0.0486
Training Epoch: 84 [4736/50048]	Loss: 0.1037
Training Epoch: 84 [4864/50048]	Loss: 0.0803
Training Epoch: 84 [4992/50048]	Loss: 0.0888
Training Epoch: 84 [5120/50048]	Loss: 0.0631
Training Epoch: 84 [5248/50048]	Loss: 0.0618
Training Epoch: 84 [5376/50048]	Loss: 0.0897
Training Epoch: 84 [5504/50048]	Loss: 0.0733
Training Epoch: 84 [5632/50048]	Loss: 0.1260
Training Epoch: 84 [5760/50048]	Loss: 0.1046
Training Epoch: 84 [5888/50048]	Loss: 0.0727
Training Epoch: 84 [6016/50048]	Loss: 0.0937
Training Epoch: 84 [6144/50048]	Loss: 0.1414
Training Epoch: 84 [6272/50048]	Loss: 0.0993
Training Epoch: 84 [6400/50048]	Loss: 0.0632
Training Epoch: 84 [6528/50048]	Loss: 0.0599
Training Epoch: 84 [6656/50048]	Loss: 0.0896
Training Epoch: 84 [6784/50048]	Loss: 0.1282
Training Epoch: 84 [6912/50048]	Loss: 0.1022
Training Epoch: 84 [7040/50048]	Loss: 0.1067
Training Epoch: 84 [7168/50048]	Loss: 0.0822
Training Epoch: 84 [7296/50048]	Loss: 0.1833
Training Epoch: 84 [7424/50048]	Loss: 0.0848
Training Epoch: 84 [7552/50048]	Loss: 0.0852
Training Epoch: 84 [7680/50048]	Loss: 0.1737
Training Epoch: 84 [7808/50048]	Loss: 0.0826
Training Epoch: 84 [7936/50048]	Loss: 0.0916
Training Epoch: 84 [8064/50048]	Loss: 0.0895
Training Epoch: 84 [8192/50048]	Loss: 0.1057
Training Epoch: 84 [8320/50048]	Loss: 0.0902
Training Epoch: 84 [8448/50048]	Loss: 0.1556
Training Epoch: 84 [8576/50048]	Loss: 0.0902
Training Epoch: 84 [8704/50048]	Loss: 0.0952
Training Epoch: 84 [8832/50048]	Loss: 0.0985
Training Epoch: 84 [8960/50048]	Loss: 0.0924
Training Epoch: 84 [9088/50048]	Loss: 0.1013
Training Epoch: 84 [9216/50048]	Loss: 0.0706
Training Epoch: 84 [9344/50048]	Loss: 0.1434
Training Epoch: 84 [9472/50048]	Loss: 0.0609
Training Epoch: 84 [9600/50048]	Loss: 0.1265
Training Epoch: 84 [9728/50048]	Loss: 0.1506
Training Epoch: 84 [9856/50048]	Loss: 0.0922
Training Epoch: 84 [9984/50048]	Loss: 0.1625
Training Epoch: 84 [10112/50048]	Loss: 0.1031
Training Epoch: 84 [10240/50048]	Loss: 0.0941
Training Epoch: 84 [10368/50048]	Loss: 0.0747
Training Epoch: 84 [10496/50048]	Loss: 0.0742
Training Epoch: 84 [10624/50048]	Loss: 0.0805
Training Epoch: 84 [10752/50048]	Loss: 0.0675
Training Epoch: 84 [10880/50048]	Loss: 0.0936
Training Epoch: 84 [11008/50048]	Loss: 0.1105
Training Epoch: 84 [11136/50048]	Loss: 0.0662
Training Epoch: 84 [11264/50048]	Loss: 0.1145
Training Epoch: 84 [11392/50048]	Loss: 0.1018
Training Epoch: 84 [11520/50048]	Loss: 0.0651
Training Epoch: 84 [11648/50048]	Loss: 0.1534
Training Epoch: 84 [11776/50048]	Loss: 0.0616
Training Epoch: 84 [11904/50048]	Loss: 0.0553
Training Epoch: 84 [12032/50048]	Loss: 0.0980
Training Epoch: 84 [12160/50048]	Loss: 0.1165
Training Epoch: 84 [12288/50048]	Loss: 0.1121
Training Epoch: 84 [12416/50048]	Loss: 0.1305
Training Epoch: 84 [12544/50048]	Loss: 0.1175
Training Epoch: 84 [12672/50048]	Loss: 0.0874
Training Epoch: 84 [12800/50048]	Loss: 0.0584
Training Epoch: 84 [12928/50048]	Loss: 0.0709
Training Epoch: 84 [13056/50048]	Loss: 0.0580
Training Epoch: 84 [13184/50048]	Loss: 0.0666
Training Epoch: 84 [13312/50048]	Loss: 0.0486
Training Epoch: 84 [13440/50048]	Loss: 0.0635
Training Epoch: 84 [13568/50048]	Loss: 0.1252
Training Epoch: 84 [13696/50048]	Loss: 0.0714
Training Epoch: 84 [13824/50048]	Loss: 0.1035
Training Epoch: 84 [13952/50048]	Loss: 0.0712
Training Epoch: 84 [14080/50048]	Loss: 0.0647
Training Epoch: 84 [14208/50048]	Loss: 0.0967
Training Epoch: 84 [14336/50048]	Loss: 0.0892
Training Epoch: 84 [14464/50048]	Loss: 0.1025
Training Epoch: 84 [14592/50048]	Loss: 0.1131
Training Epoch: 84 [14720/50048]	Loss: 0.1321
Training Epoch: 84 [14848/50048]	Loss: 0.1023
Training Epoch: 84 [14976/50048]	Loss: 0.0959
Training Epoch: 84 [15104/50048]	Loss: 0.0309
Training Epoch: 84 [15232/50048]	Loss: 0.1300
Training Epoch: 84 [15360/50048]	Loss: 0.0757
Training Epoch: 84 [15488/50048]	Loss: 0.1856
Training Epoch: 84 [15616/50048]	Loss: 0.0884
Training Epoch: 84 [15744/50048]	Loss: 0.0787
Training Epoch: 84 [15872/50048]	Loss: 0.0667
Training Epoch: 84 [16000/50048]	Loss: 0.0669
Training Epoch: 84 [16128/50048]	Loss: 0.1152
Training Epoch: 84 [16256/50048]	Loss: 0.1151
Training Epoch: 84 [16384/50048]	Loss: 0.0797
Training Epoch: 84 [16512/50048]	Loss: 0.0804
Training Epoch: 84 [16640/50048]	Loss: 0.0719
Training Epoch: 84 [16768/50048]	Loss: 0.0960
Training Epoch: 84 [16896/50048]	Loss: 0.0875
Training Epoch: 84 [17024/50048]	Loss: 0.1116
Training Epoch: 84 [17152/50048]	Loss: 0.0756
Training Epoch: 84 [17280/50048]	Loss: 0.0749
Training Epoch: 84 [17408/50048]	Loss: 0.0225
Training Epoch: 84 [17536/50048]	Loss: 0.0900
Training Epoch: 84 [17664/50048]	Loss: 0.0390
Training Epoch: 84 [17792/50048]	Loss: 0.1141
Training Epoch: 84 [17920/50048]	Loss: 0.0948
Training Epoch: 84 [18048/50048]	Loss: 0.1036
Training Epoch: 84 [18176/50048]	Loss: 0.1286
Training Epoch: 84 [18304/50048]	Loss: 0.1156
Training Epoch: 84 [18432/50048]	Loss: 0.1662
Training Epoch: 84 [18560/50048]	Loss: 0.0836
Training Epoch: 84 [18688/50048]	Loss: 0.0696
Training Epoch: 84 [18816/50048]	Loss: 0.0617
Training Epoch: 84 [18944/50048]	Loss: 0.1307
Training Epoch: 84 [19072/50048]	Loss: 0.0778
Training Epoch: 84 [19200/50048]	Loss: 0.0957
Training Epoch: 84 [19328/50048]	Loss: 0.1507
Training Epoch: 84 [19456/50048]	Loss: 0.1521
Training Epoch: 84 [19584/50048]	Loss: 0.1110
Training Epoch: 84 [19712/50048]	Loss: 0.0636
Training Epoch: 84 [19840/50048]	Loss: 0.0939
Training Epoch: 84 [19968/50048]	Loss: 0.1034
Training Epoch: 84 [20096/50048]	Loss: 0.1123
Training Epoch: 84 [20224/50048]	Loss: 0.1260
Training Epoch: 84 [20352/50048]	Loss: 0.0748
Training Epoch: 84 [20480/50048]	Loss: 0.0553
Training Epoch: 84 [20608/50048]	Loss: 0.2046
Training Epoch: 84 [20736/50048]	Loss: 0.1219
Training Epoch: 84 [20864/50048]	Loss: 0.1445
Training Epoch: 84 [20992/50048]	Loss: 0.0859
Training Epoch: 84 [21120/50048]	Loss: 0.0598
Training Epoch: 84 [21248/50048]	Loss: 0.0636
Training Epoch: 84 [21376/50048]	Loss: 0.0866
Training Epoch: 84 [21504/50048]	Loss: 0.0537
Training Epoch: 84 [21632/50048]	Loss: 0.0333
Training Epoch: 84 [21760/50048]	Loss: 0.0656
Training Epoch: 84 [21888/50048]	Loss: 0.1045
Training Epoch: 84 [22016/50048]	Loss: 0.0752
Training Epoch: 84 [22144/50048]	Loss: 0.1198
Training Epoch: 84 [22272/50048]	Loss: 0.0917
Training Epoch: 84 [22400/50048]	Loss: 0.1262
Training Epoch: 84 [22528/50048]	Loss: 0.0363
Training Epoch: 84 [22656/50048]	Loss: 0.1126
Training Epoch: 84 [22784/50048]	Loss: 0.0875
Training Epoch: 84 [22912/50048]	Loss: 0.0429
Training Epoch: 84 [23040/50048]	Loss: 0.0507
Training Epoch: 84 [23168/50048]	Loss: 0.1652
Training Epoch: 84 [23296/50048]	Loss: 0.0660
Training Epoch: 84 [23424/50048]	Loss: 0.0522
Training Epoch: 84 [23552/50048]	Loss: 0.0811
Training Epoch: 84 [23680/50048]	Loss: 0.0707
Training Epoch: 84 [23808/50048]	Loss: 0.0810
Training Epoch: 84 [23936/50048]	Loss: 0.1148
Training Epoch: 84 [24064/50048]	Loss: 0.0735
Training Epoch: 84 [24192/50048]	Loss: 0.0651
Training Epoch: 84 [24320/50048]	Loss: 0.1237
Training Epoch: 84 [24448/50048]	Loss: 0.1280
Training Epoch: 84 [24576/50048]	Loss: 0.0942
Training Epoch: 84 [24704/50048]	Loss: 0.1378
Training Epoch: 84 [24832/50048]	Loss: 0.1061
Training Epoch: 84 [24960/50048]	Loss: 0.1002
Training Epoch: 84 [25088/50048]	Loss: 0.1317
Training Epoch: 84 [25216/50048]	Loss: 0.0793
Training Epoch: 84 [25344/50048]	Loss: 0.0789
Training Epoch: 84 [25472/50048]	Loss: 0.0972
Training Epoch: 84 [25600/50048]	Loss: 0.1185
Training Epoch: 84 [25728/50048]	Loss: 0.1316
Training Epoch: 84 [25856/50048]	Loss: 0.0842
Training Epoch: 84 [25984/50048]	Loss: 0.0693
Training Epoch: 84 [26112/50048]	Loss: 0.1121
Training Epoch: 84 [26240/50048]	Loss: 0.0907
Training Epoch: 84 [26368/50048]	Loss: 0.0591
Training Epoch: 84 [26496/50048]	Loss: 0.1819
Training Epoch: 84 [26624/50048]	Loss: 0.2035
Training Epoch: 84 [26752/50048]	Loss: 0.0519
Training Epoch: 84 [26880/50048]	Loss: 0.0802
Training Epoch: 84 [27008/50048]	Loss: 0.1042
Training Epoch: 84 [27136/50048]	Loss: 0.0678
Training Epoch: 84 [27264/50048]	Loss: 0.0348
Training Epoch: 84 [27392/50048]	Loss: 0.2286
Training Epoch: 84 [27520/50048]	Loss: 0.0503
Training Epoch: 84 [27648/50048]	Loss: 0.1255
Training Epoch: 84 [27776/50048]	Loss: 0.0956
Training Epoch: 84 [27904/50048]	Loss: 0.0681
Training Epoch: 84 [28032/50048]	Loss: 0.1109
Training Epoch: 84 [28160/50048]	Loss: 0.0678
Training Epoch: 84 [28288/50048]	Loss: 0.1495
Training Epoch: 84 [28416/50048]	Loss: 0.1123
Training Epoch: 84 [28544/50048]	Loss: 0.0769
Training Epoch: 84 [28672/50048]	Loss: 0.0899
Training Epoch: 84 [28800/50048]	Loss: 0.1258
Training Epoch: 84 [28928/50048]	Loss: 0.0631
Training Epoch: 84 [29056/50048]	Loss: 0.1074
Training Epoch: 84 [29184/50048]	Loss: 0.1146
Training Epoch: 84 [29312/50048]	Loss: 0.0654
Training Epoch: 84 [29440/50048]	Loss: 0.0475
Training Epoch: 84 [29568/50048]	Loss: 0.1637
Training Epoch: 84 [29696/50048]	Loss: 0.1358
Training Epoch: 84 [29824/50048]	Loss: 0.0953
Training Epoch: 84 [29952/50048]	Loss: 0.1475
Training Epoch: 84 [30080/50048]	Loss: 0.1079
Training Epoch: 84 [30208/50048]	Loss: 0.0712
Training Epoch: 84 [30336/50048]	Loss: 0.0597
Training Epoch: 84 [30464/50048]	Loss: 0.1225
Training Epoch: 84 [30592/50048]	Loss: 0.1021
Training Epoch: 84 [30720/50048]	Loss: 0.1651
Training Epoch: 84 [30848/50048]	Loss: 0.0963
Training Epoch: 84 [30976/50048]	Loss: 0.0722
Training Epoch: 84 [31104/50048]	Loss: 0.1401
Training Epoch: 84 [31232/50048]	Loss: 0.1745
Training Epoch: 84 [31360/50048]	Loss: 0.0855
Training Epoch: 84 [31488/50048]	Loss: 0.1377
Training Epoch: 84 [31616/50048]	Loss: 0.1158
Training Epoch: 84 [31744/50048]	Loss: 0.1214
Training Epoch: 84 [31872/50048]	Loss: 0.0620
Training Epoch: 84 [32000/50048]	Loss: 0.1731
Training Epoch: 84 [32128/50048]	Loss: 0.0962
Training Epoch: 84 [32256/50048]	Loss: 0.1043
Training Epoch: 84 [32384/50048]	Loss: 0.0605
Training Epoch: 84 [32512/50048]	Loss: 0.0591
Training Epoch: 84 [32640/50048]	Loss: 0.0746
Training Epoch: 84 [32768/50048]	Loss: 0.1658
Training Epoch: 84 [32896/50048]	Loss: 0.0416
Training Epoch: 84 [33024/50048]	Loss: 0.0898
Training Epoch: 84 [33152/50048]	Loss: 0.0837
Training Epoch: 84 [33280/50048]	Loss: 0.1224
Training Epoch: 84 [33408/50048]	Loss: 0.0934
Training Epoch: 84 [33536/50048]	Loss: 0.1334
Training Epoch: 84 [33664/50048]	Loss: 0.1709
Training Epoch: 84 [33792/50048]	Loss: 0.1726
Training Epoch: 84 [33920/50048]	Loss: 0.1645
Training Epoch: 84 [34048/50048]	Loss: 0.1599
Training Epoch: 84 [34176/50048]	Loss: 0.1179
Training Epoch: 84 [34304/50048]	Loss: 0.1055
Training Epoch: 84 [34432/50048]	Loss: 0.1600
Training Epoch: 84 [34560/50048]	Loss: 0.0890
Training Epoch: 84 [34688/50048]	Loss: 0.1532
Training Epoch: 84 [34816/50048]	Loss: 0.1198
Training Epoch: 84 [34944/50048]	Loss: 0.0911
Training Epoch: 84 [35072/50048]	Loss: 0.1311
Training Epoch: 84 [35200/50048]	Loss: 0.1112
Training Epoch: 84 [35328/50048]	Loss: 0.0575
Training Epoch: 84 [35456/50048]	Loss: 0.1544
Training Epoch: 84 [35584/50048]	Loss: 0.1223
Training Epoch: 84 [35712/50048]	Loss: 0.1947
Training Epoch: 84 [35840/50048]	Loss: 0.0764
Training Epoch: 84 [35968/50048]	Loss: 0.0843
Training Epoch: 84 [36096/50048]	Loss: 0.0918
Training Epoch: 84 [36224/50048]	Loss: 0.1851
Training Epoch: 84 [36352/50048]	Loss: 0.2025
Training Epoch: 84 [36480/50048]	Loss: 0.0731
Training Epoch: 84 [36608/50048]	Loss: 0.0650
Training Epoch: 84 [36736/50048]	Loss: 0.1536
Training Epoch: 84 [36864/50048]	Loss: 0.0409
Training Epoch: 84 [36992/50048]	Loss: 0.1474
Training Epoch: 84 [37120/50048]	Loss: 0.0651
Training Epoch: 84 [37248/50048]	Loss: 0.1100
Training Epoch: 84 [37376/50048]	Loss: 0.0858
Training Epoch: 84 [37504/50048]	Loss: 0.1630
Training Epoch: 84 [37632/50048]	Loss: 0.0916
Training Epoch: 84 [37760/50048]	Loss: 0.0585
Training Epoch: 84 [37888/50048]	Loss: 0.1865
Training Epoch: 84 [38016/50048]	Loss: 0.0797
Training Epoch: 84 [38144/50048]	Loss: 0.1680
Training Epoch: 84 [38272/50048]	Loss: 0.0678
Training Epoch: 84 [38400/50048]	Loss: 0.0818
Training Epoch: 84 [38528/50048]	Loss: 0.0537
Training Epoch: 84 [38656/50048]	Loss: 0.0778
Training Epoch: 84 [38784/50048]	Loss: 0.0995
Training Epoch: 84 [38912/50048]	Loss: 0.1528
Training Epoch: 84 [39040/50048]	Loss: 0.1267
Training Epoch: 84 [39168/50048]	Loss: 0.0513
Training Epoch: 84 [39296/50048]	Loss: 0.1171
Training Epoch: 84 [39424/50048]	Loss: 0.0380
Training Epoch: 84 [39552/50048]	Loss: 0.1014
Training Epoch: 84 [39680/50048]	Loss: 0.1268
Training Epoch: 84 [39808/50048]	Loss: 0.1279
Training Epoch: 84 [39936/50048]	Loss: 0.1159
Training Epoch: 84 [40064/50048]	Loss: 0.0811
Training Epoch: 84 [40192/50048]	Loss: 0.0569
Training Epoch: 84 [40320/50048]	Loss: 0.0445
Training Epoch: 84 [40448/50048]	Loss: 0.1649
Training Epoch: 84 [40576/50048]	Loss: 0.0914
Training Epoch: 84 [40704/50048]	Loss: 0.0714
Training Epoch: 84 [40832/50048]	Loss: 0.0918
Training Epoch: 84 [40960/50048]	Loss: 0.1415
Training Epoch: 84 [41088/50048]	Loss: 0.1072
Training Epoch: 84 [41216/50048]	Loss: 0.1111
Training Epoch: 84 [41344/50048]	Loss: 0.0854
Training Epoch: 84 [41472/50048]	Loss: 0.1362
Training Epoch: 84 [41600/50048]	Loss: 0.0664
Training Epoch: 84 [41728/50048]	Loss: 0.1155
Training Epoch: 84 [41856/50048]	Loss: 0.0789
Training Epoch: 84 [41984/50048]	Loss: 0.0888
Training Epoch: 84 [42112/50048]	Loss: 0.1216
Training Epoch: 84 [42240/50048]	Loss: 0.1190
Training Epoch: 84 [42368/50048]	Loss: 0.1113
Training Epoch: 84 [42496/50048]	Loss: 0.1465
Training Epoch: 84 [42624/50048]	Loss: 0.0623
Training Epoch: 84 [42752/50048]	Loss: 0.0718
Training Epoch: 84 [42880/50048]	Loss: 0.1425
Training Epoch: 84 [43008/50048]	Loss: 0.1410
Training Epoch: 84 [43136/50048]	Loss: 0.1656
Training Epoch: 84 [43264/50048]	Loss: 0.1067
Training Epoch: 84 [43392/50048]	Loss: 0.0886
Training Epoch: 84 [43520/50048]	Loss: 0.1560
Training Epoch: 84 [43648/50048]	Loss: 0.0519
Training Epoch: 84 [43776/50048]	Loss: 0.1677
Training Epoch: 84 [43904/50048]	Loss: 0.0608
Training Epoch: 84 [44032/50048]	Loss: 0.1526
Training Epoch: 84 [44160/50048]	Loss: 0.0662
Training Epoch: 84 [44288/50048]	Loss: 0.0404
Training Epoch: 84 [44416/50048]	Loss: 0.1687
Training Epoch: 84 [44544/50048]	Loss: 0.0926
Training Epoch: 84 [44672/50048]	Loss: 0.0477
Training Epoch: 84 [44800/50048]	Loss: 0.1494
Training Epoch: 84 [44928/50048]	Loss: 0.0832
Training Epoch: 84 [45056/50048]	Loss: 0.0529
Training Epoch: 84 [45184/50048]	Loss: 0.1048
Training Epoch: 84 [45312/50048]	Loss: 0.0819
Training Epoch: 84 [45440/50048]	Loss: 0.1572
Training Epoch: 84 [45568/50048]	Loss: 0.0733
Training Epoch: 84 [45696/50048]	Loss: 0.1148
2022-12-06 05:45:09,159 [ZeusDataLoader(train)] train epoch 85 done: time=86.46 energy=10501.98
2022-12-06 05:45:09,160 [ZeusDataLoader(eval)] Epoch 85 begin.
Training Epoch: 84 [45824/50048]	Loss: 0.1237
Training Epoch: 84 [45952/50048]	Loss: 0.1965
Training Epoch: 84 [46080/50048]	Loss: 0.1016
Training Epoch: 84 [46208/50048]	Loss: 0.0929
Training Epoch: 84 [46336/50048]	Loss: 0.1861
Training Epoch: 84 [46464/50048]	Loss: 0.1020
Training Epoch: 84 [46592/50048]	Loss: 0.2083
Training Epoch: 84 [46720/50048]	Loss: 0.0561
Training Epoch: 84 [46848/50048]	Loss: 0.0604
Training Epoch: 84 [46976/50048]	Loss: 0.1838
Training Epoch: 84 [47104/50048]	Loss: 0.0983
Training Epoch: 84 [47232/50048]	Loss: 0.0866
Training Epoch: 84 [47360/50048]	Loss: 0.1750
Training Epoch: 84 [47488/50048]	Loss: 0.0999
Training Epoch: 84 [47616/50048]	Loss: 0.1235
Training Epoch: 84 [47744/50048]	Loss: 0.1689
Training Epoch: 84 [47872/50048]	Loss: 0.1268
Training Epoch: 84 [48000/50048]	Loss: 0.0797
Training Epoch: 84 [48128/50048]	Loss: 0.1480
Training Epoch: 84 [48256/50048]	Loss: 0.1773
Training Epoch: 84 [48384/50048]	Loss: 0.0887
Training Epoch: 84 [48512/50048]	Loss: 0.1958
Training Epoch: 84 [48640/50048]	Loss: 0.1206
Training Epoch: 84 [48768/50048]	Loss: 0.0930
Training Epoch: 84 [48896/50048]	Loss: 0.1909
Training Epoch: 84 [49024/50048]	Loss: 0.1402
Training Epoch: 84 [49152/50048]	Loss: 0.0905
Training Epoch: 84 [49280/50048]	Loss: 0.1529
Training Epoch: 84 [49408/50048]	Loss: 0.0699
Training Epoch: 84 [49536/50048]	Loss: 0.1809
Training Epoch: 84 [49664/50048]	Loss: 0.0850
Training Epoch: 84 [49792/50048]	Loss: 0.1640
Training Epoch: 84 [49920/50048]	Loss: 0.1605
Training Epoch: 84 [50048/50048]	Loss: 0.0757
2022-12-06 10:45:12.826 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:45:12,850 [ZeusDataLoader(eval)] eval epoch 85 done: time=3.68 energy=444.06
2022-12-06 05:45:12,850 [ZeusDataLoader(train)] Up to epoch 85: time=7668.72, energy=930933.76, cost=1136479.90
2022-12-06 05:45:12,850 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:45:12,850 [ZeusDataLoader(train)] Expected next epoch: time=7758.52, energy=941731.78, cost=1149736.28
2022-12-06 05:45:12,851 [ZeusDataLoader(train)] Epoch 86 begin.
Validation Epoch: 84, Average loss: 0.0180, Accuracy: 0.6450
2022-12-06 05:45:13,032 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:45:13,033 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:45:13.035 [ZeusMonitor] Monitor started.
2022-12-06 10:45:13.035 [ZeusMonitor] Running indefinitely. 2022-12-06 10:45:13.035 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:45:13.035 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e86+gpu0.power.log
Training Epoch: 85 [128/50048]	Loss: 0.1011
Training Epoch: 85 [256/50048]	Loss: 0.0701
Training Epoch: 85 [384/50048]	Loss: 0.0822
Training Epoch: 85 [512/50048]	Loss: 0.0894
Training Epoch: 85 [640/50048]	Loss: 0.0870
Training Epoch: 85 [768/50048]	Loss: 0.0850
Training Epoch: 85 [896/50048]	Loss: 0.0486
Training Epoch: 85 [1024/50048]	Loss: 0.1495
Training Epoch: 85 [1152/50048]	Loss: 0.0783
Training Epoch: 85 [1280/50048]	Loss: 0.0721
Training Epoch: 85 [1408/50048]	Loss: 0.0968
Training Epoch: 85 [1536/50048]	Loss: 0.1191
Training Epoch: 85 [1664/50048]	Loss: 0.0837
Training Epoch: 85 [1792/50048]	Loss: 0.0561
Training Epoch: 85 [1920/50048]	Loss: 0.1375
Training Epoch: 85 [2048/50048]	Loss: 0.1432
Training Epoch: 85 [2176/50048]	Loss: 0.0683
Training Epoch: 85 [2304/50048]	Loss: 0.0804
Training Epoch: 85 [2432/50048]	Loss: 0.0842
Training Epoch: 85 [2560/50048]	Loss: 0.0865
Training Epoch: 85 [2688/50048]	Loss: 0.1149
Training Epoch: 85 [2816/50048]	Loss: 0.0582
Training Epoch: 85 [2944/50048]	Loss: 0.0862
Training Epoch: 85 [3072/50048]	Loss: 0.0563
Training Epoch: 85 [3200/50048]	Loss: 0.1103
Training Epoch: 85 [3328/50048]	Loss: 0.0807
Training Epoch: 85 [3456/50048]	Loss: 0.0630
Training Epoch: 85 [3584/50048]	Loss: 0.0938
Training Epoch: 85 [3712/50048]	Loss: 0.1289
Training Epoch: 85 [3840/50048]	Loss: 0.1404
Training Epoch: 85 [3968/50048]	Loss: 0.0844
Training Epoch: 85 [4096/50048]	Loss: 0.1365
Training Epoch: 85 [4224/50048]	Loss: 0.0793
Training Epoch: 85 [4352/50048]	Loss: 0.1091
Training Epoch: 85 [4480/50048]	Loss: 0.1027
Training Epoch: 85 [4608/50048]	Loss: 0.1043
Training Epoch: 85 [4736/50048]	Loss: 0.0865
Training Epoch: 85 [4864/50048]	Loss: 0.0899
Training Epoch: 85 [4992/50048]	Loss: 0.1068
Training Epoch: 85 [5120/50048]	Loss: 0.1430
Training Epoch: 85 [5248/50048]	Loss: 0.1423
Training Epoch: 85 [5376/50048]	Loss: 0.0452
Training Epoch: 85 [5504/50048]	Loss: 0.1252
Training Epoch: 85 [5632/50048]	Loss: 0.0813
Training Epoch: 85 [5760/50048]	Loss: 0.1015
Training Epoch: 85 [5888/50048]	Loss: 0.1961
Training Epoch: 85 [6016/50048]	Loss: 0.0740
Training Epoch: 85 [6144/50048]	Loss: 0.0530
Training Epoch: 85 [6272/50048]	Loss: 0.1094
Training Epoch: 85 [6400/50048]	Loss: 0.1743
Training Epoch: 85 [6528/50048]	Loss: 0.0892
Training Epoch: 85 [6656/50048]	Loss: 0.1331
Training Epoch: 85 [6784/50048]	Loss: 0.0843
Training Epoch: 85 [6912/50048]	Loss: 0.0909
Training Epoch: 85 [7040/50048]	Loss: 0.1574
Training Epoch: 85 [7168/50048]	Loss: 0.0781
Training Epoch: 85 [7296/50048]	Loss: 0.1340
Training Epoch: 85 [7424/50048]	Loss: 0.1195
Training Epoch: 85 [7552/50048]	Loss: 0.1414
Training Epoch: 85 [7680/50048]	Loss: 0.0865
Training Epoch: 85 [7808/50048]	Loss: 0.0652
Training Epoch: 85 [7936/50048]	Loss: 0.0954
Training Epoch: 85 [8064/50048]	Loss: 0.0586
Training Epoch: 85 [8192/50048]	Loss: 0.1453
Training Epoch: 85 [8320/50048]	Loss: 0.0723
Training Epoch: 85 [8448/50048]	Loss: 0.0685
Training Epoch: 85 [8576/50048]	Loss: 0.0648
Training Epoch: 85 [8704/50048]	Loss: 0.1182
Training Epoch: 85 [8832/50048]	Loss: 0.1179
Training Epoch: 85 [8960/50048]	Loss: 0.1193
Training Epoch: 85 [9088/50048]	Loss: 0.1484
Training Epoch: 85 [9216/50048]	Loss: 0.1082
Training Epoch: 85 [9344/50048]	Loss: 0.0729
Training Epoch: 85 [9472/50048]	Loss: 0.1584
Training Epoch: 85 [9600/50048]	Loss: 0.2029
Training Epoch: 85 [9728/50048]	Loss: 0.0925
Training Epoch: 85 [9856/50048]	Loss: 0.1131
Training Epoch: 85 [9984/50048]	Loss: 0.0906
Training Epoch: 85 [10112/50048]	Loss: 0.0939
Training Epoch: 85 [10240/50048]	Loss: 0.0542
Training Epoch: 85 [10368/50048]	Loss: 0.1086
Training Epoch: 85 [10496/50048]	Loss: 0.1036
Training Epoch: 85 [10624/50048]	Loss: 0.0597
Training Epoch: 85 [10752/50048]	Loss: 0.1017
Training Epoch: 85 [10880/50048]	Loss: 0.0824
Training Epoch: 85 [11008/50048]	Loss: 0.1612
Training Epoch: 85 [11136/50048]	Loss: 0.1368
Training Epoch: 85 [11264/50048]	Loss: 0.0698
Training Epoch: 85 [11392/50048]	Loss: 0.0784
Training Epoch: 85 [11520/50048]	Loss: 0.0437
Training Epoch: 85 [11648/50048]	Loss: 0.0929
Training Epoch: 85 [11776/50048]	Loss: 0.0465
Training Epoch: 85 [11904/50048]	Loss: 0.0659
Training Epoch: 85 [12032/50048]	Loss: 0.0684
Training Epoch: 85 [12160/50048]	Loss: 0.1531
Training Epoch: 85 [12288/50048]	Loss: 0.0589
Training Epoch: 85 [12416/50048]	Loss: 0.1415
Training Epoch: 85 [12544/50048]	Loss: 0.1086
Training Epoch: 85 [12672/50048]	Loss: 0.0665
Training Epoch: 85 [12800/50048]	Loss: 0.0876
Training Epoch: 85 [12928/50048]	Loss: 0.0866
Training Epoch: 85 [13056/50048]	Loss: 0.1488
Training Epoch: 85 [13184/50048]	Loss: 0.0764
Training Epoch: 85 [13312/50048]	Loss: 0.1152
Training Epoch: 85 [13440/50048]	Loss: 0.0653
Training Epoch: 85 [13568/50048]	Loss: 0.0756
Training Epoch: 85 [13696/50048]	Loss: 0.0792
Training Epoch: 85 [13824/50048]	Loss: 0.0720
Training Epoch: 85 [13952/50048]	Loss: 0.0939
Training Epoch: 85 [14080/50048]	Loss: 0.0835
Training Epoch: 85 [14208/50048]	Loss: 0.0915
Training Epoch: 85 [14336/50048]	Loss: 0.0895
Training Epoch: 85 [14464/50048]	Loss: 0.0727
Training Epoch: 85 [14592/50048]	Loss: 0.0715
Training Epoch: 85 [14720/50048]	Loss: 0.1627
Training Epoch: 85 [14848/50048]	Loss: 0.0602
Training Epoch: 85 [14976/50048]	Loss: 0.0474
Training Epoch: 85 [15104/50048]	Loss: 0.1346
Training Epoch: 85 [15232/50048]	Loss: 0.1539
Training Epoch: 85 [15360/50048]	Loss: 0.1734
Training Epoch: 85 [15488/50048]	Loss: 0.0760
Training Epoch: 85 [15616/50048]	Loss: 0.0996
Training Epoch: 85 [15744/50048]	Loss: 0.0406
Training Epoch: 85 [15872/50048]	Loss: 0.0835
Training Epoch: 85 [16000/50048]	Loss: 0.0532
Training Epoch: 85 [16128/50048]	Loss: 0.0945
Training Epoch: 85 [16256/50048]	Loss: 0.1294
Training Epoch: 85 [16384/50048]	Loss: 0.1264
Training Epoch: 85 [16512/50048]	Loss: 0.1954
Training Epoch: 85 [16640/50048]	Loss: 0.1087
Training Epoch: 85 [16768/50048]	Loss: 0.1087
Training Epoch: 85 [16896/50048]	Loss: 0.1239
Training Epoch: 85 [17024/50048]	Loss: 0.1406
Training Epoch: 85 [17152/50048]	Loss: 0.1034
Training Epoch: 85 [17280/50048]	Loss: 0.0782
Training Epoch: 85 [17408/50048]	Loss: 0.1023
Training Epoch: 85 [17536/50048]	Loss: 0.1415
Training Epoch: 85 [17664/50048]	Loss: 0.0556
Training Epoch: 85 [17792/50048]	Loss: 0.0533
Training Epoch: 85 [17920/50048]	Loss: 0.0721
Training Epoch: 85 [18048/50048]	Loss: 0.0562
Training Epoch: 85 [18176/50048]	Loss: 0.0948
Training Epoch: 85 [18304/50048]	Loss: 0.1175
Training Epoch: 85 [18432/50048]	Loss: 0.0885
Training Epoch: 85 [18560/50048]	Loss: 0.1104
Training Epoch: 85 [18688/50048]	Loss: 0.0809
Training Epoch: 85 [18816/50048]	Loss: 0.1514
Training Epoch: 85 [18944/50048]	Loss: 0.0948
Training Epoch: 85 [19072/50048]	Loss: 0.0902
Training Epoch: 85 [19200/50048]	Loss: 0.0966
Training Epoch: 85 [19328/50048]	Loss: 0.0796
Training Epoch: 85 [19456/50048]	Loss: 0.0752
Training Epoch: 85 [19584/50048]	Loss: 0.1165
Training Epoch: 85 [19712/50048]	Loss: 0.0589
Training Epoch: 85 [19840/50048]	Loss: 0.1059
Training Epoch: 85 [19968/50048]	Loss: 0.1175
Training Epoch: 85 [20096/50048]	Loss: 0.1556
Training Epoch: 85 [20224/50048]	Loss: 0.1307
Training Epoch: 85 [20352/50048]	Loss: 0.0860
Training Epoch: 85 [20480/50048]	Loss: 0.0860
Training Epoch: 85 [20608/50048]	Loss: 0.1142
Training Epoch: 85 [20736/50048]	Loss: 0.1823
Training Epoch: 85 [20864/50048]	Loss: 0.1427
Training Epoch: 85 [20992/50048]	Loss: 0.0990
Training Epoch: 85 [21120/50048]	Loss: 0.1100
Training Epoch: 85 [21248/50048]	Loss: 0.1395
Training Epoch: 85 [21376/50048]	Loss: 0.1150
Training Epoch: 85 [21504/50048]	Loss: 0.0805
Training Epoch: 85 [21632/50048]	Loss: 0.0735
Training Epoch: 85 [21760/50048]	Loss: 0.0551
Training Epoch: 85 [21888/50048]	Loss: 0.1379
Training Epoch: 85 [22016/50048]	Loss: 0.1066
Training Epoch: 85 [22144/50048]	Loss: 0.0288
Training Epoch: 85 [22272/50048]	Loss: 0.0880
Training Epoch: 85 [22400/50048]	Loss: 0.0721
Training Epoch: 85 [22528/50048]	Loss: 0.0982
Training Epoch: 85 [22656/50048]	Loss: 0.0391
Training Epoch: 85 [22784/50048]	Loss: 0.0706
Training Epoch: 85 [22912/50048]	Loss: 0.1347
Training Epoch: 85 [23040/50048]	Loss: 0.1092
Training Epoch: 85 [23168/50048]	Loss: 0.1459
Training Epoch: 85 [23296/50048]	Loss: 0.0911
Training Epoch: 85 [23424/50048]	Loss: 0.0557
Training Epoch: 85 [23552/50048]	Loss: 0.0748
Training Epoch: 85 [23680/50048]	Loss: 0.0846
Training Epoch: 85 [23808/50048]	Loss: 0.0440
Training Epoch: 85 [23936/50048]	Loss: 0.0801
Training Epoch: 85 [24064/50048]	Loss: 0.1720
Training Epoch: 85 [24192/50048]	Loss: 0.0596
Training Epoch: 85 [24320/50048]	Loss: 0.0646
Training Epoch: 85 [24448/50048]	Loss: 0.0804
Training Epoch: 85 [24576/50048]	Loss: 0.1171
Training Epoch: 85 [24704/50048]	Loss: 0.1457
Training Epoch: 85 [24832/50048]	Loss: 0.0879
Training Epoch: 85 [24960/50048]	Loss: 0.0522
Training Epoch: 85 [25088/50048]	Loss: 0.1028
Training Epoch: 85 [25216/50048]	Loss: 0.0978
Training Epoch: 85 [25344/50048]	Loss: 0.0644
Training Epoch: 85 [25472/50048]	Loss: 0.0789
Training Epoch: 85 [25600/50048]	Loss: 0.1175
Training Epoch: 85 [25728/50048]	Loss: 0.1157
Training Epoch: 85 [25856/50048]	Loss: 0.1207
Training Epoch: 85 [25984/50048]	Loss: 0.0945
Training Epoch: 85 [26112/50048]	Loss: 0.1271
Training Epoch: 85 [26240/50048]	Loss: 0.1188
Training Epoch: 85 [26368/50048]	Loss: 0.0508
Training Epoch: 85 [26496/50048]	Loss: 0.1239
Training Epoch: 85 [26624/50048]	Loss: 0.1000
Training Epoch: 85 [26752/50048]	Loss: 0.0672
Training Epoch: 85 [26880/50048]	Loss: 0.0791
Training Epoch: 85 [27008/50048]	Loss: 0.1310
Training Epoch: 85 [27136/50048]	Loss: 0.0417
Training Epoch: 85 [27264/50048]	Loss: 0.0582
Training Epoch: 85 [27392/50048]	Loss: 0.0878
Training Epoch: 85 [27520/50048]	Loss: 0.0991
Training Epoch: 85 [27648/50048]	Loss: 0.0845
Training Epoch: 85 [27776/50048]	Loss: 0.1193
Training Epoch: 85 [27904/50048]	Loss: 0.0908
Training Epoch: 85 [28032/50048]	Loss: 0.0818
Training Epoch: 85 [28160/50048]	Loss: 0.0802
Training Epoch: 85 [28288/50048]	Loss: 0.0736
Training Epoch: 85 [28416/50048]	Loss: 0.0952
Training Epoch: 85 [28544/50048]	Loss: 0.1912
Training Epoch: 85 [28672/50048]	Loss: 0.0268
Training Epoch: 85 [28800/50048]	Loss: 0.0670
Training Epoch: 85 [28928/50048]	Loss: 0.1232
Training Epoch: 85 [29056/50048]	Loss: 0.0930
Training Epoch: 85 [29184/50048]	Loss: 0.1716
Training Epoch: 85 [29312/50048]	Loss: 0.1018
Training Epoch: 85 [29440/50048]	Loss: 0.0997
Training Epoch: 85 [29568/50048]	Loss: 0.1177
Training Epoch: 85 [29696/50048]	Loss: 0.0684
Training Epoch: 85 [29824/50048]	Loss: 0.0749
Training Epoch: 85 [29952/50048]	Loss: 0.1310
Training Epoch: 85 [30080/50048]	Loss: 0.0591
Training Epoch: 85 [30208/50048]	Loss: 0.1306
Training Epoch: 85 [30336/50048]	Loss: 0.1058
Training Epoch: 85 [30464/50048]	Loss: 0.0492
Training Epoch: 85 [30592/50048]	Loss: 0.0634
Training Epoch: 85 [30720/50048]	Loss: 0.1902
Training Epoch: 85 [30848/50048]	Loss: 0.0562
Training Epoch: 85 [30976/50048]	Loss: 0.1010
Training Epoch: 85 [31104/50048]	Loss: 0.0518
Training Epoch: 85 [31232/50048]	Loss: 0.0791
Training Epoch: 85 [31360/50048]	Loss: 0.1213
Training Epoch: 85 [31488/50048]	Loss: 0.0993
Training Epoch: 85 [31616/50048]	Loss: 0.0955
Training Epoch: 85 [31744/50048]	Loss: 0.1381
Training Epoch: 85 [31872/50048]	Loss: 0.1395
Training Epoch: 85 [32000/50048]	Loss: 0.0896
Training Epoch: 85 [32128/50048]	Loss: 0.1346
Training Epoch: 85 [32256/50048]	Loss: 0.1685
Training Epoch: 85 [32384/50048]	Loss: 0.0989
Training Epoch: 85 [32512/50048]	Loss: 0.1539
Training Epoch: 85 [32640/50048]	Loss: 0.0819
Training Epoch: 85 [32768/50048]	Loss: 0.0700
Training Epoch: 85 [32896/50048]	Loss: 0.0376
Training Epoch: 85 [33024/50048]	Loss: 0.0508
Training Epoch: 85 [33152/50048]	Loss: 0.1164
Training Epoch: 85 [33280/50048]	Loss: 0.0531
Training Epoch: 85 [33408/50048]	Loss: 0.1402
Training Epoch: 85 [33536/50048]	Loss: 0.0634
Training Epoch: 85 [33664/50048]	Loss: 0.1093
Training Epoch: 85 [33792/50048]	Loss: 0.0524
Training Epoch: 85 [33920/50048]	Loss: 0.0469
Training Epoch: 85 [34048/50048]	Loss: 0.0675
Training Epoch: 85 [34176/50048]	Loss: 0.1456
Training Epoch: 85 [34304/50048]	Loss: 0.1516
Training Epoch: 85 [34432/50048]	Loss: 0.0872
Training Epoch: 85 [34560/50048]	Loss: 0.1193
Training Epoch: 85 [34688/50048]	Loss: 0.0993
Training Epoch: 85 [34816/50048]	Loss: 0.1014
Training Epoch: 85 [34944/50048]	Loss: 0.1020
Training Epoch: 85 [35072/50048]	Loss: 0.0675
Training Epoch: 85 [35200/50048]	Loss: 0.1114
Training Epoch: 85 [35328/50048]	Loss: 0.1298
Training Epoch: 85 [35456/50048]	Loss: 0.1095
Training Epoch: 85 [35584/50048]	Loss: 0.0875
Training Epoch: 85 [35712/50048]	Loss: 0.0874
Training Epoch: 85 [35840/50048]	Loss: 0.0554
Training Epoch: 85 [35968/50048]	Loss: 0.0827
Training Epoch: 85 [36096/50048]	Loss: 0.0672
Training Epoch: 85 [36224/50048]	Loss: 0.0885
Training Epoch: 85 [36352/50048]	Loss: 0.0629
Training Epoch: 85 [36480/50048]	Loss: 0.1818
Training Epoch: 85 [36608/50048]	Loss: 0.1047
Training Epoch: 85 [36736/50048]	Loss: 0.0393
Training Epoch: 85 [36864/50048]	Loss: 0.1480
Training Epoch: 85 [36992/50048]	Loss: 0.1298
Training Epoch: 85 [37120/50048]	Loss: 0.0406
Training Epoch: 85 [37248/50048]	Loss: 0.0885
Training Epoch: 85 [37376/50048]	Loss: 0.0739
Training Epoch: 85 [37504/50048]	Loss: 0.0806
Training Epoch: 85 [37632/50048]	Loss: 0.0324
Training Epoch: 85 [37760/50048]	Loss: 0.1476
Training Epoch: 85 [37888/50048]	Loss: 0.0864
Training Epoch: 85 [38016/50048]	Loss: 0.1438
Training Epoch: 85 [38144/50048]	Loss: 0.0764
Training Epoch: 85 [38272/50048]	Loss: 0.0932
Training Epoch: 85 [38400/50048]	Loss: 0.2154
Training Epoch: 85 [38528/50048]	Loss: 0.1360
Training Epoch: 85 [38656/50048]	Loss: 0.1079
Training Epoch: 85 [38784/50048]	Loss: 0.1128
Training Epoch: 85 [38912/50048]	Loss: 0.0706
Training Epoch: 85 [39040/50048]	Loss: 0.1176
Training Epoch: 85 [39168/50048]	Loss: 0.1578
Training Epoch: 85 [39296/50048]	Loss: 0.0800
Training Epoch: 85 [39424/50048]	Loss: 0.1359
Training Epoch: 85 [39552/50048]	Loss: 0.1252
Training Epoch: 85 [39680/50048]	Loss: 0.0782
Training Epoch: 85 [39808/50048]	Loss: 0.1632
Training Epoch: 85 [39936/50048]	Loss: 0.0602
Training Epoch: 85 [40064/50048]	Loss: 0.1604
Training Epoch: 85 [40192/50048]	Loss: 0.0639
Training Epoch: 85 [40320/50048]	Loss: 0.0564
Training Epoch: 85 [40448/50048]	Loss: 0.0643
Training Epoch: 85 [40576/50048]	Loss: 0.0594
Training Epoch: 85 [40704/50048]	Loss: 0.1305
Training Epoch: 85 [40832/50048]	Loss: 0.0610
Training Epoch: 85 [40960/50048]	Loss: 0.0725
Training Epoch: 85 [41088/50048]	Loss: 0.1258
Training Epoch: 85 [41216/50048]	Loss: 0.1247
Training Epoch: 85 [41344/50048]	Loss: 0.0641
Training Epoch: 85 [41472/50048]	Loss: 0.0884
Training Epoch: 85 [41600/50048]	Loss: 0.1256
Training Epoch: 85 [41728/50048]	Loss: 0.0496
Training Epoch: 85 [41856/50048]	Loss: 0.1003
Training Epoch: 85 [41984/50048]	Loss: 0.0404
Training Epoch: 85 [42112/50048]	Loss: 0.1484
Training Epoch: 85 [42240/50048]	Loss: 0.0719
Training Epoch: 85 [42368/50048]	Loss: 0.1001
Training Epoch: 85 [42496/50048]	Loss: 0.0942
Training Epoch: 85 [42624/50048]	Loss: 0.1115
Training Epoch: 85 [42752/50048]	Loss: 0.0804
Training Epoch: 85 [42880/50048]	Loss: 0.0886
Training Epoch: 85 [43008/50048]	Loss: 0.0719
Training Epoch: 85 [43136/50048]	Loss: 0.1129
Training Epoch: 85 [43264/50048]	Loss: 0.0682
Training Epoch: 85 [43392/50048]	Loss: 0.0734
Training Epoch: 85 [43520/50048]	Loss: 0.1278
Training Epoch: 85 [43648/50048]	Loss: 0.1553
Training Epoch: 85 [43776/50048]	Loss: 0.1293
Training Epoch: 85 [43904/50048]	Loss: 0.1074
Training Epoch: 85 [44032/50048]	Loss: 0.0583
Training Epoch: 85 [44160/50048]	Loss: 0.0590
Training Epoch: 85 [44288/50048]	Loss: 0.0499
Training Epoch: 85 [44416/50048]	Loss: 0.1562
Training Epoch: 85 [44544/50048]	Loss: 0.2162
Training Epoch: 85 [44672/50048]	Loss: 0.1207
Training Epoch: 85 [44800/50048]	Loss: 0.0994
Training Epoch: 85 [44928/50048]	Loss: 0.0733
Training Epoch: 85 [45056/50048]	Loss: 0.1035
Training Epoch: 85 [45184/50048]	Loss: 0.1763
Training Epoch: 85 [45312/50048]	Loss: 0.1048
Training Epoch: 85 [45440/50048]	Loss: 0.1223
Training Epoch: 85 [45568/50048]	Loss: 0.0683
Training Epoch: 85 [45696/50048]	Loss: 0.1430
2022-12-06 05:46:39,306 [ZeusDataLoader(train)] train epoch 86 done: time=86.44 energy=10491.90
2022-12-06 05:46:39,307 [ZeusDataLoader(eval)] Epoch 86 begin.
Training Epoch: 85 [45824/50048]	Loss: 0.0882
Training Epoch: 85 [45952/50048]	Loss: 0.0721
Training Epoch: 85 [46080/50048]	Loss: 0.0855
Training Epoch: 85 [46208/50048]	Loss: 0.0765
Training Epoch: 85 [46336/50048]	Loss: 0.0959
Training Epoch: 85 [46464/50048]	Loss: 0.0940
Training Epoch: 85 [46592/50048]	Loss: 0.0799
Training Epoch: 85 [46720/50048]	Loss: 0.0872
Training Epoch: 85 [46848/50048]	Loss: 0.1221
Training Epoch: 85 [46976/50048]	Loss: 0.1109
Training Epoch: 85 [47104/50048]	Loss: 0.1151
Training Epoch: 85 [47232/50048]	Loss: 0.1017
Training Epoch: 85 [47360/50048]	Loss: 0.1448
Training Epoch: 85 [47488/50048]	Loss: 0.1684
Training Epoch: 85 [47616/50048]	Loss: 0.0824
Training Epoch: 85 [47744/50048]	Loss: 0.1515
Training Epoch: 85 [47872/50048]	Loss: 0.1592
Training Epoch: 85 [48000/50048]	Loss: 0.1550
Training Epoch: 85 [48128/50048]	Loss: 0.1486
Training Epoch: 85 [48256/50048]	Loss: 0.1749
Training Epoch: 85 [48384/50048]	Loss: 0.1108
Training Epoch: 85 [48512/50048]	Loss: 0.1680
Training Epoch: 85 [48640/50048]	Loss: 0.1419
Training Epoch: 85 [48768/50048]	Loss: 0.0692
Training Epoch: 85 [48896/50048]	Loss: 0.1247
Training Epoch: 85 [49024/50048]	Loss: 0.1446
Training Epoch: 85 [49152/50048]	Loss: 0.0779
Training Epoch: 85 [49280/50048]	Loss: 0.0916
Training Epoch: 85 [49408/50048]	Loss: 0.1050
Training Epoch: 85 [49536/50048]	Loss: 0.0882
Training Epoch: 85 [49664/50048]	Loss: 0.1658
Training Epoch: 85 [49792/50048]	Loss: 0.1113
Training Epoch: 85 [49920/50048]	Loss: 0.1696
Training Epoch: 85 [50048/50048]	Loss: 0.1278
2022-12-06 10:46:43.030 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:46:43,069 [ZeusDataLoader(eval)] eval epoch 86 done: time=3.75 energy=453.36
2022-12-06 05:46:43,069 [ZeusDataLoader(train)] Up to epoch 86: time=7758.92, energy=941879.02, cost=1149844.82
2022-12-06 05:46:43,070 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:46:43,070 [ZeusDataLoader(train)] Expected next epoch: time=7848.72, energy=952677.03, cost=1163101.21
2022-12-06 05:46:43,071 [ZeusDataLoader(train)] Epoch 87 begin.
Validation Epoch: 85, Average loss: 0.0183, Accuracy: 0.6364
2022-12-06 10:46:43.258 [ZeusMonitor] Monitor started.
2022-12-06 10:46:43.258 [ZeusMonitor] Running indefinitely. 2022-12-06 10:46:43.258 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:46:43.258 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e87+gpu0.power.log
2022-12-06 05:46:43,263 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:46:43,264 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
Training Epoch: 86 [128/50048]	Loss: 0.1179
Training Epoch: 86 [256/50048]	Loss: 0.1090
Training Epoch: 86 [384/50048]	Loss: 0.0750
Training Epoch: 86 [512/50048]	Loss: 0.0632
Training Epoch: 86 [640/50048]	Loss: 0.0276
Training Epoch: 86 [768/50048]	Loss: 0.0611
Training Epoch: 86 [896/50048]	Loss: 0.1128
Training Epoch: 86 [1024/50048]	Loss: 0.1203
Training Epoch: 86 [1152/50048]	Loss: 0.0534
Training Epoch: 86 [1280/50048]	Loss: 0.0984
Training Epoch: 86 [1408/50048]	Loss: 0.0280
Training Epoch: 86 [1536/50048]	Loss: 0.0727
Training Epoch: 86 [1664/50048]	Loss: 0.0834
Training Epoch: 86 [1792/50048]	Loss: 0.1263
Training Epoch: 86 [1920/50048]	Loss: 0.1110
Training Epoch: 86 [2048/50048]	Loss: 0.1249
Training Epoch: 86 [2176/50048]	Loss: 0.1326
Training Epoch: 86 [2304/50048]	Loss: 0.1508
Training Epoch: 86 [2432/50048]	Loss: 0.0661
Training Epoch: 86 [2560/50048]	Loss: 0.0705
Training Epoch: 86 [2688/50048]	Loss: 0.0772
Training Epoch: 86 [2816/50048]	Loss: 0.1009
Training Epoch: 86 [2944/50048]	Loss: 0.1055
Training Epoch: 86 [3072/50048]	Loss: 0.0766
Training Epoch: 86 [3200/50048]	Loss: 0.1444
Training Epoch: 86 [3328/50048]	Loss: 0.0626
Training Epoch: 86 [3456/50048]	Loss: 0.0623
Training Epoch: 86 [3584/50048]	Loss: 0.1032
Training Epoch: 86 [3712/50048]	Loss: 0.0426
Training Epoch: 86 [3840/50048]	Loss: 0.0506
Training Epoch: 86 [3968/50048]	Loss: 0.0410
Training Epoch: 86 [4096/50048]	Loss: 0.0485
Training Epoch: 86 [4224/50048]	Loss: 0.1507
Training Epoch: 86 [4352/50048]	Loss: 0.1314
Training Epoch: 86 [4480/50048]	Loss: 0.0739
Training Epoch: 86 [4608/50048]	Loss: 0.0945
Training Epoch: 86 [4736/50048]	Loss: 0.1186
Training Epoch: 86 [4864/50048]	Loss: 0.0527
Training Epoch: 86 [4992/50048]	Loss: 0.0813
Training Epoch: 86 [5120/50048]	Loss: 0.0703
Training Epoch: 86 [5248/50048]	Loss: 0.1317
Training Epoch: 86 [5376/50048]	Loss: 0.0732
Training Epoch: 86 [5504/50048]	Loss: 0.1265
Training Epoch: 86 [5632/50048]	Loss: 0.0776
Training Epoch: 86 [5760/50048]	Loss: 0.1146
Training Epoch: 86 [5888/50048]	Loss: 0.0993
Training Epoch: 86 [6016/50048]	Loss: 0.0664
Training Epoch: 86 [6144/50048]	Loss: 0.1046
Training Epoch: 86 [6272/50048]	Loss: 0.0772
Training Epoch: 86 [6400/50048]	Loss: 0.1023
Training Epoch: 86 [6528/50048]	Loss: 0.1404
Training Epoch: 86 [6656/50048]	Loss: 0.1329
Training Epoch: 86 [6784/50048]	Loss: 0.0719
Training Epoch: 86 [6912/50048]	Loss: 0.0674
Training Epoch: 86 [7040/50048]	Loss: 0.0384
Training Epoch: 86 [7168/50048]	Loss: 0.1200
Training Epoch: 86 [7296/50048]	Loss: 0.0645
Training Epoch: 86 [7424/50048]	Loss: 0.0549
Training Epoch: 86 [7552/50048]	Loss: 0.0618
Training Epoch: 86 [7680/50048]	Loss: 0.0689
Training Epoch: 86 [7808/50048]	Loss: 0.1323
Training Epoch: 86 [7936/50048]	Loss: 0.1633
Training Epoch: 86 [8064/50048]	Loss: 0.1026
Training Epoch: 86 [8192/50048]	Loss: 0.1244
Training Epoch: 86 [8320/50048]	Loss: 0.1422
Training Epoch: 86 [8448/50048]	Loss: 0.0525
Training Epoch: 86 [8576/50048]	Loss: 0.0902
Training Epoch: 86 [8704/50048]	Loss: 0.0878
Training Epoch: 86 [8832/50048]	Loss: 0.0605
Training Epoch: 86 [8960/50048]	Loss: 0.0354
Training Epoch: 86 [9088/50048]	Loss: 0.0937
Training Epoch: 86 [9216/50048]	Loss: 0.0953
Training Epoch: 86 [9344/50048]	Loss: 0.1319
Training Epoch: 86 [9472/50048]	Loss: 0.0988
Training Epoch: 86 [9600/50048]	Loss: 0.1092
Training Epoch: 86 [9728/50048]	Loss: 0.0553
Training Epoch: 86 [9856/50048]	Loss: 0.0868
Training Epoch: 86 [9984/50048]	Loss: 0.0597
Training Epoch: 86 [10112/50048]	Loss: 0.0692
Training Epoch: 86 [10240/50048]	Loss: 0.0592
Training Epoch: 86 [10368/50048]	Loss: 0.0812
Training Epoch: 86 [10496/50048]	Loss: 0.0717
Training Epoch: 86 [10624/50048]	Loss: 0.1736
Training Epoch: 86 [10752/50048]	Loss: 0.0833
Training Epoch: 86 [10880/50048]	Loss: 0.0686
Training Epoch: 86 [11008/50048]	Loss: 0.0851
Training Epoch: 86 [11136/50048]	Loss: 0.0599
Training Epoch: 86 [11264/50048]	Loss: 0.1132
Training Epoch: 86 [11392/50048]	Loss: 0.0574
Training Epoch: 86 [11520/50048]	Loss: 0.0426
Training Epoch: 86 [11648/50048]	Loss: 0.0728
Training Epoch: 86 [11776/50048]	Loss: 0.1221
Training Epoch: 86 [11904/50048]	Loss: 0.0589
Training Epoch: 86 [12032/50048]	Loss: 0.0984
Training Epoch: 86 [12160/50048]	Loss: 0.0484
Training Epoch: 86 [12288/50048]	Loss: 0.0765
Training Epoch: 86 [12416/50048]	Loss: 0.1537
Training Epoch: 86 [12544/50048]	Loss: 0.1095
Training Epoch: 86 [12672/50048]	Loss: 0.0888
Training Epoch: 86 [12800/50048]	Loss: 0.0953
Training Epoch: 86 [12928/50048]	Loss: 0.1017
Training Epoch: 86 [13056/50048]	Loss: 0.1398
Training Epoch: 86 [13184/50048]	Loss: 0.0797
Training Epoch: 86 [13312/50048]	Loss: 0.0987
Training Epoch: 86 [13440/50048]	Loss: 0.1116
Training Epoch: 86 [13568/50048]	Loss: 0.1174
Training Epoch: 86 [13696/50048]	Loss: 0.1344
Training Epoch: 86 [13824/50048]	Loss: 0.0890
Training Epoch: 86 [13952/50048]	Loss: 0.0955
Training Epoch: 86 [14080/50048]	Loss: 0.0331
Training Epoch: 86 [14208/50048]	Loss: 0.1052
Training Epoch: 86 [14336/50048]	Loss: 0.1094
Training Epoch: 86 [14464/50048]	Loss: 0.1302
Training Epoch: 86 [14592/50048]	Loss: 0.1144
Training Epoch: 86 [14720/50048]	Loss: 0.1039
Training Epoch: 86 [14848/50048]	Loss: 0.0489
Training Epoch: 86 [14976/50048]	Loss: 0.1466
Training Epoch: 86 [15104/50048]	Loss: 0.0474
Training Epoch: 86 [15232/50048]	Loss: 0.1228
Training Epoch: 86 [15360/50048]	Loss: 0.1033
Training Epoch: 86 [15488/50048]	Loss: 0.1933
Training Epoch: 86 [15616/50048]	Loss: 0.0614
Training Epoch: 86 [15744/50048]	Loss: 0.1107
Training Epoch: 86 [15872/50048]	Loss: 0.1111
Training Epoch: 86 [16000/50048]	Loss: 0.1361
Training Epoch: 86 [16128/50048]	Loss: 0.1047
Training Epoch: 86 [16256/50048]	Loss: 0.1806
Training Epoch: 86 [16384/50048]	Loss: 0.1001
Training Epoch: 86 [16512/50048]	Loss: 0.0828
Training Epoch: 86 [16640/50048]	Loss: 0.0521
Training Epoch: 86 [16768/50048]	Loss: 0.1264
Training Epoch: 86 [16896/50048]	Loss: 0.0829
Training Epoch: 86 [17024/50048]	Loss: 0.0961
Training Epoch: 86 [17152/50048]	Loss: 0.1154
Training Epoch: 86 [17280/50048]	Loss: 0.1619
Training Epoch: 86 [17408/50048]	Loss: 0.0639
Training Epoch: 86 [17536/50048]	Loss: 0.0370
Training Epoch: 86 [17664/50048]	Loss: 0.0772
Training Epoch: 86 [17792/50048]	Loss: 0.1163
Training Epoch: 86 [17920/50048]	Loss: 0.1181
Training Epoch: 86 [18048/50048]	Loss: 0.0795
Training Epoch: 86 [18176/50048]	Loss: 0.0965
Training Epoch: 86 [18304/50048]	Loss: 0.1439
Training Epoch: 86 [18432/50048]	Loss: 0.0848
Training Epoch: 86 [18560/50048]	Loss: 0.0614
Training Epoch: 86 [18688/50048]	Loss: 0.1253
Training Epoch: 86 [18816/50048]	Loss: 0.1058
Training Epoch: 86 [18944/50048]	Loss: 0.0873
Training Epoch: 86 [19072/50048]	Loss: 0.0594
Training Epoch: 86 [19200/50048]	Loss: 0.0838
Training Epoch: 86 [19328/50048]	Loss: 0.0990
Training Epoch: 86 [19456/50048]	Loss: 0.1023
Training Epoch: 86 [19584/50048]	Loss: 0.0638
Training Epoch: 86 [19712/50048]	Loss: 0.0664
Training Epoch: 86 [19840/50048]	Loss: 0.0653
Training Epoch: 86 [19968/50048]	Loss: 0.0625
Training Epoch: 86 [20096/50048]	Loss: 0.0974
Training Epoch: 86 [20224/50048]	Loss: 0.0922
Training Epoch: 86 [20352/50048]	Loss: 0.0629
Training Epoch: 86 [20480/50048]	Loss: 0.0572
Training Epoch: 86 [20608/50048]	Loss: 0.0803
Training Epoch: 86 [20736/50048]	Loss: 0.0792
Training Epoch: 86 [20864/50048]	Loss: 0.0341
Training Epoch: 86 [20992/50048]	Loss: 0.0844
Training Epoch: 86 [21120/50048]	Loss: 0.0958
Training Epoch: 86 [21248/50048]	Loss: 0.1077
Training Epoch: 86 [21376/50048]	Loss: 0.0901
Training Epoch: 86 [21504/50048]	Loss: 0.0593
Training Epoch: 86 [21632/50048]	Loss: 0.0641
Training Epoch: 86 [21760/50048]	Loss: 0.0527
Training Epoch: 86 [21888/50048]	Loss: 0.0625
Training Epoch: 86 [22016/50048]	Loss: 0.0698
Training Epoch: 86 [22144/50048]	Loss: 0.1182
Training Epoch: 86 [22272/50048]	Loss: 0.0522
Training Epoch: 86 [22400/50048]	Loss: 0.1105
Training Epoch: 86 [22528/50048]	Loss: 0.0583
Training Epoch: 86 [22656/50048]	Loss: 0.0610
Training Epoch: 86 [22784/50048]	Loss: 0.1316
Training Epoch: 86 [22912/50048]	Loss: 0.0874
Training Epoch: 86 [23040/50048]	Loss: 0.0951
Training Epoch: 86 [23168/50048]	Loss: 0.1025
Training Epoch: 86 [23296/50048]	Loss: 0.0601
Training Epoch: 86 [23424/50048]	Loss: 0.1246
Training Epoch: 86 [23552/50048]	Loss: 0.1410
Training Epoch: 86 [23680/50048]	Loss: 0.0400
Training Epoch: 86 [23808/50048]	Loss: 0.0798
Training Epoch: 86 [23936/50048]	Loss: 0.0627
Training Epoch: 86 [24064/50048]	Loss: 0.1586
Training Epoch: 86 [24192/50048]	Loss: 0.0966
Training Epoch: 86 [24320/50048]	Loss: 0.1344
Training Epoch: 86 [24448/50048]	Loss: 0.1309
Training Epoch: 86 [24576/50048]	Loss: 0.0922
Training Epoch: 86 [24704/50048]	Loss: 0.0777
Training Epoch: 86 [24832/50048]	Loss: 0.0788
Training Epoch: 86 [24960/50048]	Loss: 0.0592
Training Epoch: 86 [25088/50048]	Loss: 0.0800
Training Epoch: 86 [25216/50048]	Loss: 0.0800
Training Epoch: 86 [25344/50048]	Loss: 0.1401
Training Epoch: 86 [25472/50048]	Loss: 0.1091
Training Epoch: 86 [25600/50048]	Loss: 0.0715
Training Epoch: 86 [25728/50048]	Loss: 0.0917
Training Epoch: 86 [25856/50048]	Loss: 0.0809
Training Epoch: 86 [25984/50048]	Loss: 0.0542
Training Epoch: 86 [26112/50048]	Loss: 0.0556
Training Epoch: 86 [26240/50048]	Loss: 0.0335
Training Epoch: 86 [26368/50048]	Loss: 0.1791
Training Epoch: 86 [26496/50048]	Loss: 0.1201
Training Epoch: 86 [26624/50048]	Loss: 0.0904
Training Epoch: 86 [26752/50048]	Loss: 0.0767
Training Epoch: 86 [26880/50048]	Loss: 0.1252
Training Epoch: 86 [27008/50048]	Loss: 0.0596
Training Epoch: 86 [27136/50048]	Loss: 0.0567
Training Epoch: 86 [27264/50048]	Loss: 0.0471
Training Epoch: 86 [27392/50048]	Loss: 0.1137
Training Epoch: 86 [27520/50048]	Loss: 0.1455
Training Epoch: 86 [27648/50048]	Loss: 0.1125
Training Epoch: 86 [27776/50048]	Loss: 0.0482
Training Epoch: 86 [27904/50048]	Loss: 0.1364
Training Epoch: 86 [28032/50048]	Loss: 0.0878
Training Epoch: 86 [28160/50048]	Loss: 0.1028
Training Epoch: 86 [28288/50048]	Loss: 0.1253
Training Epoch: 86 [28416/50048]	Loss: 0.1022
Training Epoch: 86 [28544/50048]	Loss: 0.0418
Training Epoch: 86 [28672/50048]	Loss: 0.1670
Training Epoch: 86 [28800/50048]	Loss: 0.1751
Training Epoch: 86 [28928/50048]	Loss: 0.0569
Training Epoch: 86 [29056/50048]	Loss: 0.0961
Training Epoch: 86 [29184/50048]	Loss: 0.0569
Training Epoch: 86 [29312/50048]	Loss: 0.1271
Training Epoch: 86 [29440/50048]	Loss: 0.1700
Training Epoch: 86 [29568/50048]	Loss: 0.0841
Training Epoch: 86 [29696/50048]	Loss: 0.0504
Training Epoch: 86 [29824/50048]	Loss: 0.1117
Training Epoch: 86 [29952/50048]	Loss: 0.0711
Training Epoch: 86 [30080/50048]	Loss: 0.1292
Training Epoch: 86 [30208/50048]	Loss: 0.1185
Training Epoch: 86 [30336/50048]	Loss: 0.0930
Training Epoch: 86 [30464/50048]	Loss: 0.0986
Training Epoch: 86 [30592/50048]	Loss: 0.1059
Training Epoch: 86 [30720/50048]	Loss: 0.0909
Training Epoch: 86 [30848/50048]	Loss: 0.1265
Training Epoch: 86 [30976/50048]	Loss: 0.0874
Training Epoch: 86 [31104/50048]	Loss: 0.1135
Training Epoch: 86 [31232/50048]	Loss: 0.1278
Training Epoch: 86 [31360/50048]	Loss: 0.2206
Training Epoch: 86 [31488/50048]	Loss: 0.0752
Training Epoch: 86 [31616/50048]	Loss: 0.0928
Training Epoch: 86 [31744/50048]	Loss: 0.0688
Training Epoch: 86 [31872/50048]	Loss: 0.1101
Training Epoch: 86 [32000/50048]	Loss: 0.1466
Training Epoch: 86 [32128/50048]	Loss: 0.0245
Training Epoch: 86 [32256/50048]	Loss: 0.1366
Training Epoch: 86 [32384/50048]	Loss: 0.0845
Training Epoch: 86 [32512/50048]	Loss: 0.1017
Training Epoch: 86 [32640/50048]	Loss: 0.0887
Training Epoch: 86 [32768/50048]	Loss: 0.0650
Training Epoch: 86 [32896/50048]	Loss: 0.1107
Training Epoch: 86 [33024/50048]	Loss: 0.1187
Training Epoch: 86 [33152/50048]	Loss: 0.0829
Training Epoch: 86 [33280/50048]	Loss: 0.0761
Training Epoch: 86 [33408/50048]	Loss: 0.1032
Training Epoch: 86 [33536/50048]	Loss: 0.0840
Training Epoch: 86 [33664/50048]	Loss: 0.1786
Training Epoch: 86 [33792/50048]	Loss: 0.0759
Training Epoch: 86 [33920/50048]	Loss: 0.0903
Training Epoch: 86 [34048/50048]	Loss: 0.0630
Training Epoch: 86 [34176/50048]	Loss: 0.0649
Training Epoch: 86 [34304/50048]	Loss: 0.1203
Training Epoch: 86 [34432/50048]	Loss: 0.1105
Training Epoch: 86 [34560/50048]	Loss: 0.2407
Training Epoch: 86 [34688/50048]	Loss: 0.0993
Training Epoch: 86 [34816/50048]	Loss: 0.0927
Training Epoch: 86 [34944/50048]	Loss: 0.1347
Training Epoch: 86 [35072/50048]	Loss: 0.0505
Training Epoch: 86 [35200/50048]	Loss: 0.1299
Training Epoch: 86 [35328/50048]	Loss: 0.1179
Training Epoch: 86 [35456/50048]	Loss: 0.1126
Training Epoch: 86 [35584/50048]	Loss: 0.0536
Training Epoch: 86 [35712/50048]	Loss: 0.0903
Training Epoch: 86 [35840/50048]	Loss: 0.0705
Training Epoch: 86 [35968/50048]	Loss: 0.1122
Training Epoch: 86 [36096/50048]	Loss: 0.0760
Training Epoch: 86 [36224/50048]	Loss: 0.0855
Training Epoch: 86 [36352/50048]	Loss: 0.1203
Training Epoch: 86 [36480/50048]	Loss: 0.0193
Training Epoch: 86 [36608/50048]	Loss: 0.1095
Training Epoch: 86 [36736/50048]	Loss: 0.0871
Training Epoch: 86 [36864/50048]	Loss: 0.1170
Training Epoch: 86 [36992/50048]	Loss: 0.0454
Training Epoch: 86 [37120/50048]	Loss: 0.1287
Training Epoch: 86 [37248/50048]	Loss: 0.0441
Training Epoch: 86 [37376/50048]	Loss: 0.1384
Training Epoch: 86 [37504/50048]	Loss: 0.0764
Training Epoch: 86 [37632/50048]	Loss: 0.0364
Training Epoch: 86 [37760/50048]	Loss: 0.0846
Training Epoch: 86 [37888/50048]	Loss: 0.0467
Training Epoch: 86 [38016/50048]	Loss: 0.1152
Training Epoch: 86 [38144/50048]	Loss: 0.1280
Training Epoch: 86 [38272/50048]	Loss: 0.0854
Training Epoch: 86 [38400/50048]	Loss: 0.0913
Training Epoch: 86 [38528/50048]	Loss: 0.1021
Training Epoch: 86 [38656/50048]	Loss: 0.1127
Training Epoch: 86 [38784/50048]	Loss: 0.0761
Training Epoch: 86 [38912/50048]	Loss: 0.1324
Training Epoch: 86 [39040/50048]	Loss: 0.1330
Training Epoch: 86 [39168/50048]	Loss: 0.0812
Training Epoch: 86 [39296/50048]	Loss: 0.1444
Training Epoch: 86 [39424/50048]	Loss: 0.0760
Training Epoch: 86 [39552/50048]	Loss: 0.1289
Training Epoch: 86 [39680/50048]	Loss: 0.0972
Training Epoch: 86 [39808/50048]	Loss: 0.1116
Training Epoch: 86 [39936/50048]	Loss: 0.1476
Training Epoch: 86 [40064/50048]	Loss: 0.1530
Training Epoch: 86 [40192/50048]	Loss: 0.1127
Training Epoch: 86 [40320/50048]	Loss: 0.0742
Training Epoch: 86 [40448/50048]	Loss: 0.1178
Training Epoch: 86 [40576/50048]	Loss: 0.1190
Training Epoch: 86 [40704/50048]	Loss: 0.0567
Training Epoch: 86 [40832/50048]	Loss: 0.0541
Training Epoch: 86 [40960/50048]	Loss: 0.0651
Training Epoch: 86 [41088/50048]	Loss: 0.1337
Training Epoch: 86 [41216/50048]	Loss: 0.1403
Training Epoch: 86 [41344/50048]	Loss: 0.0594
Training Epoch: 86 [41472/50048]	Loss: 0.1425
Training Epoch: 86 [41600/50048]	Loss: 0.1328
Training Epoch: 86 [41728/50048]	Loss: 0.0415
Training Epoch: 86 [41856/50048]	Loss: 0.0682
Training Epoch: 86 [41984/50048]	Loss: 0.0569
Training Epoch: 86 [42112/50048]	Loss: 0.0456
Training Epoch: 86 [42240/50048]	Loss: 0.0830
Training Epoch: 86 [42368/50048]	Loss: 0.0513
Training Epoch: 86 [42496/50048]	Loss: 0.1570
Training Epoch: 86 [42624/50048]	Loss: 0.1439
Training Epoch: 86 [42752/50048]	Loss: 0.0929
Training Epoch: 86 [42880/50048]	Loss: 0.1086
Training Epoch: 86 [43008/50048]	Loss: 0.0963
Training Epoch: 86 [43136/50048]	Loss: 0.1093
Training Epoch: 86 [43264/50048]	Loss: 0.0786
Training Epoch: 86 [43392/50048]	Loss: 0.0764
Training Epoch: 86 [43520/50048]	Loss: 0.1165
Training Epoch: 86 [43648/50048]	Loss: 0.0594
Training Epoch: 86 [43776/50048]	Loss: 0.0725
Training Epoch: 86 [43904/50048]	Loss: 0.0341
Training Epoch: 86 [44032/50048]	Loss: 0.1497
Training Epoch: 86 [44160/50048]	Loss: 0.1419
Training Epoch: 86 [44288/50048]	Loss: 0.1315
Training Epoch: 86 [44416/50048]	Loss: 0.1015
Training Epoch: 86 [44544/50048]	Loss: 0.1009
Training Epoch: 86 [44672/50048]	Loss: 0.0952
Training Epoch: 86 [44800/50048]	Loss: 0.1140
Training Epoch: 86 [44928/50048]	Loss: 0.1340
Training Epoch: 86 [45056/50048]	Loss: 0.0879
Training Epoch: 86 [45184/50048]	Loss: 0.0938
Training Epoch: 86 [45312/50048]	Loss: 0.0947
Training Epoch: 86 [45440/50048]	Loss: 0.1542
Training Epoch: 86 [45568/50048]	Loss: 0.0880
Training Epoch: 86 [45696/50048]	Loss: 0.1649
2022-12-06 05:48:09,512 [ZeusDataLoader(train)] train epoch 87 done: time=86.43 energy=10490.13
2022-12-06 05:48:09,514 [ZeusDataLoader(eval)] Epoch 87 begin.
Training Epoch: 86 [45824/50048]	Loss: 0.1066
Training Epoch: 86 [45952/50048]	Loss: 0.1707
Training Epoch: 86 [46080/50048]	Loss: 0.0876
Training Epoch: 86 [46208/50048]	Loss: 0.1513
Training Epoch: 86 [46336/50048]	Loss: 0.0735
Training Epoch: 86 [46464/50048]	Loss: 0.0923
Training Epoch: 86 [46592/50048]	Loss: 0.0705
Training Epoch: 86 [46720/50048]	Loss: 0.0843
Training Epoch: 86 [46848/50048]	Loss: 0.1459
Training Epoch: 86 [46976/50048]	Loss: 0.0936
Training Epoch: 86 [47104/50048]	Loss: 0.1391
Training Epoch: 86 [47232/50048]	Loss: 0.0963
Training Epoch: 86 [47360/50048]	Loss: 0.0892
Training Epoch: 86 [47488/50048]	Loss: 0.0598
Training Epoch: 86 [47616/50048]	Loss: 0.1614
Training Epoch: 86 [47744/50048]	Loss: 0.1037
Training Epoch: 86 [47872/50048]	Loss: 0.1186
Training Epoch: 86 [48000/50048]	Loss: 0.1039
Training Epoch: 86 [48128/50048]	Loss: 0.0792
Training Epoch: 86 [48256/50048]	Loss: 0.1446
Training Epoch: 86 [48384/50048]	Loss: 0.1762
Training Epoch: 86 [48512/50048]	Loss: 0.0737
Training Epoch: 86 [48640/50048]	Loss: 0.2154
Training Epoch: 86 [48768/50048]	Loss: 0.1527
Training Epoch: 86 [48896/50048]	Loss: 0.1812
Training Epoch: 86 [49024/50048]	Loss: 0.2067
Training Epoch: 86 [49152/50048]	Loss: 0.1566
Training Epoch: 86 [49280/50048]	Loss: 0.1026
Training Epoch: 86 [49408/50048]	Loss: 0.0793
Training Epoch: 86 [49536/50048]	Loss: 0.0682
Training Epoch: 86 [49664/50048]	Loss: 0.0937
Training Epoch: 86 [49792/50048]	Loss: 0.0611
Training Epoch: 86 [49920/50048]	Loss: 0.0739
Training Epoch: 86 [50048/50048]	Loss: 0.1211
2022-12-06 10:48:13.238 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:48:13,270 [ZeusDataLoader(eval)] eval epoch 87 done: time=3.75 energy=453.71
2022-12-06 05:48:13,270 [ZeusDataLoader(train)] Up to epoch 87: time=7849.10, energy=952822.85, cost=1163207.43
2022-12-06 05:48:13,271 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:48:13,271 [ZeusDataLoader(train)] Expected next epoch: time=7938.90, energy=963620.87, cost=1176463.81
2022-12-06 05:48:13,272 [ZeusDataLoader(train)] Epoch 88 begin.
Validation Epoch: 86, Average loss: 0.0182, Accuracy: 0.6441
2022-12-06 05:48:13,444 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:48:13,445 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:48:13.454 [ZeusMonitor] Monitor started.
2022-12-06 10:48:13.454 [ZeusMonitor] Running indefinitely. 2022-12-06 10:48:13.454 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:48:13.454 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e88+gpu0.power.log
Training Epoch: 87 [128/50048]	Loss: 0.1482
Training Epoch: 87 [256/50048]	Loss: 0.1553
Training Epoch: 87 [384/50048]	Loss: 0.1068
Training Epoch: 87 [512/50048]	Loss: 0.0616
Training Epoch: 87 [640/50048]	Loss: 0.1528
Training Epoch: 87 [768/50048]	Loss: 0.0785
Training Epoch: 87 [896/50048]	Loss: 0.1834
Training Epoch: 87 [1024/50048]	Loss: 0.0796
Training Epoch: 87 [1152/50048]	Loss: 0.0895
Training Epoch: 87 [1280/50048]	Loss: 0.0455
Training Epoch: 87 [1408/50048]	Loss: 0.0578
Training Epoch: 87 [1536/50048]	Loss: 0.0702
Training Epoch: 87 [1664/50048]	Loss: 0.0414
Training Epoch: 87 [1792/50048]	Loss: 0.0789
Training Epoch: 87 [1920/50048]	Loss: 0.0753
Training Epoch: 87 [2048/50048]	Loss: 0.0883
Training Epoch: 87 [2176/50048]	Loss: 0.0514
Training Epoch: 87 [2304/50048]	Loss: 0.0624
Training Epoch: 87 [2432/50048]	Loss: 0.0682
Training Epoch: 87 [2560/50048]	Loss: 0.0377
Training Epoch: 87 [2688/50048]	Loss: 0.1119
Training Epoch: 87 [2816/50048]	Loss: 0.0848
Training Epoch: 87 [2944/50048]	Loss: 0.3001
Training Epoch: 87 [3072/50048]	Loss: 0.1149
Training Epoch: 87 [3200/50048]	Loss: 0.1046
Training Epoch: 87 [3328/50048]	Loss: 0.0564
Training Epoch: 87 [3456/50048]	Loss: 0.0305
Training Epoch: 87 [3584/50048]	Loss: 0.0935
Training Epoch: 87 [3712/50048]	Loss: 0.0612
Training Epoch: 87 [3840/50048]	Loss: 0.1751
Training Epoch: 87 [3968/50048]	Loss: 0.1152
Training Epoch: 87 [4096/50048]	Loss: 0.0833
Training Epoch: 87 [4224/50048]	Loss: 0.0583
Training Epoch: 87 [4352/50048]	Loss: 0.0698
Training Epoch: 87 [4480/50048]	Loss: 0.0587
Training Epoch: 87 [4608/50048]	Loss: 0.0574
Training Epoch: 87 [4736/50048]	Loss: 0.2253
Training Epoch: 87 [4864/50048]	Loss: 0.0428
Training Epoch: 87 [4992/50048]	Loss: 0.1112
Training Epoch: 87 [5120/50048]	Loss: 0.0652
Training Epoch: 87 [5248/50048]	Loss: 0.0465
Training Epoch: 87 [5376/50048]	Loss: 0.0418
Training Epoch: 87 [5504/50048]	Loss: 0.0358
Training Epoch: 87 [5632/50048]	Loss: 0.0716
Training Epoch: 87 [5760/50048]	Loss: 0.0425
Training Epoch: 87 [5888/50048]	Loss: 0.0971
Training Epoch: 87 [6016/50048]	Loss: 0.0399
Training Epoch: 87 [6144/50048]	Loss: 0.0801
Training Epoch: 87 [6272/50048]	Loss: 0.0895
Training Epoch: 87 [6400/50048]	Loss: 0.1287
Training Epoch: 87 [6528/50048]	Loss: 0.0974
Training Epoch: 87 [6656/50048]	Loss: 0.1430
Training Epoch: 87 [6784/50048]	Loss: 0.0441
Training Epoch: 87 [6912/50048]	Loss: 0.0538
Training Epoch: 87 [7040/50048]	Loss: 0.0833
Training Epoch: 87 [7168/50048]	Loss: 0.0808
Training Epoch: 87 [7296/50048]	Loss: 0.0901
Training Epoch: 87 [7424/50048]	Loss: 0.1600
Training Epoch: 87 [7552/50048]	Loss: 0.0613
Training Epoch: 87 [7680/50048]	Loss: 0.0417
Training Epoch: 87 [7808/50048]	Loss: 0.1250
Training Epoch: 87 [7936/50048]	Loss: 0.0428
Training Epoch: 87 [8064/50048]	Loss: 0.0787
Training Epoch: 87 [8192/50048]	Loss: 0.0616
Training Epoch: 87 [8320/50048]	Loss: 0.0818
Training Epoch: 87 [8448/50048]	Loss: 0.1102
Training Epoch: 87 [8576/50048]	Loss: 0.1349
Training Epoch: 87 [8704/50048]	Loss: 0.1147
Training Epoch: 87 [8832/50048]	Loss: 0.0662
Training Epoch: 87 [8960/50048]	Loss: 0.0652
Training Epoch: 87 [9088/50048]	Loss: 0.0539
Training Epoch: 87 [9216/50048]	Loss: 0.1296
Training Epoch: 87 [9344/50048]	Loss: 0.0494
Training Epoch: 87 [9472/50048]	Loss: 0.1286
Training Epoch: 87 [9600/50048]	Loss: 0.0759
Training Epoch: 87 [9728/50048]	Loss: 0.0804
Training Epoch: 87 [9856/50048]	Loss: 0.1380
Training Epoch: 87 [9984/50048]	Loss: 0.0847
Training Epoch: 87 [10112/50048]	Loss: 0.0454
Training Epoch: 87 [10240/50048]	Loss: 0.0775
Training Epoch: 87 [10368/50048]	Loss: 0.1518
Training Epoch: 87 [10496/50048]	Loss: 0.0326
Training Epoch: 87 [10624/50048]	Loss: 0.1932
Training Epoch: 87 [10752/50048]	Loss: 0.0670
Training Epoch: 87 [10880/50048]	Loss: 0.0734
Training Epoch: 87 [11008/50048]	Loss: 0.0956
Training Epoch: 87 [11136/50048]	Loss: 0.0496
Training Epoch: 87 [11264/50048]	Loss: 0.1307
Training Epoch: 87 [11392/50048]	Loss: 0.0699
Training Epoch: 87 [11520/50048]	Loss: 0.1622
Training Epoch: 87 [11648/50048]	Loss: 0.0917
Training Epoch: 87 [11776/50048]	Loss: 0.0895
Training Epoch: 87 [11904/50048]	Loss: 0.0715
Training Epoch: 87 [12032/50048]	Loss: 0.0557
Training Epoch: 87 [12160/50048]	Loss: 0.0843
Training Epoch: 87 [12288/50048]	Loss: 0.0887
Training Epoch: 87 [12416/50048]	Loss: 0.0501
Training Epoch: 87 [12544/50048]	Loss: 0.1368
Training Epoch: 87 [12672/50048]	Loss: 0.0760
Training Epoch: 87 [12800/50048]	Loss: 0.0625
Training Epoch: 87 [12928/50048]	Loss: 0.1309
Training Epoch: 87 [13056/50048]	Loss: 0.0735
Training Epoch: 87 [13184/50048]	Loss: 0.0712
Training Epoch: 87 [13312/50048]	Loss: 0.0635
Training Epoch: 87 [13440/50048]	Loss: 0.0547
Training Epoch: 87 [13568/50048]	Loss: 0.1206
Training Epoch: 87 [13696/50048]	Loss: 0.0812
Training Epoch: 87 [13824/50048]	Loss: 0.0936
Training Epoch: 87 [13952/50048]	Loss: 0.0569
Training Epoch: 87 [14080/50048]	Loss: 0.1106
Training Epoch: 87 [14208/50048]	Loss: 0.0690
Training Epoch: 87 [14336/50048]	Loss: 0.1015
Training Epoch: 87 [14464/50048]	Loss: 0.0700
Training Epoch: 87 [14592/50048]	Loss: 0.0749
Training Epoch: 87 [14720/50048]	Loss: 0.1041
Training Epoch: 87 [14848/50048]	Loss: 0.0753
Training Epoch: 87 [14976/50048]	Loss: 0.0653
Training Epoch: 87 [15104/50048]	Loss: 0.0867
Training Epoch: 87 [15232/50048]	Loss: 0.1033
Training Epoch: 87 [15360/50048]	Loss: 0.0481
Training Epoch: 87 [15488/50048]	Loss: 0.1465
Training Epoch: 87 [15616/50048]	Loss: 0.1718
Training Epoch: 87 [15744/50048]	Loss: 0.1015
Training Epoch: 87 [15872/50048]	Loss: 0.1078
Training Epoch: 87 [16000/50048]	Loss: 0.0923
Training Epoch: 87 [16128/50048]	Loss: 0.0806
Training Epoch: 87 [16256/50048]	Loss: 0.0597
Training Epoch: 87 [16384/50048]	Loss: 0.0813
Training Epoch: 87 [16512/50048]	Loss: 0.0823
Training Epoch: 87 [16640/50048]	Loss: 0.1864
Training Epoch: 87 [16768/50048]	Loss: 0.0391
Training Epoch: 87 [16896/50048]	Loss: 0.1263
Training Epoch: 87 [17024/50048]	Loss: 0.1264
Training Epoch: 87 [17152/50048]	Loss: 0.1078
Training Epoch: 87 [17280/50048]	Loss: 0.1085
Training Epoch: 87 [17408/50048]	Loss: 0.1221
Training Epoch: 87 [17536/50048]	Loss: 0.0670
Training Epoch: 87 [17664/50048]	Loss: 0.0989
Training Epoch: 87 [17792/50048]	Loss: 0.0990
Training Epoch: 87 [17920/50048]	Loss: 0.0791
Training Epoch: 87 [18048/50048]	Loss: 0.1132
Training Epoch: 87 [18176/50048]	Loss: 0.0550
Training Epoch: 87 [18304/50048]	Loss: 0.1307
Training Epoch: 87 [18432/50048]	Loss: 0.0935
Training Epoch: 87 [18560/50048]	Loss: 0.0346
Training Epoch: 87 [18688/50048]	Loss: 0.0790
Training Epoch: 87 [18816/50048]	Loss: 0.1163
Training Epoch: 87 [18944/50048]	Loss: 0.0780
Training Epoch: 87 [19072/50048]	Loss: 0.0478
Training Epoch: 87 [19200/50048]	Loss: 0.0703
Training Epoch: 87 [19328/50048]	Loss: 0.0641
Training Epoch: 87 [19456/50048]	Loss: 0.1471
Training Epoch: 87 [19584/50048]	Loss: 0.0598
Training Epoch: 87 [19712/50048]	Loss: 0.0565
Training Epoch: 87 [19840/50048]	Loss: 0.0526
Training Epoch: 87 [19968/50048]	Loss: 0.1197
Training Epoch: 87 [20096/50048]	Loss: 0.0738
Training Epoch: 87 [20224/50048]	Loss: 0.1253
Training Epoch: 87 [20352/50048]	Loss: 0.0579
Training Epoch: 87 [20480/50048]	Loss: 0.1122
Training Epoch: 87 [20608/50048]	Loss: 0.0595
Training Epoch: 87 [20736/50048]	Loss: 0.0906
Training Epoch: 87 [20864/50048]	Loss: 0.0874
Training Epoch: 87 [20992/50048]	Loss: 0.1087
Training Epoch: 87 [21120/50048]	Loss: 0.1178
Training Epoch: 87 [21248/50048]	Loss: 0.0761
Training Epoch: 87 [21376/50048]	Loss: 0.0644
Training Epoch: 87 [21504/50048]	Loss: 0.0714
Training Epoch: 87 [21632/50048]	Loss: 0.0651
Training Epoch: 87 [21760/50048]	Loss: 0.0808
Training Epoch: 87 [21888/50048]	Loss: 0.0980
Training Epoch: 87 [22016/50048]	Loss: 0.0688
Training Epoch: 87 [22144/50048]	Loss: 0.1096
Training Epoch: 87 [22272/50048]	Loss: 0.0770
Training Epoch: 87 [22400/50048]	Loss: 0.0701
Training Epoch: 87 [22528/50048]	Loss: 0.0832
Training Epoch: 87 [22656/50048]	Loss: 0.1427
Training Epoch: 87 [22784/50048]	Loss: 0.0397
Training Epoch: 87 [22912/50048]	Loss: 0.0582
Training Epoch: 87 [23040/50048]	Loss: 0.1116
Training Epoch: 87 [23168/50048]	Loss: 0.1097
Training Epoch: 87 [23296/50048]	Loss: 0.1250
Training Epoch: 87 [23424/50048]	Loss: 0.0835
Training Epoch: 87 [23552/50048]	Loss: 0.1171
Training Epoch: 87 [23680/50048]	Loss: 0.0545
Training Epoch: 87 [23808/50048]	Loss: 0.1110
Training Epoch: 87 [23936/50048]	Loss: 0.0506
Training Epoch: 87 [24064/50048]	Loss: 0.0662
Training Epoch: 87 [24192/50048]	Loss: 0.1131
Training Epoch: 87 [24320/50048]	Loss: 0.1005
Training Epoch: 87 [24448/50048]	Loss: 0.1518
Training Epoch: 87 [24576/50048]	Loss: 0.1388
Training Epoch: 87 [24704/50048]	Loss: 0.0829
Training Epoch: 87 [24832/50048]	Loss: 0.0338
Training Epoch: 87 [24960/50048]	Loss: 0.1155
Training Epoch: 87 [25088/50048]	Loss: 0.0743
Training Epoch: 87 [25216/50048]	Loss: 0.0766
Training Epoch: 87 [25344/50048]	Loss: 0.0472
Training Epoch: 87 [25472/50048]	Loss: 0.1599
Training Epoch: 87 [25600/50048]	Loss: 0.1981
Training Epoch: 87 [25728/50048]	Loss: 0.0628
Training Epoch: 87 [25856/50048]	Loss: 0.0980
Training Epoch: 87 [25984/50048]	Loss: 0.0642
Training Epoch: 87 [26112/50048]	Loss: 0.0817
Training Epoch: 87 [26240/50048]	Loss: 0.0832
Training Epoch: 87 [26368/50048]	Loss: 0.0454
Training Epoch: 87 [26496/50048]	Loss: 0.1519
Training Epoch: 87 [26624/50048]	Loss: 0.0650
Training Epoch: 87 [26752/50048]	Loss: 0.0700
Training Epoch: 87 [26880/50048]	Loss: 0.1307
Training Epoch: 87 [27008/50048]	Loss: 0.1263
Training Epoch: 87 [27136/50048]	Loss: 0.0505
Training Epoch: 87 [27264/50048]	Loss: 0.0775
Training Epoch: 87 [27392/50048]	Loss: 0.1005
Training Epoch: 87 [27520/50048]	Loss: 0.0331
Training Epoch: 87 [27648/50048]	Loss: 0.0881
Training Epoch: 87 [27776/50048]	Loss: 0.0745
Training Epoch: 87 [27904/50048]	Loss: 0.1614
Training Epoch: 87 [28032/50048]	Loss: 0.1329
Training Epoch: 87 [28160/50048]	Loss: 0.0585
Training Epoch: 87 [28288/50048]	Loss: 0.1127
Training Epoch: 87 [28416/50048]	Loss: 0.1636
Training Epoch: 87 [28544/50048]	Loss: 0.1321
Training Epoch: 87 [28672/50048]	Loss: 0.1551
Training Epoch: 87 [28800/50048]	Loss: 0.0994
Training Epoch: 87 [28928/50048]	Loss: 0.0969
Training Epoch: 87 [29056/50048]	Loss: 0.0905
Training Epoch: 87 [29184/50048]	Loss: 0.0627
Training Epoch: 87 [29312/50048]	Loss: 0.0449
Training Epoch: 87 [29440/50048]	Loss: 0.0461
Training Epoch: 87 [29568/50048]	Loss: 0.0612
Training Epoch: 87 [29696/50048]	Loss: 0.0637
Training Epoch: 87 [29824/50048]	Loss: 0.0880
Training Epoch: 87 [29952/50048]	Loss: 0.0479
Training Epoch: 87 [30080/50048]	Loss: 0.0638
Training Epoch: 87 [30208/50048]	Loss: 0.1359
Training Epoch: 87 [30336/50048]	Loss: 0.0672
Training Epoch: 87 [30464/50048]	Loss: 0.1129
Training Epoch: 87 [30592/50048]	Loss: 0.1136
Training Epoch: 87 [30720/50048]	Loss: 0.1156
Training Epoch: 87 [30848/50048]	Loss: 0.1185
Training Epoch: 87 [30976/50048]	Loss: 0.0902
Training Epoch: 87 [31104/50048]	Loss: 0.1401
Training Epoch: 87 [31232/50048]	Loss: 0.1679
Training Epoch: 87 [31360/50048]	Loss: 0.0840
Training Epoch: 87 [31488/50048]	Loss: 0.1207
Training Epoch: 87 [31616/50048]	Loss: 0.1097
Training Epoch: 87 [31744/50048]	Loss: 0.0961
Training Epoch: 87 [31872/50048]	Loss: 0.0812
Training Epoch: 87 [32000/50048]	Loss: 0.0866
Training Epoch: 87 [32128/50048]	Loss: 0.1546
Training Epoch: 87 [32256/50048]	Loss: 0.1009
Training Epoch: 87 [32384/50048]	Loss: 0.0980
Training Epoch: 87 [32512/50048]	Loss: 0.1076
Training Epoch: 87 [32640/50048]	Loss: 0.0567
Training Epoch: 87 [32768/50048]	Loss: 0.1028
Training Epoch: 87 [32896/50048]	Loss: 0.0583
Training Epoch: 87 [33024/50048]	Loss: 0.1130
Training Epoch: 87 [33152/50048]	Loss: 0.0911
Training Epoch: 87 [33280/50048]	Loss: 0.0717
Training Epoch: 87 [33408/50048]	Loss: 0.0524
Training Epoch: 87 [33536/50048]	Loss: 0.0830
Training Epoch: 87 [33664/50048]	Loss: 0.1343
Training Epoch: 87 [33792/50048]	Loss: 0.1278
Training Epoch: 87 [33920/50048]	Loss: 0.0581
Training Epoch: 87 [34048/50048]	Loss: 0.1440
Training Epoch: 87 [34176/50048]	Loss: 0.0933
Training Epoch: 87 [34304/50048]	Loss: 0.1071
Training Epoch: 87 [34432/50048]	Loss: 0.0753
Training Epoch: 87 [34560/50048]	Loss: 0.0918
Training Epoch: 87 [34688/50048]	Loss: 0.0859
Training Epoch: 87 [34816/50048]	Loss: 0.0526
Training Epoch: 87 [34944/50048]	Loss: 0.0902
Training Epoch: 87 [35072/50048]	Loss: 0.1514
Training Epoch: 87 [35200/50048]	Loss: 0.0739
Training Epoch: 87 [35328/50048]	Loss: 0.0330
Training Epoch: 87 [35456/50048]	Loss: 0.0504
Training Epoch: 87 [35584/50048]	Loss: 0.1093
Training Epoch: 87 [35712/50048]	Loss: 0.0750
Training Epoch: 87 [35840/50048]	Loss: 0.0825
Training Epoch: 87 [35968/50048]	Loss: 0.0991
Training Epoch: 87 [36096/50048]	Loss: 0.1484
Training Epoch: 87 [36224/50048]	Loss: 0.1341
Training Epoch: 87 [36352/50048]	Loss: 0.0733
Training Epoch: 87 [36480/50048]	Loss: 0.0788
Training Epoch: 87 [36608/50048]	Loss: 0.0898
Training Epoch: 87 [36736/50048]	Loss: 0.0654
Training Epoch: 87 [36864/50048]	Loss: 0.0872
Training Epoch: 87 [36992/50048]	Loss: 0.0579
Training Epoch: 87 [37120/50048]	Loss: 0.0574
Training Epoch: 87 [37248/50048]	Loss: 0.0463
Training Epoch: 87 [37376/50048]	Loss: 0.0965
Training Epoch: 87 [37504/50048]	Loss: 0.0444
Training Epoch: 87 [37632/50048]	Loss: 0.0659
Training Epoch: 87 [37760/50048]	Loss: 0.1372
Training Epoch: 87 [37888/50048]	Loss: 0.0308
Training Epoch: 87 [38016/50048]	Loss: 0.0729
Training Epoch: 87 [38144/50048]	Loss: 0.1298
Training Epoch: 87 [38272/50048]	Loss: 0.1403
Training Epoch: 87 [38400/50048]	Loss: 0.1028
Training Epoch: 87 [38528/50048]	Loss: 0.0430
Training Epoch: 87 [38656/50048]	Loss: 0.1531
Training Epoch: 87 [38784/50048]	Loss: 0.0955
Training Epoch: 87 [38912/50048]	Loss: 0.1511
Training Epoch: 87 [39040/50048]	Loss: 0.0847
Training Epoch: 87 [39168/50048]	Loss: 0.0636
Training Epoch: 87 [39296/50048]	Loss: 0.1264
Training Epoch: 87 [39424/50048]	Loss: 0.0740
Training Epoch: 87 [39552/50048]	Loss: 0.0616
Training Epoch: 87 [39680/50048]	Loss: 0.0643
Training Epoch: 87 [39808/50048]	Loss: 0.0896
Training Epoch: 87 [39936/50048]	Loss: 0.1053
Training Epoch: 87 [40064/50048]	Loss: 0.1168
Training Epoch: 87 [40192/50048]	Loss: 0.1014
Training Epoch: 87 [40320/50048]	Loss: 0.1635
Training Epoch: 87 [40448/50048]	Loss: 0.1368
Training Epoch: 87 [40576/50048]	Loss: 0.0628
Training Epoch: 87 [40704/50048]	Loss: 0.1465
Training Epoch: 87 [40832/50048]	Loss: 0.0974
Training Epoch: 87 [40960/50048]	Loss: 0.0630
Training Epoch: 87 [41088/50048]	Loss: 0.0916
Training Epoch: 87 [41216/50048]	Loss: 0.0668
Training Epoch: 87 [41344/50048]	Loss: 0.1149
Training Epoch: 87 [41472/50048]	Loss: 0.1119
Training Epoch: 87 [41600/50048]	Loss: 0.1384
Training Epoch: 87 [41728/50048]	Loss: 0.0860
Training Epoch: 87 [41856/50048]	Loss: 0.0399
Training Epoch: 87 [41984/50048]	Loss: 0.1081
Training Epoch: 87 [42112/50048]	Loss: 0.1206
Training Epoch: 87 [42240/50048]	Loss: 0.0971
Training Epoch: 87 [42368/50048]	Loss: 0.1199
Training Epoch: 87 [42496/50048]	Loss: 0.1258
Training Epoch: 87 [42624/50048]	Loss: 0.0917
Training Epoch: 87 [42752/50048]	Loss: 0.1304
Training Epoch: 87 [42880/50048]	Loss: 0.1237
Training Epoch: 87 [43008/50048]	Loss: 0.0940
Training Epoch: 87 [43136/50048]	Loss: 0.1068
Training Epoch: 87 [43264/50048]	Loss: 0.0393
Training Epoch: 87 [43392/50048]	Loss: 0.1072
Training Epoch: 87 [43520/50048]	Loss: 0.1227
Training Epoch: 87 [43648/50048]	Loss: 0.0754
Training Epoch: 87 [43776/50048]	Loss: 0.0489
Training Epoch: 87 [43904/50048]	Loss: 0.0828
Training Epoch: 87 [44032/50048]	Loss: 0.0455
Training Epoch: 87 [44160/50048]	Loss: 0.0853
Training Epoch: 87 [44288/50048]	Loss: 0.1020
Training Epoch: 87 [44416/50048]	Loss: 0.0762
Training Epoch: 87 [44544/50048]	Loss: 0.0441
Training Epoch: 87 [44672/50048]	Loss: 0.1132
Training Epoch: 87 [44800/50048]	Loss: 0.0932
Training Epoch: 87 [44928/50048]	Loss: 0.0776
Training Epoch: 87 [45056/50048]	Loss: 0.0844
Training Epoch: 87 [45184/50048]	Loss: 0.0467
Training Epoch: 87 [45312/50048]	Loss: 0.0504
Training Epoch: 87 [45440/50048]	Loss: 0.0714
Training Epoch: 87 [45568/50048]	Loss: 0.0813
Training Epoch: 87 [45696/50048]	Loss: 0.0855
2022-12-06 05:49:39,668 [ZeusDataLoader(train)] train epoch 88 done: time=86.39 energy=10496.65
2022-12-06 05:49:39,670 [ZeusDataLoader(eval)] Epoch 88 begin.
Training Epoch: 87 [45824/50048]	Loss: 0.0939
Training Epoch: 87 [45952/50048]	Loss: 0.1933
Training Epoch: 87 [46080/50048]	Loss: 0.0676
Training Epoch: 87 [46208/50048]	Loss: 0.0896
Training Epoch: 87 [46336/50048]	Loss: 0.2142
Training Epoch: 87 [46464/50048]	Loss: 0.0758
Training Epoch: 87 [46592/50048]	Loss: 0.1055
Training Epoch: 87 [46720/50048]	Loss: 0.1324
Training Epoch: 87 [46848/50048]	Loss: 0.1009
Training Epoch: 87 [46976/50048]	Loss: 0.1029
Training Epoch: 87 [47104/50048]	Loss: 0.0536
Training Epoch: 87 [47232/50048]	Loss: 0.1385
Training Epoch: 87 [47360/50048]	Loss: 0.0784
Training Epoch: 87 [47488/50048]	Loss: 0.0629
Training Epoch: 87 [47616/50048]	Loss: 0.1337
Training Epoch: 87 [47744/50048]	Loss: 0.0794
Training Epoch: 87 [47872/50048]	Loss: 0.1926
Training Epoch: 87 [48000/50048]	Loss: 0.0872
Training Epoch: 87 [48128/50048]	Loss: 0.0583
Training Epoch: 87 [48256/50048]	Loss: 0.0910
Training Epoch: 87 [48384/50048]	Loss: 0.0843
Training Epoch: 87 [48512/50048]	Loss: 0.0879
Training Epoch: 87 [48640/50048]	Loss: 0.1328
Training Epoch: 87 [48768/50048]	Loss: 0.1050
Training Epoch: 87 [48896/50048]	Loss: 0.0635
Training Epoch: 87 [49024/50048]	Loss: 0.0901
Training Epoch: 87 [49152/50048]	Loss: 0.0580
Training Epoch: 87 [49280/50048]	Loss: 0.1101
Training Epoch: 87 [49408/50048]	Loss: 0.0812
Training Epoch: 87 [49536/50048]	Loss: 0.1126
Training Epoch: 87 [49664/50048]	Loss: 0.1084
Training Epoch: 87 [49792/50048]	Loss: 0.1391
Training Epoch: 87 [49920/50048]	Loss: 0.0926
Training Epoch: 87 [50048/50048]	Loss: 0.1384
2022-12-06 10:49:43.358 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:49:43,388 [ZeusDataLoader(eval)] eval epoch 88 done: time=3.71 energy=458.29
2022-12-06 05:49:43,388 [ZeusDataLoader(train)] Up to epoch 88: time=7939.19, energy=963777.79, cost=1176568.28
2022-12-06 05:49:43,388 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:49:43,388 [ZeusDataLoader(train)] Expected next epoch: time=8028.99, energy=974575.81, cost=1189824.66
2022-12-06 05:49:43,389 [ZeusDataLoader(train)] Epoch 89 begin.
Validation Epoch: 87, Average loss: 0.0184, Accuracy: 0.6437
2022-12-06 05:49:43,564 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:49:43,564 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:49:43.566 [ZeusMonitor] Monitor started.
2022-12-06 10:49:43.566 [ZeusMonitor] Running indefinitely. 2022-12-06 10:49:43.566 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:49:43.566 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e89+gpu0.power.log
Training Epoch: 88 [128/50048]	Loss: 0.0616
Training Epoch: 88 [256/50048]	Loss: 0.0753
Training Epoch: 88 [384/50048]	Loss: 0.0817
Training Epoch: 88 [512/50048]	Loss: 0.0330
Training Epoch: 88 [640/50048]	Loss: 0.1135
Training Epoch: 88 [768/50048]	Loss: 0.1118
Training Epoch: 88 [896/50048]	Loss: 0.0889
Training Epoch: 88 [1024/50048]	Loss: 0.1022
Training Epoch: 88 [1152/50048]	Loss: 0.0253
Training Epoch: 88 [1280/50048]	Loss: 0.0692
Training Epoch: 88 [1408/50048]	Loss: 0.0854
Training Epoch: 88 [1536/50048]	Loss: 0.1728
Training Epoch: 88 [1664/50048]	Loss: 0.0989
Training Epoch: 88 [1792/50048]	Loss: 0.0533
Training Epoch: 88 [1920/50048]	Loss: 0.0949
Training Epoch: 88 [2048/50048]	Loss: 0.0986
Training Epoch: 88 [2176/50048]	Loss: 0.0653
Training Epoch: 88 [2304/50048]	Loss: 0.1169
Training Epoch: 88 [2432/50048]	Loss: 0.1523
Training Epoch: 88 [2560/50048]	Loss: 0.0678
Training Epoch: 88 [2688/50048]	Loss: 0.0789
Training Epoch: 88 [2816/50048]	Loss: 0.0905
Training Epoch: 88 [2944/50048]	Loss: 0.0772
Training Epoch: 88 [3072/50048]	Loss: 0.0679
Training Epoch: 88 [3200/50048]	Loss: 0.1643
Training Epoch: 88 [3328/50048]	Loss: 0.0834
Training Epoch: 88 [3456/50048]	Loss: 0.1346
Training Epoch: 88 [3584/50048]	Loss: 0.1473
Training Epoch: 88 [3712/50048]	Loss: 0.0586
Training Epoch: 88 [3840/50048]	Loss: 0.0645
Training Epoch: 88 [3968/50048]	Loss: 0.1613
Training Epoch: 88 [4096/50048]	Loss: 0.0630
Training Epoch: 88 [4224/50048]	Loss: 0.0604
Training Epoch: 88 [4352/50048]	Loss: 0.0263
Training Epoch: 88 [4480/50048]	Loss: 0.1077
Training Epoch: 88 [4608/50048]	Loss: 0.0802
Training Epoch: 88 [4736/50048]	Loss: 0.0754
Training Epoch: 88 [4864/50048]	Loss: 0.0998
Training Epoch: 88 [4992/50048]	Loss: 0.0803
Training Epoch: 88 [5120/50048]	Loss: 0.1014
Training Epoch: 88 [5248/50048]	Loss: 0.0856
Training Epoch: 88 [5376/50048]	Loss: 0.0774
Training Epoch: 88 [5504/50048]	Loss: 0.0521
Training Epoch: 88 [5632/50048]	Loss: 0.0784
Training Epoch: 88 [5760/50048]	Loss: 0.1185
Training Epoch: 88 [5888/50048]	Loss: 0.0564
Training Epoch: 88 [6016/50048]	Loss: 0.0758
Training Epoch: 88 [6144/50048]	Loss: 0.1071
Training Epoch: 88 [6272/50048]	Loss: 0.0467
Training Epoch: 88 [6400/50048]	Loss: 0.1527
Training Epoch: 88 [6528/50048]	Loss: 0.0625
Training Epoch: 88 [6656/50048]	Loss: 0.0691
Training Epoch: 88 [6784/50048]	Loss: 0.1408
Training Epoch: 88 [6912/50048]	Loss: 0.0404
Training Epoch: 88 [7040/50048]	Loss: 0.0671
Training Epoch: 88 [7168/50048]	Loss: 0.1217
Training Epoch: 88 [7296/50048]	Loss: 0.1382
Training Epoch: 88 [7424/50048]	Loss: 0.1004
Training Epoch: 88 [7552/50048]	Loss: 0.1159
Training Epoch: 88 [7680/50048]	Loss: 0.0419
Training Epoch: 88 [7808/50048]	Loss: 0.1349
Training Epoch: 88 [7936/50048]	Loss: 0.1049
Training Epoch: 88 [8064/50048]	Loss: 0.0704
Training Epoch: 88 [8192/50048]	Loss: 0.0417
Training Epoch: 88 [8320/50048]	Loss: 0.0558
Training Epoch: 88 [8448/50048]	Loss: 0.1171
Training Epoch: 88 [8576/50048]	Loss: 0.1320
Training Epoch: 88 [8704/50048]	Loss: 0.0825
Training Epoch: 88 [8832/50048]	Loss: 0.0230
Training Epoch: 88 [8960/50048]	Loss: 0.0709
Training Epoch: 88 [9088/50048]	Loss: 0.0789
Training Epoch: 88 [9216/50048]	Loss: 0.1857
Training Epoch: 88 [9344/50048]	Loss: 0.1571
Training Epoch: 88 [9472/50048]	Loss: 0.0620
Training Epoch: 88 [9600/50048]	Loss: 0.1088
Training Epoch: 88 [9728/50048]	Loss: 0.1313
Training Epoch: 88 [9856/50048]	Loss: 0.0437
Training Epoch: 88 [9984/50048]	Loss: 0.0797
Training Epoch: 88 [10112/50048]	Loss: 0.0422
Training Epoch: 88 [10240/50048]	Loss: 0.0371
Training Epoch: 88 [10368/50048]	Loss: 0.0910
Training Epoch: 88 [10496/50048]	Loss: 0.0778
Training Epoch: 88 [10624/50048]	Loss: 0.1027
Training Epoch: 88 [10752/50048]	Loss: 0.1237
Training Epoch: 88 [10880/50048]	Loss: 0.0914
Training Epoch: 88 [11008/50048]	Loss: 0.0585
Training Epoch: 88 [11136/50048]	Loss: 0.0944
Training Epoch: 88 [11264/50048]	Loss: 0.0603
Training Epoch: 88 [11392/50048]	Loss: 0.0412
Training Epoch: 88 [11520/50048]	Loss: 0.1605
Training Epoch: 88 [11648/50048]	Loss: 0.0817
Training Epoch: 88 [11776/50048]	Loss: 0.1037
Training Epoch: 88 [11904/50048]	Loss: 0.1230
Training Epoch: 88 [12032/50048]	Loss: 0.0646
Training Epoch: 88 [12160/50048]	Loss: 0.0228
Training Epoch: 88 [12288/50048]	Loss: 0.0975
Training Epoch: 88 [12416/50048]	Loss: 0.1075
Training Epoch: 88 [12544/50048]	Loss: 0.1002
Training Epoch: 88 [12672/50048]	Loss: 0.1295
Training Epoch: 88 [12800/50048]	Loss: 0.0332
Training Epoch: 88 [12928/50048]	Loss: 0.1203
Training Epoch: 88 [13056/50048]	Loss: 0.1046
Training Epoch: 88 [13184/50048]	Loss: 0.1332
Training Epoch: 88 [13312/50048]	Loss: 0.0498
Training Epoch: 88 [13440/50048]	Loss: 0.1842
Training Epoch: 88 [13568/50048]	Loss: 0.1355
Training Epoch: 88 [13696/50048]	Loss: 0.1158
Training Epoch: 88 [13824/50048]	Loss: 0.1363
Training Epoch: 88 [13952/50048]	Loss: 0.1171
Training Epoch: 88 [14080/50048]	Loss: 0.0614
Training Epoch: 88 [14208/50048]	Loss: 0.0615
Training Epoch: 88 [14336/50048]	Loss: 0.0877
Training Epoch: 88 [14464/50048]	Loss: 0.1135
Training Epoch: 88 [14592/50048]	Loss: 0.0844
Training Epoch: 88 [14720/50048]	Loss: 0.1540
Training Epoch: 88 [14848/50048]	Loss: 0.0573
Training Epoch: 88 [14976/50048]	Loss: 0.0697
Training Epoch: 88 [15104/50048]	Loss: 0.1694
Training Epoch: 88 [15232/50048]	Loss: 0.0629
Training Epoch: 88 [15360/50048]	Loss: 0.1037
Training Epoch: 88 [15488/50048]	Loss: 0.0870
Training Epoch: 88 [15616/50048]	Loss: 0.0445
Training Epoch: 88 [15744/50048]	Loss: 0.0796
Training Epoch: 88 [15872/50048]	Loss: 0.1266
Training Epoch: 88 [16000/50048]	Loss: 0.0626
Training Epoch: 88 [16128/50048]	Loss: 0.0709
Training Epoch: 88 [16256/50048]	Loss: 0.1447
Training Epoch: 88 [16384/50048]	Loss: 0.1396
Training Epoch: 88 [16512/50048]	Loss: 0.0608
Training Epoch: 88 [16640/50048]	Loss: 0.0917
Training Epoch: 88 [16768/50048]	Loss: 0.1800
Training Epoch: 88 [16896/50048]	Loss: 0.0627
Training Epoch: 88 [17024/50048]	Loss: 0.1477
Training Epoch: 88 [17152/50048]	Loss: 0.0967
Training Epoch: 88 [17280/50048]	Loss: 0.0844
Training Epoch: 88 [17408/50048]	Loss: 0.1077
Training Epoch: 88 [17536/50048]	Loss: 0.0577
Training Epoch: 88 [17664/50048]	Loss: 0.1617
Training Epoch: 88 [17792/50048]	Loss: 0.0745
Training Epoch: 88 [17920/50048]	Loss: 0.1015
Training Epoch: 88 [18048/50048]	Loss: 0.0999
Training Epoch: 88 [18176/50048]	Loss: 0.0966
Training Epoch: 88 [18304/50048]	Loss: 0.1362
Training Epoch: 88 [18432/50048]	Loss: 0.0967
Training Epoch: 88 [18560/50048]	Loss: 0.0274
Training Epoch: 88 [18688/50048]	Loss: 0.0368
Training Epoch: 88 [18816/50048]	Loss: 0.0481
Training Epoch: 88 [18944/50048]	Loss: 0.1344
Training Epoch: 88 [19072/50048]	Loss: 0.0940
Training Epoch: 88 [19200/50048]	Loss: 0.0862
Training Epoch: 88 [19328/50048]	Loss: 0.0410
Training Epoch: 88 [19456/50048]	Loss: 0.0816
Training Epoch: 88 [19584/50048]	Loss: 0.1190
Training Epoch: 88 [19712/50048]	Loss: 0.0700
Training Epoch: 88 [19840/50048]	Loss: 0.1270
Training Epoch: 88 [19968/50048]	Loss: 0.0889
Training Epoch: 88 [20096/50048]	Loss: 0.1078
Training Epoch: 88 [20224/50048]	Loss: 0.0581
Training Epoch: 88 [20352/50048]	Loss: 0.1234
Training Epoch: 88 [20480/50048]	Loss: 0.0798
Training Epoch: 88 [20608/50048]	Loss: 0.0963
Training Epoch: 88 [20736/50048]	Loss: 0.0275
Training Epoch: 88 [20864/50048]	Loss: 0.0509
Training Epoch: 88 [20992/50048]	Loss: 0.0688
Training Epoch: 88 [21120/50048]	Loss: 0.0398
Training Epoch: 88 [21248/50048]	Loss: 0.0583
Training Epoch: 88 [21376/50048]	Loss: 0.0762
Training Epoch: 88 [21504/50048]	Loss: 0.0652
Training Epoch: 88 [21632/50048]	Loss: 0.0913
Training Epoch: 88 [21760/50048]	Loss: 0.0619
Training Epoch: 88 [21888/50048]	Loss: 0.1842
Training Epoch: 88 [22016/50048]	Loss: 0.0946
Training Epoch: 88 [22144/50048]	Loss: 0.0618
Training Epoch: 88 [22272/50048]	Loss: 0.1173
Training Epoch: 88 [22400/50048]	Loss: 0.0788
Training Epoch: 88 [22528/50048]	Loss: 0.0666
Training Epoch: 88 [22656/50048]	Loss: 0.0492
Training Epoch: 88 [22784/50048]	Loss: 0.0727
Training Epoch: 88 [22912/50048]	Loss: 0.1062
Training Epoch: 88 [23040/50048]	Loss: 0.1023
Training Epoch: 88 [23168/50048]	Loss: 0.0492
Training Epoch: 88 [23296/50048]	Loss: 0.1935
Training Epoch: 88 [23424/50048]	Loss: 0.0743
Training Epoch: 88 [23552/50048]	Loss: 0.0940
Training Epoch: 88 [23680/50048]	Loss: 0.0911
Training Epoch: 88 [23808/50048]	Loss: 0.0938
Training Epoch: 88 [23936/50048]	Loss: 0.1208
Training Epoch: 88 [24064/50048]	Loss: 0.0868
Training Epoch: 88 [24192/50048]	Loss: 0.1316
Training Epoch: 88 [24320/50048]	Loss: 0.0978
Training Epoch: 88 [24448/50048]	Loss: 0.1237
Training Epoch: 88 [24576/50048]	Loss: 0.0883
Training Epoch: 88 [24704/50048]	Loss: 0.0534
Training Epoch: 88 [24832/50048]	Loss: 0.0507
Training Epoch: 88 [24960/50048]	Loss: 0.0512
Training Epoch: 88 [25088/50048]	Loss: 0.0896
Training Epoch: 88 [25216/50048]	Loss: 0.1391
Training Epoch: 88 [25344/50048]	Loss: 0.0427
Training Epoch: 88 [25472/50048]	Loss: 0.1504
Training Epoch: 88 [25600/50048]	Loss: 0.0815
Training Epoch: 88 [25728/50048]	Loss: 0.0849
Training Epoch: 88 [25856/50048]	Loss: 0.0630
Training Epoch: 88 [25984/50048]	Loss: 0.0512
Training Epoch: 88 [26112/50048]	Loss: 0.0430
Training Epoch: 88 [26240/50048]	Loss: 0.1017
Training Epoch: 88 [26368/50048]	Loss: 0.1289
Training Epoch: 88 [26496/50048]	Loss: 0.1160
Training Epoch: 88 [26624/50048]	Loss: 0.1507
Training Epoch: 88 [26752/50048]	Loss: 0.0542
Training Epoch: 88 [26880/50048]	Loss: 0.1021
Training Epoch: 88 [27008/50048]	Loss: 0.1498
Training Epoch: 88 [27136/50048]	Loss: 0.0823
Training Epoch: 88 [27264/50048]	Loss: 0.0694
Training Epoch: 88 [27392/50048]	Loss: 0.0858
Training Epoch: 88 [27520/50048]	Loss: 0.1357
Training Epoch: 88 [27648/50048]	Loss: 0.0538
Training Epoch: 88 [27776/50048]	Loss: 0.1033
Training Epoch: 88 [27904/50048]	Loss: 0.1336
Training Epoch: 88 [28032/50048]	Loss: 0.0657
Training Epoch: 88 [28160/50048]	Loss: 0.1093
Training Epoch: 88 [28288/50048]	Loss: 0.1045
Training Epoch: 88 [28416/50048]	Loss: 0.0537
Training Epoch: 88 [28544/50048]	Loss: 0.0853
Training Epoch: 88 [28672/50048]	Loss: 0.0910
Training Epoch: 88 [28800/50048]	Loss: 0.1132
Training Epoch: 88 [28928/50048]	Loss: 0.0782
Training Epoch: 88 [29056/50048]	Loss: 0.2500
Training Epoch: 88 [29184/50048]	Loss: 0.1338
Training Epoch: 88 [29312/50048]	Loss: 0.0987
Training Epoch: 88 [29440/50048]	Loss: 0.1364
Training Epoch: 88 [29568/50048]	Loss: 0.0863
Training Epoch: 88 [29696/50048]	Loss: 0.0424
Training Epoch: 88 [29824/50048]	Loss: 0.0343
Training Epoch: 88 [29952/50048]	Loss: 0.1330
Training Epoch: 88 [30080/50048]	Loss: 0.1016
Training Epoch: 88 [30208/50048]	Loss: 0.0496
Training Epoch: 88 [30336/50048]	Loss: 0.1297
Training Epoch: 88 [30464/50048]	Loss: 0.1426
Training Epoch: 88 [30592/50048]	Loss: 0.0423
Training Epoch: 88 [30720/50048]	Loss: 0.0678
Training Epoch: 88 [30848/50048]	Loss: 0.1055
Training Epoch: 88 [30976/50048]	Loss: 0.1857
Training Epoch: 88 [31104/50048]	Loss: 0.0459
Training Epoch: 88 [31232/50048]	Loss: 0.1742
Training Epoch: 88 [31360/50048]	Loss: 0.1449
Training Epoch: 88 [31488/50048]	Loss: 0.0543
Training Epoch: 88 [31616/50048]	Loss: 0.1074
Training Epoch: 88 [31744/50048]	Loss: 0.1024
Training Epoch: 88 [31872/50048]	Loss: 0.1108
Training Epoch: 88 [32000/50048]	Loss: 0.1372
Training Epoch: 88 [32128/50048]	Loss: 0.1184
Training Epoch: 88 [32256/50048]	Loss: 0.0510
Training Epoch: 88 [32384/50048]	Loss: 0.0902
Training Epoch: 88 [32512/50048]	Loss: 0.0552
Training Epoch: 88 [32640/50048]	Loss: 0.1312
Training Epoch: 88 [32768/50048]	Loss: 0.1090
Training Epoch: 88 [32896/50048]	Loss: 0.0809
Training Epoch: 88 [33024/50048]	Loss: 0.1233
Training Epoch: 88 [33152/50048]	Loss: 0.1507
Training Epoch: 88 [33280/50048]	Loss: 0.1357
Training Epoch: 88 [33408/50048]	Loss: 0.1194
Training Epoch: 88 [33536/50048]	Loss: 0.0587
Training Epoch: 88 [33664/50048]	Loss: 0.0361
Training Epoch: 88 [33792/50048]	Loss: 0.1063
Training Epoch: 88 [33920/50048]	Loss: 0.0451
Training Epoch: 88 [34048/50048]	Loss: 0.0592
Training Epoch: 88 [34176/50048]	Loss: 0.1147
Training Epoch: 88 [34304/50048]	Loss: 0.1498
Training Epoch: 88 [34432/50048]	Loss: 0.1544
Training Epoch: 88 [34560/50048]	Loss: 0.0501
Training Epoch: 88 [34688/50048]	Loss: 0.0633
Training Epoch: 88 [34816/50048]	Loss: 0.0873
Training Epoch: 88 [34944/50048]	Loss: 0.1816
Training Epoch: 88 [35072/50048]	Loss: 0.2983
Training Epoch: 88 [35200/50048]	Loss: 0.0492
Training Epoch: 88 [35328/50048]	Loss: 0.1371
Training Epoch: 88 [35456/50048]	Loss: 0.1037
Training Epoch: 88 [35584/50048]	Loss: 0.1427
Training Epoch: 88 [35712/50048]	Loss: 0.1087
Training Epoch: 88 [35840/50048]	Loss: 0.0846
Training Epoch: 88 [35968/50048]	Loss: 0.1040
Training Epoch: 88 [36096/50048]	Loss: 0.0716
Training Epoch: 88 [36224/50048]	Loss: 0.1305
Training Epoch: 88 [36352/50048]	Loss: 0.1471
Training Epoch: 88 [36480/50048]	Loss: 0.1585
Training Epoch: 88 [36608/50048]	Loss: 0.0473
Training Epoch: 88 [36736/50048]	Loss: 0.0690
Training Epoch: 88 [36864/50048]	Loss: 0.1575
Training Epoch: 88 [36992/50048]	Loss: 0.0476
Training Epoch: 88 [37120/50048]	Loss: 0.0926
Training Epoch: 88 [37248/50048]	Loss: 0.0707
Training Epoch: 88 [37376/50048]	Loss: 0.0446
Training Epoch: 88 [37504/50048]	Loss: 0.1378
Training Epoch: 88 [37632/50048]	Loss: 0.1306
Training Epoch: 88 [37760/50048]	Loss: 0.1231
Training Epoch: 88 [37888/50048]	Loss: 0.0911
Training Epoch: 88 [38016/50048]	Loss: 0.0659
Training Epoch: 88 [38144/50048]	Loss: 0.0942
Training Epoch: 88 [38272/50048]	Loss: 0.1416
Training Epoch: 88 [38400/50048]	Loss: 0.1369
Training Epoch: 88 [38528/50048]	Loss: 0.1230
Training Epoch: 88 [38656/50048]	Loss: 0.0958
Training Epoch: 88 [38784/50048]	Loss: 0.0398
Training Epoch: 88 [38912/50048]	Loss: 0.1123
Training Epoch: 88 [39040/50048]	Loss: 0.0699
Training Epoch: 88 [39168/50048]	Loss: 0.0574
Training Epoch: 88 [39296/50048]	Loss: 0.0696
Training Epoch: 88 [39424/50048]	Loss: 0.1112
Training Epoch: 88 [39552/50048]	Loss: 0.0622
Training Epoch: 88 [39680/50048]	Loss: 0.1023
Training Epoch: 88 [39808/50048]	Loss: 0.1435
Training Epoch: 88 [39936/50048]	Loss: 0.1330
Training Epoch: 88 [40064/50048]	Loss: 0.0681
Training Epoch: 88 [40192/50048]	Loss: 0.1203
Training Epoch: 88 [40320/50048]	Loss: 0.0643
Training Epoch: 88 [40448/50048]	Loss: 0.0997
Training Epoch: 88 [40576/50048]	Loss: 0.0867
Training Epoch: 88 [40704/50048]	Loss: 0.0767
Training Epoch: 88 [40832/50048]	Loss: 0.0572
Training Epoch: 88 [40960/50048]	Loss: 0.1333
Training Epoch: 88 [41088/50048]	Loss: 0.0553
Training Epoch: 88 [41216/50048]	Loss: 0.1011
Training Epoch: 88 [41344/50048]	Loss: 0.1030
Training Epoch: 88 [41472/50048]	Loss: 0.1176
Training Epoch: 88 [41600/50048]	Loss: 0.0422
Training Epoch: 88 [41728/50048]	Loss: 0.1153
Training Epoch: 88 [41856/50048]	Loss: 0.0661
Training Epoch: 88 [41984/50048]	Loss: 0.1213
Training Epoch: 88 [42112/50048]	Loss: 0.1570
Training Epoch: 88 [42240/50048]	Loss: 0.0852
Training Epoch: 88 [42368/50048]	Loss: 0.0874
Training Epoch: 88 [42496/50048]	Loss: 0.1183
Training Epoch: 88 [42624/50048]	Loss: 0.0747
Training Epoch: 88 [42752/50048]	Loss: 0.1468
Training Epoch: 88 [42880/50048]	Loss: 0.0843
Training Epoch: 88 [43008/50048]	Loss: 0.0890
Training Epoch: 88 [43136/50048]	Loss: 0.0725
Training Epoch: 88 [43264/50048]	Loss: 0.0983
Training Epoch: 88 [43392/50048]	Loss: 0.1040
Training Epoch: 88 [43520/50048]	Loss: 0.1224
Training Epoch: 88 [43648/50048]	Loss: 0.0730
Training Epoch: 88 [43776/50048]	Loss: 0.0455
Training Epoch: 88 [43904/50048]	Loss: 0.1105
Training Epoch: 88 [44032/50048]	Loss: 0.0697
Training Epoch: 88 [44160/50048]	Loss: 0.1274
Training Epoch: 88 [44288/50048]	Loss: 0.0938
Training Epoch: 88 [44416/50048]	Loss: 0.0663
Training Epoch: 88 [44544/50048]	Loss: 0.0905
Training Epoch: 88 [44672/50048]	Loss: 0.0413
Training Epoch: 88 [44800/50048]	Loss: 0.0827
Training Epoch: 88 [44928/50048]	Loss: 0.0865
Training Epoch: 88 [45056/50048]	Loss: 0.0852
Training Epoch: 88 [45184/50048]	Loss: 0.0777
Training Epoch: 88 [45312/50048]	Loss: 0.1001
Training Epoch: 88 [45440/50048]	Loss: 0.0946
Training Epoch: 88 [45568/50048]	Loss: 0.0628
Training Epoch: 88 [45696/50048]	Loss: 0.1290
2022-12-06 05:51:09,822 [ZeusDataLoader(train)] train epoch 89 done: time=86.42 energy=10511.65
2022-12-06 05:51:09,824 [ZeusDataLoader(eval)] Epoch 89 begin.
Training Epoch: 88 [45824/50048]	Loss: 0.0921
Training Epoch: 88 [45952/50048]	Loss: 0.1098
Training Epoch: 88 [46080/50048]	Loss: 0.2092
Training Epoch: 88 [46208/50048]	Loss: 0.0664
Training Epoch: 88 [46336/50048]	Loss: 0.2373
Training Epoch: 88 [46464/50048]	Loss: 0.1278
Training Epoch: 88 [46592/50048]	Loss: 0.1554
Training Epoch: 88 [46720/50048]	Loss: 0.0525
Training Epoch: 88 [46848/50048]	Loss: 0.1420
Training Epoch: 88 [46976/50048]	Loss: 0.0588
Training Epoch: 88 [47104/50048]	Loss: 0.0866
Training Epoch: 88 [47232/50048]	Loss: 0.0628
Training Epoch: 88 [47360/50048]	Loss: 0.0926
Training Epoch: 88 [47488/50048]	Loss: 0.0595
Training Epoch: 88 [47616/50048]	Loss: 0.1058
Training Epoch: 88 [47744/50048]	Loss: 0.1600
Training Epoch: 88 [47872/50048]	Loss: 0.0756
Training Epoch: 88 [48000/50048]	Loss: 0.1199
Training Epoch: 88 [48128/50048]	Loss: 0.1432
Training Epoch: 88 [48256/50048]	Loss: 0.0863
Training Epoch: 88 [48384/50048]	Loss: 0.0536
Training Epoch: 88 [48512/50048]	Loss: 0.1253
Training Epoch: 88 [48640/50048]	Loss: 0.0894
Training Epoch: 88 [48768/50048]	Loss: 0.1038
Training Epoch: 88 [48896/50048]	Loss: 0.0319
Training Epoch: 88 [49024/50048]	Loss: 0.0723
Training Epoch: 88 [49152/50048]	Loss: 0.0501
Training Epoch: 88 [49280/50048]	Loss: 0.1587
Training Epoch: 88 [49408/50048]	Loss: 0.0724
Training Epoch: 88 [49536/50048]	Loss: 0.0492
Training Epoch: 88 [49664/50048]	Loss: 0.1177
Training Epoch: 88 [49792/50048]	Loss: 0.1174
Training Epoch: 88 [49920/50048]	Loss: 0.1003
Training Epoch: 88 [50048/50048]	Loss: 0.2230
2022-12-06 10:51:13.493 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:51:13,532 [ZeusDataLoader(eval)] eval epoch 89 done: time=3.70 energy=453.61
2022-12-06 05:51:13,533 [ZeusDataLoader(train)] Up to epoch 89: time=8029.31, energy=974743.05, cost=1189936.56
2022-12-06 05:51:13,533 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:51:13,533 [ZeusDataLoader(train)] Expected next epoch: time=8119.11, energy=985541.06, cost=1203192.94
2022-12-06 05:51:13,534 [ZeusDataLoader(train)] Epoch 90 begin.
Validation Epoch: 88, Average loss: 0.0183, Accuracy: 0.6433
2022-12-06 05:51:13,725 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:51:13,726 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:51:13.728 [ZeusMonitor] Monitor started.
2022-12-06 10:51:13.728 [ZeusMonitor] Running indefinitely. 2022-12-06 10:51:13.728 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:51:13.728 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e90+gpu0.power.log
Training Epoch: 89 [128/50048]	Loss: 0.1105
Training Epoch: 89 [256/50048]	Loss: 0.1135
Training Epoch: 89 [384/50048]	Loss: 0.0751
Training Epoch: 89 [512/50048]	Loss: 0.0822
Training Epoch: 89 [640/50048]	Loss: 0.0421
Training Epoch: 89 [768/50048]	Loss: 0.0704
Training Epoch: 89 [896/50048]	Loss: 0.1010
Training Epoch: 89 [1024/50048]	Loss: 0.0564
Training Epoch: 89 [1152/50048]	Loss: 0.0964
Training Epoch: 89 [1280/50048]	Loss: 0.0483
Training Epoch: 89 [1408/50048]	Loss: 0.1452
Training Epoch: 89 [1536/50048]	Loss: 0.0642
Training Epoch: 89 [1664/50048]	Loss: 0.1037
Training Epoch: 89 [1792/50048]	Loss: 0.0365
Training Epoch: 89 [1920/50048]	Loss: 0.1240
Training Epoch: 89 [2048/50048]	Loss: 0.1195
Training Epoch: 89 [2176/50048]	Loss: 0.0747
Training Epoch: 89 [2304/50048]	Loss: 0.0332
Training Epoch: 89 [2432/50048]	Loss: 0.0985
Training Epoch: 89 [2560/50048]	Loss: 0.0866
Training Epoch: 89 [2688/50048]	Loss: 0.0914
Training Epoch: 89 [2816/50048]	Loss: 0.0908
Training Epoch: 89 [2944/50048]	Loss: 0.0662
Training Epoch: 89 [3072/50048]	Loss: 0.0677
Training Epoch: 89 [3200/50048]	Loss: 0.0359
Training Epoch: 89 [3328/50048]	Loss: 0.0348
Training Epoch: 89 [3456/50048]	Loss: 0.1235
Training Epoch: 89 [3584/50048]	Loss: 0.0407
Training Epoch: 89 [3712/50048]	Loss: 0.1580
Training Epoch: 89 [3840/50048]	Loss: 0.0656
Training Epoch: 89 [3968/50048]	Loss: 0.1088
Training Epoch: 89 [4096/50048]	Loss: 0.1240
Training Epoch: 89 [4224/50048]	Loss: 0.1085
Training Epoch: 89 [4352/50048]	Loss: 0.0396
Training Epoch: 89 [4480/50048]	Loss: 0.0721
Training Epoch: 89 [4608/50048]	Loss: 0.1734
Training Epoch: 89 [4736/50048]	Loss: 0.0569
Training Epoch: 89 [4864/50048]	Loss: 0.0727
Training Epoch: 89 [4992/50048]	Loss: 0.0816
Training Epoch: 89 [5120/50048]	Loss: 0.0893
Training Epoch: 89 [5248/50048]	Loss: 0.1011
Training Epoch: 89 [5376/50048]	Loss: 0.0542
Training Epoch: 89 [5504/50048]	Loss: 0.1256
Training Epoch: 89 [5632/50048]	Loss: 0.0653
Training Epoch: 89 [5760/50048]	Loss: 0.1207
Training Epoch: 89 [5888/50048]	Loss: 0.0566
Training Epoch: 89 [6016/50048]	Loss: 0.0587
Training Epoch: 89 [6144/50048]	Loss: 0.1757
Training Epoch: 89 [6272/50048]	Loss: 0.1430
Training Epoch: 89 [6400/50048]	Loss: 0.0760
Training Epoch: 89 [6528/50048]	Loss: 0.0909
Training Epoch: 89 [6656/50048]	Loss: 0.0675
Training Epoch: 89 [6784/50048]	Loss: 0.0795
Training Epoch: 89 [6912/50048]	Loss: 0.0286
Training Epoch: 89 [7040/50048]	Loss: 0.0470
Training Epoch: 89 [7168/50048]	Loss: 0.0810
Training Epoch: 89 [7296/50048]	Loss: 0.0477
Training Epoch: 89 [7424/50048]	Loss: 0.0684
Training Epoch: 89 [7552/50048]	Loss: 0.1413
Training Epoch: 89 [7680/50048]	Loss: 0.0704
Training Epoch: 89 [7808/50048]	Loss: 0.0798
Training Epoch: 89 [7936/50048]	Loss: 0.0607
Training Epoch: 89 [8064/50048]	Loss: 0.0719
Training Epoch: 89 [8192/50048]	Loss: 0.0572
Training Epoch: 89 [8320/50048]	Loss: 0.0779
Training Epoch: 89 [8448/50048]	Loss: 0.0878
Training Epoch: 89 [8576/50048]	Loss: 0.0843
Training Epoch: 89 [8704/50048]	Loss: 0.0710
Training Epoch: 89 [8832/50048]	Loss: 0.0865
Training Epoch: 89 [8960/50048]	Loss: 0.0384
Training Epoch: 89 [9088/50048]	Loss: 0.0894
Training Epoch: 89 [9216/50048]	Loss: 0.1028
Training Epoch: 89 [9344/50048]	Loss: 0.0798
Training Epoch: 89 [9472/50048]	Loss: 0.0832
Training Epoch: 89 [9600/50048]	Loss: 0.0207
Training Epoch: 89 [9728/50048]	Loss: 0.0228
Training Epoch: 89 [9856/50048]	Loss: 0.1092
Training Epoch: 89 [9984/50048]	Loss: 0.0781
Training Epoch: 89 [10112/50048]	Loss: 0.1247
Training Epoch: 89 [10240/50048]	Loss: 0.0841
Training Epoch: 89 [10368/50048]	Loss: 0.0714
Training Epoch: 89 [10496/50048]	Loss: 0.0589
Training Epoch: 89 [10624/50048]	Loss: 0.0548
Training Epoch: 89 [10752/50048]	Loss: 0.0460
Training Epoch: 89 [10880/50048]	Loss: 0.0512
Training Epoch: 89 [11008/50048]	Loss: 0.0519
Training Epoch: 89 [11136/50048]	Loss: 0.0999
Training Epoch: 89 [11264/50048]	Loss: 0.0805
Training Epoch: 89 [11392/50048]	Loss: 0.0883
Training Epoch: 89 [11520/50048]	Loss: 0.0647
Training Epoch: 89 [11648/50048]	Loss: 0.0980
Training Epoch: 89 [11776/50048]	Loss: 0.0424
Training Epoch: 89 [11904/50048]	Loss: 0.1171
Training Epoch: 89 [12032/50048]	Loss: 0.1003
Training Epoch: 89 [12160/50048]	Loss: 0.0722
Training Epoch: 89 [12288/50048]	Loss: 0.0830
Training Epoch: 89 [12416/50048]	Loss: 0.0792
Training Epoch: 89 [12544/50048]	Loss: 0.2091
Training Epoch: 89 [12672/50048]	Loss: 0.0319
Training Epoch: 89 [12800/50048]	Loss: 0.1286
Training Epoch: 89 [12928/50048]	Loss: 0.1312
Training Epoch: 89 [13056/50048]	Loss: 0.0439
Training Epoch: 89 [13184/50048]	Loss: 0.0342
Training Epoch: 89 [13312/50048]	Loss: 0.0518
Training Epoch: 89 [13440/50048]	Loss: 0.0476
Training Epoch: 89 [13568/50048]	Loss: 0.1185
Training Epoch: 89 [13696/50048]	Loss: 0.0836
Training Epoch: 89 [13824/50048]	Loss: 0.1194
Training Epoch: 89 [13952/50048]	Loss: 0.0997
Training Epoch: 89 [14080/50048]	Loss: 0.0980
Training Epoch: 89 [14208/50048]	Loss: 0.0610
Training Epoch: 89 [14336/50048]	Loss: 0.0501
Training Epoch: 89 [14464/50048]	Loss: 0.1124
Training Epoch: 89 [14592/50048]	Loss: 0.0623
Training Epoch: 89 [14720/50048]	Loss: 0.0871
Training Epoch: 89 [14848/50048]	Loss: 0.0855
Training Epoch: 89 [14976/50048]	Loss: 0.1000
Training Epoch: 89 [15104/50048]	Loss: 0.0603
Training Epoch: 89 [15232/50048]	Loss: 0.0905
Training Epoch: 89 [15360/50048]	Loss: 0.0482
Training Epoch: 89 [15488/50048]	Loss: 0.1242
Training Epoch: 89 [15616/50048]	Loss: 0.1175
Training Epoch: 89 [15744/50048]	Loss: 0.0671
Training Epoch: 89 [15872/50048]	Loss: 0.0750
Training Epoch: 89 [16000/50048]	Loss: 0.0787
Training Epoch: 89 [16128/50048]	Loss: 0.1196
Training Epoch: 89 [16256/50048]	Loss: 0.0694
Training Epoch: 89 [16384/50048]	Loss: 0.0904
Training Epoch: 89 [16512/50048]	Loss: 0.0494
Training Epoch: 89 [16640/50048]	Loss: 0.0937
Training Epoch: 89 [16768/50048]	Loss: 0.0692
Training Epoch: 89 [16896/50048]	Loss: 0.0433
Training Epoch: 89 [17024/50048]	Loss: 0.1189
Training Epoch: 89 [17152/50048]	Loss: 0.0994
Training Epoch: 89 [17280/50048]	Loss: 0.0683
Training Epoch: 89 [17408/50048]	Loss: 0.0741
Training Epoch: 89 [17536/50048]	Loss: 0.0929
Training Epoch: 89 [17664/50048]	Loss: 0.0827
Training Epoch: 89 [17792/50048]	Loss: 0.0756
Training Epoch: 89 [17920/50048]	Loss: 0.0620
Training Epoch: 89 [18048/50048]	Loss: 0.1187
Training Epoch: 89 [18176/50048]	Loss: 0.0974
Training Epoch: 89 [18304/50048]	Loss: 0.0948
Training Epoch: 89 [18432/50048]	Loss: 0.0798
Training Epoch: 89 [18560/50048]	Loss: 0.0942
Training Epoch: 89 [18688/50048]	Loss: 0.1211
Training Epoch: 89 [18816/50048]	Loss: 0.0882
Training Epoch: 89 [18944/50048]	Loss: 0.0874
Training Epoch: 89 [19072/50048]	Loss: 0.0510
Training Epoch: 89 [19200/50048]	Loss: 0.0662
Training Epoch: 89 [19328/50048]	Loss: 0.0558
Training Epoch: 89 [19456/50048]	Loss: 0.1144
Training Epoch: 89 [19584/50048]	Loss: 0.1333
Training Epoch: 89 [19712/50048]	Loss: 0.0825
Training Epoch: 89 [19840/50048]	Loss: 0.0448
Training Epoch: 89 [19968/50048]	Loss: 0.0925
Training Epoch: 89 [20096/50048]	Loss: 0.1360
Training Epoch: 89 [20224/50048]	Loss: 0.1155
Training Epoch: 89 [20352/50048]	Loss: 0.0820
Training Epoch: 89 [20480/50048]	Loss: 0.1154
Training Epoch: 89 [20608/50048]	Loss: 0.0877
Training Epoch: 89 [20736/50048]	Loss: 0.0910
Training Epoch: 89 [20864/50048]	Loss: 0.1018
Training Epoch: 89 [20992/50048]	Loss: 0.1262
Training Epoch: 89 [21120/50048]	Loss: 0.0424
Training Epoch: 89 [21248/50048]	Loss: 0.1486
Training Epoch: 89 [21376/50048]	Loss: 0.0982
Training Epoch: 89 [21504/50048]	Loss: 0.0517
Training Epoch: 89 [21632/50048]	Loss: 0.0746
Training Epoch: 89 [21760/50048]	Loss: 0.0621
Training Epoch: 89 [21888/50048]	Loss: 0.1181
Training Epoch: 89 [22016/50048]	Loss: 0.1161
Training Epoch: 89 [22144/50048]	Loss: 0.0742
Training Epoch: 89 [22272/50048]	Loss: 0.1198
Training Epoch: 89 [22400/50048]	Loss: 0.0857
Training Epoch: 89 [22528/50048]	Loss: 0.1061
Training Epoch: 89 [22656/50048]	Loss: 0.0672
Training Epoch: 89 [22784/50048]	Loss: 0.1038
Training Epoch: 89 [22912/50048]	Loss: 0.1406
Training Epoch: 89 [23040/50048]	Loss: 0.0841
Training Epoch: 89 [23168/50048]	Loss: 0.1004
Training Epoch: 89 [23296/50048]	Loss: 0.0793
Training Epoch: 89 [23424/50048]	Loss: 0.1031
Training Epoch: 89 [23552/50048]	Loss: 0.1044
Training Epoch: 89 [23680/50048]	Loss: 0.0575
Training Epoch: 89 [23808/50048]	Loss: 0.0682
Training Epoch: 89 [23936/50048]	Loss: 0.0681
Training Epoch: 89 [24064/50048]	Loss: 0.0887
Training Epoch: 89 [24192/50048]	Loss: 0.0653
Training Epoch: 89 [24320/50048]	Loss: 0.0773
Training Epoch: 89 [24448/50048]	Loss: 0.0650
Training Epoch: 89 [24576/50048]	Loss: 0.0588
Training Epoch: 89 [24704/50048]	Loss: 0.0746
Training Epoch: 89 [24832/50048]	Loss: 0.1084
Training Epoch: 89 [24960/50048]	Loss: 0.0596
Training Epoch: 89 [25088/50048]	Loss: 0.1142
Training Epoch: 89 [25216/50048]	Loss: 0.0580
Training Epoch: 89 [25344/50048]	Loss: 0.0768
Training Epoch: 89 [25472/50048]	Loss: 0.1049
Training Epoch: 89 [25600/50048]	Loss: 0.0705
Training Epoch: 89 [25728/50048]	Loss: 0.1163
Training Epoch: 89 [25856/50048]	Loss: 0.0880
Training Epoch: 89 [25984/50048]	Loss: 0.1325
Training Epoch: 89 [26112/50048]	Loss: 0.1414
Training Epoch: 89 [26240/50048]	Loss: 0.0725
Training Epoch: 89 [26368/50048]	Loss: 0.1030
Training Epoch: 89 [26496/50048]	Loss: 0.1356
Training Epoch: 89 [26624/50048]	Loss: 0.1047
Training Epoch: 89 [26752/50048]	Loss: 0.1011
Training Epoch: 89 [26880/50048]	Loss: 0.1046
Training Epoch: 89 [27008/50048]	Loss: 0.0632
Training Epoch: 89 [27136/50048]	Loss: 0.1030
Training Epoch: 89 [27264/50048]	Loss: 0.0808
Training Epoch: 89 [27392/50048]	Loss: 0.0802
Training Epoch: 89 [27520/50048]	Loss: 0.0882
Training Epoch: 89 [27648/50048]	Loss: 0.1208
Training Epoch: 89 [27776/50048]	Loss: 0.1072
Training Epoch: 89 [27904/50048]	Loss: 0.0473
Training Epoch: 89 [28032/50048]	Loss: 0.0895
Training Epoch: 89 [28160/50048]	Loss: 0.0679
Training Epoch: 89 [28288/50048]	Loss: 0.0923
Training Epoch: 89 [28416/50048]	Loss: 0.0931
Training Epoch: 89 [28544/50048]	Loss: 0.0832
Training Epoch: 89 [28672/50048]	Loss: 0.0364
Training Epoch: 89 [28800/50048]	Loss: 0.0846
Training Epoch: 89 [28928/50048]	Loss: 0.1865
Training Epoch: 89 [29056/50048]	Loss: 0.0821
Training Epoch: 89 [29184/50048]	Loss: 0.1509
Training Epoch: 89 [29312/50048]	Loss: 0.2017
Training Epoch: 89 [29440/50048]	Loss: 0.0986
Training Epoch: 89 [29568/50048]	Loss: 0.0845
Training Epoch: 89 [29696/50048]	Loss: 0.1747
Training Epoch: 89 [29824/50048]	Loss: 0.0853
Training Epoch: 89 [29952/50048]	Loss: 0.0385
Training Epoch: 89 [30080/50048]	Loss: 0.0508
Training Epoch: 89 [30208/50048]	Loss: 0.1417
Training Epoch: 89 [30336/50048]	Loss: 0.0957
Training Epoch: 89 [30464/50048]	Loss: 0.0332
Training Epoch: 89 [30592/50048]	Loss: 0.0770
Training Epoch: 89 [30720/50048]	Loss: 0.0712
Training Epoch: 89 [30848/50048]	Loss: 0.0821
Training Epoch: 89 [30976/50048]	Loss: 0.0553
Training Epoch: 89 [31104/50048]	Loss: 0.0778
Training Epoch: 89 [31232/50048]	Loss: 0.0794
Training Epoch: 89 [31360/50048]	Loss: 0.1028
Training Epoch: 89 [31488/50048]	Loss: 0.1138
Training Epoch: 89 [31616/50048]	Loss: 0.0306
Training Epoch: 89 [31744/50048]	Loss: 0.0491
Training Epoch: 89 [31872/50048]	Loss: 0.0745
Training Epoch: 89 [32000/50048]	Loss: 0.0715
Training Epoch: 89 [32128/50048]	Loss: 0.1459
Training Epoch: 89 [32256/50048]	Loss: 0.0901
Training Epoch: 89 [32384/50048]	Loss: 0.0609
Training Epoch: 89 [32512/50048]	Loss: 0.0523
Training Epoch: 89 [32640/50048]	Loss: 0.1081
Training Epoch: 89 [32768/50048]	Loss: 0.1132
Training Epoch: 89 [32896/50048]	Loss: 0.0970
Training Epoch: 89 [33024/50048]	Loss: 0.0876
Training Epoch: 89 [33152/50048]	Loss: 0.0821
Training Epoch: 89 [33280/50048]	Loss: 0.0655
Training Epoch: 89 [33408/50048]	Loss: 0.1062
Training Epoch: 89 [33536/50048]	Loss: 0.1420
Training Epoch: 89 [33664/50048]	Loss: 0.1328
Training Epoch: 89 [33792/50048]	Loss: 0.1351
Training Epoch: 89 [33920/50048]	Loss: 0.1185
Training Epoch: 89 [34048/50048]	Loss: 0.0978
Training Epoch: 89 [34176/50048]	Loss: 0.0724
Training Epoch: 89 [34304/50048]	Loss: 0.0968
Training Epoch: 89 [34432/50048]	Loss: 0.0788
Training Epoch: 89 [34560/50048]	Loss: 0.1017
Training Epoch: 89 [34688/50048]	Loss: 0.0988
Training Epoch: 89 [34816/50048]	Loss: 0.0650
Training Epoch: 89 [34944/50048]	Loss: 0.1334
Training Epoch: 89 [35072/50048]	Loss: 0.0676
Training Epoch: 89 [35200/50048]	Loss: 0.1592
Training Epoch: 89 [35328/50048]	Loss: 0.0991
Training Epoch: 89 [35456/50048]	Loss: 0.1416
Training Epoch: 89 [35584/50048]	Loss: 0.1418
Training Epoch: 89 [35712/50048]	Loss: 0.0829
Training Epoch: 89 [35840/50048]	Loss: 0.0806
Training Epoch: 89 [35968/50048]	Loss: 0.1004
Training Epoch: 89 [36096/50048]	Loss: 0.0444
Training Epoch: 89 [36224/50048]	Loss: 0.0328
Training Epoch: 89 [36352/50048]	Loss: 0.0551
Training Epoch: 89 [36480/50048]	Loss: 0.1498
Training Epoch: 89 [36608/50048]	Loss: 0.1403
Training Epoch: 89 [36736/50048]	Loss: 0.1630
Training Epoch: 89 [36864/50048]	Loss: 0.0850
Training Epoch: 89 [36992/50048]	Loss: 0.0475
Training Epoch: 89 [37120/50048]	Loss: 0.1061
Training Epoch: 89 [37248/50048]	Loss: 0.0441
Training Epoch: 89 [37376/50048]	Loss: 0.1663
Training Epoch: 89 [37504/50048]	Loss: 0.0832
Training Epoch: 89 [37632/50048]	Loss: 0.0572
Training Epoch: 89 [37760/50048]	Loss: 0.0626
Training Epoch: 89 [37888/50048]	Loss: 0.0497
Training Epoch: 89 [38016/50048]	Loss: 0.1243
Training Epoch: 89 [38144/50048]	Loss: 0.1395
Training Epoch: 89 [38272/50048]	Loss: 0.0726
Training Epoch: 89 [38400/50048]	Loss: 0.0819
Training Epoch: 89 [38528/50048]	Loss: 0.0735
Training Epoch: 89 [38656/50048]	Loss: 0.0738
Training Epoch: 89 [38784/50048]	Loss: 0.1346
Training Epoch: 89 [38912/50048]	Loss: 0.1111
Training Epoch: 89 [39040/50048]	Loss: 0.0887
Training Epoch: 89 [39168/50048]	Loss: 0.0555
Training Epoch: 89 [39296/50048]	Loss: 0.1212
Training Epoch: 89 [39424/50048]	Loss: 0.1087
Training Epoch: 89 [39552/50048]	Loss: 0.0733
Training Epoch: 89 [39680/50048]	Loss: 0.0318
Training Epoch: 89 [39808/50048]	Loss: 0.0932
Training Epoch: 89 [39936/50048]	Loss: 0.1431
Training Epoch: 89 [40064/50048]	Loss: 0.0923
Training Epoch: 89 [40192/50048]	Loss: 0.0428
Training Epoch: 89 [40320/50048]	Loss: 0.1326
Training Epoch: 89 [40448/50048]	Loss: 0.0888
Training Epoch: 89 [40576/50048]	Loss: 0.0674
Training Epoch: 89 [40704/50048]	Loss: 0.0778
Training Epoch: 89 [40832/50048]	Loss: 0.0969
Training Epoch: 89 [40960/50048]	Loss: 0.0813
Training Epoch: 89 [41088/50048]	Loss: 0.1050
Training Epoch: 89 [41216/50048]	Loss: 0.1055
Training Epoch: 89 [41344/50048]	Loss: 0.1094
Training Epoch: 89 [41472/50048]	Loss: 0.1144
Training Epoch: 89 [41600/50048]	Loss: 0.0718
Training Epoch: 89 [41728/50048]	Loss: 0.1248
Training Epoch: 89 [41856/50048]	Loss: 0.1317
Training Epoch: 89 [41984/50048]	Loss: 0.1403
Training Epoch: 89 [42112/50048]	Loss: 0.0847
Training Epoch: 89 [42240/50048]	Loss: 0.0957
Training Epoch: 89 [42368/50048]	Loss: 0.0711
Training Epoch: 89 [42496/50048]	Loss: 0.2030
Training Epoch: 89 [42624/50048]	Loss: 0.1015
Training Epoch: 89 [42752/50048]	Loss: 0.1171
Training Epoch: 89 [42880/50048]	Loss: 0.0702
Training Epoch: 89 [43008/50048]	Loss: 0.0502
Training Epoch: 89 [43136/50048]	Loss: 0.0566
Training Epoch: 89 [43264/50048]	Loss: 0.0933
Training Epoch: 89 [43392/50048]	Loss: 0.0775
Training Epoch: 89 [43520/50048]	Loss: 0.0600
Training Epoch: 89 [43648/50048]	Loss: 0.0468
Training Epoch: 89 [43776/50048]	Loss: 0.0595
Training Epoch: 89 [43904/50048]	Loss: 0.0572
Training Epoch: 89 [44032/50048]	Loss: 0.0824
Training Epoch: 89 [44160/50048]	Loss: 0.1239
Training Epoch: 89 [44288/50048]	Loss: 0.0958
Training Epoch: 89 [44416/50048]	Loss: 0.0712
Training Epoch: 89 [44544/50048]	Loss: 0.0995
Training Epoch: 89 [44672/50048]	Loss: 0.1069
Training Epoch: 89 [44800/50048]	Loss: 0.0510
Training Epoch: 89 [44928/50048]	Loss: 0.0735
Training Epoch: 89 [45056/50048]	Loss: 0.1034
Training Epoch: 89 [45184/50048]	Loss: 0.1362
Training Epoch: 89 [45312/50048]	Loss: 0.0886
Training Epoch: 89 [45440/50048]	Loss: 0.1168
Training Epoch: 89 [45568/50048]	Loss: 0.1078
Training Epoch: 89 [45696/50048]	Loss: 0.1513
2022-12-06 05:52:40,079 [ZeusDataLoader(train)] train epoch 90 done: time=86.54 energy=10511.02
2022-12-06 05:52:40,080 [ZeusDataLoader(eval)] Epoch 90 begin.
Training Epoch: 89 [45824/50048]	Loss: 0.1179
Training Epoch: 89 [45952/50048]	Loss: 0.1058
Training Epoch: 89 [46080/50048]	Loss: 0.0770
Training Epoch: 89 [46208/50048]	Loss: 0.1285
Training Epoch: 89 [46336/50048]	Loss: 0.0742
Training Epoch: 89 [46464/50048]	Loss: 0.1124
Training Epoch: 89 [46592/50048]	Loss: 0.1194
Training Epoch: 89 [46720/50048]	Loss: 0.0646
Training Epoch: 89 [46848/50048]	Loss: 0.1053
Training Epoch: 89 [46976/50048]	Loss: 0.1422
Training Epoch: 89 [47104/50048]	Loss: 0.0907
Training Epoch: 89 [47232/50048]	Loss: 0.1155
Training Epoch: 89 [47360/50048]	Loss: 0.0511
Training Epoch: 89 [47488/50048]	Loss: 0.1196
Training Epoch: 89 [47616/50048]	Loss: 0.0601
Training Epoch: 89 [47744/50048]	Loss: 0.0694
Training Epoch: 89 [47872/50048]	Loss: 0.1131
Training Epoch: 89 [48000/50048]	Loss: 0.0714
Training Epoch: 89 [48128/50048]	Loss: 0.0697
Training Epoch: 89 [48256/50048]	Loss: 0.0788
Training Epoch: 89 [48384/50048]	Loss: 0.0919
Training Epoch: 89 [48512/50048]	Loss: 0.1213
Training Epoch: 89 [48640/50048]	Loss: 0.0451
Training Epoch: 89 [48768/50048]	Loss: 0.1130
Training Epoch: 89 [48896/50048]	Loss: 0.0569
Training Epoch: 89 [49024/50048]	Loss: 0.0911
Training Epoch: 89 [49152/50048]	Loss: 0.0789
Training Epoch: 89 [49280/50048]	Loss: 0.1201
Training Epoch: 89 [49408/50048]	Loss: 0.0981
Training Epoch: 89 [49536/50048]	Loss: 0.0822
Training Epoch: 89 [49664/50048]	Loss: 0.0612
Training Epoch: 89 [49792/50048]	Loss: 0.0529
Training Epoch: 89 [49920/50048]	Loss: 0.0875
Training Epoch: 89 [50048/50048]	Loss: 0.1293
2022-12-06 10:52:43.725 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:52:43,736 [ZeusDataLoader(eval)] eval epoch 90 done: time=3.65 energy=442.77
2022-12-06 05:52:43,736 [ZeusDataLoader(train)] Up to epoch 90: time=8119.50, energy=985696.83, cost=1203304.40
2022-12-06 05:52:43,737 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:52:43,737 [ZeusDataLoader(train)] Expected next epoch: time=8209.30, energy=996494.85, cost=1216560.78
2022-12-06 05:52:43,738 [ZeusDataLoader(train)] Epoch 91 begin.
Validation Epoch: 89, Average loss: 0.0182, Accuracy: 0.6456
2022-12-06 05:52:43,911 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:52:43,912 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:52:43.914 [ZeusMonitor] Monitor started.
2022-12-06 10:52:43.914 [ZeusMonitor] Running indefinitely. 2022-12-06 10:52:43.914 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:52:43.914 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e91+gpu0.power.log
Training Epoch: 90 [128/50048]	Loss: 0.0836
Training Epoch: 90 [256/50048]	Loss: 0.0755
Training Epoch: 90 [384/50048]	Loss: 0.0635
Training Epoch: 90 [512/50048]	Loss: 0.0817
Training Epoch: 90 [640/50048]	Loss: 0.1120
Training Epoch: 90 [768/50048]	Loss: 0.0826
Training Epoch: 90 [896/50048]	Loss: 0.0642
Training Epoch: 90 [1024/50048]	Loss: 0.0649
Training Epoch: 90 [1152/50048]	Loss: 0.0503
Training Epoch: 90 [1280/50048]	Loss: 0.1555
Training Epoch: 90 [1408/50048]	Loss: 0.0803
Training Epoch: 90 [1536/50048]	Loss: 0.0452
Training Epoch: 90 [1664/50048]	Loss: 0.1701
Training Epoch: 90 [1792/50048]	Loss: 0.0481
Training Epoch: 90 [1920/50048]	Loss: 0.0963
Training Epoch: 90 [2048/50048]	Loss: 0.0427
Training Epoch: 90 [2176/50048]	Loss: 0.0815
Training Epoch: 90 [2304/50048]	Loss: 0.1016
Training Epoch: 90 [2432/50048]	Loss: 0.0401
Training Epoch: 90 [2560/50048]	Loss: 0.1138
Training Epoch: 90 [2688/50048]	Loss: 0.0797
Training Epoch: 90 [2816/50048]	Loss: 0.0818
Training Epoch: 90 [2944/50048]	Loss: 0.0714
Training Epoch: 90 [3072/50048]	Loss: 0.0673
Training Epoch: 90 [3200/50048]	Loss: 0.1138
Training Epoch: 90 [3328/50048]	Loss: 0.0538
Training Epoch: 90 [3456/50048]	Loss: 0.0720
Training Epoch: 90 [3584/50048]	Loss: 0.0524
Training Epoch: 90 [3712/50048]	Loss: 0.0781
Training Epoch: 90 [3840/50048]	Loss: 0.0605
Training Epoch: 90 [3968/50048]	Loss: 0.0867
Training Epoch: 90 [4096/50048]	Loss: 0.0686
Training Epoch: 90 [4224/50048]	Loss: 0.0579
Training Epoch: 90 [4352/50048]	Loss: 0.0528
Training Epoch: 90 [4480/50048]	Loss: 0.0578
Training Epoch: 90 [4608/50048]	Loss: 0.1024
Training Epoch: 90 [4736/50048]	Loss: 0.0560
Training Epoch: 90 [4864/50048]	Loss: 0.0370
Training Epoch: 90 [4992/50048]	Loss: 0.0266
Training Epoch: 90 [5120/50048]	Loss: 0.0287
Training Epoch: 90 [5248/50048]	Loss: 0.0864
Training Epoch: 90 [5376/50048]	Loss: 0.0933
Training Epoch: 90 [5504/50048]	Loss: 0.0778
Training Epoch: 90 [5632/50048]	Loss: 0.0690
Training Epoch: 90 [5760/50048]	Loss: 0.0679
Training Epoch: 90 [5888/50048]	Loss: 0.0511
Training Epoch: 90 [6016/50048]	Loss: 0.0261
Training Epoch: 90 [6144/50048]	Loss: 0.0583
Training Epoch: 90 [6272/50048]	Loss: 0.0423
Training Epoch: 90 [6400/50048]	Loss: 0.1294
Training Epoch: 90 [6528/50048]	Loss: 0.1524
Training Epoch: 90 [6656/50048]	Loss: 0.0387
Training Epoch: 90 [6784/50048]	Loss: 0.0928
Training Epoch: 90 [6912/50048]	Loss: 0.1256
Training Epoch: 90 [7040/50048]	Loss: 0.0684
Training Epoch: 90 [7168/50048]	Loss: 0.0871
Training Epoch: 90 [7296/50048]	Loss: 0.0840
Training Epoch: 90 [7424/50048]	Loss: 0.0800
Training Epoch: 90 [7552/50048]	Loss: 0.0369
Training Epoch: 90 [7680/50048]	Loss: 0.0394
Training Epoch: 90 [7808/50048]	Loss: 0.0395
Training Epoch: 90 [7936/50048]	Loss: 0.0667
Training Epoch: 90 [8064/50048]	Loss: 0.0946
Training Epoch: 90 [8192/50048]	Loss: 0.1610
Training Epoch: 90 [8320/50048]	Loss: 0.0820
Training Epoch: 90 [8448/50048]	Loss: 0.0618
Training Epoch: 90 [8576/50048]	Loss: 0.0630
Training Epoch: 90 [8704/50048]	Loss: 0.0475
Training Epoch: 90 [8832/50048]	Loss: 0.1100
Training Epoch: 90 [8960/50048]	Loss: 0.1195
Training Epoch: 90 [9088/50048]	Loss: 0.0537
Training Epoch: 90 [9216/50048]	Loss: 0.1071
Training Epoch: 90 [9344/50048]	Loss: 0.0473
Training Epoch: 90 [9472/50048]	Loss: 0.0839
Training Epoch: 90 [9600/50048]	Loss: 0.1712
Training Epoch: 90 [9728/50048]	Loss: 0.0204
Training Epoch: 90 [9856/50048]	Loss: 0.1136
Training Epoch: 90 [9984/50048]	Loss: 0.0715
Training Epoch: 90 [10112/50048]	Loss: 0.1472
Training Epoch: 90 [10240/50048]	Loss: 0.0702
Training Epoch: 90 [10368/50048]	Loss: 0.0912
Training Epoch: 90 [10496/50048]	Loss: 0.0891
Training Epoch: 90 [10624/50048]	Loss: 0.0820
Training Epoch: 90 [10752/50048]	Loss: 0.0729
Training Epoch: 90 [10880/50048]	Loss: 0.1313
Training Epoch: 90 [11008/50048]	Loss: 0.0667
Training Epoch: 90 [11136/50048]	Loss: 0.0468
Training Epoch: 90 [11264/50048]	Loss: 0.0706
Training Epoch: 90 [11392/50048]	Loss: 0.1039
Training Epoch: 90 [11520/50048]	Loss: 0.1293
Training Epoch: 90 [11648/50048]	Loss: 0.0915
Training Epoch: 90 [11776/50048]	Loss: 0.0909
Training Epoch: 90 [11904/50048]	Loss: 0.1422
Training Epoch: 90 [12032/50048]	Loss: 0.1139
Training Epoch: 90 [12160/50048]	Loss: 0.0470
Training Epoch: 90 [12288/50048]	Loss: 0.0920
Training Epoch: 90 [12416/50048]	Loss: 0.0549
Training Epoch: 90 [12544/50048]	Loss: 0.0941
Training Epoch: 90 [12672/50048]	Loss: 0.1310
Training Epoch: 90 [12800/50048]	Loss: 0.0454
Training Epoch: 90 [12928/50048]	Loss: 0.0489
Training Epoch: 90 [13056/50048]	Loss: 0.0612
Training Epoch: 90 [13184/50048]	Loss: 0.0661
Training Epoch: 90 [13312/50048]	Loss: 0.0804
Training Epoch: 90 [13440/50048]	Loss: 0.0531
Training Epoch: 90 [13568/50048]	Loss: 0.1047
Training Epoch: 90 [13696/50048]	Loss: 0.0453
Training Epoch: 90 [13824/50048]	Loss: 0.0514
Training Epoch: 90 [13952/50048]	Loss: 0.0831
Training Epoch: 90 [14080/50048]	Loss: 0.0405
Training Epoch: 90 [14208/50048]	Loss: 0.0521
Training Epoch: 90 [14336/50048]	Loss: 0.0905
Training Epoch: 90 [14464/50048]	Loss: 0.1188
Training Epoch: 90 [14592/50048]	Loss: 0.0362
Training Epoch: 90 [14720/50048]	Loss: 0.0462
Training Epoch: 90 [14848/50048]	Loss: 0.0661
Training Epoch: 90 [14976/50048]	Loss: 0.0469
Training Epoch: 90 [15104/50048]	Loss: 0.0416
Training Epoch: 90 [15232/50048]	Loss: 0.0371
Training Epoch: 90 [15360/50048]	Loss: 0.1083
Training Epoch: 90 [15488/50048]	Loss: 0.1351
Training Epoch: 90 [15616/50048]	Loss: 0.0838
Training Epoch: 90 [15744/50048]	Loss: 0.0412
Training Epoch: 90 [15872/50048]	Loss: 0.1014
Training Epoch: 90 [16000/50048]	Loss: 0.0632
Training Epoch: 90 [16128/50048]	Loss: 0.1423
Training Epoch: 90 [16256/50048]	Loss: 0.0726
Training Epoch: 90 [16384/50048]	Loss: 0.0992
Training Epoch: 90 [16512/50048]	Loss: 0.1132
Training Epoch: 90 [16640/50048]	Loss: 0.0469
Training Epoch: 90 [16768/50048]	Loss: 0.0784
Training Epoch: 90 [16896/50048]	Loss: 0.0236
Training Epoch: 90 [17024/50048]	Loss: 0.0529
Training Epoch: 90 [17152/50048]	Loss: 0.0343
Training Epoch: 90 [17280/50048]	Loss: 0.0235
Training Epoch: 90 [17408/50048]	Loss: 0.0751
Training Epoch: 90 [17536/50048]	Loss: 0.0919
Training Epoch: 90 [17664/50048]	Loss: 0.1223
Training Epoch: 90 [17792/50048]	Loss: 0.0759
Training Epoch: 90 [17920/50048]	Loss: 0.0827
Training Epoch: 90 [18048/50048]	Loss: 0.0887
Training Epoch: 90 [18176/50048]	Loss: 0.1371
Training Epoch: 90 [18304/50048]	Loss: 0.1007
Training Epoch: 90 [18432/50048]	Loss: 0.0936
Training Epoch: 90 [18560/50048]	Loss: 0.0800
Training Epoch: 90 [18688/50048]	Loss: 0.0490
Training Epoch: 90 [18816/50048]	Loss: 0.1144
Training Epoch: 90 [18944/50048]	Loss: 0.0812
Training Epoch: 90 [19072/50048]	Loss: 0.1437
Training Epoch: 90 [19200/50048]	Loss: 0.0496
Training Epoch: 90 [19328/50048]	Loss: 0.1186
Training Epoch: 90 [19456/50048]	Loss: 0.1099
Training Epoch: 90 [19584/50048]	Loss: 0.0313
Training Epoch: 90 [19712/50048]	Loss: 0.0648
Training Epoch: 90 [19840/50048]	Loss: 0.0659
Training Epoch: 90 [19968/50048]	Loss: 0.1498
Training Epoch: 90 [20096/50048]	Loss: 0.0954
Training Epoch: 90 [20224/50048]	Loss: 0.1057
Training Epoch: 90 [20352/50048]	Loss: 0.0887
Training Epoch: 90 [20480/50048]	Loss: 0.1229
Training Epoch: 90 [20608/50048]	Loss: 0.0435
Training Epoch: 90 [20736/50048]	Loss: 0.0743
Training Epoch: 90 [20864/50048]	Loss: 0.0853
Training Epoch: 90 [20992/50048]	Loss: 0.0630
Training Epoch: 90 [21120/50048]	Loss: 0.0376
Training Epoch: 90 [21248/50048]	Loss: 0.1094
Training Epoch: 90 [21376/50048]	Loss: 0.0253
Training Epoch: 90 [21504/50048]	Loss: 0.1061
Training Epoch: 90 [21632/50048]	Loss: 0.0752
Training Epoch: 90 [21760/50048]	Loss: 0.1511
Training Epoch: 90 [21888/50048]	Loss: 0.0827
Training Epoch: 90 [22016/50048]	Loss: 0.0615
Training Epoch: 90 [22144/50048]	Loss: 0.0693
Training Epoch: 90 [22272/50048]	Loss: 0.0818
Training Epoch: 90 [22400/50048]	Loss: 0.1028
Training Epoch: 90 [22528/50048]	Loss: 0.1344
Training Epoch: 90 [22656/50048]	Loss: 0.0610
Training Epoch: 90 [22784/50048]	Loss: 0.0605
Training Epoch: 90 [22912/50048]	Loss: 0.0927
Training Epoch: 90 [23040/50048]	Loss: 0.0930
Training Epoch: 90 [23168/50048]	Loss: 0.0936
Training Epoch: 90 [23296/50048]	Loss: 0.0797
Training Epoch: 90 [23424/50048]	Loss: 0.0825
Training Epoch: 90 [23552/50048]	Loss: 0.1101
Training Epoch: 90 [23680/50048]	Loss: 0.1140
Training Epoch: 90 [23808/50048]	Loss: 0.0563
Training Epoch: 90 [23936/50048]	Loss: 0.0633
Training Epoch: 90 [24064/50048]	Loss: 0.0801
Training Epoch: 90 [24192/50048]	Loss: 0.1219
Training Epoch: 90 [24320/50048]	Loss: 0.1283
Training Epoch: 90 [24448/50048]	Loss: 0.1088
Training Epoch: 90 [24576/50048]	Loss: 0.1202
Training Epoch: 90 [24704/50048]	Loss: 0.1180
Training Epoch: 90 [24832/50048]	Loss: 0.0939
Training Epoch: 90 [24960/50048]	Loss: 0.0625
Training Epoch: 90 [25088/50048]	Loss: 0.0894
Training Epoch: 90 [25216/50048]	Loss: 0.0727
Training Epoch: 90 [25344/50048]	Loss: 0.0576
Training Epoch: 90 [25472/50048]	Loss: 0.0645
Training Epoch: 90 [25600/50048]	Loss: 0.0566
Training Epoch: 90 [25728/50048]	Loss: 0.0507
Training Epoch: 90 [25856/50048]	Loss: 0.1652
Training Epoch: 90 [25984/50048]	Loss: 0.1414
Training Epoch: 90 [26112/50048]	Loss: 0.1216
Training Epoch: 90 [26240/50048]	Loss: 0.0642
Training Epoch: 90 [26368/50048]	Loss: 0.1172
Training Epoch: 90 [26496/50048]	Loss: 0.0585
Training Epoch: 90 [26624/50048]	Loss: 0.1274
Training Epoch: 90 [26752/50048]	Loss: 0.0633
Training Epoch: 90 [26880/50048]	Loss: 0.0737
Training Epoch: 90 [27008/50048]	Loss: 0.0997
Training Epoch: 90 [27136/50048]	Loss: 0.0720
Training Epoch: 90 [27264/50048]	Loss: 0.2064
Training Epoch: 90 [27392/50048]	Loss: 0.0605
Training Epoch: 90 [27520/50048]	Loss: 0.0503
Training Epoch: 90 [27648/50048]	Loss: 0.0801
Training Epoch: 90 [27776/50048]	Loss: 0.0753
Training Epoch: 90 [27904/50048]	Loss: 0.0671
Training Epoch: 90 [28032/50048]	Loss: 0.1089
Training Epoch: 90 [28160/50048]	Loss: 0.1398
Training Epoch: 90 [28288/50048]	Loss: 0.0915
Training Epoch: 90 [28416/50048]	Loss: 0.0769
Training Epoch: 90 [28544/50048]	Loss: 0.1455
Training Epoch: 90 [28672/50048]	Loss: 0.0580
Training Epoch: 90 [28800/50048]	Loss: 0.0365
Training Epoch: 90 [28928/50048]	Loss: 0.1495
Training Epoch: 90 [29056/50048]	Loss: 0.1662
Training Epoch: 90 [29184/50048]	Loss: 0.1339
Training Epoch: 90 [29312/50048]	Loss: 0.1622
Training Epoch: 90 [29440/50048]	Loss: 0.0872
Training Epoch: 90 [29568/50048]	Loss: 0.0998
Training Epoch: 90 [29696/50048]	Loss: 0.0686
Training Epoch: 90 [29824/50048]	Loss: 0.1381
Training Epoch: 90 [29952/50048]	Loss: 0.0678
Training Epoch: 90 [30080/50048]	Loss: 0.0704
Training Epoch: 90 [30208/50048]	Loss: 0.0776
Training Epoch: 90 [30336/50048]	Loss: 0.0674
Training Epoch: 90 [30464/50048]	Loss: 0.0855
Training Epoch: 90 [30592/50048]	Loss: 0.1316
Training Epoch: 90 [30720/50048]	Loss: 0.1207
Training Epoch: 90 [30848/50048]	Loss: 0.1233
Training Epoch: 90 [30976/50048]	Loss: 0.2826
Training Epoch: 90 [31104/50048]	Loss: 0.1275
Training Epoch: 90 [31232/50048]	Loss: 0.1268
Training Epoch: 90 [31360/50048]	Loss: 0.1128
Training Epoch: 90 [31488/50048]	Loss: 0.1246
Training Epoch: 90 [31616/50048]	Loss: 0.0843
Training Epoch: 90 [31744/50048]	Loss: 0.0756
Training Epoch: 90 [31872/50048]	Loss: 0.0835
Training Epoch: 90 [32000/50048]	Loss: 0.0863
Training Epoch: 90 [32128/50048]	Loss: 0.0738
Training Epoch: 90 [32256/50048]	Loss: 0.0923
Training Epoch: 90 [32384/50048]	Loss: 0.0930
Training Epoch: 90 [32512/50048]	Loss: 0.0480
Training Epoch: 90 [32640/50048]	Loss: 0.0465
Training Epoch: 90 [32768/50048]	Loss: 0.0727
Training Epoch: 90 [32896/50048]	Loss: 0.0939
Training Epoch: 90 [33024/50048]	Loss: 0.0709
Training Epoch: 90 [33152/50048]	Loss: 0.0663
Training Epoch: 90 [33280/50048]	Loss: 0.1280
Training Epoch: 90 [33408/50048]	Loss: 0.0646
Training Epoch: 90 [33536/50048]	Loss: 0.0940
Training Epoch: 90 [33664/50048]	Loss: 0.0666
Training Epoch: 90 [33792/50048]	Loss: 0.0503
Training Epoch: 90 [33920/50048]	Loss: 0.0540
Training Epoch: 90 [34048/50048]	Loss: 0.0824
Training Epoch: 90 [34176/50048]	Loss: 0.0687
Training Epoch: 90 [34304/50048]	Loss: 0.1149
Training Epoch: 90 [34432/50048]	Loss: 0.0433
Training Epoch: 90 [34560/50048]	Loss: 0.1719
Training Epoch: 90 [34688/50048]	Loss: 0.1398
Training Epoch: 90 [34816/50048]	Loss: 0.1112
Training Epoch: 90 [34944/50048]	Loss: 0.1540
Training Epoch: 90 [35072/50048]	Loss: 0.0584
Training Epoch: 90 [35200/50048]	Loss: 0.0288
Training Epoch: 90 [35328/50048]	Loss: 0.1105
Training Epoch: 90 [35456/50048]	Loss: 0.1474
Training Epoch: 90 [35584/50048]	Loss: 0.0551
Training Epoch: 90 [35712/50048]	Loss: 0.1346
Training Epoch: 90 [35840/50048]	Loss: 0.1489
Training Epoch: 90 [35968/50048]	Loss: 0.0857
Training Epoch: 90 [36096/50048]	Loss: 0.1562
Training Epoch: 90 [36224/50048]	Loss: 0.0924
Training Epoch: 90 [36352/50048]	Loss: 0.0695
Training Epoch: 90 [36480/50048]	Loss: 0.0606
Training Epoch: 90 [36608/50048]	Loss: 0.1307
Training Epoch: 90 [36736/50048]	Loss: 0.1473
Training Epoch: 90 [36864/50048]	Loss: 0.0909
Training Epoch: 90 [36992/50048]	Loss: 0.0796
Training Epoch: 90 [37120/50048]	Loss: 0.0430
Training Epoch: 90 [37248/50048]	Loss: 0.0675
Training Epoch: 90 [37376/50048]	Loss: 0.0597
Training Epoch: 90 [37504/50048]	Loss: 0.0603
Training Epoch: 90 [37632/50048]	Loss: 0.0621
Training Epoch: 90 [37760/50048]	Loss: 0.2101
Training Epoch: 90 [37888/50048]	Loss: 0.0936
Training Epoch: 90 [38016/50048]	Loss: 0.0596
Training Epoch: 90 [38144/50048]	Loss: 0.1217
Training Epoch: 90 [38272/50048]	Loss: 0.0895
Training Epoch: 90 [38400/50048]	Loss: 0.1194
Training Epoch: 90 [38528/50048]	Loss: 0.1406
Training Epoch: 90 [38656/50048]	Loss: 0.0751
Training Epoch: 90 [38784/50048]	Loss: 0.0891
Training Epoch: 90 [38912/50048]	Loss: 0.0386
Training Epoch: 90 [39040/50048]	Loss: 0.0948
Training Epoch: 90 [39168/50048]	Loss: 0.0708
Training Epoch: 90 [39296/50048]	Loss: 0.1461
Training Epoch: 90 [39424/50048]	Loss: 0.0379
Training Epoch: 90 [39552/50048]	Loss: 0.0372
Training Epoch: 90 [39680/50048]	Loss: 0.0544
Training Epoch: 90 [39808/50048]	Loss: 0.0849
Training Epoch: 90 [39936/50048]	Loss: 0.0869
Training Epoch: 90 [40064/50048]	Loss: 0.0830
Training Epoch: 90 [40192/50048]	Loss: 0.0556
Training Epoch: 90 [40320/50048]	Loss: 0.0859
Training Epoch: 90 [40448/50048]	Loss: 0.0735
Training Epoch: 90 [40576/50048]	Loss: 0.0372
Training Epoch: 90 [40704/50048]	Loss: 0.1446
Training Epoch: 90 [40832/50048]	Loss: 0.1127
Training Epoch: 90 [40960/50048]	Loss: 0.0782
Training Epoch: 90 [41088/50048]	Loss: 0.0608
Training Epoch: 90 [41216/50048]	Loss: 0.0725
Training Epoch: 90 [41344/50048]	Loss: 0.0757
Training Epoch: 90 [41472/50048]	Loss: 0.0882
Training Epoch: 90 [41600/50048]	Loss: 0.0320
Training Epoch: 90 [41728/50048]	Loss: 0.0576
Training Epoch: 90 [41856/50048]	Loss: 0.1706
Training Epoch: 90 [41984/50048]	Loss: 0.1425
Training Epoch: 90 [42112/50048]	Loss: 0.0975
Training Epoch: 90 [42240/50048]	Loss: 0.0801
Training Epoch: 90 [42368/50048]	Loss: 0.1066
Training Epoch: 90 [42496/50048]	Loss: 0.0708
Training Epoch: 90 [42624/50048]	Loss: 0.0664
Training Epoch: 90 [42752/50048]	Loss: 0.0595
Training Epoch: 90 [42880/50048]	Loss: 0.0877
Training Epoch: 90 [43008/50048]	Loss: 0.0638
Training Epoch: 90 [43136/50048]	Loss: 0.0479
Training Epoch: 90 [43264/50048]	Loss: 0.1160
Training Epoch: 90 [43392/50048]	Loss: 0.0865
Training Epoch: 90 [43520/50048]	Loss: 0.1110
Training Epoch: 90 [43648/50048]	Loss: 0.1066
Training Epoch: 90 [43776/50048]	Loss: 0.0855
Training Epoch: 90 [43904/50048]	Loss: 0.0442
Training Epoch: 90 [44032/50048]	Loss: 0.1026
Training Epoch: 90 [44160/50048]	Loss: 0.1407
Training Epoch: 90 [44288/50048]	Loss: 0.0838
Training Epoch: 90 [44416/50048]	Loss: 0.1045
Training Epoch: 90 [44544/50048]	Loss: 0.0988
Training Epoch: 90 [44672/50048]	Loss: 0.0517
Training Epoch: 90 [44800/50048]	Loss: 0.1565
Training Epoch: 90 [44928/50048]	Loss: 0.1042
Training Epoch: 90 [45056/50048]	Loss: 0.0634
Training Epoch: 90 [45184/50048]	Loss: 0.1434
Training Epoch: 90 [45312/50048]	Loss: 0.1061
Training Epoch: 90 [45440/50048]	Loss: 0.2488
Training Epoch: 90 [45568/50048]	Loss: 0.0719
Training Epoch: 90 [45696/50048]	Loss: 0.0738
2022-12-06 05:54:10,234 [ZeusDataLoader(train)] train epoch 91 done: time=86.49 energy=10493.84
2022-12-06 05:54:10,236 [ZeusDataLoader(eval)] Epoch 91 begin.
Training Epoch: 90 [45824/50048]	Loss: 0.0745
Training Epoch: 90 [45952/50048]	Loss: 0.1090
Training Epoch: 90 [46080/50048]	Loss: 0.1533
Training Epoch: 90 [46208/50048]	Loss: 0.0692
Training Epoch: 90 [46336/50048]	Loss: 0.0671
Training Epoch: 90 [46464/50048]	Loss: 0.0563
Training Epoch: 90 [46592/50048]	Loss: 0.1516
Training Epoch: 90 [46720/50048]	Loss: 0.1391
Training Epoch: 90 [46848/50048]	Loss: 0.1595
Training Epoch: 90 [46976/50048]	Loss: 0.1776
Training Epoch: 90 [47104/50048]	Loss: 0.0717
Training Epoch: 90 [47232/50048]	Loss: 0.0693
Training Epoch: 90 [47360/50048]	Loss: 0.0971
Training Epoch: 90 [47488/50048]	Loss: 0.1927
Training Epoch: 90 [47616/50048]	Loss: 0.0641
Training Epoch: 90 [47744/50048]	Loss: 0.0830
Training Epoch: 90 [47872/50048]	Loss: 0.0787
Training Epoch: 90 [48000/50048]	Loss: 0.1273
Training Epoch: 90 [48128/50048]	Loss: 0.1075
Training Epoch: 90 [48256/50048]	Loss: 0.1058
Training Epoch: 90 [48384/50048]	Loss: 0.1452
Training Epoch: 90 [48512/50048]	Loss: 0.0491
Training Epoch: 90 [48640/50048]	Loss: 0.1470
Training Epoch: 90 [48768/50048]	Loss: 0.0657
Training Epoch: 90 [48896/50048]	Loss: 0.1330
Training Epoch: 90 [49024/50048]	Loss: 0.0824
Training Epoch: 90 [49152/50048]	Loss: 0.0332
Training Epoch: 90 [49280/50048]	Loss: 0.1238
Training Epoch: 90 [49408/50048]	Loss: 0.0947
Training Epoch: 90 [49536/50048]	Loss: 0.1006
Training Epoch: 90 [49664/50048]	Loss: 0.1448
Training Epoch: 90 [49792/50048]	Loss: 0.0636
Training Epoch: 90 [49920/50048]	Loss: 0.0955
Training Epoch: 90 [50048/50048]	Loss: 0.4703
2022-12-06 10:54:13.958 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:54:13,987 [ZeusDataLoader(eval)] eval epoch 91 done: time=3.74 energy=454.53
2022-12-06 05:54:13,987 [ZeusDataLoader(train)] Up to epoch 91: time=8209.72, energy=996645.20, cost=1216673.50
2022-12-06 05:54:13,987 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:54:13,987 [ZeusDataLoader(train)] Expected next epoch: time=8299.52, energy=1007443.22, cost=1229929.88
2022-12-06 05:54:13,988 [ZeusDataLoader(train)] Epoch 92 begin.
Validation Epoch: 90, Average loss: 0.0189, Accuracy: 0.6323
2022-12-06 05:54:14,168 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:54:14,169 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:54:14.170 [ZeusMonitor] Monitor started.
2022-12-06 10:54:14.170 [ZeusMonitor] Running indefinitely. 2022-12-06 10:54:14.170 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:54:14.171 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e92+gpu0.power.log
Training Epoch: 91 [128/50048]	Loss: 0.1062
Training Epoch: 91 [256/50048]	Loss: 0.0872
Training Epoch: 91 [384/50048]	Loss: 0.0656
Training Epoch: 91 [512/50048]	Loss: 0.0664
Training Epoch: 91 [640/50048]	Loss: 0.0989
Training Epoch: 91 [768/50048]	Loss: 0.1293
Training Epoch: 91 [896/50048]	Loss: 0.0673
Training Epoch: 91 [1024/50048]	Loss: 0.1736
Training Epoch: 91 [1152/50048]	Loss: 0.2704
Training Epoch: 91 [1280/50048]	Loss: 0.0952
Training Epoch: 91 [1408/50048]	Loss: 0.0986
Training Epoch: 91 [1536/50048]	Loss: 0.0963
Training Epoch: 91 [1664/50048]	Loss: 0.0582
Training Epoch: 91 [1792/50048]	Loss: 0.1220
Training Epoch: 91 [1920/50048]	Loss: 0.1540
Training Epoch: 91 [2048/50048]	Loss: 0.1585
Training Epoch: 91 [2176/50048]	Loss: 0.0752
Training Epoch: 91 [2304/50048]	Loss: 0.0630
Training Epoch: 91 [2432/50048]	Loss: 0.0457
Training Epoch: 91 [2560/50048]	Loss: 0.0735
Training Epoch: 91 [2688/50048]	Loss: 0.1170
Training Epoch: 91 [2816/50048]	Loss: 0.0714
Training Epoch: 91 [2944/50048]	Loss: 0.0387
Training Epoch: 91 [3072/50048]	Loss: 0.0465
Training Epoch: 91 [3200/50048]	Loss: 0.0932
Training Epoch: 91 [3328/50048]	Loss: 0.0502
Training Epoch: 91 [3456/50048]	Loss: 0.0650
Training Epoch: 91 [3584/50048]	Loss: 0.0369
Training Epoch: 91 [3712/50048]	Loss: 0.0590
Training Epoch: 91 [3840/50048]	Loss: 0.0738
Training Epoch: 91 [3968/50048]	Loss: 0.0885
Training Epoch: 91 [4096/50048]	Loss: 0.0503
Training Epoch: 91 [4224/50048]	Loss: 0.0517
Training Epoch: 91 [4352/50048]	Loss: 0.0842
Training Epoch: 91 [4480/50048]	Loss: 0.0665
Training Epoch: 91 [4608/50048]	Loss: 0.0776
Training Epoch: 91 [4736/50048]	Loss: 0.0755
Training Epoch: 91 [4864/50048]	Loss: 0.0990
Training Epoch: 91 [4992/50048]	Loss: 0.0494
Training Epoch: 91 [5120/50048]	Loss: 0.1257
Training Epoch: 91 [5248/50048]	Loss: 0.0690
Training Epoch: 91 [5376/50048]	Loss: 0.1301
Training Epoch: 91 [5504/50048]	Loss: 0.0686
Training Epoch: 91 [5632/50048]	Loss: 0.0589
Training Epoch: 91 [5760/50048]	Loss: 0.1289
Training Epoch: 91 [5888/50048]	Loss: 0.0683
Training Epoch: 91 [6016/50048]	Loss: 0.0467
Training Epoch: 91 [6144/50048]	Loss: 0.0179
Training Epoch: 91 [6272/50048]	Loss: 0.0595
Training Epoch: 91 [6400/50048]	Loss: 0.1159
Training Epoch: 91 [6528/50048]	Loss: 0.0356
Training Epoch: 91 [6656/50048]	Loss: 0.1840
Training Epoch: 91 [6784/50048]	Loss: 0.0335
Training Epoch: 91 [6912/50048]	Loss: 0.1085
Training Epoch: 91 [7040/50048]	Loss: 0.0902
Training Epoch: 91 [7168/50048]	Loss: 0.0568
Training Epoch: 91 [7296/50048]	Loss: 0.0531
Training Epoch: 91 [7424/50048]	Loss: 0.0914
Training Epoch: 91 [7552/50048]	Loss: 0.1025
Training Epoch: 91 [7680/50048]	Loss: 0.1490
Training Epoch: 91 [7808/50048]	Loss: 0.0902
Training Epoch: 91 [7936/50048]	Loss: 0.0603
Training Epoch: 91 [8064/50048]	Loss: 0.0879
Training Epoch: 91 [8192/50048]	Loss: 0.1585
Training Epoch: 91 [8320/50048]	Loss: 0.0475
Training Epoch: 91 [8448/50048]	Loss: 0.0610
Training Epoch: 91 [8576/50048]	Loss: 0.0484
Training Epoch: 91 [8704/50048]	Loss: 0.0692
Training Epoch: 91 [8832/50048]	Loss: 0.1183
Training Epoch: 91 [8960/50048]	Loss: 0.0659
Training Epoch: 91 [9088/50048]	Loss: 0.1057
Training Epoch: 91 [9216/50048]	Loss: 0.0600
Training Epoch: 91 [9344/50048]	Loss: 0.1038
Training Epoch: 91 [9472/50048]	Loss: 0.1114
Training Epoch: 91 [9600/50048]	Loss: 0.0295
Training Epoch: 91 [9728/50048]	Loss: 0.1064
Training Epoch: 91 [9856/50048]	Loss: 0.0749
Training Epoch: 91 [9984/50048]	Loss: 0.1375
Training Epoch: 91 [10112/50048]	Loss: 0.0969
Training Epoch: 91 [10240/50048]	Loss: 0.0805
Training Epoch: 91 [10368/50048]	Loss: 0.0592
Training Epoch: 91 [10496/50048]	Loss: 0.0716
Training Epoch: 91 [10624/50048]	Loss: 0.0743
Training Epoch: 91 [10752/50048]	Loss: 0.0547
Training Epoch: 91 [10880/50048]	Loss: 0.0600
Training Epoch: 91 [11008/50048]	Loss: 0.0252
Training Epoch: 91 [11136/50048]	Loss: 0.0682
Training Epoch: 91 [11264/50048]	Loss: 0.1041
Training Epoch: 91 [11392/50048]	Loss: 0.0940
Training Epoch: 91 [11520/50048]	Loss: 0.0624
Training Epoch: 91 [11648/50048]	Loss: 0.1072
Training Epoch: 91 [11776/50048]	Loss: 0.0665
Training Epoch: 91 [11904/50048]	Loss: 0.0484
Training Epoch: 91 [12032/50048]	Loss: 0.0451
Training Epoch: 91 [12160/50048]	Loss: 0.0704
Training Epoch: 91 [12288/50048]	Loss: 0.1181
Training Epoch: 91 [12416/50048]	Loss: 0.0775
Training Epoch: 91 [12544/50048]	Loss: 0.0980
Training Epoch: 91 [12672/50048]	Loss: 0.0707
Training Epoch: 91 [12800/50048]	Loss: 0.0557
Training Epoch: 91 [12928/50048]	Loss: 0.0640
Training Epoch: 91 [13056/50048]	Loss: 0.0517
Training Epoch: 91 [13184/50048]	Loss: 0.0724
Training Epoch: 91 [13312/50048]	Loss: 0.0546
Training Epoch: 91 [13440/50048]	Loss: 0.0699
Training Epoch: 91 [13568/50048]	Loss: 0.0684
Training Epoch: 91 [13696/50048]	Loss: 0.1387
Training Epoch: 91 [13824/50048]	Loss: 0.0904
Training Epoch: 91 [13952/50048]	Loss: 0.1333
Training Epoch: 91 [14080/50048]	Loss: 0.0621
Training Epoch: 91 [14208/50048]	Loss: 0.1098
Training Epoch: 91 [14336/50048]	Loss: 0.0758
Training Epoch: 91 [14464/50048]	Loss: 0.0943
Training Epoch: 91 [14592/50048]	Loss: 0.1078
Training Epoch: 91 [14720/50048]	Loss: 0.1130
Training Epoch: 91 [14848/50048]	Loss: 0.1138
Training Epoch: 91 [14976/50048]	Loss: 0.0674
Training Epoch: 91 [15104/50048]	Loss: 0.0737
Training Epoch: 91 [15232/50048]	Loss: 0.1221
Training Epoch: 91 [15360/50048]	Loss: 0.1175
Training Epoch: 91 [15488/50048]	Loss: 0.0435
Training Epoch: 91 [15616/50048]	Loss: 0.0734
Training Epoch: 91 [15744/50048]	Loss: 0.1280
Training Epoch: 91 [15872/50048]	Loss: 0.0898
Training Epoch: 91 [16000/50048]	Loss: 0.0547
Training Epoch: 91 [16128/50048]	Loss: 0.1064
Training Epoch: 91 [16256/50048]	Loss: 0.0853
Training Epoch: 91 [16384/50048]	Loss: 0.1411
Training Epoch: 91 [16512/50048]	Loss: 0.0295
Training Epoch: 91 [16640/50048]	Loss: 0.1632
Training Epoch: 91 [16768/50048]	Loss: 0.0906
Training Epoch: 91 [16896/50048]	Loss: 0.0573
Training Epoch: 91 [17024/50048]	Loss: 0.0973
Training Epoch: 91 [17152/50048]	Loss: 0.0472
Training Epoch: 91 [17280/50048]	Loss: 0.1404
Training Epoch: 91 [17408/50048]	Loss: 0.1318
Training Epoch: 91 [17536/50048]	Loss: 0.1002
Training Epoch: 91 [17664/50048]	Loss: 0.1023
Training Epoch: 91 [17792/50048]	Loss: 0.0725
Training Epoch: 91 [17920/50048]	Loss: 0.1283
Training Epoch: 91 [18048/50048]	Loss: 0.0649
Training Epoch: 91 [18176/50048]	Loss: 0.1376
Training Epoch: 91 [18304/50048]	Loss: 0.0425
Training Epoch: 91 [18432/50048]	Loss: 0.1459
Training Epoch: 91 [18560/50048]	Loss: 0.0724
Training Epoch: 91 [18688/50048]	Loss: 0.0553
Training Epoch: 91 [18816/50048]	Loss: 0.0495
Training Epoch: 91 [18944/50048]	Loss: 0.1003
Training Epoch: 91 [19072/50048]	Loss: 0.2483
Training Epoch: 91 [19200/50048]	Loss: 0.0800
Training Epoch: 91 [19328/50048]	Loss: 0.1286
Training Epoch: 91 [19456/50048]	Loss: 0.0957
Training Epoch: 91 [19584/50048]	Loss: 0.1372
Training Epoch: 91 [19712/50048]	Loss: 0.0941
Training Epoch: 91 [19840/50048]	Loss: 0.0356
Training Epoch: 91 [19968/50048]	Loss: 0.1608
Training Epoch: 91 [20096/50048]	Loss: 0.0565
Training Epoch: 91 [20224/50048]	Loss: 0.0840
Training Epoch: 91 [20352/50048]	Loss: 0.0273
Training Epoch: 91 [20480/50048]	Loss: 0.0587
Training Epoch: 91 [20608/50048]	Loss: 0.0932
Training Epoch: 91 [20736/50048]	Loss: 0.0695
Training Epoch: 91 [20864/50048]	Loss: 0.0212
Training Epoch: 91 [20992/50048]	Loss: 0.0370
Training Epoch: 91 [21120/50048]	Loss: 0.1621
Training Epoch: 91 [21248/50048]	Loss: 0.0653
Training Epoch: 91 [21376/50048]	Loss: 0.0797
Training Epoch: 91 [21504/50048]	Loss: 0.0835
Training Epoch: 91 [21632/50048]	Loss: 0.1363
Training Epoch: 91 [21760/50048]	Loss: 0.0910
Training Epoch: 91 [21888/50048]	Loss: 0.1074
Training Epoch: 91 [22016/50048]	Loss: 0.1214
Training Epoch: 91 [22144/50048]	Loss: 0.0995
Training Epoch: 91 [22272/50048]	Loss: 0.1051
Training Epoch: 91 [22400/50048]	Loss: 0.1203
Training Epoch: 91 [22528/50048]	Loss: 0.0775
Training Epoch: 91 [22656/50048]	Loss: 0.1008
Training Epoch: 91 [22784/50048]	Loss: 0.1292
Training Epoch: 91 [22912/50048]	Loss: 0.0498
Training Epoch: 91 [23040/50048]	Loss: 0.0636
Training Epoch: 91 [23168/50048]	Loss: 0.1016
Training Epoch: 91 [23296/50048]	Loss: 0.0730
Training Epoch: 91 [23424/50048]	Loss: 0.0193
Training Epoch: 91 [23552/50048]	Loss: 0.0794
Training Epoch: 91 [23680/50048]	Loss: 0.1015
Training Epoch: 91 [23808/50048]	Loss: 0.0490
Training Epoch: 91 [23936/50048]	Loss: 0.0903
Training Epoch: 91 [24064/50048]	Loss: 0.0855
Training Epoch: 91 [24192/50048]	Loss: 0.0836
Training Epoch: 91 [24320/50048]	Loss: 0.1621
Training Epoch: 91 [24448/50048]	Loss: 0.0582
Training Epoch: 91 [24576/50048]	Loss: 0.0787
Training Epoch: 91 [24704/50048]	Loss: 0.0817
Training Epoch: 91 [24832/50048]	Loss: 0.1009
Training Epoch: 91 [24960/50048]	Loss: 0.1835
Training Epoch: 91 [25088/50048]	Loss: 0.0662
Training Epoch: 91 [25216/50048]	Loss: 0.1238
Training Epoch: 91 [25344/50048]	Loss: 0.1052
Training Epoch: 91 [25472/50048]	Loss: 0.0764
Training Epoch: 91 [25600/50048]	Loss: 0.1172
Training Epoch: 91 [25728/50048]	Loss: 0.1473
Training Epoch: 91 [25856/50048]	Loss: 0.1428
Training Epoch: 91 [25984/50048]	Loss: 0.1254
Training Epoch: 91 [26112/50048]	Loss: 0.1028
Training Epoch: 91 [26240/50048]	Loss: 0.0993
Training Epoch: 91 [26368/50048]	Loss: 0.0753
Training Epoch: 91 [26496/50048]	Loss: 0.1417
Training Epoch: 91 [26624/50048]	Loss: 0.1013
Training Epoch: 91 [26752/50048]	Loss: 0.0517
Training Epoch: 91 [26880/50048]	Loss: 0.0805
Training Epoch: 91 [27008/50048]	Loss: 0.0746
Training Epoch: 91 [27136/50048]	Loss: 0.0903
Training Epoch: 91 [27264/50048]	Loss: 0.0425
Training Epoch: 91 [27392/50048]	Loss: 0.0620
Training Epoch: 91 [27520/50048]	Loss: 0.0340
Training Epoch: 91 [27648/50048]	Loss: 0.1132
Training Epoch: 91 [27776/50048]	Loss: 0.0731
Training Epoch: 91 [27904/50048]	Loss: 0.0757
Training Epoch: 91 [28032/50048]	Loss: 0.0563
Training Epoch: 91 [28160/50048]	Loss: 0.0638
Training Epoch: 91 [28288/50048]	Loss: 0.1489
Training Epoch: 91 [28416/50048]	Loss: 0.1498
Training Epoch: 91 [28544/50048]	Loss: 0.0455
Training Epoch: 91 [28672/50048]	Loss: 0.0536
Training Epoch: 91 [28800/50048]	Loss: 0.1162
Training Epoch: 91 [28928/50048]	Loss: 0.1114
Training Epoch: 91 [29056/50048]	Loss: 0.0565
Training Epoch: 91 [29184/50048]	Loss: 0.1439
Training Epoch: 91 [29312/50048]	Loss: 0.0978
Training Epoch: 91 [29440/50048]	Loss: 0.0912
Training Epoch: 91 [29568/50048]	Loss: 0.0953
Training Epoch: 91 [29696/50048]	Loss: 0.0710
Training Epoch: 91 [29824/50048]	Loss: 0.0798
Training Epoch: 91 [29952/50048]	Loss: 0.1234
Training Epoch: 91 [30080/50048]	Loss: 0.1390
Training Epoch: 91 [30208/50048]	Loss: 0.0932
Training Epoch: 91 [30336/50048]	Loss: 0.0597
Training Epoch: 91 [30464/50048]	Loss: 0.0715
Training Epoch: 91 [30592/50048]	Loss: 0.0617
Training Epoch: 91 [30720/50048]	Loss: 0.2360
Training Epoch: 91 [30848/50048]	Loss: 0.0835
Training Epoch: 91 [30976/50048]	Loss: 0.0358
Training Epoch: 91 [31104/50048]	Loss: 0.1273
Training Epoch: 91 [31232/50048]	Loss: 0.1049
Training Epoch: 91 [31360/50048]	Loss: 0.0584
Training Epoch: 91 [31488/50048]	Loss: 0.1017
Training Epoch: 91 [31616/50048]	Loss: 0.0664
Training Epoch: 91 [31744/50048]	Loss: 0.0587
Training Epoch: 91 [31872/50048]	Loss: 0.1204
Training Epoch: 91 [32000/50048]	Loss: 0.0625
Training Epoch: 91 [32128/50048]	Loss: 0.0831
Training Epoch: 91 [32256/50048]	Loss: 0.1281
Training Epoch: 91 [32384/50048]	Loss: 0.0443
Training Epoch: 91 [32512/50048]	Loss: 0.0785
Training Epoch: 91 [32640/50048]	Loss: 0.1769
Training Epoch: 91 [32768/50048]	Loss: 0.0968
Training Epoch: 91 [32896/50048]	Loss: 0.0831
Training Epoch: 91 [33024/50048]	Loss: 0.0624
Training Epoch: 91 [33152/50048]	Loss: 0.0529
Training Epoch: 91 [33280/50048]	Loss: 0.0541
Training Epoch: 91 [33408/50048]	Loss: 0.0940
Training Epoch: 91 [33536/50048]	Loss: 0.1164
Training Epoch: 91 [33664/50048]	Loss: 0.1003
Training Epoch: 91 [33792/50048]	Loss: 0.0411
Training Epoch: 91 [33920/50048]	Loss: 0.0589
Training Epoch: 91 [34048/50048]	Loss: 0.0592
Training Epoch: 91 [34176/50048]	Loss: 0.1637
Training Epoch: 91 [34304/50048]	Loss: 0.0881
Training Epoch: 91 [34432/50048]	Loss: 0.0478
Training Epoch: 91 [34560/50048]	Loss: 0.0637
Training Epoch: 91 [34688/50048]	Loss: 0.0586
Training Epoch: 91 [34816/50048]	Loss: 0.0638
Training Epoch: 91 [34944/50048]	Loss: 0.0612
Training Epoch: 91 [35072/50048]	Loss: 0.1057
Training Epoch: 91 [35200/50048]	Loss: 0.1541
Training Epoch: 91 [35328/50048]	Loss: 0.0776
Training Epoch: 91 [35456/50048]	Loss: 0.0577
Training Epoch: 91 [35584/50048]	Loss: 0.0324
Training Epoch: 91 [35712/50048]	Loss: 0.0771
Training Epoch: 91 [35840/50048]	Loss: 0.1375
Training Epoch: 91 [35968/50048]	Loss: 0.1306
Training Epoch: 91 [36096/50048]	Loss: 0.0416
Training Epoch: 91 [36224/50048]	Loss: 0.0908
Training Epoch: 91 [36352/50048]	Loss: 0.0877
Training Epoch: 91 [36480/50048]	Loss: 0.0863
Training Epoch: 91 [36608/50048]	Loss: 0.0513
Training Epoch: 91 [36736/50048]	Loss: 0.0696
Training Epoch: 91 [36864/50048]	Loss: 0.1332
Training Epoch: 91 [36992/50048]	Loss: 0.0456
Training Epoch: 91 [37120/50048]	Loss: 0.1372
Training Epoch: 91 [37248/50048]	Loss: 0.0399
Training Epoch: 91 [37376/50048]	Loss: 0.0662
Training Epoch: 91 [37504/50048]	Loss: 0.1123
Training Epoch: 91 [37632/50048]	Loss: 0.0611
Training Epoch: 91 [37760/50048]	Loss: 0.1370
Training Epoch: 91 [37888/50048]	Loss: 0.0524
Training Epoch: 91 [38016/50048]	Loss: 0.0633
Training Epoch: 91 [38144/50048]	Loss: 0.0900
Training Epoch: 91 [38272/50048]	Loss: 0.0911
Training Epoch: 91 [38400/50048]	Loss: 0.0589
Training Epoch: 91 [38528/50048]	Loss: 0.1124
Training Epoch: 91 [38656/50048]	Loss: 0.1196
Training Epoch: 91 [38784/50048]	Loss: 0.0349
Training Epoch: 91 [38912/50048]	Loss: 0.0692
Training Epoch: 91 [39040/50048]	Loss: 0.0532
Training Epoch: 91 [39168/50048]	Loss: 0.0665
Training Epoch: 91 [39296/50048]	Loss: 0.0520
Training Epoch: 91 [39424/50048]	Loss: 0.0693
Training Epoch: 91 [39552/50048]	Loss: 0.1169
Training Epoch: 91 [39680/50048]	Loss: 0.0750
Training Epoch: 91 [39808/50048]	Loss: 0.0715
Training Epoch: 91 [39936/50048]	Loss: 0.1103
Training Epoch: 91 [40064/50048]	Loss: 0.0905
Training Epoch: 91 [40192/50048]	Loss: 0.1644
Training Epoch: 91 [40320/50048]	Loss: 0.1157
Training Epoch: 91 [40448/50048]	Loss: 0.0710
Training Epoch: 91 [40576/50048]	Loss: 0.1136
Training Epoch: 91 [40704/50048]	Loss: 0.0421
Training Epoch: 91 [40832/50048]	Loss: 0.0941
Training Epoch: 91 [40960/50048]	Loss: 0.0576
Training Epoch: 91 [41088/50048]	Loss: 0.0756
Training Epoch: 91 [41216/50048]	Loss: 0.0544
Training Epoch: 91 [41344/50048]	Loss: 0.1221
Training Epoch: 91 [41472/50048]	Loss: 0.0838
Training Epoch: 91 [41600/50048]	Loss: 0.0180
Training Epoch: 91 [41728/50048]	Loss: 0.1443
Training Epoch: 91 [41856/50048]	Loss: 0.0918
Training Epoch: 91 [41984/50048]	Loss: 0.1017
Training Epoch: 91 [42112/50048]	Loss: 0.1658
Training Epoch: 91 [42240/50048]	Loss: 0.0736
Training Epoch: 91 [42368/50048]	Loss: 0.1056
Training Epoch: 91 [42496/50048]	Loss: 0.0997
Training Epoch: 91 [42624/50048]	Loss: 0.0319
Training Epoch: 91 [42752/50048]	Loss: 0.0533
Training Epoch: 91 [42880/50048]	Loss: 0.0659
Training Epoch: 91 [43008/50048]	Loss: 0.1521
Training Epoch: 91 [43136/50048]	Loss: 0.2029
Training Epoch: 91 [43264/50048]	Loss: 0.1082
Training Epoch: 91 [43392/50048]	Loss: 0.1353
Training Epoch: 91 [43520/50048]	Loss: 0.0681
Training Epoch: 91 [43648/50048]	Loss: 0.0497
Training Epoch: 91 [43776/50048]	Loss: 0.0763
Training Epoch: 91 [43904/50048]	Loss: 0.1266
Training Epoch: 91 [44032/50048]	Loss: 0.1352
Training Epoch: 91 [44160/50048]	Loss: 0.1307
Training Epoch: 91 [44288/50048]	Loss: 0.0707
Training Epoch: 91 [44416/50048]	Loss: 0.0353
Training Epoch: 91 [44544/50048]	Loss: 0.0535
Training Epoch: 91 [44672/50048]	Loss: 0.0855
Training Epoch: 91 [44800/50048]	Loss: 0.1169
Training Epoch: 91 [44928/50048]	Loss: 0.0770
Training Epoch: 91 [45056/50048]	Loss: 0.1140
Training Epoch: 91 [45184/50048]	Loss: 0.0995
Training Epoch: 91 [45312/50048]	Loss: 0.1091
Training Epoch: 91 [45440/50048]	Loss: 0.1242
Training Epoch: 91 [45568/50048]	Loss: 0.1369
Training Epoch: 91 [45696/50048]	Loss: 0.0928
2022-12-06 05:55:40,484 [ZeusDataLoader(train)] train epoch 92 done: time=86.49 energy=10495.61
2022-12-06 05:55:40,485 [ZeusDataLoader(eval)] Epoch 92 begin.
Training Epoch: 91 [45824/50048]	Loss: 0.0444
Training Epoch: 91 [45952/50048]	Loss: 0.0784
Training Epoch: 91 [46080/50048]	Loss: 0.0775
Training Epoch: 91 [46208/50048]	Loss: 0.0769
Training Epoch: 91 [46336/50048]	Loss: 0.0941
Training Epoch: 91 [46464/50048]	Loss: 0.0705
Training Epoch: 91 [46592/50048]	Loss: 0.0680
Training Epoch: 91 [46720/50048]	Loss: 0.1484
Training Epoch: 91 [46848/50048]	Loss: 0.0482
Training Epoch: 91 [46976/50048]	Loss: 0.0751
Training Epoch: 91 [47104/50048]	Loss: 0.0322
Training Epoch: 91 [47232/50048]	Loss: 0.1715
Training Epoch: 91 [47360/50048]	Loss: 0.0731
Training Epoch: 91 [47488/50048]	Loss: 0.0562
Training Epoch: 91 [47616/50048]	Loss: 0.0864
Training Epoch: 91 [47744/50048]	Loss: 0.1053
Training Epoch: 91 [47872/50048]	Loss: 0.1006
Training Epoch: 91 [48000/50048]	Loss: 0.0700
Training Epoch: 91 [48128/50048]	Loss: 0.0938
Training Epoch: 91 [48256/50048]	Loss: 0.0740
Training Epoch: 91 [48384/50048]	Loss: 0.0431
Training Epoch: 91 [48512/50048]	Loss: 0.0839
Training Epoch: 91 [48640/50048]	Loss: 0.1367
Training Epoch: 91 [48768/50048]	Loss: 0.1413
Training Epoch: 91 [48896/50048]	Loss: 0.0789
Training Epoch: 91 [49024/50048]	Loss: 0.0529
Training Epoch: 91 [49152/50048]	Loss: 0.0708
Training Epoch: 91 [49280/50048]	Loss: 0.0441
Training Epoch: 91 [49408/50048]	Loss: 0.0555
Training Epoch: 91 [49536/50048]	Loss: 0.0826
Training Epoch: 91 [49664/50048]	Loss: 0.1064
Training Epoch: 91 [49792/50048]	Loss: 0.0617
Training Epoch: 91 [49920/50048]	Loss: 0.1505
Training Epoch: 91 [50048/50048]	Loss: 0.0964
2022-12-06 10:55:44.135 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:55:44,145 [ZeusDataLoader(eval)] eval epoch 92 done: time=3.65 energy=440.53
2022-12-06 05:55:44,145 [ZeusDataLoader(train)] Up to epoch 92: time=8299.86, energy=1007581.35, cost=1230028.45
2022-12-06 05:55:44,145 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:55:44,145 [ZeusDataLoader(train)] Expected next epoch: time=8389.66, energy=1018379.36, cost=1243284.83
2022-12-06 05:55:44,146 [ZeusDataLoader(train)] Epoch 93 begin.
Validation Epoch: 91, Average loss: 0.0187, Accuracy: 0.6459
2022-12-06 05:55:44,286 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:55:44,286 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:55:44.290 [ZeusMonitor] Monitor started.
2022-12-06 10:55:44.290 [ZeusMonitor] Running indefinitely. 2022-12-06 10:55:44.290 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:55:44.290 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e93+gpu0.power.log
Training Epoch: 92 [128/50048]	Loss: 0.1169
Training Epoch: 92 [256/50048]	Loss: 0.0543
Training Epoch: 92 [384/50048]	Loss: 0.0936
Training Epoch: 92 [512/50048]	Loss: 0.1294
Training Epoch: 92 [640/50048]	Loss: 0.0605
Training Epoch: 92 [768/50048]	Loss: 0.1714
Training Epoch: 92 [896/50048]	Loss: 0.0685
Training Epoch: 92 [1024/50048]	Loss: 0.0457
Training Epoch: 92 [1152/50048]	Loss: 0.0697
Training Epoch: 92 [1280/50048]	Loss: 0.0557
Training Epoch: 92 [1408/50048]	Loss: 0.0731
Training Epoch: 92 [1536/50048]	Loss: 0.0400
Training Epoch: 92 [1664/50048]	Loss: 0.0731
Training Epoch: 92 [1792/50048]	Loss: 0.0292
Training Epoch: 92 [1920/50048]	Loss: 0.0907
Training Epoch: 92 [2048/50048]	Loss: 0.1018
Training Epoch: 92 [2176/50048]	Loss: 0.0666
Training Epoch: 92 [2304/50048]	Loss: 0.0602
Training Epoch: 92 [2432/50048]	Loss: 0.0421
Training Epoch: 92 [2560/50048]	Loss: 0.0977
Training Epoch: 92 [2688/50048]	Loss: 0.0653
Training Epoch: 92 [2816/50048]	Loss: 0.1127
Training Epoch: 92 [2944/50048]	Loss: 0.0472
Training Epoch: 92 [3072/50048]	Loss: 0.0364
Training Epoch: 92 [3200/50048]	Loss: 0.1020
Training Epoch: 92 [3328/50048]	Loss: 0.0516
Training Epoch: 92 [3456/50048]	Loss: 0.0932
Training Epoch: 92 [3584/50048]	Loss: 0.0511
Training Epoch: 92 [3712/50048]	Loss: 0.0303
Training Epoch: 92 [3840/50048]	Loss: 0.0473
Training Epoch: 92 [3968/50048]	Loss: 0.0248
Training Epoch: 92 [4096/50048]	Loss: 0.0943
Training Epoch: 92 [4224/50048]	Loss: 0.0565
Training Epoch: 92 [4352/50048]	Loss: 0.1294
Training Epoch: 92 [4480/50048]	Loss: 0.0563
Training Epoch: 92 [4608/50048]	Loss: 0.0819
Training Epoch: 92 [4736/50048]	Loss: 0.1443
Training Epoch: 92 [4864/50048]	Loss: 0.1153
Training Epoch: 92 [4992/50048]	Loss: 0.0433
Training Epoch: 92 [5120/50048]	Loss: 0.0842
Training Epoch: 92 [5248/50048]	Loss: 0.0850
Training Epoch: 92 [5376/50048]	Loss: 0.1004
Training Epoch: 92 [5504/50048]	Loss: 0.0927
Training Epoch: 92 [5632/50048]	Loss: 0.0593
Training Epoch: 92 [5760/50048]	Loss: 0.0642
Training Epoch: 92 [5888/50048]	Loss: 0.0837
Training Epoch: 92 [6016/50048]	Loss: 0.0646
Training Epoch: 92 [6144/50048]	Loss: 0.0545
Training Epoch: 92 [6272/50048]	Loss: 0.0722
Training Epoch: 92 [6400/50048]	Loss: 0.0705
Training Epoch: 92 [6528/50048]	Loss: 0.0815
Training Epoch: 92 [6656/50048]	Loss: 0.0667
Training Epoch: 92 [6784/50048]	Loss: 0.1178
Training Epoch: 92 [6912/50048]	Loss: 0.1298
Training Epoch: 92 [7040/50048]	Loss: 0.1540
Training Epoch: 92 [7168/50048]	Loss: 0.0816
Training Epoch: 92 [7296/50048]	Loss: 0.0835
Training Epoch: 92 [7424/50048]	Loss: 0.1922
Training Epoch: 92 [7552/50048]	Loss: 0.0751
Training Epoch: 92 [7680/50048]	Loss: 0.0727
Training Epoch: 92 [7808/50048]	Loss: 0.0633
Training Epoch: 92 [7936/50048]	Loss: 0.0919
Training Epoch: 92 [8064/50048]	Loss: 0.1009
Training Epoch: 92 [8192/50048]	Loss: 0.0686
Training Epoch: 92 [8320/50048]	Loss: 0.0964
Training Epoch: 92 [8448/50048]	Loss: 0.0430
Training Epoch: 92 [8576/50048]	Loss: 0.0236
Training Epoch: 92 [8704/50048]	Loss: 0.0822
Training Epoch: 92 [8832/50048]	Loss: 0.0872
Training Epoch: 92 [8960/50048]	Loss: 0.0513
Training Epoch: 92 [9088/50048]	Loss: 0.0511
Training Epoch: 92 [9216/50048]	Loss: 0.0406
Training Epoch: 92 [9344/50048]	Loss: 0.0164
Training Epoch: 92 [9472/50048]	Loss: 0.0856
Training Epoch: 92 [9600/50048]	Loss: 0.0575
Training Epoch: 92 [9728/50048]	Loss: 0.0668
Training Epoch: 92 [9856/50048]	Loss: 0.0709
Training Epoch: 92 [9984/50048]	Loss: 0.0367
Training Epoch: 92 [10112/50048]	Loss: 0.1471
Training Epoch: 92 [10240/50048]	Loss: 0.0392
Training Epoch: 92 [10368/50048]	Loss: 0.1009
Training Epoch: 92 [10496/50048]	Loss: 0.0937
Training Epoch: 92 [10624/50048]	Loss: 0.0485
Training Epoch: 92 [10752/50048]	Loss: 0.0607
Training Epoch: 92 [10880/50048]	Loss: 0.0635
Training Epoch: 92 [11008/50048]	Loss: 0.0327
Training Epoch: 92 [11136/50048]	Loss: 0.0758
Training Epoch: 92 [11264/50048]	Loss: 0.0420
Training Epoch: 92 [11392/50048]	Loss: 0.0775
Training Epoch: 92 [11520/50048]	Loss: 0.0317
Training Epoch: 92 [11648/50048]	Loss: 0.0515
Training Epoch: 92 [11776/50048]	Loss: 0.0825
Training Epoch: 92 [11904/50048]	Loss: 0.0795
Training Epoch: 92 [12032/50048]	Loss: 0.0460
Training Epoch: 92 [12160/50048]	Loss: 0.0629
Training Epoch: 92 [12288/50048]	Loss: 0.0924
Training Epoch: 92 [12416/50048]	Loss: 0.0749
Training Epoch: 92 [12544/50048]	Loss: 0.0280
Training Epoch: 92 [12672/50048]	Loss: 0.0270
Training Epoch: 92 [12800/50048]	Loss: 0.0953
Training Epoch: 92 [12928/50048]	Loss: 0.0575
Training Epoch: 92 [13056/50048]	Loss: 0.0482
Training Epoch: 92 [13184/50048]	Loss: 0.0825
Training Epoch: 92 [13312/50048]	Loss: 0.1437
Training Epoch: 92 [13440/50048]	Loss: 0.0638
Training Epoch: 92 [13568/50048]	Loss: 0.0659
Training Epoch: 92 [13696/50048]	Loss: 0.0513
Training Epoch: 92 [13824/50048]	Loss: 0.0664
Training Epoch: 92 [13952/50048]	Loss: 0.1495
Training Epoch: 92 [14080/50048]	Loss: 0.1522
Training Epoch: 92 [14208/50048]	Loss: 0.1146
Training Epoch: 92 [14336/50048]	Loss: 0.0489
Training Epoch: 92 [14464/50048]	Loss: 0.0497
Training Epoch: 92 [14592/50048]	Loss: 0.1091
Training Epoch: 92 [14720/50048]	Loss: 0.1223
Training Epoch: 92 [14848/50048]	Loss: 0.1337
Training Epoch: 92 [14976/50048]	Loss: 0.0544
Training Epoch: 92 [15104/50048]	Loss: 0.0384
Training Epoch: 92 [15232/50048]	Loss: 0.0494
Training Epoch: 92 [15360/50048]	Loss: 0.0599
Training Epoch: 92 [15488/50048]	Loss: 0.0493
Training Epoch: 92 [15616/50048]	Loss: 0.0626
Training Epoch: 92 [15744/50048]	Loss: 0.0543
Training Epoch: 92 [15872/50048]	Loss: 0.1281
Training Epoch: 92 [16000/50048]	Loss: 0.1363
Training Epoch: 92 [16128/50048]	Loss: 0.1846
Training Epoch: 92 [16256/50048]	Loss: 0.0673
Training Epoch: 92 [16384/50048]	Loss: 0.0810
Training Epoch: 92 [16512/50048]	Loss: 0.0542
Training Epoch: 92 [16640/50048]	Loss: 0.0537
Training Epoch: 92 [16768/50048]	Loss: 0.0491
Training Epoch: 92 [16896/50048]	Loss: 0.0401
Training Epoch: 92 [17024/50048]	Loss: 0.1801
Training Epoch: 92 [17152/50048]	Loss: 0.1290
Training Epoch: 92 [17280/50048]	Loss: 0.0274
Training Epoch: 92 [17408/50048]	Loss: 0.0686
Training Epoch: 92 [17536/50048]	Loss: 0.0532
Training Epoch: 92 [17664/50048]	Loss: 0.0370
Training Epoch: 92 [17792/50048]	Loss: 0.0604
Training Epoch: 92 [17920/50048]	Loss: 0.1209
Training Epoch: 92 [18048/50048]	Loss: 0.0505
Training Epoch: 92 [18176/50048]	Loss: 0.1109
Training Epoch: 92 [18304/50048]	Loss: 0.0587
Training Epoch: 92 [18432/50048]	Loss: 0.0405
Training Epoch: 92 [18560/50048]	Loss: 0.0967
Training Epoch: 92 [18688/50048]	Loss: 0.0400
Training Epoch: 92 [18816/50048]	Loss: 0.1010
Training Epoch: 92 [18944/50048]	Loss: 0.0864
Training Epoch: 92 [19072/50048]	Loss: 0.0978
Training Epoch: 92 [19200/50048]	Loss: 0.0910
Training Epoch: 92 [19328/50048]	Loss: 0.1005
Training Epoch: 92 [19456/50048]	Loss: 0.0762
Training Epoch: 92 [19584/50048]	Loss: 0.1055
Training Epoch: 92 [19712/50048]	Loss: 0.0885
Training Epoch: 92 [19840/50048]	Loss: 0.0819
Training Epoch: 92 [19968/50048]	Loss: 0.0590
Training Epoch: 92 [20096/50048]	Loss: 0.1148
Training Epoch: 92 [20224/50048]	Loss: 0.0852
Training Epoch: 92 [20352/50048]	Loss: 0.0949
Training Epoch: 92 [20480/50048]	Loss: 0.0659
Training Epoch: 92 [20608/50048]	Loss: 0.1012
Training Epoch: 92 [20736/50048]	Loss: 0.1586
Training Epoch: 92 [20864/50048]	Loss: 0.1950
Training Epoch: 92 [20992/50048]	Loss: 0.0831
Training Epoch: 92 [21120/50048]	Loss: 0.0293
Training Epoch: 92 [21248/50048]	Loss: 0.0414
Training Epoch: 92 [21376/50048]	Loss: 0.0338
Training Epoch: 92 [21504/50048]	Loss: 0.1212
Training Epoch: 92 [21632/50048]	Loss: 0.0752
Training Epoch: 92 [21760/50048]	Loss: 0.0368
Training Epoch: 92 [21888/50048]	Loss: 0.0728
Training Epoch: 92 [22016/50048]	Loss: 0.0314
Training Epoch: 92 [22144/50048]	Loss: 0.0861
Training Epoch: 92 [22272/50048]	Loss: 0.0958
Training Epoch: 92 [22400/50048]	Loss: 0.1666
Training Epoch: 92 [22528/50048]	Loss: 0.1232
Training Epoch: 92 [22656/50048]	Loss: 0.1438
Training Epoch: 92 [22784/50048]	Loss: 0.0754
Training Epoch: 92 [22912/50048]	Loss: 0.0760
Training Epoch: 92 [23040/50048]	Loss: 0.0694
Training Epoch: 92 [23168/50048]	Loss: 0.0750
Training Epoch: 92 [23296/50048]	Loss: 0.1056
Training Epoch: 92 [23424/50048]	Loss: 0.0623
Training Epoch: 92 [23552/50048]	Loss: 0.0688
Training Epoch: 92 [23680/50048]	Loss: 0.0833
Training Epoch: 92 [23808/50048]	Loss: 0.0903
Training Epoch: 92 [23936/50048]	Loss: 0.0942
Training Epoch: 92 [24064/50048]	Loss: 0.1123
Training Epoch: 92 [24192/50048]	Loss: 0.1217
Training Epoch: 92 [24320/50048]	Loss: 0.0463
Training Epoch: 92 [24448/50048]	Loss: 0.0768
Training Epoch: 92 [24576/50048]	Loss: 0.0651
Training Epoch: 92 [24704/50048]	Loss: 0.0990
Training Epoch: 92 [24832/50048]	Loss: 0.0279
Training Epoch: 92 [24960/50048]	Loss: 0.0822
Training Epoch: 92 [25088/50048]	Loss: 0.1268
Training Epoch: 92 [25216/50048]	Loss: 0.0788
Training Epoch: 92 [25344/50048]	Loss: 0.1192
Training Epoch: 92 [25472/50048]	Loss: 0.1243
Training Epoch: 92 [25600/50048]	Loss: 0.0381
Training Epoch: 92 [25728/50048]	Loss: 0.1258
Training Epoch: 92 [25856/50048]	Loss: 0.0384
Training Epoch: 92 [25984/50048]	Loss: 0.1541
Training Epoch: 92 [26112/50048]	Loss: 0.0469
Training Epoch: 92 [26240/50048]	Loss: 0.0739
Training Epoch: 92 [26368/50048]	Loss: 0.0897
Training Epoch: 92 [26496/50048]	Loss: 0.0687
Training Epoch: 92 [26624/50048]	Loss: 0.0764
Training Epoch: 92 [26752/50048]	Loss: 0.1060
Training Epoch: 92 [26880/50048]	Loss: 0.1690
Training Epoch: 92 [27008/50048]	Loss: 0.1191
Training Epoch: 92 [27136/50048]	Loss: 0.0686
Training Epoch: 92 [27264/50048]	Loss: 0.0691
Training Epoch: 92 [27392/50048]	Loss: 0.0828
Training Epoch: 92 [27520/50048]	Loss: 0.0955
Training Epoch: 92 [27648/50048]	Loss: 0.1529
Training Epoch: 92 [27776/50048]	Loss: 0.0352
Training Epoch: 92 [27904/50048]	Loss: 0.0360
Training Epoch: 92 [28032/50048]	Loss: 0.0673
Training Epoch: 92 [28160/50048]	Loss: 0.0587
Training Epoch: 92 [28288/50048]	Loss: 0.0462
Training Epoch: 92 [28416/50048]	Loss: 0.1058
Training Epoch: 92 [28544/50048]	Loss: 0.0922
Training Epoch: 92 [28672/50048]	Loss: 0.0615
Training Epoch: 92 [28800/50048]	Loss: 0.0641
Training Epoch: 92 [28928/50048]	Loss: 0.0863
Training Epoch: 92 [29056/50048]	Loss: 0.1076
Training Epoch: 92 [29184/50048]	Loss: 0.0485
Training Epoch: 92 [29312/50048]	Loss: 0.1088
Training Epoch: 92 [29440/50048]	Loss: 0.0387
Training Epoch: 92 [29568/50048]	Loss: 0.0716
Training Epoch: 92 [29696/50048]	Loss: 0.0584
Training Epoch: 92 [29824/50048]	Loss: 0.0394
Training Epoch: 92 [29952/50048]	Loss: 0.0747
Training Epoch: 92 [30080/50048]	Loss: 0.0681
Training Epoch: 92 [30208/50048]	Loss: 0.0653
Training Epoch: 92 [30336/50048]	Loss: 0.0304
Training Epoch: 92 [30464/50048]	Loss: 0.0798
Training Epoch: 92 [30592/50048]	Loss: 0.1025
Training Epoch: 92 [30720/50048]	Loss: 0.1012
Training Epoch: 92 [30848/50048]	Loss: 0.0862
Training Epoch: 92 [30976/50048]	Loss: 0.0862
Training Epoch: 92 [31104/50048]	Loss: 0.0554
Training Epoch: 92 [31232/50048]	Loss: 0.1377
Training Epoch: 92 [31360/50048]	Loss: 0.0505
Training Epoch: 92 [31488/50048]	Loss: 0.1418
Training Epoch: 92 [31616/50048]	Loss: 0.1093
Training Epoch: 92 [31744/50048]	Loss: 0.0629
Training Epoch: 92 [31872/50048]	Loss: 0.0964
Training Epoch: 92 [32000/50048]	Loss: 0.1089
Training Epoch: 92 [32128/50048]	Loss: 0.0610
Training Epoch: 92 [32256/50048]	Loss: 0.0230
Training Epoch: 92 [32384/50048]	Loss: 0.1372
Training Epoch: 92 [32512/50048]	Loss: 0.1490
Training Epoch: 92 [32640/50048]	Loss: 0.1333
Training Epoch: 92 [32768/50048]	Loss: 0.1639
Training Epoch: 92 [32896/50048]	Loss: 0.0819
Training Epoch: 92 [33024/50048]	Loss: 0.0704
Training Epoch: 92 [33152/50048]	Loss: 0.0634
Training Epoch: 92 [33280/50048]	Loss: 0.1336
Training Epoch: 92 [33408/50048]	Loss: 0.0856
Training Epoch: 92 [33536/50048]	Loss: 0.0800
Training Epoch: 92 [33664/50048]	Loss: 0.0606
Training Epoch: 92 [33792/50048]	Loss: 0.0772
Training Epoch: 92 [33920/50048]	Loss: 0.0972
Training Epoch: 92 [34048/50048]	Loss: 0.0296
Training Epoch: 92 [34176/50048]	Loss: 0.0293
Training Epoch: 92 [34304/50048]	Loss: 0.1070
Training Epoch: 92 [34432/50048]	Loss: 0.0863
Training Epoch: 92 [34560/50048]	Loss: 0.0825
Training Epoch: 92 [34688/50048]	Loss: 0.1723
Training Epoch: 92 [34816/50048]	Loss: 0.0371
Training Epoch: 92 [34944/50048]	Loss: 0.0624
Training Epoch: 92 [35072/50048]	Loss: 0.0736
Training Epoch: 92 [35200/50048]	Loss: 0.0809
Training Epoch: 92 [35328/50048]	Loss: 0.0669
Training Epoch: 92 [35456/50048]	Loss: 0.0957
Training Epoch: 92 [35584/50048]	Loss: 0.0388
Training Epoch: 92 [35712/50048]	Loss: 0.0381
Training Epoch: 92 [35840/50048]	Loss: 0.0748
Training Epoch: 92 [35968/50048]	Loss: 0.0771
Training Epoch: 92 [36096/50048]	Loss: 0.0932
Training Epoch: 92 [36224/50048]	Loss: 0.1002
Training Epoch: 92 [36352/50048]	Loss: 0.0814
Training Epoch: 92 [36480/50048]	Loss: 0.1320
Training Epoch: 92 [36608/50048]	Loss: 0.1288
Training Epoch: 92 [36736/50048]	Loss: 0.0690
Training Epoch: 92 [36864/50048]	Loss: 0.0795
Training Epoch: 92 [36992/50048]	Loss: 0.0349
Training Epoch: 92 [37120/50048]	Loss: 0.0882
Training Epoch: 92 [37248/50048]	Loss: 0.0718
Training Epoch: 92 [37376/50048]	Loss: 0.0595
Training Epoch: 92 [37504/50048]	Loss: 0.0690
Training Epoch: 92 [37632/50048]	Loss: 0.0490
Training Epoch: 92 [37760/50048]	Loss: 0.0691
Training Epoch: 92 [37888/50048]	Loss: 0.0622
Training Epoch: 92 [38016/50048]	Loss: 0.0401
Training Epoch: 92 [38144/50048]	Loss: 0.1071
Training Epoch: 92 [38272/50048]	Loss: 0.0830
Training Epoch: 92 [38400/50048]	Loss: 0.0905
Training Epoch: 92 [38528/50048]	Loss: 0.0752
Training Epoch: 92 [38656/50048]	Loss: 0.0896
Training Epoch: 92 [38784/50048]	Loss: 0.1324
Training Epoch: 92 [38912/50048]	Loss: 0.1593
Training Epoch: 92 [39040/50048]	Loss: 0.0874
Training Epoch: 92 [39168/50048]	Loss: 0.0857
Training Epoch: 92 [39296/50048]	Loss: 0.0542
Training Epoch: 92 [39424/50048]	Loss: 0.0596
Training Epoch: 92 [39552/50048]	Loss: 0.0702
Training Epoch: 92 [39680/50048]	Loss: 0.1295
Training Epoch: 92 [39808/50048]	Loss: 0.0971
Training Epoch: 92 [39936/50048]	Loss: 0.1613
Training Epoch: 92 [40064/50048]	Loss: 0.0549
Training Epoch: 92 [40192/50048]	Loss: 0.1143
Training Epoch: 92 [40320/50048]	Loss: 0.0578
Training Epoch: 92 [40448/50048]	Loss: 0.1392
Training Epoch: 92 [40576/50048]	Loss: 0.0614
Training Epoch: 92 [40704/50048]	Loss: 0.0965
Training Epoch: 92 [40832/50048]	Loss: 0.1416
Training Epoch: 92 [40960/50048]	Loss: 0.1334
Training Epoch: 92 [41088/50048]	Loss: 0.1264
Training Epoch: 92 [41216/50048]	Loss: 0.0648
Training Epoch: 92 [41344/50048]	Loss: 0.1150
Training Epoch: 92 [41472/50048]	Loss: 0.0630
Training Epoch: 92 [41600/50048]	Loss: 0.0531
Training Epoch: 92 [41728/50048]	Loss: 0.0938
Training Epoch: 92 [41856/50048]	Loss: 0.0938
Training Epoch: 92 [41984/50048]	Loss: 0.1168
Training Epoch: 92 [42112/50048]	Loss: 0.0975
Training Epoch: 92 [42240/50048]	Loss: 0.0490
Training Epoch: 92 [42368/50048]	Loss: 0.0492
Training Epoch: 92 [42496/50048]	Loss: 0.1081
Training Epoch: 92 [42624/50048]	Loss: 0.0571
Training Epoch: 92 [42752/50048]	Loss: 0.1367
Training Epoch: 92 [42880/50048]	Loss: 0.1060
Training Epoch: 92 [43008/50048]	Loss: 0.0764
Training Epoch: 92 [43136/50048]	Loss: 0.0982
Training Epoch: 92 [43264/50048]	Loss: 0.0769
Training Epoch: 92 [43392/50048]	Loss: 0.0596
Training Epoch: 92 [43520/50048]	Loss: 0.0850
Training Epoch: 92 [43648/50048]	Loss: 0.0766
Training Epoch: 92 [43776/50048]	Loss: 0.0804
Training Epoch: 92 [43904/50048]	Loss: 0.1041
Training Epoch: 92 [44032/50048]	Loss: 0.0610
Training Epoch: 92 [44160/50048]	Loss: 0.1059
Training Epoch: 92 [44288/50048]	Loss: 0.0528
Training Epoch: 92 [44416/50048]	Loss: 0.1008
Training Epoch: 92 [44544/50048]	Loss: 0.1017
Training Epoch: 92 [44672/50048]	Loss: 0.1448
Training Epoch: 92 [44800/50048]	Loss: 0.0967
Training Epoch: 92 [44928/50048]	Loss: 0.0470
Training Epoch: 92 [45056/50048]	Loss: 0.0662
Training Epoch: 92 [45184/50048]	Loss: 0.0749
Training Epoch: 92 [45312/50048]	Loss: 0.1016
Training Epoch: 92 [45440/50048]	Loss: 0.1077
Training Epoch: 92 [45568/50048]	Loss: 0.0926
Training Epoch: 92 [45696/50048]	Loss: 0.1063
2022-12-06 05:57:10,596 [ZeusDataLoader(train)] train epoch 93 done: time=86.44 energy=10505.93
2022-12-06 05:57:10,597 [ZeusDataLoader(eval)] Epoch 93 begin.
Training Epoch: 92 [45824/50048]	Loss: 0.0888
Training Epoch: 92 [45952/50048]	Loss: 0.0747
Training Epoch: 92 [46080/50048]	Loss: 0.1268
Training Epoch: 92 [46208/50048]	Loss: 0.1045
Training Epoch: 92 [46336/50048]	Loss: 0.0546
Training Epoch: 92 [46464/50048]	Loss: 0.0486
Training Epoch: 92 [46592/50048]	Loss: 0.1597
Training Epoch: 92 [46720/50048]	Loss: 0.1209
Training Epoch: 92 [46848/50048]	Loss: 0.1015
Training Epoch: 92 [46976/50048]	Loss: 0.0689
Training Epoch: 92 [47104/50048]	Loss: 0.0935
Training Epoch: 92 [47232/50048]	Loss: 0.1149
Training Epoch: 92 [47360/50048]	Loss: 0.0867
Training Epoch: 92 [47488/50048]	Loss: 0.1373
Training Epoch: 92 [47616/50048]	Loss: 0.0754
Training Epoch: 92 [47744/50048]	Loss: 0.0881
Training Epoch: 92 [47872/50048]	Loss: 0.1497
Training Epoch: 92 [48000/50048]	Loss: 0.1296
Training Epoch: 92 [48128/50048]	Loss: 0.1509
Training Epoch: 92 [48256/50048]	Loss: 0.0917
Training Epoch: 92 [48384/50048]	Loss: 0.0968
Training Epoch: 92 [48512/50048]	Loss: 0.1287
Training Epoch: 92 [48640/50048]	Loss: 0.0801
Training Epoch: 92 [48768/50048]	Loss: 0.0525
Training Epoch: 92 [48896/50048]	Loss: 0.1160
Training Epoch: 92 [49024/50048]	Loss: 0.0814
Training Epoch: 92 [49152/50048]	Loss: 0.0677
Training Epoch: 92 [49280/50048]	Loss: 0.1457
Training Epoch: 92 [49408/50048]	Loss: 0.0628
Training Epoch: 92 [49536/50048]	Loss: 0.0710
Training Epoch: 92 [49664/50048]	Loss: 0.0790
Training Epoch: 92 [49792/50048]	Loss: 0.0531
Training Epoch: 92 [49920/50048]	Loss: 0.0768
Training Epoch: 92 [50048/50048]	Loss: 0.0955
2022-12-06 10:57:14.256 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:57:14,295 [ZeusDataLoader(eval)] eval epoch 93 done: time=3.69 energy=455.22
2022-12-06 05:57:14,295 [ZeusDataLoader(train)] Up to epoch 93: time=8389.99, energy=1018542.50, cost=1243395.22
2022-12-06 05:57:14,295 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:57:14,295 [ZeusDataLoader(train)] Expected next epoch: time=8479.79, energy=1029340.51, cost=1256651.60
2022-12-06 05:57:14,296 [ZeusDataLoader(train)] Epoch 94 begin.
Validation Epoch: 92, Average loss: 0.0184, Accuracy: 0.6426
2022-12-06 05:57:14,436 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:57:14,437 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:57:14.439 [ZeusMonitor] Monitor started.
2022-12-06 10:57:14.439 [ZeusMonitor] Running indefinitely. 2022-12-06 10:57:14.439 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:57:14.439 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e94+gpu0.power.log
Training Epoch: 93 [128/50048]	Loss: 0.0667
Training Epoch: 93 [256/50048]	Loss: 0.1712
Training Epoch: 93 [384/50048]	Loss: 0.0493
Training Epoch: 93 [512/50048]	Loss: 0.1689
Training Epoch: 93 [640/50048]	Loss: 0.0813
Training Epoch: 93 [768/50048]	Loss: 0.0479
Training Epoch: 93 [896/50048]	Loss: 0.0706
Training Epoch: 93 [1024/50048]	Loss: 0.0603
Training Epoch: 93 [1152/50048]	Loss: 0.1672
Training Epoch: 93 [1280/50048]	Loss: 0.0549
Training Epoch: 93 [1408/50048]	Loss: 0.1146
Training Epoch: 93 [1536/50048]	Loss: 0.0161
Training Epoch: 93 [1664/50048]	Loss: 0.0835
Training Epoch: 93 [1792/50048]	Loss: 0.0748
Training Epoch: 93 [1920/50048]	Loss: 0.1327
Training Epoch: 93 [2048/50048]	Loss: 0.0774
Training Epoch: 93 [2176/50048]	Loss: 0.0310
Training Epoch: 93 [2304/50048]	Loss: 0.0811
Training Epoch: 93 [2432/50048]	Loss: 0.0535
Training Epoch: 93 [2560/50048]	Loss: 0.0852
Training Epoch: 93 [2688/50048]	Loss: 0.0593
Training Epoch: 93 [2816/50048]	Loss: 0.0532
Training Epoch: 93 [2944/50048]	Loss: 0.0738
Training Epoch: 93 [3072/50048]	Loss: 0.0709
Training Epoch: 93 [3200/50048]	Loss: 0.0932
Training Epoch: 93 [3328/50048]	Loss: 0.0298
Training Epoch: 93 [3456/50048]	Loss: 0.0686
Training Epoch: 93 [3584/50048]	Loss: 0.0849
Training Epoch: 93 [3712/50048]	Loss: 0.0627
Training Epoch: 93 [3840/50048]	Loss: 0.0643
Training Epoch: 93 [3968/50048]	Loss: 0.0876
Training Epoch: 93 [4096/50048]	Loss: 0.0785
Training Epoch: 93 [4224/50048]	Loss: 0.0434
Training Epoch: 93 [4352/50048]	Loss: 0.1345
Training Epoch: 93 [4480/50048]	Loss: 0.0509
Training Epoch: 93 [4608/50048]	Loss: 0.1467
Training Epoch: 93 [4736/50048]	Loss: 0.0723
Training Epoch: 93 [4864/50048]	Loss: 0.0808
Training Epoch: 93 [4992/50048]	Loss: 0.0399
Training Epoch: 93 [5120/50048]	Loss: 0.0304
Training Epoch: 93 [5248/50048]	Loss: 0.1620
Training Epoch: 93 [5376/50048]	Loss: 0.0679
Training Epoch: 93 [5504/50048]	Loss: 0.0842
Training Epoch: 93 [5632/50048]	Loss: 0.0770
Training Epoch: 93 [5760/50048]	Loss: 0.1114
Training Epoch: 93 [5888/50048]	Loss: 0.0511
Training Epoch: 93 [6016/50048]	Loss: 0.0846
Training Epoch: 93 [6144/50048]	Loss: 0.0257
Training Epoch: 93 [6272/50048]	Loss: 0.0809
Training Epoch: 93 [6400/50048]	Loss: 0.0864
Training Epoch: 93 [6528/50048]	Loss: 0.0423
Training Epoch: 93 [6656/50048]	Loss: 0.0967
Training Epoch: 93 [6784/50048]	Loss: 0.0654
Training Epoch: 93 [6912/50048]	Loss: 0.0263
Training Epoch: 93 [7040/50048]	Loss: 0.0232
Training Epoch: 93 [7168/50048]	Loss: 0.0886
Training Epoch: 93 [7296/50048]	Loss: 0.0626
Training Epoch: 93 [7424/50048]	Loss: 0.0972
Training Epoch: 93 [7552/50048]	Loss: 0.0190
Training Epoch: 93 [7680/50048]	Loss: 0.0238
Training Epoch: 93 [7808/50048]	Loss: 0.0678
Training Epoch: 93 [7936/50048]	Loss: 0.0546
Training Epoch: 93 [8064/50048]	Loss: 0.0736
Training Epoch: 93 [8192/50048]	Loss: 0.0898
Training Epoch: 93 [8320/50048]	Loss: 0.0759
Training Epoch: 93 [8448/50048]	Loss: 0.0236
Training Epoch: 93 [8576/50048]	Loss: 0.1568
Training Epoch: 93 [8704/50048]	Loss: 0.0747
Training Epoch: 93 [8832/50048]	Loss: 0.1124
Training Epoch: 93 [8960/50048]	Loss: 0.0586
Training Epoch: 93 [9088/50048]	Loss: 0.1211
Training Epoch: 93 [9216/50048]	Loss: 0.1099
Training Epoch: 93 [9344/50048]	Loss: 0.0491
Training Epoch: 93 [9472/50048]	Loss: 0.0553
Training Epoch: 93 [9600/50048]	Loss: 0.0937
Training Epoch: 93 [9728/50048]	Loss: 0.1217
Training Epoch: 93 [9856/50048]	Loss: 0.0484
Training Epoch: 93 [9984/50048]	Loss: 0.0624
Training Epoch: 93 [10112/50048]	Loss: 0.0937
Training Epoch: 93 [10240/50048]	Loss: 0.0891
Training Epoch: 93 [10368/50048]	Loss: 0.0949
Training Epoch: 93 [10496/50048]	Loss: 0.1011
Training Epoch: 93 [10624/50048]	Loss: 0.0562
Training Epoch: 93 [10752/50048]	Loss: 0.1715
Training Epoch: 93 [10880/50048]	Loss: 0.0444
Training Epoch: 93 [11008/50048]	Loss: 0.0598
Training Epoch: 93 [11136/50048]	Loss: 0.0895
Training Epoch: 93 [11264/50048]	Loss: 0.0392
Training Epoch: 93 [11392/50048]	Loss: 0.1097
Training Epoch: 93 [11520/50048]	Loss: 0.0636
Training Epoch: 93 [11648/50048]	Loss: 0.0738
Training Epoch: 93 [11776/50048]	Loss: 0.1485
Training Epoch: 93 [11904/50048]	Loss: 0.0796
Training Epoch: 93 [12032/50048]	Loss: 0.0619
Training Epoch: 93 [12160/50048]	Loss: 0.0983
Training Epoch: 93 [12288/50048]	Loss: 0.0597
Training Epoch: 93 [12416/50048]	Loss: 0.0563
Training Epoch: 93 [12544/50048]	Loss: 0.1223
Training Epoch: 93 [12672/50048]	Loss: 0.0300
Training Epoch: 93 [12800/50048]	Loss: 0.0859
Training Epoch: 93 [12928/50048]	Loss: 0.0844
Training Epoch: 93 [13056/50048]	Loss: 0.0869
Training Epoch: 93 [13184/50048]	Loss: 0.0642
Training Epoch: 93 [13312/50048]	Loss: 0.0521
Training Epoch: 93 [13440/50048]	Loss: 0.0581
Training Epoch: 93 [13568/50048]	Loss: 0.0437
Training Epoch: 93 [13696/50048]	Loss: 0.0838
Training Epoch: 93 [13824/50048]	Loss: 0.0836
Training Epoch: 93 [13952/50048]	Loss: 0.0594
Training Epoch: 93 [14080/50048]	Loss: 0.0493
Training Epoch: 93 [14208/50048]	Loss: 0.1511
Training Epoch: 93 [14336/50048]	Loss: 0.0768
Training Epoch: 93 [14464/50048]	Loss: 0.0966
Training Epoch: 93 [14592/50048]	Loss: 0.0537
Training Epoch: 93 [14720/50048]	Loss: 0.1423
Training Epoch: 93 [14848/50048]	Loss: 0.0753
Training Epoch: 93 [14976/50048]	Loss: 0.0348
Training Epoch: 93 [15104/50048]	Loss: 0.0666
Training Epoch: 93 [15232/50048]	Loss: 0.0737
Training Epoch: 93 [15360/50048]	Loss: 0.0719
Training Epoch: 93 [15488/50048]	Loss: 0.0585
Training Epoch: 93 [15616/50048]	Loss: 0.0990
Training Epoch: 93 [15744/50048]	Loss: 0.1107
Training Epoch: 93 [15872/50048]	Loss: 0.0328
Training Epoch: 93 [16000/50048]	Loss: 0.0855
Training Epoch: 93 [16128/50048]	Loss: 0.0334
Training Epoch: 93 [16256/50048]	Loss: 0.1146
Training Epoch: 93 [16384/50048]	Loss: 0.0676
Training Epoch: 93 [16512/50048]	Loss: 0.0664
Training Epoch: 93 [16640/50048]	Loss: 0.0416
Training Epoch: 93 [16768/50048]	Loss: 0.1044
Training Epoch: 93 [16896/50048]	Loss: 0.0889
Training Epoch: 93 [17024/50048]	Loss: 0.0281
Training Epoch: 93 [17152/50048]	Loss: 0.0720
Training Epoch: 93 [17280/50048]	Loss: 0.1092
Training Epoch: 93 [17408/50048]	Loss: 0.0649
Training Epoch: 93 [17536/50048]	Loss: 0.0416
Training Epoch: 93 [17664/50048]	Loss: 0.0971
Training Epoch: 93 [17792/50048]	Loss: 0.0764
Training Epoch: 93 [17920/50048]	Loss: 0.0733
Training Epoch: 93 [18048/50048]	Loss: 0.0515
Training Epoch: 93 [18176/50048]	Loss: 0.0400
Training Epoch: 93 [18304/50048]	Loss: 0.0760
Training Epoch: 93 [18432/50048]	Loss: 0.0448
Training Epoch: 93 [18560/50048]	Loss: 0.1259
Training Epoch: 93 [18688/50048]	Loss: 0.0924
Training Epoch: 93 [18816/50048]	Loss: 0.0226
Training Epoch: 93 [18944/50048]	Loss: 0.1794
Training Epoch: 93 [19072/50048]	Loss: 0.0666
Training Epoch: 93 [19200/50048]	Loss: 0.0634
Training Epoch: 93 [19328/50048]	Loss: 0.1329
Training Epoch: 93 [19456/50048]	Loss: 0.0655
Training Epoch: 93 [19584/50048]	Loss: 0.1278
Training Epoch: 93 [19712/50048]	Loss: 0.1182
Training Epoch: 93 [19840/50048]	Loss: 0.0258
Training Epoch: 93 [19968/50048]	Loss: 0.0348
Training Epoch: 93 [20096/50048]	Loss: 0.0510
Training Epoch: 93 [20224/50048]	Loss: 0.0995
Training Epoch: 93 [20352/50048]	Loss: 0.0537
Training Epoch: 93 [20480/50048]	Loss: 0.0372
Training Epoch: 93 [20608/50048]	Loss: 0.0780
Training Epoch: 93 [20736/50048]	Loss: 0.0963
Training Epoch: 93 [20864/50048]	Loss: 0.0387
Training Epoch: 93 [20992/50048]	Loss: 0.0471
Training Epoch: 93 [21120/50048]	Loss: 0.0659
Training Epoch: 93 [21248/50048]	Loss: 0.0980
Training Epoch: 93 [21376/50048]	Loss: 0.0591
Training Epoch: 93 [21504/50048]	Loss: 0.0851
Training Epoch: 93 [21632/50048]	Loss: 0.0582
Training Epoch: 93 [21760/50048]	Loss: 0.0640
Training Epoch: 93 [21888/50048]	Loss: 0.0746
Training Epoch: 93 [22016/50048]	Loss: 0.0255
Training Epoch: 93 [22144/50048]	Loss: 0.1050
Training Epoch: 93 [22272/50048]	Loss: 0.1021
Training Epoch: 93 [22400/50048]	Loss: 0.1392
Training Epoch: 93 [22528/50048]	Loss: 0.0470
Training Epoch: 93 [22656/50048]	Loss: 0.0862
Training Epoch: 93 [22784/50048]	Loss: 0.1053
Training Epoch: 93 [22912/50048]	Loss: 0.0405
Training Epoch: 93 [23040/50048]	Loss: 0.0870
Training Epoch: 93 [23168/50048]	Loss: 0.0668
Training Epoch: 93 [23296/50048]	Loss: 0.0806
Training Epoch: 93 [23424/50048]	Loss: 0.0794
Training Epoch: 93 [23552/50048]	Loss: 0.0579
Training Epoch: 93 [23680/50048]	Loss: 0.0647
Training Epoch: 93 [23808/50048]	Loss: 0.0557
Training Epoch: 93 [23936/50048]	Loss: 0.0533
Training Epoch: 93 [24064/50048]	Loss: 0.1603
Training Epoch: 93 [24192/50048]	Loss: 0.1200
Training Epoch: 93 [24320/50048]	Loss: 0.1908
Training Epoch: 93 [24448/50048]	Loss: 0.1007
Training Epoch: 93 [24576/50048]	Loss: 0.0639
Training Epoch: 93 [24704/50048]	Loss: 0.1416
Training Epoch: 93 [24832/50048]	Loss: 0.0553
Training Epoch: 93 [24960/50048]	Loss: 0.0279
Training Epoch: 93 [25088/50048]	Loss: 0.0778
Training Epoch: 93 [25216/50048]	Loss: 0.0750
Training Epoch: 93 [25344/50048]	Loss: 0.1191
Training Epoch: 93 [25472/50048]	Loss: 0.0269
Training Epoch: 93 [25600/50048]	Loss: 0.0896
Training Epoch: 93 [25728/50048]	Loss: 0.1171
Training Epoch: 93 [25856/50048]	Loss: 0.0761
Training Epoch: 93 [25984/50048]	Loss: 0.0955
Training Epoch: 93 [26112/50048]	Loss: 0.0646
Training Epoch: 93 [26240/50048]	Loss: 0.0908
Training Epoch: 93 [26368/50048]	Loss: 0.1014
Training Epoch: 93 [26496/50048]	Loss: 0.0628
Training Epoch: 93 [26624/50048]	Loss: 0.1267
Training Epoch: 93 [26752/50048]	Loss: 0.0832
Training Epoch: 93 [26880/50048]	Loss: 0.0698
Training Epoch: 93 [27008/50048]	Loss: 0.0626
Training Epoch: 93 [27136/50048]	Loss: 0.0887
Training Epoch: 93 [27264/50048]	Loss: 0.0619
Training Epoch: 93 [27392/50048]	Loss: 0.0886
Training Epoch: 93 [27520/50048]	Loss: 0.0518
Training Epoch: 93 [27648/50048]	Loss: 0.1127
Training Epoch: 93 [27776/50048]	Loss: 0.0979
Training Epoch: 93 [27904/50048]	Loss: 0.0722
Training Epoch: 93 [28032/50048]	Loss: 0.0834
Training Epoch: 93 [28160/50048]	Loss: 0.0935
Training Epoch: 93 [28288/50048]	Loss: 0.0813
Training Epoch: 93 [28416/50048]	Loss: 0.0622
Training Epoch: 93 [28544/50048]	Loss: 0.0502
Training Epoch: 93 [28672/50048]	Loss: 0.0384
Training Epoch: 93 [28800/50048]	Loss: 0.0756
Training Epoch: 93 [28928/50048]	Loss: 0.0788
Training Epoch: 93 [29056/50048]	Loss: 0.1445
Training Epoch: 93 [29184/50048]	Loss: 0.0974
Training Epoch: 93 [29312/50048]	Loss: 0.0352
Training Epoch: 93 [29440/50048]	Loss: 0.0670
Training Epoch: 93 [29568/50048]	Loss: 0.0678
Training Epoch: 93 [29696/50048]	Loss: 0.0522
Training Epoch: 93 [29824/50048]	Loss: 0.1243
Training Epoch: 93 [29952/50048]	Loss: 0.0971
Training Epoch: 93 [30080/50048]	Loss: 0.1322
Training Epoch: 93 [30208/50048]	Loss: 0.0478
Training Epoch: 93 [30336/50048]	Loss: 0.0543
Training Epoch: 93 [30464/50048]	Loss: 0.1039
Training Epoch: 93 [30592/50048]	Loss: 0.1228
Training Epoch: 93 [30720/50048]	Loss: 0.1111
Training Epoch: 93 [30848/50048]	Loss: 0.1041
Training Epoch: 93 [30976/50048]	Loss: 0.0508
Training Epoch: 93 [31104/50048]	Loss: 0.0716
Training Epoch: 93 [31232/50048]	Loss: 0.0428
Training Epoch: 93 [31360/50048]	Loss: 0.0755
Training Epoch: 93 [31488/50048]	Loss: 0.0556
Training Epoch: 93 [31616/50048]	Loss: 0.0516
Training Epoch: 93 [31744/50048]	Loss: 0.0673
Training Epoch: 93 [31872/50048]	Loss: 0.0445
Training Epoch: 93 [32000/50048]	Loss: 0.0780
Training Epoch: 93 [32128/50048]	Loss: 0.0579
Training Epoch: 93 [32256/50048]	Loss: 0.1246
Training Epoch: 93 [32384/50048]	Loss: 0.0866
Training Epoch: 93 [32512/50048]	Loss: 0.1172
Training Epoch: 93 [32640/50048]	Loss: 0.0749
Training Epoch: 93 [32768/50048]	Loss: 0.0439
Training Epoch: 93 [32896/50048]	Loss: 0.0693
Training Epoch: 93 [33024/50048]	Loss: 0.1655
Training Epoch: 93 [33152/50048]	Loss: 0.0387
Training Epoch: 93 [33280/50048]	Loss: 0.0945
Training Epoch: 93 [33408/50048]	Loss: 0.0732
Training Epoch: 93 [33536/50048]	Loss: 0.0756
Training Epoch: 93 [33664/50048]	Loss: 0.1349
Training Epoch: 93 [33792/50048]	Loss: 0.0528
Training Epoch: 93 [33920/50048]	Loss: 0.0417
Training Epoch: 93 [34048/50048]	Loss: 0.0760
Training Epoch: 93 [34176/50048]	Loss: 0.1337
Training Epoch: 93 [34304/50048]	Loss: 0.1062
Training Epoch: 93 [34432/50048]	Loss: 0.1221
Training Epoch: 93 [34560/50048]	Loss: 0.0615
Training Epoch: 93 [34688/50048]	Loss: 0.0964
Training Epoch: 93 [34816/50048]	Loss: 0.1110
Training Epoch: 93 [34944/50048]	Loss: 0.0498
Training Epoch: 93 [35072/50048]	Loss: 0.1052
Training Epoch: 93 [35200/50048]	Loss: 0.1247
Training Epoch: 93 [35328/50048]	Loss: 0.1187
Training Epoch: 93 [35456/50048]	Loss: 0.0808
Training Epoch: 93 [35584/50048]	Loss: 0.1413
Training Epoch: 93 [35712/50048]	Loss: 0.1439
Training Epoch: 93 [35840/50048]	Loss: 0.0851
Training Epoch: 93 [35968/50048]	Loss: 0.0879
Training Epoch: 93 [36096/50048]	Loss: 0.0546
Training Epoch: 93 [36224/50048]	Loss: 0.0731
Training Epoch: 93 [36352/50048]	Loss: 0.1228
Training Epoch: 93 [36480/50048]	Loss: 0.0918
Training Epoch: 93 [36608/50048]	Loss: 0.0361
Training Epoch: 93 [36736/50048]	Loss: 0.1096
Training Epoch: 93 [36864/50048]	Loss: 0.0515
Training Epoch: 93 [36992/50048]	Loss: 0.0820
Training Epoch: 93 [37120/50048]	Loss: 0.1118
Training Epoch: 93 [37248/50048]	Loss: 0.0667
Training Epoch: 93 [37376/50048]	Loss: 0.0345
Training Epoch: 93 [37504/50048]	Loss: 0.0903
Training Epoch: 93 [37632/50048]	Loss: 0.0625
Training Epoch: 93 [37760/50048]	Loss: 0.0488
Training Epoch: 93 [37888/50048]	Loss: 0.0843
Training Epoch: 93 [38016/50048]	Loss: 0.0975
Training Epoch: 93 [38144/50048]	Loss: 0.1133
Training Epoch: 93 [38272/50048]	Loss: 0.0725
Training Epoch: 93 [38400/50048]	Loss: 0.0726
Training Epoch: 93 [38528/50048]	Loss: 0.1279
Training Epoch: 93 [38656/50048]	Loss: 0.0387
Training Epoch: 93 [38784/50048]	Loss: 0.0788
Training Epoch: 93 [38912/50048]	Loss: 0.0495
Training Epoch: 93 [39040/50048]	Loss: 0.1030
Training Epoch: 93 [39168/50048]	Loss: 0.1411
Training Epoch: 93 [39296/50048]	Loss: 0.1852
Training Epoch: 93 [39424/50048]	Loss: 0.1051
Training Epoch: 93 [39552/50048]	Loss: 0.0921
Training Epoch: 93 [39680/50048]	Loss: 0.0757
Training Epoch: 93 [39808/50048]	Loss: 0.0469
Training Epoch: 93 [39936/50048]	Loss: 0.0869
Training Epoch: 93 [40064/50048]	Loss: 0.0508
Training Epoch: 93 [40192/50048]	Loss: 0.0604
Training Epoch: 93 [40320/50048]	Loss: 0.0709
Training Epoch: 93 [40448/50048]	Loss: 0.0811
Training Epoch: 93 [40576/50048]	Loss: 0.1486
Training Epoch: 93 [40704/50048]	Loss: 0.0924
Training Epoch: 93 [40832/50048]	Loss: 0.1917
Training Epoch: 93 [40960/50048]	Loss: 0.0979
Training Epoch: 93 [41088/50048]	Loss: 0.0622
Training Epoch: 93 [41216/50048]	Loss: 0.1027
Training Epoch: 93 [41344/50048]	Loss: 0.0906
Training Epoch: 93 [41472/50048]	Loss: 0.0933
Training Epoch: 93 [41600/50048]	Loss: 0.0468
Training Epoch: 93 [41728/50048]	Loss: 0.0858
Training Epoch: 93 [41856/50048]	Loss: 0.0651
Training Epoch: 93 [41984/50048]	Loss: 0.1329
Training Epoch: 93 [42112/50048]	Loss: 0.0897
Training Epoch: 93 [42240/50048]	Loss: 0.0882
Training Epoch: 93 [42368/50048]	Loss: 0.1372
Training Epoch: 93 [42496/50048]	Loss: 0.0567
Training Epoch: 93 [42624/50048]	Loss: 0.0689
Training Epoch: 93 [42752/50048]	Loss: 0.0911
Training Epoch: 93 [42880/50048]	Loss: 0.1456
Training Epoch: 93 [43008/50048]	Loss: 0.1437
Training Epoch: 93 [43136/50048]	Loss: 0.0888
Training Epoch: 93 [43264/50048]	Loss: 0.1374
Training Epoch: 93 [43392/50048]	Loss: 0.0442
Training Epoch: 93 [43520/50048]	Loss: 0.0664
Training Epoch: 93 [43648/50048]	Loss: 0.0590
Training Epoch: 93 [43776/50048]	Loss: 0.0879
Training Epoch: 93 [43904/50048]	Loss: 0.0750
Training Epoch: 93 [44032/50048]	Loss: 0.0834
Training Epoch: 93 [44160/50048]	Loss: 0.1215
Training Epoch: 93 [44288/50048]	Loss: 0.0544
Training Epoch: 93 [44416/50048]	Loss: 0.0832
Training Epoch: 93 [44544/50048]	Loss: 0.1815
Training Epoch: 93 [44672/50048]	Loss: 0.1374
Training Epoch: 93 [44800/50048]	Loss: 0.0531
Training Epoch: 93 [44928/50048]	Loss: 0.0782
Training Epoch: 93 [45056/50048]	Loss: 0.0563
Training Epoch: 93 [45184/50048]	Loss: 0.0769
Training Epoch: 93 [45312/50048]	Loss: 0.0666
Training Epoch: 93 [45440/50048]	Loss: 0.0826
Training Epoch: 93 [45568/50048]	Loss: 0.0577
Training Epoch: 93 [45696/50048]	Loss: 0.1020
2022-12-06 05:58:40,697 [ZeusDataLoader(train)] train epoch 94 done: time=86.39 energy=10490.83
2022-12-06 05:58:40,699 [ZeusDataLoader(eval)] Epoch 94 begin.
Training Epoch: 93 [45824/50048]	Loss: 0.1029
Training Epoch: 93 [45952/50048]	Loss: 0.0638
Training Epoch: 93 [46080/50048]	Loss: 0.0825
Training Epoch: 93 [46208/50048]	Loss: 0.0471
Training Epoch: 93 [46336/50048]	Loss: 0.1460
Training Epoch: 93 [46464/50048]	Loss: 0.0410
Training Epoch: 93 [46592/50048]	Loss: 0.0893
Training Epoch: 93 [46720/50048]	Loss: 0.0916
Training Epoch: 93 [46848/50048]	Loss: 0.1716
Training Epoch: 93 [46976/50048]	Loss: 0.0862
Training Epoch: 93 [47104/50048]	Loss: 0.0658
Training Epoch: 93 [47232/50048]	Loss: 0.0723
Training Epoch: 93 [47360/50048]	Loss: 0.1144
Training Epoch: 93 [47488/50048]	Loss: 0.1272
Training Epoch: 93 [47616/50048]	Loss: 0.0790
Training Epoch: 93 [47744/50048]	Loss: 0.0502
Training Epoch: 93 [47872/50048]	Loss: 0.0459
Training Epoch: 93 [48000/50048]	Loss: 0.0605
Training Epoch: 93 [48128/50048]	Loss: 0.0360
Training Epoch: 93 [48256/50048]	Loss: 0.1414
Training Epoch: 93 [48384/50048]	Loss: 0.0679
Training Epoch: 93 [48512/50048]	Loss: 0.1018
Training Epoch: 93 [48640/50048]	Loss: 0.0769
Training Epoch: 93 [48768/50048]	Loss: 0.0362
Training Epoch: 93 [48896/50048]	Loss: 0.1021
Training Epoch: 93 [49024/50048]	Loss: 0.1064
Training Epoch: 93 [49152/50048]	Loss: 0.1243
Training Epoch: 93 [49280/50048]	Loss: 0.0446
Training Epoch: 93 [49408/50048]	Loss: 0.1004
Training Epoch: 93 [49536/50048]	Loss: 0.0528
Training Epoch: 93 [49664/50048]	Loss: 0.1087
Training Epoch: 93 [49792/50048]	Loss: 0.0738
Training Epoch: 93 [49920/50048]	Loss: 0.1037
Training Epoch: 93 [50048/50048]	Loss: 0.0731
2022-12-06 10:58:44.432 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 05:58:44,487 [ZeusDataLoader(eval)] eval epoch 94 done: time=3.78 energy=451.64
2022-12-06 05:58:44,488 [ZeusDataLoader(train)] Up to epoch 94: time=8480.16, energy=1029484.97, cost=1256756.39
2022-12-06 05:58:44,488 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 05:58:44,488 [ZeusDataLoader(train)] Expected next epoch: time=8569.96, energy=1040282.99, cost=1270012.77
2022-12-06 05:58:44,489 [ZeusDataLoader(train)] Epoch 95 begin.
Validation Epoch: 93, Average loss: 0.0185, Accuracy: 0.6396
2022-12-06 05:58:44,632 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 05:58:44,632 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 10:58:44.636 [ZeusMonitor] Monitor started.
2022-12-06 10:58:44.636 [ZeusMonitor] Running indefinitely. 2022-12-06 10:58:44.636 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 10:58:44.636 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e95+gpu0.power.log
Training Epoch: 94 [128/50048]	Loss: 0.0912
Training Epoch: 94 [256/50048]	Loss: 0.0666
Training Epoch: 94 [384/50048]	Loss: 0.0628
Training Epoch: 94 [512/50048]	Loss: 0.0688
Training Epoch: 94 [640/50048]	Loss: 0.0791
Training Epoch: 94 [768/50048]	Loss: 0.0400
Training Epoch: 94 [896/50048]	Loss: 0.0281
Training Epoch: 94 [1024/50048]	Loss: 0.0873
Training Epoch: 94 [1152/50048]	Loss: 0.0761
Training Epoch: 94 [1280/50048]	Loss: 0.0891
Training Epoch: 94 [1408/50048]	Loss: 0.1441
Training Epoch: 94 [1536/50048]	Loss: 0.0606
Training Epoch: 94 [1664/50048]	Loss: 0.1048
Training Epoch: 94 [1792/50048]	Loss: 0.0754
Training Epoch: 94 [1920/50048]	Loss: 0.0900
Training Epoch: 94 [2048/50048]	Loss: 0.0618
Training Epoch: 94 [2176/50048]	Loss: 0.0915
Training Epoch: 94 [2304/50048]	Loss: 0.0563
Training Epoch: 94 [2432/50048]	Loss: 0.0512
Training Epoch: 94 [2560/50048]	Loss: 0.0458
Training Epoch: 94 [2688/50048]	Loss: 0.1008
Training Epoch: 94 [2816/50048]	Loss: 0.0789
Training Epoch: 94 [2944/50048]	Loss: 0.1064
Training Epoch: 94 [3072/50048]	Loss: 0.0885
Training Epoch: 94 [3200/50048]	Loss: 0.0490
Training Epoch: 94 [3328/50048]	Loss: 0.0411
Training Epoch: 94 [3456/50048]	Loss: 0.0584
Training Epoch: 94 [3584/50048]	Loss: 0.0364
Training Epoch: 94 [3712/50048]	Loss: 0.1001
Training Epoch: 94 [3840/50048]	Loss: 0.1020
Training Epoch: 94 [3968/50048]	Loss: 0.0799
Training Epoch: 94 [4096/50048]	Loss: 0.0849
Training Epoch: 94 [4224/50048]	Loss: 0.0825
Training Epoch: 94 [4352/50048]	Loss: 0.0896
Training Epoch: 94 [4480/50048]	Loss: 0.0528
Training Epoch: 94 [4608/50048]	Loss: 0.1191
Training Epoch: 94 [4736/50048]	Loss: 0.1328
Training Epoch: 94 [4864/50048]	Loss: 0.0375
Training Epoch: 94 [4992/50048]	Loss: 0.1415
Training Epoch: 94 [5120/50048]	Loss: 0.0497
Training Epoch: 94 [5248/50048]	Loss: 0.0531
Training Epoch: 94 [5376/50048]	Loss: 0.1198
Training Epoch: 94 [5504/50048]	Loss: 0.0952
Training Epoch: 94 [5632/50048]	Loss: 0.1581
Training Epoch: 94 [5760/50048]	Loss: 0.0552
Training Epoch: 94 [5888/50048]	Loss: 0.0526
Training Epoch: 94 [6016/50048]	Loss: 0.0850
Training Epoch: 94 [6144/50048]	Loss: 0.0522
Training Epoch: 94 [6272/50048]	Loss: 0.0711
Training Epoch: 94 [6400/50048]	Loss: 0.0605
Training Epoch: 94 [6528/50048]	Loss: 0.0925
Training Epoch: 94 [6656/50048]	Loss: 0.0711
Training Epoch: 94 [6784/50048]	Loss: 0.0516
Training Epoch: 94 [6912/50048]	Loss: 0.1194
Training Epoch: 94 [7040/50048]	Loss: 0.1258
Training Epoch: 94 [7168/50048]	Loss: 0.0839
Training Epoch: 94 [7296/50048]	Loss: 0.0702
Training Epoch: 94 [7424/50048]	Loss: 0.0647
Training Epoch: 94 [7552/50048]	Loss: 0.0594
Training Epoch: 94 [7680/50048]	Loss: 0.0677
Training Epoch: 94 [7808/50048]	Loss: 0.1367
Training Epoch: 94 [7936/50048]	Loss: 0.0669
Training Epoch: 94 [8064/50048]	Loss: 0.1693
Training Epoch: 94 [8192/50048]	Loss: 0.0701
Training Epoch: 94 [8320/50048]	Loss: 0.0781
Training Epoch: 94 [8448/50048]	Loss: 0.0534
Training Epoch: 94 [8576/50048]	Loss: 0.0681
Training Epoch: 94 [8704/50048]	Loss: 0.1028
Training Epoch: 94 [8832/50048]	Loss: 0.0991
Training Epoch: 94 [8960/50048]	Loss: 0.0636
Training Epoch: 94 [9088/50048]	Loss: 0.0618
Training Epoch: 94 [9216/50048]	Loss: 0.0227
Training Epoch: 94 [9344/50048]	Loss: 0.0589
Training Epoch: 94 [9472/50048]	Loss: 0.1353
Training Epoch: 94 [9600/50048]	Loss: 0.0409
Training Epoch: 94 [9728/50048]	Loss: 0.0545
Training Epoch: 94 [9856/50048]	Loss: 0.1034
Training Epoch: 94 [9984/50048]	Loss: 0.1593
Training Epoch: 94 [10112/50048]	Loss: 0.0805
Training Epoch: 94 [10240/50048]	Loss: 0.0632
Training Epoch: 94 [10368/50048]	Loss: 0.0437
Training Epoch: 94 [10496/50048]	Loss: 0.0377
Training Epoch: 94 [10624/50048]	Loss: 0.0790
Training Epoch: 94 [10752/50048]	Loss: 0.1029
Training Epoch: 94 [10880/50048]	Loss: 0.1426
Training Epoch: 94 [11008/50048]	Loss: 0.0656
Training Epoch: 94 [11136/50048]	Loss: 0.1407
Training Epoch: 94 [11264/50048]	Loss: 0.0463
Training Epoch: 94 [11392/50048]	Loss: 0.1023
Training Epoch: 94 [11520/50048]	Loss: 0.1379
Training Epoch: 94 [11648/50048]	Loss: 0.0460
Training Epoch: 94 [11776/50048]	Loss: 0.0911
Training Epoch: 94 [11904/50048]	Loss: 0.0937
Training Epoch: 94 [12032/50048]	Loss: 0.0722
Training Epoch: 94 [12160/50048]	Loss: 0.1084
Training Epoch: 94 [12288/50048]	Loss: 0.1077
Training Epoch: 94 [12416/50048]	Loss: 0.1259
Training Epoch: 94 [12544/50048]	Loss: 0.0476
Training Epoch: 94 [12672/50048]	Loss: 0.0450
Training Epoch: 94 [12800/50048]	Loss: 0.0681
Training Epoch: 94 [12928/50048]	Loss: 0.0581
Training Epoch: 94 [13056/50048]	Loss: 0.1097
Training Epoch: 94 [13184/50048]	Loss: 0.0821
Training Epoch: 94 [13312/50048]	Loss: 0.0666
Training Epoch: 94 [13440/50048]	Loss: 0.0913
Training Epoch: 94 [13568/50048]	Loss: 0.1137
Training Epoch: 94 [13696/50048]	Loss: 0.1007
Training Epoch: 94 [13824/50048]	Loss: 0.1154
Training Epoch: 94 [13952/50048]	Loss: 0.0578
Training Epoch: 94 [14080/50048]	Loss: 0.1360
Training Epoch: 94 [14208/50048]	Loss: 0.1155
Training Epoch: 94 [14336/50048]	Loss: 0.0888
Training Epoch: 94 [14464/50048]	Loss: 0.1207
Training Epoch: 94 [14592/50048]	Loss: 0.1484
Training Epoch: 94 [14720/50048]	Loss: 0.0615
Training Epoch: 94 [14848/50048]	Loss: 0.0857
Training Epoch: 94 [14976/50048]	Loss: 0.1042
Training Epoch: 94 [15104/50048]	Loss: 0.0776
Training Epoch: 94 [15232/50048]	Loss: 0.1608
Training Epoch: 94 [15360/50048]	Loss: 0.0853
Training Epoch: 94 [15488/50048]	Loss: 0.0854
Training Epoch: 94 [15616/50048]	Loss: 0.0747
Training Epoch: 94 [15744/50048]	Loss: 0.0451
Training Epoch: 94 [15872/50048]	Loss: 0.1032
Training Epoch: 94 [16000/50048]	Loss: 0.1007
Training Epoch: 94 [16128/50048]	Loss: 0.0712
Training Epoch: 94 [16256/50048]	Loss: 0.0453
Training Epoch: 94 [16384/50048]	Loss: 0.1292
Training Epoch: 94 [16512/50048]	Loss: 0.0431
Training Epoch: 94 [16640/50048]	Loss: 0.0918
Training Epoch: 94 [16768/50048]	Loss: 0.0646
Training Epoch: 94 [16896/50048]	Loss: 0.1038
Training Epoch: 94 [17024/50048]	Loss: 0.0717
Training Epoch: 94 [17152/50048]	Loss: 0.0659
Training Epoch: 94 [17280/50048]	Loss: 0.0659
Training Epoch: 94 [17408/50048]	Loss: 0.0522
Training Epoch: 94 [17536/50048]	Loss: 0.0496
Training Epoch: 94 [17664/50048]	Loss: 0.0956
Training Epoch: 94 [17792/50048]	Loss: 0.1394
Training Epoch: 94 [17920/50048]	Loss: 0.0904
Training Epoch: 94 [18048/50048]	Loss: 0.1089
Training Epoch: 94 [18176/50048]	Loss: 0.0489
Training Epoch: 94 [18304/50048]	Loss: 0.1365
Training Epoch: 94 [18432/50048]	Loss: 0.0612
Training Epoch: 94 [18560/50048]	Loss: 0.0925
Training Epoch: 94 [18688/50048]	Loss: 0.1229
Training Epoch: 94 [18816/50048]	Loss: 0.0638
Training Epoch: 94 [18944/50048]	Loss: 0.1389
Training Epoch: 94 [19072/50048]	Loss: 0.0488
Training Epoch: 94 [19200/50048]	Loss: 0.1292
Training Epoch: 94 [19328/50048]	Loss: 0.0821
Training Epoch: 94 [19456/50048]	Loss: 0.0660
Training Epoch: 94 [19584/50048]	Loss: 0.1163
Training Epoch: 94 [19712/50048]	Loss: 0.0888
Training Epoch: 94 [19840/50048]	Loss: 0.0870
Training Epoch: 94 [19968/50048]	Loss: 0.1135
Training Epoch: 94 [20096/50048]	Loss: 0.0341
Training Epoch: 94 [20224/50048]	Loss: 0.1180
Training Epoch: 94 [20352/50048]	Loss: 0.0952
Training Epoch: 94 [20480/50048]	Loss: 0.1045
Training Epoch: 94 [20608/50048]	Loss: 0.1083
Training Epoch: 94 [20736/50048]	Loss: 0.0870
Training Epoch: 94 [20864/50048]	Loss: 0.0482
Training Epoch: 94 [20992/50048]	Loss: 0.0957
Training Epoch: 94 [21120/50048]	Loss: 0.0940
Training Epoch: 94 [21248/50048]	Loss: 0.0520
Training Epoch: 94 [21376/50048]	Loss: 0.0910
Training Epoch: 94 [21504/50048]	Loss: 0.0607
Training Epoch: 94 [21632/50048]	Loss: 0.0878
Training Epoch: 94 [21760/50048]	Loss: 0.0731
Training Epoch: 94 [21888/50048]	Loss: 0.0881
Training Epoch: 94 [22016/50048]	Loss: 0.0813
Training Epoch: 94 [22144/50048]	Loss: 0.1070
Training Epoch: 94 [22272/50048]	Loss: 0.0361
Training Epoch: 94 [22400/50048]	Loss: 0.0758
Training Epoch: 94 [22528/50048]	Loss: 0.0825
Training Epoch: 94 [22656/50048]	Loss: 0.0535
Training Epoch: 94 [22784/50048]	Loss: 0.0394
Training Epoch: 94 [22912/50048]	Loss: 0.0296
Training Epoch: 94 [23040/50048]	Loss: 0.0803
Training Epoch: 94 [23168/50048]	Loss: 0.1188
Training Epoch: 94 [23296/50048]	Loss: 0.0519
Training Epoch: 94 [23424/50048]	Loss: 0.0962
Training Epoch: 94 [23552/50048]	Loss: 0.0383
Training Epoch: 94 [23680/50048]	Loss: 0.0968
Training Epoch: 94 [23808/50048]	Loss: 0.0484
Training Epoch: 94 [23936/50048]	Loss: 0.0905
Training Epoch: 94 [24064/50048]	Loss: 0.1431
Training Epoch: 94 [24192/50048]	Loss: 0.0943
Training Epoch: 94 [24320/50048]	Loss: 0.1171
Training Epoch: 94 [24448/50048]	Loss: 0.0463
Training Epoch: 94 [24576/50048]	Loss: 0.0928
Training Epoch: 94 [24704/50048]	Loss: 0.0668
Training Epoch: 94 [24832/50048]	Loss: 0.1415
Training Epoch: 94 [24960/50048]	Loss: 0.0963
Training Epoch: 94 [25088/50048]	Loss: 0.1475
Training Epoch: 94 [25216/50048]	Loss: 0.0208
Training Epoch: 94 [25344/50048]	Loss: 0.1083
Training Epoch: 94 [25472/50048]	Loss: 0.0280
Training Epoch: 94 [25600/50048]	Loss: 0.0547
Training Epoch: 94 [25728/50048]	Loss: 0.0490
Training Epoch: 94 [25856/50048]	Loss: 0.0275
Training Epoch: 94 [25984/50048]	Loss: 0.1213
Training Epoch: 94 [26112/50048]	Loss: 0.0547
Training Epoch: 94 [26240/50048]	Loss: 0.1097
Training Epoch: 94 [26368/50048]	Loss: 0.0533
Training Epoch: 94 [26496/50048]	Loss: 0.1088
Training Epoch: 94 [26624/50048]	Loss: 0.1308
Training Epoch: 94 [26752/50048]	Loss: 0.1040
Training Epoch: 94 [26880/50048]	Loss: 0.0574
Training Epoch: 94 [27008/50048]	Loss: 0.0608
Training Epoch: 94 [27136/50048]	Loss: 0.1099
Training Epoch: 94 [27264/50048]	Loss: 0.0773
Training Epoch: 94 [27392/50048]	Loss: 0.0945
Training Epoch: 94 [27520/50048]	Loss: 0.0859
Training Epoch: 94 [27648/50048]	Loss: 0.0684
Training Epoch: 94 [27776/50048]	Loss: 0.1239
Training Epoch: 94 [27904/50048]	Loss: 0.0773
Training Epoch: 94 [28032/50048]	Loss: 0.0852
Training Epoch: 94 [28160/50048]	Loss: 0.0382
Training Epoch: 94 [28288/50048]	Loss: 0.0848
Training Epoch: 94 [28416/50048]	Loss: 0.2000
Training Epoch: 94 [28544/50048]	Loss: 0.1259
Training Epoch: 94 [28672/50048]	Loss: 0.0744
Training Epoch: 94 [28800/50048]	Loss: 0.0871
Training Epoch: 94 [28928/50048]	Loss: 0.1051
Training Epoch: 94 [29056/50048]	Loss: 0.1634
Training Epoch: 94 [29184/50048]	Loss: 0.0677
Training Epoch: 94 [29312/50048]	Loss: 0.0670
Training Epoch: 94 [29440/50048]	Loss: 0.1074
Training Epoch: 94 [29568/50048]	Loss: 0.0772
Training Epoch: 94 [29696/50048]	Loss: 0.1245
Training Epoch: 94 [29824/50048]	Loss: 0.1125
Training Epoch: 94 [29952/50048]	Loss: 0.1277
Training Epoch: 94 [30080/50048]	Loss: 0.0778
Training Epoch: 94 [30208/50048]	Loss: 0.0564
Training Epoch: 94 [30336/50048]	Loss: 0.0307
Training Epoch: 94 [30464/50048]	Loss: 0.1481
Training Epoch: 94 [30592/50048]	Loss: 0.1452
Training Epoch: 94 [30720/50048]	Loss: 0.1102
Training Epoch: 94 [30848/50048]	Loss: 0.0765
Training Epoch: 94 [30976/50048]	Loss: 0.0949
Training Epoch: 94 [31104/50048]	Loss: 0.1262
Training Epoch: 94 [31232/50048]	Loss: 0.1087
Training Epoch: 94 [31360/50048]	Loss: 0.0867
Training Epoch: 94 [31488/50048]	Loss: 0.0600
Training Epoch: 94 [31616/50048]	Loss: 0.0766
Training Epoch: 94 [31744/50048]	Loss: 0.1308
Training Epoch: 94 [31872/50048]	Loss: 0.0921
Training Epoch: 94 [32000/50048]	Loss: 0.0392
Training Epoch: 94 [32128/50048]	Loss: 0.0880
Training Epoch: 94 [32256/50048]	Loss: 0.0856
Training Epoch: 94 [32384/50048]	Loss: 0.1017
Training Epoch: 94 [32512/50048]	Loss: 0.0924
Training Epoch: 94 [32640/50048]	Loss: 0.1137
Training Epoch: 94 [32768/50048]	Loss: 0.1069
Training Epoch: 94 [32896/50048]	Loss: 0.0964
Training Epoch: 94 [33024/50048]	Loss: 0.0846
Training Epoch: 94 [33152/50048]	Loss: 0.0948
Training Epoch: 94 [33280/50048]	Loss: 0.0558
Training Epoch: 94 [33408/50048]	Loss: 0.1577
Training Epoch: 94 [33536/50048]	Loss: 0.1241
Training Epoch: 94 [33664/50048]	Loss: 0.1580
Training Epoch: 94 [33792/50048]	Loss: 0.0906
Training Epoch: 94 [33920/50048]	Loss: 0.0917
Training Epoch: 94 [34048/50048]	Loss: 0.0840
Training Epoch: 94 [34176/50048]	Loss: 0.0581
Training Epoch: 94 [34304/50048]	Loss: 0.1172
Training Epoch: 94 [34432/50048]	Loss: 0.1734
Training Epoch: 94 [34560/50048]	Loss: 0.0510
Training Epoch: 94 [34688/50048]	Loss: 0.0657
Training Epoch: 94 [34816/50048]	Loss: 0.0952
Training Epoch: 94 [34944/50048]	Loss: 0.0794
Training Epoch: 94 [35072/50048]	Loss: 0.0349
Training Epoch: 94 [35200/50048]	Loss: 0.1371
Training Epoch: 94 [35328/50048]	Loss: 0.0687
Training Epoch: 94 [35456/50048]	Loss: 0.0738
Training Epoch: 94 [35584/50048]	Loss: 0.1812
Training Epoch: 94 [35712/50048]	Loss: 0.1800
Training Epoch: 94 [35840/50048]	Loss: 0.0961
Training Epoch: 94 [35968/50048]	Loss: 0.1885
Training Epoch: 94 [36096/50048]	Loss: 0.0713
Training Epoch: 94 [36224/50048]	Loss: 0.0554
Training Epoch: 94 [36352/50048]	Loss: 0.1404
Training Epoch: 94 [36480/50048]	Loss: 0.0902
Training Epoch: 94 [36608/50048]	Loss: 0.0893
Training Epoch: 94 [36736/50048]	Loss: 0.0354
Training Epoch: 94 [36864/50048]	Loss: 0.0983
Training Epoch: 94 [36992/50048]	Loss: 0.0498
Training Epoch: 94 [37120/50048]	Loss: 0.0704
Training Epoch: 94 [37248/50048]	Loss: 0.1321
Training Epoch: 94 [37376/50048]	Loss: 0.0975
Training Epoch: 94 [37504/50048]	Loss: 0.0846
Training Epoch: 94 [37632/50048]	Loss: 0.0995
Training Epoch: 94 [37760/50048]	Loss: 0.0648
Training Epoch: 94 [37888/50048]	Loss: 0.0707
Training Epoch: 94 [38016/50048]	Loss: 0.0278
Training Epoch: 94 [38144/50048]	Loss: 0.0651
Training Epoch: 94 [38272/50048]	Loss: 0.1935
Training Epoch: 94 [38400/50048]	Loss: 0.0979
Training Epoch: 94 [38528/50048]	Loss: 0.0577
Training Epoch: 94 [38656/50048]	Loss: 0.0986
Training Epoch: 94 [38784/50048]	Loss: 0.1089
Training Epoch: 94 [38912/50048]	Loss: 0.1601
Training Epoch: 94 [39040/50048]	Loss: 0.2430
Training Epoch: 94 [39168/50048]	Loss: 0.0689
Training Epoch: 94 [39296/50048]	Loss: 0.1163
Training Epoch: 94 [39424/50048]	Loss: 0.1323
Training Epoch: 94 [39552/50048]	Loss: 0.0981
Training Epoch: 94 [39680/50048]	Loss: 0.1321
Training Epoch: 94 [39808/50048]	Loss: 0.1265
Training Epoch: 94 [39936/50048]	Loss: 0.1081
Training Epoch: 94 [40064/50048]	Loss: 0.1203
Training Epoch: 94 [40192/50048]	Loss: 0.1406
Training Epoch: 94 [40320/50048]	Loss: 0.1419
Training Epoch: 94 [40448/50048]	Loss: 0.1119
Training Epoch: 94 [40576/50048]	Loss: 0.0840
Training Epoch: 94 [40704/50048]	Loss: 0.0585
Training Epoch: 94 [40832/50048]	Loss: 0.0924
Training Epoch: 94 [40960/50048]	Loss: 0.0477
Training Epoch: 94 [41088/50048]	Loss: 0.0974
Training Epoch: 94 [41216/50048]	Loss: 0.0943
Training Epoch: 94 [41344/50048]	Loss: 0.1418
Training Epoch: 94 [41472/50048]	Loss: 0.0726
Training Epoch: 94 [41600/50048]	Loss: 0.0499
Training Epoch: 94 [41728/50048]	Loss: 0.0400
Training Epoch: 94 [41856/50048]	Loss: 0.0906
Training Epoch: 94 [41984/50048]	Loss: 0.0958
Training Epoch: 94 [42112/50048]	Loss: 0.0505
Training Epoch: 94 [42240/50048]	Loss: 0.0694
Training Epoch: 94 [42368/50048]	Loss: 0.0847
Training Epoch: 94 [42496/50048]	Loss: 0.1077
Training Epoch: 94 [42624/50048]	Loss: 0.1220
Training Epoch: 94 [42752/50048]	Loss: 0.1093
Training Epoch: 94 [42880/50048]	Loss: 0.1099
Training Epoch: 94 [43008/50048]	Loss: 0.0855
Training Epoch: 94 [43136/50048]	Loss: 0.0461
Training Epoch: 94 [43264/50048]	Loss: 0.0852
Training Epoch: 94 [43392/50048]	Loss: 0.0851
Training Epoch: 94 [43520/50048]	Loss: 0.1430
Training Epoch: 94 [43648/50048]	Loss: 0.0770
Training Epoch: 94 [43776/50048]	Loss: 0.1298
Training Epoch: 94 [43904/50048]	Loss: 0.1246
Training Epoch: 94 [44032/50048]	Loss: 0.0706
Training Epoch: 94 [44160/50048]	Loss: 0.1060
Training Epoch: 94 [44288/50048]	Loss: 0.1012
Training Epoch: 94 [44416/50048]	Loss: 0.1515
Training Epoch: 94 [44544/50048]	Loss: 0.0779
Training Epoch: 94 [44672/50048]	Loss: 0.1187
Training Epoch: 94 [44800/50048]	Loss: 0.1552
Training Epoch: 94 [44928/50048]	Loss: 0.0670
Training Epoch: 94 [45056/50048]	Loss: 0.0633
Training Epoch: 94 [45184/50048]	Loss: 0.0556
Training Epoch: 94 [45312/50048]	Loss: 0.0854
Training Epoch: 94 [45440/50048]	Loss: 0.1182
Training Epoch: 94 [45568/50048]	Loss: 0.0698
Training Epoch: 94 [45696/50048]	Loss: 0.0877
2022-12-06 06:00:10,938 [ZeusDataLoader(train)] train epoch 95 done: time=86.44 energy=10506.04
2022-12-06 06:00:10,940 [ZeusDataLoader(eval)] Epoch 95 begin.
Training Epoch: 94 [45824/50048]	Loss: 0.1106
Training Epoch: 94 [45952/50048]	Loss: 0.0799
Training Epoch: 94 [46080/50048]	Loss: 0.1236
Training Epoch: 94 [46208/50048]	Loss: 0.1080
Training Epoch: 94 [46336/50048]	Loss: 0.0499
Training Epoch: 94 [46464/50048]	Loss: 0.0675
Training Epoch: 94 [46592/50048]	Loss: 0.0417
Training Epoch: 94 [46720/50048]	Loss: 0.0758
Training Epoch: 94 [46848/50048]	Loss: 0.0804
Training Epoch: 94 [46976/50048]	Loss: 0.0422
Training Epoch: 94 [47104/50048]	Loss: 0.1269
Training Epoch: 94 [47232/50048]	Loss: 0.0354
Training Epoch: 94 [47360/50048]	Loss: 0.0820
Training Epoch: 94 [47488/50048]	Loss: 0.0960
Training Epoch: 94 [47616/50048]	Loss: 0.0825
Training Epoch: 94 [47744/50048]	Loss: 0.0820
Training Epoch: 94 [47872/50048]	Loss: 0.0843
Training Epoch: 94 [48000/50048]	Loss: 0.1251
Training Epoch: 94 [48128/50048]	Loss: 0.0775
Training Epoch: 94 [48256/50048]	Loss: 0.0636
Training Epoch: 94 [48384/50048]	Loss: 0.1192
Training Epoch: 94 [48512/50048]	Loss: 0.0475
Training Epoch: 94 [48640/50048]	Loss: 0.0371
Training Epoch: 94 [48768/50048]	Loss: 0.0584
Training Epoch: 94 [48896/50048]	Loss: 0.0568
Training Epoch: 94 [49024/50048]	Loss: 0.0751
Training Epoch: 94 [49152/50048]	Loss: 0.1390
Training Epoch: 94 [49280/50048]	Loss: 0.1530
Training Epoch: 94 [49408/50048]	Loss: 0.0936
Training Epoch: 94 [49536/50048]	Loss: 0.0797
Training Epoch: 94 [49664/50048]	Loss: 0.1513
Training Epoch: 94 [49792/50048]	Loss: 0.0690
Training Epoch: 94 [49920/50048]	Loss: 0.1221
Training Epoch: 94 [50048/50048]	Loss: 0.0683
2022-12-06 11:00:14.608 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:00:14,624 [ZeusDataLoader(eval)] eval epoch 95 done: time=3.67 energy=446.28
2022-12-06 06:00:14,624 [ZeusDataLoader(train)] Up to epoch 95: time=8570.27, energy=1040437.29, cost=1270117.54
2022-12-06 06:00:14,624 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:00:14,624 [ZeusDataLoader(train)] Expected next epoch: time=8660.07, energy=1051235.31, cost=1283373.92
2022-12-06 06:00:14,625 [ZeusDataLoader(train)] Epoch 96 begin.
Validation Epoch: 94, Average loss: 0.0190, Accuracy: 0.6379
2022-12-06 06:00:14,805 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:00:14,806 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:00:14.807 [ZeusMonitor] Monitor started.
2022-12-06 11:00:14.807 [ZeusMonitor] Running indefinitely. 2022-12-06 11:00:14.807 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:00:14.807 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e96+gpu0.power.log
Training Epoch: 95 [128/50048]	Loss: 0.0862
Training Epoch: 95 [256/50048]	Loss: 0.0747
Training Epoch: 95 [384/50048]	Loss: 0.0499
Training Epoch: 95 [512/50048]	Loss: 0.1230
Training Epoch: 95 [640/50048]	Loss: 0.0419
Training Epoch: 95 [768/50048]	Loss: 0.0857
Training Epoch: 95 [896/50048]	Loss: 0.0438
Training Epoch: 95 [1024/50048]	Loss: 0.1511
Training Epoch: 95 [1152/50048]	Loss: 0.0520
Training Epoch: 95 [1280/50048]	Loss: 0.0577
Training Epoch: 95 [1408/50048]	Loss: 0.0659
Training Epoch: 95 [1536/50048]	Loss: 0.0697
Training Epoch: 95 [1664/50048]	Loss: 0.0275
Training Epoch: 95 [1792/50048]	Loss: 0.0524
Training Epoch: 95 [1920/50048]	Loss: 0.0432
Training Epoch: 95 [2048/50048]	Loss: 0.0491
Training Epoch: 95 [2176/50048]	Loss: 0.0690
Training Epoch: 95 [2304/50048]	Loss: 0.0390
Training Epoch: 95 [2432/50048]	Loss: 0.1115
Training Epoch: 95 [2560/50048]	Loss: 0.0620
Training Epoch: 95 [2688/50048]	Loss: 0.0390
Training Epoch: 95 [2816/50048]	Loss: 0.0875
Training Epoch: 95 [2944/50048]	Loss: 0.0579
Training Epoch: 95 [3072/50048]	Loss: 0.0854
Training Epoch: 95 [3200/50048]	Loss: 0.0553
Training Epoch: 95 [3328/50048]	Loss: 0.0343
Training Epoch: 95 [3456/50048]	Loss: 0.0807
Training Epoch: 95 [3584/50048]	Loss: 0.0921
Training Epoch: 95 [3712/50048]	Loss: 0.0806
Training Epoch: 95 [3840/50048]	Loss: 0.0541
Training Epoch: 95 [3968/50048]	Loss: 0.0341
Training Epoch: 95 [4096/50048]	Loss: 0.0769
Training Epoch: 95 [4224/50048]	Loss: 0.1148
Training Epoch: 95 [4352/50048]	Loss: 0.1434
Training Epoch: 95 [4480/50048]	Loss: 0.0521
Training Epoch: 95 [4608/50048]	Loss: 0.0680
Training Epoch: 95 [4736/50048]	Loss: 0.0815
Training Epoch: 95 [4864/50048]	Loss: 0.0642
Training Epoch: 95 [4992/50048]	Loss: 0.0359
Training Epoch: 95 [5120/50048]	Loss: 0.0862
Training Epoch: 95 [5248/50048]	Loss: 0.0617
Training Epoch: 95 [5376/50048]	Loss: 0.0703
Training Epoch: 95 [5504/50048]	Loss: 0.0794
Training Epoch: 95 [5632/50048]	Loss: 0.0536
Training Epoch: 95 [5760/50048]	Loss: 0.1511
Training Epoch: 95 [5888/50048]	Loss: 0.0727
Training Epoch: 95 [6016/50048]	Loss: 0.0582
Training Epoch: 95 [6144/50048]	Loss: 0.0785
Training Epoch: 95 [6272/50048]	Loss: 0.0335
Training Epoch: 95 [6400/50048]	Loss: 0.0181
Training Epoch: 95 [6528/50048]	Loss: 0.0523
Training Epoch: 95 [6656/50048]	Loss: 0.0764
Training Epoch: 95 [6784/50048]	Loss: 0.1193
Training Epoch: 95 [6912/50048]	Loss: 0.0533
Training Epoch: 95 [7040/50048]	Loss: 0.0530
Training Epoch: 95 [7168/50048]	Loss: 0.0533
Training Epoch: 95 [7296/50048]	Loss: 0.0361
Training Epoch: 95 [7424/50048]	Loss: 0.0887
Training Epoch: 95 [7552/50048]	Loss: 0.0632
Training Epoch: 95 [7680/50048]	Loss: 0.0977
Training Epoch: 95 [7808/50048]	Loss: 0.0697
Training Epoch: 95 [7936/50048]	Loss: 0.0723
Training Epoch: 95 [8064/50048]	Loss: 0.0240
Training Epoch: 95 [8192/50048]	Loss: 0.0623
Training Epoch: 95 [8320/50048]	Loss: 0.0989
Training Epoch: 95 [8448/50048]	Loss: 0.0342
Training Epoch: 95 [8576/50048]	Loss: 0.0871
Training Epoch: 95 [8704/50048]	Loss: 0.0261
Training Epoch: 95 [8832/50048]	Loss: 0.0689
Training Epoch: 95 [8960/50048]	Loss: 0.0882
Training Epoch: 95 [9088/50048]	Loss: 0.1107
Training Epoch: 95 [9216/50048]	Loss: 0.0433
Training Epoch: 95 [9344/50048]	Loss: 0.0510
Training Epoch: 95 [9472/50048]	Loss: 0.0537
Training Epoch: 95 [9600/50048]	Loss: 0.1071
Training Epoch: 95 [9728/50048]	Loss: 0.1025
Training Epoch: 95 [9856/50048]	Loss: 0.0864
Training Epoch: 95 [9984/50048]	Loss: 0.0706
Training Epoch: 95 [10112/50048]	Loss: 0.0903
Training Epoch: 95 [10240/50048]	Loss: 0.0793
Training Epoch: 95 [10368/50048]	Loss: 0.0216
Training Epoch: 95 [10496/50048]	Loss: 0.0777
Training Epoch: 95 [10624/50048]	Loss: 0.1810
Training Epoch: 95 [10752/50048]	Loss: 0.0512
Training Epoch: 95 [10880/50048]	Loss: 0.0657
Training Epoch: 95 [11008/50048]	Loss: 0.0788
Training Epoch: 95 [11136/50048]	Loss: 0.0597
Training Epoch: 95 [11264/50048]	Loss: 0.0610
Training Epoch: 95 [11392/50048]	Loss: 0.0947
Training Epoch: 95 [11520/50048]	Loss: 0.0544
Training Epoch: 95 [11648/50048]	Loss: 0.0759
Training Epoch: 95 [11776/50048]	Loss: 0.0410
Training Epoch: 95 [11904/50048]	Loss: 0.0231
Training Epoch: 95 [12032/50048]	Loss: 0.0614
Training Epoch: 95 [12160/50048]	Loss: 0.0265
Training Epoch: 95 [12288/50048]	Loss: 0.1267
Training Epoch: 95 [12416/50048]	Loss: 0.0789
Training Epoch: 95 [12544/50048]	Loss: 0.1045
Training Epoch: 95 [12672/50048]	Loss: 0.0649
Training Epoch: 95 [12800/50048]	Loss: 0.1063
Training Epoch: 95 [12928/50048]	Loss: 0.0954
Training Epoch: 95 [13056/50048]	Loss: 0.0706
Training Epoch: 95 [13184/50048]	Loss: 0.0773
Training Epoch: 95 [13312/50048]	Loss: 0.0635
Training Epoch: 95 [13440/50048]	Loss: 0.0418
Training Epoch: 95 [13568/50048]	Loss: 0.0266
Training Epoch: 95 [13696/50048]	Loss: 0.0366
Training Epoch: 95 [13824/50048]	Loss: 0.0528
Training Epoch: 95 [13952/50048]	Loss: 0.1373
Training Epoch: 95 [14080/50048]	Loss: 0.0701
Training Epoch: 95 [14208/50048]	Loss: 0.0674
Training Epoch: 95 [14336/50048]	Loss: 0.0616
Training Epoch: 95 [14464/50048]	Loss: 0.0567
Training Epoch: 95 [14592/50048]	Loss: 0.0389
Training Epoch: 95 [14720/50048]	Loss: 0.1288
Training Epoch: 95 [14848/50048]	Loss: 0.0423
Training Epoch: 95 [14976/50048]	Loss: 0.1045
Training Epoch: 95 [15104/50048]	Loss: 0.0858
Training Epoch: 95 [15232/50048]	Loss: 0.0796
Training Epoch: 95 [15360/50048]	Loss: 0.0703
Training Epoch: 95 [15488/50048]	Loss: 0.0716
Training Epoch: 95 [15616/50048]	Loss: 0.1101
Training Epoch: 95 [15744/50048]	Loss: 0.1305
Training Epoch: 95 [15872/50048]	Loss: 0.0666
Training Epoch: 95 [16000/50048]	Loss: 0.0789
Training Epoch: 95 [16128/50048]	Loss: 0.0828
Training Epoch: 95 [16256/50048]	Loss: 0.1515
Training Epoch: 95 [16384/50048]	Loss: 0.0292
Training Epoch: 95 [16512/50048]	Loss: 0.0676
Training Epoch: 95 [16640/50048]	Loss: 0.0571
Training Epoch: 95 [16768/50048]	Loss: 0.0430
Training Epoch: 95 [16896/50048]	Loss: 0.0555
Training Epoch: 95 [17024/50048]	Loss: 0.1035
Training Epoch: 95 [17152/50048]	Loss: 0.1059
Training Epoch: 95 [17280/50048]	Loss: 0.0509
Training Epoch: 95 [17408/50048]	Loss: 0.1028
Training Epoch: 95 [17536/50048]	Loss: 0.0239
Training Epoch: 95 [17664/50048]	Loss: 0.0560
Training Epoch: 95 [17792/50048]	Loss: 0.0553
Training Epoch: 95 [17920/50048]	Loss: 0.0708
Training Epoch: 95 [18048/50048]	Loss: 0.1004
Training Epoch: 95 [18176/50048]	Loss: 0.0928
Training Epoch: 95 [18304/50048]	Loss: 0.1142
Training Epoch: 95 [18432/50048]	Loss: 0.1009
Training Epoch: 95 [18560/50048]	Loss: 0.0627
Training Epoch: 95 [18688/50048]	Loss: 0.0958
Training Epoch: 95 [18816/50048]	Loss: 0.1037
Training Epoch: 95 [18944/50048]	Loss: 0.0725
Training Epoch: 95 [19072/50048]	Loss: 0.1244
Training Epoch: 95 [19200/50048]	Loss: 0.0388
Training Epoch: 95 [19328/50048]	Loss: 0.0660
Training Epoch: 95 [19456/50048]	Loss: 0.0452
Training Epoch: 95 [19584/50048]	Loss: 0.0605
Training Epoch: 95 [19712/50048]	Loss: 0.0708
Training Epoch: 95 [19840/50048]	Loss: 0.0306
Training Epoch: 95 [19968/50048]	Loss: 0.0326
Training Epoch: 95 [20096/50048]	Loss: 0.0651
Training Epoch: 95 [20224/50048]	Loss: 0.0625
Training Epoch: 95 [20352/50048]	Loss: 0.0331
Training Epoch: 95 [20480/50048]	Loss: 0.0230
Training Epoch: 95 [20608/50048]	Loss: 0.0933
Training Epoch: 95 [20736/50048]	Loss: 0.0703
Training Epoch: 95 [20864/50048]	Loss: 0.0365
Training Epoch: 95 [20992/50048]	Loss: 0.0654
Training Epoch: 95 [21120/50048]	Loss: 0.0981
Training Epoch: 95 [21248/50048]	Loss: 0.0705
Training Epoch: 95 [21376/50048]	Loss: 0.0452
Training Epoch: 95 [21504/50048]	Loss: 0.1599
Training Epoch: 95 [21632/50048]	Loss: 0.0762
Training Epoch: 95 [21760/50048]	Loss: 0.0598
Training Epoch: 95 [21888/50048]	Loss: 0.1041
Training Epoch: 95 [22016/50048]	Loss: 0.1092
Training Epoch: 95 [22144/50048]	Loss: 0.0459
Training Epoch: 95 [22272/50048]	Loss: 0.0905
Training Epoch: 95 [22400/50048]	Loss: 0.0818
Training Epoch: 95 [22528/50048]	Loss: 0.0758
Training Epoch: 95 [22656/50048]	Loss: 0.0754
Training Epoch: 95 [22784/50048]	Loss: 0.1110
Training Epoch: 95 [22912/50048]	Loss: 0.0853
Training Epoch: 95 [23040/50048]	Loss: 0.0480
Training Epoch: 95 [23168/50048]	Loss: 0.0946
Training Epoch: 95 [23296/50048]	Loss: 0.0972
Training Epoch: 95 [23424/50048]	Loss: 0.0482
Training Epoch: 95 [23552/50048]	Loss: 0.0993
Training Epoch: 95 [23680/50048]	Loss: 0.0585
Training Epoch: 95 [23808/50048]	Loss: 0.0590
Training Epoch: 95 [23936/50048]	Loss: 0.0636
Training Epoch: 95 [24064/50048]	Loss: 0.0781
Training Epoch: 95 [24192/50048]	Loss: 0.0519
Training Epoch: 95 [24320/50048]	Loss: 0.1241
Training Epoch: 95 [24448/50048]	Loss: 0.0630
Training Epoch: 95 [24576/50048]	Loss: 0.0772
Training Epoch: 95 [24704/50048]	Loss: 0.0576
Training Epoch: 95 [24832/50048]	Loss: 0.0330
Training Epoch: 95 [24960/50048]	Loss: 0.0728
Training Epoch: 95 [25088/50048]	Loss: 0.0566
Training Epoch: 95 [25216/50048]	Loss: 0.0287
Training Epoch: 95 [25344/50048]	Loss: 0.1026
Training Epoch: 95 [25472/50048]	Loss: 0.0781
Training Epoch: 95 [25600/50048]	Loss: 0.1435
Training Epoch: 95 [25728/50048]	Loss: 0.0390
Training Epoch: 95 [25856/50048]	Loss: 0.0551
Training Epoch: 95 [25984/50048]	Loss: 0.0575
Training Epoch: 95 [26112/50048]	Loss: 0.0865
Training Epoch: 95 [26240/50048]	Loss: 0.0306
Training Epoch: 95 [26368/50048]	Loss: 0.1364
Training Epoch: 95 [26496/50048]	Loss: 0.0919
Training Epoch: 95 [26624/50048]	Loss: 0.0367
Training Epoch: 95 [26752/50048]	Loss: 0.0772
Training Epoch: 95 [26880/50048]	Loss: 0.1311
Training Epoch: 95 [27008/50048]	Loss: 0.0334
Training Epoch: 95 [27136/50048]	Loss: 0.0697
Training Epoch: 95 [27264/50048]	Loss: 0.0391
Training Epoch: 95 [27392/50048]	Loss: 0.0726
Training Epoch: 95 [27520/50048]	Loss: 0.0946
Training Epoch: 95 [27648/50048]	Loss: 0.0995
Training Epoch: 95 [27776/50048]	Loss: 0.1050
Training Epoch: 95 [27904/50048]	Loss: 0.1170
Training Epoch: 95 [28032/50048]	Loss: 0.0595
Training Epoch: 95 [28160/50048]	Loss: 0.0874
Training Epoch: 95 [28288/50048]	Loss: 0.0995
Training Epoch: 95 [28416/50048]	Loss: 0.0741
Training Epoch: 95 [28544/50048]	Loss: 0.0986
Training Epoch: 95 [28672/50048]	Loss: 0.0426
Training Epoch: 95 [28800/50048]	Loss: 0.0759
Training Epoch: 95 [28928/50048]	Loss: 0.1098
Training Epoch: 95 [29056/50048]	Loss: 0.1297
Training Epoch: 95 [29184/50048]	Loss: 0.0946
Training Epoch: 95 [29312/50048]	Loss: 0.0777
Training Epoch: 95 [29440/50048]	Loss: 0.0686
Training Epoch: 95 [29568/50048]	Loss: 0.0627
Training Epoch: 95 [29696/50048]	Loss: 0.1083
Training Epoch: 95 [29824/50048]	Loss: 0.1333
Training Epoch: 95 [29952/50048]	Loss: 0.0497
Training Epoch: 95 [30080/50048]	Loss: 0.1018
Training Epoch: 95 [30208/50048]	Loss: 0.1367
Training Epoch: 95 [30336/50048]	Loss: 0.1171
Training Epoch: 95 [30464/50048]	Loss: 0.0863
Training Epoch: 95 [30592/50048]	Loss: 0.0949
Training Epoch: 95 [30720/50048]	Loss: 0.0389
Training Epoch: 95 [30848/50048]	Loss: 0.1153
Training Epoch: 95 [30976/50048]	Loss: 0.1185
Training Epoch: 95 [31104/50048]	Loss: 0.1123
Training Epoch: 95 [31232/50048]	Loss: 0.0680
Training Epoch: 95 [31360/50048]	Loss: 0.0579
Training Epoch: 95 [31488/50048]	Loss: 0.1079
Training Epoch: 95 [31616/50048]	Loss: 0.1391
Training Epoch: 95 [31744/50048]	Loss: 0.0763
Training Epoch: 95 [31872/50048]	Loss: 0.1341
Training Epoch: 95 [32000/50048]	Loss: 0.0686
Training Epoch: 95 [32128/50048]	Loss: 0.0760
Training Epoch: 95 [32256/50048]	Loss: 0.1092
Training Epoch: 95 [32384/50048]	Loss: 0.0484
Training Epoch: 95 [32512/50048]	Loss: 0.0508
Training Epoch: 95 [32640/50048]	Loss: 0.0700
Training Epoch: 95 [32768/50048]	Loss: 0.0489
Training Epoch: 95 [32896/50048]	Loss: 0.0546
Training Epoch: 95 [33024/50048]	Loss: 0.1610
Training Epoch: 95 [33152/50048]	Loss: 0.0451
Training Epoch: 95 [33280/50048]	Loss: 0.0981
Training Epoch: 95 [33408/50048]	Loss: 0.0760
Training Epoch: 95 [33536/50048]	Loss: 0.1566
Training Epoch: 95 [33664/50048]	Loss: 0.1015
Training Epoch: 95 [33792/50048]	Loss: 0.0345
Training Epoch: 95 [33920/50048]	Loss: 0.1169
Training Epoch: 95 [34048/50048]	Loss: 0.0664
Training Epoch: 95 [34176/50048]	Loss: 0.0950
Training Epoch: 95 [34304/50048]	Loss: 0.0434
Training Epoch: 95 [34432/50048]	Loss: 0.1122
Training Epoch: 95 [34560/50048]	Loss: 0.0873
Training Epoch: 95 [34688/50048]	Loss: 0.0690
Training Epoch: 95 [34816/50048]	Loss: 0.0762
Training Epoch: 95 [34944/50048]	Loss: 0.1887
Training Epoch: 95 [35072/50048]	Loss: 0.0458
Training Epoch: 95 [35200/50048]	Loss: 0.0753
Training Epoch: 95 [35328/50048]	Loss: 0.0406
Training Epoch: 95 [35456/50048]	Loss: 0.0761
Training Epoch: 95 [35584/50048]	Loss: 0.0770
Training Epoch: 95 [35712/50048]	Loss: 0.1109
Training Epoch: 95 [35840/50048]	Loss: 0.0602
Training Epoch: 95 [35968/50048]	Loss: 0.0885
Training Epoch: 95 [36096/50048]	Loss: 0.0490
Training Epoch: 95 [36224/50048]	Loss: 0.0871
Training Epoch: 95 [36352/50048]	Loss: 0.0865
Training Epoch: 95 [36480/50048]	Loss: 0.1146
Training Epoch: 95 [36608/50048]	Loss: 0.0873
Training Epoch: 95 [36736/50048]	Loss: 0.2246
Training Epoch: 95 [36864/50048]	Loss: 0.1069
Training Epoch: 95 [36992/50048]	Loss: 0.2444
Training Epoch: 95 [37120/50048]	Loss: 0.0278
Training Epoch: 95 [37248/50048]	Loss: 0.0404
Training Epoch: 95 [37376/50048]	Loss: 0.0956
Training Epoch: 95 [37504/50048]	Loss: 0.0920
Training Epoch: 95 [37632/50048]	Loss: 0.0852
Training Epoch: 95 [37760/50048]	Loss: 0.0942
Training Epoch: 95 [37888/50048]	Loss: 0.1229
Training Epoch: 95 [38016/50048]	Loss: 0.0760
Training Epoch: 95 [38144/50048]	Loss: 0.0492
Training Epoch: 95 [38272/50048]	Loss: 0.0708
Training Epoch: 95 [38400/50048]	Loss: 0.0954
Training Epoch: 95 [38528/50048]	Loss: 0.0984
Training Epoch: 95 [38656/50048]	Loss: 0.0732
Training Epoch: 95 [38784/50048]	Loss: 0.0824
Training Epoch: 95 [38912/50048]	Loss: 0.1089
Training Epoch: 95 [39040/50048]	Loss: 0.1026
Training Epoch: 95 [39168/50048]	Loss: 0.0515
Training Epoch: 95 [39296/50048]	Loss: 0.0526
Training Epoch: 95 [39424/50048]	Loss: 0.0846
Training Epoch: 95 [39552/50048]	Loss: 0.0788
Training Epoch: 95 [39680/50048]	Loss: 0.0814
Training Epoch: 95 [39808/50048]	Loss: 0.1110
Training Epoch: 95 [39936/50048]	Loss: 0.1394
Training Epoch: 95 [40064/50048]	Loss: 0.0439
Training Epoch: 95 [40192/50048]	Loss: 0.0611
Training Epoch: 95 [40320/50048]	Loss: 0.1196
Training Epoch: 95 [40448/50048]	Loss: 0.0612
Training Epoch: 95 [40576/50048]	Loss: 0.0547
Training Epoch: 95 [40704/50048]	Loss: 0.0469
Training Epoch: 95 [40832/50048]	Loss: 0.1395
Training Epoch: 95 [40960/50048]	Loss: 0.0788
Training Epoch: 95 [41088/50048]	Loss: 0.1448
Training Epoch: 95 [41216/50048]	Loss: 0.0982
Training Epoch: 95 [41344/50048]	Loss: 0.0959
Training Epoch: 95 [41472/50048]	Loss: 0.0539
Training Epoch: 95 [41600/50048]	Loss: 0.0517
Training Epoch: 95 [41728/50048]	Loss: 0.0381
Training Epoch: 95 [41856/50048]	Loss: 0.1584
Training Epoch: 95 [41984/50048]	Loss: 0.1517
Training Epoch: 95 [42112/50048]	Loss: 0.1010
Training Epoch: 95 [42240/50048]	Loss: 0.0283
Training Epoch: 95 [42368/50048]	Loss: 0.1502
Training Epoch: 95 [42496/50048]	Loss: 0.0878
Training Epoch: 95 [42624/50048]	Loss: 0.1629
Training Epoch: 95 [42752/50048]	Loss: 0.0952
Training Epoch: 95 [42880/50048]	Loss: 0.0901
Training Epoch: 95 [43008/50048]	Loss: 0.0581
Training Epoch: 95 [43136/50048]	Loss: 0.1473
Training Epoch: 95 [43264/50048]	Loss: 0.1553
Training Epoch: 95 [43392/50048]	Loss: 0.1102
Training Epoch: 95 [43520/50048]	Loss: 0.1059
Training Epoch: 95 [43648/50048]	Loss: 0.1026
Training Epoch: 95 [43776/50048]	Loss: 0.0888
Training Epoch: 95 [43904/50048]	Loss: 0.1403
Training Epoch: 95 [44032/50048]	Loss: 0.0510
Training Epoch: 95 [44160/50048]	Loss: 0.0794
Training Epoch: 95 [44288/50048]	Loss: 0.1314
Training Epoch: 95 [44416/50048]	Loss: 0.1214
Training Epoch: 95 [44544/50048]	Loss: 0.0861
Training Epoch: 95 [44672/50048]	Loss: 0.1358
Training Epoch: 95 [44800/50048]	Loss: 0.1255
Training Epoch: 95 [44928/50048]	Loss: 0.1619
Training Epoch: 95 [45056/50048]	Loss: 0.1021
Training Epoch: 95 [45184/50048]	Loss: 0.1444
Training Epoch: 95 [45312/50048]	Loss: 0.0428
Training Epoch: 95 [45440/50048]	Loss: 0.0526
Training Epoch: 95 [45568/50048]	Loss: 0.1729
Training Epoch: 95 [45696/50048]	Loss: 0.1476
2022-12-06 06:01:41,096 [ZeusDataLoader(train)] train epoch 96 done: time=86.46 energy=10487.09
2022-12-06 06:01:41,097 [ZeusDataLoader(eval)] Epoch 96 begin.
Training Epoch: 95 [45824/50048]	Loss: 0.0362
Training Epoch: 95 [45952/50048]	Loss: 0.0572
Training Epoch: 95 [46080/50048]	Loss: 0.0835
Training Epoch: 95 [46208/50048]	Loss: 0.0768
Training Epoch: 95 [46336/50048]	Loss: 0.0525
Training Epoch: 95 [46464/50048]	Loss: 0.0601
Training Epoch: 95 [46592/50048]	Loss: 0.0594
Training Epoch: 95 [46720/50048]	Loss: 0.0422
Training Epoch: 95 [46848/50048]	Loss: 0.0935
Training Epoch: 95 [46976/50048]	Loss: 0.0445
Training Epoch: 95 [47104/50048]	Loss: 0.0782
Training Epoch: 95 [47232/50048]	Loss: 0.0610
Training Epoch: 95 [47360/50048]	Loss: 0.1392
Training Epoch: 95 [47488/50048]	Loss: 0.0982
Training Epoch: 95 [47616/50048]	Loss: 0.1361
Training Epoch: 95 [47744/50048]	Loss: 0.1493
Training Epoch: 95 [47872/50048]	Loss: 0.1182
Training Epoch: 95 [48000/50048]	Loss: 0.1300
Training Epoch: 95 [48128/50048]	Loss: 0.1233
Training Epoch: 95 [48256/50048]	Loss: 0.0641
Training Epoch: 95 [48384/50048]	Loss: 0.1029
Training Epoch: 95 [48512/50048]	Loss: 0.0643
Training Epoch: 95 [48640/50048]	Loss: 0.0666
Training Epoch: 95 [48768/50048]	Loss: 0.0617
Training Epoch: 95 [48896/50048]	Loss: 0.0779
Training Epoch: 95 [49024/50048]	Loss: 0.0866
Training Epoch: 95 [49152/50048]	Loss: 0.0656
Training Epoch: 95 [49280/50048]	Loss: 0.0425
Training Epoch: 95 [49408/50048]	Loss: 0.1282
Training Epoch: 95 [49536/50048]	Loss: 0.1223
Training Epoch: 95 [49664/50048]	Loss: 0.1035
Training Epoch: 95 [49792/50048]	Loss: 0.1626
Training Epoch: 95 [49920/50048]	Loss: 0.0877
Training Epoch: 95 [50048/50048]	Loss: 0.1364
2022-12-06 11:01:44.788 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:01:44,821 [ZeusDataLoader(eval)] eval epoch 96 done: time=3.71 energy=452.25
2022-12-06 06:01:44,821 [ZeusDataLoader(train)] Up to epoch 96: time=8660.45, energy=1051376.63, cost=1283477.59
2022-12-06 06:01:44,821 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:01:44,822 [ZeusDataLoader(train)] Expected next epoch: time=8750.25, energy=1062174.64, cost=1296733.97
2022-12-06 06:01:44,822 [ZeusDataLoader(train)] Epoch 97 begin.
Validation Epoch: 95, Average loss: 0.0188, Accuracy: 0.6475
2022-12-06 06:01:45,000 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:01:45,000 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:01:45.002 [ZeusMonitor] Monitor started.
2022-12-06 11:01:45.002 [ZeusMonitor] Running indefinitely. 2022-12-06 11:01:45.002 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:01:45.002 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e97+gpu0.power.log
Training Epoch: 96 [128/50048]	Loss: 0.0508
Training Epoch: 96 [256/50048]	Loss: 0.0592
Training Epoch: 96 [384/50048]	Loss: 0.0950
Training Epoch: 96 [512/50048]	Loss: 0.0716
Training Epoch: 96 [640/50048]	Loss: 0.1143
Training Epoch: 96 [768/50048]	Loss: 0.0573
Training Epoch: 96 [896/50048]	Loss: 0.1018
Training Epoch: 96 [1024/50048]	Loss: 0.1297
Training Epoch: 96 [1152/50048]	Loss: 0.0846
Training Epoch: 96 [1280/50048]	Loss: 0.0662
Training Epoch: 96 [1408/50048]	Loss: 0.1517
Training Epoch: 96 [1536/50048]	Loss: 0.1147
Training Epoch: 96 [1664/50048]	Loss: 0.0253
Training Epoch: 96 [1792/50048]	Loss: 0.0525
Training Epoch: 96 [1920/50048]	Loss: 0.0848
Training Epoch: 96 [2048/50048]	Loss: 0.0871
Training Epoch: 96 [2176/50048]	Loss: 0.0376
Training Epoch: 96 [2304/50048]	Loss: 0.0758
Training Epoch: 96 [2432/50048]	Loss: 0.0702
Training Epoch: 96 [2560/50048]	Loss: 0.0444
Training Epoch: 96 [2688/50048]	Loss: 0.0467
Training Epoch: 96 [2816/50048]	Loss: 0.0407
Training Epoch: 96 [2944/50048]	Loss: 0.0480
Training Epoch: 96 [3072/50048]	Loss: 0.0901
Training Epoch: 96 [3200/50048]	Loss: 0.0834
Training Epoch: 96 [3328/50048]	Loss: 0.0992
Training Epoch: 96 [3456/50048]	Loss: 0.0992
Training Epoch: 96 [3584/50048]	Loss: 0.0300
Training Epoch: 96 [3712/50048]	Loss: 0.0815
Training Epoch: 96 [3840/50048]	Loss: 0.0692
Training Epoch: 96 [3968/50048]	Loss: 0.0679
Training Epoch: 96 [4096/50048]	Loss: 0.0365
Training Epoch: 96 [4224/50048]	Loss: 0.0840
Training Epoch: 96 [4352/50048]	Loss: 0.1027
Training Epoch: 96 [4480/50048]	Loss: 0.0687
Training Epoch: 96 [4608/50048]	Loss: 0.0842
Training Epoch: 96 [4736/50048]	Loss: 0.0930
Training Epoch: 96 [4864/50048]	Loss: 0.1465
Training Epoch: 96 [4992/50048]	Loss: 0.0596
Training Epoch: 96 [5120/50048]	Loss: 0.0682
Training Epoch: 96 [5248/50048]	Loss: 0.0669
Training Epoch: 96 [5376/50048]	Loss: 0.0663
Training Epoch: 96 [5504/50048]	Loss: 0.0427
Training Epoch: 96 [5632/50048]	Loss: 0.0569
Training Epoch: 96 [5760/50048]	Loss: 0.1200
Training Epoch: 96 [5888/50048]	Loss: 0.0740
Training Epoch: 96 [6016/50048]	Loss: 0.0418
Training Epoch: 96 [6144/50048]	Loss: 0.0943
Training Epoch: 96 [6272/50048]	Loss: 0.1566
Training Epoch: 96 [6400/50048]	Loss: 0.0992
Training Epoch: 96 [6528/50048]	Loss: 0.1044
Training Epoch: 96 [6656/50048]	Loss: 0.0739
Training Epoch: 96 [6784/50048]	Loss: 0.0484
Training Epoch: 96 [6912/50048]	Loss: 0.0675
Training Epoch: 96 [7040/50048]	Loss: 0.0375
Training Epoch: 96 [7168/50048]	Loss: 0.0386
Training Epoch: 96 [7296/50048]	Loss: 0.0705
Training Epoch: 96 [7424/50048]	Loss: 0.1711
Training Epoch: 96 [7552/50048]	Loss: 0.0562
Training Epoch: 96 [7680/50048]	Loss: 0.1225
Training Epoch: 96 [7808/50048]	Loss: 0.0471
Training Epoch: 96 [7936/50048]	Loss: 0.0714
Training Epoch: 96 [8064/50048]	Loss: 0.0772
Training Epoch: 96 [8192/50048]	Loss: 0.0514
Training Epoch: 96 [8320/50048]	Loss: 0.0282
Training Epoch: 96 [8448/50048]	Loss: 0.0436
Training Epoch: 96 [8576/50048]	Loss: 0.0590
Training Epoch: 96 [8704/50048]	Loss: 0.0474
Training Epoch: 96 [8832/50048]	Loss: 0.0449
Training Epoch: 96 [8960/50048]	Loss: 0.0968
Training Epoch: 96 [9088/50048]	Loss: 0.0517
Training Epoch: 96 [9216/50048]	Loss: 0.0638
Training Epoch: 96 [9344/50048]	Loss: 0.1139
Training Epoch: 96 [9472/50048]	Loss: 0.0501
Training Epoch: 96 [9600/50048]	Loss: 0.1284
Training Epoch: 96 [9728/50048]	Loss: 0.0911
Training Epoch: 96 [9856/50048]	Loss: 0.0666
Training Epoch: 96 [9984/50048]	Loss: 0.0448
Training Epoch: 96 [10112/50048]	Loss: 0.0583
Training Epoch: 96 [10240/50048]	Loss: 0.0841
Training Epoch: 96 [10368/50048]	Loss: 0.0293
Training Epoch: 96 [10496/50048]	Loss: 0.0573
Training Epoch: 96 [10624/50048]	Loss: 0.1505
Training Epoch: 96 [10752/50048]	Loss: 0.1337
Training Epoch: 96 [10880/50048]	Loss: 0.0450
Training Epoch: 96 [11008/50048]	Loss: 0.1361
Training Epoch: 96 [11136/50048]	Loss: 0.1476
Training Epoch: 96 [11264/50048]	Loss: 0.0405
Training Epoch: 96 [11392/50048]	Loss: 0.0296
Training Epoch: 96 [11520/50048]	Loss: 0.0994
Training Epoch: 96 [11648/50048]	Loss: 0.0601
Training Epoch: 96 [11776/50048]	Loss: 0.0914
Training Epoch: 96 [11904/50048]	Loss: 0.0689
Training Epoch: 96 [12032/50048]	Loss: 0.0878
Training Epoch: 96 [12160/50048]	Loss: 0.0502
Training Epoch: 96 [12288/50048]	Loss: 0.0570
Training Epoch: 96 [12416/50048]	Loss: 0.0276
Training Epoch: 96 [12544/50048]	Loss: 0.1004
Training Epoch: 96 [12672/50048]	Loss: 0.0959
Training Epoch: 96 [12800/50048]	Loss: 0.0873
Training Epoch: 96 [12928/50048]	Loss: 0.0922
Training Epoch: 96 [13056/50048]	Loss: 0.0347
Training Epoch: 96 [13184/50048]	Loss: 0.0403
Training Epoch: 96 [13312/50048]	Loss: 0.0912
Training Epoch: 96 [13440/50048]	Loss: 0.1275
Training Epoch: 96 [13568/50048]	Loss: 0.0528
Training Epoch: 96 [13696/50048]	Loss: 0.0813
Training Epoch: 96 [13824/50048]	Loss: 0.0869
Training Epoch: 96 [13952/50048]	Loss: 0.0903
Training Epoch: 96 [14080/50048]	Loss: 0.0906
Training Epoch: 96 [14208/50048]	Loss: 0.0675
Training Epoch: 96 [14336/50048]	Loss: 0.1011
Training Epoch: 96 [14464/50048]	Loss: 0.0743
Training Epoch: 96 [14592/50048]	Loss: 0.0654
Training Epoch: 96 [14720/50048]	Loss: 0.0354
Training Epoch: 96 [14848/50048]	Loss: 0.1014
Training Epoch: 96 [14976/50048]	Loss: 0.0550
Training Epoch: 96 [15104/50048]	Loss: 0.1791
Training Epoch: 96 [15232/50048]	Loss: 0.0617
Training Epoch: 96 [15360/50048]	Loss: 0.0829
Training Epoch: 96 [15488/50048]	Loss: 0.0702
Training Epoch: 96 [15616/50048]	Loss: 0.0421
Training Epoch: 96 [15744/50048]	Loss: 0.0553
Training Epoch: 96 [15872/50048]	Loss: 0.1126
Training Epoch: 96 [16000/50048]	Loss: 0.1084
Training Epoch: 96 [16128/50048]	Loss: 0.0677
Training Epoch: 96 [16256/50048]	Loss: 0.0536
Training Epoch: 96 [16384/50048]	Loss: 0.1327
Training Epoch: 96 [16512/50048]	Loss: 0.1387
Training Epoch: 96 [16640/50048]	Loss: 0.0280
Training Epoch: 96 [16768/50048]	Loss: 0.0935
Training Epoch: 96 [16896/50048]	Loss: 0.0705
Training Epoch: 96 [17024/50048]	Loss: 0.0662
Training Epoch: 96 [17152/50048]	Loss: 0.1051
Training Epoch: 96 [17280/50048]	Loss: 0.1077
Training Epoch: 96 [17408/50048]	Loss: 0.1402
Training Epoch: 96 [17536/50048]	Loss: 0.1083
Training Epoch: 96 [17664/50048]	Loss: 0.0375
Training Epoch: 96 [17792/50048]	Loss: 0.0624
Training Epoch: 96 [17920/50048]	Loss: 0.0535
Training Epoch: 96 [18048/50048]	Loss: 0.1014
Training Epoch: 96 [18176/50048]	Loss: 0.0728
Training Epoch: 96 [18304/50048]	Loss: 0.0889
Training Epoch: 96 [18432/50048]	Loss: 0.0924
Training Epoch: 96 [18560/50048]	Loss: 0.0436
Training Epoch: 96 [18688/50048]	Loss: 0.0838
Training Epoch: 96 [18816/50048]	Loss: 0.0731
Training Epoch: 96 [18944/50048]	Loss: 0.0628
Training Epoch: 96 [19072/50048]	Loss: 0.1245
Training Epoch: 96 [19200/50048]	Loss: 0.1257
Training Epoch: 96 [19328/50048]	Loss: 0.0629
Training Epoch: 96 [19456/50048]	Loss: 0.0793
Training Epoch: 96 [19584/50048]	Loss: 0.0671
Training Epoch: 96 [19712/50048]	Loss: 0.1356
Training Epoch: 96 [19840/50048]	Loss: 0.0270
Training Epoch: 96 [19968/50048]	Loss: 0.0829
Training Epoch: 96 [20096/50048]	Loss: 0.0699
Training Epoch: 96 [20224/50048]	Loss: 0.0470
Training Epoch: 96 [20352/50048]	Loss: 0.0738
Training Epoch: 96 [20480/50048]	Loss: 0.0837
Training Epoch: 96 [20608/50048]	Loss: 0.0618
Training Epoch: 96 [20736/50048]	Loss: 0.0840
Training Epoch: 96 [20864/50048]	Loss: 0.1059
Training Epoch: 96 [20992/50048]	Loss: 0.0480
Training Epoch: 96 [21120/50048]	Loss: 0.1407
Training Epoch: 96 [21248/50048]	Loss: 0.0850
Training Epoch: 96 [21376/50048]	Loss: 0.1173
Training Epoch: 96 [21504/50048]	Loss: 0.0674
Training Epoch: 96 [21632/50048]	Loss: 0.0647
Training Epoch: 96 [21760/50048]	Loss: 0.0553
Training Epoch: 96 [21888/50048]	Loss: 0.0629
Training Epoch: 96 [22016/50048]	Loss: 0.0637
Training Epoch: 96 [22144/50048]	Loss: 0.0886
Training Epoch: 96 [22272/50048]	Loss: 0.0437
Training Epoch: 96 [22400/50048]	Loss: 0.1386
Training Epoch: 96 [22528/50048]	Loss: 0.0441
Training Epoch: 96 [22656/50048]	Loss: 0.0644
Training Epoch: 96 [22784/50048]	Loss: 0.0561
Training Epoch: 96 [22912/50048]	Loss: 0.0593
Training Epoch: 96 [23040/50048]	Loss: 0.1349
Training Epoch: 96 [23168/50048]	Loss: 0.1061
Training Epoch: 96 [23296/50048]	Loss: 0.0715
Training Epoch: 96 [23424/50048]	Loss: 0.1085
Training Epoch: 96 [23552/50048]	Loss: 0.0338
Training Epoch: 96 [23680/50048]	Loss: 0.0889
Training Epoch: 96 [23808/50048]	Loss: 0.0391
Training Epoch: 96 [23936/50048]	Loss: 0.0895
Training Epoch: 96 [24064/50048]	Loss: 0.0350
Training Epoch: 96 [24192/50048]	Loss: 0.0590
Training Epoch: 96 [24320/50048]	Loss: 0.0898
Training Epoch: 96 [24448/50048]	Loss: 0.0801
Training Epoch: 96 [24576/50048]	Loss: 0.0310
Training Epoch: 96 [24704/50048]	Loss: 0.0817
Training Epoch: 96 [24832/50048]	Loss: 0.0658
Training Epoch: 96 [24960/50048]	Loss: 0.0374
Training Epoch: 96 [25088/50048]	Loss: 0.0796
Training Epoch: 96 [25216/50048]	Loss: 0.1128
Training Epoch: 96 [25344/50048]	Loss: 0.0389
Training Epoch: 96 [25472/50048]	Loss: 0.0444
Training Epoch: 96 [25600/50048]	Loss: 0.0436
Training Epoch: 96 [25728/50048]	Loss: 0.0296
Training Epoch: 96 [25856/50048]	Loss: 0.0640
Training Epoch: 96 [25984/50048]	Loss: 0.0508
Training Epoch: 96 [26112/50048]	Loss: 0.0694
Training Epoch: 96 [26240/50048]	Loss: 0.1098
Training Epoch: 96 [26368/50048]	Loss: 0.0389
Training Epoch: 96 [26496/50048]	Loss: 0.0408
Training Epoch: 96 [26624/50048]	Loss: 0.0796
Training Epoch: 96 [26752/50048]	Loss: 0.1013
Training Epoch: 96 [26880/50048]	Loss: 0.0796
Training Epoch: 96 [27008/50048]	Loss: 0.0621
Training Epoch: 96 [27136/50048]	Loss: 0.0579
Training Epoch: 96 [27264/50048]	Loss: 0.0397
Training Epoch: 96 [27392/50048]	Loss: 0.0858
Training Epoch: 96 [27520/50048]	Loss: 0.1253
Training Epoch: 96 [27648/50048]	Loss: 0.1173
Training Epoch: 96 [27776/50048]	Loss: 0.0947
Training Epoch: 96 [27904/50048]	Loss: 0.1156
Training Epoch: 96 [28032/50048]	Loss: 0.0645
Training Epoch: 96 [28160/50048]	Loss: 0.0593
Training Epoch: 96 [28288/50048]	Loss: 0.0820
Training Epoch: 96 [28416/50048]	Loss: 0.0883
Training Epoch: 96 [28544/50048]	Loss: 0.2190
Training Epoch: 96 [28672/50048]	Loss: 0.0572
Training Epoch: 96 [28800/50048]	Loss: 0.0645
Training Epoch: 96 [28928/50048]	Loss: 0.0578
Training Epoch: 96 [29056/50048]	Loss: 0.1088
Training Epoch: 96 [29184/50048]	Loss: 0.0998
Training Epoch: 96 [29312/50048]	Loss: 0.0698
Training Epoch: 96 [29440/50048]	Loss: 0.0946
Training Epoch: 96 [29568/50048]	Loss: 0.1027
Training Epoch: 96 [29696/50048]	Loss: 0.0628
Training Epoch: 96 [29824/50048]	Loss: 0.0863
Training Epoch: 96 [29952/50048]	Loss: 0.0344
Training Epoch: 96 [30080/50048]	Loss: 0.0546
Training Epoch: 96 [30208/50048]	Loss: 0.0573
Training Epoch: 96 [30336/50048]	Loss: 0.0732
Training Epoch: 96 [30464/50048]	Loss: 0.0590
Training Epoch: 96 [30592/50048]	Loss: 0.1141
Training Epoch: 96 [30720/50048]	Loss: 0.0504
Training Epoch: 96 [30848/50048]	Loss: 0.0972
Training Epoch: 96 [30976/50048]	Loss: 0.0567
Training Epoch: 96 [31104/50048]	Loss: 0.0461
Training Epoch: 96 [31232/50048]	Loss: 0.0651
Training Epoch: 96 [31360/50048]	Loss: 0.0401
Training Epoch: 96 [31488/50048]	Loss: 0.0351
Training Epoch: 96 [31616/50048]	Loss: 0.0757
Training Epoch: 96 [31744/50048]	Loss: 0.0596
Training Epoch: 96 [31872/50048]	Loss: 0.1183
Training Epoch: 96 [32000/50048]	Loss: 0.0412
Training Epoch: 96 [32128/50048]	Loss: 0.0675
Training Epoch: 96 [32256/50048]	Loss: 0.0918
Training Epoch: 96 [32384/50048]	Loss: 0.1248
Training Epoch: 96 [32512/50048]	Loss: 0.1132
Training Epoch: 96 [32640/50048]	Loss: 0.0410
Training Epoch: 96 [32768/50048]	Loss: 0.1136
Training Epoch: 96 [32896/50048]	Loss: 0.0970
Training Epoch: 96 [33024/50048]	Loss: 0.0628
Training Epoch: 96 [33152/50048]	Loss: 0.0626
Training Epoch: 96 [33280/50048]	Loss: 0.0561
Training Epoch: 96 [33408/50048]	Loss: 0.1654
Training Epoch: 96 [33536/50048]	Loss: 0.1004
Training Epoch: 96 [33664/50048]	Loss: 0.0677
Training Epoch: 96 [33792/50048]	Loss: 0.0426
Training Epoch: 96 [33920/50048]	Loss: 0.0559
Training Epoch: 96 [34048/50048]	Loss: 0.0890
Training Epoch: 96 [34176/50048]	Loss: 0.0813
Training Epoch: 96 [34304/50048]	Loss: 0.0349
Training Epoch: 96 [34432/50048]	Loss: 0.0732
Training Epoch: 96 [34560/50048]	Loss: 0.0871
Training Epoch: 96 [34688/50048]	Loss: 0.1124
Training Epoch: 96 [34816/50048]	Loss: 0.1011
Training Epoch: 96 [34944/50048]	Loss: 0.0294
Training Epoch: 96 [35072/50048]	Loss: 0.0722
Training Epoch: 96 [35200/50048]	Loss: 0.1463
Training Epoch: 96 [35328/50048]	Loss: 0.1484
Training Epoch: 96 [35456/50048]	Loss: 0.0375
Training Epoch: 96 [35584/50048]	Loss: 0.0779
Training Epoch: 96 [35712/50048]	Loss: 0.1652
Training Epoch: 96 [35840/50048]	Loss: 0.1028
Training Epoch: 96 [35968/50048]	Loss: 0.0624
Training Epoch: 96 [36096/50048]	Loss: 0.0360
Training Epoch: 96 [36224/50048]	Loss: 0.0930
Training Epoch: 96 [36352/50048]	Loss: 0.0827
Training Epoch: 96 [36480/50048]	Loss: 0.1147
Training Epoch: 96 [36608/50048]	Loss: 0.0389
Training Epoch: 96 [36736/50048]	Loss: 0.1097
Training Epoch: 96 [36864/50048]	Loss: 0.1023
Training Epoch: 96 [36992/50048]	Loss: 0.0934
Training Epoch: 96 [37120/50048]	Loss: 0.0476
Training Epoch: 96 [37248/50048]	Loss: 0.0675
Training Epoch: 96 [37376/50048]	Loss: 0.1613
Training Epoch: 96 [37504/50048]	Loss: 0.1009
Training Epoch: 96 [37632/50048]	Loss: 0.1392
Training Epoch: 96 [37760/50048]	Loss: 0.0870
Training Epoch: 96 [37888/50048]	Loss: 0.0352
Training Epoch: 96 [38016/50048]	Loss: 0.0630
Training Epoch: 96 [38144/50048]	Loss: 0.1283
Training Epoch: 96 [38272/50048]	Loss: 0.0757
Training Epoch: 96 [38400/50048]	Loss: 0.1412
Training Epoch: 96 [38528/50048]	Loss: 0.0430
Training Epoch: 96 [38656/50048]	Loss: 0.1596
Training Epoch: 96 [38784/50048]	Loss: 0.0882
Training Epoch: 96 [38912/50048]	Loss: 0.1553
Training Epoch: 96 [39040/50048]	Loss: 0.0645
Training Epoch: 96 [39168/50048]	Loss: 0.0497
Training Epoch: 96 [39296/50048]	Loss: 0.0266
Training Epoch: 96 [39424/50048]	Loss: 0.0279
Training Epoch: 96 [39552/50048]	Loss: 0.0658
Training Epoch: 96 [39680/50048]	Loss: 0.0574
Training Epoch: 96 [39808/50048]	Loss: 0.0630
Training Epoch: 96 [39936/50048]	Loss: 0.0810
Training Epoch: 96 [40064/50048]	Loss: 0.0441
Training Epoch: 96 [40192/50048]	Loss: 0.0394
Training Epoch: 96 [40320/50048]	Loss: 0.1003
Training Epoch: 96 [40448/50048]	Loss: 0.0933
Training Epoch: 96 [40576/50048]	Loss: 0.0618
Training Epoch: 96 [40704/50048]	Loss: 0.0424
Training Epoch: 96 [40832/50048]	Loss: 0.0609
Training Epoch: 96 [40960/50048]	Loss: 0.0996
Training Epoch: 96 [41088/50048]	Loss: 0.1071
Training Epoch: 96 [41216/50048]	Loss: 0.0892
Training Epoch: 96 [41344/50048]	Loss: 0.1008
Training Epoch: 96 [41472/50048]	Loss: 0.0974
Training Epoch: 96 [41600/50048]	Loss: 0.0880
Training Epoch: 96 [41728/50048]	Loss: 0.0854
Training Epoch: 96 [41856/50048]	Loss: 0.1883
Training Epoch: 96 [41984/50048]	Loss: 0.0491
Training Epoch: 96 [42112/50048]	Loss: 0.1103
Training Epoch: 96 [42240/50048]	Loss: 0.0587
Training Epoch: 96 [42368/50048]	Loss: 0.0538
Training Epoch: 96 [42496/50048]	Loss: 0.1177
Training Epoch: 96 [42624/50048]	Loss: 0.1153
Training Epoch: 96 [42752/50048]	Loss: 0.1094
Training Epoch: 96 [42880/50048]	Loss: 0.1504
Training Epoch: 96 [43008/50048]	Loss: 0.1365
Training Epoch: 96 [43136/50048]	Loss: 0.0575
Training Epoch: 96 [43264/50048]	Loss: 0.0888
Training Epoch: 96 [43392/50048]	Loss: 0.0648
Training Epoch: 96 [43520/50048]	Loss: 0.0545
Training Epoch: 96 [43648/50048]	Loss: 0.0550
Training Epoch: 96 [43776/50048]	Loss: 0.0968
Training Epoch: 96 [43904/50048]	Loss: 0.0607
Training Epoch: 96 [44032/50048]	Loss: 0.1159
Training Epoch: 96 [44160/50048]	Loss: 0.1299
Training Epoch: 96 [44288/50048]	Loss: 0.0727
Training Epoch: 96 [44416/50048]	Loss: 0.0937
Training Epoch: 96 [44544/50048]	Loss: 0.1026
Training Epoch: 96 [44672/50048]	Loss: 0.0931
Training Epoch: 96 [44800/50048]	Loss: 0.0713
Training Epoch: 96 [44928/50048]	Loss: 0.1074
Training Epoch: 96 [45056/50048]	Loss: 0.1027
Training Epoch: 96 [45184/50048]	Loss: 0.0395
Training Epoch: 96 [45312/50048]	Loss: 0.1067
Training Epoch: 96 [45440/50048]	Loss: 0.0559
Training Epoch: 96 [45568/50048]	Loss: 0.0705
Training Epoch: 96 [45696/50048]	Loss: 0.0788
2022-12-06 06:03:11,605 [ZeusDataLoader(train)] train epoch 97 done: time=86.77 energy=10510.20
2022-12-06 06:03:11,607 [ZeusDataLoader(eval)] Epoch 97 begin.
Training Epoch: 96 [45824/50048]	Loss: 0.0902
Training Epoch: 96 [45952/50048]	Loss: 0.1785
Training Epoch: 96 [46080/50048]	Loss: 0.1960
Training Epoch: 96 [46208/50048]	Loss: 0.0594
Training Epoch: 96 [46336/50048]	Loss: 0.0408
Training Epoch: 96 [46464/50048]	Loss: 0.0629
Training Epoch: 96 [46592/50048]	Loss: 0.0636
Training Epoch: 96 [46720/50048]	Loss: 0.0943
Training Epoch: 96 [46848/50048]	Loss: 0.0546
Training Epoch: 96 [46976/50048]	Loss: 0.0759
Training Epoch: 96 [47104/50048]	Loss: 0.0590
Training Epoch: 96 [47232/50048]	Loss: 0.0378
Training Epoch: 96 [47360/50048]	Loss: 0.0776
Training Epoch: 96 [47488/50048]	Loss: 0.1193
Training Epoch: 96 [47616/50048]	Loss: 0.0650
Training Epoch: 96 [47744/50048]	Loss: 0.0528
Training Epoch: 96 [47872/50048]	Loss: 0.0788
Training Epoch: 96 [48000/50048]	Loss: 0.0613
Training Epoch: 96 [48128/50048]	Loss: 0.1030
Training Epoch: 96 [48256/50048]	Loss: 0.1014
Training Epoch: 96 [48384/50048]	Loss: 0.0986
Training Epoch: 96 [48512/50048]	Loss: 0.1343
Training Epoch: 96 [48640/50048]	Loss: 0.0719
Training Epoch: 96 [48768/50048]	Loss: 0.1177
Training Epoch: 96 [48896/50048]	Loss: 0.0466
Training Epoch: 96 [49024/50048]	Loss: 0.0406
Training Epoch: 96 [49152/50048]	Loss: 0.0849
Training Epoch: 96 [49280/50048]	Loss: 0.0693
Training Epoch: 96 [49408/50048]	Loss: 0.0969
Training Epoch: 96 [49536/50048]	Loss: 0.0648
Training Epoch: 96 [49664/50048]	Loss: 0.0838
Training Epoch: 96 [49792/50048]	Loss: 0.0668
Training Epoch: 96 [49920/50048]	Loss: 0.0302
Training Epoch: 96 [50048/50048]	Loss: 0.1381
2022-12-06 11:03:15.314 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:03:15,351 [ZeusDataLoader(eval)] eval epoch 97 done: time=3.74 energy=460.88
2022-12-06 06:03:15,351 [ZeusDataLoader(train)] Up to epoch 97: time=8750.96, energy=1062347.71, cost=1296882.58
2022-12-06 06:03:15,352 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:03:15,352 [ZeusDataLoader(train)] Expected next epoch: time=8840.76, energy=1073145.73, cost=1310138.96
2022-12-06 06:03:15,352 [ZeusDataLoader(train)] Epoch 98 begin.
Validation Epoch: 96, Average loss: 0.0191, Accuracy: 0.6406
2022-12-06 06:03:15,496 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:03:15,497 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:03:15.498 [ZeusMonitor] Monitor started.
2022-12-06 11:03:15.498 [ZeusMonitor] Running indefinitely. 2022-12-06 11:03:15.498 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:03:15.498 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e98+gpu0.power.log
Training Epoch: 97 [128/50048]	Loss: 0.0287
Training Epoch: 97 [256/50048]	Loss: 0.0578
Training Epoch: 97 [384/50048]	Loss: 0.0416
Training Epoch: 97 [512/50048]	Loss: 0.0555
Training Epoch: 97 [640/50048]	Loss: 0.0588
Training Epoch: 97 [768/50048]	Loss: 0.0232
Training Epoch: 97 [896/50048]	Loss: 0.1032
Training Epoch: 97 [1024/50048]	Loss: 0.1388
Training Epoch: 97 [1152/50048]	Loss: 0.0611
Training Epoch: 97 [1280/50048]	Loss: 0.0848
Training Epoch: 97 [1408/50048]	Loss: 0.0929
Training Epoch: 97 [1536/50048]	Loss: 0.0401
Training Epoch: 97 [1664/50048]	Loss: 0.0898
Training Epoch: 97 [1792/50048]	Loss: 0.0641
Training Epoch: 97 [1920/50048]	Loss: 0.0921
Training Epoch: 97 [2048/50048]	Loss: 0.0683
Training Epoch: 97 [2176/50048]	Loss: 0.0601
Training Epoch: 97 [2304/50048]	Loss: 0.0649
Training Epoch: 97 [2432/50048]	Loss: 0.0562
Training Epoch: 97 [2560/50048]	Loss: 0.0884
Training Epoch: 97 [2688/50048]	Loss: 0.1334
Training Epoch: 97 [2816/50048]	Loss: 0.0391
Training Epoch: 97 [2944/50048]	Loss: 0.0849
Training Epoch: 97 [3072/50048]	Loss: 0.0481
Training Epoch: 97 [3200/50048]	Loss: 0.0918
Training Epoch: 97 [3328/50048]	Loss: 0.0743
Training Epoch: 97 [3456/50048]	Loss: 0.0441
Training Epoch: 97 [3584/50048]	Loss: 0.0615
Training Epoch: 97 [3712/50048]	Loss: 0.1234
Training Epoch: 97 [3840/50048]	Loss: 0.0950
Training Epoch: 97 [3968/50048]	Loss: 0.1086
Training Epoch: 97 [4096/50048]	Loss: 0.1124
Training Epoch: 97 [4224/50048]	Loss: 0.0968
Training Epoch: 97 [4352/50048]	Loss: 0.0825
Training Epoch: 97 [4480/50048]	Loss: 0.0272
Training Epoch: 97 [4608/50048]	Loss: 0.0686
Training Epoch: 97 [4736/50048]	Loss: 0.1727
Training Epoch: 97 [4864/50048]	Loss: 0.0273
Training Epoch: 97 [4992/50048]	Loss: 0.0985
Training Epoch: 97 [5120/50048]	Loss: 0.0398
Training Epoch: 97 [5248/50048]	Loss: 0.1083
Training Epoch: 97 [5376/50048]	Loss: 0.0772
Training Epoch: 97 [5504/50048]	Loss: 0.0558
Training Epoch: 97 [5632/50048]	Loss: 0.0907
Training Epoch: 97 [5760/50048]	Loss: 0.0867
Training Epoch: 97 [5888/50048]	Loss: 0.0789
Training Epoch: 97 [6016/50048]	Loss: 0.0394
Training Epoch: 97 [6144/50048]	Loss: 0.0359
Training Epoch: 97 [6272/50048]	Loss: 0.0639
Training Epoch: 97 [6400/50048]	Loss: 0.1200
Training Epoch: 97 [6528/50048]	Loss: 0.1190
Training Epoch: 97 [6656/50048]	Loss: 0.0935
Training Epoch: 97 [6784/50048]	Loss: 0.1212
Training Epoch: 97 [6912/50048]	Loss: 0.1536
Training Epoch: 97 [7040/50048]	Loss: 0.0490
Training Epoch: 97 [7168/50048]	Loss: 0.0474
Training Epoch: 97 [7296/50048]	Loss: 0.0783
Training Epoch: 97 [7424/50048]	Loss: 0.1048
Training Epoch: 97 [7552/50048]	Loss: 0.0470
Training Epoch: 97 [7680/50048]	Loss: 0.0567
Training Epoch: 97 [7808/50048]	Loss: 0.0781
Training Epoch: 97 [7936/50048]	Loss: 0.0227
Training Epoch: 97 [8064/50048]	Loss: 0.0777
Training Epoch: 97 [8192/50048]	Loss: 0.0555
Training Epoch: 97 [8320/50048]	Loss: 0.0782
Training Epoch: 97 [8448/50048]	Loss: 0.0682
Training Epoch: 97 [8576/50048]	Loss: 0.0970
Training Epoch: 97 [8704/50048]	Loss: 0.0730
Training Epoch: 97 [8832/50048]	Loss: 0.0406
Training Epoch: 97 [8960/50048]	Loss: 0.0815
Training Epoch: 97 [9088/50048]	Loss: 0.1329
Training Epoch: 97 [9216/50048]	Loss: 0.0457
Training Epoch: 97 [9344/50048]	Loss: 0.0813
Training Epoch: 97 [9472/50048]	Loss: 0.0296
Training Epoch: 97 [9600/50048]	Loss: 0.0696
Training Epoch: 97 [9728/50048]	Loss: 0.1419
Training Epoch: 97 [9856/50048]	Loss: 0.0450
Training Epoch: 97 [9984/50048]	Loss: 0.1103
Training Epoch: 97 [10112/50048]	Loss: 0.0420
Training Epoch: 97 [10240/50048]	Loss: 0.0531
Training Epoch: 97 [10368/50048]	Loss: 0.0449
Training Epoch: 97 [10496/50048]	Loss: 0.1171
Training Epoch: 97 [10624/50048]	Loss: 0.0621
Training Epoch: 97 [10752/50048]	Loss: 0.0895
Training Epoch: 97 [10880/50048]	Loss: 0.0804
Training Epoch: 97 [11008/50048]	Loss: 0.0731
Training Epoch: 97 [11136/50048]	Loss: 0.1005
Training Epoch: 97 [11264/50048]	Loss: 0.0629
Training Epoch: 97 [11392/50048]	Loss: 0.0285
Training Epoch: 97 [11520/50048]	Loss: 0.0944
Training Epoch: 97 [11648/50048]	Loss: 0.0615
Training Epoch: 97 [11776/50048]	Loss: 0.0843
Training Epoch: 97 [11904/50048]	Loss: 0.1058
Training Epoch: 97 [12032/50048]	Loss: 0.0838
Training Epoch: 97 [12160/50048]	Loss: 0.0806
Training Epoch: 97 [12288/50048]	Loss: 0.0662
Training Epoch: 97 [12416/50048]	Loss: 0.0474
Training Epoch: 97 [12544/50048]	Loss: 0.0691
Training Epoch: 97 [12672/50048]	Loss: 0.0818
Training Epoch: 97 [12800/50048]	Loss: 0.1156
Training Epoch: 97 [12928/50048]	Loss: 0.0719
Training Epoch: 97 [13056/50048]	Loss: 0.0894
Training Epoch: 97 [13184/50048]	Loss: 0.1146
Training Epoch: 97 [13312/50048]	Loss: 0.0466
Training Epoch: 97 [13440/50048]	Loss: 0.0830
Training Epoch: 97 [13568/50048]	Loss: 0.1030
Training Epoch: 97 [13696/50048]	Loss: 0.0746
Training Epoch: 97 [13824/50048]	Loss: 0.0989
Training Epoch: 97 [13952/50048]	Loss: 0.0339
Training Epoch: 97 [14080/50048]	Loss: 0.0737
Training Epoch: 97 [14208/50048]	Loss: 0.0702
Training Epoch: 97 [14336/50048]	Loss: 0.0441
Training Epoch: 97 [14464/50048]	Loss: 0.0595
Training Epoch: 97 [14592/50048]	Loss: 0.1911
Training Epoch: 97 [14720/50048]	Loss: 0.0809
Training Epoch: 97 [14848/50048]	Loss: 0.0910
Training Epoch: 97 [14976/50048]	Loss: 0.0406
Training Epoch: 97 [15104/50048]	Loss: 0.0329
Training Epoch: 97 [15232/50048]	Loss: 0.0856
Training Epoch: 97 [15360/50048]	Loss: 0.0876
Training Epoch: 97 [15488/50048]	Loss: 0.0504
Training Epoch: 97 [15616/50048]	Loss: 0.0833
Training Epoch: 97 [15744/50048]	Loss: 0.0913
Training Epoch: 97 [15872/50048]	Loss: 0.0842
Training Epoch: 97 [16000/50048]	Loss: 0.0815
Training Epoch: 97 [16128/50048]	Loss: 0.0803
Training Epoch: 97 [16256/50048]	Loss: 0.0907
Training Epoch: 97 [16384/50048]	Loss: 0.0456
Training Epoch: 97 [16512/50048]	Loss: 0.0706
Training Epoch: 97 [16640/50048]	Loss: 0.1056
Training Epoch: 97 [16768/50048]	Loss: 0.0485
Training Epoch: 97 [16896/50048]	Loss: 0.0885
Training Epoch: 97 [17024/50048]	Loss: 0.0789
Training Epoch: 97 [17152/50048]	Loss: 0.0421
Training Epoch: 97 [17280/50048]	Loss: 0.1105
Training Epoch: 97 [17408/50048]	Loss: 0.0699
Training Epoch: 97 [17536/50048]	Loss: 0.0464
Training Epoch: 97 [17664/50048]	Loss: 0.1030
Training Epoch: 97 [17792/50048]	Loss: 0.0725
Training Epoch: 97 [17920/50048]	Loss: 0.0874
Training Epoch: 97 [18048/50048]	Loss: 0.0421
Training Epoch: 97 [18176/50048]	Loss: 0.0435
Training Epoch: 97 [18304/50048]	Loss: 0.0345
Training Epoch: 97 [18432/50048]	Loss: 0.0500
Training Epoch: 97 [18560/50048]	Loss: 0.0443
Training Epoch: 97 [18688/50048]	Loss: 0.0813
Training Epoch: 97 [18816/50048]	Loss: 0.0519
Training Epoch: 97 [18944/50048]	Loss: 0.0851
Training Epoch: 97 [19072/50048]	Loss: 0.0836
Training Epoch: 97 [19200/50048]	Loss: 0.0638
Training Epoch: 97 [19328/50048]	Loss: 0.0638
Training Epoch: 97 [19456/50048]	Loss: 0.0679
Training Epoch: 97 [19584/50048]	Loss: 0.0728
Training Epoch: 97 [19712/50048]	Loss: 0.0228
Training Epoch: 97 [19840/50048]	Loss: 0.0549
Training Epoch: 97 [19968/50048]	Loss: 0.0739
Training Epoch: 97 [20096/50048]	Loss: 0.1709
Training Epoch: 97 [20224/50048]	Loss: 0.0362
Training Epoch: 97 [20352/50048]	Loss: 0.0902
Training Epoch: 97 [20480/50048]	Loss: 0.0527
Training Epoch: 97 [20608/50048]	Loss: 0.0785
Training Epoch: 97 [20736/50048]	Loss: 0.0516
Training Epoch: 97 [20864/50048]	Loss: 0.0484
Training Epoch: 97 [20992/50048]	Loss: 0.1151
Training Epoch: 97 [21120/50048]	Loss: 0.0612
Training Epoch: 97 [21248/50048]	Loss: 0.1088
Training Epoch: 97 [21376/50048]	Loss: 0.0797
Training Epoch: 97 [21504/50048]	Loss: 0.0556
Training Epoch: 97 [21632/50048]	Loss: 0.0692
Training Epoch: 97 [21760/50048]	Loss: 0.0694
Training Epoch: 97 [21888/50048]	Loss: 0.0541
Training Epoch: 97 [22016/50048]	Loss: 0.0854
Training Epoch: 97 [22144/50048]	Loss: 0.0629
Training Epoch: 97 [22272/50048]	Loss: 0.1060
Training Epoch: 97 [22400/50048]	Loss: 0.0373
Training Epoch: 97 [22528/50048]	Loss: 0.0468
Training Epoch: 97 [22656/50048]	Loss: 0.0711
Training Epoch: 97 [22784/50048]	Loss: 0.0408
Training Epoch: 97 [22912/50048]	Loss: 0.0690
Training Epoch: 97 [23040/50048]	Loss: 0.0422
Training Epoch: 97 [23168/50048]	Loss: 0.0494
Training Epoch: 97 [23296/50048]	Loss: 0.0995
Training Epoch: 97 [23424/50048]	Loss: 0.1019
Training Epoch: 97 [23552/50048]	Loss: 0.1040
Training Epoch: 97 [23680/50048]	Loss: 0.0995
Training Epoch: 97 [23808/50048]	Loss: 0.0908
Training Epoch: 97 [23936/50048]	Loss: 0.0622
Training Epoch: 97 [24064/50048]	Loss: 0.0882
Training Epoch: 97 [24192/50048]	Loss: 0.0493
Training Epoch: 97 [24320/50048]	Loss: 0.0916
Training Epoch: 97 [24448/50048]	Loss: 0.0738
Training Epoch: 97 [24576/50048]	Loss: 0.0353
Training Epoch: 97 [24704/50048]	Loss: 0.0503
Training Epoch: 97 [24832/50048]	Loss: 0.1089
Training Epoch: 97 [24960/50048]	Loss: 0.0638
Training Epoch: 97 [25088/50048]	Loss: 0.0295
Training Epoch: 97 [25216/50048]	Loss: 0.0882
Training Epoch: 97 [25344/50048]	Loss: 0.0819
Training Epoch: 97 [25472/50048]	Loss: 0.0311
Training Epoch: 97 [25600/50048]	Loss: 0.0734
Training Epoch: 97 [25728/50048]	Loss: 0.0635
Training Epoch: 97 [25856/50048]	Loss: 0.0759
Training Epoch: 97 [25984/50048]	Loss: 0.0146
Training Epoch: 97 [26112/50048]	Loss: 0.0655
Training Epoch: 97 [26240/50048]	Loss: 0.0654
Training Epoch: 97 [26368/50048]	Loss: 0.0579
Training Epoch: 97 [26496/50048]	Loss: 0.0514
Training Epoch: 97 [26624/50048]	Loss: 0.0748
Training Epoch: 97 [26752/50048]	Loss: 0.0904
Training Epoch: 97 [26880/50048]	Loss: 0.0422
Training Epoch: 97 [27008/50048]	Loss: 0.0931
Training Epoch: 97 [27136/50048]	Loss: 0.0435
Training Epoch: 97 [27264/50048]	Loss: 0.0472
Training Epoch: 97 [27392/50048]	Loss: 0.0763
Training Epoch: 97 [27520/50048]	Loss: 0.0537
Training Epoch: 97 [27648/50048]	Loss: 0.0475
Training Epoch: 97 [27776/50048]	Loss: 0.0762
Training Epoch: 97 [27904/50048]	Loss: 0.0625
Training Epoch: 97 [28032/50048]	Loss: 0.0516
Training Epoch: 97 [28160/50048]	Loss: 0.0455
Training Epoch: 97 [28288/50048]	Loss: 0.0513
Training Epoch: 97 [28416/50048]	Loss: 0.1254
Training Epoch: 97 [28544/50048]	Loss: 0.1094
Training Epoch: 97 [28672/50048]	Loss: 0.1709
Training Epoch: 97 [28800/50048]	Loss: 0.0524
Training Epoch: 97 [28928/50048]	Loss: 0.1321
Training Epoch: 97 [29056/50048]	Loss: 0.0264
Training Epoch: 97 [29184/50048]	Loss: 0.0886
Training Epoch: 97 [29312/50048]	Loss: 0.0722
Training Epoch: 97 [29440/50048]	Loss: 0.1166
Training Epoch: 97 [29568/50048]	Loss: 0.0673
Training Epoch: 97 [29696/50048]	Loss: 0.0658
Training Epoch: 97 [29824/50048]	Loss: 0.0707
Training Epoch: 97 [29952/50048]	Loss: 0.0490
Training Epoch: 97 [30080/50048]	Loss: 0.0813
Training Epoch: 97 [30208/50048]	Loss: 0.0492
Training Epoch: 97 [30336/50048]	Loss: 0.0656
Training Epoch: 97 [30464/50048]	Loss: 0.1301
Training Epoch: 97 [30592/50048]	Loss: 0.0438
Training Epoch: 97 [30720/50048]	Loss: 0.0758
Training Epoch: 97 [30848/50048]	Loss: 0.0863
Training Epoch: 97 [30976/50048]	Loss: 0.0899
Training Epoch: 97 [31104/50048]	Loss: 0.1072
Training Epoch: 97 [31232/50048]	Loss: 0.0961
Training Epoch: 97 [31360/50048]	Loss: 0.0901
Training Epoch: 97 [31488/50048]	Loss: 0.0673
Training Epoch: 97 [31616/50048]	Loss: 0.0664
Training Epoch: 97 [31744/50048]	Loss: 0.1039
Training Epoch: 97 [31872/50048]	Loss: 0.0612
Training Epoch: 97 [32000/50048]	Loss: 0.0606
Training Epoch: 97 [32128/50048]	Loss: 0.0582
Training Epoch: 97 [32256/50048]	Loss: 0.1785
Training Epoch: 97 [32384/50048]	Loss: 0.0810
Training Epoch: 97 [32512/50048]	Loss: 0.1437
Training Epoch: 97 [32640/50048]	Loss: 0.0626
Training Epoch: 97 [32768/50048]	Loss: 0.0841
Training Epoch: 97 [32896/50048]	Loss: 0.0842
Training Epoch: 97 [33024/50048]	Loss: 0.0815
Training Epoch: 97 [33152/50048]	Loss: 0.0709
Training Epoch: 97 [33280/50048]	Loss: 0.1318
Training Epoch: 97 [33408/50048]	Loss: 0.0973
Training Epoch: 97 [33536/50048]	Loss: 0.0589
Training Epoch: 97 [33664/50048]	Loss: 0.0787
Training Epoch: 97 [33792/50048]	Loss: 0.1146
Training Epoch: 97 [33920/50048]	Loss: 0.0648
Training Epoch: 97 [34048/50048]	Loss: 0.0558
Training Epoch: 97 [34176/50048]	Loss: 0.0435
Training Epoch: 97 [34304/50048]	Loss: 0.1110
Training Epoch: 97 [34432/50048]	Loss: 0.0413
Training Epoch: 97 [34560/50048]	Loss: 0.0311
Training Epoch: 97 [34688/50048]	Loss: 0.0864
Training Epoch: 97 [34816/50048]	Loss: 0.0834
Training Epoch: 97 [34944/50048]	Loss: 0.0820
Training Epoch: 97 [35072/50048]	Loss: 0.0622
Training Epoch: 97 [35200/50048]	Loss: 0.0492
Training Epoch: 97 [35328/50048]	Loss: 0.0482
Training Epoch: 97 [35456/50048]	Loss: 0.0960
Training Epoch: 97 [35584/50048]	Loss: 0.0807
Training Epoch: 97 [35712/50048]	Loss: 0.0867
Training Epoch: 97 [35840/50048]	Loss: 0.0327
Training Epoch: 97 [35968/50048]	Loss: 0.0620
Training Epoch: 97 [36096/50048]	Loss: 0.1404
Training Epoch: 97 [36224/50048]	Loss: 0.0407
Training Epoch: 97 [36352/50048]	Loss: 0.0986
Training Epoch: 97 [36480/50048]	Loss: 0.0510
Training Epoch: 97 [36608/50048]	Loss: 0.1007
Training Epoch: 97 [36736/50048]	Loss: 0.0658
Training Epoch: 97 [36864/50048]	Loss: 0.1138
Training Epoch: 97 [36992/50048]	Loss: 0.1093
Training Epoch: 97 [37120/50048]	Loss: 0.1375
Training Epoch: 97 [37248/50048]	Loss: 0.0704
Training Epoch: 97 [37376/50048]	Loss: 0.0583
Training Epoch: 97 [37504/50048]	Loss: 0.0825
Training Epoch: 97 [37632/50048]	Loss: 0.0569
Training Epoch: 97 [37760/50048]	Loss: 0.0834
Training Epoch: 97 [37888/50048]	Loss: 0.0702
Training Epoch: 97 [38016/50048]	Loss: 0.0630
Training Epoch: 97 [38144/50048]	Loss: 0.0452
Training Epoch: 97 [38272/50048]	Loss: 0.0472
Training Epoch: 97 [38400/50048]	Loss: 0.1149
Training Epoch: 97 [38528/50048]	Loss: 0.0284
Training Epoch: 97 [38656/50048]	Loss: 0.0478
Training Epoch: 97 [38784/50048]	Loss: 0.0449
Training Epoch: 97 [38912/50048]	Loss: 0.0900
Training Epoch: 97 [39040/50048]	Loss: 0.0162
Training Epoch: 97 [39168/50048]	Loss: 0.0851
Training Epoch: 97 [39296/50048]	Loss: 0.1376
Training Epoch: 97 [39424/50048]	Loss: 0.1137
Training Epoch: 97 [39552/50048]	Loss: 0.1105
Training Epoch: 97 [39680/50048]	Loss: 0.0486
Training Epoch: 97 [39808/50048]	Loss: 0.0908
Training Epoch: 97 [39936/50048]	Loss: 0.0685
Training Epoch: 97 [40064/50048]	Loss: 0.0667
Training Epoch: 97 [40192/50048]	Loss: 0.0495
Training Epoch: 97 [40320/50048]	Loss: 0.0647
Training Epoch: 97 [40448/50048]	Loss: 0.0825
Training Epoch: 97 [40576/50048]	Loss: 0.1384
Training Epoch: 97 [40704/50048]	Loss: 0.1027
Training Epoch: 97 [40832/50048]	Loss: 0.1427
Training Epoch: 97 [40960/50048]	Loss: 0.1600
Training Epoch: 97 [41088/50048]	Loss: 0.0700
Training Epoch: 97 [41216/50048]	Loss: 0.0450
Training Epoch: 97 [41344/50048]	Loss: 0.0346
Training Epoch: 97 [41472/50048]	Loss: 0.1023
Training Epoch: 97 [41600/50048]	Loss: 0.0758
Training Epoch: 97 [41728/50048]	Loss: 0.1574
Training Epoch: 97 [41856/50048]	Loss: 0.0592
Training Epoch: 97 [41984/50048]	Loss: 0.0953
Training Epoch: 97 [42112/50048]	Loss: 0.0791
Training Epoch: 97 [42240/50048]	Loss: 0.0445
Training Epoch: 97 [42368/50048]	Loss: 0.0830
Training Epoch: 97 [42496/50048]	Loss: 0.0750
Training Epoch: 97 [42624/50048]	Loss: 0.0751
Training Epoch: 97 [42752/50048]	Loss: 0.0995
Training Epoch: 97 [42880/50048]	Loss: 0.1267
Training Epoch: 97 [43008/50048]	Loss: 0.0936
Training Epoch: 97 [43136/50048]	Loss: 0.0654
Training Epoch: 97 [43264/50048]	Loss: 0.0393
Training Epoch: 97 [43392/50048]	Loss: 0.0570
Training Epoch: 97 [43520/50048]	Loss: 0.0505
Training Epoch: 97 [43648/50048]	Loss: 0.1016
Training Epoch: 97 [43776/50048]	Loss: 0.1021
Training Epoch: 97 [43904/50048]	Loss: 0.1040
Training Epoch: 97 [44032/50048]	Loss: 0.0738
Training Epoch: 97 [44160/50048]	Loss: 0.0910
Training Epoch: 97 [44288/50048]	Loss: 0.1145
Training Epoch: 97 [44416/50048]	Loss: 0.1004
Training Epoch: 97 [44544/50048]	Loss: 0.0817
Training Epoch: 97 [44672/50048]	Loss: 0.0643
Training Epoch: 97 [44800/50048]	Loss: 0.0669
Training Epoch: 97 [44928/50048]	Loss: 0.1249
Training Epoch: 97 [45056/50048]	Loss: 0.0478
Training Epoch: 97 [45184/50048]	Loss: 0.0634
Training Epoch: 97 [45312/50048]	Loss: 0.0433
Training Epoch: 97 [45440/50048]	Loss: 0.0573
Training Epoch: 97 [45568/50048]	Loss: 0.1139
Training Epoch: 97 [45696/50048]	Loss: 0.0645
2022-12-06 06:04:41,784 [ZeusDataLoader(train)] train epoch 98 done: time=86.42 energy=10497.40
2022-12-06 06:04:41,785 [ZeusDataLoader(eval)] Epoch 98 begin.
Training Epoch: 97 [45824/50048]	Loss: 0.1041
Training Epoch: 97 [45952/50048]	Loss: 0.0433
Training Epoch: 97 [46080/50048]	Loss: 0.1042
Training Epoch: 97 [46208/50048]	Loss: 0.0861
Training Epoch: 97 [46336/50048]	Loss: 0.0618
Training Epoch: 97 [46464/50048]	Loss: 0.1746
Training Epoch: 97 [46592/50048]	Loss: 0.1333
Training Epoch: 97 [46720/50048]	Loss: 0.1066
Training Epoch: 97 [46848/50048]	Loss: 0.1277
Training Epoch: 97 [46976/50048]	Loss: 0.0657
Training Epoch: 97 [47104/50048]	Loss: 0.0392
Training Epoch: 97 [47232/50048]	Loss: 0.0412
Training Epoch: 97 [47360/50048]	Loss: 0.1033
Training Epoch: 97 [47488/50048]	Loss: 0.0322
Training Epoch: 97 [47616/50048]	Loss: 0.0920
Training Epoch: 97 [47744/50048]	Loss: 0.0512
Training Epoch: 97 [47872/50048]	Loss: 0.0289
Training Epoch: 97 [48000/50048]	Loss: 0.0618
Training Epoch: 97 [48128/50048]	Loss: 0.1166
Training Epoch: 97 [48256/50048]	Loss: 0.1289
Training Epoch: 97 [48384/50048]	Loss: 0.0755
Training Epoch: 97 [48512/50048]	Loss: 0.0882
Training Epoch: 97 [48640/50048]	Loss: 0.0431
Training Epoch: 97 [48768/50048]	Loss: 0.0437
Training Epoch: 97 [48896/50048]	Loss: 0.0521
Training Epoch: 97 [49024/50048]	Loss: 0.0808
Training Epoch: 97 [49152/50048]	Loss: 0.0892
Training Epoch: 97 [49280/50048]	Loss: 0.1099
Training Epoch: 97 [49408/50048]	Loss: 0.0544
Training Epoch: 97 [49536/50048]	Loss: 0.0628
Training Epoch: 97 [49664/50048]	Loss: 0.0443
Training Epoch: 97 [49792/50048]	Loss: 0.0931
Training Epoch: 97 [49920/50048]	Loss: 0.1951
Training Epoch: 97 [50048/50048]	Loss: 0.0373
2022-12-06 11:04:45.496 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:04:45,518 [ZeusDataLoader(eval)] eval epoch 98 done: time=3.72 energy=452.21
2022-12-06 06:04:45,519 [ZeusDataLoader(train)] Up to epoch 98: time=8841.10, energy=1073297.32, cost=1310245.12
2022-12-06 06:04:45,519 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:04:45,519 [ZeusDataLoader(train)] Expected next epoch: time=8930.90, energy=1084095.34, cost=1323501.51
2022-12-06 06:04:45,520 [ZeusDataLoader(train)] Epoch 99 begin.
Validation Epoch: 97, Average loss: 0.0193, Accuracy: 0.6375
2022-12-06 06:04:45,704 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:04:45,705 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:04:45.707 [ZeusMonitor] Monitor started.
2022-12-06 11:04:45.707 [ZeusMonitor] Running indefinitely. 2022-12-06 11:04:45.707 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:04:45.707 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e99+gpu0.power.log
Training Epoch: 98 [128/50048]	Loss: 0.0943
Training Epoch: 98 [256/50048]	Loss: 0.0468
Training Epoch: 98 [384/50048]	Loss: 0.0572
Training Epoch: 98 [512/50048]	Loss: 0.0673
Training Epoch: 98 [640/50048]	Loss: 0.0566
Training Epoch: 98 [768/50048]	Loss: 0.0278
Training Epoch: 98 [896/50048]	Loss: 0.0586
Training Epoch: 98 [1024/50048]	Loss: 0.0554
Training Epoch: 98 [1152/50048]	Loss: 0.0265
Training Epoch: 98 [1280/50048]	Loss: 0.0319
Training Epoch: 98 [1408/50048]	Loss: 0.0608
Training Epoch: 98 [1536/50048]	Loss: 0.0599
Training Epoch: 98 [1664/50048]	Loss: 0.0450
Training Epoch: 98 [1792/50048]	Loss: 0.0416
Training Epoch: 98 [1920/50048]	Loss: 0.0614
Training Epoch: 98 [2048/50048]	Loss: 0.0260
Training Epoch: 98 [2176/50048]	Loss: 0.0351
Training Epoch: 98 [2304/50048]	Loss: 0.0688
Training Epoch: 98 [2432/50048]	Loss: 0.0875
Training Epoch: 98 [2560/50048]	Loss: 0.0201
Training Epoch: 98 [2688/50048]	Loss: 0.0399
Training Epoch: 98 [2816/50048]	Loss: 0.0448
Training Epoch: 98 [2944/50048]	Loss: 0.0380
Training Epoch: 98 [3072/50048]	Loss: 0.0556
Training Epoch: 98 [3200/50048]	Loss: 0.0614
Training Epoch: 98 [3328/50048]	Loss: 0.0909
Training Epoch: 98 [3456/50048]	Loss: 0.0693
Training Epoch: 98 [3584/50048]	Loss: 0.0441
Training Epoch: 98 [3712/50048]	Loss: 0.0486
Training Epoch: 98 [3840/50048]	Loss: 0.0416
Training Epoch: 98 [3968/50048]	Loss: 0.0609
Training Epoch: 98 [4096/50048]	Loss: 0.0608
Training Epoch: 98 [4224/50048]	Loss: 0.0655
Training Epoch: 98 [4352/50048]	Loss: 0.0345
Training Epoch: 98 [4480/50048]	Loss: 0.0546
Training Epoch: 98 [4608/50048]	Loss: 0.0202
Training Epoch: 98 [4736/50048]	Loss: 0.0256
Training Epoch: 98 [4864/50048]	Loss: 0.0958
Training Epoch: 98 [4992/50048]	Loss: 0.0339
Training Epoch: 98 [5120/50048]	Loss: 0.0666
Training Epoch: 98 [5248/50048]	Loss: 0.0890
Training Epoch: 98 [5376/50048]	Loss: 0.0509
Training Epoch: 98 [5504/50048]	Loss: 0.0706
Training Epoch: 98 [5632/50048]	Loss: 0.0394
Training Epoch: 98 [5760/50048]	Loss: 0.0719
Training Epoch: 98 [5888/50048]	Loss: 0.0489
Training Epoch: 98 [6016/50048]	Loss: 0.1343
Training Epoch: 98 [6144/50048]	Loss: 0.1153
Training Epoch: 98 [6272/50048]	Loss: 0.0924
Training Epoch: 98 [6400/50048]	Loss: 0.0611
Training Epoch: 98 [6528/50048]	Loss: 0.0499
Training Epoch: 98 [6656/50048]	Loss: 0.0649
Training Epoch: 98 [6784/50048]	Loss: 0.0498
Training Epoch: 98 [6912/50048]	Loss: 0.0968
Training Epoch: 98 [7040/50048]	Loss: 0.0880
Training Epoch: 98 [7168/50048]	Loss: 0.1101
Training Epoch: 98 [7296/50048]	Loss: 0.0726
Training Epoch: 98 [7424/50048]	Loss: 0.0800
Training Epoch: 98 [7552/50048]	Loss: 0.0760
Training Epoch: 98 [7680/50048]	Loss: 0.1144
Training Epoch: 98 [7808/50048]	Loss: 0.1536
Training Epoch: 98 [7936/50048]	Loss: 0.0787
Training Epoch: 98 [8064/50048]	Loss: 0.0536
Training Epoch: 98 [8192/50048]	Loss: 0.1631
Training Epoch: 98 [8320/50048]	Loss: 0.0823
Training Epoch: 98 [8448/50048]	Loss: 0.0448
Training Epoch: 98 [8576/50048]	Loss: 0.1367
Training Epoch: 98 [8704/50048]	Loss: 0.0293
Training Epoch: 98 [8832/50048]	Loss: 0.0538
Training Epoch: 98 [8960/50048]	Loss: 0.0740
Training Epoch: 98 [9088/50048]	Loss: 0.0429
Training Epoch: 98 [9216/50048]	Loss: 0.0599
Training Epoch: 98 [9344/50048]	Loss: 0.0513
Training Epoch: 98 [9472/50048]	Loss: 0.0489
Training Epoch: 98 [9600/50048]	Loss: 0.0484
Training Epoch: 98 [9728/50048]	Loss: 0.1262
Training Epoch: 98 [9856/50048]	Loss: 0.0553
Training Epoch: 98 [9984/50048]	Loss: 0.0670
Training Epoch: 98 [10112/50048]	Loss: 0.0768
Training Epoch: 98 [10240/50048]	Loss: 0.0483
Training Epoch: 98 [10368/50048]	Loss: 0.1532
Training Epoch: 98 [10496/50048]	Loss: 0.0829
Training Epoch: 98 [10624/50048]	Loss: 0.1129
Training Epoch: 98 [10752/50048]	Loss: 0.0749
Training Epoch: 98 [10880/50048]	Loss: 0.0736
Training Epoch: 98 [11008/50048]	Loss: 0.0487
Training Epoch: 98 [11136/50048]	Loss: 0.0423
Training Epoch: 98 [11264/50048]	Loss: 0.0593
Training Epoch: 98 [11392/50048]	Loss: 0.0902
Training Epoch: 98 [11520/50048]	Loss: 0.0576
Training Epoch: 98 [11648/50048]	Loss: 0.0758
Training Epoch: 98 [11776/50048]	Loss: 0.0900
Training Epoch: 98 [11904/50048]	Loss: 0.0593
Training Epoch: 98 [12032/50048]	Loss: 0.0467
Training Epoch: 98 [12160/50048]	Loss: 0.0784
Training Epoch: 98 [12288/50048]	Loss: 0.0860
Training Epoch: 98 [12416/50048]	Loss: 0.0783
Training Epoch: 98 [12544/50048]	Loss: 0.0332
Training Epoch: 98 [12672/50048]	Loss: 0.0557
Training Epoch: 98 [12800/50048]	Loss: 0.1036
Training Epoch: 98 [12928/50048]	Loss: 0.0486
Training Epoch: 98 [13056/50048]	Loss: 0.0918
Training Epoch: 98 [13184/50048]	Loss: 0.0737
Training Epoch: 98 [13312/50048]	Loss: 0.0554
Training Epoch: 98 [13440/50048]	Loss: 0.1061
Training Epoch: 98 [13568/50048]	Loss: 0.0491
Training Epoch: 98 [13696/50048]	Loss: 0.0295
Training Epoch: 98 [13824/50048]	Loss: 0.1487
Training Epoch: 98 [13952/50048]	Loss: 0.0883
Training Epoch: 98 [14080/50048]	Loss: 0.0911
Training Epoch: 98 [14208/50048]	Loss: 0.0534
Training Epoch: 98 [14336/50048]	Loss: 0.0990
Training Epoch: 98 [14464/50048]	Loss: 0.0474
Training Epoch: 98 [14592/50048]	Loss: 0.0831
Training Epoch: 98 [14720/50048]	Loss: 0.0859
Training Epoch: 98 [14848/50048]	Loss: 0.0883
Training Epoch: 98 [14976/50048]	Loss: 0.0445
Training Epoch: 98 [15104/50048]	Loss: 0.0653
Training Epoch: 98 [15232/50048]	Loss: 0.0532
Training Epoch: 98 [15360/50048]	Loss: 0.0572
Training Epoch: 98 [15488/50048]	Loss: 0.0713
Training Epoch: 98 [15616/50048]	Loss: 0.1502
Training Epoch: 98 [15744/50048]	Loss: 0.0409
Training Epoch: 98 [15872/50048]	Loss: 0.0640
Training Epoch: 98 [16000/50048]	Loss: 0.0354
Training Epoch: 98 [16128/50048]	Loss: 0.0285
Training Epoch: 98 [16256/50048]	Loss: 0.0901
Training Epoch: 98 [16384/50048]	Loss: 0.0464
Training Epoch: 98 [16512/50048]	Loss: 0.1186
Training Epoch: 98 [16640/50048]	Loss: 0.0562
Training Epoch: 98 [16768/50048]	Loss: 0.0574
Training Epoch: 98 [16896/50048]	Loss: 0.1232
Training Epoch: 98 [17024/50048]	Loss: 0.0892
Training Epoch: 98 [17152/50048]	Loss: 0.0306
Training Epoch: 98 [17280/50048]	Loss: 0.0842
Training Epoch: 98 [17408/50048]	Loss: 0.0946
Training Epoch: 98 [17536/50048]	Loss: 0.1416
Training Epoch: 98 [17664/50048]	Loss: 0.1112
Training Epoch: 98 [17792/50048]	Loss: 0.0868
Training Epoch: 98 [17920/50048]	Loss: 0.0895
Training Epoch: 98 [18048/50048]	Loss: 0.0322
Training Epoch: 98 [18176/50048]	Loss: 0.0518
Training Epoch: 98 [18304/50048]	Loss: 0.1149
Training Epoch: 98 [18432/50048]	Loss: 0.0224
Training Epoch: 98 [18560/50048]	Loss: 0.0811
Training Epoch: 98 [18688/50048]	Loss: 0.0785
Training Epoch: 98 [18816/50048]	Loss: 0.0173
Training Epoch: 98 [18944/50048]	Loss: 0.0702
Training Epoch: 98 [19072/50048]	Loss: 0.0858
Training Epoch: 98 [19200/50048]	Loss: 0.0789
Training Epoch: 98 [19328/50048]	Loss: 0.0666
Training Epoch: 98 [19456/50048]	Loss: 0.0473
Training Epoch: 98 [19584/50048]	Loss: 0.0650
Training Epoch: 98 [19712/50048]	Loss: 0.0520
Training Epoch: 98 [19840/50048]	Loss: 0.1093
Training Epoch: 98 [19968/50048]	Loss: 0.0836
Training Epoch: 98 [20096/50048]	Loss: 0.0243
Training Epoch: 98 [20224/50048]	Loss: 0.1000
Training Epoch: 98 [20352/50048]	Loss: 0.0423
Training Epoch: 98 [20480/50048]	Loss: 0.0537
Training Epoch: 98 [20608/50048]	Loss: 0.0988
Training Epoch: 98 [20736/50048]	Loss: 0.0449
Training Epoch: 98 [20864/50048]	Loss: 0.0596
Training Epoch: 98 [20992/50048]	Loss: 0.1059
Training Epoch: 98 [21120/50048]	Loss: 0.0653
Training Epoch: 98 [21248/50048]	Loss: 0.0374
Training Epoch: 98 [21376/50048]	Loss: 0.0568
Training Epoch: 98 [21504/50048]	Loss: 0.0433
Training Epoch: 98 [21632/50048]	Loss: 0.1088
Training Epoch: 98 [21760/50048]	Loss: 0.0853
Training Epoch: 98 [21888/50048]	Loss: 0.0729
Training Epoch: 98 [22016/50048]	Loss: 0.0970
Training Epoch: 98 [22144/50048]	Loss: 0.0789
Training Epoch: 98 [22272/50048]	Loss: 0.0476
Training Epoch: 98 [22400/50048]	Loss: 0.0587
Training Epoch: 98 [22528/50048]	Loss: 0.0945
Training Epoch: 98 [22656/50048]	Loss: 0.0239
Training Epoch: 98 [22784/50048]	Loss: 0.0856
Training Epoch: 98 [22912/50048]	Loss: 0.0325
Training Epoch: 98 [23040/50048]	Loss: 0.0639
Training Epoch: 98 [23168/50048]	Loss: 0.0423
Training Epoch: 98 [23296/50048]	Loss: 0.0690
Training Epoch: 98 [23424/50048]	Loss: 0.0621
Training Epoch: 98 [23552/50048]	Loss: 0.0534
Training Epoch: 98 [23680/50048]	Loss: 0.0877
Training Epoch: 98 [23808/50048]	Loss: 0.1025
Training Epoch: 98 [23936/50048]	Loss: 0.1104
Training Epoch: 98 [24064/50048]	Loss: 0.0399
Training Epoch: 98 [24192/50048]	Loss: 0.0970
Training Epoch: 98 [24320/50048]	Loss: 0.1115
Training Epoch: 98 [24448/50048]	Loss: 0.0587
Training Epoch: 98 [24576/50048]	Loss: 0.0562
Training Epoch: 98 [24704/50048]	Loss: 0.0382
Training Epoch: 98 [24832/50048]	Loss: 0.0577
Training Epoch: 98 [24960/50048]	Loss: 0.0935
Training Epoch: 98 [25088/50048]	Loss: 0.0375
Training Epoch: 98 [25216/50048]	Loss: 0.0475
Training Epoch: 98 [25344/50048]	Loss: 0.0703
Training Epoch: 98 [25472/50048]	Loss: 0.1494
Training Epoch: 98 [25600/50048]	Loss: 0.0242
Training Epoch: 98 [25728/50048]	Loss: 0.0308
Training Epoch: 98 [25856/50048]	Loss: 0.0646
Training Epoch: 98 [25984/50048]	Loss: 0.0574
Training Epoch: 98 [26112/50048]	Loss: 0.0741
Training Epoch: 98 [26240/50048]	Loss: 0.1388
Training Epoch: 98 [26368/50048]	Loss: 0.0596
Training Epoch: 98 [26496/50048]	Loss: 0.0392
Training Epoch: 98 [26624/50048]	Loss: 0.0685
Training Epoch: 98 [26752/50048]	Loss: 0.0358
Training Epoch: 98 [26880/50048]	Loss: 0.0930
Training Epoch: 98 [27008/50048]	Loss: 0.0928
Training Epoch: 98 [27136/50048]	Loss: 0.0798
Training Epoch: 98 [27264/50048]	Loss: 0.1218
Training Epoch: 98 [27392/50048]	Loss: 0.1068
Training Epoch: 98 [27520/50048]	Loss: 0.0706
Training Epoch: 98 [27648/50048]	Loss: 0.0299
Training Epoch: 98 [27776/50048]	Loss: 0.0851
Training Epoch: 98 [27904/50048]	Loss: 0.0641
Training Epoch: 98 [28032/50048]	Loss: 0.1068
Training Epoch: 98 [28160/50048]	Loss: 0.0380
Training Epoch: 98 [28288/50048]	Loss: 0.0649
Training Epoch: 98 [28416/50048]	Loss: 0.1424
Training Epoch: 98 [28544/50048]	Loss: 0.0431
Training Epoch: 98 [28672/50048]	Loss: 0.0501
Training Epoch: 98 [28800/50048]	Loss: 0.0402
Training Epoch: 98 [28928/50048]	Loss: 0.0891
Training Epoch: 98 [29056/50048]	Loss: 0.0759
Training Epoch: 98 [29184/50048]	Loss: 0.0324
Training Epoch: 98 [29312/50048]	Loss: 0.1035
Training Epoch: 98 [29440/50048]	Loss: 0.0883
Training Epoch: 98 [29568/50048]	Loss: 0.0719
Training Epoch: 98 [29696/50048]	Loss: 0.0891
Training Epoch: 98 [29824/50048]	Loss: 0.0987
Training Epoch: 98 [29952/50048]	Loss: 0.0538
Training Epoch: 98 [30080/50048]	Loss: 0.0903
Training Epoch: 98 [30208/50048]	Loss: 0.0917
Training Epoch: 98 [30336/50048]	Loss: 0.0413
Training Epoch: 98 [30464/50048]	Loss: 0.1109
Training Epoch: 98 [30592/50048]	Loss: 0.0789
Training Epoch: 98 [30720/50048]	Loss: 0.0754
Training Epoch: 98 [30848/50048]	Loss: 0.0825
Training Epoch: 98 [30976/50048]	Loss: 0.2840
Training Epoch: 98 [31104/50048]	Loss: 0.0395
Training Epoch: 98 [31232/50048]	Loss: 0.0759
Training Epoch: 98 [31360/50048]	Loss: 0.0828
Training Epoch: 98 [31488/50048]	Loss: 0.0738
Training Epoch: 98 [31616/50048]	Loss: 0.0509
Training Epoch: 98 [31744/50048]	Loss: 0.1042
Training Epoch: 98 [31872/50048]	Loss: 0.0680
Training Epoch: 98 [32000/50048]	Loss: 0.1196
Training Epoch: 98 [32128/50048]	Loss: 0.1022
Training Epoch: 98 [32256/50048]	Loss: 0.0484
Training Epoch: 98 [32384/50048]	Loss: 0.0820
Training Epoch: 98 [32512/50048]	Loss: 0.1464
Training Epoch: 98 [32640/50048]	Loss: 0.0773
Training Epoch: 98 [32768/50048]	Loss: 0.0656
Training Epoch: 98 [32896/50048]	Loss: 0.0342
Training Epoch: 98 [33024/50048]	Loss: 0.1393
Training Epoch: 98 [33152/50048]	Loss: 0.0404
Training Epoch: 98 [33280/50048]	Loss: 0.0801
Training Epoch: 98 [33408/50048]	Loss: 0.0439
Training Epoch: 98 [33536/50048]	Loss: 0.0641
Training Epoch: 98 [33664/50048]	Loss: 0.1202
Training Epoch: 98 [33792/50048]	Loss: 0.0829
Training Epoch: 98 [33920/50048]	Loss: 0.0990
Training Epoch: 98 [34048/50048]	Loss: 0.1115
Training Epoch: 98 [34176/50048]	Loss: 0.0455
Training Epoch: 98 [34304/50048]	Loss: 0.0798
Training Epoch: 98 [34432/50048]	Loss: 0.0731
Training Epoch: 98 [34560/50048]	Loss: 0.0580
Training Epoch: 98 [34688/50048]	Loss: 0.0798
Training Epoch: 98 [34816/50048]	Loss: 0.1096
Training Epoch: 98 [34944/50048]	Loss: 0.0664
Training Epoch: 98 [35072/50048]	Loss: 0.0879
Training Epoch: 98 [35200/50048]	Loss: 0.0441
Training Epoch: 98 [35328/50048]	Loss: 0.0596
Training Epoch: 98 [35456/50048]	Loss: 0.0330
Training Epoch: 98 [35584/50048]	Loss: 0.1031
Training Epoch: 98 [35712/50048]	Loss: 0.0976
Training Epoch: 98 [35840/50048]	Loss: 0.0681
Training Epoch: 98 [35968/50048]	Loss: 0.1187
Training Epoch: 98 [36096/50048]	Loss: 0.0833
Training Epoch: 98 [36224/50048]	Loss: 0.0835
Training Epoch: 98 [36352/50048]	Loss: 0.0528
Training Epoch: 98 [36480/50048]	Loss: 0.0352
Training Epoch: 98 [36608/50048]	Loss: 0.0668
Training Epoch: 98 [36736/50048]	Loss: 0.1435
Training Epoch: 98 [36864/50048]	Loss: 0.0763
Training Epoch: 98 [36992/50048]	Loss: 0.0505
Training Epoch: 98 [37120/50048]	Loss: 0.0751
Training Epoch: 98 [37248/50048]	Loss: 0.0431
Training Epoch: 98 [37376/50048]	Loss: 0.0790
Training Epoch: 98 [37504/50048]	Loss: 0.0828
Training Epoch: 98 [37632/50048]	Loss: 0.0681
Training Epoch: 98 [37760/50048]	Loss: 0.0913
Training Epoch: 98 [37888/50048]	Loss: 0.1082
Training Epoch: 98 [38016/50048]	Loss: 0.1023
Training Epoch: 98 [38144/50048]	Loss: 0.1132
Training Epoch: 98 [38272/50048]	Loss: 0.0409
Training Epoch: 98 [38400/50048]	Loss: 0.0669
Training Epoch: 98 [38528/50048]	Loss: 0.0675
Training Epoch: 98 [38656/50048]	Loss: 0.0558
Training Epoch: 98 [38784/50048]	Loss: 0.0683
Training Epoch: 98 [38912/50048]	Loss: 0.0524
Training Epoch: 98 [39040/50048]	Loss: 0.0534
Training Epoch: 98 [39168/50048]	Loss: 0.0608
Training Epoch: 98 [39296/50048]	Loss: 0.1247
Training Epoch: 98 [39424/50048]	Loss: 0.0662
Training Epoch: 98 [39552/50048]	Loss: 0.0842
Training Epoch: 98 [39680/50048]	Loss: 0.0793
Training Epoch: 98 [39808/50048]	Loss: 0.1076
Training Epoch: 98 [39936/50048]	Loss: 0.1115
Training Epoch: 98 [40064/50048]	Loss: 0.0413
Training Epoch: 98 [40192/50048]	Loss: 0.0788
Training Epoch: 98 [40320/50048]	Loss: 0.0381
Training Epoch: 98 [40448/50048]	Loss: 0.0926
Training Epoch: 98 [40576/50048]	Loss: 0.0882
Training Epoch: 98 [40704/50048]	Loss: 0.0762
Training Epoch: 98 [40832/50048]	Loss: 0.0542
Training Epoch: 98 [40960/50048]	Loss: 0.0425
Training Epoch: 98 [41088/50048]	Loss: 0.1389
Training Epoch: 98 [41216/50048]	Loss: 0.0375
Training Epoch: 98 [41344/50048]	Loss: 0.0569
Training Epoch: 98 [41472/50048]	Loss: 0.1139
Training Epoch: 98 [41600/50048]	Loss: 0.0378
Training Epoch: 98 [41728/50048]	Loss: 0.0422
Training Epoch: 98 [41856/50048]	Loss: 0.0796
Training Epoch: 98 [41984/50048]	Loss: 0.0224
Training Epoch: 98 [42112/50048]	Loss: 0.0677
Training Epoch: 98 [42240/50048]	Loss: 0.0620
Training Epoch: 98 [42368/50048]	Loss: 0.0843
Training Epoch: 98 [42496/50048]	Loss: 0.0316
Training Epoch: 98 [42624/50048]	Loss: 0.0987
Training Epoch: 98 [42752/50048]	Loss: 0.1063
Training Epoch: 98 [42880/50048]	Loss: 0.1569
Training Epoch: 98 [43008/50048]	Loss: 0.0632
Training Epoch: 98 [43136/50048]	Loss: 0.0782
Training Epoch: 98 [43264/50048]	Loss: 0.0789
Training Epoch: 98 [43392/50048]	Loss: 0.0616
Training Epoch: 98 [43520/50048]	Loss: 0.0827
Training Epoch: 98 [43648/50048]	Loss: 0.0762
Training Epoch: 98 [43776/50048]	Loss: 0.0656
Training Epoch: 98 [43904/50048]	Loss: 0.0487
Training Epoch: 98 [44032/50048]	Loss: 0.0847
Training Epoch: 98 [44160/50048]	Loss: 0.1786
Training Epoch: 98 [44288/50048]	Loss: 0.0436
Training Epoch: 98 [44416/50048]	Loss: 0.0535
Training Epoch: 98 [44544/50048]	Loss: 0.0808
Training Epoch: 98 [44672/50048]	Loss: 0.1040
Training Epoch: 98 [44800/50048]	Loss: 0.0473
Training Epoch: 98 [44928/50048]	Loss: 0.0801
Training Epoch: 98 [45056/50048]	Loss: 0.0364
Training Epoch: 98 [45184/50048]	Loss: 0.0887
Training Epoch: 98 [45312/50048]	Loss: 0.1049
Training Epoch: 98 [45440/50048]	Loss: 0.1283
Training Epoch: 98 [45568/50048]	Loss: 0.0467
Training Epoch: 98 [45696/50048]	Loss: 0.1266
2022-12-06 06:06:11,985 [ZeusDataLoader(train)] train epoch 99 done: time=86.45 energy=10494.27
2022-12-06 06:06:11,986 [ZeusDataLoader(eval)] Epoch 99 begin.
Training Epoch: 98 [45824/50048]	Loss: 0.0778
Training Epoch: 98 [45952/50048]	Loss: 0.0548
Training Epoch: 98 [46080/50048]	Loss: 0.0538
Training Epoch: 98 [46208/50048]	Loss: 0.0670
Training Epoch: 98 [46336/50048]	Loss: 0.1027
Training Epoch: 98 [46464/50048]	Loss: 0.1119
Training Epoch: 98 [46592/50048]	Loss: 0.1124
Training Epoch: 98 [46720/50048]	Loss: 0.0710
Training Epoch: 98 [46848/50048]	Loss: 0.0625
Training Epoch: 98 [46976/50048]	Loss: 0.0652
Training Epoch: 98 [47104/50048]	Loss: 0.0919
Training Epoch: 98 [47232/50048]	Loss: 0.0422
Training Epoch: 98 [47360/50048]	Loss: 0.0286
Training Epoch: 98 [47488/50048]	Loss: 0.0754
Training Epoch: 98 [47616/50048]	Loss: 0.0769
Training Epoch: 98 [47744/50048]	Loss: 0.0542
Training Epoch: 98 [47872/50048]	Loss: 0.0897
Training Epoch: 98 [48000/50048]	Loss: 0.1331
Training Epoch: 98 [48128/50048]	Loss: 0.0501
Training Epoch: 98 [48256/50048]	Loss: 0.0913
Training Epoch: 98 [48384/50048]	Loss: 0.0721
Training Epoch: 98 [48512/50048]	Loss: 0.1005
Training Epoch: 98 [48640/50048]	Loss: 0.0672
Training Epoch: 98 [48768/50048]	Loss: 0.0671
Training Epoch: 98 [48896/50048]	Loss: 0.0741
Training Epoch: 98 [49024/50048]	Loss: 0.0658
Training Epoch: 98 [49152/50048]	Loss: 0.1592
Training Epoch: 98 [49280/50048]	Loss: 0.1244
Training Epoch: 98 [49408/50048]	Loss: 0.0646
Training Epoch: 98 [49536/50048]	Loss: 0.0756
Training Epoch: 98 [49664/50048]	Loss: 0.1042
Training Epoch: 98 [49792/50048]	Loss: 0.0505
Training Epoch: 98 [49920/50048]	Loss: 0.0788
Training Epoch: 98 [50048/50048]	Loss: 0.1575
2022-12-06 11:06:15.680 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:06:15,697 [ZeusDataLoader(eval)] eval epoch 99 done: time=3.70 energy=454.51
2022-12-06 06:06:15,697 [ZeusDataLoader(train)] Up to epoch 99: time=8931.26, energy=1084246.10, cost=1323608.21
2022-12-06 06:06:15,697 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:06:15,697 [ZeusDataLoader(train)] Expected next epoch: time=9021.06, energy=1095044.11, cost=1336864.59
2022-12-06 06:06:15,698 [ZeusDataLoader(train)] Epoch 100 begin.
Validation Epoch: 98, Average loss: 0.0187, Accuracy: 0.6473
2022-12-06 06:06:15,837 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:06:15,838 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:06:15.841 [ZeusMonitor] Monitor started.
2022-12-06 11:06:15.841 [ZeusMonitor] Running indefinitely. 2022-12-06 11:06:15.841 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:06:15.841 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e100+gpu0.power.log
Training Epoch: 99 [128/50048]	Loss: 0.0790
Training Epoch: 99 [256/50048]	Loss: 0.0810
Training Epoch: 99 [384/50048]	Loss: 0.0548
Training Epoch: 99 [512/50048]	Loss: 0.0637
Training Epoch: 99 [640/50048]	Loss: 0.0559
Training Epoch: 99 [768/50048]	Loss: 0.0259
Training Epoch: 99 [896/50048]	Loss: 0.0386
Training Epoch: 99 [1024/50048]	Loss: 0.0494
Training Epoch: 99 [1152/50048]	Loss: 0.0302
Training Epoch: 99 [1280/50048]	Loss: 0.0313
Training Epoch: 99 [1408/50048]	Loss: 0.0200
Training Epoch: 99 [1536/50048]	Loss: 0.0589
Training Epoch: 99 [1664/50048]	Loss: 0.0555
Training Epoch: 99 [1792/50048]	Loss: 0.1104
Training Epoch: 99 [1920/50048]	Loss: 0.0520
Training Epoch: 99 [2048/50048]	Loss: 0.1028
Training Epoch: 99 [2176/50048]	Loss: 0.0411
Training Epoch: 99 [2304/50048]	Loss: 0.1061
Training Epoch: 99 [2432/50048]	Loss: 0.0801
Training Epoch: 99 [2560/50048]	Loss: 0.1174
Training Epoch: 99 [2688/50048]	Loss: 0.0418
Training Epoch: 99 [2816/50048]	Loss: 0.0494
Training Epoch: 99 [2944/50048]	Loss: 0.0569
Training Epoch: 99 [3072/50048]	Loss: 0.0526
Training Epoch: 99 [3200/50048]	Loss: 0.0738
Training Epoch: 99 [3328/50048]	Loss: 0.0998
Training Epoch: 99 [3456/50048]	Loss: 0.0480
Training Epoch: 99 [3584/50048]	Loss: 0.0907
Training Epoch: 99 [3712/50048]	Loss: 0.0672
Training Epoch: 99 [3840/50048]	Loss: 0.0543
Training Epoch: 99 [3968/50048]	Loss: 0.0911
Training Epoch: 99 [4096/50048]	Loss: 0.0906
Training Epoch: 99 [4224/50048]	Loss: 0.0405
Training Epoch: 99 [4352/50048]	Loss: 0.0396
Training Epoch: 99 [4480/50048]	Loss: 0.0893
Training Epoch: 99 [4608/50048]	Loss: 0.0763
Training Epoch: 99 [4736/50048]	Loss: 0.1045
Training Epoch: 99 [4864/50048]	Loss: 0.1167
Training Epoch: 99 [4992/50048]	Loss: 0.1718
Training Epoch: 99 [5120/50048]	Loss: 0.0604
Training Epoch: 99 [5248/50048]	Loss: 0.0803
Training Epoch: 99 [5376/50048]	Loss: 0.0547
Training Epoch: 99 [5504/50048]	Loss: 0.0831
Training Epoch: 99 [5632/50048]	Loss: 0.1116
Training Epoch: 99 [5760/50048]	Loss: 0.0596
Training Epoch: 99 [5888/50048]	Loss: 0.0596
Training Epoch: 99 [6016/50048]	Loss: 0.0410
Training Epoch: 99 [6144/50048]	Loss: 0.0697
Training Epoch: 99 [6272/50048]	Loss: 0.0688
Training Epoch: 99 [6400/50048]	Loss: 0.0611
Training Epoch: 99 [6528/50048]	Loss: 0.0527
Training Epoch: 99 [6656/50048]	Loss: 0.0554
Training Epoch: 99 [6784/50048]	Loss: 0.0302
Training Epoch: 99 [6912/50048]	Loss: 0.0865
Training Epoch: 99 [7040/50048]	Loss: 0.0599
Training Epoch: 99 [7168/50048]	Loss: 0.0870
Training Epoch: 99 [7296/50048]	Loss: 0.1347
Training Epoch: 99 [7424/50048]	Loss: 0.0702
Training Epoch: 99 [7552/50048]	Loss: 0.0583
Training Epoch: 99 [7680/50048]	Loss: 0.0686
Training Epoch: 99 [7808/50048]	Loss: 0.0609
Training Epoch: 99 [7936/50048]	Loss: 0.0490
Training Epoch: 99 [8064/50048]	Loss: 0.0506
Training Epoch: 99 [8192/50048]	Loss: 0.0372
Training Epoch: 99 [8320/50048]	Loss: 0.1026
Training Epoch: 99 [8448/50048]	Loss: 0.0363
Training Epoch: 99 [8576/50048]	Loss: 0.1224
Training Epoch: 99 [8704/50048]	Loss: 0.0487
Training Epoch: 99 [8832/50048]	Loss: 0.0576
Training Epoch: 99 [8960/50048]	Loss: 0.0450
Training Epoch: 99 [9088/50048]	Loss: 0.0186
Training Epoch: 99 [9216/50048]	Loss: 0.0818
Training Epoch: 99 [9344/50048]	Loss: 0.0358
Training Epoch: 99 [9472/50048]	Loss: 0.1126
Training Epoch: 99 [9600/50048]	Loss: 0.0561
Training Epoch: 99 [9728/50048]	Loss: 0.0742
Training Epoch: 99 [9856/50048]	Loss: 0.0866
Training Epoch: 99 [9984/50048]	Loss: 0.1559
Training Epoch: 99 [10112/50048]	Loss: 0.0501
Training Epoch: 99 [10240/50048]	Loss: 0.1137
Training Epoch: 99 [10368/50048]	Loss: 0.0735
Training Epoch: 99 [10496/50048]	Loss: 0.0679
Training Epoch: 99 [10624/50048]	Loss: 0.0460
Training Epoch: 99 [10752/50048]	Loss: 0.0666
Training Epoch: 99 [10880/50048]	Loss: 0.0780
Training Epoch: 99 [11008/50048]	Loss: 0.1405
Training Epoch: 99 [11136/50048]	Loss: 0.0809
Training Epoch: 99 [11264/50048]	Loss: 0.0598
Training Epoch: 99 [11392/50048]	Loss: 0.0557
Training Epoch: 99 [11520/50048]	Loss: 0.1078
Training Epoch: 99 [11648/50048]	Loss: 0.0335
Training Epoch: 99 [11776/50048]	Loss: 0.1023
Training Epoch: 99 [11904/50048]	Loss: 0.0251
Training Epoch: 99 [12032/50048]	Loss: 0.0335
Training Epoch: 99 [12160/50048]	Loss: 0.0404
Training Epoch: 99 [12288/50048]	Loss: 0.0939
Training Epoch: 99 [12416/50048]	Loss: 0.0878
Training Epoch: 99 [12544/50048]	Loss: 0.0505
Training Epoch: 99 [12672/50048]	Loss: 0.1818
Training Epoch: 99 [12800/50048]	Loss: 0.1134
Training Epoch: 99 [12928/50048]	Loss: 0.0716
Training Epoch: 99 [13056/50048]	Loss: 0.1238
Training Epoch: 99 [13184/50048]	Loss: 0.1772
Training Epoch: 99 [13312/50048]	Loss: 0.0520
Training Epoch: 99 [13440/50048]	Loss: 0.0393
Training Epoch: 99 [13568/50048]	Loss: 0.0689
Training Epoch: 99 [13696/50048]	Loss: 0.0544
Training Epoch: 99 [13824/50048]	Loss: 0.0606
Training Epoch: 99 [13952/50048]	Loss: 0.0336
Training Epoch: 99 [14080/50048]	Loss: 0.0451
Training Epoch: 99 [14208/50048]	Loss: 0.0768
Training Epoch: 99 [14336/50048]	Loss: 0.0970
Training Epoch: 99 [14464/50048]	Loss: 0.0570
Training Epoch: 99 [14592/50048]	Loss: 0.0856
Training Epoch: 99 [14720/50048]	Loss: 0.0404
Training Epoch: 99 [14848/50048]	Loss: 0.1245
Training Epoch: 99 [14976/50048]	Loss: 0.0538
Training Epoch: 99 [15104/50048]	Loss: 0.0625
Training Epoch: 99 [15232/50048]	Loss: 0.0897
Training Epoch: 99 [15360/50048]	Loss: 0.0268
Training Epoch: 99 [15488/50048]	Loss: 0.0365
Training Epoch: 99 [15616/50048]	Loss: 0.1008
Training Epoch: 99 [15744/50048]	Loss: 0.0762
Training Epoch: 99 [15872/50048]	Loss: 0.0971
Training Epoch: 99 [16000/50048]	Loss: 0.1006
Training Epoch: 99 [16128/50048]	Loss: 0.0510
Training Epoch: 99 [16256/50048]	Loss: 0.0619
Training Epoch: 99 [16384/50048]	Loss: 0.0638
Training Epoch: 99 [16512/50048]	Loss: 0.0595
Training Epoch: 99 [16640/50048]	Loss: 0.0364
Training Epoch: 99 [16768/50048]	Loss: 0.0549
Training Epoch: 99 [16896/50048]	Loss: 0.0274
Training Epoch: 99 [17024/50048]	Loss: 0.0655
Training Epoch: 99 [17152/50048]	Loss: 0.0787
Training Epoch: 99 [17280/50048]	Loss: 0.1008
Training Epoch: 99 [17408/50048]	Loss: 0.0880
Training Epoch: 99 [17536/50048]	Loss: 0.0487
Training Epoch: 99 [17664/50048]	Loss: 0.0478
Training Epoch: 99 [17792/50048]	Loss: 0.1397
Training Epoch: 99 [17920/50048]	Loss: 0.0769
Training Epoch: 99 [18048/50048]	Loss: 0.0611
Training Epoch: 99 [18176/50048]	Loss: 0.0341
Training Epoch: 99 [18304/50048]	Loss: 0.1132
Training Epoch: 99 [18432/50048]	Loss: 0.0532
Training Epoch: 99 [18560/50048]	Loss: 0.0636
Training Epoch: 99 [18688/50048]	Loss: 0.0517
Training Epoch: 99 [18816/50048]	Loss: 0.0499
Training Epoch: 99 [18944/50048]	Loss: 0.0810
Training Epoch: 99 [19072/50048]	Loss: 0.1076
Training Epoch: 99 [19200/50048]	Loss: 0.0636
Training Epoch: 99 [19328/50048]	Loss: 0.0814
Training Epoch: 99 [19456/50048]	Loss: 0.0374
Training Epoch: 99 [19584/50048]	Loss: 0.0703
Training Epoch: 99 [19712/50048]	Loss: 0.0500
Training Epoch: 99 [19840/50048]	Loss: 0.0416
Training Epoch: 99 [19968/50048]	Loss: 0.0772
Training Epoch: 99 [20096/50048]	Loss: 0.0553
Training Epoch: 99 [20224/50048]	Loss: 0.0971
Training Epoch: 99 [20352/50048]	Loss: 0.0573
Training Epoch: 99 [20480/50048]	Loss: 0.1044
Training Epoch: 99 [20608/50048]	Loss: 0.0788
Training Epoch: 99 [20736/50048]	Loss: 0.0677
Training Epoch: 99 [20864/50048]	Loss: 0.0654
Training Epoch: 99 [20992/50048]	Loss: 0.0928
Training Epoch: 99 [21120/50048]	Loss: 0.1058
Training Epoch: 99 [21248/50048]	Loss: 0.0421
Training Epoch: 99 [21376/50048]	Loss: 0.0574
Training Epoch: 99 [21504/50048]	Loss: 0.0463
Training Epoch: 99 [21632/50048]	Loss: 0.0841
Training Epoch: 99 [21760/50048]	Loss: 0.0227
Training Epoch: 99 [21888/50048]	Loss: 0.0549
Training Epoch: 99 [22016/50048]	Loss: 0.0539
Training Epoch: 99 [22144/50048]	Loss: 0.0752
Training Epoch: 99 [22272/50048]	Loss: 0.0724
Training Epoch: 99 [22400/50048]	Loss: 0.0817
Training Epoch: 99 [22528/50048]	Loss: 0.0540
Training Epoch: 99 [22656/50048]	Loss: 0.0552
Training Epoch: 99 [22784/50048]	Loss: 0.0530
Training Epoch: 99 [22912/50048]	Loss: 0.0749
Training Epoch: 99 [23040/50048]	Loss: 0.0738
Training Epoch: 99 [23168/50048]	Loss: 0.0478
Training Epoch: 99 [23296/50048]	Loss: 0.0354
Training Epoch: 99 [23424/50048]	Loss: 0.1004
Training Epoch: 99 [23552/50048]	Loss: 0.0550
Training Epoch: 99 [23680/50048]	Loss: 0.0604
Training Epoch: 99 [23808/50048]	Loss: 0.0556
Training Epoch: 99 [23936/50048]	Loss: 0.0912
Training Epoch: 99 [24064/50048]	Loss: 0.1089
Training Epoch: 99 [24192/50048]	Loss: 0.0648
Training Epoch: 99 [24320/50048]	Loss: 0.0972
Training Epoch: 99 [24448/50048]	Loss: 0.0563
Training Epoch: 99 [24576/50048]	Loss: 0.0493
Training Epoch: 99 [24704/50048]	Loss: 0.0774
Training Epoch: 99 [24832/50048]	Loss: 0.1141
Training Epoch: 99 [24960/50048]	Loss: 0.0983
Training Epoch: 99 [25088/50048]	Loss: 0.0441
Training Epoch: 99 [25216/50048]	Loss: 0.0584
Training Epoch: 99 [25344/50048]	Loss: 0.0770
Training Epoch: 99 [25472/50048]	Loss: 0.0457
Training Epoch: 99 [25600/50048]	Loss: 0.0731
Training Epoch: 99 [25728/50048]	Loss: 0.0438
Training Epoch: 99 [25856/50048]	Loss: 0.0534
Training Epoch: 99 [25984/50048]	Loss: 0.1117
Training Epoch: 99 [26112/50048]	Loss: 0.0277
Training Epoch: 99 [26240/50048]	Loss: 0.1010
Training Epoch: 99 [26368/50048]	Loss: 0.0690
Training Epoch: 99 [26496/50048]	Loss: 0.0603
Training Epoch: 99 [26624/50048]	Loss: 0.0847
Training Epoch: 99 [26752/50048]	Loss: 0.0509
Training Epoch: 99 [26880/50048]	Loss: 0.0400
Training Epoch: 99 [27008/50048]	Loss: 0.0849
Training Epoch: 99 [27136/50048]	Loss: 0.0787
Training Epoch: 99 [27264/50048]	Loss: 0.0370
Training Epoch: 99 [27392/50048]	Loss: 0.0829
Training Epoch: 99 [27520/50048]	Loss: 0.0697
Training Epoch: 99 [27648/50048]	Loss: 0.0682
Training Epoch: 99 [27776/50048]	Loss: 0.0149
Training Epoch: 99 [27904/50048]	Loss: 0.0281
Training Epoch: 99 [28032/50048]	Loss: 0.0516
Training Epoch: 99 [28160/50048]	Loss: 0.0235
Training Epoch: 99 [28288/50048]	Loss: 0.0862
Training Epoch: 99 [28416/50048]	Loss: 0.0833
Training Epoch: 99 [28544/50048]	Loss: 0.1126
Training Epoch: 99 [28672/50048]	Loss: 0.0698
Training Epoch: 99 [28800/50048]	Loss: 0.0342
Training Epoch: 99 [28928/50048]	Loss: 0.0464
Training Epoch: 99 [29056/50048]	Loss: 0.0446
Training Epoch: 99 [29184/50048]	Loss: 0.0268
Training Epoch: 99 [29312/50048]	Loss: 0.0427
Training Epoch: 99 [29440/50048]	Loss: 0.0213
Training Epoch: 99 [29568/50048]	Loss: 0.0536
Training Epoch: 99 [29696/50048]	Loss: 0.1175
Training Epoch: 99 [29824/50048]	Loss: 0.0800
Training Epoch: 99 [29952/50048]	Loss: 0.0801
Training Epoch: 99 [30080/50048]	Loss: 0.0231
Training Epoch: 99 [30208/50048]	Loss: 0.0746
Training Epoch: 99 [30336/50048]	Loss: 0.0885
Training Epoch: 99 [30464/50048]	Loss: 0.0776
Training Epoch: 99 [30592/50048]	Loss: 0.0982
Training Epoch: 99 [30720/50048]	Loss: 0.0475
Training Epoch: 99 [30848/50048]	Loss: 0.0399
Training Epoch: 99 [30976/50048]	Loss: 0.0825
Training Epoch: 99 [31104/50048]	Loss: 0.1202
Training Epoch: 99 [31232/50048]	Loss: 0.1430
Training Epoch: 99 [31360/50048]	Loss: 0.0438
Training Epoch: 99 [31488/50048]	Loss: 0.1372
Training Epoch: 99 [31616/50048]	Loss: 0.1481
Training Epoch: 99 [31744/50048]	Loss: 0.1041
Training Epoch: 99 [31872/50048]	Loss: 0.0918
Training Epoch: 99 [32000/50048]	Loss: 0.0934
Training Epoch: 99 [32128/50048]	Loss: 0.1098
Training Epoch: 99 [32256/50048]	Loss: 0.0616
Training Epoch: 99 [32384/50048]	Loss: 0.0321
Training Epoch: 99 [32512/50048]	Loss: 0.0581
Training Epoch: 99 [32640/50048]	Loss: 0.0968
Training Epoch: 99 [32768/50048]	Loss: 0.0564
Training Epoch: 99 [32896/50048]	Loss: 0.0852
Training Epoch: 99 [33024/50048]	Loss: 0.1335
Training Epoch: 99 [33152/50048]	Loss: 0.0780
Training Epoch: 99 [33280/50048]	Loss: 0.0875
Training Epoch: 99 [33408/50048]	Loss: 0.0316
Training Epoch: 99 [33536/50048]	Loss: 0.0817
Training Epoch: 99 [33664/50048]	Loss: 0.0683
Training Epoch: 99 [33792/50048]	Loss: 0.0395
Training Epoch: 99 [33920/50048]	Loss: 0.1323
Training Epoch: 99 [34048/50048]	Loss: 0.0783
Training Epoch: 99 [34176/50048]	Loss: 0.0747
Training Epoch: 99 [34304/50048]	Loss: 0.0899
Training Epoch: 99 [34432/50048]	Loss: 0.0522
Training Epoch: 99 [34560/50048]	Loss: 0.0863
Training Epoch: 99 [34688/50048]	Loss: 0.0978
Training Epoch: 99 [34816/50048]	Loss: 0.1006
Training Epoch: 99 [34944/50048]	Loss: 0.0820
Training Epoch: 99 [35072/50048]	Loss: 0.1028
Training Epoch: 99 [35200/50048]	Loss: 0.0586
Training Epoch: 99 [35328/50048]	Loss: 0.1143
Training Epoch: 99 [35456/50048]	Loss: 0.0911
Training Epoch: 99 [35584/50048]	Loss: 0.0150
Training Epoch: 99 [35712/50048]	Loss: 0.1043
Training Epoch: 99 [35840/50048]	Loss: 0.0979
Training Epoch: 99 [35968/50048]	Loss: 0.0949
Training Epoch: 99 [36096/50048]	Loss: 0.0874
Training Epoch: 99 [36224/50048]	Loss: 0.1463
Training Epoch: 99 [36352/50048]	Loss: 0.1029
Training Epoch: 99 [36480/50048]	Loss: 0.1353
Training Epoch: 99 [36608/50048]	Loss: 0.0533
Training Epoch: 99 [36736/50048]	Loss: 0.1354
Training Epoch: 99 [36864/50048]	Loss: 0.0664
Training Epoch: 99 [36992/50048]	Loss: 0.0502
Training Epoch: 99 [37120/50048]	Loss: 0.0381
Training Epoch: 99 [37248/50048]	Loss: 0.0567
Training Epoch: 99 [37376/50048]	Loss: 0.0775
Training Epoch: 99 [37504/50048]	Loss: 0.1057
Training Epoch: 99 [37632/50048]	Loss: 0.1155
Training Epoch: 99 [37760/50048]	Loss: 0.0667
Training Epoch: 99 [37888/50048]	Loss: 0.0541
Training Epoch: 99 [38016/50048]	Loss: 0.0856
Training Epoch: 99 [38144/50048]	Loss: 0.0685
Training Epoch: 99 [38272/50048]	Loss: 0.0420
Training Epoch: 99 [38400/50048]	Loss: 0.0866
Training Epoch: 99 [38528/50048]	Loss: 0.1390
Training Epoch: 99 [38656/50048]	Loss: 0.0745
Training Epoch: 99 [38784/50048]	Loss: 0.0500
Training Epoch: 99 [38912/50048]	Loss: 0.0477
Training Epoch: 99 [39040/50048]	Loss: 0.0643
Training Epoch: 99 [39168/50048]	Loss: 0.1242
Training Epoch: 99 [39296/50048]	Loss: 0.0872
Training Epoch: 99 [39424/50048]	Loss: 0.0717
Training Epoch: 99 [39552/50048]	Loss: 0.0909
Training Epoch: 99 [39680/50048]	Loss: 0.0937
Training Epoch: 99 [39808/50048]	Loss: 0.0794
Training Epoch: 99 [39936/50048]	Loss: 0.0926
Training Epoch: 99 [40064/50048]	Loss: 0.0528
Training Epoch: 99 [40192/50048]	Loss: 0.0601
Training Epoch: 99 [40320/50048]	Loss: 0.0951
Training Epoch: 99 [40448/50048]	Loss: 0.0159
Training Epoch: 99 [40576/50048]	Loss: 0.0628
Training Epoch: 99 [40704/50048]	Loss: 0.0626
Training Epoch: 99 [40832/50048]	Loss: 0.0610
Training Epoch: 99 [40960/50048]	Loss: 0.0676
Training Epoch: 99 [41088/50048]	Loss: 0.0568
Training Epoch: 99 [41216/50048]	Loss: 0.0329
Training Epoch: 99 [41344/50048]	Loss: 0.0925
Training Epoch: 99 [41472/50048]	Loss: 0.0468
Training Epoch: 99 [41600/50048]	Loss: 0.0334
Training Epoch: 99 [41728/50048]	Loss: 0.0278
Training Epoch: 99 [41856/50048]	Loss: 0.1159
Training Epoch: 99 [41984/50048]	Loss: 0.0314
Training Epoch: 99 [42112/50048]	Loss: 0.0549
Training Epoch: 99 [42240/50048]	Loss: 0.0520
Training Epoch: 99 [42368/50048]	Loss: 0.0446
Training Epoch: 99 [42496/50048]	Loss: 0.0842
Training Epoch: 99 [42624/50048]	Loss: 0.0445
Training Epoch: 99 [42752/50048]	Loss: 0.0531
Training Epoch: 99 [42880/50048]	Loss: 0.0903
Training Epoch: 99 [43008/50048]	Loss: 0.0612
Training Epoch: 99 [43136/50048]	Loss: 0.0839
Training Epoch: 99 [43264/50048]	Loss: 0.1295
Training Epoch: 99 [43392/50048]	Loss: 0.0527
Training Epoch: 99 [43520/50048]	Loss: 0.0723
Training Epoch: 99 [43648/50048]	Loss: 0.0509
Training Epoch: 99 [43776/50048]	Loss: 0.0663
Training Epoch: 99 [43904/50048]	Loss: 0.0653
Training Epoch: 99 [44032/50048]	Loss: 0.0516
Training Epoch: 99 [44160/50048]	Loss: 0.1079
Training Epoch: 99 [44288/50048]	Loss: 0.0561
Training Epoch: 99 [44416/50048]	Loss: 0.1594
Training Epoch: 99 [44544/50048]	Loss: 0.0813
Training Epoch: 99 [44672/50048]	Loss: 0.0699
Training Epoch: 99 [44800/50048]	Loss: 0.0416
Training Epoch: 99 [44928/50048]	Loss: 0.0415
Training Epoch: 99 [45056/50048]	Loss: 0.0414
Training Epoch: 99 [45184/50048]	Loss: 0.1038
Training Epoch: 99 [45312/50048]	Loss: 0.1038
Training Epoch: 99 [45440/50048]	Loss: 0.1140
Training Epoch: 99 [45568/50048]	Loss: 0.0813
Training Epoch: 99 [45696/50048]	Loss: 0.0274
2022-12-06 06:07:42,265 [ZeusDataLoader(train)] train epoch 100 done: time=86.56 energy=10498.73
2022-12-06 06:07:42,266 [ZeusDataLoader(eval)] Epoch 100 begin.
Training Epoch: 99 [45824/50048]	Loss: 0.0822
Training Epoch: 99 [45952/50048]	Loss: 0.0389
Training Epoch: 99 [46080/50048]	Loss: 0.0700
Training Epoch: 99 [46208/50048]	Loss: 0.0760
Training Epoch: 99 [46336/50048]	Loss: 0.0229
Training Epoch: 99 [46464/50048]	Loss: 0.0461
Training Epoch: 99 [46592/50048]	Loss: 0.0358
Training Epoch: 99 [46720/50048]	Loss: 0.0444
Training Epoch: 99 [46848/50048]	Loss: 0.0430
Training Epoch: 99 [46976/50048]	Loss: 0.0423
Training Epoch: 99 [47104/50048]	Loss: 0.1239
Training Epoch: 99 [47232/50048]	Loss: 0.0880
Training Epoch: 99 [47360/50048]	Loss: 0.0748
Training Epoch: 99 [47488/50048]	Loss: 0.0707
Training Epoch: 99 [47616/50048]	Loss: 0.0717
Training Epoch: 99 [47744/50048]	Loss: 0.0409
Training Epoch: 99 [47872/50048]	Loss: 0.0155
Training Epoch: 99 [48000/50048]	Loss: 0.1231
Training Epoch: 99 [48128/50048]	Loss: 0.0732
Training Epoch: 99 [48256/50048]	Loss: 0.0989
Training Epoch: 99 [48384/50048]	Loss: 0.1139
Training Epoch: 99 [48512/50048]	Loss: 0.1375
Training Epoch: 99 [48640/50048]	Loss: 0.0194
Training Epoch: 99 [48768/50048]	Loss: 0.0961
Training Epoch: 99 [48896/50048]	Loss: 0.0423
Training Epoch: 99 [49024/50048]	Loss: 0.0784
Training Epoch: 99 [49152/50048]	Loss: 0.0426
Training Epoch: 99 [49280/50048]	Loss: 0.0648
Training Epoch: 99 [49408/50048]	Loss: 0.1260
Training Epoch: 99 [49536/50048]	Loss: 0.1267
Training Epoch: 99 [49664/50048]	Loss: 0.1371
Training Epoch: 99 [49792/50048]	Loss: 0.1206
Training Epoch: 99 [49920/50048]	Loss: 0.0401
Training Epoch: 99 [50048/50048]	Loss: 0.0912
2022-12-06 11:07:45.940 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:07:45,975 [ZeusDataLoader(eval)] eval epoch 100 done: time=3.70 energy=453.33
2022-12-06 06:07:45,976 [ZeusDataLoader(train)] Up to epoch 100: time=9021.52, energy=1095198.16, cost=1336981.71
2022-12-06 06:07:45,976 [ZeusDataLoader(train)] Maximum number of epochs 100 reached. Stopping.
2022-12-06 06:07:45,976 [ZeusDataLoader(train)] Training done.
2022-12-06 06:07:45,976 [ZeusDataLoader(train)] Saved /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/rec00+try01+bs128+lr0.0100000.train.json: {"energy": 1095198.162072911, "time": 9021.515720660003, "cost": 1336981.706594206, "num_epochs": 100, "reached": false}
Validation Epoch: 99, Average loss: 0.0186, Accuracy: 0.6429

[run job] Job terminated with exit code 0.
[run job] stats={'energy': 1095198.162072911, 'time': 9021.515720660003, 'cost': 1336981.706594206, 'num_epochs': 100, 'reached': False}
[Zeus Master] cost=1336981.706594206
[run job] Launching job with BS 128: and LR: 0.01
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835', 'ZEUS_JOB_ID': 'rec00+try02', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_TARGET_METRIC': '0.8', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_USE_OPTIMAL_PL': 'True'}
[run job] cwd=/workspace/zeus/examples/cifar100
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '0.01']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/rec00+try02.train.log'
2022-12-06 06:07:51,517 [ZeusDataLoader(train)] Distributed data parallel: OFF
2022-12-06 06:07:51,558 [ZeusDataLoader(train)] Power limit range: [175000, 150000, 125000, 100000]
2022-12-06 06:07:51,558 [ZeusDataLoader(train)] Power profiling: OFF
2022-12-06 06:07:51,559 [ZeusDataLoader(train)] Loaded /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+lr0.0100000.power.json: {'job_id': 'rec00+try01', 'train_power': {'175000': 120.25616099179643, '150000': 120.91003189146555, '125000': 121.11901311576284, '100000': 98.69303482927677}, 'train_throughput': {'175000': 4.544606647863609, '150000': 4.5365038969654385, '125000': 4.536710151601713, '100000': 3.9448251884904706}, 'eval_power': {'175000': 120.03848402041383}, 'eval_throughput': {'175000': 20.996480248389204}, 'optimal_pl': 175000}
2022-12-06 06:07:51,559 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 06:07:51,562 [ZeusDataLoader(train)] [GPU_0] Set GPU power limit to 175W.
2022-12-06 06:07:53,604 [ZeusDataLoader(train)] Up to epoch 0: time=0.00, energy=0.00, cost=0.00
2022-12-06 06:07:53,605 [ZeusDataLoader(train)] Epoch 1 begin.
Files already downloaded and verified
Files already downloaded and verified
2022-12-06 06:07:53,777 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:07:53,778 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:07:53.780 [ZeusMonitor] Monitor started.
2022-12-06 11:07:53.780 [ZeusMonitor] Running indefinitely. 2022-12-06 11:07:53.780 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:07:53.780 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e1+gpu0.power.log
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 4.8435
Training Epoch: 0 [384/50048]	Loss: 4.9283
Training Epoch: 0 [512/50048]	Loss: 4.7351
Training Epoch: 0 [640/50048]	Loss: 4.8986
Training Epoch: 0 [768/50048]	Loss: 4.8171
Training Epoch: 0 [896/50048]	Loss: 4.9262
Training Epoch: 0 [1024/50048]	Loss: 4.7815
Training Epoch: 0 [1152/50048]	Loss: 4.9027
Training Epoch: 0 [1280/50048]	Loss: 4.7896
Training Epoch: 0 [1408/50048]	Loss: 4.7568
Training Epoch: 0 [1536/50048]	Loss: 4.6684
Training Epoch: 0 [1664/50048]	Loss: 4.9823
Training Epoch: 0 [1792/50048]	Loss: 4.7214
Training Epoch: 0 [1920/50048]	Loss: 4.8467
Training Epoch: 0 [2048/50048]	Loss: 4.6727
Training Epoch: 0 [2176/50048]	Loss: 4.8962
Training Epoch: 0 [2304/50048]	Loss: 4.8131
Training Epoch: 0 [2432/50048]	Loss: 4.6532
Training Epoch: 0 [2560/50048]	Loss: 4.6480
Training Epoch: 0 [2688/50048]	Loss: 4.7268
Training Epoch: 0 [2816/50048]	Loss: 4.6176
Training Epoch: 0 [2944/50048]	Loss: 4.5794
Training Epoch: 0 [3072/50048]	Loss: 4.6568
Training Epoch: 0 [3200/50048]	Loss: 4.7401
Training Epoch: 0 [3328/50048]	Loss: 4.6172
Training Epoch: 0 [3456/50048]	Loss: 4.4904
Training Epoch: 0 [3584/50048]	Loss: 4.5330
Training Epoch: 0 [3712/50048]	Loss: 4.5995
Training Epoch: 0 [3840/50048]	Loss: 4.4687
Training Epoch: 0 [3968/50048]	Loss: 4.6066
Training Epoch: 0 [4096/50048]	Loss: 4.7523
Training Epoch: 0 [4224/50048]	Loss: 4.5932
Training Epoch: 0 [4352/50048]	Loss: 4.6002
Training Epoch: 0 [4480/50048]	Loss: 4.4447
Training Epoch: 0 [4608/50048]	Loss: 4.4578
Training Epoch: 0 [4736/50048]	Loss: 4.4525
Training Epoch: 0 [4864/50048]	Loss: 4.4328
Training Epoch: 0 [4992/50048]	Loss: 4.5564
Training Epoch: 0 [5120/50048]	Loss: 4.4867
Training Epoch: 0 [5248/50048]	Loss: 4.5116
Training Epoch: 0 [5376/50048]	Loss: 4.3885
Training Epoch: 0 [5504/50048]	Loss: 4.3619
Training Epoch: 0 [5632/50048]	Loss: 4.4317
Training Epoch: 0 [5760/50048]	Loss: 4.4848
Training Epoch: 0 [5888/50048]	Loss: 4.3467
Training Epoch: 0 [6016/50048]	Loss: 4.2336
Training Epoch: 0 [6144/50048]	Loss: 4.1125
Training Epoch: 0 [6272/50048]	Loss: 4.5456
Training Epoch: 0 [6400/50048]	Loss: 4.2772
Training Epoch: 0 [6528/50048]	Loss: 4.2275
Training Epoch: 0 [6656/50048]	Loss: 4.4406
Training Epoch: 0 [6784/50048]	Loss: 4.3294
Training Epoch: 0 [6912/50048]	Loss: 4.3872
Training Epoch: 0 [7040/50048]	Loss: 4.2378
Training Epoch: 0 [7168/50048]	Loss: 4.2486
Training Epoch: 0 [7296/50048]	Loss: 4.3621
Training Epoch: 0 [7424/50048]	Loss: 4.2107
Training Epoch: 0 [7552/50048]	Loss: 4.2323
Training Epoch: 0 [7680/50048]	Loss: 4.3381
Training Epoch: 0 [7808/50048]	Loss: 4.3464
Training Epoch: 0 [7936/50048]	Loss: 4.2753
Training Epoch: 0 [8064/50048]	Loss: 4.1283
Training Epoch: 0 [8192/50048]	Loss: 4.2315
Training Epoch: 0 [8320/50048]	Loss: 4.3313
Training Epoch: 0 [8448/50048]	Loss: 4.1641
Training Epoch: 0 [8576/50048]	Loss: 4.0557
Training Epoch: 0 [8704/50048]	Loss: 4.1873
Training Epoch: 0 [8832/50048]	Loss: 4.0826
Training Epoch: 0 [8960/50048]	Loss: 4.2131
Training Epoch: 0 [9088/50048]	Loss: 4.1553
Training Epoch: 0 [9216/50048]	Loss: 4.2327
Training Epoch: 0 [9344/50048]	Loss: 4.0953
Training Epoch: 0 [9472/50048]	Loss: 4.1133
Training Epoch: 0 [9600/50048]	Loss: 4.0928
Training Epoch: 0 [9728/50048]	Loss: 4.2926
Training Epoch: 0 [9856/50048]	Loss: 4.1715
Training Epoch: 0 [9984/50048]	Loss: 4.2347
Training Epoch: 0 [10112/50048]	Loss: 4.2087
Training Epoch: 0 [10240/50048]	Loss: 4.2610
Training Epoch: 0 [10368/50048]	Loss: 4.3016
Training Epoch: 0 [10496/50048]	Loss: 4.1242
Training Epoch: 0 [10624/50048]	Loss: 4.1411
Training Epoch: 0 [10752/50048]	Loss: 4.0219
Training Epoch: 0 [10880/50048]	Loss: 4.1079
Training Epoch: 0 [11008/50048]	Loss: 3.9403
Training Epoch: 0 [11136/50048]	Loss: 4.1751
Training Epoch: 0 [11264/50048]	Loss: 4.1584
Training Epoch: 0 [11392/50048]	Loss: 4.0695
Training Epoch: 0 [11520/50048]	Loss: 3.9941
Training Epoch: 0 [11648/50048]	Loss: 4.1099
Training Epoch: 0 [11776/50048]	Loss: 3.9957
Training Epoch: 0 [11904/50048]	Loss: 4.1240
Training Epoch: 0 [12032/50048]	Loss: 4.0888
Training Epoch: 0 [12160/50048]	Loss: 4.1210
Training Epoch: 0 [12288/50048]	Loss: 3.9184
Training Epoch: 0 [12416/50048]	Loss: 4.1639
Training Epoch: 0 [12544/50048]	Loss: 3.9947
Training Epoch: 0 [12672/50048]	Loss: 4.0970
Training Epoch: 0 [12800/50048]	Loss: 3.9784
Training Epoch: 0 [12928/50048]	Loss: 3.9857
Training Epoch: 0 [13056/50048]	Loss: 3.9591
Training Epoch: 0 [13184/50048]	Loss: 4.1693
Training Epoch: 0 [13312/50048]	Loss: 4.1310
Training Epoch: 0 [13440/50048]	Loss: 4.0898
Training Epoch: 0 [13568/50048]	Loss: 4.0929
Training Epoch: 0 [13696/50048]	Loss: 3.7934
Training Epoch: 0 [13824/50048]	Loss: 4.1235
Training Epoch: 0 [13952/50048]	Loss: 4.0421
Training Epoch: 0 [14080/50048]	Loss: 3.9439
Training Epoch: 0 [14208/50048]	Loss: 4.1974
Training Epoch: 0 [14336/50048]	Loss: 4.0681
Training Epoch: 0 [14464/50048]	Loss: 4.0795
Training Epoch: 0 [14592/50048]	Loss: 4.0191
Training Epoch: 0 [14720/50048]	Loss: 3.9683
Training Epoch: 0 [14848/50048]	Loss: 3.7857
Training Epoch: 0 [14976/50048]	Loss: 4.0443
Training Epoch: 0 [15104/50048]	Loss: 3.9425
Training Epoch: 0 [15232/50048]	Loss: 4.1083
Training Epoch: 0 [15360/50048]	Loss: 4.2403
Training Epoch: 0 [15488/50048]	Loss: 3.9725
Training Epoch: 0 [15616/50048]	Loss: 4.0140
Training Epoch: 0 [15744/50048]	Loss: 4.0167
Training Epoch: 0 [15872/50048]	Loss: 4.1357
Training Epoch: 0 [16000/50048]	Loss: 3.9566
Training Epoch: 0 [16128/50048]	Loss: 3.9662
Training Epoch: 0 [16256/50048]	Loss: 3.9070
Training Epoch: 0 [16384/50048]	Loss: 3.9029
Training Epoch: 0 [16512/50048]	Loss: 4.0755
Training Epoch: 0 [16640/50048]	Loss: 3.9541
Training Epoch: 0 [16768/50048]	Loss: 3.7645
Training Epoch: 0 [16896/50048]	Loss: 4.1099
Training Epoch: 0 [17024/50048]	Loss: 3.9485
Training Epoch: 0 [17152/50048]	Loss: 3.9096
Training Epoch: 0 [17280/50048]	Loss: 4.1518
Training Epoch: 0 [17408/50048]	Loss: 4.0507
Training Epoch: 0 [17536/50048]	Loss: 3.9992
Training Epoch: 0 [17664/50048]	Loss: 3.8803
Training Epoch: 0 [17792/50048]	Loss: 3.9628
Training Epoch: 0 [17920/50048]	Loss: 4.1064
Training Epoch: 0 [18048/50048]	Loss: 3.9352
Training Epoch: 0 [18176/50048]	Loss: 3.9058
Training Epoch: 0 [18304/50048]	Loss: 4.1575
Training Epoch: 0 [18432/50048]	Loss: 3.9811
Training Epoch: 0 [18560/50048]	Loss: 3.8889
Training Epoch: 0 [18688/50048]	Loss: 3.9039
Training Epoch: 0 [18816/50048]	Loss: 3.9709
Training Epoch: 0 [18944/50048]	Loss: 3.9212
Training Epoch: 0 [19072/50048]	Loss: 4.1569
Training Epoch: 0 [19200/50048]	Loss: 3.9446
Training Epoch: 0 [19328/50048]	Loss: 3.9113
Training Epoch: 0 [19456/50048]	Loss: 3.7852
Training Epoch: 0 [19584/50048]	Loss: 3.8991
Training Epoch: 0 [19712/50048]	Loss: 3.9205
Training Epoch: 0 [19840/50048]	Loss: 4.0334
Training Epoch: 0 [19968/50048]	Loss: 4.1576
Training Epoch: 0 [20096/50048]	Loss: 3.8147
Training Epoch: 0 [20224/50048]	Loss: 3.8238
Training Epoch: 0 [20352/50048]	Loss: 3.7772
Training Epoch: 0 [20480/50048]	Loss: 4.0306
Training Epoch: 0 [20608/50048]	Loss: 3.7993
Training Epoch: 0 [20736/50048]	Loss: 3.8187
Training Epoch: 0 [20864/50048]	Loss: 3.7785
Training Epoch: 0 [20992/50048]	Loss: 3.8619
Training Epoch: 0 [21120/50048]	Loss: 4.0387
Training Epoch: 0 [21248/50048]	Loss: 4.1168
Training Epoch: 0 [21376/50048]	Loss: 4.0114
Training Epoch: 0 [21504/50048]	Loss: 3.7902
Training Epoch: 0 [21632/50048]	Loss: 3.8896
Training Epoch: 0 [21760/50048]	Loss: 3.9474
Training Epoch: 0 [21888/50048]	Loss: 3.9167
Training Epoch: 0 [22016/50048]	Loss: 3.9826
Training Epoch: 0 [22144/50048]	Loss: 4.0014
Training Epoch: 0 [22272/50048]	Loss: 3.7477
Training Epoch: 0 [22400/50048]	Loss: 3.9960
Training Epoch: 0 [22528/50048]	Loss: 3.9236
Training Epoch: 0 [22656/50048]	Loss: 3.7459
Training Epoch: 0 [22784/50048]	Loss: 3.8340
Training Epoch: 0 [22912/50048]	Loss: 3.7709
Training Epoch: 0 [23040/50048]	Loss: 3.8079
Training Epoch: 0 [23168/50048]	Loss: 3.8880
Training Epoch: 0 [23296/50048]	Loss: 3.9069
Training Epoch: 0 [23424/50048]	Loss: 3.9740
Training Epoch: 0 [23552/50048]	Loss: 3.8912
Training Epoch: 0 [23680/50048]	Loss: 3.8985
Training Epoch: 0 [23808/50048]	Loss: 3.8115
Training Epoch: 0 [23936/50048]	Loss: 3.9379
Training Epoch: 0 [24064/50048]	Loss: 3.9210
Training Epoch: 0 [24192/50048]	Loss: 3.7625
Training Epoch: 0 [24320/50048]	Loss: 3.8428
Training Epoch: 0 [24448/50048]	Loss: 3.6940
Training Epoch: 0 [24576/50048]	Loss: 4.0571
Training Epoch: 0 [24704/50048]	Loss: 3.9917
Training Epoch: 0 [24832/50048]	Loss: 4.0170
Training Epoch: 0 [24960/50048]	Loss: 3.8420
Training Epoch: 0 [25088/50048]	Loss: 3.7397
Training Epoch: 0 [25216/50048]	Loss: 3.8002
Training Epoch: 0 [25344/50048]	Loss: 3.8767
Training Epoch: 0 [25472/50048]	Loss: 3.7898
Training Epoch: 0 [25600/50048]	Loss: 3.8874
Training Epoch: 0 [25728/50048]	Loss: 3.8200
Training Epoch: 0 [25856/50048]	Loss: 3.7894
Training Epoch: 0 [25984/50048]	Loss: 3.7460
Training Epoch: 0 [26112/50048]	Loss: 3.7934
Training Epoch: 0 [26240/50048]	Loss: 4.0132
Training Epoch: 0 [26368/50048]	Loss: 3.8442
Training Epoch: 0 [26496/50048]	Loss: 3.8595
Training Epoch: 0 [26624/50048]	Loss: 3.8101
Training Epoch: 0 [26752/50048]	Loss: 3.9256
Training Epoch: 0 [26880/50048]	Loss: 3.7292
Training Epoch: 0 [27008/50048]	Loss: 3.7269
Training Epoch: 0 [27136/50048]	Loss: 3.7830
Training Epoch: 0 [27264/50048]	Loss: 3.9379
Training Epoch: 0 [27392/50048]	Loss: 3.9793
Training Epoch: 0 [27520/50048]	Loss: 3.8619
Training Epoch: 0 [27648/50048]	Loss: 3.7685
Training Epoch: 0 [27776/50048]	Loss: 3.8649
Training Epoch: 0 [27904/50048]	Loss: 4.0007
Training Epoch: 0 [28032/50048]	Loss: 3.7917
Training Epoch: 0 [28160/50048]	Loss: 3.8768
Training Epoch: 0 [28288/50048]	Loss: 3.7316
Training Epoch: 0 [28416/50048]	Loss: 3.6639
Training Epoch: 0 [28544/50048]	Loss: 3.7689
Training Epoch: 0 [28672/50048]	Loss: 3.7074
Training Epoch: 0 [28800/50048]	Loss: 3.8386
Training Epoch: 0 [28928/50048]	Loss: 3.8151
Training Epoch: 0 [29056/50048]	Loss: 3.8865
Training Epoch: 0 [29184/50048]	Loss: 3.8782
Training Epoch: 0 [29312/50048]	Loss: 3.6557
Training Epoch: 0 [29440/50048]	Loss: 3.8991
Training Epoch: 0 [29568/50048]	Loss: 3.8570
Training Epoch: 0 [29696/50048]	Loss: 3.8246
Training Epoch: 0 [29824/50048]	Loss: 3.7948
Training Epoch: 0 [29952/50048]	Loss: 3.8744
Training Epoch: 0 [30080/50048]	Loss: 3.8449
Training Epoch: 0 [30208/50048]	Loss: 4.0852
Training Epoch: 0 [30336/50048]	Loss: 3.9063
Training Epoch: 0 [30464/50048]	Loss: 3.9340
Training Epoch: 0 [30592/50048]	Loss: 3.7820
Training Epoch: 0 [30720/50048]	Loss: 3.6726
Training Epoch: 0 [30848/50048]	Loss: 3.9612
Training Epoch: 0 [30976/50048]	Loss: 3.7044
Training Epoch: 0 [31104/50048]	Loss: 3.8348
Training Epoch: 0 [31232/50048]	Loss: 3.9250
Training Epoch: 0 [31360/50048]	Loss: 3.7133
Training Epoch: 0 [31488/50048]	Loss: 3.9604
Training Epoch: 0 [31616/50048]	Loss: 3.8117
Training Epoch: 0 [31744/50048]	Loss: 3.8578
Training Epoch: 0 [31872/50048]	Loss: 3.7901
Training Epoch: 0 [32000/50048]	Loss: 3.9646
Training Epoch: 0 [32128/50048]	Loss: 3.4867
Training Epoch: 0 [32256/50048]	Loss: 3.8662
Training Epoch: 0 [32384/50048]	Loss: 3.6047
Training Epoch: 0 [32512/50048]	Loss: 3.4757
Training Epoch: 0 [32640/50048]	Loss: 3.8333
Training Epoch: 0 [32768/50048]	Loss: 3.6788
Training Epoch: 0 [32896/50048]	Loss: 3.8140
Training Epoch: 0 [33024/50048]	Loss: 3.8792
Training Epoch: 0 [33152/50048]	Loss: 3.8272
Training Epoch: 0 [33280/50048]	Loss: 3.6840
Training Epoch: 0 [33408/50048]	Loss: 3.7148
Training Epoch: 0 [33536/50048]	Loss: 3.8549
Training Epoch: 0 [33664/50048]	Loss: 3.6558
Training Epoch: 0 [33792/50048]	Loss: 3.5590
Training Epoch: 0 [33920/50048]	Loss: 3.5923
Training Epoch: 0 [34048/50048]	Loss: 3.7970
Training Epoch: 0 [34176/50048]	Loss: 3.8932
Training Epoch: 0 [34304/50048]	Loss: 3.6642
Training Epoch: 0 [34432/50048]	Loss: 3.8712
Training Epoch: 0 [34560/50048]	Loss: 3.7735
Training Epoch: 0 [34688/50048]	Loss: 3.8960
Training Epoch: 0 [34816/50048]	Loss: 3.9268
Training Epoch: 0 [34944/50048]	Loss: 3.7407
Training Epoch: 0 [35072/50048]	Loss: 3.6925
Training Epoch: 0 [35200/50048]	Loss: 3.5559
Training Epoch: 0 [35328/50048]	Loss: 3.6902
Training Epoch: 0 [35456/50048]	Loss: 3.6633
Training Epoch: 0 [35584/50048]	Loss: 3.8042
Training Epoch: 0 [35712/50048]	Loss: 3.7439
Training Epoch: 0 [35840/50048]	Loss: 3.8217
Training Epoch: 0 [35968/50048]	Loss: 3.7735
Training Epoch: 0 [36096/50048]	Loss: 3.6618
Training Epoch: 0 [36224/50048]	Loss: 3.6330
Training Epoch: 0 [36352/50048]	Loss: 3.5366
Training Epoch: 0 [36480/50048]	Loss: 3.6723
Training Epoch: 0 [36608/50048]	Loss: 3.8701
Training Epoch: 0 [36736/50048]	Loss: 3.7682
Training Epoch: 0 [36864/50048]	Loss: 3.6468
Training Epoch: 0 [36992/50048]	Loss: 3.6748
Training Epoch: 0 [37120/50048]	Loss: 3.7357
Training Epoch: 0 [37248/50048]	Loss: 3.5291
Training Epoch: 0 [37376/50048]	Loss: 3.7941
Training Epoch: 0 [37504/50048]	Loss: 3.8435
Training Epoch: 0 [37632/50048]	Loss: 3.7480
Training Epoch: 0 [37760/50048]	Loss: 3.5822
Training Epoch: 0 [37888/50048]	Loss: 3.9237
Training Epoch: 0 [38016/50048]	Loss: 3.7305
Training Epoch: 0 [38144/50048]	Loss: 3.7551
Training Epoch: 0 [38272/50048]	Loss: 3.7759
Training Epoch: 0 [38400/50048]	Loss: 3.7180
Training Epoch: 0 [38528/50048]	Loss: 3.7927
Training Epoch: 0 [38656/50048]	Loss: 3.5330
Training Epoch: 0 [38784/50048]	Loss: 3.5488
Training Epoch: 0 [38912/50048]	Loss: 3.6594
Training Epoch: 0 [39040/50048]	Loss: 3.6630
Training Epoch: 0 [39168/50048]	Loss: 3.8314
Training Epoch: 0 [39296/50048]	Loss: 3.7490
Training Epoch: 0 [39424/50048]	Loss: 3.6146
Training Epoch: 0 [39552/50048]	Loss: 3.4339
Training Epoch: 0 [39680/50048]	Loss: 3.6586
Training Epoch: 0 [39808/50048]	Loss: 3.6558
Training Epoch: 0 [39936/50048]	Loss: 3.8919
Training Epoch: 0 [40064/50048]	Loss: 3.5712
Training Epoch: 0 [40192/50048]	Loss: 3.5509
Training Epoch: 0 [40320/50048]	Loss: 3.6250
Training Epoch: 0 [40448/50048]	Loss: 3.7558
Training Epoch: 0 [40576/50048]	Loss: 3.6837
Training Epoch: 0 [40704/50048]	Loss: 3.5937
Training Epoch: 0 [40832/50048]	Loss: 3.6388
Training Epoch: 0 [40960/50048]	Loss: 3.7485
Training Epoch: 0 [41088/50048]	Loss: 3.6632
Training Epoch: 0 [41216/50048]	Loss: 3.5463
Training Epoch: 0 [41344/50048]	Loss: 3.9236
Training Epoch: 0 [41472/50048]	Loss: 3.6734
Training Epoch: 0 [41600/50048]	Loss: 3.4620
Training Epoch: 0 [41728/50048]	Loss: 3.5094
Training Epoch: 0 [41856/50048]	Loss: 3.6740
Training Epoch: 0 [41984/50048]	Loss: 3.7935
Training Epoch: 0 [42112/50048]	Loss: 3.6838
Training Epoch: 0 [42240/50048]	Loss: 3.7842
Training Epoch: 0 [42368/50048]	Loss: 3.6628
Training Epoch: 0 [42496/50048]	Loss: 3.9919
Training Epoch: 0 [42624/50048]	Loss: 3.7485
Training Epoch: 0 [42752/50048]	Loss: 3.7716
Training Epoch: 0 [42880/50048]	Loss: 3.7454
Training Epoch: 0 [43008/50048]	Loss: 3.6068
Training Epoch: 0 [43136/50048]	Loss: 3.4906
Training Epoch: 0 [43264/50048]	Loss: 3.5792
Training Epoch: 0 [43392/50048]	Loss: 3.5408
Training Epoch: 0 [43520/50048]	Loss: 3.7257
Training Epoch: 0 [43648/50048]	Loss: 3.4029
Training Epoch: 0 [43776/50048]	Loss: 3.5262
Training Epoch: 0 [43904/50048]	Loss: 3.7457
Training Epoch: 0 [44032/50048]	Loss: 3.5027
Training Epoch: 0 [44160/50048]	Loss: 3.6766
Training Epoch: 0 [44288/50048]	Loss: 3.5075
Training Epoch: 0 [44416/50048]	Loss: 3.6598
Training Epoch: 0 [44544/50048]	Loss: 3.5626
Training Epoch: 0 [44672/50048]	Loss: 3.8658
Training Epoch: 0 [44800/50048]	Loss: 3.5401
Training Epoch: 0 [44928/50048]	Loss: 3.5320
Training Epoch: 0 [45056/50048]	Loss: 3.6082
Training Epoch: 0 [45184/50048]	Loss: 3.9620
Training Epoch: 0 [45312/50048]	Loss: 3.7442
Training Epoch: 0 [45440/50048]	Loss: 3.5387
Training Epoch: 0 [45568/50048]	Loss: 3.5692
Training Epoch: 0 [45696/50048]	Loss: 3.4306
Training Epoch: 0 [45824/50048]	Loss: 3.7494
Training Epoch: 0 [45952/50048]	Loss: 3.6074
Training Epoch: 0 [46080/50048]	Loss: 3.6302
Training Epoch: 0 [46208/50048]	Loss: 3.6716
Training Epoch: 0 [46336/50048]	Loss: 3.5270
Training Epoch: 0 [46464/50048]	Loss: 3.4473
Training Epoch: 0 [46592/50048]	Loss: 3.7687
Training Epoch: 0 [46720/50048]	Loss: 3.6150
2022-12-06 06:09:20,477 [ZeusDataLoader(train)] train epoch 1 done: time=86.86 energy=10503.16
2022-12-06 06:09:20,478 [ZeusDataLoader(eval)] Epoch 1 begin.
Training Epoch: 0 [46848/50048]	Loss: 3.6934
Training Epoch: 0 [46976/50048]	Loss: 3.7908
Training Epoch: 0 [47104/50048]	Loss: 3.5694
Training Epoch: 0 [47232/50048]	Loss: 3.4557
Training Epoch: 0 [47360/50048]	Loss: 3.3087
Training Epoch: 0 [47488/50048]	Loss: 3.5413
Training Epoch: 0 [47616/50048]	Loss: 3.6002
Training Epoch: 0 [47744/50048]	Loss: 3.7643
Training Epoch: 0 [47872/50048]	Loss: 3.6501
Training Epoch: 0 [48000/50048]	Loss: 3.6779
Training Epoch: 0 [48128/50048]	Loss: 3.8043
Training Epoch: 0 [48256/50048]	Loss: 3.6096
Training Epoch: 0 [48384/50048]	Loss: 3.6554
Training Epoch: 0 [48512/50048]	Loss: 3.6709
Training Epoch: 0 [48640/50048]	Loss: 3.6021
Training Epoch: 0 [48768/50048]	Loss: 3.7280
Training Epoch: 0 [48896/50048]	Loss: 3.5881
Training Epoch: 0 [49024/50048]	Loss: 3.6464
Training Epoch: 0 [49152/50048]	Loss: 3.6090
Training Epoch: 0 [49280/50048]	Loss: 3.4415
Training Epoch: 0 [49408/50048]	Loss: 3.3537
Training Epoch: 0 [49536/50048]	Loss: 3.6009
Training Epoch: 0 [49664/50048]	Loss: 3.5023
Training Epoch: 0 [49792/50048]	Loss: 3.5865
Training Epoch: 0 [49920/50048]	Loss: 3.6121
Training Epoch: 0 [50048/50048]	Loss: 3.6872
2022-12-06 11:09:24.142 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:09:24,152 [ZeusDataLoader(eval)] eval epoch 1 done: time=3.66 energy=439.99
2022-12-06 06:09:24,153 [ZeusDataLoader(train)] Up to epoch 1: time=90.53, energy=10943.15, cost=13392.53
2022-12-06 06:09:24,153 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:09:24,153 [ZeusDataLoader(train)] Expected next epoch: time=180.32, energy=21741.17, cost=26648.91
2022-12-06 06:09:24,154 [ZeusDataLoader(train)] Epoch 2 begin.
Validation Epoch: 0, Average loss: 0.0298, Accuracy: 0.1204
2022-12-06 06:09:24,338 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:09:24,339 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:09:24.345 [ZeusMonitor] Monitor started.
2022-12-06 11:09:24.345 [ZeusMonitor] Running indefinitely. 2022-12-06 11:09:24.345 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:09:24.345 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e2+gpu0.power.log
Training Epoch: 1 [128/50048]	Loss: 3.6955
Training Epoch: 1 [256/50048]	Loss: 3.5391
Training Epoch: 1 [384/50048]	Loss: 3.4373
Training Epoch: 1 [512/50048]	Loss: 3.7325
Training Epoch: 1 [640/50048]	Loss: 3.4549
Training Epoch: 1 [768/50048]	Loss: 3.5354
Training Epoch: 1 [896/50048]	Loss: 3.5246
Training Epoch: 1 [1024/50048]	Loss: 3.5624
Training Epoch: 1 [1152/50048]	Loss: 3.5101
Training Epoch: 1 [1280/50048]	Loss: 3.6374
Training Epoch: 1 [1408/50048]	Loss: 3.5832
Training Epoch: 1 [1536/50048]	Loss: 3.7276
Training Epoch: 1 [1664/50048]	Loss: 3.4197
Training Epoch: 1 [1792/50048]	Loss: 3.2636
Training Epoch: 1 [1920/50048]	Loss: 3.4743
Training Epoch: 1 [2048/50048]	Loss: 3.5550
Training Epoch: 1 [2176/50048]	Loss: 3.8329
Training Epoch: 1 [2304/50048]	Loss: 3.6114
Training Epoch: 1 [2432/50048]	Loss: 3.6604
Training Epoch: 1 [2560/50048]	Loss: 3.4355
Training Epoch: 1 [2688/50048]	Loss: 3.5082
Training Epoch: 1 [2816/50048]	Loss: 3.3863
Training Epoch: 1 [2944/50048]	Loss: 3.5077
Training Epoch: 1 [3072/50048]	Loss: 3.5007
Training Epoch: 1 [3200/50048]	Loss: 3.5360
Training Epoch: 1 [3328/50048]	Loss: 3.4119
Training Epoch: 1 [3456/50048]	Loss: 3.3786
Training Epoch: 1 [3584/50048]	Loss: 3.6273
Training Epoch: 1 [3712/50048]	Loss: 3.5524
Training Epoch: 1 [3840/50048]	Loss: 3.5957
Training Epoch: 1 [3968/50048]	Loss: 3.4791
Training Epoch: 1 [4096/50048]	Loss: 3.5949
Training Epoch: 1 [4224/50048]	Loss: 3.6562
Training Epoch: 1 [4352/50048]	Loss: 3.5554
Training Epoch: 1 [4480/50048]	Loss: 3.1551
Training Epoch: 1 [4608/50048]	Loss: 3.3851
Training Epoch: 1 [4736/50048]	Loss: 3.5707
Training Epoch: 1 [4864/50048]	Loss: 3.5033
Training Epoch: 1 [4992/50048]	Loss: 3.5837
Training Epoch: 1 [5120/50048]	Loss: 3.4036
Training Epoch: 1 [5248/50048]	Loss: 3.5112
Training Epoch: 1 [5376/50048]	Loss: 3.4741
Training Epoch: 1 [5504/50048]	Loss: 3.4060
Training Epoch: 1 [5632/50048]	Loss: 3.3428
Training Epoch: 1 [5760/50048]	Loss: 3.5817
Training Epoch: 1 [5888/50048]	Loss: 3.4192
Training Epoch: 1 [6016/50048]	Loss: 3.4937
Training Epoch: 1 [6144/50048]	Loss: 3.6045
Training Epoch: 1 [6272/50048]	Loss: 3.2894
Training Epoch: 1 [6400/50048]	Loss: 3.5641
Training Epoch: 1 [6528/50048]	Loss: 3.4962
Training Epoch: 1 [6656/50048]	Loss: 3.3288
Training Epoch: 1 [6784/50048]	Loss: 3.5049
Training Epoch: 1 [6912/50048]	Loss: 3.3007
Training Epoch: 1 [7040/50048]	Loss: 3.4991
Training Epoch: 1 [7168/50048]	Loss: 3.4024
Training Epoch: 1 [7296/50048]	Loss: 3.5082
Training Epoch: 1 [7424/50048]	Loss: 3.4994
Training Epoch: 1 [7552/50048]	Loss: 3.4399
Training Epoch: 1 [7680/50048]	Loss: 3.4747
Training Epoch: 1 [7808/50048]	Loss: 3.4972
Training Epoch: 1 [7936/50048]	Loss: 3.4408
Training Epoch: 1 [8064/50048]	Loss: 3.2266
Training Epoch: 1 [8192/50048]	Loss: 3.2660
Training Epoch: 1 [8320/50048]	Loss: 3.5595
Training Epoch: 1 [8448/50048]	Loss: 3.5396
Training Epoch: 1 [8576/50048]	Loss: 3.3677
Training Epoch: 1 [8704/50048]	Loss: 3.4953
Training Epoch: 1 [8832/50048]	Loss: 3.5708
Training Epoch: 1 [8960/50048]	Loss: 3.4400
Training Epoch: 1 [9088/50048]	Loss: 3.6072
Training Epoch: 1 [9216/50048]	Loss: 3.8819
Training Epoch: 1 [9344/50048]	Loss: 3.4469
Training Epoch: 1 [9472/50048]	Loss: 3.5828
Training Epoch: 1 [9600/50048]	Loss: 3.6399
Training Epoch: 1 [9728/50048]	Loss: 3.3060
Training Epoch: 1 [9856/50048]	Loss: 3.3275
Training Epoch: 1 [9984/50048]	Loss: 3.3832
Training Epoch: 1 [10112/50048]	Loss: 3.3786
Training Epoch: 1 [10240/50048]	Loss: 3.6605
Training Epoch: 1 [10368/50048]	Loss: 3.3680
Training Epoch: 1 [10496/50048]	Loss: 3.5696
Training Epoch: 1 [10624/50048]	Loss: 3.5707
Training Epoch: 1 [10752/50048]	Loss: 3.4284
Training Epoch: 1 [10880/50048]	Loss: 3.5236
Training Epoch: 1 [11008/50048]	Loss: 3.5927
Training Epoch: 1 [11136/50048]	Loss: 3.3118
Training Epoch: 1 [11264/50048]	Loss: 3.5982
Training Epoch: 1 [11392/50048]	Loss: 3.5057
Training Epoch: 1 [11520/50048]	Loss: 3.5006
Training Epoch: 1 [11648/50048]	Loss: 3.4768
Training Epoch: 1 [11776/50048]	Loss: 3.6156
Training Epoch: 1 [11904/50048]	Loss: 3.4400
Training Epoch: 1 [12032/50048]	Loss: 3.1954
Training Epoch: 1 [12160/50048]	Loss: 3.3416
Training Epoch: 1 [12288/50048]	Loss: 3.5461
Training Epoch: 1 [12416/50048]	Loss: 3.4284
Training Epoch: 1 [12544/50048]	Loss: 3.3082
Training Epoch: 1 [12672/50048]	Loss: 3.2634
Training Epoch: 1 [12800/50048]	Loss: 3.5031
Training Epoch: 1 [12928/50048]	Loss: 3.3244
Training Epoch: 1 [13056/50048]	Loss: 3.1872
Training Epoch: 1 [13184/50048]	Loss: 3.6262
Training Epoch: 1 [13312/50048]	Loss: 3.2407
Training Epoch: 1 [13440/50048]	Loss: 3.6133
Training Epoch: 1 [13568/50048]	Loss: 3.5716
Training Epoch: 1 [13696/50048]	Loss: 3.4676
Training Epoch: 1 [13824/50048]	Loss: 3.5414
Training Epoch: 1 [13952/50048]	Loss: 3.3131
Training Epoch: 1 [14080/50048]	Loss: 3.3144
Training Epoch: 1 [14208/50048]	Loss: 3.5392
Training Epoch: 1 [14336/50048]	Loss: 3.3283
Training Epoch: 1 [14464/50048]	Loss: 3.3744
Training Epoch: 1 [14592/50048]	Loss: 3.6518
Training Epoch: 1 [14720/50048]	Loss: 3.3932
Training Epoch: 1 [14848/50048]	Loss: 3.4945
Training Epoch: 1 [14976/50048]	Loss: 3.4968
Training Epoch: 1 [15104/50048]	Loss: 3.4302
Training Epoch: 1 [15232/50048]	Loss: 3.3733
Training Epoch: 1 [15360/50048]	Loss: 3.2753
Training Epoch: 1 [15488/50048]	Loss: 3.5055
Training Epoch: 1 [15616/50048]	Loss: 3.4679
Training Epoch: 1 [15744/50048]	Loss: 3.4170
Training Epoch: 1 [15872/50048]	Loss: 3.2417
Training Epoch: 1 [16000/50048]	Loss: 3.7004
Training Epoch: 1 [16128/50048]	Loss: 3.1304
Training Epoch: 1 [16256/50048]	Loss: 3.3686
Training Epoch: 1 [16384/50048]	Loss: 3.3847
Training Epoch: 1 [16512/50048]	Loss: 3.2676
Training Epoch: 1 [16640/50048]	Loss: 3.3495
Training Epoch: 1 [16768/50048]	Loss: 3.5084
Training Epoch: 1 [16896/50048]	Loss: 3.1656
Training Epoch: 1 [17024/50048]	Loss: 3.6377
Training Epoch: 1 [17152/50048]	Loss: 3.3372
Training Epoch: 1 [17280/50048]	Loss: 3.4013
Training Epoch: 1 [17408/50048]	Loss: 3.3764
Training Epoch: 1 [17536/50048]	Loss: 3.3834
Training Epoch: 1 [17664/50048]	Loss: 3.1871
Training Epoch: 1 [17792/50048]	Loss: 3.1216
Training Epoch: 1 [17920/50048]	Loss: 3.4487
Training Epoch: 1 [18048/50048]	Loss: 3.4165
Training Epoch: 1 [18176/50048]	Loss: 3.2715
Training Epoch: 1 [18304/50048]	Loss: 3.2341
Training Epoch: 1 [18432/50048]	Loss: 3.2631
Training Epoch: 1 [18560/50048]	Loss: 3.3730
Training Epoch: 1 [18688/50048]	Loss: 3.4051
Training Epoch: 1 [18816/50048]	Loss: 3.3869
Training Epoch: 1 [18944/50048]	Loss: 3.2803
Training Epoch: 1 [19072/50048]	Loss: 3.2967
Training Epoch: 1 [19200/50048]	Loss: 3.4144
Training Epoch: 1 [19328/50048]	Loss: 3.4955
Training Epoch: 1 [19456/50048]	Loss: 3.4363
Training Epoch: 1 [19584/50048]	Loss: 3.3412
Training Epoch: 1 [19712/50048]	Loss: 3.1465
Training Epoch: 1 [19840/50048]	Loss: 3.1840
Training Epoch: 1 [19968/50048]	Loss: 3.2892
Training Epoch: 1 [20096/50048]	Loss: 3.5122
Training Epoch: 1 [20224/50048]	Loss: 3.5852
Training Epoch: 1 [20352/50048]	Loss: 3.3364
Training Epoch: 1 [20480/50048]	Loss: 3.3675
Training Epoch: 1 [20608/50048]	Loss: 3.5190
Training Epoch: 1 [20736/50048]	Loss: 3.3104
Training Epoch: 1 [20864/50048]	Loss: 3.3367
Training Epoch: 1 [20992/50048]	Loss: 3.4073
Training Epoch: 1 [21120/50048]	Loss: 3.1973
Training Epoch: 1 [21248/50048]	Loss: 3.2592
Training Epoch: 1 [21376/50048]	Loss: 3.3423
Training Epoch: 1 [21504/50048]	Loss: 3.2755
Training Epoch: 1 [21632/50048]	Loss: 3.2236
Training Epoch: 1 [21760/50048]	Loss: 3.3340
Training Epoch: 1 [21888/50048]	Loss: 3.2911
Training Epoch: 1 [22016/50048]	Loss: 3.4488
Training Epoch: 1 [22144/50048]	Loss: 3.3502
Training Epoch: 1 [22272/50048]	Loss: 3.1643
Training Epoch: 1 [22400/50048]	Loss: 3.3414
Training Epoch: 1 [22528/50048]	Loss: 3.2724
Training Epoch: 1 [22656/50048]	Loss: 3.1069
Training Epoch: 1 [22784/50048]	Loss: 3.3098
Training Epoch: 1 [22912/50048]	Loss: 3.4595
Training Epoch: 1 [23040/50048]	Loss: 3.2665
Training Epoch: 1 [23168/50048]	Loss: 3.5872
Training Epoch: 1 [23296/50048]	Loss: 3.2529
Training Epoch: 1 [23424/50048]	Loss: 3.1345
Training Epoch: 1 [23552/50048]	Loss: 3.3972
Training Epoch: 1 [23680/50048]	Loss: 3.4456
Training Epoch: 1 [23808/50048]	Loss: 3.3716
Training Epoch: 1 [23936/50048]	Loss: 3.2783
Training Epoch: 1 [24064/50048]	Loss: 3.4272
Training Epoch: 1 [24192/50048]	Loss: 2.9920
Training Epoch: 1 [24320/50048]	Loss: 3.1102
Training Epoch: 1 [24448/50048]	Loss: 3.2638
Training Epoch: 1 [24576/50048]	Loss: 3.2662
Training Epoch: 1 [24704/50048]	Loss: 3.4164
Training Epoch: 1 [24832/50048]	Loss: 3.0890
Training Epoch: 1 [24960/50048]	Loss: 3.2014
Training Epoch: 1 [25088/50048]	Loss: 3.2812
Training Epoch: 1 [25216/50048]	Loss: 3.4599
Training Epoch: 1 [25344/50048]	Loss: 3.3464
Training Epoch: 1 [25472/50048]	Loss: 3.3514
Training Epoch: 1 [25600/50048]	Loss: 3.4045
Training Epoch: 1 [25728/50048]	Loss: 3.3873
Training Epoch: 1 [25856/50048]	Loss: 3.4282
Training Epoch: 1 [25984/50048]	Loss: 3.1864
Training Epoch: 1 [26112/50048]	Loss: 3.2787
Training Epoch: 1 [26240/50048]	Loss: 3.3541
Training Epoch: 1 [26368/50048]	Loss: 3.3884
Training Epoch: 1 [26496/50048]	Loss: 3.2264
Training Epoch: 1 [26624/50048]	Loss: 3.3213
Training Epoch: 1 [26752/50048]	Loss: 3.1953
Training Epoch: 1 [26880/50048]	Loss: 3.1255
Training Epoch: 1 [27008/50048]	Loss: 3.3025
Training Epoch: 1 [27136/50048]	Loss: 3.1768
Training Epoch: 1 [27264/50048]	Loss: 3.2594
Training Epoch: 1 [27392/50048]	Loss: 3.3038
Training Epoch: 1 [27520/50048]	Loss: 3.2806
Training Epoch: 1 [27648/50048]	Loss: 3.1338
Training Epoch: 1 [27776/50048]	Loss: 3.2141
Training Epoch: 1 [27904/50048]	Loss: 3.3378
Training Epoch: 1 [28032/50048]	Loss: 3.1257
Training Epoch: 1 [28160/50048]	Loss: 3.0927
Training Epoch: 1 [28288/50048]	Loss: 3.2314
Training Epoch: 1 [28416/50048]	Loss: 3.3841
Training Epoch: 1 [28544/50048]	Loss: 3.3389
Training Epoch: 1 [28672/50048]	Loss: 3.0581
Training Epoch: 1 [28800/50048]	Loss: 3.5558
Training Epoch: 1 [28928/50048]	Loss: 3.3760
Training Epoch: 1 [29056/50048]	Loss: 3.2835
Training Epoch: 1 [29184/50048]	Loss: 3.3019
Training Epoch: 1 [29312/50048]	Loss: 3.3568
Training Epoch: 1 [29440/50048]	Loss: 3.2067
Training Epoch: 1 [29568/50048]	Loss: 3.3590
Training Epoch: 1 [29696/50048]	Loss: 3.1735
Training Epoch: 1 [29824/50048]	Loss: 3.0839
Training Epoch: 1 [29952/50048]	Loss: 3.3208
Training Epoch: 1 [30080/50048]	Loss: 3.2534
Training Epoch: 1 [30208/50048]	Loss: 3.3390
Training Epoch: 1 [30336/50048]	Loss: 2.9811
Training Epoch: 1 [30464/50048]	Loss: 3.2841
Training Epoch: 1 [30592/50048]	Loss: 3.2538
Training Epoch: 1 [30720/50048]	Loss: 3.4098
Training Epoch: 1 [30848/50048]	Loss: 3.2441
Training Epoch: 1 [30976/50048]	Loss: 3.0546
Training Epoch: 1 [31104/50048]	Loss: 3.1736
Training Epoch: 1 [31232/50048]	Loss: 3.5218
Training Epoch: 1 [31360/50048]	Loss: 3.1328
Training Epoch: 1 [31488/50048]	Loss: 3.1616
Training Epoch: 1 [31616/50048]	Loss: 3.3648
Training Epoch: 1 [31744/50048]	Loss: 3.3214
Training Epoch: 1 [31872/50048]	Loss: 3.2436
Training Epoch: 1 [32000/50048]	Loss: 3.2248
Training Epoch: 1 [32128/50048]	Loss: 3.2165
Training Epoch: 1 [32256/50048]	Loss: 3.1209
Training Epoch: 1 [32384/50048]	Loss: 3.2220
Training Epoch: 1 [32512/50048]	Loss: 3.2671
Training Epoch: 1 [32640/50048]	Loss: 3.1225
Training Epoch: 1 [32768/50048]	Loss: 3.0845
Training Epoch: 1 [32896/50048]	Loss: 3.2041
Training Epoch: 1 [33024/50048]	Loss: 3.1125
Training Epoch: 1 [33152/50048]	Loss: 3.4093
Training Epoch: 1 [33280/50048]	Loss: 3.1410
Training Epoch: 1 [33408/50048]	Loss: 3.2977
Training Epoch: 1 [33536/50048]	Loss: 3.1190
Training Epoch: 1 [33664/50048]	Loss: 3.2688
Training Epoch: 1 [33792/50048]	Loss: 3.1875
Training Epoch: 1 [33920/50048]	Loss: 3.3200
Training Epoch: 1 [34048/50048]	Loss: 3.1235
Training Epoch: 1 [34176/50048]	Loss: 3.3580
Training Epoch: 1 [34304/50048]	Loss: 3.1429
Training Epoch: 1 [34432/50048]	Loss: 3.3707
Training Epoch: 1 [34560/50048]	Loss: 3.0324
Training Epoch: 1 [34688/50048]	Loss: 3.2045
Training Epoch: 1 [34816/50048]	Loss: 3.4233
Training Epoch: 1 [34944/50048]	Loss: 3.1911
Training Epoch: 1 [35072/50048]	Loss: 3.3793
Training Epoch: 1 [35200/50048]	Loss: 3.1468
Training Epoch: 1 [35328/50048]	Loss: 3.2678
Training Epoch: 1 [35456/50048]	Loss: 3.3260
Training Epoch: 1 [35584/50048]	Loss: 3.2906
Training Epoch: 1 [35712/50048]	Loss: 3.1706
Training Epoch: 1 [35840/50048]	Loss: 3.1950
Training Epoch: 1 [35968/50048]	Loss: 3.0854
Training Epoch: 1 [36096/50048]	Loss: 3.1954
Training Epoch: 1 [36224/50048]	Loss: 3.4866
Training Epoch: 1 [36352/50048]	Loss: 3.1688
Training Epoch: 1 [36480/50048]	Loss: 3.2154
Training Epoch: 1 [36608/50048]	Loss: 3.1418
Training Epoch: 1 [36736/50048]	Loss: 3.3750
Training Epoch: 1 [36864/50048]	Loss: 3.1779
Training Epoch: 1 [36992/50048]	Loss: 3.0623
Training Epoch: 1 [37120/50048]	Loss: 3.1073
Training Epoch: 1 [37248/50048]	Loss: 3.2475
Training Epoch: 1 [37376/50048]	Loss: 3.0484
Training Epoch: 1 [37504/50048]	Loss: 2.9361
Training Epoch: 1 [37632/50048]	Loss: 3.0623
Training Epoch: 1 [37760/50048]	Loss: 3.2781
Training Epoch: 1 [37888/50048]	Loss: 3.2289
Training Epoch: 1 [38016/50048]	Loss: 3.0258
Training Epoch: 1 [38144/50048]	Loss: 3.1105
Training Epoch: 1 [38272/50048]	Loss: 3.1772
Training Epoch: 1 [38400/50048]	Loss: 3.3796
Training Epoch: 1 [38528/50048]	Loss: 2.9796
Training Epoch: 1 [38656/50048]	Loss: 3.2464
Training Epoch: 1 [38784/50048]	Loss: 3.3887
Training Epoch: 1 [38912/50048]	Loss: 2.9039
Training Epoch: 1 [39040/50048]	Loss: 3.3602
Training Epoch: 1 [39168/50048]	Loss: 2.9618
Training Epoch: 1 [39296/50048]	Loss: 3.1906
Training Epoch: 1 [39424/50048]	Loss: 3.1024
Training Epoch: 1 [39552/50048]	Loss: 3.2497
Training Epoch: 1 [39680/50048]	Loss: 3.2456
Training Epoch: 1 [39808/50048]	Loss: 3.1107
Training Epoch: 1 [39936/50048]	Loss: 3.3091
Training Epoch: 1 [40064/50048]	Loss: 3.0149
Training Epoch: 1 [40192/50048]	Loss: 3.2692
Training Epoch: 1 [40320/50048]	Loss: 3.3009
Training Epoch: 1 [40448/50048]	Loss: 3.1708
Training Epoch: 1 [40576/50048]	Loss: 3.0904
Training Epoch: 1 [40704/50048]	Loss: 3.0547
Training Epoch: 1 [40832/50048]	Loss: 3.1479
Training Epoch: 1 [40960/50048]	Loss: 3.1178
Training Epoch: 1 [41088/50048]	Loss: 3.0979
Training Epoch: 1 [41216/50048]	Loss: 3.2621
Training Epoch: 1 [41344/50048]	Loss: 3.0578
Training Epoch: 1 [41472/50048]	Loss: 3.0483
Training Epoch: 1 [41600/50048]	Loss: 3.1052
Training Epoch: 1 [41728/50048]	Loss: 3.0474
Training Epoch: 1 [41856/50048]	Loss: 3.1959
Training Epoch: 1 [41984/50048]	Loss: 3.2382
Training Epoch: 1 [42112/50048]	Loss: 3.0634
Training Epoch: 1 [42240/50048]	Loss: 3.2037
Training Epoch: 1 [42368/50048]	Loss: 3.2673
Training Epoch: 1 [42496/50048]	Loss: 3.4053
Training Epoch: 1 [42624/50048]	Loss: 3.0832
Training Epoch: 1 [42752/50048]	Loss: 2.9562
Training Epoch: 1 [42880/50048]	Loss: 3.4677
Training Epoch: 1 [43008/50048]	Loss: 3.1654
Training Epoch: 1 [43136/50048]	Loss: 3.2109
Training Epoch: 1 [43264/50048]	Loss: 2.9600
Training Epoch: 1 [43392/50048]	Loss: 2.9885
Training Epoch: 1 [43520/50048]	Loss: 3.2357
Training Epoch: 1 [43648/50048]	Loss: 2.9929
Training Epoch: 1 [43776/50048]	Loss: 2.9770
Training Epoch: 1 [43904/50048]	Loss: 3.2767
Training Epoch: 1 [44032/50048]	Loss: 3.1456
Training Epoch: 1 [44160/50048]	Loss: 2.8491
Training Epoch: 1 [44288/50048]	Loss: 3.1624
Training Epoch: 1 [44416/50048]	Loss: 3.1860
Training Epoch: 1 [44544/50048]	Loss: 3.0552
Training Epoch: 1 [44672/50048]	Loss: 2.7626
Training Epoch: 1 [44800/50048]	Loss: 3.2260
Training Epoch: 1 [44928/50048]	Loss: 3.4903
Training Epoch: 1 [45056/50048]	Loss: 3.2605
Training Epoch: 1 [45184/50048]	Loss: 3.3101
Training Epoch: 1 [45312/50048]	Loss: 3.1161
Training Epoch: 1 [45440/50048]	Loss: 2.9010
Training Epoch: 1 [45568/50048]	Loss: 3.2070
Training Epoch: 1 [45696/50048]	Loss: 3.1757
Training Epoch: 1 [45824/50048]	Loss: 2.9550
Training Epoch: 1 [45952/50048]	Loss: 3.1514
Training Epoch: 1 [46080/50048]	Loss: 3.0013
Training Epoch: 1 [46208/50048]	Loss: 2.9839
Training Epoch: 1 [46336/50048]	Loss: 3.1635
Training Epoch: 1 [46464/50048]	Loss: 2.8970
Training Epoch: 1 [46592/50048]	Loss: 3.0500
Training Epoch: 1 [46720/50048]	Loss: 3.1234
2022-12-06 06:10:50,493 [ZeusDataLoader(train)] train epoch 2 done: time=86.33 energy=10485.76
2022-12-06 06:10:50,495 [ZeusDataLoader(eval)] Epoch 2 begin.
Training Epoch: 1 [46848/50048]	Loss: 3.2238
Training Epoch: 1 [46976/50048]	Loss: 3.1277
Training Epoch: 1 [47104/50048]	Loss: 3.4124
Training Epoch: 1 [47232/50048]	Loss: 3.1471
Training Epoch: 1 [47360/50048]	Loss: 3.0022
Training Epoch: 1 [47488/50048]	Loss: 3.4838
Training Epoch: 1 [47616/50048]	Loss: 3.2497
Training Epoch: 1 [47744/50048]	Loss: 2.9951
Training Epoch: 1 [47872/50048]	Loss: 3.3585
Training Epoch: 1 [48000/50048]	Loss: 2.9962
Training Epoch: 1 [48128/50048]	Loss: 3.1281
Training Epoch: 1 [48256/50048]	Loss: 3.1881
Training Epoch: 1 [48384/50048]	Loss: 3.1119
Training Epoch: 1 [48512/50048]	Loss: 3.1356
Training Epoch: 1 [48640/50048]	Loss: 3.2319
Training Epoch: 1 [48768/50048]	Loss: 3.3734
Training Epoch: 1 [48896/50048]	Loss: 3.2332
Training Epoch: 1 [49024/50048]	Loss: 3.2668
Training Epoch: 1 [49152/50048]	Loss: 3.2825
Training Epoch: 1 [49280/50048]	Loss: 3.0133
Training Epoch: 1 [49408/50048]	Loss: 3.2221
Training Epoch: 1 [49536/50048]	Loss: 2.9574
Training Epoch: 1 [49664/50048]	Loss: 3.1616
Training Epoch: 1 [49792/50048]	Loss: 3.0419
Training Epoch: 1 [49920/50048]	Loss: 3.0121
Training Epoch: 1 [50048/50048]	Loss: 3.1708
2022-12-06 11:10:54.247 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:10:54,272 [ZeusDataLoader(eval)] eval epoch 2 done: time=3.77 energy=450.95
2022-12-06 06:10:54,273 [ZeusDataLoader(train)] Up to epoch 2: time=180.62, energy=21879.87, cost=26744.45
2022-12-06 06:10:54,273 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:10:54,273 [ZeusDataLoader(train)] Expected next epoch: time=270.42, energy=32677.88, cost=40000.84
2022-12-06 06:10:54,274 [ZeusDataLoader(train)] Epoch 3 begin.
Validation Epoch: 1, Average loss: 0.0248, Accuracy: 0.2206
2022-12-06 06:10:54,460 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:10:54,461 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:10:54.463 [ZeusMonitor] Monitor started.
2022-12-06 11:10:54.463 [ZeusMonitor] Running indefinitely. 2022-12-06 11:10:54.463 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:10:54.463 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e3+gpu0.power.log
Training Epoch: 2 [128/50048]	Loss: 3.1223
Training Epoch: 2 [256/50048]	Loss: 2.8353
Training Epoch: 2 [384/50048]	Loss: 3.3276
Training Epoch: 2 [512/50048]	Loss: 3.1335
Training Epoch: 2 [640/50048]	Loss: 2.6868
Training Epoch: 2 [768/50048]	Loss: 2.8332
Training Epoch: 2 [896/50048]	Loss: 3.0950
Training Epoch: 2 [1024/50048]	Loss: 3.1058
Training Epoch: 2 [1152/50048]	Loss: 2.9686
Training Epoch: 2 [1280/50048]	Loss: 3.2033
Training Epoch: 2 [1408/50048]	Loss: 3.2181
Training Epoch: 2 [1536/50048]	Loss: 3.0191
Training Epoch: 2 [1664/50048]	Loss: 3.0197
Training Epoch: 2 [1792/50048]	Loss: 2.9165
Training Epoch: 2 [1920/50048]	Loss: 3.0685
Training Epoch: 2 [2048/50048]	Loss: 2.8662
Training Epoch: 2 [2176/50048]	Loss: 3.1493
Training Epoch: 2 [2304/50048]	Loss: 2.7453
Training Epoch: 2 [2432/50048]	Loss: 3.1285
Training Epoch: 2 [2560/50048]	Loss: 3.2225
Training Epoch: 2 [2688/50048]	Loss: 2.9639
Training Epoch: 2 [2816/50048]	Loss: 2.8228
Training Epoch: 2 [2944/50048]	Loss: 2.7274
Training Epoch: 2 [3072/50048]	Loss: 3.0486
Training Epoch: 2 [3200/50048]	Loss: 3.0814
Training Epoch: 2 [3328/50048]	Loss: 2.9916
Training Epoch: 2 [3456/50048]	Loss: 2.8305
Training Epoch: 2 [3584/50048]	Loss: 3.2985
Training Epoch: 2 [3712/50048]	Loss: 2.7909
Training Epoch: 2 [3840/50048]	Loss: 2.9310
Training Epoch: 2 [3968/50048]	Loss: 2.8327
Training Epoch: 2 [4096/50048]	Loss: 3.0485
Training Epoch: 2 [4224/50048]	Loss: 3.1486
Training Epoch: 2 [4352/50048]	Loss: 2.9411
Training Epoch: 2 [4480/50048]	Loss: 2.9918
Training Epoch: 2 [4608/50048]	Loss: 2.9241
Training Epoch: 2 [4736/50048]	Loss: 2.8555
Training Epoch: 2 [4864/50048]	Loss: 2.8665
Training Epoch: 2 [4992/50048]	Loss: 2.9160
Training Epoch: 2 [5120/50048]	Loss: 2.9416
Training Epoch: 2 [5248/50048]	Loss: 3.0825
Training Epoch: 2 [5376/50048]	Loss: 2.8578
Training Epoch: 2 [5504/50048]	Loss: 3.0842
Training Epoch: 2 [5632/50048]	Loss: 3.2114
Training Epoch: 2 [5760/50048]	Loss: 2.9107
Training Epoch: 2 [5888/50048]	Loss: 2.8571
Training Epoch: 2 [6016/50048]	Loss: 2.9393
Training Epoch: 2 [6144/50048]	Loss: 3.0389
Training Epoch: 2 [6272/50048]	Loss: 3.2162
Training Epoch: 2 [6400/50048]	Loss: 2.9045
Training Epoch: 2 [6528/50048]	Loss: 3.0356
Training Epoch: 2 [6656/50048]	Loss: 3.1300
Training Epoch: 2 [6784/50048]	Loss: 2.9726
Training Epoch: 2 [6912/50048]	Loss: 3.0628
Training Epoch: 2 [7040/50048]	Loss: 2.8361
Training Epoch: 2 [7168/50048]	Loss: 3.0701
Training Epoch: 2 [7296/50048]	Loss: 3.0927
Training Epoch: 2 [7424/50048]	Loss: 2.9139
Training Epoch: 2 [7552/50048]	Loss: 3.3539
Training Epoch: 2 [7680/50048]	Loss: 3.0225
Training Epoch: 2 [7808/50048]	Loss: 3.0305
Training Epoch: 2 [7936/50048]	Loss: 2.9493
Training Epoch: 2 [8064/50048]	Loss: 2.9752
Training Epoch: 2 [8192/50048]	Loss: 3.2378
Training Epoch: 2 [8320/50048]	Loss: 2.8845
Training Epoch: 2 [8448/50048]	Loss: 2.9396
Training Epoch: 2 [8576/50048]	Loss: 2.8751
Training Epoch: 2 [8704/50048]	Loss: 2.8271
Training Epoch: 2 [8832/50048]	Loss: 3.1490
Training Epoch: 2 [8960/50048]	Loss: 2.8251
Training Epoch: 2 [9088/50048]	Loss: 3.0584
Training Epoch: 2 [9216/50048]	Loss: 3.0859
Training Epoch: 2 [9344/50048]	Loss: 2.9522
Training Epoch: 2 [9472/50048]	Loss: 2.8558
Training Epoch: 2 [9600/50048]	Loss: 2.7795
Training Epoch: 2 [9728/50048]	Loss: 3.0327
Training Epoch: 2 [9856/50048]	Loss: 2.7562
Training Epoch: 2 [9984/50048]	Loss: 2.8679
Training Epoch: 2 [10112/50048]	Loss: 2.7766
Training Epoch: 2 [10240/50048]	Loss: 2.7885
Training Epoch: 2 [10368/50048]	Loss: 2.9949
Training Epoch: 2 [10496/50048]	Loss: 3.1459
Training Epoch: 2 [10624/50048]	Loss: 2.9910
Training Epoch: 2 [10752/50048]	Loss: 2.8965
Training Epoch: 2 [10880/50048]	Loss: 2.7533
Training Epoch: 2 [11008/50048]	Loss: 2.7374
Training Epoch: 2 [11136/50048]	Loss: 2.7448
Training Epoch: 2 [11264/50048]	Loss: 2.9661
Training Epoch: 2 [11392/50048]	Loss: 2.8552
Training Epoch: 2 [11520/50048]	Loss: 3.0742
Training Epoch: 2 [11648/50048]	Loss: 2.8887
Training Epoch: 2 [11776/50048]	Loss: 3.0160
Training Epoch: 2 [11904/50048]	Loss: 2.9982
Training Epoch: 2 [12032/50048]	Loss: 3.0257
Training Epoch: 2 [12160/50048]	Loss: 3.0050
Training Epoch: 2 [12288/50048]	Loss: 2.9798
Training Epoch: 2 [12416/50048]	Loss: 3.1528
Training Epoch: 2 [12544/50048]	Loss: 2.9205
Training Epoch: 2 [12672/50048]	Loss: 3.2139
Training Epoch: 2 [12800/50048]	Loss: 2.8151
Training Epoch: 2 [12928/50048]	Loss: 2.8079
Training Epoch: 2 [13056/50048]	Loss: 3.0056
Training Epoch: 2 [13184/50048]	Loss: 2.8179
Training Epoch: 2 [13312/50048]	Loss: 3.0309
Training Epoch: 2 [13440/50048]	Loss: 3.0469
Training Epoch: 2 [13568/50048]	Loss: 3.0228
Training Epoch: 2 [13696/50048]	Loss: 2.9281
Training Epoch: 2 [13824/50048]	Loss: 2.8609
Training Epoch: 2 [13952/50048]	Loss: 2.8973
Training Epoch: 2 [14080/50048]	Loss: 2.9885
Training Epoch: 2 [14208/50048]	Loss: 2.9560
Training Epoch: 2 [14336/50048]	Loss: 2.8114
Training Epoch: 2 [14464/50048]	Loss: 2.8420
Training Epoch: 2 [14592/50048]	Loss: 2.8503
Training Epoch: 2 [14720/50048]	Loss: 2.9518
Training Epoch: 2 [14848/50048]	Loss: 2.8057
Training Epoch: 2 [14976/50048]	Loss: 2.9016
Training Epoch: 2 [15104/50048]	Loss: 2.8526
Training Epoch: 2 [15232/50048]	Loss: 2.9570
Training Epoch: 2 [15360/50048]	Loss: 3.0346
Training Epoch: 2 [15488/50048]	Loss: 3.1925
Training Epoch: 2 [15616/50048]	Loss: 3.0725
Training Epoch: 2 [15744/50048]	Loss: 3.0943
Training Epoch: 2 [15872/50048]	Loss: 2.9636
Training Epoch: 2 [16000/50048]	Loss: 3.3397
Training Epoch: 2 [16128/50048]	Loss: 3.0524
Training Epoch: 2 [16256/50048]	Loss: 2.8974
Training Epoch: 2 [16384/50048]	Loss: 2.9660
Training Epoch: 2 [16512/50048]	Loss: 2.9199
Training Epoch: 2 [16640/50048]	Loss: 2.8854
Training Epoch: 2 [16768/50048]	Loss: 3.0237
Training Epoch: 2 [16896/50048]	Loss: 3.0236
Training Epoch: 2 [17024/50048]	Loss: 2.8477
Training Epoch: 2 [17152/50048]	Loss: 3.0687
Training Epoch: 2 [17280/50048]	Loss: 3.1195
Training Epoch: 2 [17408/50048]	Loss: 2.8226
Training Epoch: 2 [17536/50048]	Loss: 2.9179
Training Epoch: 2 [17664/50048]	Loss: 2.7901
Training Epoch: 2 [17792/50048]	Loss: 2.8166
Training Epoch: 2 [17920/50048]	Loss: 2.7323
Training Epoch: 2 [18048/50048]	Loss: 2.7548
Training Epoch: 2 [18176/50048]	Loss: 3.2041
Training Epoch: 2 [18304/50048]	Loss: 3.0088
Training Epoch: 2 [18432/50048]	Loss: 3.0209
Training Epoch: 2 [18560/50048]	Loss: 2.8423
Training Epoch: 2 [18688/50048]	Loss: 3.0045
Training Epoch: 2 [18816/50048]	Loss: 2.9566
Training Epoch: 2 [18944/50048]	Loss: 3.0390
Training Epoch: 2 [19072/50048]	Loss: 2.9869
Training Epoch: 2 [19200/50048]	Loss: 3.0553
Training Epoch: 2 [19328/50048]	Loss: 2.8884
Training Epoch: 2 [19456/50048]	Loss: 2.9657
Training Epoch: 2 [19584/50048]	Loss: 3.0358
Training Epoch: 2 [19712/50048]	Loss: 2.6368
Training Epoch: 2 [19840/50048]	Loss: 2.9682
Training Epoch: 2 [19968/50048]	Loss: 2.8983
Training Epoch: 2 [20096/50048]	Loss: 3.0887
Training Epoch: 2 [20224/50048]	Loss: 2.7439
Training Epoch: 2 [20352/50048]	Loss: 2.8277
Training Epoch: 2 [20480/50048]	Loss: 2.6883
Training Epoch: 2 [20608/50048]	Loss: 3.2474
Training Epoch: 2 [20736/50048]	Loss: 2.9232
Training Epoch: 2 [20864/50048]	Loss: 2.8714
Training Epoch: 2 [20992/50048]	Loss: 2.9557
Training Epoch: 2 [21120/50048]	Loss: 2.8996
Training Epoch: 2 [21248/50048]	Loss: 2.8369
Training Epoch: 2 [21376/50048]	Loss: 2.9064
Training Epoch: 2 [21504/50048]	Loss: 2.9435
Training Epoch: 2 [21632/50048]	Loss: 3.0721
Training Epoch: 2 [21760/50048]	Loss: 3.0431
Training Epoch: 2 [21888/50048]	Loss: 3.0006
Training Epoch: 2 [22016/50048]	Loss: 2.9431
Training Epoch: 2 [22144/50048]	Loss: 3.0470
Training Epoch: 2 [22272/50048]	Loss: 2.8696
Training Epoch: 2 [22400/50048]	Loss: 2.9084
Training Epoch: 2 [22528/50048]	Loss: 2.9563
Training Epoch: 2 [22656/50048]	Loss: 2.9891
Training Epoch: 2 [22784/50048]	Loss: 2.9562
Training Epoch: 2 [22912/50048]	Loss: 2.6614
Training Epoch: 2 [23040/50048]	Loss: 2.9416
Training Epoch: 2 [23168/50048]	Loss: 3.1211
Training Epoch: 2 [23296/50048]	Loss: 2.8611
Training Epoch: 2 [23424/50048]	Loss: 2.9351
Training Epoch: 2 [23552/50048]	Loss: 3.0104
Training Epoch: 2 [23680/50048]	Loss: 2.9486
Training Epoch: 2 [23808/50048]	Loss: 3.0942
Training Epoch: 2 [23936/50048]	Loss: 3.0443
Training Epoch: 2 [24064/50048]	Loss: 2.7401
Training Epoch: 2 [24192/50048]	Loss: 2.8838
Training Epoch: 2 [24320/50048]	Loss: 2.9722
Training Epoch: 2 [24448/50048]	Loss: 2.8644
Training Epoch: 2 [24576/50048]	Loss: 3.0699
Training Epoch: 2 [24704/50048]	Loss: 2.8487
Training Epoch: 2 [24832/50048]	Loss: 2.8910
Training Epoch: 2 [24960/50048]	Loss: 2.7619
Training Epoch: 2 [25088/50048]	Loss: 2.8458
Training Epoch: 2 [25216/50048]	Loss: 2.9751
Training Epoch: 2 [25344/50048]	Loss: 2.9558
Training Epoch: 2 [25472/50048]	Loss: 3.0074
Training Epoch: 2 [25600/50048]	Loss: 2.9038
Training Epoch: 2 [25728/50048]	Loss: 2.9496
Training Epoch: 2 [25856/50048]	Loss: 3.1026
Training Epoch: 2 [25984/50048]	Loss: 2.9407
Training Epoch: 2 [26112/50048]	Loss: 2.9671
Training Epoch: 2 [26240/50048]	Loss: 2.9501
Training Epoch: 2 [26368/50048]	Loss: 2.9690
Training Epoch: 2 [26496/50048]	Loss: 3.0337
Training Epoch: 2 [26624/50048]	Loss: 3.1417
Training Epoch: 2 [26752/50048]	Loss: 3.1158
Training Epoch: 2 [26880/50048]	Loss: 3.1246
Training Epoch: 2 [27008/50048]	Loss: 3.0609
Training Epoch: 2 [27136/50048]	Loss: 2.8086
Training Epoch: 2 [27264/50048]	Loss: 2.8493
Training Epoch: 2 [27392/50048]	Loss: 3.0027
Training Epoch: 2 [27520/50048]	Loss: 2.8242
Training Epoch: 2 [27648/50048]	Loss: 2.9584
Training Epoch: 2 [27776/50048]	Loss: 2.8426
Training Epoch: 2 [27904/50048]	Loss: 3.1248
Training Epoch: 2 [28032/50048]	Loss: 2.9392
Training Epoch: 2 [28160/50048]	Loss: 3.0958
Training Epoch: 2 [28288/50048]	Loss: 2.8732
Training Epoch: 2 [28416/50048]	Loss: 3.0090
Training Epoch: 2 [28544/50048]	Loss: 2.8114
Training Epoch: 2 [28672/50048]	Loss: 3.0007
Training Epoch: 2 [28800/50048]	Loss: 2.9232
Training Epoch: 2 [28928/50048]	Loss: 2.8561
Training Epoch: 2 [29056/50048]	Loss: 2.8891
Training Epoch: 2 [29184/50048]	Loss: 2.8866
Training Epoch: 2 [29312/50048]	Loss: 2.8411
Training Epoch: 2 [29440/50048]	Loss: 2.9981
Training Epoch: 2 [29568/50048]	Loss: 2.8410
Training Epoch: 2 [29696/50048]	Loss: 2.6432
Training Epoch: 2 [29824/50048]	Loss: 2.9567
Training Epoch: 2 [29952/50048]	Loss: 3.0308
Training Epoch: 2 [30080/50048]	Loss: 2.8415
Training Epoch: 2 [30208/50048]	Loss: 3.1720
Training Epoch: 2 [30336/50048]	Loss: 2.9890
Training Epoch: 2 [30464/50048]	Loss: 3.0979
Training Epoch: 2 [30592/50048]	Loss: 2.8914
Training Epoch: 2 [30720/50048]	Loss: 2.6605
Training Epoch: 2 [30848/50048]	Loss: 2.9517
Training Epoch: 2 [30976/50048]	Loss: 2.7690
Training Epoch: 2 [31104/50048]	Loss: 2.9264
Training Epoch: 2 [31232/50048]	Loss: 2.6982
Training Epoch: 2 [31360/50048]	Loss: 3.0905
Training Epoch: 2 [31488/50048]	Loss: 2.8867
Training Epoch: 2 [31616/50048]	Loss: 2.8849
Training Epoch: 2 [31744/50048]	Loss: 2.6926
Training Epoch: 2 [31872/50048]	Loss: 2.8652
Training Epoch: 2 [32000/50048]	Loss: 2.9957
Training Epoch: 2 [32128/50048]	Loss: 2.9137
Training Epoch: 2 [32256/50048]	Loss: 2.9132
Training Epoch: 2 [32384/50048]	Loss: 2.6389
Training Epoch: 2 [32512/50048]	Loss: 2.7533
Training Epoch: 2 [32640/50048]	Loss: 2.9350
Training Epoch: 2 [32768/50048]	Loss: 2.8072
Training Epoch: 2 [32896/50048]	Loss: 2.7311
Training Epoch: 2 [33024/50048]	Loss: 2.9577
Training Epoch: 2 [33152/50048]	Loss: 2.7090
Training Epoch: 2 [33280/50048]	Loss: 3.0071
Training Epoch: 2 [33408/50048]	Loss: 2.7527
Training Epoch: 2 [33536/50048]	Loss: 3.0334
Training Epoch: 2 [33664/50048]	Loss: 2.8916
Training Epoch: 2 [33792/50048]	Loss: 2.9278
Training Epoch: 2 [33920/50048]	Loss: 2.7833
Training Epoch: 2 [34048/50048]	Loss: 3.0096
Training Epoch: 2 [34176/50048]	Loss: 3.0042
Training Epoch: 2 [34304/50048]	Loss: 3.0208
Training Epoch: 2 [34432/50048]	Loss: 2.7758
Training Epoch: 2 [34560/50048]	Loss: 2.9942
Training Epoch: 2 [34688/50048]	Loss: 2.8187
Training Epoch: 2 [34816/50048]	Loss: 2.7855
Training Epoch: 2 [34944/50048]	Loss: 3.0306
Training Epoch: 2 [35072/50048]	Loss: 2.9281
Training Epoch: 2 [35200/50048]	Loss: 2.6212
Training Epoch: 2 [35328/50048]	Loss: 2.8134
Training Epoch: 2 [35456/50048]	Loss: 2.6960
Training Epoch: 2 [35584/50048]	Loss: 2.7953
Training Epoch: 2 [35712/50048]	Loss: 3.0039
Training Epoch: 2 [35840/50048]	Loss: 2.6651
Training Epoch: 2 [35968/50048]	Loss: 2.9680
Training Epoch: 2 [36096/50048]	Loss: 2.9745
Training Epoch: 2 [36224/50048]	Loss: 2.8995
Training Epoch: 2 [36352/50048]	Loss: 2.9404
Training Epoch: 2 [36480/50048]	Loss: 2.6144
Training Epoch: 2 [36608/50048]	Loss: 2.8938
Training Epoch: 2 [36736/50048]	Loss: 2.7466
Training Epoch: 2 [36864/50048]	Loss: 2.9064
Training Epoch: 2 [36992/50048]	Loss: 2.7274
Training Epoch: 2 [37120/50048]	Loss: 3.1451
Training Epoch: 2 [37248/50048]	Loss: 2.9225
Training Epoch: 2 [37376/50048]	Loss: 2.9025
Training Epoch: 2 [37504/50048]	Loss: 2.8329
Training Epoch: 2 [37632/50048]	Loss: 2.9991
Training Epoch: 2 [37760/50048]	Loss: 2.7358
Training Epoch: 2 [37888/50048]	Loss: 2.5711
Training Epoch: 2 [38016/50048]	Loss: 2.7510
Training Epoch: 2 [38144/50048]	Loss: 2.9546
Training Epoch: 2 [38272/50048]	Loss: 2.7639
Training Epoch: 2 [38400/50048]	Loss: 2.7508
Training Epoch: 2 [38528/50048]	Loss: 3.0272
Training Epoch: 2 [38656/50048]	Loss: 2.7872
Training Epoch: 2 [38784/50048]	Loss: 2.7533
Training Epoch: 2 [38912/50048]	Loss: 2.7354
Training Epoch: 2 [39040/50048]	Loss: 2.7864
Training Epoch: 2 [39168/50048]	Loss: 2.7078
Training Epoch: 2 [39296/50048]	Loss: 3.0770
Training Epoch: 2 [39424/50048]	Loss: 2.6844
Training Epoch: 2 [39552/50048]	Loss: 2.7586
Training Epoch: 2 [39680/50048]	Loss: 2.7220
Training Epoch: 2 [39808/50048]	Loss: 2.8934
Training Epoch: 2 [39936/50048]	Loss: 2.8672
Training Epoch: 2 [40064/50048]	Loss: 3.0108
Training Epoch: 2 [40192/50048]	Loss: 2.8853
Training Epoch: 2 [40320/50048]	Loss: 2.8298
Training Epoch: 2 [40448/50048]	Loss: 2.6932
Training Epoch: 2 [40576/50048]	Loss: 2.7922
Training Epoch: 2 [40704/50048]	Loss: 2.8948
Training Epoch: 2 [40832/50048]	Loss: 3.0862
Training Epoch: 2 [40960/50048]	Loss: 2.9798
Training Epoch: 2 [41088/50048]	Loss: 2.7003
Training Epoch: 2 [41216/50048]	Loss: 2.6979
Training Epoch: 2 [41344/50048]	Loss: 2.6457
Training Epoch: 2 [41472/50048]	Loss: 2.6570
Training Epoch: 2 [41600/50048]	Loss: 2.7124
Training Epoch: 2 [41728/50048]	Loss: 2.9161
Training Epoch: 2 [41856/50048]	Loss: 2.7677
Training Epoch: 2 [41984/50048]	Loss: 3.1191
Training Epoch: 2 [42112/50048]	Loss: 2.9817
Training Epoch: 2 [42240/50048]	Loss: 2.7326
Training Epoch: 2 [42368/50048]	Loss: 3.1631
Training Epoch: 2 [42496/50048]	Loss: 2.6790
Training Epoch: 2 [42624/50048]	Loss: 2.6339
Training Epoch: 2 [42752/50048]	Loss: 2.7068
Training Epoch: 2 [42880/50048]	Loss: 2.9780
Training Epoch: 2 [43008/50048]	Loss: 2.5810
Training Epoch: 2 [43136/50048]	Loss: 2.8743
Training Epoch: 2 [43264/50048]	Loss: 2.7883
Training Epoch: 2 [43392/50048]	Loss: 2.6110
Training Epoch: 2 [43520/50048]	Loss: 2.7190
Training Epoch: 2 [43648/50048]	Loss: 2.7635
Training Epoch: 2 [43776/50048]	Loss: 2.6829
Training Epoch: 2 [43904/50048]	Loss: 2.6789
Training Epoch: 2 [44032/50048]	Loss: 2.6963
Training Epoch: 2 [44160/50048]	Loss: 2.6189
Training Epoch: 2 [44288/50048]	Loss: 2.7153
Training Epoch: 2 [44416/50048]	Loss: 2.6035
Training Epoch: 2 [44544/50048]	Loss: 2.8252
Training Epoch: 2 [44672/50048]	Loss: 2.6850
Training Epoch: 2 [44800/50048]	Loss: 2.8904
Training Epoch: 2 [44928/50048]	Loss: 2.6784
Training Epoch: 2 [45056/50048]	Loss: 2.7824
Training Epoch: 2 [45184/50048]	Loss: 2.8493
Training Epoch: 2 [45312/50048]	Loss: 2.9543
Training Epoch: 2 [45440/50048]	Loss: 3.0254
Training Epoch: 2 [45568/50048]	Loss: 2.6405
Training Epoch: 2 [45696/50048]	Loss: 2.9037
Training Epoch: 2 [45824/50048]	Loss: 2.8042
Training Epoch: 2 [45952/50048]	Loss: 2.9539
Training Epoch: 2 [46080/50048]	Loss: 2.7565
Training Epoch: 2 [46208/50048]	Loss: 2.9806
Training Epoch: 2 [46336/50048]	Loss: 2.9940
Training Epoch: 2 [46464/50048]	Loss: 2.7596
Training Epoch: 2 [46592/50048]	Loss: 2.7515
Training Epoch: 2 [46720/50048]	Loss: 2.5754
2022-12-06 06:12:20,747 [ZeusDataLoader(train)] train epoch 3 done: time=86.46 energy=10491.59
2022-12-06 06:12:20,749 [ZeusDataLoader(eval)] Epoch 3 begin.
Training Epoch: 2 [46848/50048]	Loss: 2.4626
Training Epoch: 2 [46976/50048]	Loss: 2.6538
Training Epoch: 2 [47104/50048]	Loss: 2.7740
Training Epoch: 2 [47232/50048]	Loss: 2.6483
Training Epoch: 2 [47360/50048]	Loss: 2.8657
Training Epoch: 2 [47488/50048]	Loss: 2.7210
Training Epoch: 2 [47616/50048]	Loss: 2.6623
Training Epoch: 2 [47744/50048]	Loss: 2.7065
Training Epoch: 2 [47872/50048]	Loss: 2.6731
Training Epoch: 2 [48000/50048]	Loss: 2.7601
Training Epoch: 2 [48128/50048]	Loss: 3.0512
Training Epoch: 2 [48256/50048]	Loss: 2.8566
Training Epoch: 2 [48384/50048]	Loss: 2.7273
Training Epoch: 2 [48512/50048]	Loss: 2.7300
Training Epoch: 2 [48640/50048]	Loss: 2.7843
Training Epoch: 2 [48768/50048]	Loss: 2.7313
Training Epoch: 2 [48896/50048]	Loss: 2.8726
Training Epoch: 2 [49024/50048]	Loss: 2.9650
Training Epoch: 2 [49152/50048]	Loss: 2.6150
Training Epoch: 2 [49280/50048]	Loss: 2.7791
Training Epoch: 2 [49408/50048]	Loss: 2.8143
Training Epoch: 2 [49536/50048]	Loss: 2.7984
Training Epoch: 2 [49664/50048]	Loss: 2.4461
Training Epoch: 2 [49792/50048]	Loss: 2.6758
Training Epoch: 2 [49920/50048]	Loss: 2.7421
Training Epoch: 2 [50048/50048]	Loss: 2.5929
2022-12-06 11:12:24.469 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:12:24,512 [ZeusDataLoader(eval)] eval epoch 3 done: time=3.75 energy=453.23
2022-12-06 06:12:24,512 [ZeusDataLoader(train)] Up to epoch 3: time=270.84, energy=32824.69, cost=40110.92
2022-12-06 06:12:24,512 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:12:24,513 [ZeusDataLoader(train)] Expected next epoch: time=360.64, energy=43622.70, cost=53367.30
2022-12-06 06:12:24,513 [ZeusDataLoader(train)] Epoch 4 begin.
Validation Epoch: 2, Average loss: 0.0228, Accuracy: 0.2725
2022-12-06 06:12:24,693 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:12:24,693 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:12:24.707 [ZeusMonitor] Monitor started.
2022-12-06 11:12:24.707 [ZeusMonitor] Running indefinitely. 2022-12-06 11:12:24.707 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:12:24.707 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e4+gpu0.power.log
Training Epoch: 3 [128/50048]	Loss: 2.8747
Training Epoch: 3 [256/50048]	Loss: 2.4431
Training Epoch: 3 [384/50048]	Loss: 2.5084
Training Epoch: 3 [512/50048]	Loss: 2.6680
Training Epoch: 3 [640/50048]	Loss: 2.4266
Training Epoch: 3 [768/50048]	Loss: 2.4546
Training Epoch: 3 [896/50048]	Loss: 2.8807
Training Epoch: 3 [1024/50048]	Loss: 2.7387
Training Epoch: 3 [1152/50048]	Loss: 2.7192
Training Epoch: 3 [1280/50048]	Loss: 2.8148
Training Epoch: 3 [1408/50048]	Loss: 2.4397
Training Epoch: 3 [1536/50048]	Loss: 2.5361
Training Epoch: 3 [1664/50048]	Loss: 2.6663
Training Epoch: 3 [1792/50048]	Loss: 2.6888
Training Epoch: 3 [1920/50048]	Loss: 2.6268
Training Epoch: 3 [2048/50048]	Loss: 2.7941
Training Epoch: 3 [2176/50048]	Loss: 2.6256
Training Epoch: 3 [2304/50048]	Loss: 2.7570
Training Epoch: 3 [2432/50048]	Loss: 2.7652
Training Epoch: 3 [2560/50048]	Loss: 2.4605
Training Epoch: 3 [2688/50048]	Loss: 2.4199
Training Epoch: 3 [2816/50048]	Loss: 2.7990
Training Epoch: 3 [2944/50048]	Loss: 2.7682
Training Epoch: 3 [3072/50048]	Loss: 2.6521
Training Epoch: 3 [3200/50048]	Loss: 2.8875
Training Epoch: 3 [3328/50048]	Loss: 2.8782
Training Epoch: 3 [3456/50048]	Loss: 2.6940
Training Epoch: 3 [3584/50048]	Loss: 2.8585
Training Epoch: 3 [3712/50048]	Loss: 2.6703
Training Epoch: 3 [3840/50048]	Loss: 2.6946
Training Epoch: 3 [3968/50048]	Loss: 2.5882
Training Epoch: 3 [4096/50048]	Loss: 2.5326
Training Epoch: 3 [4224/50048]	Loss: 2.5841
Training Epoch: 3 [4352/50048]	Loss: 2.9152
Training Epoch: 3 [4480/50048]	Loss: 2.6406
Training Epoch: 3 [4608/50048]	Loss: 2.5362
Training Epoch: 3 [4736/50048]	Loss: 2.2920
Training Epoch: 3 [4864/50048]	Loss: 2.9148
Training Epoch: 3 [4992/50048]	Loss: 2.5473
Training Epoch: 3 [5120/50048]	Loss: 2.5354
Training Epoch: 3 [5248/50048]	Loss: 2.6087
Training Epoch: 3 [5376/50048]	Loss: 2.6028
Training Epoch: 3 [5504/50048]	Loss: 2.3404
Training Epoch: 3 [5632/50048]	Loss: 2.4649
Training Epoch: 3 [5760/50048]	Loss: 2.8807
Training Epoch: 3 [5888/50048]	Loss: 2.6898
Training Epoch: 3 [6016/50048]	Loss: 2.8750
Training Epoch: 3 [6144/50048]	Loss: 2.7109
Training Epoch: 3 [6272/50048]	Loss: 2.4756
Training Epoch: 3 [6400/50048]	Loss: 2.3345
Training Epoch: 3 [6528/50048]	Loss: 2.8795
Training Epoch: 3 [6656/50048]	Loss: 2.6495
Training Epoch: 3 [6784/50048]	Loss: 2.7574
Training Epoch: 3 [6912/50048]	Loss: 2.8494
Training Epoch: 3 [7040/50048]	Loss: 2.8906
Training Epoch: 3 [7168/50048]	Loss: 2.8895
Training Epoch: 3 [7296/50048]	Loss: 2.7343
Training Epoch: 3 [7424/50048]	Loss: 2.6735
Training Epoch: 3 [7552/50048]	Loss: 2.7719
Training Epoch: 3 [7680/50048]	Loss: 2.7112
Training Epoch: 3 [7808/50048]	Loss: 2.4306
Training Epoch: 3 [7936/50048]	Loss: 2.4985
Training Epoch: 3 [8064/50048]	Loss: 2.6775
Training Epoch: 3 [8192/50048]	Loss: 2.6773
Training Epoch: 3 [8320/50048]	Loss: 2.7721
Training Epoch: 3 [8448/50048]	Loss: 2.7562
Training Epoch: 3 [8576/50048]	Loss: 2.6396
Training Epoch: 3 [8704/50048]	Loss: 2.7459
Training Epoch: 3 [8832/50048]	Loss: 2.5383
Training Epoch: 3 [8960/50048]	Loss: 2.6923
Training Epoch: 3 [9088/50048]	Loss: 2.4450
Training Epoch: 3 [9216/50048]	Loss: 2.4176
Training Epoch: 3 [9344/50048]	Loss: 2.6762
Training Epoch: 3 [9472/50048]	Loss: 2.7911
Training Epoch: 3 [9600/50048]	Loss: 2.8364
Training Epoch: 3 [9728/50048]	Loss: 2.6292
Training Epoch: 3 [9856/50048]	Loss: 2.7192
Training Epoch: 3 [9984/50048]	Loss: 2.7684
Training Epoch: 3 [10112/50048]	Loss: 2.3889
Training Epoch: 3 [10240/50048]	Loss: 2.6263
Training Epoch: 3 [10368/50048]	Loss: 2.5447
Training Epoch: 3 [10496/50048]	Loss: 2.8043
Training Epoch: 3 [10624/50048]	Loss: 2.7055
Training Epoch: 3 [10752/50048]	Loss: 2.6591
Training Epoch: 3 [10880/50048]	Loss: 2.8199
Training Epoch: 3 [11008/50048]	Loss: 2.4680
Training Epoch: 3 [11136/50048]	Loss: 2.8919
Training Epoch: 3 [11264/50048]	Loss: 2.7139
Training Epoch: 3 [11392/50048]	Loss: 2.3761
Training Epoch: 3 [11520/50048]	Loss: 2.7302
Training Epoch: 3 [11648/50048]	Loss: 2.7395
Training Epoch: 3 [11776/50048]	Loss: 2.6188
Training Epoch: 3 [11904/50048]	Loss: 2.7294
Training Epoch: 3 [12032/50048]	Loss: 2.6341
Training Epoch: 3 [12160/50048]	Loss: 2.7799
Training Epoch: 3 [12288/50048]	Loss: 2.6115
Training Epoch: 3 [12416/50048]	Loss: 2.5302
Training Epoch: 3 [12544/50048]	Loss: 2.7469
Training Epoch: 3 [12672/50048]	Loss: 2.6718
Training Epoch: 3 [12800/50048]	Loss: 2.7703
Training Epoch: 3 [12928/50048]	Loss: 2.5354
Training Epoch: 3 [13056/50048]	Loss: 2.5990
Training Epoch: 3 [13184/50048]	Loss: 2.4644
Training Epoch: 3 [13312/50048]	Loss: 2.9681
Training Epoch: 3 [13440/50048]	Loss: 2.7771
Training Epoch: 3 [13568/50048]	Loss: 2.5101
Training Epoch: 3 [13696/50048]	Loss: 2.6124
Training Epoch: 3 [13824/50048]	Loss: 2.5349
Training Epoch: 3 [13952/50048]	Loss: 2.6913
Training Epoch: 3 [14080/50048]	Loss: 2.5069
Training Epoch: 3 [14208/50048]	Loss: 2.7907
Training Epoch: 3 [14336/50048]	Loss: 2.6289
Training Epoch: 3 [14464/50048]	Loss: 2.6067
Training Epoch: 3 [14592/50048]	Loss: 2.6776
Training Epoch: 3 [14720/50048]	Loss: 2.3555
Training Epoch: 3 [14848/50048]	Loss: 2.7869
Training Epoch: 3 [14976/50048]	Loss: 2.6753
Training Epoch: 3 [15104/50048]	Loss: 2.6657
Training Epoch: 3 [15232/50048]	Loss: 2.7411
Training Epoch: 3 [15360/50048]	Loss: 2.8242
Training Epoch: 3 [15488/50048]	Loss: 2.4574
Training Epoch: 3 [15616/50048]	Loss: 2.7022
Training Epoch: 3 [15744/50048]	Loss: 2.8129
Training Epoch: 3 [15872/50048]	Loss: 2.5991
Training Epoch: 3 [16000/50048]	Loss: 2.6260
Training Epoch: 3 [16128/50048]	Loss: 2.7997
Training Epoch: 3 [16256/50048]	Loss: 2.5830
Training Epoch: 3 [16384/50048]	Loss: 2.6510
Training Epoch: 3 [16512/50048]	Loss: 2.7297
Training Epoch: 3 [16640/50048]	Loss: 2.5683
Training Epoch: 3 [16768/50048]	Loss: 3.1202
Training Epoch: 3 [16896/50048]	Loss: 2.6624
Training Epoch: 3 [17024/50048]	Loss: 2.5783
Training Epoch: 3 [17152/50048]	Loss: 2.5520
Training Epoch: 3 [17280/50048]	Loss: 2.6252
Training Epoch: 3 [17408/50048]	Loss: 2.4533
Training Epoch: 3 [17536/50048]	Loss: 2.5738
Training Epoch: 3 [17664/50048]	Loss: 2.6018
Training Epoch: 3 [17792/50048]	Loss: 2.7117
Training Epoch: 3 [17920/50048]	Loss: 2.6300
Training Epoch: 3 [18048/50048]	Loss: 2.7752
Training Epoch: 3 [18176/50048]	Loss: 2.7655
Training Epoch: 3 [18304/50048]	Loss: 2.8281
Training Epoch: 3 [18432/50048]	Loss: 2.5922
Training Epoch: 3 [18560/50048]	Loss: 2.4861
Training Epoch: 3 [18688/50048]	Loss: 2.4920
Training Epoch: 3 [18816/50048]	Loss: 2.5976
Training Epoch: 3 [18944/50048]	Loss: 2.6324
Training Epoch: 3 [19072/50048]	Loss: 2.6441
Training Epoch: 3 [19200/50048]	Loss: 2.5159
Training Epoch: 3 [19328/50048]	Loss: 2.5352
Training Epoch: 3 [19456/50048]	Loss: 2.8249
Training Epoch: 3 [19584/50048]	Loss: 2.4073
Training Epoch: 3 [19712/50048]	Loss: 2.5597
Training Epoch: 3 [19840/50048]	Loss: 2.9805
Training Epoch: 3 [19968/50048]	Loss: 2.7999
Training Epoch: 3 [20096/50048]	Loss: 2.5101
Training Epoch: 3 [20224/50048]	Loss: 2.7287
Training Epoch: 3 [20352/50048]	Loss: 2.6276
Training Epoch: 3 [20480/50048]	Loss: 2.4531
Training Epoch: 3 [20608/50048]	Loss: 2.7406
Training Epoch: 3 [20736/50048]	Loss: 2.8201
Training Epoch: 3 [20864/50048]	Loss: 2.5660
Training Epoch: 3 [20992/50048]	Loss: 2.6785
Training Epoch: 3 [21120/50048]	Loss: 2.4292
Training Epoch: 3 [21248/50048]	Loss: 2.8531
Training Epoch: 3 [21376/50048]	Loss: 2.8695
Training Epoch: 3 [21504/50048]	Loss: 2.8068
Training Epoch: 3 [21632/50048]	Loss: 2.6034
Training Epoch: 3 [21760/50048]	Loss: 2.7196
Training Epoch: 3 [21888/50048]	Loss: 2.6462
Training Epoch: 3 [22016/50048]	Loss: 2.4435
Training Epoch: 3 [22144/50048]	Loss: 2.4200
Training Epoch: 3 [22272/50048]	Loss: 2.5839
Training Epoch: 3 [22400/50048]	Loss: 2.6128
Training Epoch: 3 [22528/50048]	Loss: 2.6937
Training Epoch: 3 [22656/50048]	Loss: 2.8846
Training Epoch: 3 [22784/50048]	Loss: 2.6884
Training Epoch: 3 [22912/50048]	Loss: 2.7005
Training Epoch: 3 [23040/50048]	Loss: 2.4678
Training Epoch: 3 [23168/50048]	Loss: 2.4731
Training Epoch: 3 [23296/50048]	Loss: 2.7443
Training Epoch: 3 [23424/50048]	Loss: 2.5342
Training Epoch: 3 [23552/50048]	Loss: 2.5688
Training Epoch: 3 [23680/50048]	Loss: 2.6587
Training Epoch: 3 [23808/50048]	Loss: 2.5209
Training Epoch: 3 [23936/50048]	Loss: 2.5594
Training Epoch: 3 [24064/50048]	Loss: 2.7501
Training Epoch: 3 [24192/50048]	Loss: 2.5038
Training Epoch: 3 [24320/50048]	Loss: 2.5835
Training Epoch: 3 [24448/50048]	Loss: 2.4728
Training Epoch: 3 [24576/50048]	Loss: 2.6836
Training Epoch: 3 [24704/50048]	Loss: 2.7576
Training Epoch: 3 [24832/50048]	Loss: 2.6356
Training Epoch: 3 [24960/50048]	Loss: 2.6893
Training Epoch: 3 [25088/50048]	Loss: 2.4979
Training Epoch: 3 [25216/50048]	Loss: 2.4288
Training Epoch: 3 [25344/50048]	Loss: 2.5735
Training Epoch: 3 [25472/50048]	Loss: 2.5812
Training Epoch: 3 [25600/50048]	Loss: 2.3300
Training Epoch: 3 [25728/50048]	Loss: 2.9460
Training Epoch: 3 [25856/50048]	Loss: 2.6929
Training Epoch: 3 [25984/50048]	Loss: 2.5025
Training Epoch: 3 [26112/50048]	Loss: 2.4746
Training Epoch: 3 [26240/50048]	Loss: 2.7577
Training Epoch: 3 [26368/50048]	Loss: 2.6255
Training Epoch: 3 [26496/50048]	Loss: 2.6424
Training Epoch: 3 [26624/50048]	Loss: 2.5057
Training Epoch: 3 [26752/50048]	Loss: 2.5988
Training Epoch: 3 [26880/50048]	Loss: 2.9946
Training Epoch: 3 [27008/50048]	Loss: 2.5646
Training Epoch: 3 [27136/50048]	Loss: 2.7101
Training Epoch: 3 [27264/50048]	Loss: 2.6224
Training Epoch: 3 [27392/50048]	Loss: 2.5706
Training Epoch: 3 [27520/50048]	Loss: 2.6613
Training Epoch: 3 [27648/50048]	Loss: 2.6561
Training Epoch: 3 [27776/50048]	Loss: 2.5259
Training Epoch: 3 [27904/50048]	Loss: 2.6679
Training Epoch: 3 [28032/50048]	Loss: 2.3987
Training Epoch: 3 [28160/50048]	Loss: 2.4718
Training Epoch: 3 [28288/50048]	Loss: 2.7129
Training Epoch: 3 [28416/50048]	Loss: 2.7464
Training Epoch: 3 [28544/50048]	Loss: 2.6145
Training Epoch: 3 [28672/50048]	Loss: 2.4334
Training Epoch: 3 [28800/50048]	Loss: 2.4738
Training Epoch: 3 [28928/50048]	Loss: 2.6430
Training Epoch: 3 [29056/50048]	Loss: 2.8823
Training Epoch: 3 [29184/50048]	Loss: 2.6225
Training Epoch: 3 [29312/50048]	Loss: 2.4141
Training Epoch: 3 [29440/50048]	Loss: 2.6403
Training Epoch: 3 [29568/50048]	Loss: 2.7630
Training Epoch: 3 [29696/50048]	Loss: 2.4495
Training Epoch: 3 [29824/50048]	Loss: 2.3179
Training Epoch: 3 [29952/50048]	Loss: 2.5870
Training Epoch: 3 [30080/50048]	Loss: 2.4136
Training Epoch: 3 [30208/50048]	Loss: 2.7874
Training Epoch: 3 [30336/50048]	Loss: 2.6527
Training Epoch: 3 [30464/50048]	Loss: 2.5619
Training Epoch: 3 [30592/50048]	Loss: 2.6802
Training Epoch: 3 [30720/50048]	Loss: 2.3811
Training Epoch: 3 [30848/50048]	Loss: 2.6713
Training Epoch: 3 [30976/50048]	Loss: 2.6681
Training Epoch: 3 [31104/50048]	Loss: 2.6914
Training Epoch: 3 [31232/50048]	Loss: 2.4351
Training Epoch: 3 [31360/50048]	Loss: 2.6354
Training Epoch: 3 [31488/50048]	Loss: 2.6172
Training Epoch: 3 [31616/50048]	Loss: 2.8286
Training Epoch: 3 [31744/50048]	Loss: 2.5030
Training Epoch: 3 [31872/50048]	Loss: 2.7455
Training Epoch: 3 [32000/50048]	Loss: 2.4643
Training Epoch: 3 [32128/50048]	Loss: 2.4996
Training Epoch: 3 [32256/50048]	Loss: 2.7034
Training Epoch: 3 [32384/50048]	Loss: 2.5799
Training Epoch: 3 [32512/50048]	Loss: 2.7494
Training Epoch: 3 [32640/50048]	Loss: 2.6128
Training Epoch: 3 [32768/50048]	Loss: 2.6923
Training Epoch: 3 [32896/50048]	Loss: 2.3265
Training Epoch: 3 [33024/50048]	Loss: 2.2264
Training Epoch: 3 [33152/50048]	Loss: 2.6039
Training Epoch: 3 [33280/50048]	Loss: 2.6226
Training Epoch: 3 [33408/50048]	Loss: 2.4837
Training Epoch: 3 [33536/50048]	Loss: 2.4751
Training Epoch: 3 [33664/50048]	Loss: 2.6311
Training Epoch: 3 [33792/50048]	Loss: 2.3748
Training Epoch: 3 [33920/50048]	Loss: 2.7175
Training Epoch: 3 [34048/50048]	Loss: 2.5193
Training Epoch: 3 [34176/50048]	Loss: 2.7449
Training Epoch: 3 [34304/50048]	Loss: 2.5274
Training Epoch: 3 [34432/50048]	Loss: 2.5357
Training Epoch: 3 [34560/50048]	Loss: 2.3879
Training Epoch: 3 [34688/50048]	Loss: 2.6157
Training Epoch: 3 [34816/50048]	Loss: 2.8078
Training Epoch: 3 [34944/50048]	Loss: 2.5037
Training Epoch: 3 [35072/50048]	Loss: 2.3190
Training Epoch: 3 [35200/50048]	Loss: 2.4209
Training Epoch: 3 [35328/50048]	Loss: 2.5029
Training Epoch: 3 [35456/50048]	Loss: 2.6507
Training Epoch: 3 [35584/50048]	Loss: 2.3362
Training Epoch: 3 [35712/50048]	Loss: 2.6605
Training Epoch: 3 [35840/50048]	Loss: 2.6736
Training Epoch: 3 [35968/50048]	Loss: 2.7539
Training Epoch: 3 [36096/50048]	Loss: 2.6066
Training Epoch: 3 [36224/50048]	Loss: 2.5805
Training Epoch: 3 [36352/50048]	Loss: 2.6954
Training Epoch: 3 [36480/50048]	Loss: 2.3594
Training Epoch: 3 [36608/50048]	Loss: 2.7519
Training Epoch: 3 [36736/50048]	Loss: 2.5535
Training Epoch: 3 [36864/50048]	Loss: 2.3878
Training Epoch: 3 [36992/50048]	Loss: 2.6616
Training Epoch: 3 [37120/50048]	Loss: 2.2865
Training Epoch: 3 [37248/50048]	Loss: 2.5580
Training Epoch: 3 [37376/50048]	Loss: 2.5242
Training Epoch: 3 [37504/50048]	Loss: 2.6227
Training Epoch: 3 [37632/50048]	Loss: 2.7937
Training Epoch: 3 [37760/50048]	Loss: 2.4147
Training Epoch: 3 [37888/50048]	Loss: 2.3347
Training Epoch: 3 [38016/50048]	Loss: 2.4919
Training Epoch: 3 [38144/50048]	Loss: 2.4221
Training Epoch: 3 [38272/50048]	Loss: 2.5083
Training Epoch: 3 [38400/50048]	Loss: 2.7414
Training Epoch: 3 [38528/50048]	Loss: 2.5155
Training Epoch: 3 [38656/50048]	Loss: 2.6714
Training Epoch: 3 [38784/50048]	Loss: 2.4904
Training Epoch: 3 [38912/50048]	Loss: 2.7404
Training Epoch: 3 [39040/50048]	Loss: 2.5660
Training Epoch: 3 [39168/50048]	Loss: 2.5492
Training Epoch: 3 [39296/50048]	Loss: 2.5030
Training Epoch: 3 [39424/50048]	Loss: 2.5192
Training Epoch: 3 [39552/50048]	Loss: 2.5129
Training Epoch: 3 [39680/50048]	Loss: 2.5561
Training Epoch: 3 [39808/50048]	Loss: 2.6244
Training Epoch: 3 [39936/50048]	Loss: 2.4870
Training Epoch: 3 [40064/50048]	Loss: 2.1798
Training Epoch: 3 [40192/50048]	Loss: 2.6530
Training Epoch: 3 [40320/50048]	Loss: 2.4020
Training Epoch: 3 [40448/50048]	Loss: 2.4863
Training Epoch: 3 [40576/50048]	Loss: 2.4645
Training Epoch: 3 [40704/50048]	Loss: 2.3812
Training Epoch: 3 [40832/50048]	Loss: 2.5386
Training Epoch: 3 [40960/50048]	Loss: 2.3980
Training Epoch: 3 [41088/50048]	Loss: 2.9158
Training Epoch: 3 [41216/50048]	Loss: 2.4408
Training Epoch: 3 [41344/50048]	Loss: 2.7223
Training Epoch: 3 [41472/50048]	Loss: 2.4194
Training Epoch: 3 [41600/50048]	Loss: 2.3084
Training Epoch: 3 [41728/50048]	Loss: 2.4736
Training Epoch: 3 [41856/50048]	Loss: 2.5062
Training Epoch: 3 [41984/50048]	Loss: 2.6409
Training Epoch: 3 [42112/50048]	Loss: 2.3442
Training Epoch: 3 [42240/50048]	Loss: 2.4362
Training Epoch: 3 [42368/50048]	Loss: 2.4785
Training Epoch: 3 [42496/50048]	Loss: 2.4339
Training Epoch: 3 [42624/50048]	Loss: 2.7300
Training Epoch: 3 [42752/50048]	Loss: 2.4909
Training Epoch: 3 [42880/50048]	Loss: 2.4978
Training Epoch: 3 [43008/50048]	Loss: 2.3170
Training Epoch: 3 [43136/50048]	Loss: 2.5197
Training Epoch: 3 [43264/50048]	Loss: 2.5831
Training Epoch: 3 [43392/50048]	Loss: 2.4862
Training Epoch: 3 [43520/50048]	Loss: 2.4401
Training Epoch: 3 [43648/50048]	Loss: 2.5262
Training Epoch: 3 [43776/50048]	Loss: 2.4707
Training Epoch: 3 [43904/50048]	Loss: 2.4191
Training Epoch: 3 [44032/50048]	Loss: 2.6375
Training Epoch: 3 [44160/50048]	Loss: 2.6477
Training Epoch: 3 [44288/50048]	Loss: 2.6754
Training Epoch: 3 [44416/50048]	Loss: 2.2287
Training Epoch: 3 [44544/50048]	Loss: 2.2563
Training Epoch: 3 [44672/50048]	Loss: 2.6065
Training Epoch: 3 [44800/50048]	Loss: 2.4419
Training Epoch: 3 [44928/50048]	Loss: 2.3669
Training Epoch: 3 [45056/50048]	Loss: 2.8269
Training Epoch: 3 [45184/50048]	Loss: 2.6039
Training Epoch: 3 [45312/50048]	Loss: 2.4408
Training Epoch: 3 [45440/50048]	Loss: 2.6663
Training Epoch: 3 [45568/50048]	Loss: 2.3929
Training Epoch: 3 [45696/50048]	Loss: 2.4192
Training Epoch: 3 [45824/50048]	Loss: 2.3472
Training Epoch: 3 [45952/50048]	Loss: 2.5948
Training Epoch: 3 [46080/50048]	Loss: 2.7209
Training Epoch: 3 [46208/50048]	Loss: 2.4484
Training Epoch: 3 [46336/50048]	Loss: 2.4658
Training Epoch: 3 [46464/50048]	Loss: 2.4396
Training Epoch: 3 [46592/50048]	Loss: 2.1052
Training Epoch: 3 [46720/50048]	Loss: 2.4881
2022-12-06 06:13:51,013 [ZeusDataLoader(train)] train epoch 4 done: time=86.49 energy=10501.46
2022-12-06 06:13:51,015 [ZeusDataLoader(eval)] Epoch 4 begin.
Training Epoch: 3 [46848/50048]	Loss: 2.4639
Training Epoch: 3 [46976/50048]	Loss: 2.5743
Training Epoch: 3 [47104/50048]	Loss: 2.3448
Training Epoch: 3 [47232/50048]	Loss: 2.5678
Training Epoch: 3 [47360/50048]	Loss: 2.6569
Training Epoch: 3 [47488/50048]	Loss: 2.6043
Training Epoch: 3 [47616/50048]	Loss: 2.5540
Training Epoch: 3 [47744/50048]	Loss: 2.5934
Training Epoch: 3 [47872/50048]	Loss: 2.6200
Training Epoch: 3 [48000/50048]	Loss: 2.5251
Training Epoch: 3 [48128/50048]	Loss: 2.8197
Training Epoch: 3 [48256/50048]	Loss: 2.2697
Training Epoch: 3 [48384/50048]	Loss: 2.5401
Training Epoch: 3 [48512/50048]	Loss: 2.5376
Training Epoch: 3 [48640/50048]	Loss: 2.6447
Training Epoch: 3 [48768/50048]	Loss: 2.6939
Training Epoch: 3 [48896/50048]	Loss: 2.6244
Training Epoch: 3 [49024/50048]	Loss: 2.4568
Training Epoch: 3 [49152/50048]	Loss: 2.6953
Training Epoch: 3 [49280/50048]	Loss: 2.4959
Training Epoch: 3 [49408/50048]	Loss: 2.3904
Training Epoch: 3 [49536/50048]	Loss: 2.6482
Training Epoch: 3 [49664/50048]	Loss: 2.4573
Training Epoch: 3 [49792/50048]	Loss: 2.4932
Training Epoch: 3 [49920/50048]	Loss: 2.6125
Training Epoch: 3 [50048/50048]	Loss: 2.3177
2022-12-06 11:13:54.730 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:13:54,762 [ZeusDataLoader(eval)] eval epoch 4 done: time=3.74 energy=453.68
2022-12-06 06:13:54,762 [ZeusDataLoader(train)] Up to epoch 4: time=361.07, energy=43779.82, cost=53483.37
2022-12-06 06:13:54,762 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:13:54,762 [ZeusDataLoader(train)] Expected next epoch: time=450.87, energy=54577.84, cost=66739.76
2022-12-06 06:13:54,763 [ZeusDataLoader(train)] Epoch 5 begin.
Validation Epoch: 3, Average loss: 0.0197, Accuracy: 0.3408
2022-12-06 06:13:54,964 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:13:54,965 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:13:54.967 [ZeusMonitor] Monitor started.
2022-12-06 11:13:54.967 [ZeusMonitor] Running indefinitely. 2022-12-06 11:13:54.967 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:13:54.967 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e5+gpu0.power.log
Training Epoch: 4 [128/50048]	Loss: 2.4863
Training Epoch: 4 [256/50048]	Loss: 2.4170
Training Epoch: 4 [384/50048]	Loss: 2.6715
Training Epoch: 4 [512/50048]	Loss: 2.3870
Training Epoch: 4 [640/50048]	Loss: 2.5135
Training Epoch: 4 [768/50048]	Loss: 2.4560
Training Epoch: 4 [896/50048]	Loss: 2.4741
Training Epoch: 4 [1024/50048]	Loss: 2.5089
Training Epoch: 4 [1152/50048]	Loss: 2.4166
Training Epoch: 4 [1280/50048]	Loss: 2.1327
Training Epoch: 4 [1408/50048]	Loss: 2.1395
Training Epoch: 4 [1536/50048]	Loss: 2.3016
Training Epoch: 4 [1664/50048]	Loss: 2.5100
Training Epoch: 4 [1792/50048]	Loss: 2.4443
Training Epoch: 4 [1920/50048]	Loss: 2.2144
Training Epoch: 4 [2048/50048]	Loss: 2.2915
Training Epoch: 4 [2176/50048]	Loss: 2.4643
Training Epoch: 4 [2304/50048]	Loss: 2.2893
Training Epoch: 4 [2432/50048]	Loss: 2.4720
Training Epoch: 4 [2560/50048]	Loss: 2.3400
Training Epoch: 4 [2688/50048]	Loss: 2.3749
Training Epoch: 4 [2816/50048]	Loss: 2.4339
Training Epoch: 4 [2944/50048]	Loss: 2.1467
Training Epoch: 4 [3072/50048]	Loss: 2.4647
Training Epoch: 4 [3200/50048]	Loss: 2.6038
Training Epoch: 4 [3328/50048]	Loss: 2.5104
Training Epoch: 4 [3456/50048]	Loss: 2.3138
Training Epoch: 4 [3584/50048]	Loss: 2.4985
Training Epoch: 4 [3712/50048]	Loss: 2.3640
Training Epoch: 4 [3840/50048]	Loss: 2.3304
Training Epoch: 4 [3968/50048]	Loss: 2.3389
Training Epoch: 4 [4096/50048]	Loss: 2.3702
Training Epoch: 4 [4224/50048]	Loss: 2.3659
Training Epoch: 4 [4352/50048]	Loss: 2.1476
Training Epoch: 4 [4480/50048]	Loss: 2.5480
Training Epoch: 4 [4608/50048]	Loss: 2.3207
Training Epoch: 4 [4736/50048]	Loss: 2.4823
Training Epoch: 4 [4864/50048]	Loss: 2.4698
Training Epoch: 4 [4992/50048]	Loss: 2.0827
Training Epoch: 4 [5120/50048]	Loss: 2.4218
Training Epoch: 4 [5248/50048]	Loss: 2.2695
Training Epoch: 4 [5376/50048]	Loss: 2.5247
Training Epoch: 4 [5504/50048]	Loss: 2.2596
Training Epoch: 4 [5632/50048]	Loss: 2.5740
Training Epoch: 4 [5760/50048]	Loss: 2.2450
Training Epoch: 4 [5888/50048]	Loss: 2.3951
Training Epoch: 4 [6016/50048]	Loss: 2.6239
Training Epoch: 4 [6144/50048]	Loss: 2.6219
Training Epoch: 4 [6272/50048]	Loss: 2.1755
Training Epoch: 4 [6400/50048]	Loss: 2.1854
Training Epoch: 4 [6528/50048]	Loss: 2.2497
Training Epoch: 4 [6656/50048]	Loss: 2.4122
Training Epoch: 4 [6784/50048]	Loss: 2.3938
Training Epoch: 4 [6912/50048]	Loss: 2.4920
Training Epoch: 4 [7040/50048]	Loss: 2.1859
Training Epoch: 4 [7168/50048]	Loss: 2.4871
Training Epoch: 4 [7296/50048]	Loss: 2.3852
Training Epoch: 4 [7424/50048]	Loss: 2.3845
Training Epoch: 4 [7552/50048]	Loss: 2.5218
Training Epoch: 4 [7680/50048]	Loss: 2.5041
Training Epoch: 4 [7808/50048]	Loss: 2.5886
Training Epoch: 4 [7936/50048]	Loss: 2.5971
Training Epoch: 4 [8064/50048]	Loss: 2.4462
Training Epoch: 4 [8192/50048]	Loss: 2.4829
Training Epoch: 4 [8320/50048]	Loss: 2.3198
Training Epoch: 4 [8448/50048]	Loss: 2.5100
Training Epoch: 4 [8576/50048]	Loss: 2.3582
Training Epoch: 4 [8704/50048]	Loss: 2.3780
Training Epoch: 4 [8832/50048]	Loss: 2.3057
Training Epoch: 4 [8960/50048]	Loss: 2.5111
Training Epoch: 4 [9088/50048]	Loss: 2.3510
Training Epoch: 4 [9216/50048]	Loss: 2.5690
Training Epoch: 4 [9344/50048]	Loss: 2.3792
Training Epoch: 4 [9472/50048]	Loss: 2.4856
Training Epoch: 4 [9600/50048]	Loss: 2.2717
Training Epoch: 4 [9728/50048]	Loss: 2.3552
Training Epoch: 4 [9856/50048]	Loss: 2.5479
Training Epoch: 4 [9984/50048]	Loss: 2.5480
Training Epoch: 4 [10112/50048]	Loss: 2.3441
Training Epoch: 4 [10240/50048]	Loss: 2.2969
Training Epoch: 4 [10368/50048]	Loss: 2.5659
Training Epoch: 4 [10496/50048]	Loss: 2.6388
Training Epoch: 4 [10624/50048]	Loss: 2.2727
Training Epoch: 4 [10752/50048]	Loss: 2.3844
Training Epoch: 4 [10880/50048]	Loss: 2.4436
Training Epoch: 4 [11008/50048]	Loss: 2.4741
Training Epoch: 4 [11136/50048]	Loss: 2.2798
Training Epoch: 4 [11264/50048]	Loss: 2.5383
Training Epoch: 4 [11392/50048]	Loss: 2.2900
Training Epoch: 4 [11520/50048]	Loss: 2.2922
Training Epoch: 4 [11648/50048]	Loss: 2.2462
Training Epoch: 4 [11776/50048]	Loss: 2.5648
Training Epoch: 4 [11904/50048]	Loss: 2.4362
Training Epoch: 4 [12032/50048]	Loss: 2.3372
Training Epoch: 4 [12160/50048]	Loss: 2.3591
Training Epoch: 4 [12288/50048]	Loss: 2.5826
Training Epoch: 4 [12416/50048]	Loss: 2.5503
Training Epoch: 4 [12544/50048]	Loss: 2.5627
Training Epoch: 4 [12672/50048]	Loss: 2.4119
Training Epoch: 4 [12800/50048]	Loss: 2.3909
Training Epoch: 4 [12928/50048]	Loss: 2.2265
Training Epoch: 4 [13056/50048]	Loss: 2.3552
Training Epoch: 4 [13184/50048]	Loss: 2.3937
Training Epoch: 4 [13312/50048]	Loss: 2.4158
Training Epoch: 4 [13440/50048]	Loss: 2.3009
Training Epoch: 4 [13568/50048]	Loss: 2.3429
Training Epoch: 4 [13696/50048]	Loss: 2.4116
Training Epoch: 4 [13824/50048]	Loss: 2.5128
Training Epoch: 4 [13952/50048]	Loss: 2.7582
Training Epoch: 4 [14080/50048]	Loss: 2.4205
Training Epoch: 4 [14208/50048]	Loss: 2.2983
Training Epoch: 4 [14336/50048]	Loss: 2.3727
Training Epoch: 4 [14464/50048]	Loss: 2.4565
Training Epoch: 4 [14592/50048]	Loss: 2.1729
Training Epoch: 4 [14720/50048]	Loss: 2.5249
Training Epoch: 4 [14848/50048]	Loss: 2.4593
Training Epoch: 4 [14976/50048]	Loss: 2.3332
Training Epoch: 4 [15104/50048]	Loss: 2.5050
Training Epoch: 4 [15232/50048]	Loss: 2.4445
Training Epoch: 4 [15360/50048]	Loss: 2.4039
Training Epoch: 4 [15488/50048]	Loss: 2.1782
Training Epoch: 4 [15616/50048]	Loss: 2.2846
Training Epoch: 4 [15744/50048]	Loss: 2.1048
Training Epoch: 4 [15872/50048]	Loss: 2.3005
Training Epoch: 4 [16000/50048]	Loss: 2.2419
Training Epoch: 4 [16128/50048]	Loss: 2.3149
Training Epoch: 4 [16256/50048]	Loss: 2.5452
Training Epoch: 4 [16384/50048]	Loss: 2.2673
Training Epoch: 4 [16512/50048]	Loss: 2.6295
Training Epoch: 4 [16640/50048]	Loss: 2.4406
Training Epoch: 4 [16768/50048]	Loss: 2.2166
Training Epoch: 4 [16896/50048]	Loss: 2.4167
Training Epoch: 4 [17024/50048]	Loss: 2.3275
Training Epoch: 4 [17152/50048]	Loss: 2.4482
Training Epoch: 4 [17280/50048]	Loss: 2.4366
Training Epoch: 4 [17408/50048]	Loss: 2.3878
Training Epoch: 4 [17536/50048]	Loss: 2.3457
Training Epoch: 4 [17664/50048]	Loss: 2.3768
Training Epoch: 4 [17792/50048]	Loss: 2.7240
Training Epoch: 4 [17920/50048]	Loss: 2.2378
Training Epoch: 4 [18048/50048]	Loss: 2.2476
Training Epoch: 4 [18176/50048]	Loss: 2.5229
Training Epoch: 4 [18304/50048]	Loss: 2.2105
Training Epoch: 4 [18432/50048]	Loss: 2.2491
Training Epoch: 4 [18560/50048]	Loss: 2.4390
Training Epoch: 4 [18688/50048]	Loss: 2.5382
Training Epoch: 4 [18816/50048]	Loss: 2.0565
Training Epoch: 4 [18944/50048]	Loss: 2.2200
Training Epoch: 4 [19072/50048]	Loss: 2.4021
Training Epoch: 4 [19200/50048]	Loss: 2.6041
Training Epoch: 4 [19328/50048]	Loss: 2.1869
Training Epoch: 4 [19456/50048]	Loss: 2.4803
Training Epoch: 4 [19584/50048]	Loss: 2.4055
Training Epoch: 4 [19712/50048]	Loss: 2.3732
Training Epoch: 4 [19840/50048]	Loss: 2.3367
Training Epoch: 4 [19968/50048]	Loss: 2.4143
Training Epoch: 4 [20096/50048]	Loss: 2.3188
Training Epoch: 4 [20224/50048]	Loss: 2.4162
Training Epoch: 4 [20352/50048]	Loss: 2.1892
Training Epoch: 4 [20480/50048]	Loss: 2.6146
Training Epoch: 4 [20608/50048]	Loss: 2.4196
Training Epoch: 4 [20736/50048]	Loss: 2.3623
Training Epoch: 4 [20864/50048]	Loss: 2.2749
Training Epoch: 4 [20992/50048]	Loss: 2.2330
Training Epoch: 4 [21120/50048]	Loss: 2.3962
Training Epoch: 4 [21248/50048]	Loss: 2.3553
Training Epoch: 4 [21376/50048]	Loss: 2.3038
Training Epoch: 4 [21504/50048]	Loss: 2.6575
Training Epoch: 4 [21632/50048]	Loss: 2.2652
Training Epoch: 4 [21760/50048]	Loss: 2.4380
Training Epoch: 4 [21888/50048]	Loss: 2.6167
Training Epoch: 4 [22016/50048]	Loss: 2.5044
Training Epoch: 4 [22144/50048]	Loss: 2.2894
Training Epoch: 4 [22272/50048]	Loss: 2.3023
Training Epoch: 4 [22400/50048]	Loss: 1.9802
Training Epoch: 4 [22528/50048]	Loss: 2.3036
Training Epoch: 4 [22656/50048]	Loss: 2.5266
Training Epoch: 4 [22784/50048]	Loss: 2.5192
Training Epoch: 4 [22912/50048]	Loss: 2.1154
Training Epoch: 4 [23040/50048]	Loss: 2.3685
Training Epoch: 4 [23168/50048]	Loss: 2.0259
Training Epoch: 4 [23296/50048]	Loss: 2.5183
Training Epoch: 4 [23424/50048]	Loss: 2.5535
Training Epoch: 4 [23552/50048]	Loss: 2.5152
Training Epoch: 4 [23680/50048]	Loss: 2.3116
Training Epoch: 4 [23808/50048]	Loss: 2.4919
Training Epoch: 4 [23936/50048]	Loss: 2.6639
Training Epoch: 4 [24064/50048]	Loss: 2.3558
Training Epoch: 4 [24192/50048]	Loss: 2.3146
Training Epoch: 4 [24320/50048]	Loss: 2.3891
Training Epoch: 4 [24448/50048]	Loss: 2.3835
Training Epoch: 4 [24576/50048]	Loss: 2.3886
Training Epoch: 4 [24704/50048]	Loss: 2.3411
Training Epoch: 4 [24832/50048]	Loss: 2.4096
Training Epoch: 4 [24960/50048]	Loss: 2.2547
Training Epoch: 4 [25088/50048]	Loss: 2.4008
Training Epoch: 4 [25216/50048]	Loss: 2.2797
Training Epoch: 4 [25344/50048]	Loss: 2.4121
Training Epoch: 4 [25472/50048]	Loss: 2.2108
Training Epoch: 4 [25600/50048]	Loss: 2.1043
Training Epoch: 4 [25728/50048]	Loss: 2.1619
Training Epoch: 4 [25856/50048]	Loss: 2.3851
Training Epoch: 4 [25984/50048]	Loss: 2.3185
Training Epoch: 4 [26112/50048]	Loss: 2.4885
Training Epoch: 4 [26240/50048]	Loss: 2.3292
Training Epoch: 4 [26368/50048]	Loss: 2.4657
Training Epoch: 4 [26496/50048]	Loss: 2.3335
Training Epoch: 4 [26624/50048]	Loss: 2.1398
Training Epoch: 4 [26752/50048]	Loss: 2.4355
Training Epoch: 4 [26880/50048]	Loss: 2.5093
Training Epoch: 4 [27008/50048]	Loss: 2.3941
Training Epoch: 4 [27136/50048]	Loss: 2.5065
Training Epoch: 4 [27264/50048]	Loss: 2.2449
Training Epoch: 4 [27392/50048]	Loss: 2.3173
Training Epoch: 4 [27520/50048]	Loss: 2.4531
Training Epoch: 4 [27648/50048]	Loss: 2.5625
Training Epoch: 4 [27776/50048]	Loss: 2.2738
Training Epoch: 4 [27904/50048]	Loss: 2.5174
Training Epoch: 4 [28032/50048]	Loss: 2.3698
Training Epoch: 4 [28160/50048]	Loss: 2.2550
Training Epoch: 4 [28288/50048]	Loss: 2.3174
Training Epoch: 4 [28416/50048]	Loss: 2.3219
Training Epoch: 4 [28544/50048]	Loss: 2.2533
Training Epoch: 4 [28672/50048]	Loss: 2.5027
Training Epoch: 4 [28800/50048]	Loss: 2.5627
Training Epoch: 4 [28928/50048]	Loss: 2.2031
Training Epoch: 4 [29056/50048]	Loss: 2.4837
Training Epoch: 4 [29184/50048]	Loss: 2.3314
Training Epoch: 4 [29312/50048]	Loss: 2.2480
Training Epoch: 4 [29440/50048]	Loss: 2.2462
Training Epoch: 4 [29568/50048]	Loss: 2.2470
Training Epoch: 4 [29696/50048]	Loss: 2.3282
Training Epoch: 4 [29824/50048]	Loss: 2.1895
Training Epoch: 4 [29952/50048]	Loss: 2.3973
Training Epoch: 4 [30080/50048]	Loss: 2.3779
Training Epoch: 4 [30208/50048]	Loss: 2.6182
Training Epoch: 4 [30336/50048]	Loss: 2.3992
Training Epoch: 4 [30464/50048]	Loss: 2.4079
Training Epoch: 4 [30592/50048]	Loss: 2.2315
Training Epoch: 4 [30720/50048]	Loss: 2.1243
Training Epoch: 4 [30848/50048]	Loss: 2.2209
Training Epoch: 4 [30976/50048]	Loss: 2.4919
Training Epoch: 4 [31104/50048]	Loss: 2.4182
Training Epoch: 4 [31232/50048]	Loss: 2.1636
Training Epoch: 4 [31360/50048]	Loss: 2.1629
Training Epoch: 4 [31488/50048]	Loss: 2.1929
Training Epoch: 4 [31616/50048]	Loss: 2.2703
Training Epoch: 4 [31744/50048]	Loss: 2.4034
Training Epoch: 4 [31872/50048]	Loss: 2.4128
Training Epoch: 4 [32000/50048]	Loss: 2.5069
Training Epoch: 4 [32128/50048]	Loss: 2.1675
Training Epoch: 4 [32256/50048]	Loss: 2.4215
Training Epoch: 4 [32384/50048]	Loss: 2.6330
Training Epoch: 4 [32512/50048]	Loss: 2.0363
Training Epoch: 4 [32640/50048]	Loss: 2.5095
Training Epoch: 4 [32768/50048]	Loss: 2.1450
Training Epoch: 4 [32896/50048]	Loss: 2.6534
Training Epoch: 4 [33024/50048]	Loss: 2.4243
Training Epoch: 4 [33152/50048]	Loss: 2.2799
Training Epoch: 4 [33280/50048]	Loss: 2.0436
Training Epoch: 4 [33408/50048]	Loss: 2.2129
Training Epoch: 4 [33536/50048]	Loss: 2.3959
Training Epoch: 4 [33664/50048]	Loss: 2.2178
Training Epoch: 4 [33792/50048]	Loss: 2.2050
Training Epoch: 4 [33920/50048]	Loss: 2.0155
Training Epoch: 4 [34048/50048]	Loss: 2.2070
Training Epoch: 4 [34176/50048]	Loss: 2.2908
Training Epoch: 4 [34304/50048]	Loss: 2.4208
Training Epoch: 4 [34432/50048]	Loss: 2.2523
Training Epoch: 4 [34560/50048]	Loss: 2.2234
Training Epoch: 4 [34688/50048]	Loss: 2.1340
Training Epoch: 4 [34816/50048]	Loss: 2.2292
Training Epoch: 4 [34944/50048]	Loss: 2.4724
Training Epoch: 4 [35072/50048]	Loss: 2.2436
Training Epoch: 4 [35200/50048]	Loss: 2.3541
Training Epoch: 4 [35328/50048]	Loss: 2.0027
Training Epoch: 4 [35456/50048]	Loss: 2.4388
Training Epoch: 4 [35584/50048]	Loss: 2.1928
Training Epoch: 4 [35712/50048]	Loss: 2.1764
Training Epoch: 4 [35840/50048]	Loss: 2.4323
Training Epoch: 4 [35968/50048]	Loss: 2.0742
Training Epoch: 4 [36096/50048]	Loss: 2.3441
Training Epoch: 4 [36224/50048]	Loss: 2.1980
Training Epoch: 4 [36352/50048]	Loss: 2.3695
Training Epoch: 4 [36480/50048]	Loss: 2.6378
Training Epoch: 4 [36608/50048]	Loss: 2.1628
Training Epoch: 4 [36736/50048]	Loss: 2.2621
Training Epoch: 4 [36864/50048]	Loss: 2.2876
Training Epoch: 4 [36992/50048]	Loss: 2.5069
Training Epoch: 4 [37120/50048]	Loss: 2.5564
Training Epoch: 4 [37248/50048]	Loss: 2.4246
Training Epoch: 4 [37376/50048]	Loss: 2.6180
Training Epoch: 4 [37504/50048]	Loss: 2.4057
Training Epoch: 4 [37632/50048]	Loss: 2.5100
Training Epoch: 4 [37760/50048]	Loss: 2.2535
Training Epoch: 4 [37888/50048]	Loss: 2.4279
Training Epoch: 4 [38016/50048]	Loss: 2.3073
Training Epoch: 4 [38144/50048]	Loss: 2.3312
Training Epoch: 4 [38272/50048]	Loss: 2.1781
Training Epoch: 4 [38400/50048]	Loss: 2.1814
Training Epoch: 4 [38528/50048]	Loss: 2.4484
Training Epoch: 4 [38656/50048]	Loss: 2.1390
Training Epoch: 4 [38784/50048]	Loss: 2.2249
Training Epoch: 4 [38912/50048]	Loss: 2.3690
Training Epoch: 4 [39040/50048]	Loss: 1.9773
Training Epoch: 4 [39168/50048]	Loss: 2.1800
Training Epoch: 4 [39296/50048]	Loss: 2.2744
Training Epoch: 4 [39424/50048]	Loss: 2.3732
Training Epoch: 4 [39552/50048]	Loss: 2.3673
Training Epoch: 4 [39680/50048]	Loss: 2.2387
Training Epoch: 4 [39808/50048]	Loss: 2.3288
Training Epoch: 4 [39936/50048]	Loss: 2.2484
Training Epoch: 4 [40064/50048]	Loss: 2.3548
Training Epoch: 4 [40192/50048]	Loss: 2.2224
Training Epoch: 4 [40320/50048]	Loss: 2.2535
Training Epoch: 4 [40448/50048]	Loss: 2.4807
Training Epoch: 4 [40576/50048]	Loss: 2.1074
Training Epoch: 4 [40704/50048]	Loss: 2.1550
Training Epoch: 4 [40832/50048]	Loss: 2.0153
Training Epoch: 4 [40960/50048]	Loss: 2.3030
Training Epoch: 4 [41088/50048]	Loss: 2.0248
Training Epoch: 4 [41216/50048]	Loss: 2.4356
Training Epoch: 4 [41344/50048]	Loss: 2.2316
Training Epoch: 4 [41472/50048]	Loss: 2.3324
Training Epoch: 4 [41600/50048]	Loss: 2.3529
Training Epoch: 4 [41728/50048]	Loss: 2.0906
Training Epoch: 4 [41856/50048]	Loss: 2.2541
Training Epoch: 4 [41984/50048]	Loss: 2.2530
Training Epoch: 4 [42112/50048]	Loss: 2.1385
Training Epoch: 4 [42240/50048]	Loss: 2.2174
Training Epoch: 4 [42368/50048]	Loss: 2.5498
Training Epoch: 4 [42496/50048]	Loss: 2.2655
Training Epoch: 4 [42624/50048]	Loss: 2.4279
Training Epoch: 4 [42752/50048]	Loss: 2.3351
Training Epoch: 4 [42880/50048]	Loss: 2.2210
Training Epoch: 4 [43008/50048]	Loss: 2.5702
Training Epoch: 4 [43136/50048]	Loss: 2.1887
Training Epoch: 4 [43264/50048]	Loss: 2.1479
Training Epoch: 4 [43392/50048]	Loss: 2.4961
Training Epoch: 4 [43520/50048]	Loss: 2.0773
Training Epoch: 4 [43648/50048]	Loss: 2.0822
Training Epoch: 4 [43776/50048]	Loss: 1.9288
Training Epoch: 4 [43904/50048]	Loss: 2.2270
Training Epoch: 4 [44032/50048]	Loss: 2.5936
Training Epoch: 4 [44160/50048]	Loss: 2.5498
Training Epoch: 4 [44288/50048]	Loss: 2.1709
Training Epoch: 4 [44416/50048]	Loss: 2.3404
Training Epoch: 4 [44544/50048]	Loss: 2.3241
Training Epoch: 4 [44672/50048]	Loss: 2.2164
Training Epoch: 4 [44800/50048]	Loss: 2.3525
Training Epoch: 4 [44928/50048]	Loss: 2.3501
Training Epoch: 4 [45056/50048]	Loss: 2.0967
Training Epoch: 4 [45184/50048]	Loss: 2.3383
Training Epoch: 4 [45312/50048]	Loss: 2.3964
Training Epoch: 4 [45440/50048]	Loss: 2.4047
Training Epoch: 4 [45568/50048]	Loss: 2.3653
Training Epoch: 4 [45696/50048]	Loss: 2.2391
Training Epoch: 4 [45824/50048]	Loss: 2.2748
Training Epoch: 4 [45952/50048]	Loss: 2.2377
Training Epoch: 4 [46080/50048]	Loss: 2.2212
Training Epoch: 4 [46208/50048]	Loss: 2.3991
Training Epoch: 4 [46336/50048]	Loss: 2.0863
Training Epoch: 4 [46464/50048]	Loss: 2.4904
Training Epoch: 4 [46592/50048]	Loss: 2.3238
Training Epoch: 4 [46720/50048]	Loss: 2.4375
2022-12-06 06:15:21,227 [ZeusDataLoader(train)] train epoch 5 done: time=86.45 energy=10505.97
2022-12-06 06:15:21,228 [ZeusDataLoader(eval)] Epoch 5 begin.
Training Epoch: 4 [46848/50048]	Loss: 2.2658
Training Epoch: 4 [46976/50048]	Loss: 2.4370
Training Epoch: 4 [47104/50048]	Loss: 2.2744
Training Epoch: 4 [47232/50048]	Loss: 2.5526
Training Epoch: 4 [47360/50048]	Loss: 2.4039
Training Epoch: 4 [47488/50048]	Loss: 2.1021
Training Epoch: 4 [47616/50048]	Loss: 2.2200
Training Epoch: 4 [47744/50048]	Loss: 2.2165
Training Epoch: 4 [47872/50048]	Loss: 2.2165
Training Epoch: 4 [48000/50048]	Loss: 2.3103
Training Epoch: 4 [48128/50048]	Loss: 2.4751
Training Epoch: 4 [48256/50048]	Loss: 2.2783
Training Epoch: 4 [48384/50048]	Loss: 2.4914
Training Epoch: 4 [48512/50048]	Loss: 2.1513
Training Epoch: 4 [48640/50048]	Loss: 2.4023
Training Epoch: 4 [48768/50048]	Loss: 2.1076
Training Epoch: 4 [48896/50048]	Loss: 2.5764
Training Epoch: 4 [49024/50048]	Loss: 2.1534
Training Epoch: 4 [49152/50048]	Loss: 2.0949
Training Epoch: 4 [49280/50048]	Loss: 2.3640
Training Epoch: 4 [49408/50048]	Loss: 2.2887
Training Epoch: 4 [49536/50048]	Loss: 2.5120
Training Epoch: 4 [49664/50048]	Loss: 2.1582
Training Epoch: 4 [49792/50048]	Loss: 2.2460
Training Epoch: 4 [49920/50048]	Loss: 2.5303
Training Epoch: 4 [50048/50048]	Loss: 2.2899
2022-12-06 11:15:24.867 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:15:24,892 [ZeusDataLoader(eval)] eval epoch 5 done: time=3.66 energy=442.26
2022-12-06 06:15:24,892 [ZeusDataLoader(train)] Up to epoch 5: time=451.18, energy=54728.05, cost=66842.01
2022-12-06 06:15:24,893 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:15:24,893 [ZeusDataLoader(train)] Expected next epoch: time=540.98, energy=65526.06, cost=80098.39
2022-12-06 06:15:24,894 [ZeusDataLoader(train)] Epoch 6 begin.
Validation Epoch: 4, Average loss: 0.0185, Accuracy: 0.3765
2022-12-06 06:15:25,085 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:15:25,085 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:15:25.095 [ZeusMonitor] Monitor started.
2022-12-06 11:15:25.095 [ZeusMonitor] Running indefinitely. 2022-12-06 11:15:25.095 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:15:25.095 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e6+gpu0.power.log
Training Epoch: 5 [128/50048]	Loss: 2.1907
Training Epoch: 5 [256/50048]	Loss: 2.2370
Training Epoch: 5 [384/50048]	Loss: 1.9640
Training Epoch: 5 [512/50048]	Loss: 2.0384
Training Epoch: 5 [640/50048]	Loss: 2.0489
Training Epoch: 5 [768/50048]	Loss: 1.9108
Training Epoch: 5 [896/50048]	Loss: 2.1062
Training Epoch: 5 [1024/50048]	Loss: 1.9831
Training Epoch: 5 [1152/50048]	Loss: 2.2665
Training Epoch: 5 [1280/50048]	Loss: 2.2329
Training Epoch: 5 [1408/50048]	Loss: 2.3446
Training Epoch: 5 [1536/50048]	Loss: 2.2410
Training Epoch: 5 [1664/50048]	Loss: 2.0200
Training Epoch: 5 [1792/50048]	Loss: 2.2294
Training Epoch: 5 [1920/50048]	Loss: 1.7940
Training Epoch: 5 [2048/50048]	Loss: 2.1623
Training Epoch: 5 [2176/50048]	Loss: 2.2633
Training Epoch: 5 [2304/50048]	Loss: 2.1213
Training Epoch: 5 [2432/50048]	Loss: 2.2745
Training Epoch: 5 [2560/50048]	Loss: 2.2895
Training Epoch: 5 [2688/50048]	Loss: 2.3722
Training Epoch: 5 [2816/50048]	Loss: 2.4708
Training Epoch: 5 [2944/50048]	Loss: 2.2796
Training Epoch: 5 [3072/50048]	Loss: 2.3074
Training Epoch: 5 [3200/50048]	Loss: 2.1197
Training Epoch: 5 [3328/50048]	Loss: 2.3702
Training Epoch: 5 [3456/50048]	Loss: 1.9850
Training Epoch: 5 [3584/50048]	Loss: 2.1469
Training Epoch: 5 [3712/50048]	Loss: 2.2032
Training Epoch: 5 [3840/50048]	Loss: 2.2062
Training Epoch: 5 [3968/50048]	Loss: 1.8327
Training Epoch: 5 [4096/50048]	Loss: 2.2772
Training Epoch: 5 [4224/50048]	Loss: 2.0833
Training Epoch: 5 [4352/50048]	Loss: 2.0472
Training Epoch: 5 [4480/50048]	Loss: 2.0750
Training Epoch: 5 [4608/50048]	Loss: 2.3136
Training Epoch: 5 [4736/50048]	Loss: 2.0720
Training Epoch: 5 [4864/50048]	Loss: 2.0613
Training Epoch: 5 [4992/50048]	Loss: 2.2366
Training Epoch: 5 [5120/50048]	Loss: 2.4988
Training Epoch: 5 [5248/50048]	Loss: 1.8935
Training Epoch: 5 [5376/50048]	Loss: 2.3671
Training Epoch: 5 [5504/50048]	Loss: 2.1317
Training Epoch: 5 [5632/50048]	Loss: 2.5443
Training Epoch: 5 [5760/50048]	Loss: 2.1900
Training Epoch: 5 [5888/50048]	Loss: 2.1816
Training Epoch: 5 [6016/50048]	Loss: 2.1515
Training Epoch: 5 [6144/50048]	Loss: 2.2660
Training Epoch: 5 [6272/50048]	Loss: 2.3442
Training Epoch: 5 [6400/50048]	Loss: 2.1817
Training Epoch: 5 [6528/50048]	Loss: 1.9806
Training Epoch: 5 [6656/50048]	Loss: 1.9047
Training Epoch: 5 [6784/50048]	Loss: 2.3679
Training Epoch: 5 [6912/50048]	Loss: 2.1329
Training Epoch: 5 [7040/50048]	Loss: 2.2740
Training Epoch: 5 [7168/50048]	Loss: 2.1838
Training Epoch: 5 [7296/50048]	Loss: 2.1671
Training Epoch: 5 [7424/50048]	Loss: 2.2955
Training Epoch: 5 [7552/50048]	Loss: 2.0482
Training Epoch: 5 [7680/50048]	Loss: 2.1629
Training Epoch: 5 [7808/50048]	Loss: 2.0451
Training Epoch: 5 [7936/50048]	Loss: 2.2464
Training Epoch: 5 [8064/50048]	Loss: 2.2222
Training Epoch: 5 [8192/50048]	Loss: 2.0880
Training Epoch: 5 [8320/50048]	Loss: 2.1272
Training Epoch: 5 [8448/50048]	Loss: 1.9553
Training Epoch: 5 [8576/50048]	Loss: 2.2263
Training Epoch: 5 [8704/50048]	Loss: 2.3122
Training Epoch: 5 [8832/50048]	Loss: 2.0940
Training Epoch: 5 [8960/50048]	Loss: 2.2889
Training Epoch: 5 [9088/50048]	Loss: 1.9951
Training Epoch: 5 [9216/50048]	Loss: 2.1050
Training Epoch: 5 [9344/50048]	Loss: 2.0461
Training Epoch: 5 [9472/50048]	Loss: 2.2433
Training Epoch: 5 [9600/50048]	Loss: 2.1441
Training Epoch: 5 [9728/50048]	Loss: 2.0575
Training Epoch: 5 [9856/50048]	Loss: 2.3929
Training Epoch: 5 [9984/50048]	Loss: 2.2118
Training Epoch: 5 [10112/50048]	Loss: 2.4404
Training Epoch: 5 [10240/50048]	Loss: 2.2391
Training Epoch: 5 [10368/50048]	Loss: 2.2101
Training Epoch: 5 [10496/50048]	Loss: 2.2493
Training Epoch: 5 [10624/50048]	Loss: 2.1781
Training Epoch: 5 [10752/50048]	Loss: 2.2561
Training Epoch: 5 [10880/50048]	Loss: 2.2753
Training Epoch: 5 [11008/50048]	Loss: 2.1777
Training Epoch: 5 [11136/50048]	Loss: 2.2629
Training Epoch: 5 [11264/50048]	Loss: 2.0103
Training Epoch: 5 [11392/50048]	Loss: 2.1807
Training Epoch: 5 [11520/50048]	Loss: 1.7935
Training Epoch: 5 [11648/50048]	Loss: 2.3411
Training Epoch: 5 [11776/50048]	Loss: 2.1795
Training Epoch: 5 [11904/50048]	Loss: 2.3187
Training Epoch: 5 [12032/50048]	Loss: 2.0747
Training Epoch: 5 [12160/50048]	Loss: 1.9192
Training Epoch: 5 [12288/50048]	Loss: 2.2999
Training Epoch: 5 [12416/50048]	Loss: 2.1476
Training Epoch: 5 [12544/50048]	Loss: 2.3106
Training Epoch: 5 [12672/50048]	Loss: 2.2868
Training Epoch: 5 [12800/50048]	Loss: 2.0362
Training Epoch: 5 [12928/50048]	Loss: 2.2420
Training Epoch: 5 [13056/50048]	Loss: 2.2331
Training Epoch: 5 [13184/50048]	Loss: 2.3388
Training Epoch: 5 [13312/50048]	Loss: 2.1220
Training Epoch: 5 [13440/50048]	Loss: 2.1917
Training Epoch: 5 [13568/50048]	Loss: 1.8004
Training Epoch: 5 [13696/50048]	Loss: 2.2368
Training Epoch: 5 [13824/50048]	Loss: 2.0109
Training Epoch: 5 [13952/50048]	Loss: 1.9914
Training Epoch: 5 [14080/50048]	Loss: 2.3043
Training Epoch: 5 [14208/50048]	Loss: 2.2995
Training Epoch: 5 [14336/50048]	Loss: 2.1127
Training Epoch: 5 [14464/50048]	Loss: 2.4255
Training Epoch: 5 [14592/50048]	Loss: 2.0712
Training Epoch: 5 [14720/50048]	Loss: 2.1730
Training Epoch: 5 [14848/50048]	Loss: 2.2950
Training Epoch: 5 [14976/50048]	Loss: 2.0830
Training Epoch: 5 [15104/50048]	Loss: 2.1954
Training Epoch: 5 [15232/50048]	Loss: 2.4482
Training Epoch: 5 [15360/50048]	Loss: 2.1773
Training Epoch: 5 [15488/50048]	Loss: 1.9772
Training Epoch: 5 [15616/50048]	Loss: 1.9622
Training Epoch: 5 [15744/50048]	Loss: 2.2768
Training Epoch: 5 [15872/50048]	Loss: 2.2192
Training Epoch: 5 [16000/50048]	Loss: 2.4559
Training Epoch: 5 [16128/50048]	Loss: 2.1316
Training Epoch: 5 [16256/50048]	Loss: 2.1100
Training Epoch: 5 [16384/50048]	Loss: 2.3445
Training Epoch: 5 [16512/50048]	Loss: 2.3475
Training Epoch: 5 [16640/50048]	Loss: 2.2135
Training Epoch: 5 [16768/50048]	Loss: 2.3284
Training Epoch: 5 [16896/50048]	Loss: 2.1454
Training Epoch: 5 [17024/50048]	Loss: 2.4533
Training Epoch: 5 [17152/50048]	Loss: 2.2574
Training Epoch: 5 [17280/50048]	Loss: 2.0771
Training Epoch: 5 [17408/50048]	Loss: 2.0062
Training Epoch: 5 [17536/50048]	Loss: 2.3783
Training Epoch: 5 [17664/50048]	Loss: 2.3618
Training Epoch: 5 [17792/50048]	Loss: 2.1566
Training Epoch: 5 [17920/50048]	Loss: 2.2133
Training Epoch: 5 [18048/50048]	Loss: 2.1878
Training Epoch: 5 [18176/50048]	Loss: 1.9979
Training Epoch: 5 [18304/50048]	Loss: 1.9251
Training Epoch: 5 [18432/50048]	Loss: 2.1101
Training Epoch: 5 [18560/50048]	Loss: 2.2311
Training Epoch: 5 [18688/50048]	Loss: 2.5151
Training Epoch: 5 [18816/50048]	Loss: 2.0708
Training Epoch: 5 [18944/50048]	Loss: 2.3423
Training Epoch: 5 [19072/50048]	Loss: 2.0040
Training Epoch: 5 [19200/50048]	Loss: 2.2583
Training Epoch: 5 [19328/50048]	Loss: 1.8746
Training Epoch: 5 [19456/50048]	Loss: 2.0364
Training Epoch: 5 [19584/50048]	Loss: 2.3792
Training Epoch: 5 [19712/50048]	Loss: 2.3175
Training Epoch: 5 [19840/50048]	Loss: 2.2586
Training Epoch: 5 [19968/50048]	Loss: 2.0089
Training Epoch: 5 [20096/50048]	Loss: 2.1508
Training Epoch: 5 [20224/50048]	Loss: 2.2551
Training Epoch: 5 [20352/50048]	Loss: 2.3922
Training Epoch: 5 [20480/50048]	Loss: 2.0994
Training Epoch: 5 [20608/50048]	Loss: 2.1076
Training Epoch: 5 [20736/50048]	Loss: 2.0600
Training Epoch: 5 [20864/50048]	Loss: 2.2272
Training Epoch: 5 [20992/50048]	Loss: 2.2076
Training Epoch: 5 [21120/50048]	Loss: 1.9497
Training Epoch: 5 [21248/50048]	Loss: 2.1612
Training Epoch: 5 [21376/50048]	Loss: 2.3292
Training Epoch: 5 [21504/50048]	Loss: 2.3423
Training Epoch: 5 [21632/50048]	Loss: 2.1135
Training Epoch: 5 [21760/50048]	Loss: 2.2933
Training Epoch: 5 [21888/50048]	Loss: 2.5452
Training Epoch: 5 [22016/50048]	Loss: 2.0865
Training Epoch: 5 [22144/50048]	Loss: 2.2942
Training Epoch: 5 [22272/50048]	Loss: 2.1048
Training Epoch: 5 [22400/50048]	Loss: 2.0655
Training Epoch: 5 [22528/50048]	Loss: 2.3131
Training Epoch: 5 [22656/50048]	Loss: 2.0674
Training Epoch: 5 [22784/50048]	Loss: 2.1580
Training Epoch: 5 [22912/50048]	Loss: 2.2306
Training Epoch: 5 [23040/50048]	Loss: 2.2471
Training Epoch: 5 [23168/50048]	Loss: 1.9839
Training Epoch: 5 [23296/50048]	Loss: 2.2266
Training Epoch: 5 [23424/50048]	Loss: 2.3218
Training Epoch: 5 [23552/50048]	Loss: 2.3304
Training Epoch: 5 [23680/50048]	Loss: 2.0681
Training Epoch: 5 [23808/50048]	Loss: 2.5133
Training Epoch: 5 [23936/50048]	Loss: 2.1417
Training Epoch: 5 [24064/50048]	Loss: 2.2145
Training Epoch: 5 [24192/50048]	Loss: 2.1835
Training Epoch: 5 [24320/50048]	Loss: 2.1738
Training Epoch: 5 [24448/50048]	Loss: 2.2092
Training Epoch: 5 [24576/50048]	Loss: 2.0954
Training Epoch: 5 [24704/50048]	Loss: 2.1140
Training Epoch: 5 [24832/50048]	Loss: 1.9938
Training Epoch: 5 [24960/50048]	Loss: 2.1728
Training Epoch: 5 [25088/50048]	Loss: 2.2145
Training Epoch: 5 [25216/50048]	Loss: 2.0106
Training Epoch: 5 [25344/50048]	Loss: 2.1562
Training Epoch: 5 [25472/50048]	Loss: 2.1382
Training Epoch: 5 [25600/50048]	Loss: 2.2783
Training Epoch: 5 [25728/50048]	Loss: 2.3217
Training Epoch: 5 [25856/50048]	Loss: 2.0957
Training Epoch: 5 [25984/50048]	Loss: 1.9621
Training Epoch: 5 [26112/50048]	Loss: 1.8841
Training Epoch: 5 [26240/50048]	Loss: 2.1683
Training Epoch: 5 [26368/50048]	Loss: 2.0099
Training Epoch: 5 [26496/50048]	Loss: 1.9280
Training Epoch: 5 [26624/50048]	Loss: 2.2982
Training Epoch: 5 [26752/50048]	Loss: 2.4988
Training Epoch: 5 [26880/50048]	Loss: 2.0713
Training Epoch: 5 [27008/50048]	Loss: 2.2141
Training Epoch: 5 [27136/50048]	Loss: 2.0096
Training Epoch: 5 [27264/50048]	Loss: 2.1726
Training Epoch: 5 [27392/50048]	Loss: 2.2210
Training Epoch: 5 [27520/50048]	Loss: 2.2977
Training Epoch: 5 [27648/50048]	Loss: 1.9458
Training Epoch: 5 [27776/50048]	Loss: 1.9173
Training Epoch: 5 [27904/50048]	Loss: 2.3563
Training Epoch: 5 [28032/50048]	Loss: 2.0566
Training Epoch: 5 [28160/50048]	Loss: 2.0824
Training Epoch: 5 [28288/50048]	Loss: 2.0434
Training Epoch: 5 [28416/50048]	Loss: 1.9698
Training Epoch: 5 [28544/50048]	Loss: 2.0379
Training Epoch: 5 [28672/50048]	Loss: 2.5542
Training Epoch: 5 [28800/50048]	Loss: 1.9792
Training Epoch: 5 [28928/50048]	Loss: 2.1291
Training Epoch: 5 [29056/50048]	Loss: 2.3624
Training Epoch: 5 [29184/50048]	Loss: 2.0870
Training Epoch: 5 [29312/50048]	Loss: 2.2560
Training Epoch: 5 [29440/50048]	Loss: 2.4011
Training Epoch: 5 [29568/50048]	Loss: 2.0954
Training Epoch: 5 [29696/50048]	Loss: 2.1063
Training Epoch: 5 [29824/50048]	Loss: 2.0810
Training Epoch: 5 [29952/50048]	Loss: 2.2762
Training Epoch: 5 [30080/50048]	Loss: 2.0093
Training Epoch: 5 [30208/50048]	Loss: 2.1458
Training Epoch: 5 [30336/50048]	Loss: 2.0139
Training Epoch: 5 [30464/50048]	Loss: 2.1920
Training Epoch: 5 [30592/50048]	Loss: 2.0391
Training Epoch: 5 [30720/50048]	Loss: 1.9959
Training Epoch: 5 [30848/50048]	Loss: 2.1997
Training Epoch: 5 [30976/50048]	Loss: 2.0772
Training Epoch: 5 [31104/50048]	Loss: 2.2067
Training Epoch: 5 [31232/50048]	Loss: 2.2667
Training Epoch: 5 [31360/50048]	Loss: 1.9064
Training Epoch: 5 [31488/50048]	Loss: 2.1666
Training Epoch: 5 [31616/50048]	Loss: 2.0759
Training Epoch: 5 [31744/50048]	Loss: 2.2648
Training Epoch: 5 [31872/50048]	Loss: 2.2537
Training Epoch: 5 [32000/50048]	Loss: 2.1467
Training Epoch: 5 [32128/50048]	Loss: 2.2790
Training Epoch: 5 [32256/50048]	Loss: 1.9391
Training Epoch: 5 [32384/50048]	Loss: 2.2113
Training Epoch: 5 [32512/50048]	Loss: 2.2889
Training Epoch: 5 [32640/50048]	Loss: 2.0326
Training Epoch: 5 [32768/50048]	Loss: 1.8462
Training Epoch: 5 [32896/50048]	Loss: 2.3870
Training Epoch: 5 [33024/50048]	Loss: 2.1163
Training Epoch: 5 [33152/50048]	Loss: 2.3138
Training Epoch: 5 [33280/50048]	Loss: 2.2762
Training Epoch: 5 [33408/50048]	Loss: 1.9587
Training Epoch: 5 [33536/50048]	Loss: 1.9696
Training Epoch: 5 [33664/50048]	Loss: 1.8378
Training Epoch: 5 [33792/50048]	Loss: 1.8121
Training Epoch: 5 [33920/50048]	Loss: 1.9907
Training Epoch: 5 [34048/50048]	Loss: 2.1088
Training Epoch: 5 [34176/50048]	Loss: 1.8903
Training Epoch: 5 [34304/50048]	Loss: 2.2382
Training Epoch: 5 [34432/50048]	Loss: 2.0089
Training Epoch: 5 [34560/50048]	Loss: 2.1210
Training Epoch: 5 [34688/50048]	Loss: 2.3408
Training Epoch: 5 [34816/50048]	Loss: 2.0022
Training Epoch: 5 [34944/50048]	Loss: 2.1781
Training Epoch: 5 [35072/50048]	Loss: 2.2460
Training Epoch: 5 [35200/50048]	Loss: 1.9041
Training Epoch: 5 [35328/50048]	Loss: 2.1801
Training Epoch: 5 [35456/50048]	Loss: 2.0221
Training Epoch: 5 [35584/50048]	Loss: 2.2838
Training Epoch: 5 [35712/50048]	Loss: 2.3120
Training Epoch: 5 [35840/50048]	Loss: 2.1961
Training Epoch: 5 [35968/50048]	Loss: 2.0936
Training Epoch: 5 [36096/50048]	Loss: 1.8737
Training Epoch: 5 [36224/50048]	Loss: 1.8332
Training Epoch: 5 [36352/50048]	Loss: 2.0344
Training Epoch: 5 [36480/50048]	Loss: 2.0036
Training Epoch: 5 [36608/50048]	Loss: 2.1580
Training Epoch: 5 [36736/50048]	Loss: 2.0643
Training Epoch: 5 [36864/50048]	Loss: 2.1701
Training Epoch: 5 [36992/50048]	Loss: 2.2947
Training Epoch: 5 [37120/50048]	Loss: 1.8092
Training Epoch: 5 [37248/50048]	Loss: 2.2482
Training Epoch: 5 [37376/50048]	Loss: 1.8508
Training Epoch: 5 [37504/50048]	Loss: 1.8387
Training Epoch: 5 [37632/50048]	Loss: 2.1581
Training Epoch: 5 [37760/50048]	Loss: 2.1790
Training Epoch: 5 [37888/50048]	Loss: 2.2218
Training Epoch: 5 [38016/50048]	Loss: 2.2647
Training Epoch: 5 [38144/50048]	Loss: 2.2168
Training Epoch: 5 [38272/50048]	Loss: 2.1064
Training Epoch: 5 [38400/50048]	Loss: 2.1894
Training Epoch: 5 [38528/50048]	Loss: 2.1116
Training Epoch: 5 [38656/50048]	Loss: 1.8466
Training Epoch: 5 [38784/50048]	Loss: 2.2990
Training Epoch: 5 [38912/50048]	Loss: 1.8732
Training Epoch: 5 [39040/50048]	Loss: 2.1162
Training Epoch: 5 [39168/50048]	Loss: 2.4148
Training Epoch: 5 [39296/50048]	Loss: 2.1035
Training Epoch: 5 [39424/50048]	Loss: 2.1291
Training Epoch: 5 [39552/50048]	Loss: 2.1806
Training Epoch: 5 [39680/50048]	Loss: 2.1402
Training Epoch: 5 [39808/50048]	Loss: 2.2055
Training Epoch: 5 [39936/50048]	Loss: 2.0116
Training Epoch: 5 [40064/50048]	Loss: 2.1973
Training Epoch: 5 [40192/50048]	Loss: 1.8223
Training Epoch: 5 [40320/50048]	Loss: 2.2684
Training Epoch: 5 [40448/50048]	Loss: 1.9671
Training Epoch: 5 [40576/50048]	Loss: 2.0866
Training Epoch: 5 [40704/50048]	Loss: 2.2256
Training Epoch: 5 [40832/50048]	Loss: 2.1532
Training Epoch: 5 [40960/50048]	Loss: 1.8510
Training Epoch: 5 [41088/50048]	Loss: 2.3193
Training Epoch: 5 [41216/50048]	Loss: 2.0821
Training Epoch: 5 [41344/50048]	Loss: 2.2193
Training Epoch: 5 [41472/50048]	Loss: 2.3086
Training Epoch: 5 [41600/50048]	Loss: 1.8033
Training Epoch: 5 [41728/50048]	Loss: 2.1732
Training Epoch: 5 [41856/50048]	Loss: 2.0726
Training Epoch: 5 [41984/50048]	Loss: 2.0734
Training Epoch: 5 [42112/50048]	Loss: 2.0400
Training Epoch: 5 [42240/50048]	Loss: 2.2163
Training Epoch: 5 [42368/50048]	Loss: 1.7641
Training Epoch: 5 [42496/50048]	Loss: 2.3496
Training Epoch: 5 [42624/50048]	Loss: 2.1888
Training Epoch: 5 [42752/50048]	Loss: 2.1755
Training Epoch: 5 [42880/50048]	Loss: 2.2414
Training Epoch: 5 [43008/50048]	Loss: 1.8839
Training Epoch: 5 [43136/50048]	Loss: 1.8278
Training Epoch: 5 [43264/50048]	Loss: 2.1972
Training Epoch: 5 [43392/50048]	Loss: 2.1896
Training Epoch: 5 [43520/50048]	Loss: 1.9098
Training Epoch: 5 [43648/50048]	Loss: 2.3287
Training Epoch: 5 [43776/50048]	Loss: 2.4807
Training Epoch: 5 [43904/50048]	Loss: 1.9307
Training Epoch: 5 [44032/50048]	Loss: 2.0464
Training Epoch: 5 [44160/50048]	Loss: 2.0355
Training Epoch: 5 [44288/50048]	Loss: 2.2303
Training Epoch: 5 [44416/50048]	Loss: 2.1464
Training Epoch: 5 [44544/50048]	Loss: 2.1632
Training Epoch: 5 [44672/50048]	Loss: 2.1920
Training Epoch: 5 [44800/50048]	Loss: 2.3130
Training Epoch: 5 [44928/50048]	Loss: 2.2690
Training Epoch: 5 [45056/50048]	Loss: 2.3141
Training Epoch: 5 [45184/50048]	Loss: 2.1421
Training Epoch: 5 [45312/50048]	Loss: 2.0982
Training Epoch: 5 [45440/50048]	Loss: 2.1658
Training Epoch: 5 [45568/50048]	Loss: 1.7255
Training Epoch: 5 [45696/50048]	Loss: 2.2159
Training Epoch: 5 [45824/50048]	Loss: 1.9477
Training Epoch: 5 [45952/50048]	Loss: 2.0678
Training Epoch: 5 [46080/50048]	Loss: 2.0355
Training Epoch: 5 [46208/50048]	Loss: 2.1950
Training Epoch: 5 [46336/50048]	Loss: 2.1251
Training Epoch: 5 [46464/50048]	Loss: 2.1503
Training Epoch: 5 [46592/50048]	Loss: 1.9272
Training Epoch: 5 [46720/50048]	Loss: 1.9875
2022-12-06 06:16:51,345 [ZeusDataLoader(train)] train epoch 6 done: time=86.44 energy=10495.32
2022-12-06 06:16:51,347 [ZeusDataLoader(eval)] Epoch 6 begin.
Training Epoch: 5 [46848/50048]	Loss: 2.1381
Training Epoch: 5 [46976/50048]	Loss: 2.2446
Training Epoch: 5 [47104/50048]	Loss: 2.2105
Training Epoch: 5 [47232/50048]	Loss: 1.9598
Training Epoch: 5 [47360/50048]	Loss: 1.9517
Training Epoch: 5 [47488/50048]	Loss: 1.8834
Training Epoch: 5 [47616/50048]	Loss: 2.0921
Training Epoch: 5 [47744/50048]	Loss: 2.4069
Training Epoch: 5 [47872/50048]	Loss: 2.0243
Training Epoch: 5 [48000/50048]	Loss: 2.1222
Training Epoch: 5 [48128/50048]	Loss: 2.1545
Training Epoch: 5 [48256/50048]	Loss: 1.9554
Training Epoch: 5 [48384/50048]	Loss: 2.2920
Training Epoch: 5 [48512/50048]	Loss: 2.2098
Training Epoch: 5 [48640/50048]	Loss: 2.3729
Training Epoch: 5 [48768/50048]	Loss: 2.2668
Training Epoch: 5 [48896/50048]	Loss: 2.0425
Training Epoch: 5 [49024/50048]	Loss: 2.0813
Training Epoch: 5 [49152/50048]	Loss: 2.1496
Training Epoch: 5 [49280/50048]	Loss: 1.8861
Training Epoch: 5 [49408/50048]	Loss: 2.1033
Training Epoch: 5 [49536/50048]	Loss: 2.2440
Training Epoch: 5 [49664/50048]	Loss: 1.9180
Training Epoch: 5 [49792/50048]	Loss: 2.2846
Training Epoch: 5 [49920/50048]	Loss: 2.2263
Training Epoch: 5 [50048/50048]	Loss: 1.9933
2022-12-06 11:16:54.971 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:16:54,986 [ZeusDataLoader(eval)] eval epoch 6 done: time=3.63 energy=443.28
2022-12-06 06:16:54,986 [ZeusDataLoader(train)] Up to epoch 6: time=541.25, energy=65666.65, cost=80192.59
2022-12-06 06:16:54,986 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:16:54,986 [ZeusDataLoader(train)] Expected next epoch: time=631.05, energy=76464.66, cost=93448.98
2022-12-06 06:16:54,987 [ZeusDataLoader(train)] Epoch 7 begin.
Validation Epoch: 5, Average loss: 0.0163, Accuracy: 0.4365
2022-12-06 06:16:55,176 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:16:55,176 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:16:55.178 [ZeusMonitor] Monitor started.
2022-12-06 11:16:55.178 [ZeusMonitor] Running indefinitely. 2022-12-06 11:16:55.178 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:16:55.178 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e7+gpu0.power.log
Training Epoch: 6 [128/50048]	Loss: 2.0023
Training Epoch: 6 [256/50048]	Loss: 1.8523
Training Epoch: 6 [384/50048]	Loss: 2.2909
Training Epoch: 6 [512/50048]	Loss: 1.9958
Training Epoch: 6 [640/50048]	Loss: 2.1149
Training Epoch: 6 [768/50048]	Loss: 1.9621
Training Epoch: 6 [896/50048]	Loss: 2.0851
Training Epoch: 6 [1024/50048]	Loss: 1.8281
Training Epoch: 6 [1152/50048]	Loss: 1.8437
Training Epoch: 6 [1280/50048]	Loss: 2.0825
Training Epoch: 6 [1408/50048]	Loss: 2.1315
Training Epoch: 6 [1536/50048]	Loss: 1.9742
Training Epoch: 6 [1664/50048]	Loss: 2.0163
Training Epoch: 6 [1792/50048]	Loss: 1.7280
Training Epoch: 6 [1920/50048]	Loss: 2.0747
Training Epoch: 6 [2048/50048]	Loss: 1.9954
Training Epoch: 6 [2176/50048]	Loss: 2.1424
Training Epoch: 6 [2304/50048]	Loss: 1.9742
Training Epoch: 6 [2432/50048]	Loss: 1.9455
Training Epoch: 6 [2560/50048]	Loss: 2.2453
Training Epoch: 6 [2688/50048]	Loss: 2.0161
Training Epoch: 6 [2816/50048]	Loss: 2.0575
Training Epoch: 6 [2944/50048]	Loss: 1.8604
Training Epoch: 6 [3072/50048]	Loss: 1.9362
Training Epoch: 6 [3200/50048]	Loss: 2.2225
Training Epoch: 6 [3328/50048]	Loss: 2.2924
Training Epoch: 6 [3456/50048]	Loss: 1.8361
Training Epoch: 6 [3584/50048]	Loss: 2.0751
Training Epoch: 6 [3712/50048]	Loss: 1.8244
Training Epoch: 6 [3840/50048]	Loss: 2.1356
Training Epoch: 6 [3968/50048]	Loss: 1.9495
Training Epoch: 6 [4096/50048]	Loss: 2.0360
Training Epoch: 6 [4224/50048]	Loss: 2.0055
Training Epoch: 6 [4352/50048]	Loss: 1.8658
Training Epoch: 6 [4480/50048]	Loss: 2.1218
Training Epoch: 6 [4608/50048]	Loss: 1.9529
Training Epoch: 6 [4736/50048]	Loss: 1.8076
Training Epoch: 6 [4864/50048]	Loss: 1.8982
Training Epoch: 6 [4992/50048]	Loss: 2.0620
Training Epoch: 6 [5120/50048]	Loss: 2.0667
Training Epoch: 6 [5248/50048]	Loss: 1.8246
Training Epoch: 6 [5376/50048]	Loss: 1.7765
Training Epoch: 6 [5504/50048]	Loss: 2.1363
Training Epoch: 6 [5632/50048]	Loss: 2.2157
Training Epoch: 6 [5760/50048]	Loss: 2.2530
Training Epoch: 6 [5888/50048]	Loss: 1.9747
Training Epoch: 6 [6016/50048]	Loss: 1.8952
Training Epoch: 6 [6144/50048]	Loss: 2.1722
Training Epoch: 6 [6272/50048]	Loss: 1.9912
Training Epoch: 6 [6400/50048]	Loss: 1.9695
Training Epoch: 6 [6528/50048]	Loss: 2.1763
Training Epoch: 6 [6656/50048]	Loss: 2.0488
Training Epoch: 6 [6784/50048]	Loss: 2.0894
Training Epoch: 6 [6912/50048]	Loss: 2.0239
Training Epoch: 6 [7040/50048]	Loss: 2.2470
Training Epoch: 6 [7168/50048]	Loss: 2.0309
Training Epoch: 6 [7296/50048]	Loss: 1.7760
Training Epoch: 6 [7424/50048]	Loss: 1.9052
Training Epoch: 6 [7552/50048]	Loss: 1.8022
Training Epoch: 6 [7680/50048]	Loss: 1.9778
Training Epoch: 6 [7808/50048]	Loss: 2.2023
Training Epoch: 6 [7936/50048]	Loss: 2.0293
Training Epoch: 6 [8064/50048]	Loss: 2.2015
Training Epoch: 6 [8192/50048]	Loss: 1.8281
Training Epoch: 6 [8320/50048]	Loss: 2.1192
Training Epoch: 6 [8448/50048]	Loss: 2.1516
Training Epoch: 6 [8576/50048]	Loss: 2.1722
Training Epoch: 6 [8704/50048]	Loss: 2.2933
Training Epoch: 6 [8832/50048]	Loss: 2.1174
Training Epoch: 6 [8960/50048]	Loss: 1.9275
Training Epoch: 6 [9088/50048]	Loss: 2.0616
Training Epoch: 6 [9216/50048]	Loss: 2.0525
Training Epoch: 6 [9344/50048]	Loss: 2.2516
Training Epoch: 6 [9472/50048]	Loss: 2.1645
Training Epoch: 6 [9600/50048]	Loss: 2.2874
Training Epoch: 6 [9728/50048]	Loss: 2.2815
Training Epoch: 6 [9856/50048]	Loss: 1.8468
Training Epoch: 6 [9984/50048]	Loss: 1.7503
Training Epoch: 6 [10112/50048]	Loss: 2.2029
Training Epoch: 6 [10240/50048]	Loss: 2.1444
Training Epoch: 6 [10368/50048]	Loss: 2.1486
Training Epoch: 6 [10496/50048]	Loss: 2.1638
Training Epoch: 6 [10624/50048]	Loss: 1.9816
Training Epoch: 6 [10752/50048]	Loss: 1.8772
Training Epoch: 6 [10880/50048]	Loss: 1.7513
Training Epoch: 6 [11008/50048]	Loss: 2.1975
Training Epoch: 6 [11136/50048]	Loss: 2.1022
Training Epoch: 6 [11264/50048]	Loss: 1.8685
Training Epoch: 6 [11392/50048]	Loss: 1.6715
Training Epoch: 6 [11520/50048]	Loss: 2.1457
Training Epoch: 6 [11648/50048]	Loss: 2.1189
Training Epoch: 6 [11776/50048]	Loss: 1.9029
Training Epoch: 6 [11904/50048]	Loss: 2.0794
Training Epoch: 6 [12032/50048]	Loss: 1.8357
Training Epoch: 6 [12160/50048]	Loss: 2.1722
Training Epoch: 6 [12288/50048]	Loss: 2.2709
Training Epoch: 6 [12416/50048]	Loss: 1.9846
Training Epoch: 6 [12544/50048]	Loss: 2.0116
Training Epoch: 6 [12672/50048]	Loss: 1.8348
Training Epoch: 6 [12800/50048]	Loss: 1.8470
Training Epoch: 6 [12928/50048]	Loss: 2.1851
Training Epoch: 6 [13056/50048]	Loss: 2.0119
Training Epoch: 6 [13184/50048]	Loss: 2.1569
Training Epoch: 6 [13312/50048]	Loss: 2.2649
Training Epoch: 6 [13440/50048]	Loss: 1.9582
Training Epoch: 6 [13568/50048]	Loss: 1.8958
Training Epoch: 6 [13696/50048]	Loss: 2.0192
Training Epoch: 6 [13824/50048]	Loss: 2.2535
Training Epoch: 6 [13952/50048]	Loss: 2.2198
Training Epoch: 6 [14080/50048]	Loss: 2.1087
Training Epoch: 6 [14208/50048]	Loss: 1.8620
Training Epoch: 6 [14336/50048]	Loss: 2.1993
Training Epoch: 6 [14464/50048]	Loss: 1.9205
Training Epoch: 6 [14592/50048]	Loss: 1.9691
Training Epoch: 6 [14720/50048]	Loss: 1.8578
Training Epoch: 6 [14848/50048]	Loss: 1.9044
Training Epoch: 6 [14976/50048]	Loss: 1.8584
Training Epoch: 6 [15104/50048]	Loss: 1.8418
Training Epoch: 6 [15232/50048]	Loss: 2.0431
Training Epoch: 6 [15360/50048]	Loss: 1.9495
Training Epoch: 6 [15488/50048]	Loss: 2.0442
Training Epoch: 6 [15616/50048]	Loss: 2.1394
Training Epoch: 6 [15744/50048]	Loss: 1.8868
Training Epoch: 6 [15872/50048]	Loss: 2.0504
Training Epoch: 6 [16000/50048]	Loss: 1.8703
Training Epoch: 6 [16128/50048]	Loss: 1.8578
Training Epoch: 6 [16256/50048]	Loss: 2.1773
Training Epoch: 6 [16384/50048]	Loss: 2.1387
Training Epoch: 6 [16512/50048]	Loss: 1.9091
Training Epoch: 6 [16640/50048]	Loss: 2.0186
Training Epoch: 6 [16768/50048]	Loss: 2.2979
Training Epoch: 6 [16896/50048]	Loss: 1.8175
Training Epoch: 6 [17024/50048]	Loss: 1.6893
Training Epoch: 6 [17152/50048]	Loss: 1.9192
Training Epoch: 6 [17280/50048]	Loss: 2.0696
Training Epoch: 6 [17408/50048]	Loss: 2.1588
Training Epoch: 6 [17536/50048]	Loss: 1.9092
Training Epoch: 6 [17664/50048]	Loss: 2.0684
Training Epoch: 6 [17792/50048]	Loss: 2.0374
Training Epoch: 6 [17920/50048]	Loss: 1.9724
Training Epoch: 6 [18048/50048]	Loss: 2.0640
Training Epoch: 6 [18176/50048]	Loss: 2.2504
Training Epoch: 6 [18304/50048]	Loss: 1.7731
Training Epoch: 6 [18432/50048]	Loss: 2.3223
Training Epoch: 6 [18560/50048]	Loss: 1.9601
Training Epoch: 6 [18688/50048]	Loss: 2.0988
Training Epoch: 6 [18816/50048]	Loss: 1.8818
Training Epoch: 6 [18944/50048]	Loss: 2.1940
Training Epoch: 6 [19072/50048]	Loss: 1.9862
Training Epoch: 6 [19200/50048]	Loss: 2.0172
Training Epoch: 6 [19328/50048]	Loss: 1.9205
Training Epoch: 6 [19456/50048]	Loss: 1.9648
Training Epoch: 6 [19584/50048]	Loss: 2.0545
Training Epoch: 6 [19712/50048]	Loss: 1.9815
Training Epoch: 6 [19840/50048]	Loss: 2.0358
Training Epoch: 6 [19968/50048]	Loss: 2.0141
Training Epoch: 6 [20096/50048]	Loss: 1.9666
Training Epoch: 6 [20224/50048]	Loss: 1.9843
Training Epoch: 6 [20352/50048]	Loss: 1.8911
Training Epoch: 6 [20480/50048]	Loss: 2.0532
Training Epoch: 6 [20608/50048]	Loss: 1.9990
Training Epoch: 6 [20736/50048]	Loss: 1.9801
Training Epoch: 6 [20864/50048]	Loss: 2.0395
Training Epoch: 6 [20992/50048]	Loss: 2.1737
Training Epoch: 6 [21120/50048]	Loss: 2.0890
Training Epoch: 6 [21248/50048]	Loss: 2.1329
Training Epoch: 6 [21376/50048]	Loss: 2.0900
Training Epoch: 6 [21504/50048]	Loss: 2.0565
Training Epoch: 6 [21632/50048]	Loss: 1.7664
Training Epoch: 6 [21760/50048]	Loss: 2.2933
Training Epoch: 6 [21888/50048]	Loss: 1.9064
Training Epoch: 6 [22016/50048]	Loss: 2.2499
Training Epoch: 6 [22144/50048]	Loss: 1.9828
Training Epoch: 6 [22272/50048]	Loss: 1.9225
Training Epoch: 6 [22400/50048]	Loss: 1.8408
Training Epoch: 6 [22528/50048]	Loss: 1.9771
Training Epoch: 6 [22656/50048]	Loss: 2.0892
Training Epoch: 6 [22784/50048]	Loss: 2.4022
Training Epoch: 6 [22912/50048]	Loss: 1.8212
Training Epoch: 6 [23040/50048]	Loss: 2.1427
Training Epoch: 6 [23168/50048]	Loss: 1.8511
Training Epoch: 6 [23296/50048]	Loss: 2.0293
Training Epoch: 6 [23424/50048]	Loss: 2.0201
Training Epoch: 6 [23552/50048]	Loss: 1.9062
Training Epoch: 6 [23680/50048]	Loss: 1.9165
Training Epoch: 6 [23808/50048]	Loss: 2.1996
Training Epoch: 6 [23936/50048]	Loss: 1.9304
Training Epoch: 6 [24064/50048]	Loss: 2.1182
Training Epoch: 6 [24192/50048]	Loss: 1.9772
Training Epoch: 6 [24320/50048]	Loss: 2.0211
Training Epoch: 6 [24448/50048]	Loss: 2.1048
Training Epoch: 6 [24576/50048]	Loss: 2.1506
Training Epoch: 6 [24704/50048]	Loss: 1.9472
Training Epoch: 6 [24832/50048]	Loss: 1.9265
Training Epoch: 6 [24960/50048]	Loss: 1.8863
Training Epoch: 6 [25088/50048]	Loss: 2.1564
Training Epoch: 6 [25216/50048]	Loss: 1.8933
Training Epoch: 6 [25344/50048]	Loss: 1.9578
Training Epoch: 6 [25472/50048]	Loss: 1.9258
Training Epoch: 6 [25600/50048]	Loss: 1.8275
Training Epoch: 6 [25728/50048]	Loss: 2.2484
Training Epoch: 6 [25856/50048]	Loss: 1.9060
Training Epoch: 6 [25984/50048]	Loss: 1.8898
Training Epoch: 6 [26112/50048]	Loss: 2.0621
Training Epoch: 6 [26240/50048]	Loss: 2.2001
Training Epoch: 6 [26368/50048]	Loss: 2.1319
Training Epoch: 6 [26496/50048]	Loss: 2.0591
Training Epoch: 6 [26624/50048]	Loss: 2.0280
Training Epoch: 6 [26752/50048]	Loss: 2.0221
Training Epoch: 6 [26880/50048]	Loss: 2.1115
Training Epoch: 6 [27008/50048]	Loss: 1.9157
Training Epoch: 6 [27136/50048]	Loss: 2.0044
Training Epoch: 6 [27264/50048]	Loss: 1.7000
Training Epoch: 6 [27392/50048]	Loss: 1.6876
Training Epoch: 6 [27520/50048]	Loss: 2.0344
Training Epoch: 6 [27648/50048]	Loss: 2.1862
Training Epoch: 6 [27776/50048]	Loss: 2.0102
Training Epoch: 6 [27904/50048]	Loss: 1.9875
Training Epoch: 6 [28032/50048]	Loss: 1.6104
Training Epoch: 6 [28160/50048]	Loss: 1.7638
Training Epoch: 6 [28288/50048]	Loss: 1.7826
Training Epoch: 6 [28416/50048]	Loss: 1.9267
Training Epoch: 6 [28544/50048]	Loss: 1.9275
Training Epoch: 6 [28672/50048]	Loss: 1.8197
Training Epoch: 6 [28800/50048]	Loss: 1.9639
Training Epoch: 6 [28928/50048]	Loss: 2.0350
Training Epoch: 6 [29056/50048]	Loss: 2.1841
Training Epoch: 6 [29184/50048]	Loss: 2.2458
Training Epoch: 6 [29312/50048]	Loss: 2.0425
Training Epoch: 6 [29440/50048]	Loss: 1.8892
Training Epoch: 6 [29568/50048]	Loss: 2.0676
Training Epoch: 6 [29696/50048]	Loss: 1.8738
Training Epoch: 6 [29824/50048]	Loss: 1.8802
Training Epoch: 6 [29952/50048]	Loss: 2.0520
Training Epoch: 6 [30080/50048]	Loss: 2.0834
Training Epoch: 6 [30208/50048]	Loss: 1.7395
Training Epoch: 6 [30336/50048]	Loss: 2.0166
Training Epoch: 6 [30464/50048]	Loss: 2.1347
Training Epoch: 6 [30592/50048]	Loss: 1.9349
Training Epoch: 6 [30720/50048]	Loss: 1.7794
Training Epoch: 6 [30848/50048]	Loss: 1.9443
Training Epoch: 6 [30976/50048]	Loss: 2.0874
Training Epoch: 6 [31104/50048]	Loss: 2.1944
Training Epoch: 6 [31232/50048]	Loss: 1.8538
Training Epoch: 6 [31360/50048]	Loss: 1.9797
Training Epoch: 6 [31488/50048]	Loss: 2.0241
Training Epoch: 6 [31616/50048]	Loss: 2.0318
Training Epoch: 6 [31744/50048]	Loss: 2.0222
Training Epoch: 6 [31872/50048]	Loss: 1.8625
Training Epoch: 6 [32000/50048]	Loss: 1.9036
Training Epoch: 6 [32128/50048]	Loss: 2.1038
Training Epoch: 6 [32256/50048]	Loss: 2.0638
Training Epoch: 6 [32384/50048]	Loss: 2.0783
Training Epoch: 6 [32512/50048]	Loss: 2.0761
Training Epoch: 6 [32640/50048]	Loss: 2.0109
Training Epoch: 6 [32768/50048]	Loss: 1.8728
Training Epoch: 6 [32896/50048]	Loss: 1.8075
Training Epoch: 6 [33024/50048]	Loss: 1.9814
Training Epoch: 6 [33152/50048]	Loss: 2.1085
Training Epoch: 6 [33280/50048]	Loss: 2.1544
Training Epoch: 6 [33408/50048]	Loss: 1.8234
Training Epoch: 6 [33536/50048]	Loss: 2.1120
Training Epoch: 6 [33664/50048]	Loss: 2.0080
Training Epoch: 6 [33792/50048]	Loss: 1.9172
Training Epoch: 6 [33920/50048]	Loss: 1.8447
Training Epoch: 6 [34048/50048]	Loss: 2.0657
Training Epoch: 6 [34176/50048]	Loss: 2.1238
Training Epoch: 6 [34304/50048]	Loss: 2.0473
Training Epoch: 6 [34432/50048]	Loss: 1.9901
Training Epoch: 6 [34560/50048]	Loss: 1.8315
Training Epoch: 6 [34688/50048]	Loss: 1.8414
Training Epoch: 6 [34816/50048]	Loss: 2.1901
Training Epoch: 6 [34944/50048]	Loss: 1.9237
Training Epoch: 6 [35072/50048]	Loss: 2.0689
Training Epoch: 6 [35200/50048]	Loss: 2.0215
Training Epoch: 6 [35328/50048]	Loss: 2.0573
Training Epoch: 6 [35456/50048]	Loss: 2.0266
Training Epoch: 6 [35584/50048]	Loss: 1.7732
Training Epoch: 6 [35712/50048]	Loss: 1.9280
Training Epoch: 6 [35840/50048]	Loss: 2.1037
Training Epoch: 6 [35968/50048]	Loss: 2.0592
Training Epoch: 6 [36096/50048]	Loss: 1.9056
Training Epoch: 6 [36224/50048]	Loss: 1.7585
Training Epoch: 6 [36352/50048]	Loss: 1.8651
Training Epoch: 6 [36480/50048]	Loss: 2.2691
Training Epoch: 6 [36608/50048]	Loss: 2.1946
Training Epoch: 6 [36736/50048]	Loss: 2.1434
Training Epoch: 6 [36864/50048]	Loss: 2.2165
Training Epoch: 6 [36992/50048]	Loss: 1.8627
Training Epoch: 6 [37120/50048]	Loss: 2.1173
Training Epoch: 6 [37248/50048]	Loss: 2.2201
Training Epoch: 6 [37376/50048]	Loss: 2.1168
Training Epoch: 6 [37504/50048]	Loss: 1.9598
Training Epoch: 6 [37632/50048]	Loss: 1.9901
Training Epoch: 6 [37760/50048]	Loss: 1.9713
Training Epoch: 6 [37888/50048]	Loss: 1.7856
Training Epoch: 6 [38016/50048]	Loss: 2.0237
Training Epoch: 6 [38144/50048]	Loss: 2.1139
Training Epoch: 6 [38272/50048]	Loss: 2.0326
Training Epoch: 6 [38400/50048]	Loss: 2.4483
Training Epoch: 6 [38528/50048]	Loss: 1.8333
Training Epoch: 6 [38656/50048]	Loss: 1.9490
Training Epoch: 6 [38784/50048]	Loss: 2.0966
Training Epoch: 6 [38912/50048]	Loss: 1.7396
Training Epoch: 6 [39040/50048]	Loss: 2.0184
Training Epoch: 6 [39168/50048]	Loss: 2.1094
Training Epoch: 6 [39296/50048]	Loss: 2.0963
Training Epoch: 6 [39424/50048]	Loss: 1.9668
Training Epoch: 6 [39552/50048]	Loss: 1.9936
Training Epoch: 6 [39680/50048]	Loss: 1.8691
Training Epoch: 6 [39808/50048]	Loss: 2.1319
Training Epoch: 6 [39936/50048]	Loss: 2.0652
Training Epoch: 6 [40064/50048]	Loss: 1.9349
Training Epoch: 6 [40192/50048]	Loss: 2.2225
Training Epoch: 6 [40320/50048]	Loss: 1.9520
Training Epoch: 6 [40448/50048]	Loss: 1.7471
Training Epoch: 6 [40576/50048]	Loss: 1.8739
Training Epoch: 6 [40704/50048]	Loss: 1.9222
Training Epoch: 6 [40832/50048]	Loss: 1.8617
Training Epoch: 6 [40960/50048]	Loss: 1.9656
Training Epoch: 6 [41088/50048]	Loss: 2.0395
Training Epoch: 6 [41216/50048]	Loss: 2.0102
Training Epoch: 6 [41344/50048]	Loss: 1.9007
Training Epoch: 6 [41472/50048]	Loss: 1.7716
Training Epoch: 6 [41600/50048]	Loss: 2.0220
Training Epoch: 6 [41728/50048]	Loss: 2.0826
Training Epoch: 6 [41856/50048]	Loss: 1.9677
Training Epoch: 6 [41984/50048]	Loss: 2.0698
Training Epoch: 6 [42112/50048]	Loss: 1.9412
Training Epoch: 6 [42240/50048]	Loss: 1.9987
Training Epoch: 6 [42368/50048]	Loss: 2.0567
Training Epoch: 6 [42496/50048]	Loss: 1.5701
Training Epoch: 6 [42624/50048]	Loss: 1.8621
Training Epoch: 6 [42752/50048]	Loss: 1.9985
Training Epoch: 6 [42880/50048]	Loss: 1.9043
Training Epoch: 6 [43008/50048]	Loss: 2.0460
Training Epoch: 6 [43136/50048]	Loss: 2.1677
Training Epoch: 6 [43264/50048]	Loss: 2.0638
Training Epoch: 6 [43392/50048]	Loss: 1.8756
Training Epoch: 6 [43520/50048]	Loss: 1.9484
Training Epoch: 6 [43648/50048]	Loss: 2.0855
Training Epoch: 6 [43776/50048]	Loss: 1.7407
Training Epoch: 6 [43904/50048]	Loss: 2.0455
Training Epoch: 6 [44032/50048]	Loss: 2.0542
Training Epoch: 6 [44160/50048]	Loss: 2.0129
Training Epoch: 6 [44288/50048]	Loss: 1.9300
Training Epoch: 6 [44416/50048]	Loss: 2.0571
Training Epoch: 6 [44544/50048]	Loss: 1.9158
Training Epoch: 6 [44672/50048]	Loss: 2.2754
Training Epoch: 6 [44800/50048]	Loss: 1.8058
Training Epoch: 6 [44928/50048]	Loss: 1.6607
Training Epoch: 6 [45056/50048]	Loss: 1.6727
Training Epoch: 6 [45184/50048]	Loss: 1.9674
Training Epoch: 6 [45312/50048]	Loss: 2.0056
Training Epoch: 6 [45440/50048]	Loss: 2.1981
Training Epoch: 6 [45568/50048]	Loss: 1.9366
Training Epoch: 6 [45696/50048]	Loss: 1.7765
Training Epoch: 6 [45824/50048]	Loss: 1.8344
Training Epoch: 6 [45952/50048]	Loss: 1.8994
Training Epoch: 6 [46080/50048]	Loss: 2.1431
Training Epoch: 6 [46208/50048]	Loss: 1.9251
Training Epoch: 6 [46336/50048]	Loss: 1.9417
Training Epoch: 6 [46464/50048]	Loss: 2.1192
Training Epoch: 6 [46592/50048]	Loss: 2.0765
Training Epoch: 6 [46720/50048]	Loss: 2.1437
2022-12-06 06:18:21,577 [ZeusDataLoader(train)] train epoch 7 done: time=86.58 energy=10506.12
2022-12-06 06:18:21,579 [ZeusDataLoader(eval)] Epoch 7 begin.
Training Epoch: 6 [46848/50048]	Loss: 1.8883
Training Epoch: 6 [46976/50048]	Loss: 1.9466
Training Epoch: 6 [47104/50048]	Loss: 2.0034
Training Epoch: 6 [47232/50048]	Loss: 2.0461
Training Epoch: 6 [47360/50048]	Loss: 1.8264
Training Epoch: 6 [47488/50048]	Loss: 1.8332
Training Epoch: 6 [47616/50048]	Loss: 1.9053
Training Epoch: 6 [47744/50048]	Loss: 1.9482
Training Epoch: 6 [47872/50048]	Loss: 1.7819
Training Epoch: 6 [48000/50048]	Loss: 2.0956
Training Epoch: 6 [48128/50048]	Loss: 2.2466
Training Epoch: 6 [48256/50048]	Loss: 2.1304
Training Epoch: 6 [48384/50048]	Loss: 1.9565
Training Epoch: 6 [48512/50048]	Loss: 1.8105
Training Epoch: 6 [48640/50048]	Loss: 1.8918
Training Epoch: 6 [48768/50048]	Loss: 1.8582
Training Epoch: 6 [48896/50048]	Loss: 1.7827
Training Epoch: 6 [49024/50048]	Loss: 1.9090
Training Epoch: 6 [49152/50048]	Loss: 1.8212
Training Epoch: 6 [49280/50048]	Loss: 1.9787
Training Epoch: 6 [49408/50048]	Loss: 1.9435
Training Epoch: 6 [49536/50048]	Loss: 1.8480
Training Epoch: 6 [49664/50048]	Loss: 2.1373
Training Epoch: 6 [49792/50048]	Loss: 1.7022
Training Epoch: 6 [49920/50048]	Loss: 2.1344
Training Epoch: 6 [50048/50048]	Loss: 1.8094
2022-12-06 11:18:25.306 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:18:25,353 [ZeusDataLoader(eval)] eval epoch 7 done: time=3.77 energy=453.54
2022-12-06 06:18:25,353 [ZeusDataLoader(train)] Up to epoch 7: time=631.59, energy=76626.31, cost=93577.61
2022-12-06 06:18:25,354 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:18:25,354 [ZeusDataLoader(train)] Expected next epoch: time=721.39, energy=87424.33, cost=106833.99
2022-12-06 06:18:25,355 [ZeusDataLoader(train)] Epoch 8 begin.
Validation Epoch: 6, Average loss: 0.0159, Accuracy: 0.4543
2022-12-06 06:18:25,510 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:18:25,511 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:18:25.513 [ZeusMonitor] Monitor started.
2022-12-06 11:18:25.513 [ZeusMonitor] Running indefinitely. 2022-12-06 11:18:25.513 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:18:25.513 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e8+gpu0.power.log
Training Epoch: 7 [128/50048]	Loss: 1.7374
Training Epoch: 7 [256/50048]	Loss: 1.8489
Training Epoch: 7 [384/50048]	Loss: 1.8492
Training Epoch: 7 [512/50048]	Loss: 2.1627
Training Epoch: 7 [640/50048]	Loss: 2.0050
Training Epoch: 7 [768/50048]	Loss: 2.0122
Training Epoch: 7 [896/50048]	Loss: 1.9024
Training Epoch: 7 [1024/50048]	Loss: 2.1798
Training Epoch: 7 [1152/50048]	Loss: 1.8423
Training Epoch: 7 [1280/50048]	Loss: 1.8793
Training Epoch: 7 [1408/50048]	Loss: 1.9820
Training Epoch: 7 [1536/50048]	Loss: 1.8800
Training Epoch: 7 [1664/50048]	Loss: 2.1588
Training Epoch: 7 [1792/50048]	Loss: 1.8546
Training Epoch: 7 [1920/50048]	Loss: 1.7467
Training Epoch: 7 [2048/50048]	Loss: 1.8054
Training Epoch: 7 [2176/50048]	Loss: 1.7138
Training Epoch: 7 [2304/50048]	Loss: 1.9779
Training Epoch: 7 [2432/50048]	Loss: 1.8011
Training Epoch: 7 [2560/50048]	Loss: 1.8747
Training Epoch: 7 [2688/50048]	Loss: 1.6570
Training Epoch: 7 [2816/50048]	Loss: 1.9169
Training Epoch: 7 [2944/50048]	Loss: 1.9826
Training Epoch: 7 [3072/50048]	Loss: 1.7711
Training Epoch: 7 [3200/50048]	Loss: 1.8200
Training Epoch: 7 [3328/50048]	Loss: 1.9801
Training Epoch: 7 [3456/50048]	Loss: 1.6989
Training Epoch: 7 [3584/50048]	Loss: 1.8343
Training Epoch: 7 [3712/50048]	Loss: 1.9602
Training Epoch: 7 [3840/50048]	Loss: 1.7012
Training Epoch: 7 [3968/50048]	Loss: 1.9672
Training Epoch: 7 [4096/50048]	Loss: 1.7199
Training Epoch: 7 [4224/50048]	Loss: 1.8230
Training Epoch: 7 [4352/50048]	Loss: 1.7784
Training Epoch: 7 [4480/50048]	Loss: 1.9572
Training Epoch: 7 [4608/50048]	Loss: 1.9147
Training Epoch: 7 [4736/50048]	Loss: 2.0109
Training Epoch: 7 [4864/50048]	Loss: 1.9134
Training Epoch: 7 [4992/50048]	Loss: 1.7734
Training Epoch: 7 [5120/50048]	Loss: 1.8760
Training Epoch: 7 [5248/50048]	Loss: 1.6257
Training Epoch: 7 [5376/50048]	Loss: 1.7558
Training Epoch: 7 [5504/50048]	Loss: 2.0195
Training Epoch: 7 [5632/50048]	Loss: 1.9652
Training Epoch: 7 [5760/50048]	Loss: 1.8655
Training Epoch: 7 [5888/50048]	Loss: 1.6190
Training Epoch: 7 [6016/50048]	Loss: 2.0162
Training Epoch: 7 [6144/50048]	Loss: 1.7598
Training Epoch: 7 [6272/50048]	Loss: 1.8177
Training Epoch: 7 [6400/50048]	Loss: 2.0292
Training Epoch: 7 [6528/50048]	Loss: 1.7111
Training Epoch: 7 [6656/50048]	Loss: 1.7407
Training Epoch: 7 [6784/50048]	Loss: 1.9010
Training Epoch: 7 [6912/50048]	Loss: 1.7918
Training Epoch: 7 [7040/50048]	Loss: 1.9465
Training Epoch: 7 [7168/50048]	Loss: 1.8830
Training Epoch: 7 [7296/50048]	Loss: 1.7295
Training Epoch: 7 [7424/50048]	Loss: 2.0886
Training Epoch: 7 [7552/50048]	Loss: 1.8971
Training Epoch: 7 [7680/50048]	Loss: 1.7787
Training Epoch: 7 [7808/50048]	Loss: 2.1193
Training Epoch: 7 [7936/50048]	Loss: 1.7163
Training Epoch: 7 [8064/50048]	Loss: 1.8823
Training Epoch: 7 [8192/50048]	Loss: 2.0449
Training Epoch: 7 [8320/50048]	Loss: 1.5023
Training Epoch: 7 [8448/50048]	Loss: 1.8034
Training Epoch: 7 [8576/50048]	Loss: 1.6740
Training Epoch: 7 [8704/50048]	Loss: 1.6902
Training Epoch: 7 [8832/50048]	Loss: 2.1961
Training Epoch: 7 [8960/50048]	Loss: 1.7787
Training Epoch: 7 [9088/50048]	Loss: 1.7942
Training Epoch: 7 [9216/50048]	Loss: 1.6424
Training Epoch: 7 [9344/50048]	Loss: 1.9867
Training Epoch: 7 [9472/50048]	Loss: 1.8615
Training Epoch: 7 [9600/50048]	Loss: 1.7944
Training Epoch: 7 [9728/50048]	Loss: 1.7921
Training Epoch: 7 [9856/50048]	Loss: 1.8192
Training Epoch: 7 [9984/50048]	Loss: 2.0381
Training Epoch: 7 [10112/50048]	Loss: 1.6723
Training Epoch: 7 [10240/50048]	Loss: 1.6553
Training Epoch: 7 [10368/50048]	Loss: 1.7366
Training Epoch: 7 [10496/50048]	Loss: 2.0555
Training Epoch: 7 [10624/50048]	Loss: 1.9093
Training Epoch: 7 [10752/50048]	Loss: 1.7671
Training Epoch: 7 [10880/50048]	Loss: 1.7502
Training Epoch: 7 [11008/50048]	Loss: 1.8142
Training Epoch: 7 [11136/50048]	Loss: 1.9320
Training Epoch: 7 [11264/50048]	Loss: 1.8145
Training Epoch: 7 [11392/50048]	Loss: 1.9772
Training Epoch: 7 [11520/50048]	Loss: 2.2697
Training Epoch: 7 [11648/50048]	Loss: 1.5226
Training Epoch: 7 [11776/50048]	Loss: 1.7075
Training Epoch: 7 [11904/50048]	Loss: 1.7295
Training Epoch: 7 [12032/50048]	Loss: 1.8942
Training Epoch: 7 [12160/50048]	Loss: 1.9430
Training Epoch: 7 [12288/50048]	Loss: 1.9630
Training Epoch: 7 [12416/50048]	Loss: 1.9615
Training Epoch: 7 [12544/50048]	Loss: 1.7295
Training Epoch: 7 [12672/50048]	Loss: 1.9716
Training Epoch: 7 [12800/50048]	Loss: 1.7516
Training Epoch: 7 [12928/50048]	Loss: 2.2106
Training Epoch: 7 [13056/50048]	Loss: 1.6782
Training Epoch: 7 [13184/50048]	Loss: 2.0220
Training Epoch: 7 [13312/50048]	Loss: 2.1880
Training Epoch: 7 [13440/50048]	Loss: 1.8358
Training Epoch: 7 [13568/50048]	Loss: 1.9784
Training Epoch: 7 [13696/50048]	Loss: 1.8034
Training Epoch: 7 [13824/50048]	Loss: 2.1682
Training Epoch: 7 [13952/50048]	Loss: 1.9464
Training Epoch: 7 [14080/50048]	Loss: 2.1632
Training Epoch: 7 [14208/50048]	Loss: 1.7825
Training Epoch: 7 [14336/50048]	Loss: 2.0174
Training Epoch: 7 [14464/50048]	Loss: 1.9822
Training Epoch: 7 [14592/50048]	Loss: 1.9717
Training Epoch: 7 [14720/50048]	Loss: 1.8951
Training Epoch: 7 [14848/50048]	Loss: 1.7170
Training Epoch: 7 [14976/50048]	Loss: 1.7626
Training Epoch: 7 [15104/50048]	Loss: 1.7597
Training Epoch: 7 [15232/50048]	Loss: 1.8897
Training Epoch: 7 [15360/50048]	Loss: 1.9307
Training Epoch: 7 [15488/50048]	Loss: 1.9012
Training Epoch: 7 [15616/50048]	Loss: 1.7165
Training Epoch: 7 [15744/50048]	Loss: 1.9476
Training Epoch: 7 [15872/50048]	Loss: 1.6532
Training Epoch: 7 [16000/50048]	Loss: 2.0073
Training Epoch: 7 [16128/50048]	Loss: 1.8910
Training Epoch: 7 [16256/50048]	Loss: 1.9316
Training Epoch: 7 [16384/50048]	Loss: 1.9415
Training Epoch: 7 [16512/50048]	Loss: 1.9920
Training Epoch: 7 [16640/50048]	Loss: 1.8199
Training Epoch: 7 [16768/50048]	Loss: 1.7967
Training Epoch: 7 [16896/50048]	Loss: 1.9113
Training Epoch: 7 [17024/50048]	Loss: 2.0153
Training Epoch: 7 [17152/50048]	Loss: 1.9435
Training Epoch: 7 [17280/50048]	Loss: 2.2001
Training Epoch: 7 [17408/50048]	Loss: 1.8206
Training Epoch: 7 [17536/50048]	Loss: 1.8634
Training Epoch: 7 [17664/50048]	Loss: 2.0050
Training Epoch: 7 [17792/50048]	Loss: 1.8290
Training Epoch: 7 [17920/50048]	Loss: 1.8544
Training Epoch: 7 [18048/50048]	Loss: 1.8416
Training Epoch: 7 [18176/50048]	Loss: 1.8415
Training Epoch: 7 [18304/50048]	Loss: 1.7941
Training Epoch: 7 [18432/50048]	Loss: 1.7270
Training Epoch: 7 [18560/50048]	Loss: 1.9067
Training Epoch: 7 [18688/50048]	Loss: 1.6248
Training Epoch: 7 [18816/50048]	Loss: 1.6733
Training Epoch: 7 [18944/50048]	Loss: 2.0312
Training Epoch: 7 [19072/50048]	Loss: 1.9409
Training Epoch: 7 [19200/50048]	Loss: 2.1404
Training Epoch: 7 [19328/50048]	Loss: 1.8135
Training Epoch: 7 [19456/50048]	Loss: 1.9711
Training Epoch: 7 [19584/50048]	Loss: 1.9800
Training Epoch: 7 [19712/50048]	Loss: 1.7704
Training Epoch: 7 [19840/50048]	Loss: 1.9932
Training Epoch: 7 [19968/50048]	Loss: 1.8004
Training Epoch: 7 [20096/50048]	Loss: 1.9643
Training Epoch: 7 [20224/50048]	Loss: 1.8700
Training Epoch: 7 [20352/50048]	Loss: 2.0140
Training Epoch: 7 [20480/50048]	Loss: 1.8942
Training Epoch: 7 [20608/50048]	Loss: 1.9324
Training Epoch: 7 [20736/50048]	Loss: 1.9095
Training Epoch: 7 [20864/50048]	Loss: 2.2123
Training Epoch: 7 [20992/50048]	Loss: 1.8623
Training Epoch: 7 [21120/50048]	Loss: 1.8145
Training Epoch: 7 [21248/50048]	Loss: 1.9394
Training Epoch: 7 [21376/50048]	Loss: 1.9942
Training Epoch: 7 [21504/50048]	Loss: 1.6754
Training Epoch: 7 [21632/50048]	Loss: 1.7889
Training Epoch: 7 [21760/50048]	Loss: 1.6757
Training Epoch: 7 [21888/50048]	Loss: 1.7425
Training Epoch: 7 [22016/50048]	Loss: 1.7349
Training Epoch: 7 [22144/50048]	Loss: 1.6774
Training Epoch: 7 [22272/50048]	Loss: 1.7906
Training Epoch: 7 [22400/50048]	Loss: 2.0177
Training Epoch: 7 [22528/50048]	Loss: 2.1160
Training Epoch: 7 [22656/50048]	Loss: 1.8438
Training Epoch: 7 [22784/50048]	Loss: 1.7826
Training Epoch: 7 [22912/50048]	Loss: 1.7767
Training Epoch: 7 [23040/50048]	Loss: 1.8199
Training Epoch: 7 [23168/50048]	Loss: 1.9716
Training Epoch: 7 [23296/50048]	Loss: 1.8929
Training Epoch: 7 [23424/50048]	Loss: 2.0684
Training Epoch: 7 [23552/50048]	Loss: 2.1202
Training Epoch: 7 [23680/50048]	Loss: 1.9279
Training Epoch: 7 [23808/50048]	Loss: 1.8888
Training Epoch: 7 [23936/50048]	Loss: 1.7231
Training Epoch: 7 [24064/50048]	Loss: 1.7318
Training Epoch: 7 [24192/50048]	Loss: 1.7428
Training Epoch: 7 [24320/50048]	Loss: 1.6975
Training Epoch: 7 [24448/50048]	Loss: 2.1250
Training Epoch: 7 [24576/50048]	Loss: 1.8280
Training Epoch: 7 [24704/50048]	Loss: 2.0308
Training Epoch: 7 [24832/50048]	Loss: 1.9574
Training Epoch: 7 [24960/50048]	Loss: 2.0599
Training Epoch: 7 [25088/50048]	Loss: 2.3374
Training Epoch: 7 [25216/50048]	Loss: 1.7307
Training Epoch: 7 [25344/50048]	Loss: 1.7733
Training Epoch: 7 [25472/50048]	Loss: 1.7028
Training Epoch: 7 [25600/50048]	Loss: 1.9009
Training Epoch: 7 [25728/50048]	Loss: 1.6435
Training Epoch: 7 [25856/50048]	Loss: 1.7896
Training Epoch: 7 [25984/50048]	Loss: 1.9634
Training Epoch: 7 [26112/50048]	Loss: 1.9503
Training Epoch: 7 [26240/50048]	Loss: 1.7754
Training Epoch: 7 [26368/50048]	Loss: 2.0455
Training Epoch: 7 [26496/50048]	Loss: 1.8761
Training Epoch: 7 [26624/50048]	Loss: 1.9182
Training Epoch: 7 [26752/50048]	Loss: 1.9068
Training Epoch: 7 [26880/50048]	Loss: 2.0188
Training Epoch: 7 [27008/50048]	Loss: 1.9495
Training Epoch: 7 [27136/50048]	Loss: 1.9090
Training Epoch: 7 [27264/50048]	Loss: 1.8807
Training Epoch: 7 [27392/50048]	Loss: 1.8164
Training Epoch: 7 [27520/50048]	Loss: 2.0845
Training Epoch: 7 [27648/50048]	Loss: 1.8031
Training Epoch: 7 [27776/50048]	Loss: 1.8515
Training Epoch: 7 [27904/50048]	Loss: 1.6304
Training Epoch: 7 [28032/50048]	Loss: 2.0034
Training Epoch: 7 [28160/50048]	Loss: 1.9145
Training Epoch: 7 [28288/50048]	Loss: 1.6526
Training Epoch: 7 [28416/50048]	Loss: 1.7028
Training Epoch: 7 [28544/50048]	Loss: 1.6228
Training Epoch: 7 [28672/50048]	Loss: 1.9178
Training Epoch: 7 [28800/50048]	Loss: 1.9958
Training Epoch: 7 [28928/50048]	Loss: 1.9615
Training Epoch: 7 [29056/50048]	Loss: 1.7291
Training Epoch: 7 [29184/50048]	Loss: 1.7735
Training Epoch: 7 [29312/50048]	Loss: 1.9978
Training Epoch: 7 [29440/50048]	Loss: 1.7771
Training Epoch: 7 [29568/50048]	Loss: 1.7726
Training Epoch: 7 [29696/50048]	Loss: 1.8382
Training Epoch: 7 [29824/50048]	Loss: 2.0439
Training Epoch: 7 [29952/50048]	Loss: 1.7965
Training Epoch: 7 [30080/50048]	Loss: 1.8553
Training Epoch: 7 [30208/50048]	Loss: 1.7282
Training Epoch: 7 [30336/50048]	Loss: 1.7242
Training Epoch: 7 [30464/50048]	Loss: 1.9833
Training Epoch: 7 [30592/50048]	Loss: 1.7349
Training Epoch: 7 [30720/50048]	Loss: 1.9477
Training Epoch: 7 [30848/50048]	Loss: 1.6661
Training Epoch: 7 [30976/50048]	Loss: 1.8402
Training Epoch: 7 [31104/50048]	Loss: 1.9443
Training Epoch: 7 [31232/50048]	Loss: 1.9146
Training Epoch: 7 [31360/50048]	Loss: 1.9868
Training Epoch: 7 [31488/50048]	Loss: 1.9508
Training Epoch: 7 [31616/50048]	Loss: 2.2580
Training Epoch: 7 [31744/50048]	Loss: 1.9341
Training Epoch: 7 [31872/50048]	Loss: 1.8332
Training Epoch: 7 [32000/50048]	Loss: 1.7381
Training Epoch: 7 [32128/50048]	Loss: 1.7149
Training Epoch: 7 [32256/50048]	Loss: 2.1069
Training Epoch: 7 [32384/50048]	Loss: 1.6660
Training Epoch: 7 [32512/50048]	Loss: 1.8150
Training Epoch: 7 [32640/50048]	Loss: 1.8526
Training Epoch: 7 [32768/50048]	Loss: 1.8570
Training Epoch: 7 [32896/50048]	Loss: 1.8265
Training Epoch: 7 [33024/50048]	Loss: 1.8701
Training Epoch: 7 [33152/50048]	Loss: 1.7661
Training Epoch: 7 [33280/50048]	Loss: 1.7731
Training Epoch: 7 [33408/50048]	Loss: 1.7228
Training Epoch: 7 [33536/50048]	Loss: 1.7984
Training Epoch: 7 [33664/50048]	Loss: 1.8993
Training Epoch: 7 [33792/50048]	Loss: 1.8272
Training Epoch: 7 [33920/50048]	Loss: 2.1506
Training Epoch: 7 [34048/50048]	Loss: 1.8662
Training Epoch: 7 [34176/50048]	Loss: 1.7400
Training Epoch: 7 [34304/50048]	Loss: 1.7210
Training Epoch: 7 [34432/50048]	Loss: 1.9164
Training Epoch: 7 [34560/50048]	Loss: 1.9999
Training Epoch: 7 [34688/50048]	Loss: 2.2276
Training Epoch: 7 [34816/50048]	Loss: 2.0781
Training Epoch: 7 [34944/50048]	Loss: 1.9497
Training Epoch: 7 [35072/50048]	Loss: 1.9792
Training Epoch: 7 [35200/50048]	Loss: 1.8470
Training Epoch: 7 [35328/50048]	Loss: 1.8313
Training Epoch: 7 [35456/50048]	Loss: 1.7472
Training Epoch: 7 [35584/50048]	Loss: 1.8421
Training Epoch: 7 [35712/50048]	Loss: 1.8190
Training Epoch: 7 [35840/50048]	Loss: 2.1284
Training Epoch: 7 [35968/50048]	Loss: 1.7083
Training Epoch: 7 [36096/50048]	Loss: 1.7666
Training Epoch: 7 [36224/50048]	Loss: 1.8302
Training Epoch: 7 [36352/50048]	Loss: 1.7660
Training Epoch: 7 [36480/50048]	Loss: 1.7112
Training Epoch: 7 [36608/50048]	Loss: 1.9143
Training Epoch: 7 [36736/50048]	Loss: 1.6998
Training Epoch: 7 [36864/50048]	Loss: 1.4294
Training Epoch: 7 [36992/50048]	Loss: 1.7609
Training Epoch: 7 [37120/50048]	Loss: 1.8789
Training Epoch: 7 [37248/50048]	Loss: 1.6803
Training Epoch: 7 [37376/50048]	Loss: 1.8824
Training Epoch: 7 [37504/50048]	Loss: 1.7879
Training Epoch: 7 [37632/50048]	Loss: 1.8981
Training Epoch: 7 [37760/50048]	Loss: 1.8189
Training Epoch: 7 [37888/50048]	Loss: 1.8747
Training Epoch: 7 [38016/50048]	Loss: 1.7028
Training Epoch: 7 [38144/50048]	Loss: 1.7303
Training Epoch: 7 [38272/50048]	Loss: 1.8128
Training Epoch: 7 [38400/50048]	Loss: 1.9146
Training Epoch: 7 [38528/50048]	Loss: 2.1711
Training Epoch: 7 [38656/50048]	Loss: 1.7954
Training Epoch: 7 [38784/50048]	Loss: 1.8347
Training Epoch: 7 [38912/50048]	Loss: 1.8867
Training Epoch: 7 [39040/50048]	Loss: 1.8098
Training Epoch: 7 [39168/50048]	Loss: 2.2623
Training Epoch: 7 [39296/50048]	Loss: 1.9065
Training Epoch: 7 [39424/50048]	Loss: 2.0960
Training Epoch: 7 [39552/50048]	Loss: 1.8928
Training Epoch: 7 [39680/50048]	Loss: 1.7164
Training Epoch: 7 [39808/50048]	Loss: 1.6968
Training Epoch: 7 [39936/50048]	Loss: 2.1656
Training Epoch: 7 [40064/50048]	Loss: 2.2209
Training Epoch: 7 [40192/50048]	Loss: 2.1446
Training Epoch: 7 [40320/50048]	Loss: 1.8908
Training Epoch: 7 [40448/50048]	Loss: 1.8782
Training Epoch: 7 [40576/50048]	Loss: 1.8895
Training Epoch: 7 [40704/50048]	Loss: 2.0265
Training Epoch: 7 [40832/50048]	Loss: 1.6667
Training Epoch: 7 [40960/50048]	Loss: 1.7905
Training Epoch: 7 [41088/50048]	Loss: 1.7436
Training Epoch: 7 [41216/50048]	Loss: 1.6595
Training Epoch: 7 [41344/50048]	Loss: 1.8461
Training Epoch: 7 [41472/50048]	Loss: 1.8727
Training Epoch: 7 [41600/50048]	Loss: 1.6148
Training Epoch: 7 [41728/50048]	Loss: 1.8622
Training Epoch: 7 [41856/50048]	Loss: 1.8046
Training Epoch: 7 [41984/50048]	Loss: 1.9653
Training Epoch: 7 [42112/50048]	Loss: 2.0260
Training Epoch: 7 [42240/50048]	Loss: 1.8266
Training Epoch: 7 [42368/50048]	Loss: 1.6266
Training Epoch: 7 [42496/50048]	Loss: 1.8816
Training Epoch: 7 [42624/50048]	Loss: 1.7209
Training Epoch: 7 [42752/50048]	Loss: 2.0064
Training Epoch: 7 [42880/50048]	Loss: 1.4809
Training Epoch: 7 [43008/50048]	Loss: 1.8303
Training Epoch: 7 [43136/50048]	Loss: 1.8116
Training Epoch: 7 [43264/50048]	Loss: 1.8683
Training Epoch: 7 [43392/50048]	Loss: 2.2023
Training Epoch: 7 [43520/50048]	Loss: 1.5969
Training Epoch: 7 [43648/50048]	Loss: 1.7807
Training Epoch: 7 [43776/50048]	Loss: 1.7463
Training Epoch: 7 [43904/50048]	Loss: 1.9805
Training Epoch: 7 [44032/50048]	Loss: 2.0732
Training Epoch: 7 [44160/50048]	Loss: 1.9268
Training Epoch: 7 [44288/50048]	Loss: 1.9819
Training Epoch: 7 [44416/50048]	Loss: 1.7828
Training Epoch: 7 [44544/50048]	Loss: 1.8984
Training Epoch: 7 [44672/50048]	Loss: 1.7667
Training Epoch: 7 [44800/50048]	Loss: 1.8085
Training Epoch: 7 [44928/50048]	Loss: 2.0173
Training Epoch: 7 [45056/50048]	Loss: 1.7051
Training Epoch: 7 [45184/50048]	Loss: 1.9990
Training Epoch: 7 [45312/50048]	Loss: 1.9525
Training Epoch: 7 [45440/50048]	Loss: 1.7847
Training Epoch: 7 [45568/50048]	Loss: 1.7055
Training Epoch: 7 [45696/50048]	Loss: 1.9401
Training Epoch: 7 [45824/50048]	Loss: 1.6752
Training Epoch: 7 [45952/50048]	Loss: 1.7004
Training Epoch: 7 [46080/50048]	Loss: 2.0553
Training Epoch: 7 [46208/50048]	Loss: 1.9468
Training Epoch: 7 [46336/50048]	Loss: 1.8936
Training Epoch: 7 [46464/50048]	Loss: 1.7858
Training Epoch: 7 [46592/50048]	Loss: 1.8729
Training Epoch: 7 [46720/50048]	Loss: 1.8672
2022-12-06 06:19:51,981 [ZeusDataLoader(train)] train epoch 8 done: time=86.62 energy=10505.27
2022-12-06 06:19:51,982 [ZeusDataLoader(eval)] Epoch 8 begin.
Training Epoch: 7 [46848/50048]	Loss: 1.9484
Training Epoch: 7 [46976/50048]	Loss: 1.8304
Training Epoch: 7 [47104/50048]	Loss: 1.6627
Training Epoch: 7 [47232/50048]	Loss: 1.7820
Training Epoch: 7 [47360/50048]	Loss: 1.6606
Training Epoch: 7 [47488/50048]	Loss: 1.7211
Training Epoch: 7 [47616/50048]	Loss: 2.0402
Training Epoch: 7 [47744/50048]	Loss: 1.9209
Training Epoch: 7 [47872/50048]	Loss: 1.6608
Training Epoch: 7 [48000/50048]	Loss: 1.5294
Training Epoch: 7 [48128/50048]	Loss: 1.9629
Training Epoch: 7 [48256/50048]	Loss: 1.8004
Training Epoch: 7 [48384/50048]	Loss: 2.0321
Training Epoch: 7 [48512/50048]	Loss: 1.6576
Training Epoch: 7 [48640/50048]	Loss: 2.0912
Training Epoch: 7 [48768/50048]	Loss: 2.0140
Training Epoch: 7 [48896/50048]	Loss: 1.7940
Training Epoch: 7 [49024/50048]	Loss: 2.0568
Training Epoch: 7 [49152/50048]	Loss: 2.0140
Training Epoch: 7 [49280/50048]	Loss: 1.7274
Training Epoch: 7 [49408/50048]	Loss: 1.8999
Training Epoch: 7 [49536/50048]	Loss: 1.8525
Training Epoch: 7 [49664/50048]	Loss: 1.6692
Training Epoch: 7 [49792/50048]	Loss: 1.7945
Training Epoch: 7 [49920/50048]	Loss: 1.8603
Training Epoch: 7 [50048/50048]	Loss: 2.1037
2022-12-06 11:19:55.664 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:19:55,697 [ZeusDataLoader(eval)] eval epoch 8 done: time=3.71 energy=451.47
2022-12-06 06:19:55,698 [ZeusDataLoader(train)] Up to epoch 8: time=721.92, energy=87583.05, cost=106959.14
2022-12-06 06:19:55,698 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:19:55,698 [ZeusDataLoader(train)] Expected next epoch: time=811.71, energy=98381.07, cost=120215.53
2022-12-06 06:19:55,699 [ZeusDataLoader(train)] Epoch 9 begin.
Validation Epoch: 7, Average loss: 0.0154, Accuracy: 0.4751
2022-12-06 06:19:55,862 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:19:55,862 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:19:55.864 [ZeusMonitor] Monitor started.
2022-12-06 11:19:55.864 [ZeusMonitor] Running indefinitely. 2022-12-06 11:19:55.864 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:19:55.864 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e9+gpu0.power.log
Training Epoch: 8 [128/50048]	Loss: 1.7697
Training Epoch: 8 [256/50048]	Loss: 1.6608
Training Epoch: 8 [384/50048]	Loss: 1.8857
Training Epoch: 8 [512/50048]	Loss: 1.9981
Training Epoch: 8 [640/50048]	Loss: 2.0144
Training Epoch: 8 [768/50048]	Loss: 1.8486
Training Epoch: 8 [896/50048]	Loss: 1.8827
Training Epoch: 8 [1024/50048]	Loss: 1.9760
Training Epoch: 8 [1152/50048]	Loss: 1.7458
Training Epoch: 8 [1280/50048]	Loss: 1.6619
Training Epoch: 8 [1408/50048]	Loss: 1.6440
Training Epoch: 8 [1536/50048]	Loss: 1.7173
Training Epoch: 8 [1664/50048]	Loss: 1.7794
Training Epoch: 8 [1792/50048]	Loss: 1.7138
Training Epoch: 8 [1920/50048]	Loss: 2.1033
Training Epoch: 8 [2048/50048]	Loss: 1.8574
Training Epoch: 8 [2176/50048]	Loss: 1.7690
Training Epoch: 8 [2304/50048]	Loss: 1.7863
Training Epoch: 8 [2432/50048]	Loss: 2.0767
Training Epoch: 8 [2560/50048]	Loss: 1.6804
Training Epoch: 8 [2688/50048]	Loss: 1.8394
Training Epoch: 8 [2816/50048]	Loss: 1.5521
Training Epoch: 8 [2944/50048]	Loss: 1.8229
Training Epoch: 8 [3072/50048]	Loss: 1.7648
Training Epoch: 8 [3200/50048]	Loss: 1.9934
Training Epoch: 8 [3328/50048]	Loss: 1.5239
Training Epoch: 8 [3456/50048]	Loss: 1.6553
Training Epoch: 8 [3584/50048]	Loss: 1.7082
Training Epoch: 8 [3712/50048]	Loss: 1.5870
Training Epoch: 8 [3840/50048]	Loss: 1.8674
Training Epoch: 8 [3968/50048]	Loss: 1.6928
Training Epoch: 8 [4096/50048]	Loss: 1.7630
Training Epoch: 8 [4224/50048]	Loss: 1.5473
Training Epoch: 8 [4352/50048]	Loss: 1.7411
Training Epoch: 8 [4480/50048]	Loss: 1.6452
Training Epoch: 8 [4608/50048]	Loss: 1.6634
Training Epoch: 8 [4736/50048]	Loss: 1.8728
Training Epoch: 8 [4864/50048]	Loss: 1.9962
Training Epoch: 8 [4992/50048]	Loss: 1.5141
Training Epoch: 8 [5120/50048]	Loss: 1.5399
Training Epoch: 8 [5248/50048]	Loss: 1.9231
Training Epoch: 8 [5376/50048]	Loss: 1.8128
Training Epoch: 8 [5504/50048]	Loss: 1.6384
Training Epoch: 8 [5632/50048]	Loss: 1.8533
Training Epoch: 8 [5760/50048]	Loss: 1.6734
Training Epoch: 8 [5888/50048]	Loss: 1.5216
Training Epoch: 8 [6016/50048]	Loss: 1.4071
Training Epoch: 8 [6144/50048]	Loss: 1.7104
Training Epoch: 8 [6272/50048]	Loss: 1.4759
Training Epoch: 8 [6400/50048]	Loss: 1.6883
Training Epoch: 8 [6528/50048]	Loss: 1.9775
Training Epoch: 8 [6656/50048]	Loss: 1.8692
Training Epoch: 8 [6784/50048]	Loss: 1.7943
Training Epoch: 8 [6912/50048]	Loss: 1.8287
Training Epoch: 8 [7040/50048]	Loss: 1.9948
Training Epoch: 8 [7168/50048]	Loss: 1.7671
Training Epoch: 8 [7296/50048]	Loss: 1.9968
Training Epoch: 8 [7424/50048]	Loss: 1.8306
Training Epoch: 8 [7552/50048]	Loss: 1.6873
Training Epoch: 8 [7680/50048]	Loss: 1.6781
Training Epoch: 8 [7808/50048]	Loss: 1.9749
Training Epoch: 8 [7936/50048]	Loss: 1.7808
Training Epoch: 8 [8064/50048]	Loss: 1.5845
Training Epoch: 8 [8192/50048]	Loss: 1.6951
Training Epoch: 8 [8320/50048]	Loss: 1.6295
Training Epoch: 8 [8448/50048]	Loss: 1.6272
Training Epoch: 8 [8576/50048]	Loss: 1.7845
Training Epoch: 8 [8704/50048]	Loss: 1.9854
Training Epoch: 8 [8832/50048]	Loss: 1.8316
Training Epoch: 8 [8960/50048]	Loss: 1.8330
Training Epoch: 8 [9088/50048]	Loss: 1.7600
Training Epoch: 8 [9216/50048]	Loss: 1.5542
Training Epoch: 8 [9344/50048]	Loss: 1.9456
Training Epoch: 8 [9472/50048]	Loss: 1.6669
Training Epoch: 8 [9600/50048]	Loss: 1.6703
Training Epoch: 8 [9728/50048]	Loss: 2.0439
Training Epoch: 8 [9856/50048]	Loss: 1.6260
Training Epoch: 8 [9984/50048]	Loss: 1.6585
Training Epoch: 8 [10112/50048]	Loss: 1.8137
Training Epoch: 8 [10240/50048]	Loss: 1.8172
Training Epoch: 8 [10368/50048]	Loss: 1.9660
Training Epoch: 8 [10496/50048]	Loss: 1.8052
Training Epoch: 8 [10624/50048]	Loss: 1.8034
Training Epoch: 8 [10752/50048]	Loss: 1.8604
Training Epoch: 8 [10880/50048]	Loss: 1.8063
Training Epoch: 8 [11008/50048]	Loss: 1.7445
Training Epoch: 8 [11136/50048]	Loss: 1.9488
Training Epoch: 8 [11264/50048]	Loss: 2.0413
Training Epoch: 8 [11392/50048]	Loss: 1.9493
Training Epoch: 8 [11520/50048]	Loss: 1.8409
Training Epoch: 8 [11648/50048]	Loss: 1.6602
Training Epoch: 8 [11776/50048]	Loss: 1.6530
Training Epoch: 8 [11904/50048]	Loss: 1.5844
Training Epoch: 8 [12032/50048]	Loss: 1.8247
Training Epoch: 8 [12160/50048]	Loss: 1.4776
Training Epoch: 8 [12288/50048]	Loss: 1.7633
Training Epoch: 8 [12416/50048]	Loss: 1.7684
Training Epoch: 8 [12544/50048]	Loss: 1.9701
Training Epoch: 8 [12672/50048]	Loss: 1.7467
Training Epoch: 8 [12800/50048]	Loss: 1.7983
Training Epoch: 8 [12928/50048]	Loss: 1.6518
Training Epoch: 8 [13056/50048]	Loss: 1.6881
Training Epoch: 8 [13184/50048]	Loss: 1.5995
Training Epoch: 8 [13312/50048]	Loss: 1.5839
Training Epoch: 8 [13440/50048]	Loss: 1.9274
Training Epoch: 8 [13568/50048]	Loss: 1.6496
Training Epoch: 8 [13696/50048]	Loss: 1.6941
Training Epoch: 8 [13824/50048]	Loss: 1.8103
Training Epoch: 8 [13952/50048]	Loss: 2.0086
Training Epoch: 8 [14080/50048]	Loss: 1.5927
Training Epoch: 8 [14208/50048]	Loss: 1.8760
Training Epoch: 8 [14336/50048]	Loss: 1.8650
Training Epoch: 8 [14464/50048]	Loss: 1.7542
Training Epoch: 8 [14592/50048]	Loss: 1.9621
Training Epoch: 8 [14720/50048]	Loss: 1.6661
Training Epoch: 8 [14848/50048]	Loss: 1.6191
Training Epoch: 8 [14976/50048]	Loss: 1.5584
Training Epoch: 8 [15104/50048]	Loss: 1.7857
Training Epoch: 8 [15232/50048]	Loss: 2.0047
Training Epoch: 8 [15360/50048]	Loss: 1.7540
Training Epoch: 8 [15488/50048]	Loss: 1.5630
Training Epoch: 8 [15616/50048]	Loss: 1.6049
Training Epoch: 8 [15744/50048]	Loss: 1.9116
Training Epoch: 8 [15872/50048]	Loss: 1.7658
Training Epoch: 8 [16000/50048]	Loss: 1.5515
Training Epoch: 8 [16128/50048]	Loss: 1.7980
Training Epoch: 8 [16256/50048]	Loss: 1.6444
Training Epoch: 8 [16384/50048]	Loss: 1.6747
Training Epoch: 8 [16512/50048]	Loss: 1.5390
Training Epoch: 8 [16640/50048]	Loss: 1.9501
Training Epoch: 8 [16768/50048]	Loss: 1.6561
Training Epoch: 8 [16896/50048]	Loss: 1.6465
Training Epoch: 8 [17024/50048]	Loss: 2.0275
Training Epoch: 8 [17152/50048]	Loss: 2.0824
Training Epoch: 8 [17280/50048]	Loss: 1.8379
Training Epoch: 8 [17408/50048]	Loss: 1.8891
Training Epoch: 8 [17536/50048]	Loss: 1.5481
Training Epoch: 8 [17664/50048]	Loss: 1.6092
Training Epoch: 8 [17792/50048]	Loss: 1.5939
Training Epoch: 8 [17920/50048]	Loss: 1.7252
Training Epoch: 8 [18048/50048]	Loss: 2.1687
Training Epoch: 8 [18176/50048]	Loss: 1.7542
Training Epoch: 8 [18304/50048]	Loss: 1.8836
Training Epoch: 8 [18432/50048]	Loss: 1.7745
Training Epoch: 8 [18560/50048]	Loss: 1.8947
Training Epoch: 8 [18688/50048]	Loss: 1.6042
Training Epoch: 8 [18816/50048]	Loss: 1.7415
Training Epoch: 8 [18944/50048]	Loss: 1.7404
Training Epoch: 8 [19072/50048]	Loss: 1.5980
Training Epoch: 8 [19200/50048]	Loss: 1.8587
Training Epoch: 8 [19328/50048]	Loss: 1.5575
Training Epoch: 8 [19456/50048]	Loss: 1.6351
Training Epoch: 8 [19584/50048]	Loss: 1.5643
Training Epoch: 8 [19712/50048]	Loss: 1.6669
Training Epoch: 8 [19840/50048]	Loss: 2.0795
Training Epoch: 8 [19968/50048]	Loss: 1.8622
Training Epoch: 8 [20096/50048]	Loss: 1.5149
Training Epoch: 8 [20224/50048]	Loss: 1.7225
Training Epoch: 8 [20352/50048]	Loss: 1.8416
Training Epoch: 8 [20480/50048]	Loss: 1.9653
Training Epoch: 8 [20608/50048]	Loss: 1.5481
Training Epoch: 8 [20736/50048]	Loss: 2.0172
Training Epoch: 8 [20864/50048]	Loss: 1.6959
Training Epoch: 8 [20992/50048]	Loss: 1.7204
Training Epoch: 8 [21120/50048]	Loss: 1.6958
Training Epoch: 8 [21248/50048]	Loss: 1.3715
Training Epoch: 8 [21376/50048]	Loss: 1.6751
Training Epoch: 8 [21504/50048]	Loss: 1.6860
Training Epoch: 8 [21632/50048]	Loss: 1.4570
Training Epoch: 8 [21760/50048]	Loss: 1.6493
Training Epoch: 8 [21888/50048]	Loss: 1.7986
Training Epoch: 8 [22016/50048]	Loss: 1.8520
Training Epoch: 8 [22144/50048]	Loss: 1.5843
Training Epoch: 8 [22272/50048]	Loss: 1.8764
Training Epoch: 8 [22400/50048]	Loss: 1.6816
Training Epoch: 8 [22528/50048]	Loss: 1.6396
Training Epoch: 8 [22656/50048]	Loss: 1.8650
Training Epoch: 8 [22784/50048]	Loss: 1.8327
Training Epoch: 8 [22912/50048]	Loss: 1.8735
Training Epoch: 8 [23040/50048]	Loss: 1.7680
Training Epoch: 8 [23168/50048]	Loss: 1.6217
Training Epoch: 8 [23296/50048]	Loss: 1.7277
Training Epoch: 8 [23424/50048]	Loss: 1.6666
Training Epoch: 8 [23552/50048]	Loss: 1.7635
Training Epoch: 8 [23680/50048]	Loss: 1.8309
Training Epoch: 8 [23808/50048]	Loss: 1.9558
Training Epoch: 8 [23936/50048]	Loss: 1.4558
Training Epoch: 8 [24064/50048]	Loss: 1.8245
Training Epoch: 8 [24192/50048]	Loss: 1.6360
Training Epoch: 8 [24320/50048]	Loss: 1.8186
Training Epoch: 8 [24448/50048]	Loss: 1.6857
Training Epoch: 8 [24576/50048]	Loss: 1.7614
Training Epoch: 8 [24704/50048]	Loss: 1.6678
Training Epoch: 8 [24832/50048]	Loss: 1.8391
Training Epoch: 8 [24960/50048]	Loss: 1.7755
Training Epoch: 8 [25088/50048]	Loss: 1.9316
Training Epoch: 8 [25216/50048]	Loss: 1.6300
Training Epoch: 8 [25344/50048]	Loss: 1.6858
Training Epoch: 8 [25472/50048]	Loss: 1.7435
Training Epoch: 8 [25600/50048]	Loss: 1.8597
Training Epoch: 8 [25728/50048]	Loss: 1.6925
Training Epoch: 8 [25856/50048]	Loss: 1.5554
Training Epoch: 8 [25984/50048]	Loss: 1.8358
Training Epoch: 8 [26112/50048]	Loss: 1.7499
Training Epoch: 8 [26240/50048]	Loss: 1.6663
Training Epoch: 8 [26368/50048]	Loss: 1.9629
Training Epoch: 8 [26496/50048]	Loss: 1.5770
Training Epoch: 8 [26624/50048]	Loss: 1.6391
Training Epoch: 8 [26752/50048]	Loss: 1.8278
Training Epoch: 8 [26880/50048]	Loss: 1.7633
Training Epoch: 8 [27008/50048]	Loss: 1.8080
Training Epoch: 8 [27136/50048]	Loss: 1.4458
Training Epoch: 8 [27264/50048]	Loss: 1.8179
Training Epoch: 8 [27392/50048]	Loss: 1.7387
Training Epoch: 8 [27520/50048]	Loss: 1.7717
Training Epoch: 8 [27648/50048]	Loss: 1.6981
Training Epoch: 8 [27776/50048]	Loss: 2.0069
Training Epoch: 8 [27904/50048]	Loss: 1.7545
Training Epoch: 8 [28032/50048]	Loss: 1.6164
Training Epoch: 8 [28160/50048]	Loss: 1.8099
Training Epoch: 8 [28288/50048]	Loss: 1.4952
Training Epoch: 8 [28416/50048]	Loss: 1.5953
Training Epoch: 8 [28544/50048]	Loss: 1.9556
Training Epoch: 8 [28672/50048]	Loss: 1.7172
Training Epoch: 8 [28800/50048]	Loss: 1.5608
Training Epoch: 8 [28928/50048]	Loss: 1.4653
Training Epoch: 8 [29056/50048]	Loss: 1.8079
Training Epoch: 8 [29184/50048]	Loss: 1.9341
Training Epoch: 8 [29312/50048]	Loss: 1.6965
Training Epoch: 8 [29440/50048]	Loss: 2.0826
Training Epoch: 8 [29568/50048]	Loss: 1.7201
Training Epoch: 8 [29696/50048]	Loss: 1.6173
Training Epoch: 8 [29824/50048]	Loss: 1.6769
Training Epoch: 8 [29952/50048]	Loss: 1.9452
Training Epoch: 8 [30080/50048]	Loss: 1.6809
Training Epoch: 8 [30208/50048]	Loss: 1.6428
Training Epoch: 8 [30336/50048]	Loss: 1.9415
Training Epoch: 8 [30464/50048]	Loss: 1.9801
Training Epoch: 8 [30592/50048]	Loss: 1.7171
Training Epoch: 8 [30720/50048]	Loss: 1.8527
Training Epoch: 8 [30848/50048]	Loss: 1.8031
Training Epoch: 8 [30976/50048]	Loss: 1.7571
Training Epoch: 8 [31104/50048]	Loss: 1.7090
Training Epoch: 8 [31232/50048]	Loss: 1.7812
Training Epoch: 8 [31360/50048]	Loss: 1.6152
Training Epoch: 8 [31488/50048]	Loss: 1.6747
Training Epoch: 8 [31616/50048]	Loss: 1.5723
Training Epoch: 8 [31744/50048]	Loss: 1.6150
Training Epoch: 8 [31872/50048]	Loss: 1.6386
Training Epoch: 8 [32000/50048]	Loss: 1.5749
Training Epoch: 8 [32128/50048]	Loss: 1.9438
Training Epoch: 8 [32256/50048]	Loss: 1.7944
Training Epoch: 8 [32384/50048]	Loss: 1.5826
Training Epoch: 8 [32512/50048]	Loss: 1.5178
Training Epoch: 8 [32640/50048]	Loss: 1.8582
Training Epoch: 8 [32768/50048]	Loss: 1.5677
Training Epoch: 8 [32896/50048]	Loss: 1.7584
Training Epoch: 8 [33024/50048]	Loss: 1.7991
Training Epoch: 8 [33152/50048]	Loss: 1.4632
Training Epoch: 8 [33280/50048]	Loss: 1.9036
Training Epoch: 8 [33408/50048]	Loss: 2.0161
Training Epoch: 8 [33536/50048]	Loss: 1.8630
Training Epoch: 8 [33664/50048]	Loss: 1.6474
Training Epoch: 8 [33792/50048]	Loss: 1.8830
Training Epoch: 8 [33920/50048]	Loss: 1.8627
Training Epoch: 8 [34048/50048]	Loss: 1.8891
Training Epoch: 8 [34176/50048]	Loss: 1.7661
Training Epoch: 8 [34304/50048]	Loss: 1.6588
Training Epoch: 8 [34432/50048]	Loss: 1.6513
Training Epoch: 8 [34560/50048]	Loss: 1.8776
Training Epoch: 8 [34688/50048]	Loss: 1.8004
Training Epoch: 8 [34816/50048]	Loss: 1.9183
Training Epoch: 8 [34944/50048]	Loss: 2.0886
Training Epoch: 8 [35072/50048]	Loss: 1.7316
Training Epoch: 8 [35200/50048]	Loss: 2.0247
Training Epoch: 8 [35328/50048]	Loss: 1.7058
Training Epoch: 8 [35456/50048]	Loss: 1.5975
Training Epoch: 8 [35584/50048]	Loss: 1.5947
Training Epoch: 8 [35712/50048]	Loss: 1.8475
Training Epoch: 8 [35840/50048]	Loss: 1.9691
Training Epoch: 8 [35968/50048]	Loss: 1.9122
Training Epoch: 8 [36096/50048]	Loss: 1.8517
Training Epoch: 8 [36224/50048]	Loss: 1.8693
Training Epoch: 8 [36352/50048]	Loss: 1.6850
Training Epoch: 8 [36480/50048]	Loss: 1.6634
Training Epoch: 8 [36608/50048]	Loss: 1.6336
Training Epoch: 8 [36736/50048]	Loss: 1.6435
Training Epoch: 8 [36864/50048]	Loss: 1.5572
Training Epoch: 8 [36992/50048]	Loss: 1.8912
Training Epoch: 8 [37120/50048]	Loss: 1.7567
Training Epoch: 8 [37248/50048]	Loss: 1.6338
Training Epoch: 8 [37376/50048]	Loss: 1.9697
Training Epoch: 8 [37504/50048]	Loss: 1.6199
Training Epoch: 8 [37632/50048]	Loss: 1.7484
Training Epoch: 8 [37760/50048]	Loss: 1.4606
Training Epoch: 8 [37888/50048]	Loss: 1.8989
Training Epoch: 8 [38016/50048]	Loss: 1.8576
Training Epoch: 8 [38144/50048]	Loss: 1.7026
Training Epoch: 8 [38272/50048]	Loss: 1.6533
Training Epoch: 8 [38400/50048]	Loss: 1.7001
Training Epoch: 8 [38528/50048]	Loss: 1.7471
Training Epoch: 8 [38656/50048]	Loss: 1.8296
Training Epoch: 8 [38784/50048]	Loss: 1.8323
Training Epoch: 8 [38912/50048]	Loss: 1.7042
Training Epoch: 8 [39040/50048]	Loss: 1.8822
Training Epoch: 8 [39168/50048]	Loss: 1.5786
Training Epoch: 8 [39296/50048]	Loss: 2.1273
Training Epoch: 8 [39424/50048]	Loss: 1.7279
Training Epoch: 8 [39552/50048]	Loss: 1.6533
Training Epoch: 8 [39680/50048]	Loss: 1.9899
Training Epoch: 8 [39808/50048]	Loss: 1.9719
Training Epoch: 8 [39936/50048]	Loss: 1.8730
Training Epoch: 8 [40064/50048]	Loss: 1.8629
Training Epoch: 8 [40192/50048]	Loss: 1.7482
Training Epoch: 8 [40320/50048]	Loss: 1.7294
Training Epoch: 8 [40448/50048]	Loss: 1.7221
Training Epoch: 8 [40576/50048]	Loss: 1.8308
Training Epoch: 8 [40704/50048]	Loss: 1.5446
Training Epoch: 8 [40832/50048]	Loss: 1.5756
Training Epoch: 8 [40960/50048]	Loss: 1.6817
Training Epoch: 8 [41088/50048]	Loss: 1.7886
Training Epoch: 8 [41216/50048]	Loss: 1.6389
Training Epoch: 8 [41344/50048]	Loss: 1.6367
Training Epoch: 8 [41472/50048]	Loss: 1.7186
Training Epoch: 8 [41600/50048]	Loss: 1.5340
Training Epoch: 8 [41728/50048]	Loss: 1.7654
Training Epoch: 8 [41856/50048]	Loss: 2.0969
Training Epoch: 8 [41984/50048]	Loss: 1.9490
Training Epoch: 8 [42112/50048]	Loss: 1.6756
Training Epoch: 8 [42240/50048]	Loss: 1.7570
Training Epoch: 8 [42368/50048]	Loss: 1.9662
Training Epoch: 8 [42496/50048]	Loss: 1.7137
Training Epoch: 8 [42624/50048]	Loss: 1.7006
Training Epoch: 8 [42752/50048]	Loss: 1.7771
Training Epoch: 8 [42880/50048]	Loss: 1.7658
Training Epoch: 8 [43008/50048]	Loss: 1.7122
Training Epoch: 8 [43136/50048]	Loss: 1.7095
Training Epoch: 8 [43264/50048]	Loss: 1.7143
Training Epoch: 8 [43392/50048]	Loss: 1.7024
Training Epoch: 8 [43520/50048]	Loss: 1.8967
Training Epoch: 8 [43648/50048]	Loss: 1.7852
Training Epoch: 8 [43776/50048]	Loss: 1.9810
Training Epoch: 8 [43904/50048]	Loss: 1.8978
Training Epoch: 8 [44032/50048]	Loss: 1.6606
Training Epoch: 8 [44160/50048]	Loss: 1.5358
Training Epoch: 8 [44288/50048]	Loss: 1.6030
Training Epoch: 8 [44416/50048]	Loss: 1.9591
Training Epoch: 8 [44544/50048]	Loss: 1.8080
Training Epoch: 8 [44672/50048]	Loss: 2.0201
Training Epoch: 8 [44800/50048]	Loss: 1.8732
Training Epoch: 8 [44928/50048]	Loss: 2.1163
Training Epoch: 8 [45056/50048]	Loss: 1.6224
Training Epoch: 8 [45184/50048]	Loss: 1.8806
Training Epoch: 8 [45312/50048]	Loss: 1.9109
Training Epoch: 8 [45440/50048]	Loss: 1.5619
Training Epoch: 8 [45568/50048]	Loss: 1.9018
Training Epoch: 8 [45696/50048]	Loss: 1.6897
Training Epoch: 8 [45824/50048]	Loss: 1.5588
Training Epoch: 8 [45952/50048]	Loss: 1.7895
Training Epoch: 8 [46080/50048]	Loss: 1.5525
Training Epoch: 8 [46208/50048]	Loss: 1.6515
Training Epoch: 8 [46336/50048]	Loss: 1.6752
Training Epoch: 8 [46464/50048]	Loss: 1.6360
Training Epoch: 8 [46592/50048]	Loss: 1.9385
Training Epoch: 8 [46720/50048]	Loss: 1.7564
2022-12-06 06:21:22,175 [ZeusDataLoader(train)] train epoch 9 done: time=86.47 energy=10514.54
2022-12-06 06:21:22,177 [ZeusDataLoader(eval)] Epoch 9 begin.
Training Epoch: 8 [46848/50048]	Loss: 1.9530
Training Epoch: 8 [46976/50048]	Loss: 1.8547
Training Epoch: 8 [47104/50048]	Loss: 1.8726
Training Epoch: 8 [47232/50048]	Loss: 1.6886
Training Epoch: 8 [47360/50048]	Loss: 1.7778
Training Epoch: 8 [47488/50048]	Loss: 1.7009
Training Epoch: 8 [47616/50048]	Loss: 1.8244
Training Epoch: 8 [47744/50048]	Loss: 1.7541
Training Epoch: 8 [47872/50048]	Loss: 1.6393
Training Epoch: 8 [48000/50048]	Loss: 1.6201
Training Epoch: 8 [48128/50048]	Loss: 1.9524
Training Epoch: 8 [48256/50048]	Loss: 1.8373
Training Epoch: 8 [48384/50048]	Loss: 2.0678
Training Epoch: 8 [48512/50048]	Loss: 1.8384
Training Epoch: 8 [48640/50048]	Loss: 1.7330
Training Epoch: 8 [48768/50048]	Loss: 1.7730
Training Epoch: 8 [48896/50048]	Loss: 1.6319
Training Epoch: 8 [49024/50048]	Loss: 1.5864
Training Epoch: 8 [49152/50048]	Loss: 1.7900
Training Epoch: 8 [49280/50048]	Loss: 1.9505
Training Epoch: 8 [49408/50048]	Loss: 1.6149
Training Epoch: 8 [49536/50048]	Loss: 1.8489
Training Epoch: 8 [49664/50048]	Loss: 1.6686
Training Epoch: 8 [49792/50048]	Loss: 1.7334
Training Epoch: 8 [49920/50048]	Loss: 1.7559
Training Epoch: 8 [50048/50048]	Loss: 1.4248
2022-12-06 11:21:25.820 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:21:25,834 [ZeusDataLoader(eval)] eval epoch 9 done: time=3.65 energy=441.12
2022-12-06 06:21:25,834 [ZeusDataLoader(train)] Up to epoch 9: time=812.03, energy=98538.72, cost=120321.99
2022-12-06 06:21:25,834 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:21:25,834 [ZeusDataLoader(train)] Expected next epoch: time=901.83, energy=109336.73, cost=133578.37
2022-12-06 06:21:25,835 [ZeusDataLoader(train)] Epoch 10 begin.
Validation Epoch: 8, Average loss: 0.0144, Accuracy: 0.5020
2022-12-06 06:21:26,018 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:21:26,019 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:21:26.033 [ZeusMonitor] Monitor started.
2022-12-06 11:21:26.033 [ZeusMonitor] Running indefinitely. 2022-12-06 11:21:26.033 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:21:26.033 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e10+gpu0.power.log
Training Epoch: 9 [128/50048]	Loss: 1.7877
Training Epoch: 9 [256/50048]	Loss: 1.7257
Training Epoch: 9 [384/50048]	Loss: 1.7082
Training Epoch: 9 [512/50048]	Loss: 1.7179
Training Epoch: 9 [640/50048]	Loss: 1.7212
Training Epoch: 9 [768/50048]	Loss: 1.7405
Training Epoch: 9 [896/50048]	Loss: 1.7658
Training Epoch: 9 [1024/50048]	Loss: 1.6007
Training Epoch: 9 [1152/50048]	Loss: 1.5897
Training Epoch: 9 [1280/50048]	Loss: 1.9724
Training Epoch: 9 [1408/50048]	Loss: 1.7740
Training Epoch: 9 [1536/50048]	Loss: 1.4639
Training Epoch: 9 [1664/50048]	Loss: 1.7344
Training Epoch: 9 [1792/50048]	Loss: 1.5609
Training Epoch: 9 [1920/50048]	Loss: 1.7346
Training Epoch: 9 [2048/50048]	Loss: 1.7408
Training Epoch: 9 [2176/50048]	Loss: 1.7871
Training Epoch: 9 [2304/50048]	Loss: 1.5284
Training Epoch: 9 [2432/50048]	Loss: 1.6281
Training Epoch: 9 [2560/50048]	Loss: 1.7873
Training Epoch: 9 [2688/50048]	Loss: 1.8366
Training Epoch: 9 [2816/50048]	Loss: 1.6838
Training Epoch: 9 [2944/50048]	Loss: 1.7761
Training Epoch: 9 [3072/50048]	Loss: 1.5700
Training Epoch: 9 [3200/50048]	Loss: 1.3786
Training Epoch: 9 [3328/50048]	Loss: 1.4870
Training Epoch: 9 [3456/50048]	Loss: 1.7301
Training Epoch: 9 [3584/50048]	Loss: 1.8826
Training Epoch: 9 [3712/50048]	Loss: 1.5541
Training Epoch: 9 [3840/50048]	Loss: 1.5023
Training Epoch: 9 [3968/50048]	Loss: 1.7894
Training Epoch: 9 [4096/50048]	Loss: 1.6938
Training Epoch: 9 [4224/50048]	Loss: 1.9394
Training Epoch: 9 [4352/50048]	Loss: 1.6673
Training Epoch: 9 [4480/50048]	Loss: 1.7330
Training Epoch: 9 [4608/50048]	Loss: 1.7739
Training Epoch: 9 [4736/50048]	Loss: 1.6531
Training Epoch: 9 [4864/50048]	Loss: 1.7063
Training Epoch: 9 [4992/50048]	Loss: 1.5725
Training Epoch: 9 [5120/50048]	Loss: 1.9299
Training Epoch: 9 [5248/50048]	Loss: 1.3978
Training Epoch: 9 [5376/50048]	Loss: 1.9152
Training Epoch: 9 [5504/50048]	Loss: 1.4449
Training Epoch: 9 [5632/50048]	Loss: 1.6440
Training Epoch: 9 [5760/50048]	Loss: 1.5912
Training Epoch: 9 [5888/50048]	Loss: 1.4046
Training Epoch: 9 [6016/50048]	Loss: 1.7159
Training Epoch: 9 [6144/50048]	Loss: 1.5389
Training Epoch: 9 [6272/50048]	Loss: 1.5898
Training Epoch: 9 [6400/50048]	Loss: 1.7756
Training Epoch: 9 [6528/50048]	Loss: 1.6495
Training Epoch: 9 [6656/50048]	Loss: 1.9198
Training Epoch: 9 [6784/50048]	Loss: 1.7043
Training Epoch: 9 [6912/50048]	Loss: 1.6021
Training Epoch: 9 [7040/50048]	Loss: 1.7479
Training Epoch: 9 [7168/50048]	Loss: 1.5948
Training Epoch: 9 [7296/50048]	Loss: 1.4012
Training Epoch: 9 [7424/50048]	Loss: 1.7843
Training Epoch: 9 [7552/50048]	Loss: 1.4408
Training Epoch: 9 [7680/50048]	Loss: 1.6101
Training Epoch: 9 [7808/50048]	Loss: 1.7090
Training Epoch: 9 [7936/50048]	Loss: 1.6921
Training Epoch: 9 [8064/50048]	Loss: 1.5594
Training Epoch: 9 [8192/50048]	Loss: 1.8073
Training Epoch: 9 [8320/50048]	Loss: 1.6930
Training Epoch: 9 [8448/50048]	Loss: 1.5102
Training Epoch: 9 [8576/50048]	Loss: 1.6667
Training Epoch: 9 [8704/50048]	Loss: 1.6439
Training Epoch: 9 [8832/50048]	Loss: 1.4810
Training Epoch: 9 [8960/50048]	Loss: 1.8521
Training Epoch: 9 [9088/50048]	Loss: 1.8178
Training Epoch: 9 [9216/50048]	Loss: 1.7284
Training Epoch: 9 [9344/50048]	Loss: 1.5973
Training Epoch: 9 [9472/50048]	Loss: 1.7266
Training Epoch: 9 [9600/50048]	Loss: 1.5596
Training Epoch: 9 [9728/50048]	Loss: 1.7154
Training Epoch: 9 [9856/50048]	Loss: 1.8805
Training Epoch: 9 [9984/50048]	Loss: 1.5035
Training Epoch: 9 [10112/50048]	Loss: 1.8582
Training Epoch: 9 [10240/50048]	Loss: 1.6540
Training Epoch: 9 [10368/50048]	Loss: 1.8051
Training Epoch: 9 [10496/50048]	Loss: 1.7372
Training Epoch: 9 [10624/50048]	Loss: 1.7750
Training Epoch: 9 [10752/50048]	Loss: 1.7915
Training Epoch: 9 [10880/50048]	Loss: 1.7122
Training Epoch: 9 [11008/50048]	Loss: 1.6161
Training Epoch: 9 [11136/50048]	Loss: 1.6612
Training Epoch: 9 [11264/50048]	Loss: 1.8071
Training Epoch: 9 [11392/50048]	Loss: 1.5970
Training Epoch: 9 [11520/50048]	Loss: 1.4661
Training Epoch: 9 [11648/50048]	Loss: 1.8159
Training Epoch: 9 [11776/50048]	Loss: 1.4061
Training Epoch: 9 [11904/50048]	Loss: 1.8070
Training Epoch: 9 [12032/50048]	Loss: 1.7549
Training Epoch: 9 [12160/50048]	Loss: 1.7954
Training Epoch: 9 [12288/50048]	Loss: 1.7681
Training Epoch: 9 [12416/50048]	Loss: 1.6010
Training Epoch: 9 [12544/50048]	Loss: 1.4024
Training Epoch: 9 [12672/50048]	Loss: 1.4498
Training Epoch: 9 [12800/50048]	Loss: 1.6707
Training Epoch: 9 [12928/50048]	Loss: 1.5188
Training Epoch: 9 [13056/50048]	Loss: 1.6763
Training Epoch: 9 [13184/50048]	Loss: 1.9168
Training Epoch: 9 [13312/50048]	Loss: 1.4748
Training Epoch: 9 [13440/50048]	Loss: 1.9280
Training Epoch: 9 [13568/50048]	Loss: 1.7259
Training Epoch: 9 [13696/50048]	Loss: 2.1039
Training Epoch: 9 [13824/50048]	Loss: 1.4180
Training Epoch: 9 [13952/50048]	Loss: 1.6433
Training Epoch: 9 [14080/50048]	Loss: 1.7170
Training Epoch: 9 [14208/50048]	Loss: 1.7398
Training Epoch: 9 [14336/50048]	Loss: 1.6452
Training Epoch: 9 [14464/50048]	Loss: 1.5277
Training Epoch: 9 [14592/50048]	Loss: 1.7220
Training Epoch: 9 [14720/50048]	Loss: 1.7203
Training Epoch: 9 [14848/50048]	Loss: 1.5988
Training Epoch: 9 [14976/50048]	Loss: 1.6928
Training Epoch: 9 [15104/50048]	Loss: 1.5493
Training Epoch: 9 [15232/50048]	Loss: 1.8744
Training Epoch: 9 [15360/50048]	Loss: 1.7538
Training Epoch: 9 [15488/50048]	Loss: 1.8903
Training Epoch: 9 [15616/50048]	Loss: 1.6314
Training Epoch: 9 [15744/50048]	Loss: 1.7158
Training Epoch: 9 [15872/50048]	Loss: 1.7021
Training Epoch: 9 [16000/50048]	Loss: 1.6532
Training Epoch: 9 [16128/50048]	Loss: 2.0549
Training Epoch: 9 [16256/50048]	Loss: 1.8609
Training Epoch: 9 [16384/50048]	Loss: 1.5517
Training Epoch: 9 [16512/50048]	Loss: 1.6930
Training Epoch: 9 [16640/50048]	Loss: 1.4411
Training Epoch: 9 [16768/50048]	Loss: 1.6018
Training Epoch: 9 [16896/50048]	Loss: 1.7608
Training Epoch: 9 [17024/50048]	Loss: 1.5700
Training Epoch: 9 [17152/50048]	Loss: 1.7812
Training Epoch: 9 [17280/50048]	Loss: 1.4467
Training Epoch: 9 [17408/50048]	Loss: 1.3150
Training Epoch: 9 [17536/50048]	Loss: 1.6975
Training Epoch: 9 [17664/50048]	Loss: 1.7211
Training Epoch: 9 [17792/50048]	Loss: 1.5587
Training Epoch: 9 [17920/50048]	Loss: 1.6941
Training Epoch: 9 [18048/50048]	Loss: 1.7004
Training Epoch: 9 [18176/50048]	Loss: 1.7133
Training Epoch: 9 [18304/50048]	Loss: 1.8766
Training Epoch: 9 [18432/50048]	Loss: 1.5799
Training Epoch: 9 [18560/50048]	Loss: 1.5324
Training Epoch: 9 [18688/50048]	Loss: 1.4999
Training Epoch: 9 [18816/50048]	Loss: 1.7918
Training Epoch: 9 [18944/50048]	Loss: 1.6558
Training Epoch: 9 [19072/50048]	Loss: 1.8156
Training Epoch: 9 [19200/50048]	Loss: 1.6101
Training Epoch: 9 [19328/50048]	Loss: 1.4736
Training Epoch: 9 [19456/50048]	Loss: 1.7735
Training Epoch: 9 [19584/50048]	Loss: 1.4775
Training Epoch: 9 [19712/50048]	Loss: 1.5154
Training Epoch: 9 [19840/50048]	Loss: 1.7660
Training Epoch: 9 [19968/50048]	Loss: 1.9992
Training Epoch: 9 [20096/50048]	Loss: 1.7761
Training Epoch: 9 [20224/50048]	Loss: 1.5953
Training Epoch: 9 [20352/50048]	Loss: 1.5073
Training Epoch: 9 [20480/50048]	Loss: 1.6411
Training Epoch: 9 [20608/50048]	Loss: 1.6340
Training Epoch: 9 [20736/50048]	Loss: 1.5953
Training Epoch: 9 [20864/50048]	Loss: 1.5460
Training Epoch: 9 [20992/50048]	Loss: 1.6286
Training Epoch: 9 [21120/50048]	Loss: 1.6280
Training Epoch: 9 [21248/50048]	Loss: 1.7961
Training Epoch: 9 [21376/50048]	Loss: 1.9700
Training Epoch: 9 [21504/50048]	Loss: 1.4487
Training Epoch: 9 [21632/50048]	Loss: 1.8489
Training Epoch: 9 [21760/50048]	Loss: 1.5553
Training Epoch: 9 [21888/50048]	Loss: 1.7330
Training Epoch: 9 [22016/50048]	Loss: 1.5288
Training Epoch: 9 [22144/50048]	Loss: 1.5914
Training Epoch: 9 [22272/50048]	Loss: 1.6167
Training Epoch: 9 [22400/50048]	Loss: 1.5062
Training Epoch: 9 [22528/50048]	Loss: 1.7554
Training Epoch: 9 [22656/50048]	Loss: 1.6498
Training Epoch: 9 [22784/50048]	Loss: 1.6323
Training Epoch: 9 [22912/50048]	Loss: 2.1774
Training Epoch: 9 [23040/50048]	Loss: 1.7136
Training Epoch: 9 [23168/50048]	Loss: 1.5144
Training Epoch: 9 [23296/50048]	Loss: 1.7363
Training Epoch: 9 [23424/50048]	Loss: 1.4996
Training Epoch: 9 [23552/50048]	Loss: 1.6516
Training Epoch: 9 [23680/50048]	Loss: 1.7321
Training Epoch: 9 [23808/50048]	Loss: 1.3569
Training Epoch: 9 [23936/50048]	Loss: 1.7055
Training Epoch: 9 [24064/50048]	Loss: 1.5740
Training Epoch: 9 [24192/50048]	Loss: 1.5963
Training Epoch: 9 [24320/50048]	Loss: 1.7717
Training Epoch: 9 [24448/50048]	Loss: 1.6481
Training Epoch: 9 [24576/50048]	Loss: 1.6136
Training Epoch: 9 [24704/50048]	Loss: 1.4507
Training Epoch: 9 [24832/50048]	Loss: 1.5775
Training Epoch: 9 [24960/50048]	Loss: 1.6445
Training Epoch: 9 [25088/50048]	Loss: 1.4844
Training Epoch: 9 [25216/50048]	Loss: 1.5071
Training Epoch: 9 [25344/50048]	Loss: 1.5183
Training Epoch: 9 [25472/50048]	Loss: 1.4938
Training Epoch: 9 [25600/50048]	Loss: 1.6839
Training Epoch: 9 [25728/50048]	Loss: 1.5628
Training Epoch: 9 [25856/50048]	Loss: 1.5561
Training Epoch: 9 [25984/50048]	Loss: 1.7629
Training Epoch: 9 [26112/50048]	Loss: 1.5491
Training Epoch: 9 [26240/50048]	Loss: 1.7233
Training Epoch: 9 [26368/50048]	Loss: 1.7633
Training Epoch: 9 [26496/50048]	Loss: 1.6980
Training Epoch: 9 [26624/50048]	Loss: 1.5907
Training Epoch: 9 [26752/50048]	Loss: 1.6039
Training Epoch: 9 [26880/50048]	Loss: 2.0838
Training Epoch: 9 [27008/50048]	Loss: 1.7242
Training Epoch: 9 [27136/50048]	Loss: 1.6600
Training Epoch: 9 [27264/50048]	Loss: 1.8541
Training Epoch: 9 [27392/50048]	Loss: 1.7435
Training Epoch: 9 [27520/50048]	Loss: 1.6985
Training Epoch: 9 [27648/50048]	Loss: 1.8887
Training Epoch: 9 [27776/50048]	Loss: 1.5896
Training Epoch: 9 [27904/50048]	Loss: 1.5811
Training Epoch: 9 [28032/50048]	Loss: 1.5281
Training Epoch: 9 [28160/50048]	Loss: 1.6153
Training Epoch: 9 [28288/50048]	Loss: 1.6974
Training Epoch: 9 [28416/50048]	Loss: 1.7655
Training Epoch: 9 [28544/50048]	Loss: 1.6748
Training Epoch: 9 [28672/50048]	Loss: 1.9062
Training Epoch: 9 [28800/50048]	Loss: 1.5737
Training Epoch: 9 [28928/50048]	Loss: 1.9562
Training Epoch: 9 [29056/50048]	Loss: 1.7349
Training Epoch: 9 [29184/50048]	Loss: 1.6377
Training Epoch: 9 [29312/50048]	Loss: 1.5030
Training Epoch: 9 [29440/50048]	Loss: 1.7059
Training Epoch: 9 [29568/50048]	Loss: 1.6668
Training Epoch: 9 [29696/50048]	Loss: 1.8149
Training Epoch: 9 [29824/50048]	Loss: 1.3420
Training Epoch: 9 [29952/50048]	Loss: 1.6708
Training Epoch: 9 [30080/50048]	Loss: 1.6384
Training Epoch: 9 [30208/50048]	Loss: 1.4363
Training Epoch: 9 [30336/50048]	Loss: 1.8057
Training Epoch: 9 [30464/50048]	Loss: 1.6120
Training Epoch: 9 [30592/50048]	Loss: 1.6406
Training Epoch: 9 [30720/50048]	Loss: 1.5676
Training Epoch: 9 [30848/50048]	Loss: 1.5773
Training Epoch: 9 [30976/50048]	Loss: 1.8765
Training Epoch: 9 [31104/50048]	Loss: 1.4868
Training Epoch: 9 [31232/50048]	Loss: 1.5273
Training Epoch: 9 [31360/50048]	Loss: 1.7423
Training Epoch: 9 [31488/50048]	Loss: 1.7407
Training Epoch: 9 [31616/50048]	Loss: 1.8184
Training Epoch: 9 [31744/50048]	Loss: 1.6138
Training Epoch: 9 [31872/50048]	Loss: 1.9564
Training Epoch: 9 [32000/50048]	Loss: 1.7666
Training Epoch: 9 [32128/50048]	Loss: 1.4097
Training Epoch: 9 [32256/50048]	Loss: 1.6043
Training Epoch: 9 [32384/50048]	Loss: 1.5747
Training Epoch: 9 [32512/50048]	Loss: 1.6340
Training Epoch: 9 [32640/50048]	Loss: 1.5721
Training Epoch: 9 [32768/50048]	Loss: 1.4345
Training Epoch: 9 [32896/50048]	Loss: 1.7868
Training Epoch: 9 [33024/50048]	Loss: 1.7347
Training Epoch: 9 [33152/50048]	Loss: 1.8462
Training Epoch: 9 [33280/50048]	Loss: 1.5224
Training Epoch: 9 [33408/50048]	Loss: 1.9158
Training Epoch: 9 [33536/50048]	Loss: 1.4681
Training Epoch: 9 [33664/50048]	Loss: 1.3729
Training Epoch: 9 [33792/50048]	Loss: 1.4896
Training Epoch: 9 [33920/50048]	Loss: 1.5298
Training Epoch: 9 [34048/50048]	Loss: 2.0550
Training Epoch: 9 [34176/50048]	Loss: 1.3609
Training Epoch: 9 [34304/50048]	Loss: 1.9379
Training Epoch: 9 [34432/50048]	Loss: 1.9019
Training Epoch: 9 [34560/50048]	Loss: 1.7285
Training Epoch: 9 [34688/50048]	Loss: 1.6250
Training Epoch: 9 [34816/50048]	Loss: 1.9279
Training Epoch: 9 [34944/50048]	Loss: 1.7937
Training Epoch: 9 [35072/50048]	Loss: 1.5335
Training Epoch: 9 [35200/50048]	Loss: 1.5230
Training Epoch: 9 [35328/50048]	Loss: 1.4250
Training Epoch: 9 [35456/50048]	Loss: 1.6641
Training Epoch: 9 [35584/50048]	Loss: 1.5121
Training Epoch: 9 [35712/50048]	Loss: 1.8192
Training Epoch: 9 [35840/50048]	Loss: 1.8453
Training Epoch: 9 [35968/50048]	Loss: 1.7038
Training Epoch: 9 [36096/50048]	Loss: 1.5938
Training Epoch: 9 [36224/50048]	Loss: 1.7888
Training Epoch: 9 [36352/50048]	Loss: 1.5247
Training Epoch: 9 [36480/50048]	Loss: 1.8097
Training Epoch: 9 [36608/50048]	Loss: 1.6059
Training Epoch: 9 [36736/50048]	Loss: 1.7946
Training Epoch: 9 [36864/50048]	Loss: 1.5122
Training Epoch: 9 [36992/50048]	Loss: 1.6527
Training Epoch: 9 [37120/50048]	Loss: 1.7780
Training Epoch: 9 [37248/50048]	Loss: 1.7298
Training Epoch: 9 [37376/50048]	Loss: 1.7047
Training Epoch: 9 [37504/50048]	Loss: 1.7608
Training Epoch: 9 [37632/50048]	Loss: 1.8798
Training Epoch: 9 [37760/50048]	Loss: 1.4802
Training Epoch: 9 [37888/50048]	Loss: 1.6764
Training Epoch: 9 [38016/50048]	Loss: 1.9056
Training Epoch: 9 [38144/50048]	Loss: 1.4925
Training Epoch: 9 [38272/50048]	Loss: 1.5820
Training Epoch: 9 [38400/50048]	Loss: 1.6541
Training Epoch: 9 [38528/50048]	Loss: 1.7504
Training Epoch: 9 [38656/50048]	Loss: 1.5256
Training Epoch: 9 [38784/50048]	Loss: 1.6151
Training Epoch: 9 [38912/50048]	Loss: 1.7203
Training Epoch: 9 [39040/50048]	Loss: 1.5209
Training Epoch: 9 [39168/50048]	Loss: 1.6512
Training Epoch: 9 [39296/50048]	Loss: 1.6409
Training Epoch: 9 [39424/50048]	Loss: 1.5551
Training Epoch: 9 [39552/50048]	Loss: 1.8269
Training Epoch: 9 [39680/50048]	Loss: 1.9068
Training Epoch: 9 [39808/50048]	Loss: 1.7393
Training Epoch: 9 [39936/50048]	Loss: 1.6920
Training Epoch: 9 [40064/50048]	Loss: 1.8072
Training Epoch: 9 [40192/50048]	Loss: 1.6022
Training Epoch: 9 [40320/50048]	Loss: 1.6672
Training Epoch: 9 [40448/50048]	Loss: 1.5482
Training Epoch: 9 [40576/50048]	Loss: 1.7522
Training Epoch: 9 [40704/50048]	Loss: 1.8225
Training Epoch: 9 [40832/50048]	Loss: 1.8310
Training Epoch: 9 [40960/50048]	Loss: 1.8485
Training Epoch: 9 [41088/50048]	Loss: 1.7076
Training Epoch: 9 [41216/50048]	Loss: 1.6582
Training Epoch: 9 [41344/50048]	Loss: 1.6098
Training Epoch: 9 [41472/50048]	Loss: 1.7757
Training Epoch: 9 [41600/50048]	Loss: 1.4724
Training Epoch: 9 [41728/50048]	Loss: 1.6075
Training Epoch: 9 [41856/50048]	Loss: 1.5544
Training Epoch: 9 [41984/50048]	Loss: 1.2981
Training Epoch: 9 [42112/50048]	Loss: 1.5411
Training Epoch: 9 [42240/50048]	Loss: 1.5121
Training Epoch: 9 [42368/50048]	Loss: 1.7994
Training Epoch: 9 [42496/50048]	Loss: 1.4606
Training Epoch: 9 [42624/50048]	Loss: 1.7379
Training Epoch: 9 [42752/50048]	Loss: 1.3915
Training Epoch: 9 [42880/50048]	Loss: 1.6700
Training Epoch: 9 [43008/50048]	Loss: 2.1482
Training Epoch: 9 [43136/50048]	Loss: 1.4913
Training Epoch: 9 [43264/50048]	Loss: 1.8259
Training Epoch: 9 [43392/50048]	Loss: 1.7782
Training Epoch: 9 [43520/50048]	Loss: 1.6435
Training Epoch: 9 [43648/50048]	Loss: 1.9443
Training Epoch: 9 [43776/50048]	Loss: 1.7141
Training Epoch: 9 [43904/50048]	Loss: 1.6879
Training Epoch: 9 [44032/50048]	Loss: 1.7445
Training Epoch: 9 [44160/50048]	Loss: 1.6928
Training Epoch: 9 [44288/50048]	Loss: 1.6423
Training Epoch: 9 [44416/50048]	Loss: 1.4712
Training Epoch: 9 [44544/50048]	Loss: 1.5565
Training Epoch: 9 [44672/50048]	Loss: 1.6114
Training Epoch: 9 [44800/50048]	Loss: 1.5979
Training Epoch: 9 [44928/50048]	Loss: 1.6984
Training Epoch: 9 [45056/50048]	Loss: 1.8354
Training Epoch: 9 [45184/50048]	Loss: 1.6922
Training Epoch: 9 [45312/50048]	Loss: 1.7461
Training Epoch: 9 [45440/50048]	Loss: 1.6435
Training Epoch: 9 [45568/50048]	Loss: 1.6509
Training Epoch: 9 [45696/50048]	Loss: 1.5736
Training Epoch: 9 [45824/50048]	Loss: 1.5212
Training Epoch: 9 [45952/50048]	Loss: 1.6785
Training Epoch: 9 [46080/50048]	Loss: 1.6713
Training Epoch: 9 [46208/50048]	Loss: 1.5662
Training Epoch: 9 [46336/50048]	Loss: 1.6761
Training Epoch: 9 [46464/50048]	Loss: 1.6749
Training Epoch: 9 [46592/50048]	Loss: 1.7843
Training Epoch: 9 [46720/50048]	Loss: 1.6937
2022-12-06 06:22:52,272 [ZeusDataLoader(train)] train epoch 10 done: time=86.43 energy=10507.13
2022-12-06 06:22:52,273 [ZeusDataLoader(eval)] Epoch 10 begin.
Training Epoch: 9 [46848/50048]	Loss: 1.6304
Training Epoch: 9 [46976/50048]	Loss: 1.5942
Training Epoch: 9 [47104/50048]	Loss: 1.6099
Training Epoch: 9 [47232/50048]	Loss: 1.7682
Training Epoch: 9 [47360/50048]	Loss: 1.7374
Training Epoch: 9 [47488/50048]	Loss: 1.7940
Training Epoch: 9 [47616/50048]	Loss: 1.4416
Training Epoch: 9 [47744/50048]	Loss: 1.9195
Training Epoch: 9 [47872/50048]	Loss: 1.6299
Training Epoch: 9 [48000/50048]	Loss: 1.3022
Training Epoch: 9 [48128/50048]	Loss: 1.4414
Training Epoch: 9 [48256/50048]	Loss: 1.3686
Training Epoch: 9 [48384/50048]	Loss: 1.7373
Training Epoch: 9 [48512/50048]	Loss: 1.4085
Training Epoch: 9 [48640/50048]	Loss: 1.5743
Training Epoch: 9 [48768/50048]	Loss: 1.5570
Training Epoch: 9 [48896/50048]	Loss: 1.6615
Training Epoch: 9 [49024/50048]	Loss: 1.7976
Training Epoch: 9 [49152/50048]	Loss: 1.4274
Training Epoch: 9 [49280/50048]	Loss: 1.5568
Training Epoch: 9 [49408/50048]	Loss: 1.5352
Training Epoch: 9 [49536/50048]	Loss: 1.8851
Training Epoch: 9 [49664/50048]	Loss: 1.7605
Training Epoch: 9 [49792/50048]	Loss: 1.4049
Training Epoch: 9 [49920/50048]	Loss: 1.4765
Training Epoch: 9 [50048/50048]	Loss: 1.9149
2022-12-06 11:22:56.006 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:22:56,039 [ZeusDataLoader(eval)] eval epoch 10 done: time=3.76 energy=452.13
2022-12-06 06:22:56,039 [ZeusDataLoader(train)] Up to epoch 10: time=902.21, energy=109497.97, cost=133692.67
2022-12-06 06:22:56,040 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:22:56,040 [ZeusDataLoader(train)] Expected next epoch: time=992.01, energy=120295.99, cost=146949.06
2022-12-06 06:22:56,041 [ZeusDataLoader(train)] Epoch 11 begin.
Validation Epoch: 9, Average loss: 0.0147, Accuracy: 0.4964
2022-12-06 06:22:56,224 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:22:56,225 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:22:56.227 [ZeusMonitor] Monitor started.
2022-12-06 11:22:56.247 [ZeusMonitor] Running indefinitely. 2022-12-06 11:22:56.247 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:22:56.247 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e11+gpu0.power.log
Training Epoch: 10 [128/50048]	Loss: 1.5770
Training Epoch: 10 [256/50048]	Loss: 1.5589
Training Epoch: 10 [384/50048]	Loss: 1.6229
Training Epoch: 10 [512/50048]	Loss: 1.5759
Training Epoch: 10 [640/50048]	Loss: 1.6634
Training Epoch: 10 [768/50048]	Loss: 1.6094
Training Epoch: 10 [896/50048]	Loss: 1.5491
Training Epoch: 10 [1024/50048]	Loss: 1.9040
Training Epoch: 10 [1152/50048]	Loss: 1.2624
Training Epoch: 10 [1280/50048]	Loss: 1.4611
Training Epoch: 10 [1408/50048]	Loss: 1.5344
Training Epoch: 10 [1536/50048]	Loss: 1.4728
Training Epoch: 10 [1664/50048]	Loss: 1.7337
Training Epoch: 10 [1792/50048]	Loss: 1.6092
Training Epoch: 10 [1920/50048]	Loss: 1.5066
Training Epoch: 10 [2048/50048]	Loss: 1.3480
Training Epoch: 10 [2176/50048]	Loss: 1.6710
Training Epoch: 10 [2304/50048]	Loss: 1.5258
Training Epoch: 10 [2432/50048]	Loss: 1.2828
Training Epoch: 10 [2560/50048]	Loss: 1.5298
Training Epoch: 10 [2688/50048]	Loss: 1.5692
Training Epoch: 10 [2816/50048]	Loss: 1.4745
Training Epoch: 10 [2944/50048]	Loss: 1.4439
Training Epoch: 10 [3072/50048]	Loss: 1.7680
Training Epoch: 10 [3200/50048]	Loss: 1.7806
Training Epoch: 10 [3328/50048]	Loss: 1.6025
Training Epoch: 10 [3456/50048]	Loss: 1.6091
Training Epoch: 10 [3584/50048]	Loss: 1.6543
Training Epoch: 10 [3712/50048]	Loss: 1.6688
Training Epoch: 10 [3840/50048]	Loss: 1.7175
Training Epoch: 10 [3968/50048]	Loss: 1.4432
Training Epoch: 10 [4096/50048]	Loss: 1.5958
Training Epoch: 10 [4224/50048]	Loss: 1.7150
Training Epoch: 10 [4352/50048]	Loss: 1.5992
Training Epoch: 10 [4480/50048]	Loss: 1.2951
Training Epoch: 10 [4608/50048]	Loss: 1.5071
Training Epoch: 10 [4736/50048]	Loss: 1.5459
Training Epoch: 10 [4864/50048]	Loss: 1.2381
Training Epoch: 10 [4992/50048]	Loss: 1.6797
Training Epoch: 10 [5120/50048]	Loss: 1.5297
Training Epoch: 10 [5248/50048]	Loss: 1.5701
Training Epoch: 10 [5376/50048]	Loss: 1.6435
Training Epoch: 10 [5504/50048]	Loss: 1.6155
Training Epoch: 10 [5632/50048]	Loss: 1.6006
Training Epoch: 10 [5760/50048]	Loss: 1.6333
Training Epoch: 10 [5888/50048]	Loss: 1.5731
Training Epoch: 10 [6016/50048]	Loss: 1.7069
Training Epoch: 10 [6144/50048]	Loss: 1.3529
Training Epoch: 10 [6272/50048]	Loss: 1.4268
Training Epoch: 10 [6400/50048]	Loss: 1.8340
Training Epoch: 10 [6528/50048]	Loss: 1.5726
Training Epoch: 10 [6656/50048]	Loss: 1.6594
Training Epoch: 10 [6784/50048]	Loss: 1.4654
Training Epoch: 10 [6912/50048]	Loss: 1.7048
Training Epoch: 10 [7040/50048]	Loss: 1.6222
Training Epoch: 10 [7168/50048]	Loss: 1.5241
Training Epoch: 10 [7296/50048]	Loss: 1.4510
Training Epoch: 10 [7424/50048]	Loss: 1.5370
Training Epoch: 10 [7552/50048]	Loss: 1.5906
Training Epoch: 10 [7680/50048]	Loss: 1.6799
Training Epoch: 10 [7808/50048]	Loss: 2.0454
Training Epoch: 10 [7936/50048]	Loss: 1.2662
Training Epoch: 10 [8064/50048]	Loss: 1.3968
Training Epoch: 10 [8192/50048]	Loss: 1.6070
Training Epoch: 10 [8320/50048]	Loss: 1.5737
Training Epoch: 10 [8448/50048]	Loss: 1.2640
Training Epoch: 10 [8576/50048]	Loss: 1.4693
Training Epoch: 10 [8704/50048]	Loss: 1.3896
Training Epoch: 10 [8832/50048]	Loss: 1.6321
Training Epoch: 10 [8960/50048]	Loss: 1.6407
Training Epoch: 10 [9088/50048]	Loss: 1.4923
Training Epoch: 10 [9216/50048]	Loss: 1.6382
Training Epoch: 10 [9344/50048]	Loss: 1.3829
Training Epoch: 10 [9472/50048]	Loss: 1.5807
Training Epoch: 10 [9600/50048]	Loss: 1.3833
Training Epoch: 10 [9728/50048]	Loss: 1.4419
Training Epoch: 10 [9856/50048]	Loss: 1.5037
Training Epoch: 10 [9984/50048]	Loss: 1.4393
Training Epoch: 10 [10112/50048]	Loss: 1.4911
Training Epoch: 10 [10240/50048]	Loss: 1.5884
Training Epoch: 10 [10368/50048]	Loss: 1.7062
Training Epoch: 10 [10496/50048]	Loss: 1.7389
Training Epoch: 10 [10624/50048]	Loss: 1.6161
Training Epoch: 10 [10752/50048]	Loss: 1.8015
Training Epoch: 10 [10880/50048]	Loss: 1.5700
Training Epoch: 10 [11008/50048]	Loss: 1.7502
Training Epoch: 10 [11136/50048]	Loss: 1.6709
Training Epoch: 10 [11264/50048]	Loss: 1.6456
Training Epoch: 10 [11392/50048]	Loss: 1.5642
Training Epoch: 10 [11520/50048]	Loss: 1.4714
Training Epoch: 10 [11648/50048]	Loss: 1.5540
Training Epoch: 10 [11776/50048]	Loss: 1.4972
Training Epoch: 10 [11904/50048]	Loss: 1.4406
Training Epoch: 10 [12032/50048]	Loss: 1.5966
Training Epoch: 10 [12160/50048]	Loss: 1.5605
Training Epoch: 10 [12288/50048]	Loss: 1.5016
Training Epoch: 10 [12416/50048]	Loss: 1.6298
Training Epoch: 10 [12544/50048]	Loss: 1.5868
Training Epoch: 10 [12672/50048]	Loss: 1.5951
Training Epoch: 10 [12800/50048]	Loss: 1.6290
Training Epoch: 10 [12928/50048]	Loss: 1.7554
Training Epoch: 10 [13056/50048]	Loss: 1.8146
Training Epoch: 10 [13184/50048]	Loss: 1.6461
Training Epoch: 10 [13312/50048]	Loss: 1.6982
Training Epoch: 10 [13440/50048]	Loss: 1.5277
Training Epoch: 10 [13568/50048]	Loss: 1.6256
Training Epoch: 10 [13696/50048]	Loss: 1.7580
Training Epoch: 10 [13824/50048]	Loss: 1.8532
Training Epoch: 10 [13952/50048]	Loss: 1.6279
Training Epoch: 10 [14080/50048]	Loss: 1.5163
Training Epoch: 10 [14208/50048]	Loss: 1.5495
Training Epoch: 10 [14336/50048]	Loss: 1.3893
Training Epoch: 10 [14464/50048]	Loss: 1.5979
Training Epoch: 10 [14592/50048]	Loss: 1.5645
Training Epoch: 10 [14720/50048]	Loss: 1.3406
Training Epoch: 10 [14848/50048]	Loss: 1.6540
Training Epoch: 10 [14976/50048]	Loss: 1.4775
Training Epoch: 10 [15104/50048]	Loss: 1.5044
Training Epoch: 10 [15232/50048]	Loss: 1.7656
Training Epoch: 10 [15360/50048]	Loss: 1.7433
Training Epoch: 10 [15488/50048]	Loss: 1.6514
Training Epoch: 10 [15616/50048]	Loss: 1.7641
Training Epoch: 10 [15744/50048]	Loss: 1.4011
Training Epoch: 10 [15872/50048]	Loss: 1.7329
Training Epoch: 10 [16000/50048]	Loss: 1.6821
Training Epoch: 10 [16128/50048]	Loss: 1.5794
Training Epoch: 10 [16256/50048]	Loss: 1.4361
Training Epoch: 10 [16384/50048]	Loss: 1.6778
Training Epoch: 10 [16512/50048]	Loss: 1.5407
Training Epoch: 10 [16640/50048]	Loss: 1.9322
Training Epoch: 10 [16768/50048]	Loss: 1.5801
Training Epoch: 10 [16896/50048]	Loss: 1.9527
Training Epoch: 10 [17024/50048]	Loss: 1.5825
Training Epoch: 10 [17152/50048]	Loss: 1.7260
Training Epoch: 10 [17280/50048]	Loss: 1.7092
Training Epoch: 10 [17408/50048]	Loss: 1.6036
Training Epoch: 10 [17536/50048]	Loss: 1.5306
Training Epoch: 10 [17664/50048]	Loss: 1.6027
Training Epoch: 10 [17792/50048]	Loss: 1.5665
Training Epoch: 10 [17920/50048]	Loss: 1.3971
Training Epoch: 10 [18048/50048]	Loss: 1.6261
Training Epoch: 10 [18176/50048]	Loss: 1.4921
Training Epoch: 10 [18304/50048]	Loss: 1.5654
Training Epoch: 10 [18432/50048]	Loss: 1.4844
Training Epoch: 10 [18560/50048]	Loss: 1.5033
Training Epoch: 10 [18688/50048]	Loss: 1.6725
Training Epoch: 10 [18816/50048]	Loss: 1.6169
Training Epoch: 10 [18944/50048]	Loss: 1.5767
Training Epoch: 10 [19072/50048]	Loss: 1.8675
Training Epoch: 10 [19200/50048]	Loss: 1.6110
Training Epoch: 10 [19328/50048]	Loss: 1.4471
Training Epoch: 10 [19456/50048]	Loss: 1.7081
Training Epoch: 10 [19584/50048]	Loss: 1.4638
Training Epoch: 10 [19712/50048]	Loss: 1.7708
Training Epoch: 10 [19840/50048]	Loss: 1.5225
Training Epoch: 10 [19968/50048]	Loss: 1.5916
Training Epoch: 10 [20096/50048]	Loss: 1.5627
Training Epoch: 10 [20224/50048]	Loss: 1.6031
Training Epoch: 10 [20352/50048]	Loss: 1.4557
Training Epoch: 10 [20480/50048]	Loss: 1.5725
Training Epoch: 10 [20608/50048]	Loss: 1.4947
Training Epoch: 10 [20736/50048]	Loss: 1.5685
Training Epoch: 10 [20864/50048]	Loss: 1.6251
Training Epoch: 10 [20992/50048]	Loss: 1.5971
Training Epoch: 10 [21120/50048]	Loss: 1.5035
Training Epoch: 10 [21248/50048]	Loss: 1.8544
Training Epoch: 10 [21376/50048]	Loss: 1.5154
Training Epoch: 10 [21504/50048]	Loss: 1.6609
Training Epoch: 10 [21632/50048]	Loss: 1.6264
Training Epoch: 10 [21760/50048]	Loss: 1.5146
Training Epoch: 10 [21888/50048]	Loss: 1.5034
Training Epoch: 10 [22016/50048]	Loss: 1.5926
Training Epoch: 10 [22144/50048]	Loss: 1.4876
Training Epoch: 10 [22272/50048]	Loss: 1.6769
Training Epoch: 10 [22400/50048]	Loss: 1.5769
Training Epoch: 10 [22528/50048]	Loss: 1.8466
Training Epoch: 10 [22656/50048]	Loss: 1.7974
Training Epoch: 10 [22784/50048]	Loss: 1.7142
Training Epoch: 10 [22912/50048]	Loss: 1.3666
Training Epoch: 10 [23040/50048]	Loss: 1.3514
Training Epoch: 10 [23168/50048]	Loss: 1.6046
Training Epoch: 10 [23296/50048]	Loss: 1.4479
Training Epoch: 10 [23424/50048]	Loss: 1.3993
Training Epoch: 10 [23552/50048]	Loss: 1.5434
Training Epoch: 10 [23680/50048]	Loss: 1.3514
Training Epoch: 10 [23808/50048]	Loss: 1.8240
Training Epoch: 10 [23936/50048]	Loss: 1.6162
Training Epoch: 10 [24064/50048]	Loss: 1.5368
Training Epoch: 10 [24192/50048]	Loss: 1.5796
Training Epoch: 10 [24320/50048]	Loss: 1.7409
Training Epoch: 10 [24448/50048]	Loss: 1.5036
Training Epoch: 10 [24576/50048]	Loss: 1.6601
Training Epoch: 10 [24704/50048]	Loss: 1.4266
Training Epoch: 10 [24832/50048]	Loss: 1.5551
Training Epoch: 10 [24960/50048]	Loss: 1.6110
Training Epoch: 10 [25088/50048]	Loss: 1.6036
Training Epoch: 10 [25216/50048]	Loss: 1.6814
Training Epoch: 10 [25344/50048]	Loss: 1.4505
Training Epoch: 10 [25472/50048]	Loss: 1.8532
Training Epoch: 10 [25600/50048]	Loss: 1.4889
Training Epoch: 10 [25728/50048]	Loss: 1.5713
Training Epoch: 10 [25856/50048]	Loss: 1.4683
Training Epoch: 10 [25984/50048]	Loss: 1.5522
Training Epoch: 10 [26112/50048]	Loss: 1.6870
Training Epoch: 10 [26240/50048]	Loss: 1.3021
Training Epoch: 10 [26368/50048]	Loss: 1.5046
Training Epoch: 10 [26496/50048]	Loss: 1.7036
Training Epoch: 10 [26624/50048]	Loss: 1.6124
Training Epoch: 10 [26752/50048]	Loss: 1.7010
Training Epoch: 10 [26880/50048]	Loss: 1.5886
Training Epoch: 10 [27008/50048]	Loss: 1.4925
Training Epoch: 10 [27136/50048]	Loss: 1.6085
Training Epoch: 10 [27264/50048]	Loss: 1.4466
Training Epoch: 10 [27392/50048]	Loss: 1.7301
Training Epoch: 10 [27520/50048]	Loss: 1.3061
Training Epoch: 10 [27648/50048]	Loss: 1.3710
Training Epoch: 10 [27776/50048]	Loss: 1.5933
Training Epoch: 10 [27904/50048]	Loss: 1.5443
Training Epoch: 10 [28032/50048]	Loss: 1.5527
Training Epoch: 10 [28160/50048]	Loss: 1.4175
Training Epoch: 10 [28288/50048]	Loss: 1.4877
Training Epoch: 10 [28416/50048]	Loss: 1.6555
Training Epoch: 10 [28544/50048]	Loss: 1.7088
Training Epoch: 10 [28672/50048]	Loss: 1.6260
Training Epoch: 10 [28800/50048]	Loss: 1.5629
Training Epoch: 10 [28928/50048]	Loss: 1.7719
Training Epoch: 10 [29056/50048]	Loss: 1.5180
Training Epoch: 10 [29184/50048]	Loss: 1.7201
Training Epoch: 10 [29312/50048]	Loss: 1.6989
Training Epoch: 10 [29440/50048]	Loss: 1.5618
Training Epoch: 10 [29568/50048]	Loss: 1.4015
Training Epoch: 10 [29696/50048]	Loss: 1.7355
Training Epoch: 10 [29824/50048]	Loss: 1.7033
Training Epoch: 10 [29952/50048]	Loss: 1.6457
Training Epoch: 10 [30080/50048]	Loss: 1.3332
Training Epoch: 10 [30208/50048]	Loss: 1.5150
Training Epoch: 10 [30336/50048]	Loss: 1.4617
Training Epoch: 10 [30464/50048]	Loss: 1.5802
Training Epoch: 10 [30592/50048]	Loss: 1.6918
Training Epoch: 10 [30720/50048]	Loss: 1.5616
Training Epoch: 10 [30848/50048]	Loss: 1.4542
Training Epoch: 10 [30976/50048]	Loss: 1.5985
Training Epoch: 10 [31104/50048]	Loss: 1.4677
Training Epoch: 10 [31232/50048]	Loss: 1.7412
Training Epoch: 10 [31360/50048]	Loss: 1.5614
Training Epoch: 10 [31488/50048]	Loss: 1.8452
Training Epoch: 10 [31616/50048]	Loss: 1.8548
Training Epoch: 10 [31744/50048]	Loss: 1.5218
Training Epoch: 10 [31872/50048]	Loss: 1.6051
Training Epoch: 10 [32000/50048]	Loss: 1.5078
Training Epoch: 10 [32128/50048]	Loss: 1.6681
Training Epoch: 10 [32256/50048]	Loss: 1.5119
Training Epoch: 10 [32384/50048]	Loss: 1.6112
Training Epoch: 10 [32512/50048]	Loss: 1.4996
Training Epoch: 10 [32640/50048]	Loss: 1.6629
Training Epoch: 10 [32768/50048]	Loss: 1.4841
Training Epoch: 10 [32896/50048]	Loss: 1.5232
Training Epoch: 10 [33024/50048]	Loss: 1.5527
Training Epoch: 10 [33152/50048]	Loss: 1.2034
Training Epoch: 10 [33280/50048]	Loss: 1.6630
Training Epoch: 10 [33408/50048]	Loss: 1.7611
Training Epoch: 10 [33536/50048]	Loss: 1.5940
Training Epoch: 10 [33664/50048]	Loss: 1.4089
Training Epoch: 10 [33792/50048]	Loss: 1.7142
Training Epoch: 10 [33920/50048]	Loss: 1.5212
Training Epoch: 10 [34048/50048]	Loss: 1.6703
Training Epoch: 10 [34176/50048]	Loss: 1.5540
Training Epoch: 10 [34304/50048]	Loss: 1.4827
Training Epoch: 10 [34432/50048]	Loss: 1.6758
Training Epoch: 10 [34560/50048]	Loss: 1.6381
Training Epoch: 10 [34688/50048]	Loss: 1.4973
Training Epoch: 10 [34816/50048]	Loss: 1.6171
Training Epoch: 10 [34944/50048]	Loss: 1.6577
Training Epoch: 10 [35072/50048]	Loss: 1.5680
Training Epoch: 10 [35200/50048]	Loss: 1.7256
Training Epoch: 10 [35328/50048]	Loss: 1.6225
Training Epoch: 10 [35456/50048]	Loss: 1.7431
Training Epoch: 10 [35584/50048]	Loss: 1.5619
Training Epoch: 10 [35712/50048]	Loss: 1.8333
Training Epoch: 10 [35840/50048]	Loss: 1.6846
Training Epoch: 10 [35968/50048]	Loss: 1.7490
Training Epoch: 10 [36096/50048]	Loss: 1.8310
Training Epoch: 10 [36224/50048]	Loss: 1.5932
Training Epoch: 10 [36352/50048]	Loss: 1.5886
Training Epoch: 10 [36480/50048]	Loss: 1.5069
Training Epoch: 10 [36608/50048]	Loss: 1.3210
Training Epoch: 10 [36736/50048]	Loss: 1.7002
Training Epoch: 10 [36864/50048]	Loss: 1.4982
Training Epoch: 10 [36992/50048]	Loss: 1.4717
Training Epoch: 10 [37120/50048]	Loss: 1.3503
Training Epoch: 10 [37248/50048]	Loss: 1.4686
Training Epoch: 10 [37376/50048]	Loss: 1.6232
Training Epoch: 10 [37504/50048]	Loss: 1.6385
Training Epoch: 10 [37632/50048]	Loss: 1.4253
Training Epoch: 10 [37760/50048]	Loss: 1.6215
Training Epoch: 10 [37888/50048]	Loss: 1.8408
Training Epoch: 10 [38016/50048]	Loss: 1.6039
Training Epoch: 10 [38144/50048]	Loss: 1.6055
Training Epoch: 10 [38272/50048]	Loss: 1.7265
Training Epoch: 10 [38400/50048]	Loss: 1.5739
Training Epoch: 10 [38528/50048]	Loss: 1.5121
Training Epoch: 10 [38656/50048]	Loss: 1.5020
Training Epoch: 10 [38784/50048]	Loss: 1.6383
Training Epoch: 10 [38912/50048]	Loss: 1.3529
Training Epoch: 10 [39040/50048]	Loss: 1.7060
Training Epoch: 10 [39168/50048]	Loss: 1.8954
Training Epoch: 10 [39296/50048]	Loss: 1.5478
Training Epoch: 10 [39424/50048]	Loss: 1.6522
Training Epoch: 10 [39552/50048]	Loss: 1.4674
Training Epoch: 10 [39680/50048]	Loss: 1.6216
Training Epoch: 10 [39808/50048]	Loss: 1.7681
Training Epoch: 10 [39936/50048]	Loss: 1.6358
Training Epoch: 10 [40064/50048]	Loss: 1.6890
Training Epoch: 10 [40192/50048]	Loss: 1.2690
Training Epoch: 10 [40320/50048]	Loss: 1.3834
Training Epoch: 10 [40448/50048]	Loss: 1.3399
Training Epoch: 10 [40576/50048]	Loss: 1.5626
Training Epoch: 10 [40704/50048]	Loss: 1.4579
Training Epoch: 10 [40832/50048]	Loss: 1.4823
Training Epoch: 10 [40960/50048]	Loss: 1.8014
Training Epoch: 10 [41088/50048]	Loss: 1.2827
Training Epoch: 10 [41216/50048]	Loss: 1.5381
Training Epoch: 10 [41344/50048]	Loss: 1.5093
Training Epoch: 10 [41472/50048]	Loss: 1.4801
Training Epoch: 10 [41600/50048]	Loss: 1.4772
Training Epoch: 10 [41728/50048]	Loss: 1.5435
Training Epoch: 10 [41856/50048]	Loss: 1.5942
Training Epoch: 10 [41984/50048]	Loss: 1.4508
Training Epoch: 10 [42112/50048]	Loss: 1.6703
Training Epoch: 10 [42240/50048]	Loss: 1.4562
Training Epoch: 10 [42368/50048]	Loss: 1.6773
Training Epoch: 10 [42496/50048]	Loss: 1.4128
Training Epoch: 10 [42624/50048]	Loss: 1.7506
Training Epoch: 10 [42752/50048]	Loss: 1.7142
Training Epoch: 10 [42880/50048]	Loss: 1.6402
Training Epoch: 10 [43008/50048]	Loss: 1.7007
Training Epoch: 10 [43136/50048]	Loss: 1.4170
Training Epoch: 10 [43264/50048]	Loss: 1.6845
Training Epoch: 10 [43392/50048]	Loss: 1.6187
Training Epoch: 10 [43520/50048]	Loss: 1.6145
Training Epoch: 10 [43648/50048]	Loss: 1.5915
Training Epoch: 10 [43776/50048]	Loss: 1.5717
Training Epoch: 10 [43904/50048]	Loss: 1.5181
Training Epoch: 10 [44032/50048]	Loss: 1.7358
Training Epoch: 10 [44160/50048]	Loss: 1.4437
Training Epoch: 10 [44288/50048]	Loss: 1.7267
Training Epoch: 10 [44416/50048]	Loss: 1.6635
Training Epoch: 10 [44544/50048]	Loss: 1.5949
Training Epoch: 10 [44672/50048]	Loss: 1.3984
Training Epoch: 10 [44800/50048]	Loss: 1.4517
Training Epoch: 10 [44928/50048]	Loss: 1.4706
Training Epoch: 10 [45056/50048]	Loss: 1.6796
Training Epoch: 10 [45184/50048]	Loss: 1.2558
Training Epoch: 10 [45312/50048]	Loss: 1.6246
Training Epoch: 10 [45440/50048]	Loss: 1.6861
Training Epoch: 10 [45568/50048]	Loss: 1.4944
Training Epoch: 10 [45696/50048]	Loss: 1.8158
2022-12-06 06:24:22,507 [ZeusDataLoader(train)] train epoch 11 done: time=86.46 energy=10496.87
2022-12-06 06:24:22,508 [ZeusDataLoader(eval)] Epoch 11 begin.
Training Epoch: 10 [45824/50048]	Loss: 1.5510
Training Epoch: 10 [45952/50048]	Loss: 1.4609
Training Epoch: 10 [46080/50048]	Loss: 1.3910
Training Epoch: 10 [46208/50048]	Loss: 1.2467
Training Epoch: 10 [46336/50048]	Loss: 1.7860
Training Epoch: 10 [46464/50048]	Loss: 1.6480
Training Epoch: 10 [46592/50048]	Loss: 1.5482
Training Epoch: 10 [46720/50048]	Loss: 1.6930
Training Epoch: 10 [46848/50048]	Loss: 1.8481
Training Epoch: 10 [46976/50048]	Loss: 1.6173
Training Epoch: 10 [47104/50048]	Loss: 1.7151
Training Epoch: 10 [47232/50048]	Loss: 1.4642
Training Epoch: 10 [47360/50048]	Loss: 1.7948
Training Epoch: 10 [47488/50048]	Loss: 1.6453
Training Epoch: 10 [47616/50048]	Loss: 1.4608
Training Epoch: 10 [47744/50048]	Loss: 1.7318
Training Epoch: 10 [47872/50048]	Loss: 1.4510
Training Epoch: 10 [48000/50048]	Loss: 1.5590
Training Epoch: 10 [48128/50048]	Loss: 1.5554
Training Epoch: 10 [48256/50048]	Loss: 1.6895
Training Epoch: 10 [48384/50048]	Loss: 1.6150
Training Epoch: 10 [48512/50048]	Loss: 1.6550
Training Epoch: 10 [48640/50048]	Loss: 1.6356
Training Epoch: 10 [48768/50048]	Loss: 1.5790
Training Epoch: 10 [48896/50048]	Loss: 1.5490
Training Epoch: 10 [49024/50048]	Loss: 1.4835
Training Epoch: 10 [49152/50048]	Loss: 1.8062
Training Epoch: 10 [49280/50048]	Loss: 1.6774
Training Epoch: 10 [49408/50048]	Loss: 1.5967
Training Epoch: 10 [49536/50048]	Loss: 1.6800
Training Epoch: 10 [49664/50048]	Loss: 1.1401
Training Epoch: 10 [49792/50048]	Loss: 1.4435
Training Epoch: 10 [49920/50048]	Loss: 1.6235
Training Epoch: 10 [50048/50048]	Loss: 1.9258
2022-12-06 11:24:26.230 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:24:26,271 [ZeusDataLoader(eval)] eval epoch 11 done: time=3.75 energy=452.65
2022-12-06 06:24:26,271 [ZeusDataLoader(train)] Up to epoch 11: time=992.42, energy=120447.50, cost=147060.77
2022-12-06 06:24:26,271 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:24:26,271 [ZeusDataLoader(train)] Expected next epoch: time=1082.22, energy=131245.51, cost=160317.15
2022-12-06 06:24:26,272 [ZeusDataLoader(train)] Epoch 12 begin.
Validation Epoch: 10, Average loss: 0.0135, Accuracy: 0.5333
2022-12-06 06:24:26,458 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:24:26,459 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:24:26.462 [ZeusMonitor] Monitor started.
2022-12-06 11:24:26.462 [ZeusMonitor] Running indefinitely. 2022-12-06 11:24:26.462 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:24:26.462 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e12+gpu0.power.log
Training Epoch: 11 [128/50048]	Loss: 1.3873
Training Epoch: 11 [256/50048]	Loss: 1.3714
Training Epoch: 11 [384/50048]	Loss: 1.4546
Training Epoch: 11 [512/50048]	Loss: 1.5881
Training Epoch: 11 [640/50048]	Loss: 1.1837
Training Epoch: 11 [768/50048]	Loss: 1.0952
Training Epoch: 11 [896/50048]	Loss: 1.4384
Training Epoch: 11 [1024/50048]	Loss: 1.4055
Training Epoch: 11 [1152/50048]	Loss: 1.5588
Training Epoch: 11 [1280/50048]	Loss: 1.3761
Training Epoch: 11 [1408/50048]	Loss: 1.4439
Training Epoch: 11 [1536/50048]	Loss: 1.3111
Training Epoch: 11 [1664/50048]	Loss: 1.3973
Training Epoch: 11 [1792/50048]	Loss: 1.3323
Training Epoch: 11 [1920/50048]	Loss: 1.6755
Training Epoch: 11 [2048/50048]	Loss: 1.4978
Training Epoch: 11 [2176/50048]	Loss: 1.3995
Training Epoch: 11 [2304/50048]	Loss: 1.4424
Training Epoch: 11 [2432/50048]	Loss: 1.3446
Training Epoch: 11 [2560/50048]	Loss: 1.4237
Training Epoch: 11 [2688/50048]	Loss: 1.6417
Training Epoch: 11 [2816/50048]	Loss: 1.3265
Training Epoch: 11 [2944/50048]	Loss: 1.5654
Training Epoch: 11 [3072/50048]	Loss: 1.4629
Training Epoch: 11 [3200/50048]	Loss: 1.5713
Training Epoch: 11 [3328/50048]	Loss: 1.2827
Training Epoch: 11 [3456/50048]	Loss: 1.6824
Training Epoch: 11 [3584/50048]	Loss: 1.4518
Training Epoch: 11 [3712/50048]	Loss: 1.2755
Training Epoch: 11 [3840/50048]	Loss: 1.6433
Training Epoch: 11 [3968/50048]	Loss: 1.7613
Training Epoch: 11 [4096/50048]	Loss: 1.6019
Training Epoch: 11 [4224/50048]	Loss: 1.5170
Training Epoch: 11 [4352/50048]	Loss: 1.4655
Training Epoch: 11 [4480/50048]	Loss: 1.5017
Training Epoch: 11 [4608/50048]	Loss: 1.5066
Training Epoch: 11 [4736/50048]	Loss: 1.7115
Training Epoch: 11 [4864/50048]	Loss: 1.4522
Training Epoch: 11 [4992/50048]	Loss: 1.6263
Training Epoch: 11 [5120/50048]	Loss: 1.7342
Training Epoch: 11 [5248/50048]	Loss: 1.3698
Training Epoch: 11 [5376/50048]	Loss: 1.2147
Training Epoch: 11 [5504/50048]	Loss: 1.8248
Training Epoch: 11 [5632/50048]	Loss: 1.4038
Training Epoch: 11 [5760/50048]	Loss: 1.3462
Training Epoch: 11 [5888/50048]	Loss: 1.4114
Training Epoch: 11 [6016/50048]	Loss: 1.2546
Training Epoch: 11 [6144/50048]	Loss: 1.3769
Training Epoch: 11 [6272/50048]	Loss: 1.4005
Training Epoch: 11 [6400/50048]	Loss: 1.2263
Training Epoch: 11 [6528/50048]	Loss: 1.3889
Training Epoch: 11 [6656/50048]	Loss: 1.6104
Training Epoch: 11 [6784/50048]	Loss: 1.1822
Training Epoch: 11 [6912/50048]	Loss: 1.6632
Training Epoch: 11 [7040/50048]	Loss: 1.5232
Training Epoch: 11 [7168/50048]	Loss: 1.3278
Training Epoch: 11 [7296/50048]	Loss: 1.4753
Training Epoch: 11 [7424/50048]	Loss: 1.4350
Training Epoch: 11 [7552/50048]	Loss: 1.3845
Training Epoch: 11 [7680/50048]	Loss: 1.6712
Training Epoch: 11 [7808/50048]	Loss: 1.4514
Training Epoch: 11 [7936/50048]	Loss: 1.6090
Training Epoch: 11 [8064/50048]	Loss: 1.5417
Training Epoch: 11 [8192/50048]	Loss: 1.5281
Training Epoch: 11 [8320/50048]	Loss: 1.3734
Training Epoch: 11 [8448/50048]	Loss: 1.4630
Training Epoch: 11 [8576/50048]	Loss: 1.3211
Training Epoch: 11 [8704/50048]	Loss: 1.3750
Training Epoch: 11 [8832/50048]	Loss: 1.6792
Training Epoch: 11 [8960/50048]	Loss: 1.3580
Training Epoch: 11 [9088/50048]	Loss: 1.4073
Training Epoch: 11 [9216/50048]	Loss: 1.6358
Training Epoch: 11 [9344/50048]	Loss: 1.5841
Training Epoch: 11 [9472/50048]	Loss: 1.3599
Training Epoch: 11 [9600/50048]	Loss: 1.2966
Training Epoch: 11 [9728/50048]	Loss: 1.5982
Training Epoch: 11 [9856/50048]	Loss: 1.6985
Training Epoch: 11 [9984/50048]	Loss: 1.6403
Training Epoch: 11 [10112/50048]	Loss: 1.6273
Training Epoch: 11 [10240/50048]	Loss: 1.4678
Training Epoch: 11 [10368/50048]	Loss: 1.3510
Training Epoch: 11 [10496/50048]	Loss: 1.4164
Training Epoch: 11 [10624/50048]	Loss: 1.4571
Training Epoch: 11 [10752/50048]	Loss: 1.5964
Training Epoch: 11 [10880/50048]	Loss: 1.3600
Training Epoch: 11 [11008/50048]	Loss: 1.2343
Training Epoch: 11 [11136/50048]	Loss: 1.4758
Training Epoch: 11 [11264/50048]	Loss: 1.4896
Training Epoch: 11 [11392/50048]	Loss: 1.5741
Training Epoch: 11 [11520/50048]	Loss: 1.4445
Training Epoch: 11 [11648/50048]	Loss: 1.6684
Training Epoch: 11 [11776/50048]	Loss: 1.4665
Training Epoch: 11 [11904/50048]	Loss: 1.6289
Training Epoch: 11 [12032/50048]	Loss: 1.3502
Training Epoch: 11 [12160/50048]	Loss: 1.4433
Training Epoch: 11 [12288/50048]	Loss: 1.4722
Training Epoch: 11 [12416/50048]	Loss: 1.5657
Training Epoch: 11 [12544/50048]	Loss: 1.7595
Training Epoch: 11 [12672/50048]	Loss: 1.3928
Training Epoch: 11 [12800/50048]	Loss: 1.4744
Training Epoch: 11 [12928/50048]	Loss: 1.6459
Training Epoch: 11 [13056/50048]	Loss: 1.6795
Training Epoch: 11 [13184/50048]	Loss: 1.6194
Training Epoch: 11 [13312/50048]	Loss: 1.5651
Training Epoch: 11 [13440/50048]	Loss: 1.4227
Training Epoch: 11 [13568/50048]	Loss: 1.4646
Training Epoch: 11 [13696/50048]	Loss: 1.5051
Training Epoch: 11 [13824/50048]	Loss: 1.3166
Training Epoch: 11 [13952/50048]	Loss: 1.6363
Training Epoch: 11 [14080/50048]	Loss: 1.5670
Training Epoch: 11 [14208/50048]	Loss: 1.3873
Training Epoch: 11 [14336/50048]	Loss: 1.5393
Training Epoch: 11 [14464/50048]	Loss: 1.4824
Training Epoch: 11 [14592/50048]	Loss: 1.6142
Training Epoch: 11 [14720/50048]	Loss: 1.6238
Training Epoch: 11 [14848/50048]	Loss: 1.2596
Training Epoch: 11 [14976/50048]	Loss: 1.7431
Training Epoch: 11 [15104/50048]	Loss: 1.5356
Training Epoch: 11 [15232/50048]	Loss: 1.2461
Training Epoch: 11 [15360/50048]	Loss: 1.6345
Training Epoch: 11 [15488/50048]	Loss: 1.3442
Training Epoch: 11 [15616/50048]	Loss: 1.3456
Training Epoch: 11 [15744/50048]	Loss: 1.7494
Training Epoch: 11 [15872/50048]	Loss: 1.4952
Training Epoch: 11 [16000/50048]	Loss: 1.6924
Training Epoch: 11 [16128/50048]	Loss: 1.4021
Training Epoch: 11 [16256/50048]	Loss: 1.3246
Training Epoch: 11 [16384/50048]	Loss: 1.4113
Training Epoch: 11 [16512/50048]	Loss: 1.3931
Training Epoch: 11 [16640/50048]	Loss: 1.4472
Training Epoch: 11 [16768/50048]	Loss: 1.4668
Training Epoch: 11 [16896/50048]	Loss: 1.5350
Training Epoch: 11 [17024/50048]	Loss: 1.4651
Training Epoch: 11 [17152/50048]	Loss: 1.5905
Training Epoch: 11 [17280/50048]	Loss: 1.4323
Training Epoch: 11 [17408/50048]	Loss: 1.3766
Training Epoch: 11 [17536/50048]	Loss: 1.3821
Training Epoch: 11 [17664/50048]	Loss: 1.4974
Training Epoch: 11 [17792/50048]	Loss: 1.7975
Training Epoch: 11 [17920/50048]	Loss: 1.4135
Training Epoch: 11 [18048/50048]	Loss: 1.4902
Training Epoch: 11 [18176/50048]	Loss: 1.2401
Training Epoch: 11 [18304/50048]	Loss: 1.5206
Training Epoch: 11 [18432/50048]	Loss: 1.6983
Training Epoch: 11 [18560/50048]	Loss: 1.5481
Training Epoch: 11 [18688/50048]	Loss: 1.7444
Training Epoch: 11 [18816/50048]	Loss: 1.5588
Training Epoch: 11 [18944/50048]	Loss: 1.3541
Training Epoch: 11 [19072/50048]	Loss: 1.3195
Training Epoch: 11 [19200/50048]	Loss: 1.4002
Training Epoch: 11 [19328/50048]	Loss: 1.4256
Training Epoch: 11 [19456/50048]	Loss: 1.5321
Training Epoch: 11 [19584/50048]	Loss: 1.7606
Training Epoch: 11 [19712/50048]	Loss: 1.6562
Training Epoch: 11 [19840/50048]	Loss: 1.6625
Training Epoch: 11 [19968/50048]	Loss: 1.6846
Training Epoch: 11 [20096/50048]	Loss: 1.3482
Training Epoch: 11 [20224/50048]	Loss: 1.5916
Training Epoch: 11 [20352/50048]	Loss: 1.5860
Training Epoch: 11 [20480/50048]	Loss: 1.4868
Training Epoch: 11 [20608/50048]	Loss: 1.5296
Training Epoch: 11 [20736/50048]	Loss: 1.8221
Training Epoch: 11 [20864/50048]	Loss: 1.5352
Training Epoch: 11 [20992/50048]	Loss: 1.6222
Training Epoch: 11 [21120/50048]	Loss: 1.2244
Training Epoch: 11 [21248/50048]	Loss: 1.5149
Training Epoch: 11 [21376/50048]	Loss: 1.6190
Training Epoch: 11 [21504/50048]	Loss: 1.4303
Training Epoch: 11 [21632/50048]	Loss: 1.6135
Training Epoch: 11 [21760/50048]	Loss: 1.6481
Training Epoch: 11 [21888/50048]	Loss: 1.5440
Training Epoch: 11 [22016/50048]	Loss: 1.4970
Training Epoch: 11 [22144/50048]	Loss: 1.4124
Training Epoch: 11 [22272/50048]	Loss: 1.4369
Training Epoch: 11 [22400/50048]	Loss: 1.6875
Training Epoch: 11 [22528/50048]	Loss: 1.5901
Training Epoch: 11 [22656/50048]	Loss: 1.5669
Training Epoch: 11 [22784/50048]	Loss: 1.4801
Training Epoch: 11 [22912/50048]	Loss: 1.4231
Training Epoch: 11 [23040/50048]	Loss: 1.3124
Training Epoch: 11 [23168/50048]	Loss: 1.5461
Training Epoch: 11 [23296/50048]	Loss: 1.7010
Training Epoch: 11 [23424/50048]	Loss: 1.4702
Training Epoch: 11 [23552/50048]	Loss: 1.7216
Training Epoch: 11 [23680/50048]	Loss: 1.3238
Training Epoch: 11 [23808/50048]	Loss: 1.4881
Training Epoch: 11 [23936/50048]	Loss: 1.4629
Training Epoch: 11 [24064/50048]	Loss: 1.6068
Training Epoch: 11 [24192/50048]	Loss: 1.4067
Training Epoch: 11 [24320/50048]	Loss: 1.4540
Training Epoch: 11 [24448/50048]	Loss: 1.5229
Training Epoch: 11 [24576/50048]	Loss: 1.7186
Training Epoch: 11 [24704/50048]	Loss: 1.5526
Training Epoch: 11 [24832/50048]	Loss: 1.6902
Training Epoch: 11 [24960/50048]	Loss: 1.4257
Training Epoch: 11 [25088/50048]	Loss: 1.5481
Training Epoch: 11 [25216/50048]	Loss: 1.6613
Training Epoch: 11 [25344/50048]	Loss: 1.5382
Training Epoch: 11 [25472/50048]	Loss: 1.2262
Training Epoch: 11 [25600/50048]	Loss: 1.4972
Training Epoch: 11 [25728/50048]	Loss: 1.9018
Training Epoch: 11 [25856/50048]	Loss: 1.7573
Training Epoch: 11 [25984/50048]	Loss: 1.2901
Training Epoch: 11 [26112/50048]	Loss: 1.5440
Training Epoch: 11 [26240/50048]	Loss: 1.5054
Training Epoch: 11 [26368/50048]	Loss: 1.3694
Training Epoch: 11 [26496/50048]	Loss: 1.4223
Training Epoch: 11 [26624/50048]	Loss: 1.3171
Training Epoch: 11 [26752/50048]	Loss: 1.3272
Training Epoch: 11 [26880/50048]	Loss: 1.5662
Training Epoch: 11 [27008/50048]	Loss: 1.6001
Training Epoch: 11 [27136/50048]	Loss: 1.5415
Training Epoch: 11 [27264/50048]	Loss: 1.4263
Training Epoch: 11 [27392/50048]	Loss: 1.5338
Training Epoch: 11 [27520/50048]	Loss: 1.3982
Training Epoch: 11 [27648/50048]	Loss: 1.4056
Training Epoch: 11 [27776/50048]	Loss: 1.4581
Training Epoch: 11 [27904/50048]	Loss: 1.6070
Training Epoch: 11 [28032/50048]	Loss: 1.6895
Training Epoch: 11 [28160/50048]	Loss: 1.6702
Training Epoch: 11 [28288/50048]	Loss: 1.3779
Training Epoch: 11 [28416/50048]	Loss: 1.5437
Training Epoch: 11 [28544/50048]	Loss: 1.4493
Training Epoch: 11 [28672/50048]	Loss: 1.4052
Training Epoch: 11 [28800/50048]	Loss: 1.5598
Training Epoch: 11 [28928/50048]	Loss: 1.4752
Training Epoch: 11 [29056/50048]	Loss: 1.6298
Training Epoch: 11 [29184/50048]	Loss: 1.5448
Training Epoch: 11 [29312/50048]	Loss: 1.5666
Training Epoch: 11 [29440/50048]	Loss: 1.6219
Training Epoch: 11 [29568/50048]	Loss: 1.7737
Training Epoch: 11 [29696/50048]	Loss: 1.6053
Training Epoch: 11 [29824/50048]	Loss: 1.5101
Training Epoch: 11 [29952/50048]	Loss: 1.3746
Training Epoch: 11 [30080/50048]	Loss: 1.4074
Training Epoch: 11 [30208/50048]	Loss: 1.5322
Training Epoch: 11 [30336/50048]	Loss: 1.3855
Training Epoch: 11 [30464/50048]	Loss: 1.6288
Training Epoch: 11 [30592/50048]	Loss: 1.4475
Training Epoch: 11 [30720/50048]	Loss: 1.2366
Training Epoch: 11 [30848/50048]	Loss: 1.5173
Training Epoch: 11 [30976/50048]	Loss: 1.3224
Training Epoch: 11 [31104/50048]	Loss: 1.4627
Training Epoch: 11 [31232/50048]	Loss: 1.7144
Training Epoch: 11 [31360/50048]	Loss: 1.4370
Training Epoch: 11 [31488/50048]	Loss: 1.3696
Training Epoch: 11 [31616/50048]	Loss: 1.5221
Training Epoch: 11 [31744/50048]	Loss: 1.4373
Training Epoch: 11 [31872/50048]	Loss: 1.5687
Training Epoch: 11 [32000/50048]	Loss: 1.5682
Training Epoch: 11 [32128/50048]	Loss: 1.5925
Training Epoch: 11 [32256/50048]	Loss: 1.4938
Training Epoch: 11 [32384/50048]	Loss: 1.5653
Training Epoch: 11 [32512/50048]	Loss: 1.3987
Training Epoch: 11 [32640/50048]	Loss: 1.5831
Training Epoch: 11 [32768/50048]	Loss: 1.4355
Training Epoch: 11 [32896/50048]	Loss: 1.1570
Training Epoch: 11 [33024/50048]	Loss: 1.5077
Training Epoch: 11 [33152/50048]	Loss: 1.6719
Training Epoch: 11 [33280/50048]	Loss: 1.7060
Training Epoch: 11 [33408/50048]	Loss: 1.5500
Training Epoch: 11 [33536/50048]	Loss: 1.3642
Training Epoch: 11 [33664/50048]	Loss: 1.6441
Training Epoch: 11 [33792/50048]	Loss: 1.3413
Training Epoch: 11 [33920/50048]	Loss: 1.7503
Training Epoch: 11 [34048/50048]	Loss: 1.6815
Training Epoch: 11 [34176/50048]	Loss: 1.5856
Training Epoch: 11 [34304/50048]	Loss: 1.4949
Training Epoch: 11 [34432/50048]	Loss: 1.6177
Training Epoch: 11 [34560/50048]	Loss: 1.5556
Training Epoch: 11 [34688/50048]	Loss: 1.6386
Training Epoch: 11 [34816/50048]	Loss: 1.4232
Training Epoch: 11 [34944/50048]	Loss: 1.6663
Training Epoch: 11 [35072/50048]	Loss: 1.5622
Training Epoch: 11 [35200/50048]	Loss: 1.3926
Training Epoch: 11 [35328/50048]	Loss: 1.5776
Training Epoch: 11 [35456/50048]	Loss: 1.5517
Training Epoch: 11 [35584/50048]	Loss: 1.5571
Training Epoch: 11 [35712/50048]	Loss: 1.5095
Training Epoch: 11 [35840/50048]	Loss: 1.5345
Training Epoch: 11 [35968/50048]	Loss: 1.5380
Training Epoch: 11 [36096/50048]	Loss: 1.3895
Training Epoch: 11 [36224/50048]	Loss: 1.4439
Training Epoch: 11 [36352/50048]	Loss: 1.5313
Training Epoch: 11 [36480/50048]	Loss: 1.6553
Training Epoch: 11 [36608/50048]	Loss: 1.4967
Training Epoch: 11 [36736/50048]	Loss: 1.6681
Training Epoch: 11 [36864/50048]	Loss: 1.4922
Training Epoch: 11 [36992/50048]	Loss: 1.6998
Training Epoch: 11 [37120/50048]	Loss: 1.6112
Training Epoch: 11 [37248/50048]	Loss: 1.3766
Training Epoch: 11 [37376/50048]	Loss: 1.7083
Training Epoch: 11 [37504/50048]	Loss: 1.6904
Training Epoch: 11 [37632/50048]	Loss: 1.5058
Training Epoch: 11 [37760/50048]	Loss: 1.5418
Training Epoch: 11 [37888/50048]	Loss: 1.3169
Training Epoch: 11 [38016/50048]	Loss: 1.7588
Training Epoch: 11 [38144/50048]	Loss: 1.2623
Training Epoch: 11 [38272/50048]	Loss: 1.5226
Training Epoch: 11 [38400/50048]	Loss: 1.4059
Training Epoch: 11 [38528/50048]	Loss: 1.2899
Training Epoch: 11 [38656/50048]	Loss: 1.5962
Training Epoch: 11 [38784/50048]	Loss: 1.6369
Training Epoch: 11 [38912/50048]	Loss: 1.6649
Training Epoch: 11 [39040/50048]	Loss: 1.5408
Training Epoch: 11 [39168/50048]	Loss: 1.3439
Training Epoch: 11 [39296/50048]	Loss: 1.5393
Training Epoch: 11 [39424/50048]	Loss: 1.5132
Training Epoch: 11 [39552/50048]	Loss: 1.5558
Training Epoch: 11 [39680/50048]	Loss: 1.4614
Training Epoch: 11 [39808/50048]	Loss: 1.6194
Training Epoch: 11 [39936/50048]	Loss: 1.5720
Training Epoch: 11 [40064/50048]	Loss: 1.4389
Training Epoch: 11 [40192/50048]	Loss: 1.4481
Training Epoch: 11 [40320/50048]	Loss: 1.5942
Training Epoch: 11 [40448/50048]	Loss: 1.3755
Training Epoch: 11 [40576/50048]	Loss: 1.4035
Training Epoch: 11 [40704/50048]	Loss: 1.5457
Training Epoch: 11 [40832/50048]	Loss: 1.6857
Training Epoch: 11 [40960/50048]	Loss: 1.6735
Training Epoch: 11 [41088/50048]	Loss: 1.3145
Training Epoch: 11 [41216/50048]	Loss: 1.6463
Training Epoch: 11 [41344/50048]	Loss: 1.2767
Training Epoch: 11 [41472/50048]	Loss: 1.7749
Training Epoch: 11 [41600/50048]	Loss: 1.6299
Training Epoch: 11 [41728/50048]	Loss: 1.3548
Training Epoch: 11 [41856/50048]	Loss: 1.3404
Training Epoch: 11 [41984/50048]	Loss: 1.4195
Training Epoch: 11 [42112/50048]	Loss: 1.6565
Training Epoch: 11 [42240/50048]	Loss: 1.2343
Training Epoch: 11 [42368/50048]	Loss: 1.5530
Training Epoch: 11 [42496/50048]	Loss: 1.7658
Training Epoch: 11 [42624/50048]	Loss: 1.7489
Training Epoch: 11 [42752/50048]	Loss: 1.5493
Training Epoch: 11 [42880/50048]	Loss: 1.5524
Training Epoch: 11 [43008/50048]	Loss: 1.5236
Training Epoch: 11 [43136/50048]	Loss: 1.5106
Training Epoch: 11 [43264/50048]	Loss: 1.3650
Training Epoch: 11 [43392/50048]	Loss: 1.4351
Training Epoch: 11 [43520/50048]	Loss: 1.5715
Training Epoch: 11 [43648/50048]	Loss: 1.4777
Training Epoch: 11 [43776/50048]	Loss: 1.5798
Training Epoch: 11 [43904/50048]	Loss: 1.5994
Training Epoch: 11 [44032/50048]	Loss: 1.2124
Training Epoch: 11 [44160/50048]	Loss: 1.4388
Training Epoch: 11 [44288/50048]	Loss: 1.6806
Training Epoch: 11 [44416/50048]	Loss: 1.6763
Training Epoch: 11 [44544/50048]	Loss: 1.3342
Training Epoch: 11 [44672/50048]	Loss: 1.7654
Training Epoch: 11 [44800/50048]	Loss: 1.5555
Training Epoch: 11 [44928/50048]	Loss: 1.5897
Training Epoch: 11 [45056/50048]	Loss: 1.4454
Training Epoch: 11 [45184/50048]	Loss: 1.4305
Training Epoch: 11 [45312/50048]	Loss: 1.3805
Training Epoch: 11 [45440/50048]	Loss: 1.4394
Training Epoch: 11 [45568/50048]	Loss: 1.2455
Training Epoch: 11 [45696/50048]	Loss: 1.4040
2022-12-06 06:25:52,744 [ZeusDataLoader(train)] train epoch 12 done: time=86.46 energy=10492.44
2022-12-06 06:25:52,746 [ZeusDataLoader(eval)] Epoch 12 begin.
Training Epoch: 11 [45824/50048]	Loss: 1.7398
Training Epoch: 11 [45952/50048]	Loss: 1.4419
Training Epoch: 11 [46080/50048]	Loss: 1.2551
Training Epoch: 11 [46208/50048]	Loss: 1.6885
Training Epoch: 11 [46336/50048]	Loss: 1.2250
Training Epoch: 11 [46464/50048]	Loss: 1.6116
Training Epoch: 11 [46592/50048]	Loss: 1.3948
Training Epoch: 11 [46720/50048]	Loss: 1.5913
Training Epoch: 11 [46848/50048]	Loss: 1.2019
Training Epoch: 11 [46976/50048]	Loss: 1.4901
Training Epoch: 11 [47104/50048]	Loss: 1.7376
Training Epoch: 11 [47232/50048]	Loss: 1.5194
Training Epoch: 11 [47360/50048]	Loss: 1.4898
Training Epoch: 11 [47488/50048]	Loss: 1.6616
Training Epoch: 11 [47616/50048]	Loss: 1.4765
Training Epoch: 11 [47744/50048]	Loss: 1.3177
Training Epoch: 11 [47872/50048]	Loss: 1.3991
Training Epoch: 11 [48000/50048]	Loss: 1.5814
Training Epoch: 11 [48128/50048]	Loss: 1.5363
Training Epoch: 11 [48256/50048]	Loss: 1.4427
Training Epoch: 11 [48384/50048]	Loss: 1.6531
Training Epoch: 11 [48512/50048]	Loss: 1.6412
Training Epoch: 11 [48640/50048]	Loss: 1.4214
Training Epoch: 11 [48768/50048]	Loss: 1.4558
Training Epoch: 11 [48896/50048]	Loss: 1.6248
Training Epoch: 11 [49024/50048]	Loss: 1.4241
Training Epoch: 11 [49152/50048]	Loss: 1.3872
Training Epoch: 11 [49280/50048]	Loss: 1.6830
Training Epoch: 11 [49408/50048]	Loss: 1.3640
Training Epoch: 11 [49536/50048]	Loss: 1.4664
Training Epoch: 11 [49664/50048]	Loss: 1.2745
Training Epoch: 11 [49792/50048]	Loss: 1.7446
Training Epoch: 11 [49920/50048]	Loss: 1.5591
Training Epoch: 11 [50048/50048]	Loss: 1.6800
2022-12-06 11:25:56.435 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:25:56,456 [ZeusDataLoader(eval)] eval epoch 12 done: time=3.70 energy=453.85
2022-12-06 06:25:56,456 [ZeusDataLoader(train)] Up to epoch 12: time=1082.59, energy=131393.79, cost=160423.21
2022-12-06 06:25:56,457 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:25:56,457 [ZeusDataLoader(train)] Expected next epoch: time=1172.39, energy=142191.81, cost=173679.60
2022-12-06 06:25:56,457 [ZeusDataLoader(train)] Epoch 13 begin.
Validation Epoch: 11, Average loss: 0.0131, Accuracy: 0.5417
2022-12-06 06:25:56,646 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:25:56,646 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:25:56.648 [ZeusMonitor] Monitor started.
2022-12-06 11:25:56.648 [ZeusMonitor] Running indefinitely. 2022-12-06 11:25:56.648 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:25:56.648 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e13+gpu0.power.log
Training Epoch: 12 [128/50048]	Loss: 1.6144
Training Epoch: 12 [256/50048]	Loss: 1.2952
Training Epoch: 12 [384/50048]	Loss: 1.4657
Training Epoch: 12 [512/50048]	Loss: 1.2704
Training Epoch: 12 [640/50048]	Loss: 1.4978
Training Epoch: 12 [768/50048]	Loss: 1.5420
Training Epoch: 12 [896/50048]	Loss: 1.2394
Training Epoch: 12 [1024/50048]	Loss: 1.3352
Training Epoch: 12 [1152/50048]	Loss: 1.4256
Training Epoch: 12 [1280/50048]	Loss: 1.2045
Training Epoch: 12 [1408/50048]	Loss: 1.4538
Training Epoch: 12 [1536/50048]	Loss: 1.3228
Training Epoch: 12 [1664/50048]	Loss: 1.3716
Training Epoch: 12 [1792/50048]	Loss: 1.4114
Training Epoch: 12 [1920/50048]	Loss: 1.3577
Training Epoch: 12 [2048/50048]	Loss: 1.3916
Training Epoch: 12 [2176/50048]	Loss: 1.5727
Training Epoch: 12 [2304/50048]	Loss: 1.4820
Training Epoch: 12 [2432/50048]	Loss: 1.3750
Training Epoch: 12 [2560/50048]	Loss: 1.3383
Training Epoch: 12 [2688/50048]	Loss: 1.3209
Training Epoch: 12 [2816/50048]	Loss: 1.2795
Training Epoch: 12 [2944/50048]	Loss: 1.4500
Training Epoch: 12 [3072/50048]	Loss: 1.3662
Training Epoch: 12 [3200/50048]	Loss: 1.4464
Training Epoch: 12 [3328/50048]	Loss: 1.4606
Training Epoch: 12 [3456/50048]	Loss: 1.3239
Training Epoch: 12 [3584/50048]	Loss: 1.3290
Training Epoch: 12 [3712/50048]	Loss: 1.2461
Training Epoch: 12 [3840/50048]	Loss: 1.2355
Training Epoch: 12 [3968/50048]	Loss: 1.4280
Training Epoch: 12 [4096/50048]	Loss: 1.4980
Training Epoch: 12 [4224/50048]	Loss: 1.5329
Training Epoch: 12 [4352/50048]	Loss: 1.3733
Training Epoch: 12 [4480/50048]	Loss: 1.2456
Training Epoch: 12 [4608/50048]	Loss: 1.4484
Training Epoch: 12 [4736/50048]	Loss: 1.2787
Training Epoch: 12 [4864/50048]	Loss: 1.6786
Training Epoch: 12 [4992/50048]	Loss: 1.3235
Training Epoch: 12 [5120/50048]	Loss: 1.3506
Training Epoch: 12 [5248/50048]	Loss: 1.3747
Training Epoch: 12 [5376/50048]	Loss: 1.3614
Training Epoch: 12 [5504/50048]	Loss: 1.6318
Training Epoch: 12 [5632/50048]	Loss: 1.4560
Training Epoch: 12 [5760/50048]	Loss: 1.3014
Training Epoch: 12 [5888/50048]	Loss: 1.4564
Training Epoch: 12 [6016/50048]	Loss: 1.3127
Training Epoch: 12 [6144/50048]	Loss: 1.6110
Training Epoch: 12 [6272/50048]	Loss: 1.6571
Training Epoch: 12 [6400/50048]	Loss: 1.4904
Training Epoch: 12 [6528/50048]	Loss: 1.7158
Training Epoch: 12 [6656/50048]	Loss: 1.4293
Training Epoch: 12 [6784/50048]	Loss: 1.8013
Training Epoch: 12 [6912/50048]	Loss: 1.4561
Training Epoch: 12 [7040/50048]	Loss: 1.3732
Training Epoch: 12 [7168/50048]	Loss: 1.4701
Training Epoch: 12 [7296/50048]	Loss: 1.4034
Training Epoch: 12 [7424/50048]	Loss: 1.1820
Training Epoch: 12 [7552/50048]	Loss: 1.5578
Training Epoch: 12 [7680/50048]	Loss: 1.1170
Training Epoch: 12 [7808/50048]	Loss: 1.5367
Training Epoch: 12 [7936/50048]	Loss: 1.4436
Training Epoch: 12 [8064/50048]	Loss: 1.0512
Training Epoch: 12 [8192/50048]	Loss: 1.3344
Training Epoch: 12 [8320/50048]	Loss: 1.6121
Training Epoch: 12 [8448/50048]	Loss: 1.6681
Training Epoch: 12 [8576/50048]	Loss: 1.4022
Training Epoch: 12 [8704/50048]	Loss: 1.2164
Training Epoch: 12 [8832/50048]	Loss: 1.5067
Training Epoch: 12 [8960/50048]	Loss: 1.5150
Training Epoch: 12 [9088/50048]	Loss: 1.4954
Training Epoch: 12 [9216/50048]	Loss: 1.4458
Training Epoch: 12 [9344/50048]	Loss: 1.3123
Training Epoch: 12 [9472/50048]	Loss: 1.3343
Training Epoch: 12 [9600/50048]	Loss: 1.4427
Training Epoch: 12 [9728/50048]	Loss: 1.4554
Training Epoch: 12 [9856/50048]	Loss: 1.2853
Training Epoch: 12 [9984/50048]	Loss: 1.4848
Training Epoch: 12 [10112/50048]	Loss: 1.3488
Training Epoch: 12 [10240/50048]	Loss: 1.4807
Training Epoch: 12 [10368/50048]	Loss: 1.8243
Training Epoch: 12 [10496/50048]	Loss: 1.7464
Training Epoch: 12 [10624/50048]	Loss: 1.6455
Training Epoch: 12 [10752/50048]	Loss: 1.4391
Training Epoch: 12 [10880/50048]	Loss: 1.5062
Training Epoch: 12 [11008/50048]	Loss: 1.3337
Training Epoch: 12 [11136/50048]	Loss: 1.3462
Training Epoch: 12 [11264/50048]	Loss: 1.4237
Training Epoch: 12 [11392/50048]	Loss: 1.4279
Training Epoch: 12 [11520/50048]	Loss: 1.5600
Training Epoch: 12 [11648/50048]	Loss: 1.3203
Training Epoch: 12 [11776/50048]	Loss: 1.5173
Training Epoch: 12 [11904/50048]	Loss: 1.4176
Training Epoch: 12 [12032/50048]	Loss: 1.3641
Training Epoch: 12 [12160/50048]	Loss: 1.3024
Training Epoch: 12 [12288/50048]	Loss: 1.3492
Training Epoch: 12 [12416/50048]	Loss: 1.3255
Training Epoch: 12 [12544/50048]	Loss: 1.5974
Training Epoch: 12 [12672/50048]	Loss: 1.5285
Training Epoch: 12 [12800/50048]	Loss: 1.5213
Training Epoch: 12 [12928/50048]	Loss: 1.2639
Training Epoch: 12 [13056/50048]	Loss: 1.6339
Training Epoch: 12 [13184/50048]	Loss: 1.2468
Training Epoch: 12 [13312/50048]	Loss: 1.4082
Training Epoch: 12 [13440/50048]	Loss: 1.3472
Training Epoch: 12 [13568/50048]	Loss: 1.5484
Training Epoch: 12 [13696/50048]	Loss: 1.5796
Training Epoch: 12 [13824/50048]	Loss: 1.6050
Training Epoch: 12 [13952/50048]	Loss: 1.3989
Training Epoch: 12 [14080/50048]	Loss: 1.4736
Training Epoch: 12 [14208/50048]	Loss: 1.2438
Training Epoch: 12 [14336/50048]	Loss: 1.3472
Training Epoch: 12 [14464/50048]	Loss: 1.2793
Training Epoch: 12 [14592/50048]	Loss: 1.3863
Training Epoch: 12 [14720/50048]	Loss: 1.4823
Training Epoch: 12 [14848/50048]	Loss: 1.2827
Training Epoch: 12 [14976/50048]	Loss: 1.2114
Training Epoch: 12 [15104/50048]	Loss: 1.2956
Training Epoch: 12 [15232/50048]	Loss: 1.2706
Training Epoch: 12 [15360/50048]	Loss: 1.4314
Training Epoch: 12 [15488/50048]	Loss: 1.3852
Training Epoch: 12 [15616/50048]	Loss: 1.4031
Training Epoch: 12 [15744/50048]	Loss: 1.5225
Training Epoch: 12 [15872/50048]	Loss: 1.1850
Training Epoch: 12 [16000/50048]	Loss: 1.4151
Training Epoch: 12 [16128/50048]	Loss: 1.5199
Training Epoch: 12 [16256/50048]	Loss: 1.4869
Training Epoch: 12 [16384/50048]	Loss: 1.6423
Training Epoch: 12 [16512/50048]	Loss: 1.4023
Training Epoch: 12 [16640/50048]	Loss: 1.4393
Training Epoch: 12 [16768/50048]	Loss: 1.3633
Training Epoch: 12 [16896/50048]	Loss: 1.5337
Training Epoch: 12 [17024/50048]	Loss: 1.3350
Training Epoch: 12 [17152/50048]	Loss: 1.3717
Training Epoch: 12 [17280/50048]	Loss: 1.4912
Training Epoch: 12 [17408/50048]	Loss: 1.4679
Training Epoch: 12 [17536/50048]	Loss: 1.3701
Training Epoch: 12 [17664/50048]	Loss: 1.3379
Training Epoch: 12 [17792/50048]	Loss: 1.5266
Training Epoch: 12 [17920/50048]	Loss: 1.4201
Training Epoch: 12 [18048/50048]	Loss: 1.4817
Training Epoch: 12 [18176/50048]	Loss: 1.3981
Training Epoch: 12 [18304/50048]	Loss: 1.3201
Training Epoch: 12 [18432/50048]	Loss: 1.3415
Training Epoch: 12 [18560/50048]	Loss: 1.5790
Training Epoch: 12 [18688/50048]	Loss: 1.3240
Training Epoch: 12 [18816/50048]	Loss: 1.3211
Training Epoch: 12 [18944/50048]	Loss: 1.4769
Training Epoch: 12 [19072/50048]	Loss: 1.1295
Training Epoch: 12 [19200/50048]	Loss: 1.4482
Training Epoch: 12 [19328/50048]	Loss: 1.4953
Training Epoch: 12 [19456/50048]	Loss: 1.3709
Training Epoch: 12 [19584/50048]	Loss: 1.3474
Training Epoch: 12 [19712/50048]	Loss: 1.4403
Training Epoch: 12 [19840/50048]	Loss: 1.3978
Training Epoch: 12 [19968/50048]	Loss: 1.3574
Training Epoch: 12 [20096/50048]	Loss: 1.1838
Training Epoch: 12 [20224/50048]	Loss: 1.7096
Training Epoch: 12 [20352/50048]	Loss: 1.4113
Training Epoch: 12 [20480/50048]	Loss: 1.3278
Training Epoch: 12 [20608/50048]	Loss: 1.6640
Training Epoch: 12 [20736/50048]	Loss: 1.6714
Training Epoch: 12 [20864/50048]	Loss: 1.4088
Training Epoch: 12 [20992/50048]	Loss: 1.6013
Training Epoch: 12 [21120/50048]	Loss: 1.4423
Training Epoch: 12 [21248/50048]	Loss: 1.3013
Training Epoch: 12 [21376/50048]	Loss: 1.2538
Training Epoch: 12 [21504/50048]	Loss: 1.5351
Training Epoch: 12 [21632/50048]	Loss: 1.6028
Training Epoch: 12 [21760/50048]	Loss: 1.4909
Training Epoch: 12 [21888/50048]	Loss: 1.3204
Training Epoch: 12 [22016/50048]	Loss: 1.5548
Training Epoch: 12 [22144/50048]	Loss: 1.3857
Training Epoch: 12 [22272/50048]	Loss: 1.2167
Training Epoch: 12 [22400/50048]	Loss: 1.5510
Training Epoch: 12 [22528/50048]	Loss: 1.3983
Training Epoch: 12 [22656/50048]	Loss: 1.6728
Training Epoch: 12 [22784/50048]	Loss: 1.6280
Training Epoch: 12 [22912/50048]	Loss: 1.5720
Training Epoch: 12 [23040/50048]	Loss: 1.3696
Training Epoch: 12 [23168/50048]	Loss: 1.5449
Training Epoch: 12 [23296/50048]	Loss: 1.3264
Training Epoch: 12 [23424/50048]	Loss: 1.4255
Training Epoch: 12 [23552/50048]	Loss: 1.5247
Training Epoch: 12 [23680/50048]	Loss: 1.4866
Training Epoch: 12 [23808/50048]	Loss: 1.4848
Training Epoch: 12 [23936/50048]	Loss: 1.3817
Training Epoch: 12 [24064/50048]	Loss: 1.2832
Training Epoch: 12 [24192/50048]	Loss: 1.3310
Training Epoch: 12 [24320/50048]	Loss: 1.1862
Training Epoch: 12 [24448/50048]	Loss: 1.4076
Training Epoch: 12 [24576/50048]	Loss: 1.5628
Training Epoch: 12 [24704/50048]	Loss: 1.3746
Training Epoch: 12 [24832/50048]	Loss: 1.8713
Training Epoch: 12 [24960/50048]	Loss: 1.4767
Training Epoch: 12 [25088/50048]	Loss: 1.6958
Training Epoch: 12 [25216/50048]	Loss: 1.5597
Training Epoch: 12 [25344/50048]	Loss: 1.4793
Training Epoch: 12 [25472/50048]	Loss: 1.4915
Training Epoch: 12 [25600/50048]	Loss: 1.3295
Training Epoch: 12 [25728/50048]	Loss: 1.5011
Training Epoch: 12 [25856/50048]	Loss: 1.7694
Training Epoch: 12 [25984/50048]	Loss: 1.4567
Training Epoch: 12 [26112/50048]	Loss: 1.6719
Training Epoch: 12 [26240/50048]	Loss: 1.4776
Training Epoch: 12 [26368/50048]	Loss: 1.4039
Training Epoch: 12 [26496/50048]	Loss: 1.6996
Training Epoch: 12 [26624/50048]	Loss: 1.3303
Training Epoch: 12 [26752/50048]	Loss: 1.2813
Training Epoch: 12 [26880/50048]	Loss: 1.4470
Training Epoch: 12 [27008/50048]	Loss: 1.3137
Training Epoch: 12 [27136/50048]	Loss: 1.3838
Training Epoch: 12 [27264/50048]	Loss: 1.2253
Training Epoch: 12 [27392/50048]	Loss: 1.3462
Training Epoch: 12 [27520/50048]	Loss: 1.3974
Training Epoch: 12 [27648/50048]	Loss: 1.6312
Training Epoch: 12 [27776/50048]	Loss: 1.5297
Training Epoch: 12 [27904/50048]	Loss: 1.4581
Training Epoch: 12 [28032/50048]	Loss: 1.3908
Training Epoch: 12 [28160/50048]	Loss: 1.3322
Training Epoch: 12 [28288/50048]	Loss: 1.1811
Training Epoch: 12 [28416/50048]	Loss: 1.4469
Training Epoch: 12 [28544/50048]	Loss: 1.7038
Training Epoch: 12 [28672/50048]	Loss: 1.4612
Training Epoch: 12 [28800/50048]	Loss: 1.1873
Training Epoch: 12 [28928/50048]	Loss: 1.6942
Training Epoch: 12 [29056/50048]	Loss: 1.4610
Training Epoch: 12 [29184/50048]	Loss: 1.2543
Training Epoch: 12 [29312/50048]	Loss: 1.3821
Training Epoch: 12 [29440/50048]	Loss: 1.4405
Training Epoch: 12 [29568/50048]	Loss: 1.4818
Training Epoch: 12 [29696/50048]	Loss: 1.7223
Training Epoch: 12 [29824/50048]	Loss: 1.4553
Training Epoch: 12 [29952/50048]	Loss: 1.3599
Training Epoch: 12 [30080/50048]	Loss: 1.2849
Training Epoch: 12 [30208/50048]	Loss: 1.1088
Training Epoch: 12 [30336/50048]	Loss: 1.7174
Training Epoch: 12 [30464/50048]	Loss: 1.3030
Training Epoch: 12 [30592/50048]	Loss: 1.2985
Training Epoch: 12 [30720/50048]	Loss: 1.2729
Training Epoch: 12 [30848/50048]	Loss: 1.2770
Training Epoch: 12 [30976/50048]	Loss: 1.3467
Training Epoch: 12 [31104/50048]	Loss: 1.2050
Training Epoch: 12 [31232/50048]	Loss: 1.5785
Training Epoch: 12 [31360/50048]	Loss: 1.4561
Training Epoch: 12 [31488/50048]	Loss: 1.4821
Training Epoch: 12 [31616/50048]	Loss: 1.4433
Training Epoch: 12 [31744/50048]	Loss: 1.2676
Training Epoch: 12 [31872/50048]	Loss: 1.2038
Training Epoch: 12 [32000/50048]	Loss: 1.7013
Training Epoch: 12 [32128/50048]	Loss: 1.5336
Training Epoch: 12 [32256/50048]	Loss: 1.3433
Training Epoch: 12 [32384/50048]	Loss: 1.3238
Training Epoch: 12 [32512/50048]	Loss: 1.4204
Training Epoch: 12 [32640/50048]	Loss: 1.5311
Training Epoch: 12 [32768/50048]	Loss: 1.5016
Training Epoch: 12 [32896/50048]	Loss: 1.4387
Training Epoch: 12 [33024/50048]	Loss: 1.3637
Training Epoch: 12 [33152/50048]	Loss: 1.3702
Training Epoch: 12 [33280/50048]	Loss: 1.5633
Training Epoch: 12 [33408/50048]	Loss: 1.3743
Training Epoch: 12 [33536/50048]	Loss: 1.4250
Training Epoch: 12 [33664/50048]	Loss: 1.2470
Training Epoch: 12 [33792/50048]	Loss: 1.2237
Training Epoch: 12 [33920/50048]	Loss: 1.1746
Training Epoch: 12 [34048/50048]	Loss: 1.4724
Training Epoch: 12 [34176/50048]	Loss: 1.4368
Training Epoch: 12 [34304/50048]	Loss: 1.4466
Training Epoch: 12 [34432/50048]	Loss: 1.4843
Training Epoch: 12 [34560/50048]	Loss: 1.3157
Training Epoch: 12 [34688/50048]	Loss: 1.4159
Training Epoch: 12 [34816/50048]	Loss: 1.2859
Training Epoch: 12 [34944/50048]	Loss: 1.6315
Training Epoch: 12 [35072/50048]	Loss: 1.2036
Training Epoch: 12 [35200/50048]	Loss: 1.2580
Training Epoch: 12 [35328/50048]	Loss: 1.4106
Training Epoch: 12 [35456/50048]	Loss: 1.4816
Training Epoch: 12 [35584/50048]	Loss: 1.6399
Training Epoch: 12 [35712/50048]	Loss: 1.3067
Training Epoch: 12 [35840/50048]	Loss: 1.3525
Training Epoch: 12 [35968/50048]	Loss: 1.1916
Training Epoch: 12 [36096/50048]	Loss: 1.4746
Training Epoch: 12 [36224/50048]	Loss: 1.4601
Training Epoch: 12 [36352/50048]	Loss: 1.6507
Training Epoch: 12 [36480/50048]	Loss: 1.4660
Training Epoch: 12 [36608/50048]	Loss: 1.5255
Training Epoch: 12 [36736/50048]	Loss: 1.3809
Training Epoch: 12 [36864/50048]	Loss: 1.4816
Training Epoch: 12 [36992/50048]	Loss: 1.2600
Training Epoch: 12 [37120/50048]	Loss: 1.4830
Training Epoch: 12 [37248/50048]	Loss: 1.7552
Training Epoch: 12 [37376/50048]	Loss: 1.7542
Training Epoch: 12 [37504/50048]	Loss: 1.7625
Training Epoch: 12 [37632/50048]	Loss: 1.7844
Training Epoch: 12 [37760/50048]	Loss: 1.2653
Training Epoch: 12 [37888/50048]	Loss: 1.3828
Training Epoch: 12 [38016/50048]	Loss: 1.4274
Training Epoch: 12 [38144/50048]	Loss: 1.7168
Training Epoch: 12 [38272/50048]	Loss: 1.3205
Training Epoch: 12 [38400/50048]	Loss: 1.5859
Training Epoch: 12 [38528/50048]	Loss: 1.3639
Training Epoch: 12 [38656/50048]	Loss: 1.2330
Training Epoch: 12 [38784/50048]	Loss: 1.6104
Training Epoch: 12 [38912/50048]	Loss: 1.4963
Training Epoch: 12 [39040/50048]	Loss: 1.3666
Training Epoch: 12 [39168/50048]	Loss: 1.5197
Training Epoch: 12 [39296/50048]	Loss: 1.5032
Training Epoch: 12 [39424/50048]	Loss: 1.6134
Training Epoch: 12 [39552/50048]	Loss: 1.3973
Training Epoch: 12 [39680/50048]	Loss: 1.4381
Training Epoch: 12 [39808/50048]	Loss: 1.2242
Training Epoch: 12 [39936/50048]	Loss: 1.3982
Training Epoch: 12 [40064/50048]	Loss: 1.4668
Training Epoch: 12 [40192/50048]	Loss: 1.4404
Training Epoch: 12 [40320/50048]	Loss: 1.3649
Training Epoch: 12 [40448/50048]	Loss: 1.4263
Training Epoch: 12 [40576/50048]	Loss: 1.2670
Training Epoch: 12 [40704/50048]	Loss: 1.3366
Training Epoch: 12 [40832/50048]	Loss: 1.4841
Training Epoch: 12 [40960/50048]	Loss: 1.3850
Training Epoch: 12 [41088/50048]	Loss: 1.6228
Training Epoch: 12 [41216/50048]	Loss: 1.3177
Training Epoch: 12 [41344/50048]	Loss: 1.4064
Training Epoch: 12 [41472/50048]	Loss: 1.6515
Training Epoch: 12 [41600/50048]	Loss: 1.2414
Training Epoch: 12 [41728/50048]	Loss: 1.4576
Training Epoch: 12 [41856/50048]	Loss: 1.5869
Training Epoch: 12 [41984/50048]	Loss: 1.4252
Training Epoch: 12 [42112/50048]	Loss: 1.4274
Training Epoch: 12 [42240/50048]	Loss: 1.2125
Training Epoch: 12 [42368/50048]	Loss: 1.6532
Training Epoch: 12 [42496/50048]	Loss: 1.1429
Training Epoch: 12 [42624/50048]	Loss: 1.3293
Training Epoch: 12 [42752/50048]	Loss: 1.6947
Training Epoch: 12 [42880/50048]	Loss: 1.1652
Training Epoch: 12 [43008/50048]	Loss: 1.6487
Training Epoch: 12 [43136/50048]	Loss: 1.4067
Training Epoch: 12 [43264/50048]	Loss: 1.4441
Training Epoch: 12 [43392/50048]	Loss: 1.5406
Training Epoch: 12 [43520/50048]	Loss: 1.5576
Training Epoch: 12 [43648/50048]	Loss: 1.5514
Training Epoch: 12 [43776/50048]	Loss: 1.3138
Training Epoch: 12 [43904/50048]	Loss: 1.2872
Training Epoch: 12 [44032/50048]	Loss: 1.6630
Training Epoch: 12 [44160/50048]	Loss: 1.4359
Training Epoch: 12 [44288/50048]	Loss: 1.3872
Training Epoch: 12 [44416/50048]	Loss: 1.4048
Training Epoch: 12 [44544/50048]	Loss: 1.4195
Training Epoch: 12 [44672/50048]	Loss: 1.4665
Training Epoch: 12 [44800/50048]	Loss: 1.2653
Training Epoch: 12 [44928/50048]	Loss: 1.3715
Training Epoch: 12 [45056/50048]	Loss: 1.3928
Training Epoch: 12 [45184/50048]	Loss: 1.4938
Training Epoch: 12 [45312/50048]	Loss: 1.7487
Training Epoch: 12 [45440/50048]	Loss: 1.6002
Training Epoch: 12 [45568/50048]	Loss: 1.4627
Training Epoch: 12 [45696/50048]	Loss: 1.5422
2022-12-06 06:27:22,996 [ZeusDataLoader(train)] train epoch 13 done: time=86.53 energy=10506.95
2022-12-06 06:27:22,997 [ZeusDataLoader(eval)] Epoch 13 begin.
Training Epoch: 12 [45824/50048]	Loss: 1.3169
Training Epoch: 12 [45952/50048]	Loss: 1.4436
Training Epoch: 12 [46080/50048]	Loss: 1.4800
Training Epoch: 12 [46208/50048]	Loss: 1.5251
Training Epoch: 12 [46336/50048]	Loss: 1.2576
Training Epoch: 12 [46464/50048]	Loss: 1.3084
Training Epoch: 12 [46592/50048]	Loss: 1.4163
Training Epoch: 12 [46720/50048]	Loss: 1.2364
Training Epoch: 12 [46848/50048]	Loss: 1.5600
Training Epoch: 12 [46976/50048]	Loss: 1.5239
Training Epoch: 12 [47104/50048]	Loss: 1.2351
Training Epoch: 12 [47232/50048]	Loss: 1.3986
Training Epoch: 12 [47360/50048]	Loss: 1.5044
Training Epoch: 12 [47488/50048]	Loss: 1.3319
Training Epoch: 12 [47616/50048]	Loss: 1.3471
Training Epoch: 12 [47744/50048]	Loss: 1.4923
Training Epoch: 12 [47872/50048]	Loss: 1.2927
Training Epoch: 12 [48000/50048]	Loss: 1.5008
Training Epoch: 12 [48128/50048]	Loss: 1.3799
Training Epoch: 12 [48256/50048]	Loss: 1.3479
Training Epoch: 12 [48384/50048]	Loss: 1.3090
Training Epoch: 12 [48512/50048]	Loss: 1.4416
Training Epoch: 12 [48640/50048]	Loss: 1.3893
Training Epoch: 12 [48768/50048]	Loss: 1.4838
Training Epoch: 12 [48896/50048]	Loss: 1.2705
Training Epoch: 12 [49024/50048]	Loss: 1.5173
Training Epoch: 12 [49152/50048]	Loss: 1.2346
Training Epoch: 12 [49280/50048]	Loss: 1.2608
Training Epoch: 12 [49408/50048]	Loss: 1.5489
Training Epoch: 12 [49536/50048]	Loss: 1.2755
Training Epoch: 12 [49664/50048]	Loss: 1.5936
Training Epoch: 12 [49792/50048]	Loss: 1.3742
Training Epoch: 12 [49920/50048]	Loss: 1.4469
Training Epoch: 12 [50048/50048]	Loss: 1.1887
2022-12-06 11:27:26.660 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:27:26,685 [ZeusDataLoader(eval)] eval epoch 13 done: time=3.68 energy=440.01
2022-12-06 06:27:26,685 [ZeusDataLoader(train)] Up to epoch 13: time=1172.79, energy=142340.76, cost=173789.81
2022-12-06 06:27:26,686 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:27:26,686 [ZeusDataLoader(train)] Expected next epoch: time=1262.59, energy=153138.77, cost=187046.20
2022-12-06 06:27:26,687 [ZeusDataLoader(train)] Epoch 14 begin.
Validation Epoch: 12, Average loss: 0.0127, Accuracy: 0.5486
2022-12-06 06:27:26,863 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:27:26,864 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:27:26.881 [ZeusMonitor] Monitor started.
2022-12-06 11:27:26.882 [ZeusMonitor] Running indefinitely. 2022-12-06 11:27:26.882 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:27:26.882 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e14+gpu0.power.log
Training Epoch: 13 [128/50048]	Loss: 1.2976
Training Epoch: 13 [256/50048]	Loss: 1.3569
Training Epoch: 13 [384/50048]	Loss: 1.5492
Training Epoch: 13 [512/50048]	Loss: 1.0433
Training Epoch: 13 [640/50048]	Loss: 1.3259
Training Epoch: 13 [768/50048]	Loss: 1.0895
Training Epoch: 13 [896/50048]	Loss: 1.5738
Training Epoch: 13 [1024/50048]	Loss: 1.1755
Training Epoch: 13 [1152/50048]	Loss: 1.3871
Training Epoch: 13 [1280/50048]	Loss: 1.3980
Training Epoch: 13 [1408/50048]	Loss: 1.4016
Training Epoch: 13 [1536/50048]	Loss: 1.3918
Training Epoch: 13 [1664/50048]	Loss: 1.2408
Training Epoch: 13 [1792/50048]	Loss: 1.2267
Training Epoch: 13 [1920/50048]	Loss: 1.3969
Training Epoch: 13 [2048/50048]	Loss: 1.3799
Training Epoch: 13 [2176/50048]	Loss: 1.4974
Training Epoch: 13 [2304/50048]	Loss: 1.3533
Training Epoch: 13 [2432/50048]	Loss: 1.1645
Training Epoch: 13 [2560/50048]	Loss: 1.3590
Training Epoch: 13 [2688/50048]	Loss: 1.4590
Training Epoch: 13 [2816/50048]	Loss: 1.4380
Training Epoch: 13 [2944/50048]	Loss: 1.2021
Training Epoch: 13 [3072/50048]	Loss: 1.3886
Training Epoch: 13 [3200/50048]	Loss: 1.3079
Training Epoch: 13 [3328/50048]	Loss: 1.1955
Training Epoch: 13 [3456/50048]	Loss: 1.3483
Training Epoch: 13 [3584/50048]	Loss: 1.2592
Training Epoch: 13 [3712/50048]	Loss: 1.5238
Training Epoch: 13 [3840/50048]	Loss: 1.1685
Training Epoch: 13 [3968/50048]	Loss: 1.3127
Training Epoch: 13 [4096/50048]	Loss: 1.2915
Training Epoch: 13 [4224/50048]	Loss: 1.3792
Training Epoch: 13 [4352/50048]	Loss: 1.2649
Training Epoch: 13 [4480/50048]	Loss: 1.4178
Training Epoch: 13 [4608/50048]	Loss: 1.2425
Training Epoch: 13 [4736/50048]	Loss: 1.5216
Training Epoch: 13 [4864/50048]	Loss: 1.3493
Training Epoch: 13 [4992/50048]	Loss: 1.4081
Training Epoch: 13 [5120/50048]	Loss: 1.2891
Training Epoch: 13 [5248/50048]	Loss: 1.5288
Training Epoch: 13 [5376/50048]	Loss: 1.2197
Training Epoch: 13 [5504/50048]	Loss: 1.4455
Training Epoch: 13 [5632/50048]	Loss: 1.2342
Training Epoch: 13 [5760/50048]	Loss: 1.5991
Training Epoch: 13 [5888/50048]	Loss: 1.3456
Training Epoch: 13 [6016/50048]	Loss: 1.2381
Training Epoch: 13 [6144/50048]	Loss: 1.2719
Training Epoch: 13 [6272/50048]	Loss: 1.3938
Training Epoch: 13 [6400/50048]	Loss: 1.2553
Training Epoch: 13 [6528/50048]	Loss: 1.2856
Training Epoch: 13 [6656/50048]	Loss: 1.1391
Training Epoch: 13 [6784/50048]	Loss: 1.3174
Training Epoch: 13 [6912/50048]	Loss: 1.2587
Training Epoch: 13 [7040/50048]	Loss: 1.1143
Training Epoch: 13 [7168/50048]	Loss: 1.6730
Training Epoch: 13 [7296/50048]	Loss: 1.4409
Training Epoch: 13 [7424/50048]	Loss: 1.4011
Training Epoch: 13 [7552/50048]	Loss: 1.5256
Training Epoch: 13 [7680/50048]	Loss: 1.1950
Training Epoch: 13 [7808/50048]	Loss: 1.5703
Training Epoch: 13 [7936/50048]	Loss: 1.3588
Training Epoch: 13 [8064/50048]	Loss: 1.3168
Training Epoch: 13 [8192/50048]	Loss: 1.2475
Training Epoch: 13 [8320/50048]	Loss: 1.0050
Training Epoch: 13 [8448/50048]	Loss: 1.2102
Training Epoch: 13 [8576/50048]	Loss: 1.1838
Training Epoch: 13 [8704/50048]	Loss: 1.3020
Training Epoch: 13 [8832/50048]	Loss: 1.3793
Training Epoch: 13 [8960/50048]	Loss: 1.3616
Training Epoch: 13 [9088/50048]	Loss: 1.3973
Training Epoch: 13 [9216/50048]	Loss: 1.3679
Training Epoch: 13 [9344/50048]	Loss: 1.2849
Training Epoch: 13 [9472/50048]	Loss: 1.4389
Training Epoch: 13 [9600/50048]	Loss: 1.6246
Training Epoch: 13 [9728/50048]	Loss: 1.4376
Training Epoch: 13 [9856/50048]	Loss: 1.5413
Training Epoch: 13 [9984/50048]	Loss: 1.2645
Training Epoch: 13 [10112/50048]	Loss: 1.5157
Training Epoch: 13 [10240/50048]	Loss: 1.4297
Training Epoch: 13 [10368/50048]	Loss: 1.3313
Training Epoch: 13 [10496/50048]	Loss: 1.3815
Training Epoch: 13 [10624/50048]	Loss: 1.2062
Training Epoch: 13 [10752/50048]	Loss: 1.3416
Training Epoch: 13 [10880/50048]	Loss: 1.4456
Training Epoch: 13 [11008/50048]	Loss: 1.4453
Training Epoch: 13 [11136/50048]	Loss: 1.1734
Training Epoch: 13 [11264/50048]	Loss: 1.4279
Training Epoch: 13 [11392/50048]	Loss: 1.4546
Training Epoch: 13 [11520/50048]	Loss: 1.2139
Training Epoch: 13 [11648/50048]	Loss: 1.3084
Training Epoch: 13 [11776/50048]	Loss: 1.3777
Training Epoch: 13 [11904/50048]	Loss: 1.3485
Training Epoch: 13 [12032/50048]	Loss: 1.2541
Training Epoch: 13 [12160/50048]	Loss: 1.4587
Training Epoch: 13 [12288/50048]	Loss: 1.3070
Training Epoch: 13 [12416/50048]	Loss: 1.4201
Training Epoch: 13 [12544/50048]	Loss: 1.2531
Training Epoch: 13 [12672/50048]	Loss: 1.3307
Training Epoch: 13 [12800/50048]	Loss: 1.4536
Training Epoch: 13 [12928/50048]	Loss: 1.3795
Training Epoch: 13 [13056/50048]	Loss: 1.4586
Training Epoch: 13 [13184/50048]	Loss: 1.5097
Training Epoch: 13 [13312/50048]	Loss: 1.0878
Training Epoch: 13 [13440/50048]	Loss: 1.2879
Training Epoch: 13 [13568/50048]	Loss: 1.1771
Training Epoch: 13 [13696/50048]	Loss: 1.3949
Training Epoch: 13 [13824/50048]	Loss: 1.3011
Training Epoch: 13 [13952/50048]	Loss: 1.3664
Training Epoch: 13 [14080/50048]	Loss: 1.4407
Training Epoch: 13 [14208/50048]	Loss: 1.3812
Training Epoch: 13 [14336/50048]	Loss: 1.4259
Training Epoch: 13 [14464/50048]	Loss: 1.6045
Training Epoch: 13 [14592/50048]	Loss: 1.2676
Training Epoch: 13 [14720/50048]	Loss: 1.1892
Training Epoch: 13 [14848/50048]	Loss: 1.4820
Training Epoch: 13 [14976/50048]	Loss: 1.3809
Training Epoch: 13 [15104/50048]	Loss: 1.3135
Training Epoch: 13 [15232/50048]	Loss: 1.3870
Training Epoch: 13 [15360/50048]	Loss: 1.3153
Training Epoch: 13 [15488/50048]	Loss: 1.2628
Training Epoch: 13 [15616/50048]	Loss: 1.3220
Training Epoch: 13 [15744/50048]	Loss: 1.2891
Training Epoch: 13 [15872/50048]	Loss: 1.5450
Training Epoch: 13 [16000/50048]	Loss: 1.3569
Training Epoch: 13 [16128/50048]	Loss: 1.4302
Training Epoch: 13 [16256/50048]	Loss: 1.3923
Training Epoch: 13 [16384/50048]	Loss: 1.6260
Training Epoch: 13 [16512/50048]	Loss: 1.3182
Training Epoch: 13 [16640/50048]	Loss: 1.3115
Training Epoch: 13 [16768/50048]	Loss: 1.4684
Training Epoch: 13 [16896/50048]	Loss: 1.3696
Training Epoch: 13 [17024/50048]	Loss: 1.2212
Training Epoch: 13 [17152/50048]	Loss: 1.1453
Training Epoch: 13 [17280/50048]	Loss: 1.4588
Training Epoch: 13 [17408/50048]	Loss: 1.5120
Training Epoch: 13 [17536/50048]	Loss: 1.4083
Training Epoch: 13 [17664/50048]	Loss: 1.2181
Training Epoch: 13 [17792/50048]	Loss: 1.3524
Training Epoch: 13 [17920/50048]	Loss: 1.2261
Training Epoch: 13 [18048/50048]	Loss: 1.3071
Training Epoch: 13 [18176/50048]	Loss: 1.2956
Training Epoch: 13 [18304/50048]	Loss: 1.2477
Training Epoch: 13 [18432/50048]	Loss: 1.1085
Training Epoch: 13 [18560/50048]	Loss: 1.4741
Training Epoch: 13 [18688/50048]	Loss: 1.2762
Training Epoch: 13 [18816/50048]	Loss: 1.1678
Training Epoch: 13 [18944/50048]	Loss: 1.4106
Training Epoch: 13 [19072/50048]	Loss: 1.5406
Training Epoch: 13 [19200/50048]	Loss: 1.1683
Training Epoch: 13 [19328/50048]	Loss: 1.4595
Training Epoch: 13 [19456/50048]	Loss: 1.4478
Training Epoch: 13 [19584/50048]	Loss: 1.2408
Training Epoch: 13 [19712/50048]	Loss: 1.6023
Training Epoch: 13 [19840/50048]	Loss: 1.3582
Training Epoch: 13 [19968/50048]	Loss: 1.2795
Training Epoch: 13 [20096/50048]	Loss: 1.5453
Training Epoch: 13 [20224/50048]	Loss: 1.3397
Training Epoch: 13 [20352/50048]	Loss: 1.1674
Training Epoch: 13 [20480/50048]	Loss: 1.3449
Training Epoch: 13 [20608/50048]	Loss: 1.1489
Training Epoch: 13 [20736/50048]	Loss: 1.4543
Training Epoch: 13 [20864/50048]	Loss: 1.3636
Training Epoch: 13 [20992/50048]	Loss: 1.0806
Training Epoch: 13 [21120/50048]	Loss: 1.5783
Training Epoch: 13 [21248/50048]	Loss: 1.5473
Training Epoch: 13 [21376/50048]	Loss: 1.3571
Training Epoch: 13 [21504/50048]	Loss: 1.4715
Training Epoch: 13 [21632/50048]	Loss: 1.1800
Training Epoch: 13 [21760/50048]	Loss: 1.5954
Training Epoch: 13 [21888/50048]	Loss: 1.2606
Training Epoch: 13 [22016/50048]	Loss: 1.5146
Training Epoch: 13 [22144/50048]	Loss: 1.3959
Training Epoch: 13 [22272/50048]	Loss: 1.3333
Training Epoch: 13 [22400/50048]	Loss: 1.3818
Training Epoch: 13 [22528/50048]	Loss: 1.3638
Training Epoch: 13 [22656/50048]	Loss: 1.3401
Training Epoch: 13 [22784/50048]	Loss: 1.6823
Training Epoch: 13 [22912/50048]	Loss: 1.4069
Training Epoch: 13 [23040/50048]	Loss: 1.4064
Training Epoch: 13 [23168/50048]	Loss: 1.3293
Training Epoch: 13 [23296/50048]	Loss: 1.3476
Training Epoch: 13 [23424/50048]	Loss: 1.4732
Training Epoch: 13 [23552/50048]	Loss: 1.2394
Training Epoch: 13 [23680/50048]	Loss: 0.9647
Training Epoch: 13 [23808/50048]	Loss: 1.2980
Training Epoch: 13 [23936/50048]	Loss: 1.2119
Training Epoch: 13 [24064/50048]	Loss: 1.5771
Training Epoch: 13 [24192/50048]	Loss: 1.3992
Training Epoch: 13 [24320/50048]	Loss: 1.4978
Training Epoch: 13 [24448/50048]	Loss: 1.1863
Training Epoch: 13 [24576/50048]	Loss: 1.3152
Training Epoch: 13 [24704/50048]	Loss: 1.2334
Training Epoch: 13 [24832/50048]	Loss: 1.1306
Training Epoch: 13 [24960/50048]	Loss: 1.2496
Training Epoch: 13 [25088/50048]	Loss: 1.3636
Training Epoch: 13 [25216/50048]	Loss: 1.5288
Training Epoch: 13 [25344/50048]	Loss: 1.5328
Training Epoch: 13 [25472/50048]	Loss: 1.4438
Training Epoch: 13 [25600/50048]	Loss: 1.4207
Training Epoch: 13 [25728/50048]	Loss: 1.5544
Training Epoch: 13 [25856/50048]	Loss: 1.2239
Training Epoch: 13 [25984/50048]	Loss: 1.3072
Training Epoch: 13 [26112/50048]	Loss: 1.4936
Training Epoch: 13 [26240/50048]	Loss: 1.6093
Training Epoch: 13 [26368/50048]	Loss: 1.4723
Training Epoch: 13 [26496/50048]	Loss: 1.7228
Training Epoch: 13 [26624/50048]	Loss: 1.6888
Training Epoch: 13 [26752/50048]	Loss: 1.4171
Training Epoch: 13 [26880/50048]	Loss: 1.1917
Training Epoch: 13 [27008/50048]	Loss: 1.3433
Training Epoch: 13 [27136/50048]	Loss: 1.4664
Training Epoch: 13 [27264/50048]	Loss: 1.4668
Training Epoch: 13 [27392/50048]	Loss: 1.5657
Training Epoch: 13 [27520/50048]	Loss: 1.3313
Training Epoch: 13 [27648/50048]	Loss: 1.1399
Training Epoch: 13 [27776/50048]	Loss: 1.4596
Training Epoch: 13 [27904/50048]	Loss: 1.1773
Training Epoch: 13 [28032/50048]	Loss: 1.2758
Training Epoch: 13 [28160/50048]	Loss: 1.5261
Training Epoch: 13 [28288/50048]	Loss: 1.2265
Training Epoch: 13 [28416/50048]	Loss: 1.3326
Training Epoch: 13 [28544/50048]	Loss: 1.3673
Training Epoch: 13 [28672/50048]	Loss: 1.3149
Training Epoch: 13 [28800/50048]	Loss: 1.4612
Training Epoch: 13 [28928/50048]	Loss: 1.3411
Training Epoch: 13 [29056/50048]	Loss: 1.0581
Training Epoch: 13 [29184/50048]	Loss: 1.3115
Training Epoch: 13 [29312/50048]	Loss: 1.5882
Training Epoch: 13 [29440/50048]	Loss: 1.2835
Training Epoch: 13 [29568/50048]	Loss: 1.1336
Training Epoch: 13 [29696/50048]	Loss: 1.4349
Training Epoch: 13 [29824/50048]	Loss: 1.2451
Training Epoch: 13 [29952/50048]	Loss: 1.3916
Training Epoch: 13 [30080/50048]	Loss: 1.3883
Training Epoch: 13 [30208/50048]	Loss: 1.3343
Training Epoch: 13 [30336/50048]	Loss: 1.3969
Training Epoch: 13 [30464/50048]	Loss: 1.4067
Training Epoch: 13 [30592/50048]	Loss: 1.4867
Training Epoch: 13 [30720/50048]	Loss: 1.3578
Training Epoch: 13 [30848/50048]	Loss: 1.5950
Training Epoch: 13 [30976/50048]	Loss: 1.2578
Training Epoch: 13 [31104/50048]	Loss: 1.3107
Training Epoch: 13 [31232/50048]	Loss: 1.4911
Training Epoch: 13 [31360/50048]	Loss: 1.6105
Training Epoch: 13 [31488/50048]	Loss: 1.3593
Training Epoch: 13 [31616/50048]	Loss: 1.3800
Training Epoch: 13 [31744/50048]	Loss: 1.2498
Training Epoch: 13 [31872/50048]	Loss: 1.2380
Training Epoch: 13 [32000/50048]	Loss: 1.3593
Training Epoch: 13 [32128/50048]	Loss: 1.1991
Training Epoch: 13 [32256/50048]	Loss: 1.3170
Training Epoch: 13 [32384/50048]	Loss: 1.1191
Training Epoch: 13 [32512/50048]	Loss: 1.3368
Training Epoch: 13 [32640/50048]	Loss: 1.4567
Training Epoch: 13 [32768/50048]	Loss: 1.1779
Training Epoch: 13 [32896/50048]	Loss: 1.4777
Training Epoch: 13 [33024/50048]	Loss: 1.5091
Training Epoch: 13 [33152/50048]	Loss: 1.4528
Training Epoch: 13 [33280/50048]	Loss: 1.4346
Training Epoch: 13 [33408/50048]	Loss: 1.1684
Training Epoch: 13 [33536/50048]	Loss: 1.5801
Training Epoch: 13 [33664/50048]	Loss: 1.5160
Training Epoch: 13 [33792/50048]	Loss: 1.4101
Training Epoch: 13 [33920/50048]	Loss: 1.2621
Training Epoch: 13 [34048/50048]	Loss: 1.5530
Training Epoch: 13 [34176/50048]	Loss: 1.5704
Training Epoch: 13 [34304/50048]	Loss: 1.2212
Training Epoch: 13 [34432/50048]	Loss: 1.7254
Training Epoch: 13 [34560/50048]	Loss: 1.6424
Training Epoch: 13 [34688/50048]	Loss: 1.4032
Training Epoch: 13 [34816/50048]	Loss: 1.4783
Training Epoch: 13 [34944/50048]	Loss: 1.4079
Training Epoch: 13 [35072/50048]	Loss: 1.1809
Training Epoch: 13 [35200/50048]	Loss: 1.8235
Training Epoch: 13 [35328/50048]	Loss: 1.3340
Training Epoch: 13 [35456/50048]	Loss: 1.5023
Training Epoch: 13 [35584/50048]	Loss: 1.4101
Training Epoch: 13 [35712/50048]	Loss: 1.4859
Training Epoch: 13 [35840/50048]	Loss: 1.3664
Training Epoch: 13 [35968/50048]	Loss: 1.1440
Training Epoch: 13 [36096/50048]	Loss: 1.3319
Training Epoch: 13 [36224/50048]	Loss: 1.5555
Training Epoch: 13 [36352/50048]	Loss: 1.5407
Training Epoch: 13 [36480/50048]	Loss: 1.4806
Training Epoch: 13 [36608/50048]	Loss: 1.4475
Training Epoch: 13 [36736/50048]	Loss: 1.4656
Training Epoch: 13 [36864/50048]	Loss: 1.5350
Training Epoch: 13 [36992/50048]	Loss: 1.4506
Training Epoch: 13 [37120/50048]	Loss: 1.3911
Training Epoch: 13 [37248/50048]	Loss: 1.5458
Training Epoch: 13 [37376/50048]	Loss: 1.5309
Training Epoch: 13 [37504/50048]	Loss: 1.4412
Training Epoch: 13 [37632/50048]	Loss: 1.2973
Training Epoch: 13 [37760/50048]	Loss: 1.6453
Training Epoch: 13 [37888/50048]	Loss: 1.4810
Training Epoch: 13 [38016/50048]	Loss: 1.3025
Training Epoch: 13 [38144/50048]	Loss: 1.2469
Training Epoch: 13 [38272/50048]	Loss: 1.2127
Training Epoch: 13 [38400/50048]	Loss: 1.4229
Training Epoch: 13 [38528/50048]	Loss: 1.3299
Training Epoch: 13 [38656/50048]	Loss: 1.3938
Training Epoch: 13 [38784/50048]	Loss: 1.3919
Training Epoch: 13 [38912/50048]	Loss: 1.1619
Training Epoch: 13 [39040/50048]	Loss: 1.4510
Training Epoch: 13 [39168/50048]	Loss: 1.2929
Training Epoch: 13 [39296/50048]	Loss: 1.2562
Training Epoch: 13 [39424/50048]	Loss: 1.3589
Training Epoch: 13 [39552/50048]	Loss: 1.4951
Training Epoch: 13 [39680/50048]	Loss: 1.5280
Training Epoch: 13 [39808/50048]	Loss: 1.5551
Training Epoch: 13 [39936/50048]	Loss: 1.4121
Training Epoch: 13 [40064/50048]	Loss: 1.4049
Training Epoch: 13 [40192/50048]	Loss: 1.3588
Training Epoch: 13 [40320/50048]	Loss: 1.3562
Training Epoch: 13 [40448/50048]	Loss: 1.3578
Training Epoch: 13 [40576/50048]	Loss: 1.4162
Training Epoch: 13 [40704/50048]	Loss: 1.1682
Training Epoch: 13 [40832/50048]	Loss: 1.3777
Training Epoch: 13 [40960/50048]	Loss: 1.5380
Training Epoch: 13 [41088/50048]	Loss: 1.3608
Training Epoch: 13 [41216/50048]	Loss: 1.4297
Training Epoch: 13 [41344/50048]	Loss: 1.2968
Training Epoch: 13 [41472/50048]	Loss: 1.3200
Training Epoch: 13 [41600/50048]	Loss: 1.4824
Training Epoch: 13 [41728/50048]	Loss: 1.3150
Training Epoch: 13 [41856/50048]	Loss: 1.3943
Training Epoch: 13 [41984/50048]	Loss: 1.5789
Training Epoch: 13 [42112/50048]	Loss: 1.2540
Training Epoch: 13 [42240/50048]	Loss: 1.3290
Training Epoch: 13 [42368/50048]	Loss: 1.5291
Training Epoch: 13 [42496/50048]	Loss: 1.2958
Training Epoch: 13 [42624/50048]	Loss: 1.1178
Training Epoch: 13 [42752/50048]	Loss: 1.5439
Training Epoch: 13 [42880/50048]	Loss: 1.2518
Training Epoch: 13 [43008/50048]	Loss: 1.2337
Training Epoch: 13 [43136/50048]	Loss: 1.5331
Training Epoch: 13 [43264/50048]	Loss: 1.4210
Training Epoch: 13 [43392/50048]	Loss: 1.3955
Training Epoch: 13 [43520/50048]	Loss: 1.4450
Training Epoch: 13 [43648/50048]	Loss: 1.4637
Training Epoch: 13 [43776/50048]	Loss: 1.2443
Training Epoch: 13 [43904/50048]	Loss: 1.5263
Training Epoch: 13 [44032/50048]	Loss: 1.2791
Training Epoch: 13 [44160/50048]	Loss: 1.3566
Training Epoch: 13 [44288/50048]	Loss: 1.4022
Training Epoch: 13 [44416/50048]	Loss: 1.2989
Training Epoch: 13 [44544/50048]	Loss: 1.5011
Training Epoch: 13 [44672/50048]	Loss: 1.3058
Training Epoch: 13 [44800/50048]	Loss: 1.2286
Training Epoch: 13 [44928/50048]	Loss: 1.3808
Training Epoch: 13 [45056/50048]	Loss: 1.3457
Training Epoch: 13 [45184/50048]	Loss: 1.3936
Training Epoch: 13 [45312/50048]	Loss: 1.3202
Training Epoch: 13 [45440/50048]	Loss: 1.6496
Training Epoch: 13 [45568/50048]	Loss: 1.2945
Training Epoch: 13 [45696/50048]	Loss: 1.2519
2022-12-06 06:28:53,150 [ZeusDataLoader(train)] train epoch 14 done: time=86.45 energy=10496.31
2022-12-06 06:28:53,151 [ZeusDataLoader(eval)] Epoch 14 begin.
Training Epoch: 13 [45824/50048]	Loss: 1.1332
Training Epoch: 13 [45952/50048]	Loss: 1.4086
Training Epoch: 13 [46080/50048]	Loss: 1.1609
Training Epoch: 13 [46208/50048]	Loss: 1.1544
Training Epoch: 13 [46336/50048]	Loss: 1.3787
Training Epoch: 13 [46464/50048]	Loss: 1.4326
Training Epoch: 13 [46592/50048]	Loss: 1.2843
Training Epoch: 13 [46720/50048]	Loss: 1.3074
Training Epoch: 13 [46848/50048]	Loss: 1.4843
Training Epoch: 13 [46976/50048]	Loss: 1.5473
Training Epoch: 13 [47104/50048]	Loss: 1.4161
Training Epoch: 13 [47232/50048]	Loss: 1.3165
Training Epoch: 13 [47360/50048]	Loss: 1.7357
Training Epoch: 13 [47488/50048]	Loss: 1.4871
Training Epoch: 13 [47616/50048]	Loss: 1.4217
Training Epoch: 13 [47744/50048]	Loss: 1.4207
Training Epoch: 13 [47872/50048]	Loss: 1.2389
Training Epoch: 13 [48000/50048]	Loss: 1.4956
Training Epoch: 13 [48128/50048]	Loss: 1.3842
Training Epoch: 13 [48256/50048]	Loss: 1.3379
Training Epoch: 13 [48384/50048]	Loss: 1.1172
Training Epoch: 13 [48512/50048]	Loss: 1.5762
Training Epoch: 13 [48640/50048]	Loss: 1.3915
Training Epoch: 13 [48768/50048]	Loss: 1.4965
Training Epoch: 13 [48896/50048]	Loss: 1.2748
Training Epoch: 13 [49024/50048]	Loss: 1.3210
Training Epoch: 13 [49152/50048]	Loss: 1.3074
Training Epoch: 13 [49280/50048]	Loss: 1.4513
Training Epoch: 13 [49408/50048]	Loss: 1.4847
Training Epoch: 13 [49536/50048]	Loss: 1.5871
Training Epoch: 13 [49664/50048]	Loss: 1.7144
Training Epoch: 13 [49792/50048]	Loss: 1.6114
Training Epoch: 13 [49920/50048]	Loss: 1.4941
Training Epoch: 13 [50048/50048]	Loss: 1.5031
2022-12-06 11:28:56.902 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:28:56,937 [ZeusDataLoader(eval)] eval epoch 14 done: time=3.78 energy=451.56
2022-12-06 06:28:56,937 [ZeusDataLoader(train)] Up to epoch 14: time=1263.02, energy=153288.63, cost=187158.85
2022-12-06 06:28:56,937 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:28:56,937 [ZeusDataLoader(train)] Expected next epoch: time=1352.82, energy=164086.64, cost=200415.23
2022-12-06 06:28:56,938 [ZeusDataLoader(train)] Epoch 15 begin.
Validation Epoch: 13, Average loss: 0.0125, Accuracy: 0.5649
2022-12-06 06:28:57,117 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:28:57,118 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:28:57.122 [ZeusMonitor] Monitor started.
2022-12-06 11:28:57.122 [ZeusMonitor] Running indefinitely. 2022-12-06 11:28:57.122 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:28:57.122 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e15+gpu0.power.log
Training Epoch: 14 [128/50048]	Loss: 1.2086
Training Epoch: 14 [256/50048]	Loss: 1.3214
Training Epoch: 14 [384/50048]	Loss: 1.3901
Training Epoch: 14 [512/50048]	Loss: 1.0962
Training Epoch: 14 [640/50048]	Loss: 1.3294
Training Epoch: 14 [768/50048]	Loss: 1.3414
Training Epoch: 14 [896/50048]	Loss: 1.1847
Training Epoch: 14 [1024/50048]	Loss: 1.3104
Training Epoch: 14 [1152/50048]	Loss: 1.1662
Training Epoch: 14 [1280/50048]	Loss: 1.5925
Training Epoch: 14 [1408/50048]	Loss: 1.2441
Training Epoch: 14 [1536/50048]	Loss: 1.0864
Training Epoch: 14 [1664/50048]	Loss: 1.2221
Training Epoch: 14 [1792/50048]	Loss: 1.3792
Training Epoch: 14 [1920/50048]	Loss: 1.4393
Training Epoch: 14 [2048/50048]	Loss: 1.0695
Training Epoch: 14 [2176/50048]	Loss: 1.0905
Training Epoch: 14 [2304/50048]	Loss: 1.3840
Training Epoch: 14 [2432/50048]	Loss: 1.5800
Training Epoch: 14 [2560/50048]	Loss: 1.4654
Training Epoch: 14 [2688/50048]	Loss: 1.1978
Training Epoch: 14 [2816/50048]	Loss: 1.2192
Training Epoch: 14 [2944/50048]	Loss: 1.1885
Training Epoch: 14 [3072/50048]	Loss: 1.3182
Training Epoch: 14 [3200/50048]	Loss: 1.2939
Training Epoch: 14 [3328/50048]	Loss: 1.5788
Training Epoch: 14 [3456/50048]	Loss: 1.2885
Training Epoch: 14 [3584/50048]	Loss: 1.6340
Training Epoch: 14 [3712/50048]	Loss: 1.4202
Training Epoch: 14 [3840/50048]	Loss: 1.3011
Training Epoch: 14 [3968/50048]	Loss: 1.6614
Training Epoch: 14 [4096/50048]	Loss: 1.1067
Training Epoch: 14 [4224/50048]	Loss: 1.4825
Training Epoch: 14 [4352/50048]	Loss: 1.0190
Training Epoch: 14 [4480/50048]	Loss: 1.3050
Training Epoch: 14 [4608/50048]	Loss: 1.2698
Training Epoch: 14 [4736/50048]	Loss: 1.1747
Training Epoch: 14 [4864/50048]	Loss: 1.2212
Training Epoch: 14 [4992/50048]	Loss: 1.2909
Training Epoch: 14 [5120/50048]	Loss: 1.1191
Training Epoch: 14 [5248/50048]	Loss: 1.2713
Training Epoch: 14 [5376/50048]	Loss: 1.4299
Training Epoch: 14 [5504/50048]	Loss: 1.0508
Training Epoch: 14 [5632/50048]	Loss: 1.5650
Training Epoch: 14 [5760/50048]	Loss: 1.2641
Training Epoch: 14 [5888/50048]	Loss: 1.2797
Training Epoch: 14 [6016/50048]	Loss: 1.0796
Training Epoch: 14 [6144/50048]	Loss: 1.2143
Training Epoch: 14 [6272/50048]	Loss: 1.0221
Training Epoch: 14 [6400/50048]	Loss: 1.0745
Training Epoch: 14 [6528/50048]	Loss: 1.2898
Training Epoch: 14 [6656/50048]	Loss: 1.1999
Training Epoch: 14 [6784/50048]	Loss: 1.2693
Training Epoch: 14 [6912/50048]	Loss: 1.3900
Training Epoch: 14 [7040/50048]	Loss: 1.3410
Training Epoch: 14 [7168/50048]	Loss: 1.3018
Training Epoch: 14 [7296/50048]	Loss: 1.2001
Training Epoch: 14 [7424/50048]	Loss: 1.6017
Training Epoch: 14 [7552/50048]	Loss: 1.5198
Training Epoch: 14 [7680/50048]	Loss: 1.3715
Training Epoch: 14 [7808/50048]	Loss: 1.3357
Training Epoch: 14 [7936/50048]	Loss: 1.0801
Training Epoch: 14 [8064/50048]	Loss: 1.1404
Training Epoch: 14 [8192/50048]	Loss: 1.2933
Training Epoch: 14 [8320/50048]	Loss: 1.3730
Training Epoch: 14 [8448/50048]	Loss: 1.1156
Training Epoch: 14 [8576/50048]	Loss: 1.2095
Training Epoch: 14 [8704/50048]	Loss: 1.1244
Training Epoch: 14 [8832/50048]	Loss: 1.1801
Training Epoch: 14 [8960/50048]	Loss: 1.2623
Training Epoch: 14 [9088/50048]	Loss: 1.2289
Training Epoch: 14 [9216/50048]	Loss: 1.0272
Training Epoch: 14 [9344/50048]	Loss: 1.3835
Training Epoch: 14 [9472/50048]	Loss: 1.2952
Training Epoch: 14 [9600/50048]	Loss: 1.1277
Training Epoch: 14 [9728/50048]	Loss: 1.0838
Training Epoch: 14 [9856/50048]	Loss: 1.2673
Training Epoch: 14 [9984/50048]	Loss: 1.3329
Training Epoch: 14 [10112/50048]	Loss: 1.2596
Training Epoch: 14 [10240/50048]	Loss: 1.2854
Training Epoch: 14 [10368/50048]	Loss: 1.0352
Training Epoch: 14 [10496/50048]	Loss: 1.3903
Training Epoch: 14 [10624/50048]	Loss: 1.5215
Training Epoch: 14 [10752/50048]	Loss: 1.0952
Training Epoch: 14 [10880/50048]	Loss: 1.2782
Training Epoch: 14 [11008/50048]	Loss: 1.2451
Training Epoch: 14 [11136/50048]	Loss: 1.2261
Training Epoch: 14 [11264/50048]	Loss: 1.4323
Training Epoch: 14 [11392/50048]	Loss: 1.1812
Training Epoch: 14 [11520/50048]	Loss: 1.2571
Training Epoch: 14 [11648/50048]	Loss: 1.6675
Training Epoch: 14 [11776/50048]	Loss: 1.0444
Training Epoch: 14 [11904/50048]	Loss: 1.3593
Training Epoch: 14 [12032/50048]	Loss: 1.2615
Training Epoch: 14 [12160/50048]	Loss: 1.1679
Training Epoch: 14 [12288/50048]	Loss: 1.2126
Training Epoch: 14 [12416/50048]	Loss: 1.4440
Training Epoch: 14 [12544/50048]	Loss: 1.2934
Training Epoch: 14 [12672/50048]	Loss: 1.4349
Training Epoch: 14 [12800/50048]	Loss: 1.0736
Training Epoch: 14 [12928/50048]	Loss: 1.3831
Training Epoch: 14 [13056/50048]	Loss: 1.4966
Training Epoch: 14 [13184/50048]	Loss: 1.3351
Training Epoch: 14 [13312/50048]	Loss: 1.2056
Training Epoch: 14 [13440/50048]	Loss: 1.4263
Training Epoch: 14 [13568/50048]	Loss: 1.2137
Training Epoch: 14 [13696/50048]	Loss: 1.0195
Training Epoch: 14 [13824/50048]	Loss: 1.3688
Training Epoch: 14 [13952/50048]	Loss: 1.2080
Training Epoch: 14 [14080/50048]	Loss: 1.4696
Training Epoch: 14 [14208/50048]	Loss: 1.3866
Training Epoch: 14 [14336/50048]	Loss: 1.3671
Training Epoch: 14 [14464/50048]	Loss: 1.3597
Training Epoch: 14 [14592/50048]	Loss: 1.3081
Training Epoch: 14 [14720/50048]	Loss: 1.4258
Training Epoch: 14 [14848/50048]	Loss: 0.9980
Training Epoch: 14 [14976/50048]	Loss: 1.3121
Training Epoch: 14 [15104/50048]	Loss: 1.6291
Training Epoch: 14 [15232/50048]	Loss: 1.2816
Training Epoch: 14 [15360/50048]	Loss: 1.2147
Training Epoch: 14 [15488/50048]	Loss: 0.8971
Training Epoch: 14 [15616/50048]	Loss: 1.1391
Training Epoch: 14 [15744/50048]	Loss: 1.2811
Training Epoch: 14 [15872/50048]	Loss: 1.4808
Training Epoch: 14 [16000/50048]	Loss: 1.2416
Training Epoch: 14 [16128/50048]	Loss: 1.0735
Training Epoch: 14 [16256/50048]	Loss: 1.4302
Training Epoch: 14 [16384/50048]	Loss: 1.4895
Training Epoch: 14 [16512/50048]	Loss: 1.2715
Training Epoch: 14 [16640/50048]	Loss: 1.4657
Training Epoch: 14 [16768/50048]	Loss: 1.2621
Training Epoch: 14 [16896/50048]	Loss: 1.4815
Training Epoch: 14 [17024/50048]	Loss: 1.2376
Training Epoch: 14 [17152/50048]	Loss: 1.2867
Training Epoch: 14 [17280/50048]	Loss: 1.3300
Training Epoch: 14 [17408/50048]	Loss: 1.5682
Training Epoch: 14 [17536/50048]	Loss: 1.2451
Training Epoch: 14 [17664/50048]	Loss: 1.3423
Training Epoch: 14 [17792/50048]	Loss: 1.0893
Training Epoch: 14 [17920/50048]	Loss: 1.3569
Training Epoch: 14 [18048/50048]	Loss: 1.2547
Training Epoch: 14 [18176/50048]	Loss: 1.4693
Training Epoch: 14 [18304/50048]	Loss: 1.3963
Training Epoch: 14 [18432/50048]	Loss: 1.2758
Training Epoch: 14 [18560/50048]	Loss: 1.3977
Training Epoch: 14 [18688/50048]	Loss: 1.1602
Training Epoch: 14 [18816/50048]	Loss: 1.1912
Training Epoch: 14 [18944/50048]	Loss: 1.3827
Training Epoch: 14 [19072/50048]	Loss: 1.3312
Training Epoch: 14 [19200/50048]	Loss: 1.4821
Training Epoch: 14 [19328/50048]	Loss: 1.3329
Training Epoch: 14 [19456/50048]	Loss: 1.2847
Training Epoch: 14 [19584/50048]	Loss: 1.3888
Training Epoch: 14 [19712/50048]	Loss: 1.5053
Training Epoch: 14 [19840/50048]	Loss: 1.1552
Training Epoch: 14 [19968/50048]	Loss: 1.1700
Training Epoch: 14 [20096/50048]	Loss: 1.3113
Training Epoch: 14 [20224/50048]	Loss: 1.1609
Training Epoch: 14 [20352/50048]	Loss: 1.0479
Training Epoch: 14 [20480/50048]	Loss: 1.1315
Training Epoch: 14 [20608/50048]	Loss: 1.3208
Training Epoch: 14 [20736/50048]	Loss: 1.0723
Training Epoch: 14 [20864/50048]	Loss: 1.3742
Training Epoch: 14 [20992/50048]	Loss: 1.6182
Training Epoch: 14 [21120/50048]	Loss: 1.2169
Training Epoch: 14 [21248/50048]	Loss: 1.1180
Training Epoch: 14 [21376/50048]	Loss: 1.2147
Training Epoch: 14 [21504/50048]	Loss: 1.3404
Training Epoch: 14 [21632/50048]	Loss: 1.2612
Training Epoch: 14 [21760/50048]	Loss: 1.2540
Training Epoch: 14 [21888/50048]	Loss: 1.7297
Training Epoch: 14 [22016/50048]	Loss: 1.3497
Training Epoch: 14 [22144/50048]	Loss: 1.2492
Training Epoch: 14 [22272/50048]	Loss: 1.3563
Training Epoch: 14 [22400/50048]	Loss: 1.4793
Training Epoch: 14 [22528/50048]	Loss: 1.2224
Training Epoch: 14 [22656/50048]	Loss: 1.4027
Training Epoch: 14 [22784/50048]	Loss: 1.2791
Training Epoch: 14 [22912/50048]	Loss: 1.2022
Training Epoch: 14 [23040/50048]	Loss: 1.2678
Training Epoch: 14 [23168/50048]	Loss: 1.5209
Training Epoch: 14 [23296/50048]	Loss: 1.3121
Training Epoch: 14 [23424/50048]	Loss: 1.4506
Training Epoch: 14 [23552/50048]	Loss: 1.3774
Training Epoch: 14 [23680/50048]	Loss: 1.3124
Training Epoch: 14 [23808/50048]	Loss: 1.3803
Training Epoch: 14 [23936/50048]	Loss: 1.1714
Training Epoch: 14 [24064/50048]	Loss: 1.0624
Training Epoch: 14 [24192/50048]	Loss: 1.2056
Training Epoch: 14 [24320/50048]	Loss: 1.4645
Training Epoch: 14 [24448/50048]	Loss: 1.2414
Training Epoch: 14 [24576/50048]	Loss: 1.4351
Training Epoch: 14 [24704/50048]	Loss: 1.5162
Training Epoch: 14 [24832/50048]	Loss: 1.1729
Training Epoch: 14 [24960/50048]	Loss: 1.4631
Training Epoch: 14 [25088/50048]	Loss: 1.5100
Training Epoch: 14 [25216/50048]	Loss: 1.3457
Training Epoch: 14 [25344/50048]	Loss: 1.1936
Training Epoch: 14 [25472/50048]	Loss: 1.1819
Training Epoch: 14 [25600/50048]	Loss: 1.2782
Training Epoch: 14 [25728/50048]	Loss: 1.4102
Training Epoch: 14 [25856/50048]	Loss: 1.4185
Training Epoch: 14 [25984/50048]	Loss: 1.3305
Training Epoch: 14 [26112/50048]	Loss: 1.2854
Training Epoch: 14 [26240/50048]	Loss: 1.2738
Training Epoch: 14 [26368/50048]	Loss: 1.1605
Training Epoch: 14 [26496/50048]	Loss: 1.0340
Training Epoch: 14 [26624/50048]	Loss: 1.5517
Training Epoch: 14 [26752/50048]	Loss: 1.5244
Training Epoch: 14 [26880/50048]	Loss: 1.2971
Training Epoch: 14 [27008/50048]	Loss: 1.3478
Training Epoch: 14 [27136/50048]	Loss: 1.1855
Training Epoch: 14 [27264/50048]	Loss: 1.0172
Training Epoch: 14 [27392/50048]	Loss: 1.0324
Training Epoch: 14 [27520/50048]	Loss: 1.4155
Training Epoch: 14 [27648/50048]	Loss: 1.1175
Training Epoch: 14 [27776/50048]	Loss: 1.4077
Training Epoch: 14 [27904/50048]	Loss: 1.0927
Training Epoch: 14 [28032/50048]	Loss: 1.3797
Training Epoch: 14 [28160/50048]	Loss: 1.6437
Training Epoch: 14 [28288/50048]	Loss: 1.2945
Training Epoch: 14 [28416/50048]	Loss: 1.3213
Training Epoch: 14 [28544/50048]	Loss: 1.5053
Training Epoch: 14 [28672/50048]	Loss: 1.1464
Training Epoch: 14 [28800/50048]	Loss: 1.3976
Training Epoch: 14 [28928/50048]	Loss: 1.0900
Training Epoch: 14 [29056/50048]	Loss: 1.2403
Training Epoch: 14 [29184/50048]	Loss: 1.1375
Training Epoch: 14 [29312/50048]	Loss: 1.3497
Training Epoch: 14 [29440/50048]	Loss: 1.4015
Training Epoch: 14 [29568/50048]	Loss: 1.6670
Training Epoch: 14 [29696/50048]	Loss: 1.2202
Training Epoch: 14 [29824/50048]	Loss: 1.2792
Training Epoch: 14 [29952/50048]	Loss: 1.1964
Training Epoch: 14 [30080/50048]	Loss: 1.2244
Training Epoch: 14 [30208/50048]	Loss: 1.1623
Training Epoch: 14 [30336/50048]	Loss: 1.3122
Training Epoch: 14 [30464/50048]	Loss: 1.3645
Training Epoch: 14 [30592/50048]	Loss: 1.4634
Training Epoch: 14 [30720/50048]	Loss: 1.4612
Training Epoch: 14 [30848/50048]	Loss: 1.1728
Training Epoch: 14 [30976/50048]	Loss: 1.2963
Training Epoch: 14 [31104/50048]	Loss: 1.3228
Training Epoch: 14 [31232/50048]	Loss: 1.3153
Training Epoch: 14 [31360/50048]	Loss: 1.4185
Training Epoch: 14 [31488/50048]	Loss: 1.3559
Training Epoch: 14 [31616/50048]	Loss: 1.3655
Training Epoch: 14 [31744/50048]	Loss: 1.2298
Training Epoch: 14 [31872/50048]	Loss: 1.3021
Training Epoch: 14 [32000/50048]	Loss: 1.3023
Training Epoch: 14 [32128/50048]	Loss: 1.4306
Training Epoch: 14 [32256/50048]	Loss: 1.4449
Training Epoch: 14 [32384/50048]	Loss: 1.3716
Training Epoch: 14 [32512/50048]	Loss: 1.3544
Training Epoch: 14 [32640/50048]	Loss: 1.2355
Training Epoch: 14 [32768/50048]	Loss: 1.5833
Training Epoch: 14 [32896/50048]	Loss: 1.2809
Training Epoch: 14 [33024/50048]	Loss: 1.3681
Training Epoch: 14 [33152/50048]	Loss: 1.5469
Training Epoch: 14 [33280/50048]	Loss: 1.4608
Training Epoch: 14 [33408/50048]	Loss: 1.3433
Training Epoch: 14 [33536/50048]	Loss: 1.4120
Training Epoch: 14 [33664/50048]	Loss: 1.3354
Training Epoch: 14 [33792/50048]	Loss: 1.3120
Training Epoch: 14 [33920/50048]	Loss: 1.3449
Training Epoch: 14 [34048/50048]	Loss: 1.2072
Training Epoch: 14 [34176/50048]	Loss: 1.2356
Training Epoch: 14 [34304/50048]	Loss: 1.3189
Training Epoch: 14 [34432/50048]	Loss: 1.5793
Training Epoch: 14 [34560/50048]	Loss: 1.1938
Training Epoch: 14 [34688/50048]	Loss: 1.2590
Training Epoch: 14 [34816/50048]	Loss: 1.4793
Training Epoch: 14 [34944/50048]	Loss: 1.4324
Training Epoch: 14 [35072/50048]	Loss: 1.3093
Training Epoch: 14 [35200/50048]	Loss: 1.2808
Training Epoch: 14 [35328/50048]	Loss: 1.2692
Training Epoch: 14 [35456/50048]	Loss: 1.4897
Training Epoch: 14 [35584/50048]	Loss: 1.2257
Training Epoch: 14 [35712/50048]	Loss: 1.0840
Training Epoch: 14 [35840/50048]	Loss: 1.3275
Training Epoch: 14 [35968/50048]	Loss: 1.3164
Training Epoch: 14 [36096/50048]	Loss: 1.4289
Training Epoch: 14 [36224/50048]	Loss: 1.2479
Training Epoch: 14 [36352/50048]	Loss: 1.4140
Training Epoch: 14 [36480/50048]	Loss: 1.4991
Training Epoch: 14 [36608/50048]	Loss: 1.6015
Training Epoch: 14 [36736/50048]	Loss: 1.1335
Training Epoch: 14 [36864/50048]	Loss: 1.4308
Training Epoch: 14 [36992/50048]	Loss: 1.3277
Training Epoch: 14 [37120/50048]	Loss: 1.2507
Training Epoch: 14 [37248/50048]	Loss: 1.5385
Training Epoch: 14 [37376/50048]	Loss: 1.3816
Training Epoch: 14 [37504/50048]	Loss: 1.2524
Training Epoch: 14 [37632/50048]	Loss: 1.1300
Training Epoch: 14 [37760/50048]	Loss: 1.1788
Training Epoch: 14 [37888/50048]	Loss: 1.1733
Training Epoch: 14 [38016/50048]	Loss: 1.3916
Training Epoch: 14 [38144/50048]	Loss: 1.3173
Training Epoch: 14 [38272/50048]	Loss: 1.2494
Training Epoch: 14 [38400/50048]	Loss: 1.3470
Training Epoch: 14 [38528/50048]	Loss: 1.3250
Training Epoch: 14 [38656/50048]	Loss: 1.2702
Training Epoch: 14 [38784/50048]	Loss: 1.2563
Training Epoch: 14 [38912/50048]	Loss: 1.3828
Training Epoch: 14 [39040/50048]	Loss: 1.1476
Training Epoch: 14 [39168/50048]	Loss: 1.2010
Training Epoch: 14 [39296/50048]	Loss: 1.2745
Training Epoch: 14 [39424/50048]	Loss: 1.1845
Training Epoch: 14 [39552/50048]	Loss: 1.4558
Training Epoch: 14 [39680/50048]	Loss: 1.3912
Training Epoch: 14 [39808/50048]	Loss: 1.2513
Training Epoch: 14 [39936/50048]	Loss: 1.3793
Training Epoch: 14 [40064/50048]	Loss: 1.2931
Training Epoch: 14 [40192/50048]	Loss: 1.3022
Training Epoch: 14 [40320/50048]	Loss: 1.3559
Training Epoch: 14 [40448/50048]	Loss: 1.3274
Training Epoch: 14 [40576/50048]	Loss: 1.4369
Training Epoch: 14 [40704/50048]	Loss: 1.2406
Training Epoch: 14 [40832/50048]	Loss: 1.4183
Training Epoch: 14 [40960/50048]	Loss: 1.3085
Training Epoch: 14 [41088/50048]	Loss: 1.1217
Training Epoch: 14 [41216/50048]	Loss: 1.3973
Training Epoch: 14 [41344/50048]	Loss: 1.2981
Training Epoch: 14 [41472/50048]	Loss: 1.5154
Training Epoch: 14 [41600/50048]	Loss: 1.2613
Training Epoch: 14 [41728/50048]	Loss: 1.4021
Training Epoch: 14 [41856/50048]	Loss: 1.5387
Training Epoch: 14 [41984/50048]	Loss: 1.6100
Training Epoch: 14 [42112/50048]	Loss: 1.2923
Training Epoch: 14 [42240/50048]	Loss: 1.3892
Training Epoch: 14 [42368/50048]	Loss: 1.3959
Training Epoch: 14 [42496/50048]	Loss: 1.4395
Training Epoch: 14 [42624/50048]	Loss: 1.2879
Training Epoch: 14 [42752/50048]	Loss: 1.3215
Training Epoch: 14 [42880/50048]	Loss: 1.3088
Training Epoch: 14 [43008/50048]	Loss: 1.3054
Training Epoch: 14 [43136/50048]	Loss: 1.3842
Training Epoch: 14 [43264/50048]	Loss: 1.2556
Training Epoch: 14 [43392/50048]	Loss: 1.2141
Training Epoch: 14 [43520/50048]	Loss: 1.3568
Training Epoch: 14 [43648/50048]	Loss: 1.4074
Training Epoch: 14 [43776/50048]	Loss: 1.4231
Training Epoch: 14 [43904/50048]	Loss: 1.5785
Training Epoch: 14 [44032/50048]	Loss: 1.2106
Training Epoch: 14 [44160/50048]	Loss: 1.4215
Training Epoch: 14 [44288/50048]	Loss: 1.4333
Training Epoch: 14 [44416/50048]	Loss: 1.1444
Training Epoch: 14 [44544/50048]	Loss: 1.4321
Training Epoch: 14 [44672/50048]	Loss: 1.2698
Training Epoch: 14 [44800/50048]	Loss: 1.3917
Training Epoch: 14 [44928/50048]	Loss: 1.1667
Training Epoch: 14 [45056/50048]	Loss: 1.4240
Training Epoch: 14 [45184/50048]	Loss: 1.3216
Training Epoch: 14 [45312/50048]	Loss: 1.3198
Training Epoch: 14 [45440/50048]	Loss: 1.5638
Training Epoch: 14 [45568/50048]	Loss: 1.3279
Training Epoch: 14 [45696/50048]	Loss: 1.1250
2022-12-06 06:30:23,463 [ZeusDataLoader(train)] train epoch 15 done: time=86.51 energy=10501.93
2022-12-06 06:30:23,464 [ZeusDataLoader(eval)] Epoch 15 begin.
Training Epoch: 14 [45824/50048]	Loss: 1.3111
Training Epoch: 14 [45952/50048]	Loss: 1.4681
Training Epoch: 14 [46080/50048]	Loss: 1.1271
Training Epoch: 14 [46208/50048]	Loss: 1.5081
Training Epoch: 14 [46336/50048]	Loss: 1.2834
Training Epoch: 14 [46464/50048]	Loss: 1.3643
Training Epoch: 14 [46592/50048]	Loss: 1.1819
Training Epoch: 14 [46720/50048]	Loss: 1.2681
Training Epoch: 14 [46848/50048]	Loss: 1.6036
Training Epoch: 14 [46976/50048]	Loss: 1.2949
Training Epoch: 14 [47104/50048]	Loss: 1.2094
Training Epoch: 14 [47232/50048]	Loss: 1.3324
Training Epoch: 14 [47360/50048]	Loss: 1.2811
Training Epoch: 14 [47488/50048]	Loss: 1.2814
Training Epoch: 14 [47616/50048]	Loss: 1.5184
Training Epoch: 14 [47744/50048]	Loss: 1.4241
Training Epoch: 14 [47872/50048]	Loss: 1.5085
Training Epoch: 14 [48000/50048]	Loss: 1.1071
Training Epoch: 14 [48128/50048]	Loss: 1.1956
Training Epoch: 14 [48256/50048]	Loss: 1.4647
Training Epoch: 14 [48384/50048]	Loss: 1.5413
Training Epoch: 14 [48512/50048]	Loss: 1.1562
Training Epoch: 14 [48640/50048]	Loss: 1.1980
Training Epoch: 14 [48768/50048]	Loss: 1.3319
Training Epoch: 14 [48896/50048]	Loss: 1.2657
Training Epoch: 14 [49024/50048]	Loss: 1.2445
Training Epoch: 14 [49152/50048]	Loss: 1.5966
Training Epoch: 14 [49280/50048]	Loss: 1.1935
Training Epoch: 14 [49408/50048]	Loss: 1.4508
Training Epoch: 14 [49536/50048]	Loss: 1.2530
Training Epoch: 14 [49664/50048]	Loss: 1.5751
Training Epoch: 14 [49792/50048]	Loss: 1.2105
Training Epoch: 14 [49920/50048]	Loss: 1.2370
Training Epoch: 14 [50048/50048]	Loss: 1.5998
2022-12-06 11:30:27.193 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:30:27,222 [ZeusDataLoader(eval)] eval epoch 15 done: time=3.75 energy=454.18
2022-12-06 06:30:27,223 [ZeusDataLoader(train)] Up to epoch 15: time=1353.29, energy=164244.74, cost=200534.92
2022-12-06 06:30:27,223 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:30:27,223 [ZeusDataLoader(train)] Expected next epoch: time=1443.08, energy=175042.75, cost=213791.30
2022-12-06 06:30:27,224 [ZeusDataLoader(train)] Epoch 16 begin.
Validation Epoch: 14, Average loss: 0.0128, Accuracy: 0.5565
2022-12-06 06:30:27,408 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:30:27,409 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:30:27.411 [ZeusMonitor] Monitor started.
2022-12-06 11:30:27.411 [ZeusMonitor] Running indefinitely. 2022-12-06 11:30:27.411 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:30:27.411 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e16+gpu0.power.log
Training Epoch: 15 [128/50048]	Loss: 1.3582
Training Epoch: 15 [256/50048]	Loss: 1.2926
Training Epoch: 15 [384/50048]	Loss: 1.3928
Training Epoch: 15 [512/50048]	Loss: 1.3782
Training Epoch: 15 [640/50048]	Loss: 1.2001
Training Epoch: 15 [768/50048]	Loss: 1.1541
Training Epoch: 15 [896/50048]	Loss: 1.0278
Training Epoch: 15 [1024/50048]	Loss: 0.9980
Training Epoch: 15 [1152/50048]	Loss: 1.3387
Training Epoch: 15 [1280/50048]	Loss: 1.2581
Training Epoch: 15 [1408/50048]	Loss: 1.3211
Training Epoch: 15 [1536/50048]	Loss: 1.2174
Training Epoch: 15 [1664/50048]	Loss: 1.0284
Training Epoch: 15 [1792/50048]	Loss: 1.2144
Training Epoch: 15 [1920/50048]	Loss: 1.0087
Training Epoch: 15 [2048/50048]	Loss: 1.5267
Training Epoch: 15 [2176/50048]	Loss: 1.4515
Training Epoch: 15 [2304/50048]	Loss: 1.2063
Training Epoch: 15 [2432/50048]	Loss: 1.4069
Training Epoch: 15 [2560/50048]	Loss: 1.2572
Training Epoch: 15 [2688/50048]	Loss: 1.2237
Training Epoch: 15 [2816/50048]	Loss: 1.2105
Training Epoch: 15 [2944/50048]	Loss: 1.0985
Training Epoch: 15 [3072/50048]	Loss: 0.9944
Training Epoch: 15 [3200/50048]	Loss: 1.3318
Training Epoch: 15 [3328/50048]	Loss: 1.2635
Training Epoch: 15 [3456/50048]	Loss: 1.3452
Training Epoch: 15 [3584/50048]	Loss: 1.0095
Training Epoch: 15 [3712/50048]	Loss: 1.2771
Training Epoch: 15 [3840/50048]	Loss: 1.4477
Training Epoch: 15 [3968/50048]	Loss: 1.3757
Training Epoch: 15 [4096/50048]	Loss: 1.1476
Training Epoch: 15 [4224/50048]	Loss: 1.0017
Training Epoch: 15 [4352/50048]	Loss: 1.1764
Training Epoch: 15 [4480/50048]	Loss: 1.1465
Training Epoch: 15 [4608/50048]	Loss: 1.3460
Training Epoch: 15 [4736/50048]	Loss: 1.0155
Training Epoch: 15 [4864/50048]	Loss: 1.2410
Training Epoch: 15 [4992/50048]	Loss: 1.3590
Training Epoch: 15 [5120/50048]	Loss: 1.4584
Training Epoch: 15 [5248/50048]	Loss: 1.0265
Training Epoch: 15 [5376/50048]	Loss: 1.6456
Training Epoch: 15 [5504/50048]	Loss: 1.2806
Training Epoch: 15 [5632/50048]	Loss: 1.1363
Training Epoch: 15 [5760/50048]	Loss: 0.9686
Training Epoch: 15 [5888/50048]	Loss: 1.1655
Training Epoch: 15 [6016/50048]	Loss: 1.1570
Training Epoch: 15 [6144/50048]	Loss: 1.1566
Training Epoch: 15 [6272/50048]	Loss: 1.3157
Training Epoch: 15 [6400/50048]	Loss: 1.2357
Training Epoch: 15 [6528/50048]	Loss: 1.2054
Training Epoch: 15 [6656/50048]	Loss: 1.1672
Training Epoch: 15 [6784/50048]	Loss: 1.1672
Training Epoch: 15 [6912/50048]	Loss: 1.1251
Training Epoch: 15 [7040/50048]	Loss: 1.0592
Training Epoch: 15 [7168/50048]	Loss: 1.1652
Training Epoch: 15 [7296/50048]	Loss: 1.3112
Training Epoch: 15 [7424/50048]	Loss: 1.1817
Training Epoch: 15 [7552/50048]	Loss: 1.4073
Training Epoch: 15 [7680/50048]	Loss: 1.1719
Training Epoch: 15 [7808/50048]	Loss: 1.2494
Training Epoch: 15 [7936/50048]	Loss: 1.3367
Training Epoch: 15 [8064/50048]	Loss: 1.3798
Training Epoch: 15 [8192/50048]	Loss: 1.2304
Training Epoch: 15 [8320/50048]	Loss: 1.5161
Training Epoch: 15 [8448/50048]	Loss: 1.2304
Training Epoch: 15 [8576/50048]	Loss: 1.2072
Training Epoch: 15 [8704/50048]	Loss: 1.2695
Training Epoch: 15 [8832/50048]	Loss: 1.2654
Training Epoch: 15 [8960/50048]	Loss: 1.2764
Training Epoch: 15 [9088/50048]	Loss: 0.9210
Training Epoch: 15 [9216/50048]	Loss: 1.1744
Training Epoch: 15 [9344/50048]	Loss: 1.2981
Training Epoch: 15 [9472/50048]	Loss: 1.1781
Training Epoch: 15 [9600/50048]	Loss: 1.0792
Training Epoch: 15 [9728/50048]	Loss: 1.1889
Training Epoch: 15 [9856/50048]	Loss: 1.1165
Training Epoch: 15 [9984/50048]	Loss: 1.2589
Training Epoch: 15 [10112/50048]	Loss: 1.2053
Training Epoch: 15 [10240/50048]	Loss: 1.3105
Training Epoch: 15 [10368/50048]	Loss: 1.0683
Training Epoch: 15 [10496/50048]	Loss: 1.3782
Training Epoch: 15 [10624/50048]	Loss: 1.0946
Training Epoch: 15 [10752/50048]	Loss: 1.2794
Training Epoch: 15 [10880/50048]	Loss: 1.3003
Training Epoch: 15 [11008/50048]	Loss: 1.1646
Training Epoch: 15 [11136/50048]	Loss: 1.2223
Training Epoch: 15 [11264/50048]	Loss: 1.3257
Training Epoch: 15 [11392/50048]	Loss: 1.3656
Training Epoch: 15 [11520/50048]	Loss: 1.2083
Training Epoch: 15 [11648/50048]	Loss: 1.3518
Training Epoch: 15 [11776/50048]	Loss: 1.2717
Training Epoch: 15 [11904/50048]	Loss: 1.2556
Training Epoch: 15 [12032/50048]	Loss: 1.2377
Training Epoch: 15 [12160/50048]	Loss: 1.2167
Training Epoch: 15 [12288/50048]	Loss: 1.4184
Training Epoch: 15 [12416/50048]	Loss: 1.3264
Training Epoch: 15 [12544/50048]	Loss: 1.1972
Training Epoch: 15 [12672/50048]	Loss: 1.3218
Training Epoch: 15 [12800/50048]	Loss: 1.0496
Training Epoch: 15 [12928/50048]	Loss: 1.3780
Training Epoch: 15 [13056/50048]	Loss: 1.0086
Training Epoch: 15 [13184/50048]	Loss: 1.1512
Training Epoch: 15 [13312/50048]	Loss: 0.9941
Training Epoch: 15 [13440/50048]	Loss: 1.2325
Training Epoch: 15 [13568/50048]	Loss: 1.2568
Training Epoch: 15 [13696/50048]	Loss: 1.2841
Training Epoch: 15 [13824/50048]	Loss: 1.2882
Training Epoch: 15 [13952/50048]	Loss: 1.2594
Training Epoch: 15 [14080/50048]	Loss: 1.3282
Training Epoch: 15 [14208/50048]	Loss: 1.2179
Training Epoch: 15 [14336/50048]	Loss: 1.1912
Training Epoch: 15 [14464/50048]	Loss: 1.2635
Training Epoch: 15 [14592/50048]	Loss: 1.4945
Training Epoch: 15 [14720/50048]	Loss: 1.1663
Training Epoch: 15 [14848/50048]	Loss: 1.3715
Training Epoch: 15 [14976/50048]	Loss: 1.4786
Training Epoch: 15 [15104/50048]	Loss: 1.1273
Training Epoch: 15 [15232/50048]	Loss: 1.2037
Training Epoch: 15 [15360/50048]	Loss: 1.2992
Training Epoch: 15 [15488/50048]	Loss: 1.3066
Training Epoch: 15 [15616/50048]	Loss: 1.4190
Training Epoch: 15 [15744/50048]	Loss: 1.2245
Training Epoch: 15 [15872/50048]	Loss: 1.0989
Training Epoch: 15 [16000/50048]	Loss: 1.3621
Training Epoch: 15 [16128/50048]	Loss: 1.5379
Training Epoch: 15 [16256/50048]	Loss: 1.2997
Training Epoch: 15 [16384/50048]	Loss: 1.1466
Training Epoch: 15 [16512/50048]	Loss: 1.2965
Training Epoch: 15 [16640/50048]	Loss: 1.1468
Training Epoch: 15 [16768/50048]	Loss: 1.2107
Training Epoch: 15 [16896/50048]	Loss: 1.4282
Training Epoch: 15 [17024/50048]	Loss: 1.1838
Training Epoch: 15 [17152/50048]	Loss: 1.1557
Training Epoch: 15 [17280/50048]	Loss: 1.3021
Training Epoch: 15 [17408/50048]	Loss: 1.2741
Training Epoch: 15 [17536/50048]	Loss: 1.2386
Training Epoch: 15 [17664/50048]	Loss: 1.3921
Training Epoch: 15 [17792/50048]	Loss: 1.4023
Training Epoch: 15 [17920/50048]	Loss: 1.1579
Training Epoch: 15 [18048/50048]	Loss: 1.1772
Training Epoch: 15 [18176/50048]	Loss: 1.3641
Training Epoch: 15 [18304/50048]	Loss: 1.1951
Training Epoch: 15 [18432/50048]	Loss: 1.0603
Training Epoch: 15 [18560/50048]	Loss: 1.1938
Training Epoch: 15 [18688/50048]	Loss: 1.2864
Training Epoch: 15 [18816/50048]	Loss: 1.2470
Training Epoch: 15 [18944/50048]	Loss: 1.1528
Training Epoch: 15 [19072/50048]	Loss: 1.2228
Training Epoch: 15 [19200/50048]	Loss: 1.3676
Training Epoch: 15 [19328/50048]	Loss: 1.3058
Training Epoch: 15 [19456/50048]	Loss: 1.2441
Training Epoch: 15 [19584/50048]	Loss: 1.2507
Training Epoch: 15 [19712/50048]	Loss: 1.2197
Training Epoch: 15 [19840/50048]	Loss: 1.3400
Training Epoch: 15 [19968/50048]	Loss: 1.1897
Training Epoch: 15 [20096/50048]	Loss: 1.1322
Training Epoch: 15 [20224/50048]	Loss: 1.4245
Training Epoch: 15 [20352/50048]	Loss: 0.9743
Training Epoch: 15 [20480/50048]	Loss: 1.3149
Training Epoch: 15 [20608/50048]	Loss: 0.9997
Training Epoch: 15 [20736/50048]	Loss: 1.2292
Training Epoch: 15 [20864/50048]	Loss: 1.1216
Training Epoch: 15 [20992/50048]	Loss: 1.1431
Training Epoch: 15 [21120/50048]	Loss: 1.2340
Training Epoch: 15 [21248/50048]	Loss: 1.4231
Training Epoch: 15 [21376/50048]	Loss: 1.1455
Training Epoch: 15 [21504/50048]	Loss: 1.2297
Training Epoch: 15 [21632/50048]	Loss: 1.2877
Training Epoch: 15 [21760/50048]	Loss: 1.4687
Training Epoch: 15 [21888/50048]	Loss: 1.3285
Training Epoch: 15 [22016/50048]	Loss: 1.3235
Training Epoch: 15 [22144/50048]	Loss: 1.2632
Training Epoch: 15 [22272/50048]	Loss: 1.2828
Training Epoch: 15 [22400/50048]	Loss: 1.0756
Training Epoch: 15 [22528/50048]	Loss: 1.3341
Training Epoch: 15 [22656/50048]	Loss: 1.2184
Training Epoch: 15 [22784/50048]	Loss: 1.2842
Training Epoch: 15 [22912/50048]	Loss: 1.4310
Training Epoch: 15 [23040/50048]	Loss: 1.3403
Training Epoch: 15 [23168/50048]	Loss: 1.3426
Training Epoch: 15 [23296/50048]	Loss: 1.1347
Training Epoch: 15 [23424/50048]	Loss: 1.3309
Training Epoch: 15 [23552/50048]	Loss: 1.0700
Training Epoch: 15 [23680/50048]	Loss: 1.0949
Training Epoch: 15 [23808/50048]	Loss: 1.1958
Training Epoch: 15 [23936/50048]	Loss: 1.2387
Training Epoch: 15 [24064/50048]	Loss: 1.2653
Training Epoch: 15 [24192/50048]	Loss: 1.2376
Training Epoch: 15 [24320/50048]	Loss: 1.2795
Training Epoch: 15 [24448/50048]	Loss: 1.1002
Training Epoch: 15 [24576/50048]	Loss: 1.1360
Training Epoch: 15 [24704/50048]	Loss: 1.1298
Training Epoch: 15 [24832/50048]	Loss: 1.4057
Training Epoch: 15 [24960/50048]	Loss: 1.2060
Training Epoch: 15 [25088/50048]	Loss: 1.0214
Training Epoch: 15 [25216/50048]	Loss: 1.2799
Training Epoch: 15 [25344/50048]	Loss: 1.3388
Training Epoch: 15 [25472/50048]	Loss: 1.2447
Training Epoch: 15 [25600/50048]	Loss: 1.5127
Training Epoch: 15 [25728/50048]	Loss: 1.2236
Training Epoch: 15 [25856/50048]	Loss: 1.1187
Training Epoch: 15 [25984/50048]	Loss: 1.1701
Training Epoch: 15 [26112/50048]	Loss: 1.2651
Training Epoch: 15 [26240/50048]	Loss: 1.1262
Training Epoch: 15 [26368/50048]	Loss: 1.3485
Training Epoch: 15 [26496/50048]	Loss: 1.4033
Training Epoch: 15 [26624/50048]	Loss: 1.2203
Training Epoch: 15 [26752/50048]	Loss: 1.3101
Training Epoch: 15 [26880/50048]	Loss: 1.5265
Training Epoch: 15 [27008/50048]	Loss: 1.1407
Training Epoch: 15 [27136/50048]	Loss: 1.4572
Training Epoch: 15 [27264/50048]	Loss: 1.2432
Training Epoch: 15 [27392/50048]	Loss: 1.2605
Training Epoch: 15 [27520/50048]	Loss: 1.2982
Training Epoch: 15 [27648/50048]	Loss: 1.1664
Training Epoch: 15 [27776/50048]	Loss: 1.1166
Training Epoch: 15 [27904/50048]	Loss: 1.0938
Training Epoch: 15 [28032/50048]	Loss: 1.3789
Training Epoch: 15 [28160/50048]	Loss: 1.2746
Training Epoch: 15 [28288/50048]	Loss: 1.0549
Training Epoch: 15 [28416/50048]	Loss: 0.8663
Training Epoch: 15 [28544/50048]	Loss: 1.1483
Training Epoch: 15 [28672/50048]	Loss: 1.1301
Training Epoch: 15 [28800/50048]	Loss: 1.2792
Training Epoch: 15 [28928/50048]	Loss: 1.1004
Training Epoch: 15 [29056/50048]	Loss: 1.1238
Training Epoch: 15 [29184/50048]	Loss: 1.3730
Training Epoch: 15 [29312/50048]	Loss: 1.7368
Training Epoch: 15 [29440/50048]	Loss: 1.3945
Training Epoch: 15 [29568/50048]	Loss: 1.2894
Training Epoch: 15 [29696/50048]	Loss: 1.3415
Training Epoch: 15 [29824/50048]	Loss: 1.2281
Training Epoch: 15 [29952/50048]	Loss: 1.2784
Training Epoch: 15 [30080/50048]	Loss: 1.0898
Training Epoch: 15 [30208/50048]	Loss: 0.9947
Training Epoch: 15 [30336/50048]	Loss: 1.3067
Training Epoch: 15 [30464/50048]	Loss: 1.1268
Training Epoch: 15 [30592/50048]	Loss: 1.4259
Training Epoch: 15 [30720/50048]	Loss: 1.2023
Training Epoch: 15 [30848/50048]	Loss: 1.3402
Training Epoch: 15 [30976/50048]	Loss: 1.1327
Training Epoch: 15 [31104/50048]	Loss: 1.3140
Training Epoch: 15 [31232/50048]	Loss: 1.1637
Training Epoch: 15 [31360/50048]	Loss: 1.3840
Training Epoch: 15 [31488/50048]	Loss: 1.2431
Training Epoch: 15 [31616/50048]	Loss: 1.3060
Training Epoch: 15 [31744/50048]	Loss: 1.1541
Training Epoch: 15 [31872/50048]	Loss: 1.2690
Training Epoch: 15 [32000/50048]	Loss: 1.2903
Training Epoch: 15 [32128/50048]	Loss: 1.2367
Training Epoch: 15 [32256/50048]	Loss: 1.2343
Training Epoch: 15 [32384/50048]	Loss: 1.2366
Training Epoch: 15 [32512/50048]	Loss: 1.1088
Training Epoch: 15 [32640/50048]	Loss: 1.2742
Training Epoch: 15 [32768/50048]	Loss: 1.1740
Training Epoch: 15 [32896/50048]	Loss: 1.2867
Training Epoch: 15 [33024/50048]	Loss: 1.4369
Training Epoch: 15 [33152/50048]	Loss: 1.1570
Training Epoch: 15 [33280/50048]	Loss: 1.0759
Training Epoch: 15 [33408/50048]	Loss: 1.1521
Training Epoch: 15 [33536/50048]	Loss: 1.3127
Training Epoch: 15 [33664/50048]	Loss: 1.2560
Training Epoch: 15 [33792/50048]	Loss: 1.1568
Training Epoch: 15 [33920/50048]	Loss: 1.3920
Training Epoch: 15 [34048/50048]	Loss: 1.3993
Training Epoch: 15 [34176/50048]	Loss: 1.1765
Training Epoch: 15 [34304/50048]	Loss: 1.3004
Training Epoch: 15 [34432/50048]	Loss: 1.6706
Training Epoch: 15 [34560/50048]	Loss: 1.1476
Training Epoch: 15 [34688/50048]	Loss: 1.1479
Training Epoch: 15 [34816/50048]	Loss: 1.2044
Training Epoch: 15 [34944/50048]	Loss: 1.0348
Training Epoch: 15 [35072/50048]	Loss: 1.2742
Training Epoch: 15 [35200/50048]	Loss: 1.2251
Training Epoch: 15 [35328/50048]	Loss: 1.2944
Training Epoch: 15 [35456/50048]	Loss: 1.1538
Training Epoch: 15 [35584/50048]	Loss: 1.1419
Training Epoch: 15 [35712/50048]	Loss: 1.3589
Training Epoch: 15 [35840/50048]	Loss: 1.0998
Training Epoch: 15 [35968/50048]	Loss: 1.1504
Training Epoch: 15 [36096/50048]	Loss: 1.2435
Training Epoch: 15 [36224/50048]	Loss: 1.1542
Training Epoch: 15 [36352/50048]	Loss: 1.0766
Training Epoch: 15 [36480/50048]	Loss: 1.1059
Training Epoch: 15 [36608/50048]	Loss: 1.1739
Training Epoch: 15 [36736/50048]	Loss: 1.1223
Training Epoch: 15 [36864/50048]	Loss: 1.3740
Training Epoch: 15 [36992/50048]	Loss: 1.3363
Training Epoch: 15 [37120/50048]	Loss: 1.1411
Training Epoch: 15 [37248/50048]	Loss: 1.2570
Training Epoch: 15 [37376/50048]	Loss: 1.3844
Training Epoch: 15 [37504/50048]	Loss: 1.1778
Training Epoch: 15 [37632/50048]	Loss: 1.4501
Training Epoch: 15 [37760/50048]	Loss: 1.3637
Training Epoch: 15 [37888/50048]	Loss: 1.0252
Training Epoch: 15 [38016/50048]	Loss: 1.2780
Training Epoch: 15 [38144/50048]	Loss: 1.3079
Training Epoch: 15 [38272/50048]	Loss: 1.1629
Training Epoch: 15 [38400/50048]	Loss: 1.3339
Training Epoch: 15 [38528/50048]	Loss: 1.3902
Training Epoch: 15 [38656/50048]	Loss: 1.2574
Training Epoch: 15 [38784/50048]	Loss: 1.1340
Training Epoch: 15 [38912/50048]	Loss: 1.1681
Training Epoch: 15 [39040/50048]	Loss: 1.0660
Training Epoch: 15 [39168/50048]	Loss: 1.2945
Training Epoch: 15 [39296/50048]	Loss: 1.2123
Training Epoch: 15 [39424/50048]	Loss: 1.2229
Training Epoch: 15 [39552/50048]	Loss: 1.3790
Training Epoch: 15 [39680/50048]	Loss: 1.3210
Training Epoch: 15 [39808/50048]	Loss: 1.1984
Training Epoch: 15 [39936/50048]	Loss: 1.6480
Training Epoch: 15 [40064/50048]	Loss: 1.4009
Training Epoch: 15 [40192/50048]	Loss: 1.2184
Training Epoch: 15 [40320/50048]	Loss: 1.3583
Training Epoch: 15 [40448/50048]	Loss: 1.2794
Training Epoch: 15 [40576/50048]	Loss: 1.1679
Training Epoch: 15 [40704/50048]	Loss: 1.2890
Training Epoch: 15 [40832/50048]	Loss: 1.4698
Training Epoch: 15 [40960/50048]	Loss: 1.2037
Training Epoch: 15 [41088/50048]	Loss: 1.1854
Training Epoch: 15 [41216/50048]	Loss: 1.3187
Training Epoch: 15 [41344/50048]	Loss: 1.6016
Training Epoch: 15 [41472/50048]	Loss: 1.0683
Training Epoch: 15 [41600/50048]	Loss: 1.2338
Training Epoch: 15 [41728/50048]	Loss: 1.3471
Training Epoch: 15 [41856/50048]	Loss: 1.4240
Training Epoch: 15 [41984/50048]	Loss: 1.2296
Training Epoch: 15 [42112/50048]	Loss: 1.3130
Training Epoch: 15 [42240/50048]	Loss: 1.2117
Training Epoch: 15 [42368/50048]	Loss: 1.2458
Training Epoch: 15 [42496/50048]	Loss: 1.1424
Training Epoch: 15 [42624/50048]	Loss: 1.4925
Training Epoch: 15 [42752/50048]	Loss: 1.1503
Training Epoch: 15 [42880/50048]	Loss: 1.1978
Training Epoch: 15 [43008/50048]	Loss: 1.4545
Training Epoch: 15 [43136/50048]	Loss: 1.3256
Training Epoch: 15 [43264/50048]	Loss: 1.3714
Training Epoch: 15 [43392/50048]	Loss: 1.3377
Training Epoch: 15 [43520/50048]	Loss: 1.1809
Training Epoch: 15 [43648/50048]	Loss: 1.1348
Training Epoch: 15 [43776/50048]	Loss: 1.4998
Training Epoch: 15 [43904/50048]	Loss: 1.2211
Training Epoch: 15 [44032/50048]	Loss: 1.3481
Training Epoch: 15 [44160/50048]	Loss: 1.1652
Training Epoch: 15 [44288/50048]	Loss: 1.1120
Training Epoch: 15 [44416/50048]	Loss: 1.2701
Training Epoch: 15 [44544/50048]	Loss: 1.2697
Training Epoch: 15 [44672/50048]	Loss: 1.0798
Training Epoch: 15 [44800/50048]	Loss: 1.1062
Training Epoch: 15 [44928/50048]	Loss: 1.3864
Training Epoch: 15 [45056/50048]	Loss: 1.3275
Training Epoch: 15 [45184/50048]	Loss: 1.3197
Training Epoch: 15 [45312/50048]	Loss: 1.3925
Training Epoch: 15 [45440/50048]	Loss: 1.3032
Training Epoch: 15 [45568/50048]	Loss: 1.1263
Training Epoch: 15 [45696/50048]	Loss: 1.6561
2022-12-06 06:31:53,685 [ZeusDataLoader(train)] train epoch 16 done: time=86.45 energy=10495.91
2022-12-06 06:31:53,686 [ZeusDataLoader(eval)] Epoch 16 begin.
Training Epoch: 15 [45824/50048]	Loss: 1.1781
Training Epoch: 15 [45952/50048]	Loss: 1.3530
Training Epoch: 15 [46080/50048]	Loss: 1.3946
Training Epoch: 15 [46208/50048]	Loss: 1.1087
Training Epoch: 15 [46336/50048]	Loss: 1.1672
Training Epoch: 15 [46464/50048]	Loss: 0.9249
Training Epoch: 15 [46592/50048]	Loss: 1.3991
Training Epoch: 15 [46720/50048]	Loss: 1.1695
Training Epoch: 15 [46848/50048]	Loss: 1.3388
Training Epoch: 15 [46976/50048]	Loss: 1.0652
Training Epoch: 15 [47104/50048]	Loss: 1.3606
Training Epoch: 15 [47232/50048]	Loss: 1.3056
Training Epoch: 15 [47360/50048]	Loss: 1.2702
Training Epoch: 15 [47488/50048]	Loss: 1.1086
Training Epoch: 15 [47616/50048]	Loss: 1.5848
Training Epoch: 15 [47744/50048]	Loss: 0.9908
Training Epoch: 15 [47872/50048]	Loss: 1.4478
Training Epoch: 15 [48000/50048]	Loss: 1.1830
Training Epoch: 15 [48128/50048]	Loss: 1.3796
Training Epoch: 15 [48256/50048]	Loss: 1.1131
Training Epoch: 15 [48384/50048]	Loss: 1.1540
Training Epoch: 15 [48512/50048]	Loss: 1.3036
Training Epoch: 15 [48640/50048]	Loss: 1.2821
Training Epoch: 15 [48768/50048]	Loss: 1.0404
Training Epoch: 15 [48896/50048]	Loss: 1.3733
Training Epoch: 15 [49024/50048]	Loss: 1.1749
Training Epoch: 15 [49152/50048]	Loss: 1.0881
Training Epoch: 15 [49280/50048]	Loss: 1.4247
Training Epoch: 15 [49408/50048]	Loss: 1.1374
Training Epoch: 15 [49536/50048]	Loss: 1.2221
Training Epoch: 15 [49664/50048]	Loss: 1.2381
Training Epoch: 15 [49792/50048]	Loss: 1.0250
Training Epoch: 15 [49920/50048]	Loss: 1.0658
Training Epoch: 15 [50048/50048]	Loss: 1.1951
2022-12-06 11:31:57.370 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:31:57,399 [ZeusDataLoader(eval)] eval epoch 16 done: time=3.70 energy=453.26
2022-12-06 06:31:57,400 [ZeusDataLoader(train)] Up to epoch 16: time=1443.44, energy=175193.91, cost=213897.99
2022-12-06 06:31:57,400 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:31:57,400 [ZeusDataLoader(train)] Expected next epoch: time=1533.24, energy=185991.92, cost=227154.37
2022-12-06 06:31:57,401 [ZeusDataLoader(train)] Epoch 17 begin.
Validation Epoch: 15, Average loss: 0.0124, Accuracy: 0.5713
2022-12-06 06:31:57,598 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:31:57,599 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:31:57.601 [ZeusMonitor] Monitor started.
2022-12-06 11:31:57.601 [ZeusMonitor] Running indefinitely. 2022-12-06 11:31:57.601 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:31:57.601 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e17+gpu0.power.log
Training Epoch: 16 [128/50048]	Loss: 1.4138
Training Epoch: 16 [256/50048]	Loss: 1.1812
Training Epoch: 16 [384/50048]	Loss: 1.1884
Training Epoch: 16 [512/50048]	Loss: 1.2791
Training Epoch: 16 [640/50048]	Loss: 1.1050
Training Epoch: 16 [768/50048]	Loss: 1.1892
Training Epoch: 16 [896/50048]	Loss: 1.3512
Training Epoch: 16 [1024/50048]	Loss: 1.0791
Training Epoch: 16 [1152/50048]	Loss: 1.0425
Training Epoch: 16 [1280/50048]	Loss: 1.0142
Training Epoch: 16 [1408/50048]	Loss: 1.0434
Training Epoch: 16 [1536/50048]	Loss: 1.1561
Training Epoch: 16 [1664/50048]	Loss: 1.1818
Training Epoch: 16 [1792/50048]	Loss: 1.1221
Training Epoch: 16 [1920/50048]	Loss: 1.4303
Training Epoch: 16 [2048/50048]	Loss: 1.1162
Training Epoch: 16 [2176/50048]	Loss: 1.2571
Training Epoch: 16 [2304/50048]	Loss: 1.0894
Training Epoch: 16 [2432/50048]	Loss: 0.9637
Training Epoch: 16 [2560/50048]	Loss: 1.1129
Training Epoch: 16 [2688/50048]	Loss: 1.0880
Training Epoch: 16 [2816/50048]	Loss: 1.1322
Training Epoch: 16 [2944/50048]	Loss: 0.9972
Training Epoch: 16 [3072/50048]	Loss: 1.0562
Training Epoch: 16 [3200/50048]	Loss: 1.0817
Training Epoch: 16 [3328/50048]	Loss: 1.5028
Training Epoch: 16 [3456/50048]	Loss: 1.1250
Training Epoch: 16 [3584/50048]	Loss: 0.9707
Training Epoch: 16 [3712/50048]	Loss: 1.2340
Training Epoch: 16 [3840/50048]	Loss: 1.1061
Training Epoch: 16 [3968/50048]	Loss: 1.0262
Training Epoch: 16 [4096/50048]	Loss: 1.3394
Training Epoch: 16 [4224/50048]	Loss: 1.0872
Training Epoch: 16 [4352/50048]	Loss: 1.2195
Training Epoch: 16 [4480/50048]	Loss: 1.0726
Training Epoch: 16 [4608/50048]	Loss: 1.1760
Training Epoch: 16 [4736/50048]	Loss: 1.3756
Training Epoch: 16 [4864/50048]	Loss: 1.3289
Training Epoch: 16 [4992/50048]	Loss: 1.1281
Training Epoch: 16 [5120/50048]	Loss: 1.0882
Training Epoch: 16 [5248/50048]	Loss: 1.2513
Training Epoch: 16 [5376/50048]	Loss: 1.3292
Training Epoch: 16 [5504/50048]	Loss: 1.3529
Training Epoch: 16 [5632/50048]	Loss: 1.0988
Training Epoch: 16 [5760/50048]	Loss: 1.0918
Training Epoch: 16 [5888/50048]	Loss: 1.2226
Training Epoch: 16 [6016/50048]	Loss: 1.1962
Training Epoch: 16 [6144/50048]	Loss: 1.0588
Training Epoch: 16 [6272/50048]	Loss: 1.1995
Training Epoch: 16 [6400/50048]	Loss: 1.3932
Training Epoch: 16 [6528/50048]	Loss: 0.9956
Training Epoch: 16 [6656/50048]	Loss: 1.0905
Training Epoch: 16 [6784/50048]	Loss: 1.2973
Training Epoch: 16 [6912/50048]	Loss: 1.0563
Training Epoch: 16 [7040/50048]	Loss: 1.1853
Training Epoch: 16 [7168/50048]	Loss: 1.0875
Training Epoch: 16 [7296/50048]	Loss: 1.0733
Training Epoch: 16 [7424/50048]	Loss: 1.0383
Training Epoch: 16 [7552/50048]	Loss: 1.1258
Training Epoch: 16 [7680/50048]	Loss: 1.0039
Training Epoch: 16 [7808/50048]	Loss: 1.1017
Training Epoch: 16 [7936/50048]	Loss: 1.1036
Training Epoch: 16 [8064/50048]	Loss: 1.0813
Training Epoch: 16 [8192/50048]	Loss: 0.9816
Training Epoch: 16 [8320/50048]	Loss: 1.1608
Training Epoch: 16 [8448/50048]	Loss: 1.3674
Training Epoch: 16 [8576/50048]	Loss: 1.2667
Training Epoch: 16 [8704/50048]	Loss: 0.9898
Training Epoch: 16 [8832/50048]	Loss: 1.4594
Training Epoch: 16 [8960/50048]	Loss: 1.0804
Training Epoch: 16 [9088/50048]	Loss: 1.0873
Training Epoch: 16 [9216/50048]	Loss: 1.3506
Training Epoch: 16 [9344/50048]	Loss: 1.2444
Training Epoch: 16 [9472/50048]	Loss: 1.0968
Training Epoch: 16 [9600/50048]	Loss: 0.9232
Training Epoch: 16 [9728/50048]	Loss: 1.2035
Training Epoch: 16 [9856/50048]	Loss: 1.0206
Training Epoch: 16 [9984/50048]	Loss: 1.2280
Training Epoch: 16 [10112/50048]	Loss: 1.1023
Training Epoch: 16 [10240/50048]	Loss: 1.0910
Training Epoch: 16 [10368/50048]	Loss: 1.1419
Training Epoch: 16 [10496/50048]	Loss: 1.1698
Training Epoch: 16 [10624/50048]	Loss: 1.2397
Training Epoch: 16 [10752/50048]	Loss: 1.1239
Training Epoch: 16 [10880/50048]	Loss: 1.3378
Training Epoch: 16 [11008/50048]	Loss: 1.2894
Training Epoch: 16 [11136/50048]	Loss: 1.2381
Training Epoch: 16 [11264/50048]	Loss: 1.0850
Training Epoch: 16 [11392/50048]	Loss: 1.1129
Training Epoch: 16 [11520/50048]	Loss: 1.1866
Training Epoch: 16 [11648/50048]	Loss: 1.1545
Training Epoch: 16 [11776/50048]	Loss: 1.2435
Training Epoch: 16 [11904/50048]	Loss: 0.9689
Training Epoch: 16 [12032/50048]	Loss: 1.1638
Training Epoch: 16 [12160/50048]	Loss: 1.2160
Training Epoch: 16 [12288/50048]	Loss: 1.1371
Training Epoch: 16 [12416/50048]	Loss: 1.1494
Training Epoch: 16 [12544/50048]	Loss: 1.3564
Training Epoch: 16 [12672/50048]	Loss: 0.9227
Training Epoch: 16 [12800/50048]	Loss: 1.0805
Training Epoch: 16 [12928/50048]	Loss: 1.1794
Training Epoch: 16 [13056/50048]	Loss: 1.0515
Training Epoch: 16 [13184/50048]	Loss: 1.2279
Training Epoch: 16 [13312/50048]	Loss: 1.1321
Training Epoch: 16 [13440/50048]	Loss: 1.3094
Training Epoch: 16 [13568/50048]	Loss: 1.2786
Training Epoch: 16 [13696/50048]	Loss: 1.2429
Training Epoch: 16 [13824/50048]	Loss: 1.2264
Training Epoch: 16 [13952/50048]	Loss: 1.2573
Training Epoch: 16 [14080/50048]	Loss: 1.1540
Training Epoch: 16 [14208/50048]	Loss: 1.3182
Training Epoch: 16 [14336/50048]	Loss: 1.3462
Training Epoch: 16 [14464/50048]	Loss: 1.2199
Training Epoch: 16 [14592/50048]	Loss: 1.3170
Training Epoch: 16 [14720/50048]	Loss: 1.0860
Training Epoch: 16 [14848/50048]	Loss: 1.1031
Training Epoch: 16 [14976/50048]	Loss: 1.3326
Training Epoch: 16 [15104/50048]	Loss: 1.1661
Training Epoch: 16 [15232/50048]	Loss: 1.2853
Training Epoch: 16 [15360/50048]	Loss: 1.3435
Training Epoch: 16 [15488/50048]	Loss: 1.1494
Training Epoch: 16 [15616/50048]	Loss: 1.3413
Training Epoch: 16 [15744/50048]	Loss: 1.0348
Training Epoch: 16 [15872/50048]	Loss: 1.1673
Training Epoch: 16 [16000/50048]	Loss: 1.2994
Training Epoch: 16 [16128/50048]	Loss: 1.2631
Training Epoch: 16 [16256/50048]	Loss: 1.2713
Training Epoch: 16 [16384/50048]	Loss: 1.2290
Training Epoch: 16 [16512/50048]	Loss: 1.0724
Training Epoch: 16 [16640/50048]	Loss: 1.1263
Training Epoch: 16 [16768/50048]	Loss: 1.0495
Training Epoch: 16 [16896/50048]	Loss: 1.1377
Training Epoch: 16 [17024/50048]	Loss: 1.3345
Training Epoch: 16 [17152/50048]	Loss: 1.1187
Training Epoch: 16 [17280/50048]	Loss: 1.1682
Training Epoch: 16 [17408/50048]	Loss: 1.3624
Training Epoch: 16 [17536/50048]	Loss: 1.2912
Training Epoch: 16 [17664/50048]	Loss: 1.1188
Training Epoch: 16 [17792/50048]	Loss: 1.1760
Training Epoch: 16 [17920/50048]	Loss: 1.4797
Training Epoch: 16 [18048/50048]	Loss: 1.1545
Training Epoch: 16 [18176/50048]	Loss: 1.0473
Training Epoch: 16 [18304/50048]	Loss: 1.2014
Training Epoch: 16 [18432/50048]	Loss: 1.4521
Training Epoch: 16 [18560/50048]	Loss: 1.2050
Training Epoch: 16 [18688/50048]	Loss: 1.2580
Training Epoch: 16 [18816/50048]	Loss: 1.1576
Training Epoch: 16 [18944/50048]	Loss: 1.0647
Training Epoch: 16 [19072/50048]	Loss: 1.1937
Training Epoch: 16 [19200/50048]	Loss: 1.2959
Training Epoch: 16 [19328/50048]	Loss: 1.0601
Training Epoch: 16 [19456/50048]	Loss: 1.3269
Training Epoch: 16 [19584/50048]	Loss: 1.0573
Training Epoch: 16 [19712/50048]	Loss: 1.4715
Training Epoch: 16 [19840/50048]	Loss: 1.0883
Training Epoch: 16 [19968/50048]	Loss: 1.2079
Training Epoch: 16 [20096/50048]	Loss: 0.8724
Training Epoch: 16 [20224/50048]	Loss: 1.0227
Training Epoch: 16 [20352/50048]	Loss: 1.1144
Training Epoch: 16 [20480/50048]	Loss: 1.1954
Training Epoch: 16 [20608/50048]	Loss: 1.0170
Training Epoch: 16 [20736/50048]	Loss: 1.2380
Training Epoch: 16 [20864/50048]	Loss: 1.2383
Training Epoch: 16 [20992/50048]	Loss: 1.4497
Training Epoch: 16 [21120/50048]	Loss: 1.0816
Training Epoch: 16 [21248/50048]	Loss: 1.1809
Training Epoch: 16 [21376/50048]	Loss: 1.1028
Training Epoch: 16 [21504/50048]	Loss: 1.4441
Training Epoch: 16 [21632/50048]	Loss: 1.2600
Training Epoch: 16 [21760/50048]	Loss: 1.4300
Training Epoch: 16 [21888/50048]	Loss: 0.9727
Training Epoch: 16 [22016/50048]	Loss: 1.1303
Training Epoch: 16 [22144/50048]	Loss: 1.1821
Training Epoch: 16 [22272/50048]	Loss: 1.2484
Training Epoch: 16 [22400/50048]	Loss: 1.1693
Training Epoch: 16 [22528/50048]	Loss: 1.1405
Training Epoch: 16 [22656/50048]	Loss: 1.1244
Training Epoch: 16 [22784/50048]	Loss: 0.9790
Training Epoch: 16 [22912/50048]	Loss: 1.3172
Training Epoch: 16 [23040/50048]	Loss: 1.0792
Training Epoch: 16 [23168/50048]	Loss: 1.0974
Training Epoch: 16 [23296/50048]	Loss: 1.3106
Training Epoch: 16 [23424/50048]	Loss: 1.1572
Training Epoch: 16 [23552/50048]	Loss: 1.2010
Training Epoch: 16 [23680/50048]	Loss: 1.1902
Training Epoch: 16 [23808/50048]	Loss: 0.9935
Training Epoch: 16 [23936/50048]	Loss: 1.2438
Training Epoch: 16 [24064/50048]	Loss: 1.3823
Training Epoch: 16 [24192/50048]	Loss: 1.3024
Training Epoch: 16 [24320/50048]	Loss: 1.3637
Training Epoch: 16 [24448/50048]	Loss: 1.2435
Training Epoch: 16 [24576/50048]	Loss: 1.0616
Training Epoch: 16 [24704/50048]	Loss: 1.2043
Training Epoch: 16 [24832/50048]	Loss: 1.1578
Training Epoch: 16 [24960/50048]	Loss: 1.3008
Training Epoch: 16 [25088/50048]	Loss: 1.1017
Training Epoch: 16 [25216/50048]	Loss: 1.2240
Training Epoch: 16 [25344/50048]	Loss: 1.1017
Training Epoch: 16 [25472/50048]	Loss: 1.1957
Training Epoch: 16 [25600/50048]	Loss: 1.1482
Training Epoch: 16 [25728/50048]	Loss: 1.0970
Training Epoch: 16 [25856/50048]	Loss: 1.1134
Training Epoch: 16 [25984/50048]	Loss: 1.0201
Training Epoch: 16 [26112/50048]	Loss: 1.4725
Training Epoch: 16 [26240/50048]	Loss: 1.2266
Training Epoch: 16 [26368/50048]	Loss: 1.1818
Training Epoch: 16 [26496/50048]	Loss: 1.3162
Training Epoch: 16 [26624/50048]	Loss: 1.1461
Training Epoch: 16 [26752/50048]	Loss: 1.3522
Training Epoch: 16 [26880/50048]	Loss: 1.1211
Training Epoch: 16 [27008/50048]	Loss: 1.1291
Training Epoch: 16 [27136/50048]	Loss: 1.2278
Training Epoch: 16 [27264/50048]	Loss: 1.2414
Training Epoch: 16 [27392/50048]	Loss: 1.2484
Training Epoch: 16 [27520/50048]	Loss: 1.2720
Training Epoch: 16 [27648/50048]	Loss: 1.1938
Training Epoch: 16 [27776/50048]	Loss: 1.3912
Training Epoch: 16 [27904/50048]	Loss: 1.2928
Training Epoch: 16 [28032/50048]	Loss: 1.1034
Training Epoch: 16 [28160/50048]	Loss: 1.3212
Training Epoch: 16 [28288/50048]	Loss: 1.2166
Training Epoch: 16 [28416/50048]	Loss: 1.1051
Training Epoch: 16 [28544/50048]	Loss: 1.1600
Training Epoch: 16 [28672/50048]	Loss: 0.9017
Training Epoch: 16 [28800/50048]	Loss: 1.2109
Training Epoch: 16 [28928/50048]	Loss: 1.1793
Training Epoch: 16 [29056/50048]	Loss: 1.1373
Training Epoch: 16 [29184/50048]	Loss: 1.3718
Training Epoch: 16 [29312/50048]	Loss: 1.3733
Training Epoch: 16 [29440/50048]	Loss: 1.2314
Training Epoch: 16 [29568/50048]	Loss: 1.0245
Training Epoch: 16 [29696/50048]	Loss: 1.1557
Training Epoch: 16 [29824/50048]	Loss: 1.1783
Training Epoch: 16 [29952/50048]	Loss: 1.3747
Training Epoch: 16 [30080/50048]	Loss: 1.1925
Training Epoch: 16 [30208/50048]	Loss: 1.2844
Training Epoch: 16 [30336/50048]	Loss: 1.1811
Training Epoch: 16 [30464/50048]	Loss: 1.1203
Training Epoch: 16 [30592/50048]	Loss: 1.2031
Training Epoch: 16 [30720/50048]	Loss: 1.2340
Training Epoch: 16 [30848/50048]	Loss: 1.3244
Training Epoch: 16 [30976/50048]	Loss: 1.1165
Training Epoch: 16 [31104/50048]	Loss: 1.2096
Training Epoch: 16 [31232/50048]	Loss: 1.3327
Training Epoch: 16 [31360/50048]	Loss: 1.1421
Training Epoch: 16 [31488/50048]	Loss: 1.2120
Training Epoch: 16 [31616/50048]	Loss: 1.0847
Training Epoch: 16 [31744/50048]	Loss: 1.0735
Training Epoch: 16 [31872/50048]	Loss: 1.4544
Training Epoch: 16 [32000/50048]	Loss: 1.4096
Training Epoch: 16 [32128/50048]	Loss: 1.2027
Training Epoch: 16 [32256/50048]	Loss: 1.2053
Training Epoch: 16 [32384/50048]	Loss: 1.2763
Training Epoch: 16 [32512/50048]	Loss: 1.1215
Training Epoch: 16 [32640/50048]	Loss: 0.9431
Training Epoch: 16 [32768/50048]	Loss: 1.1764
Training Epoch: 16 [32896/50048]	Loss: 1.1965
Training Epoch: 16 [33024/50048]	Loss: 1.1218
Training Epoch: 16 [33152/50048]	Loss: 1.2833
Training Epoch: 16 [33280/50048]	Loss: 1.1249
Training Epoch: 16 [33408/50048]	Loss: 1.1714
Training Epoch: 16 [33536/50048]	Loss: 1.0274
Training Epoch: 16 [33664/50048]	Loss: 1.1422
Training Epoch: 16 [33792/50048]	Loss: 1.1507
Training Epoch: 16 [33920/50048]	Loss: 1.3331
Training Epoch: 16 [34048/50048]	Loss: 1.3310
Training Epoch: 16 [34176/50048]	Loss: 1.0102
Training Epoch: 16 [34304/50048]	Loss: 1.2859
Training Epoch: 16 [34432/50048]	Loss: 1.0305
Training Epoch: 16 [34560/50048]	Loss: 1.3050
Training Epoch: 16 [34688/50048]	Loss: 1.3033
Training Epoch: 16 [34816/50048]	Loss: 1.3269
Training Epoch: 16 [34944/50048]	Loss: 1.2737
Training Epoch: 16 [35072/50048]	Loss: 1.3478
Training Epoch: 16 [35200/50048]	Loss: 1.1240
Training Epoch: 16 [35328/50048]	Loss: 1.2752
Training Epoch: 16 [35456/50048]	Loss: 1.0585
Training Epoch: 16 [35584/50048]	Loss: 1.4237
Training Epoch: 16 [35712/50048]	Loss: 1.1444
Training Epoch: 16 [35840/50048]	Loss: 1.1317
Training Epoch: 16 [35968/50048]	Loss: 1.3814
Training Epoch: 16 [36096/50048]	Loss: 1.2116
Training Epoch: 16 [36224/50048]	Loss: 1.0546
Training Epoch: 16 [36352/50048]	Loss: 1.1163
Training Epoch: 16 [36480/50048]	Loss: 1.4584
Training Epoch: 16 [36608/50048]	Loss: 1.2865
Training Epoch: 16 [36736/50048]	Loss: 1.3865
Training Epoch: 16 [36864/50048]	Loss: 1.2444
Training Epoch: 16 [36992/50048]	Loss: 1.0217
Training Epoch: 16 [37120/50048]	Loss: 1.0492
Training Epoch: 16 [37248/50048]	Loss: 1.0558
Training Epoch: 16 [37376/50048]	Loss: 1.1341
Training Epoch: 16 [37504/50048]	Loss: 1.3062
Training Epoch: 16 [37632/50048]	Loss: 1.0196
Training Epoch: 16 [37760/50048]	Loss: 1.1314
Training Epoch: 16 [37888/50048]	Loss: 1.3239
Training Epoch: 16 [38016/50048]	Loss: 1.2535
Training Epoch: 16 [38144/50048]	Loss: 1.1899
Training Epoch: 16 [38272/50048]	Loss: 1.0605
Training Epoch: 16 [38400/50048]	Loss: 1.3362
Training Epoch: 16 [38528/50048]	Loss: 1.0947
Training Epoch: 16 [38656/50048]	Loss: 0.9770
Training Epoch: 16 [38784/50048]	Loss: 1.1758
Training Epoch: 16 [38912/50048]	Loss: 1.3131
Training Epoch: 16 [39040/50048]	Loss: 1.3827
Training Epoch: 16 [39168/50048]	Loss: 1.3920
Training Epoch: 16 [39296/50048]	Loss: 1.4800
Training Epoch: 16 [39424/50048]	Loss: 1.5945
Training Epoch: 16 [39552/50048]	Loss: 1.4535
Training Epoch: 16 [39680/50048]	Loss: 1.3363
Training Epoch: 16 [39808/50048]	Loss: 1.2190
Training Epoch: 16 [39936/50048]	Loss: 1.2141
Training Epoch: 16 [40064/50048]	Loss: 1.1670
Training Epoch: 16 [40192/50048]	Loss: 1.1889
Training Epoch: 16 [40320/50048]	Loss: 1.3809
Training Epoch: 16 [40448/50048]	Loss: 1.2611
Training Epoch: 16 [40576/50048]	Loss: 1.2441
Training Epoch: 16 [40704/50048]	Loss: 1.5400
Training Epoch: 16 [40832/50048]	Loss: 1.2763
Training Epoch: 16 [40960/50048]	Loss: 1.2928
Training Epoch: 16 [41088/50048]	Loss: 1.3217
Training Epoch: 16 [41216/50048]	Loss: 1.2858
Training Epoch: 16 [41344/50048]	Loss: 0.9567
Training Epoch: 16 [41472/50048]	Loss: 1.2810
Training Epoch: 16 [41600/50048]	Loss: 1.2146
Training Epoch: 16 [41728/50048]	Loss: 1.1180
Training Epoch: 16 [41856/50048]	Loss: 1.5119
Training Epoch: 16 [41984/50048]	Loss: 1.1321
Training Epoch: 16 [42112/50048]	Loss: 1.1638
Training Epoch: 16 [42240/50048]	Loss: 1.2777
Training Epoch: 16 [42368/50048]	Loss: 1.1976
Training Epoch: 16 [42496/50048]	Loss: 1.3362
Training Epoch: 16 [42624/50048]	Loss: 1.1887
Training Epoch: 16 [42752/50048]	Loss: 1.1959
Training Epoch: 16 [42880/50048]	Loss: 1.1328
Training Epoch: 16 [43008/50048]	Loss: 1.3304
Training Epoch: 16 [43136/50048]	Loss: 1.1799
Training Epoch: 16 [43264/50048]	Loss: 1.4161
Training Epoch: 16 [43392/50048]	Loss: 1.2554
Training Epoch: 16 [43520/50048]	Loss: 1.1368
Training Epoch: 16 [43648/50048]	Loss: 1.1654
Training Epoch: 16 [43776/50048]	Loss: 1.2764
Training Epoch: 16 [43904/50048]	Loss: 1.2178
Training Epoch: 16 [44032/50048]	Loss: 1.4695
Training Epoch: 16 [44160/50048]	Loss: 1.2990
Training Epoch: 16 [44288/50048]	Loss: 1.3411
Training Epoch: 16 [44416/50048]	Loss: 1.1622
Training Epoch: 16 [44544/50048]	Loss: 1.2988
Training Epoch: 16 [44672/50048]	Loss: 1.2785
Training Epoch: 16 [44800/50048]	Loss: 1.2687
Training Epoch: 16 [44928/50048]	Loss: 1.5052
Training Epoch: 16 [45056/50048]	Loss: 1.3921
Training Epoch: 16 [45184/50048]	Loss: 1.2437
Training Epoch: 16 [45312/50048]	Loss: 1.2124
Training Epoch: 16 [45440/50048]	Loss: 1.1052
Training Epoch: 16 [45568/50048]	Loss: 1.2136
Training Epoch: 16 [45696/50048]	Loss: 1.5018
2022-12-06 06:33:23,836 [ZeusDataLoader(train)] train epoch 17 done: time=86.43 energy=10502.34
2022-12-06 06:33:23,837 [ZeusDataLoader(eval)] Epoch 17 begin.
Training Epoch: 16 [45824/50048]	Loss: 1.1555
Training Epoch: 16 [45952/50048]	Loss: 1.3223
Training Epoch: 16 [46080/50048]	Loss: 1.3782
Training Epoch: 16 [46208/50048]	Loss: 1.1975
Training Epoch: 16 [46336/50048]	Loss: 1.4580
Training Epoch: 16 [46464/50048]	Loss: 1.1608
Training Epoch: 16 [46592/50048]	Loss: 1.2778
Training Epoch: 16 [46720/50048]	Loss: 1.2122
Training Epoch: 16 [46848/50048]	Loss: 1.1688
Training Epoch: 16 [46976/50048]	Loss: 1.0852
Training Epoch: 16 [47104/50048]	Loss: 1.2722
Training Epoch: 16 [47232/50048]	Loss: 1.3541
Training Epoch: 16 [47360/50048]	Loss: 1.2549
Training Epoch: 16 [47488/50048]	Loss: 1.1333
Training Epoch: 16 [47616/50048]	Loss: 0.9407
Training Epoch: 16 [47744/50048]	Loss: 1.2423
Training Epoch: 16 [47872/50048]	Loss: 1.2023
Training Epoch: 16 [48000/50048]	Loss: 1.2679
Training Epoch: 16 [48128/50048]	Loss: 1.3472
Training Epoch: 16 [48256/50048]	Loss: 1.0999
Training Epoch: 16 [48384/50048]	Loss: 1.1536
Training Epoch: 16 [48512/50048]	Loss: 1.2826
Training Epoch: 16 [48640/50048]	Loss: 1.2874
Training Epoch: 16 [48768/50048]	Loss: 1.3352
Training Epoch: 16 [48896/50048]	Loss: 1.3177
Training Epoch: 16 [49024/50048]	Loss: 1.2714
Training Epoch: 16 [49152/50048]	Loss: 1.1412
Training Epoch: 16 [49280/50048]	Loss: 1.1420
Training Epoch: 16 [49408/50048]	Loss: 1.0284
Training Epoch: 16 [49536/50048]	Loss: 1.4406
Training Epoch: 16 [49664/50048]	Loss: 1.3630
Training Epoch: 16 [49792/50048]	Loss: 1.0005
Training Epoch: 16 [49920/50048]	Loss: 1.2858
Training Epoch: 16 [50048/50048]	Loss: 1.2061
2022-12-06 11:33:27.575 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:33:27,598 [ZeusDataLoader(eval)] eval epoch 17 done: time=3.75 energy=453.10
2022-12-06 06:33:27,598 [ZeusDataLoader(train)] Up to epoch 17: time=1533.62, energy=186149.35, cost=227266.19
2022-12-06 06:33:27,599 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:33:27,599 [ZeusDataLoader(train)] Expected next epoch: time=1623.42, energy=196947.36, cost=240522.58
2022-12-06 06:33:27,600 [ZeusDataLoader(train)] Epoch 18 begin.
Validation Epoch: 16, Average loss: 0.0121, Accuracy: 0.5783
2022-12-06 06:33:27,750 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:33:27,751 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:33:27.754 [ZeusMonitor] Monitor started.
2022-12-06 11:33:27.754 [ZeusMonitor] Running indefinitely. 2022-12-06 11:33:27.754 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:33:27.754 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e18+gpu0.power.log
Training Epoch: 17 [128/50048]	Loss: 1.1979
Training Epoch: 17 [256/50048]	Loss: 1.0441
Training Epoch: 17 [384/50048]	Loss: 1.1056
Training Epoch: 17 [512/50048]	Loss: 1.1644
Training Epoch: 17 [640/50048]	Loss: 1.3425
Training Epoch: 17 [768/50048]	Loss: 1.0005
Training Epoch: 17 [896/50048]	Loss: 1.3258
Training Epoch: 17 [1024/50048]	Loss: 1.1590
Training Epoch: 17 [1152/50048]	Loss: 1.0782
Training Epoch: 17 [1280/50048]	Loss: 1.0770
Training Epoch: 17 [1408/50048]	Loss: 0.9553
Training Epoch: 17 [1536/50048]	Loss: 1.1692
Training Epoch: 17 [1664/50048]	Loss: 1.0673
Training Epoch: 17 [1792/50048]	Loss: 0.9475
Training Epoch: 17 [1920/50048]	Loss: 1.2424
Training Epoch: 17 [2048/50048]	Loss: 0.9569
Training Epoch: 17 [2176/50048]	Loss: 0.8604
Training Epoch: 17 [2304/50048]	Loss: 1.0668
Training Epoch: 17 [2432/50048]	Loss: 0.8547
Training Epoch: 17 [2560/50048]	Loss: 1.1642
Training Epoch: 17 [2688/50048]	Loss: 0.9724
Training Epoch: 17 [2816/50048]	Loss: 1.1166
Training Epoch: 17 [2944/50048]	Loss: 0.7517
Training Epoch: 17 [3072/50048]	Loss: 1.0438
Training Epoch: 17 [3200/50048]	Loss: 1.0868
Training Epoch: 17 [3328/50048]	Loss: 0.9543
Training Epoch: 17 [3456/50048]	Loss: 0.8631
Training Epoch: 17 [3584/50048]	Loss: 1.3452
Training Epoch: 17 [3712/50048]	Loss: 0.9604
Training Epoch: 17 [3840/50048]	Loss: 1.1379
Training Epoch: 17 [3968/50048]	Loss: 1.0472
Training Epoch: 17 [4096/50048]	Loss: 1.1656
Training Epoch: 17 [4224/50048]	Loss: 1.1783
Training Epoch: 17 [4352/50048]	Loss: 1.3694
Training Epoch: 17 [4480/50048]	Loss: 1.3179
Training Epoch: 17 [4608/50048]	Loss: 1.1902
Training Epoch: 17 [4736/50048]	Loss: 0.9536
Training Epoch: 17 [4864/50048]	Loss: 1.2789
Training Epoch: 17 [4992/50048]	Loss: 1.0576
Training Epoch: 17 [5120/50048]	Loss: 1.1391
Training Epoch: 17 [5248/50048]	Loss: 1.3006
Training Epoch: 17 [5376/50048]	Loss: 1.0622
Training Epoch: 17 [5504/50048]	Loss: 1.2713
Training Epoch: 17 [5632/50048]	Loss: 1.1205
Training Epoch: 17 [5760/50048]	Loss: 1.2240
Training Epoch: 17 [5888/50048]	Loss: 0.9668
Training Epoch: 17 [6016/50048]	Loss: 0.8579
Training Epoch: 17 [6144/50048]	Loss: 1.3639
Training Epoch: 17 [6272/50048]	Loss: 1.0200
Training Epoch: 17 [6400/50048]	Loss: 1.0454
Training Epoch: 17 [6528/50048]	Loss: 0.9100
Training Epoch: 17 [6656/50048]	Loss: 1.0699
Training Epoch: 17 [6784/50048]	Loss: 1.1988
Training Epoch: 17 [6912/50048]	Loss: 1.0797
Training Epoch: 17 [7040/50048]	Loss: 1.0664
Training Epoch: 17 [7168/50048]	Loss: 1.2532
Training Epoch: 17 [7296/50048]	Loss: 1.0949
Training Epoch: 17 [7424/50048]	Loss: 1.0492
Training Epoch: 17 [7552/50048]	Loss: 1.0933
Training Epoch: 17 [7680/50048]	Loss: 1.1783
Training Epoch: 17 [7808/50048]	Loss: 1.1691
Training Epoch: 17 [7936/50048]	Loss: 1.2644
Training Epoch: 17 [8064/50048]	Loss: 1.0660
Training Epoch: 17 [8192/50048]	Loss: 0.8242
Training Epoch: 17 [8320/50048]	Loss: 0.9574
Training Epoch: 17 [8448/50048]	Loss: 1.3359
Training Epoch: 17 [8576/50048]	Loss: 1.0309
Training Epoch: 17 [8704/50048]	Loss: 0.9645
Training Epoch: 17 [8832/50048]	Loss: 1.1482
Training Epoch: 17 [8960/50048]	Loss: 1.0649
Training Epoch: 17 [9088/50048]	Loss: 1.0231
Training Epoch: 17 [9216/50048]	Loss: 1.0950
Training Epoch: 17 [9344/50048]	Loss: 1.0312
Training Epoch: 17 [9472/50048]	Loss: 1.2293
Training Epoch: 17 [9600/50048]	Loss: 1.3180
Training Epoch: 17 [9728/50048]	Loss: 1.0492
Training Epoch: 17 [9856/50048]	Loss: 0.9403
Training Epoch: 17 [9984/50048]	Loss: 1.2090
Training Epoch: 17 [10112/50048]	Loss: 1.1956
Training Epoch: 17 [10240/50048]	Loss: 1.0892
Training Epoch: 17 [10368/50048]	Loss: 0.9197
Training Epoch: 17 [10496/50048]	Loss: 1.0554
Training Epoch: 17 [10624/50048]	Loss: 0.9792
Training Epoch: 17 [10752/50048]	Loss: 1.0908
Training Epoch: 17 [10880/50048]	Loss: 1.3443
Training Epoch: 17 [11008/50048]	Loss: 1.1835
Training Epoch: 17 [11136/50048]	Loss: 1.3952
Training Epoch: 17 [11264/50048]	Loss: 1.1726
Training Epoch: 17 [11392/50048]	Loss: 1.2105
Training Epoch: 17 [11520/50048]	Loss: 0.8489
Training Epoch: 17 [11648/50048]	Loss: 1.0583
Training Epoch: 17 [11776/50048]	Loss: 1.0690
Training Epoch: 17 [11904/50048]	Loss: 1.2271
Training Epoch: 17 [12032/50048]	Loss: 0.9965
Training Epoch: 17 [12160/50048]	Loss: 0.9961
Training Epoch: 17 [12288/50048]	Loss: 1.1521
Training Epoch: 17 [12416/50048]	Loss: 1.2225
Training Epoch: 17 [12544/50048]	Loss: 1.1364
Training Epoch: 17 [12672/50048]	Loss: 1.1021
Training Epoch: 17 [12800/50048]	Loss: 1.1855
Training Epoch: 17 [12928/50048]	Loss: 1.1642
Training Epoch: 17 [13056/50048]	Loss: 1.1797
Training Epoch: 17 [13184/50048]	Loss: 1.1134
Training Epoch: 17 [13312/50048]	Loss: 1.2862
Training Epoch: 17 [13440/50048]	Loss: 1.1684
Training Epoch: 17 [13568/50048]	Loss: 1.1581
Training Epoch: 17 [13696/50048]	Loss: 1.0982
Training Epoch: 17 [13824/50048]	Loss: 1.2675
Training Epoch: 17 [13952/50048]	Loss: 1.3153
Training Epoch: 17 [14080/50048]	Loss: 1.2033
Training Epoch: 17 [14208/50048]	Loss: 0.9661
Training Epoch: 17 [14336/50048]	Loss: 1.0699
Training Epoch: 17 [14464/50048]	Loss: 0.9757
Training Epoch: 17 [14592/50048]	Loss: 1.2465
Training Epoch: 17 [14720/50048]	Loss: 1.1328
Training Epoch: 17 [14848/50048]	Loss: 1.1782
Training Epoch: 17 [14976/50048]	Loss: 1.0685
Training Epoch: 17 [15104/50048]	Loss: 1.1196
Training Epoch: 17 [15232/50048]	Loss: 1.0333
Training Epoch: 17 [15360/50048]	Loss: 1.0418
Training Epoch: 17 [15488/50048]	Loss: 1.2230
Training Epoch: 17 [15616/50048]	Loss: 1.2111
Training Epoch: 17 [15744/50048]	Loss: 1.2012
Training Epoch: 17 [15872/50048]	Loss: 1.1274
Training Epoch: 17 [16000/50048]	Loss: 0.9355
Training Epoch: 17 [16128/50048]	Loss: 1.0883
Training Epoch: 17 [16256/50048]	Loss: 0.9345
Training Epoch: 17 [16384/50048]	Loss: 1.2170
Training Epoch: 17 [16512/50048]	Loss: 1.2612
Training Epoch: 17 [16640/50048]	Loss: 1.3184
Training Epoch: 17 [16768/50048]	Loss: 1.0910
Training Epoch: 17 [16896/50048]	Loss: 1.2471
Training Epoch: 17 [17024/50048]	Loss: 0.9820
Training Epoch: 17 [17152/50048]	Loss: 0.9721
Training Epoch: 17 [17280/50048]	Loss: 1.2305
Training Epoch: 17 [17408/50048]	Loss: 1.0600
Training Epoch: 17 [17536/50048]	Loss: 1.0233
Training Epoch: 17 [17664/50048]	Loss: 1.1275
Training Epoch: 17 [17792/50048]	Loss: 1.4091
Training Epoch: 17 [17920/50048]	Loss: 1.4718
Training Epoch: 17 [18048/50048]	Loss: 1.0981
Training Epoch: 17 [18176/50048]	Loss: 1.3595
Training Epoch: 17 [18304/50048]	Loss: 1.2397
Training Epoch: 17 [18432/50048]	Loss: 0.8698
Training Epoch: 17 [18560/50048]	Loss: 1.2734
Training Epoch: 17 [18688/50048]	Loss: 0.8861
Training Epoch: 17 [18816/50048]	Loss: 0.9916
Training Epoch: 17 [18944/50048]	Loss: 1.1529
Training Epoch: 17 [19072/50048]	Loss: 0.8473
Training Epoch: 17 [19200/50048]	Loss: 1.2455
Training Epoch: 17 [19328/50048]	Loss: 1.1923
Training Epoch: 17 [19456/50048]	Loss: 1.0695
Training Epoch: 17 [19584/50048]	Loss: 1.2479
Training Epoch: 17 [19712/50048]	Loss: 1.1386
Training Epoch: 17 [19840/50048]	Loss: 1.0204
Training Epoch: 17 [19968/50048]	Loss: 0.9784
Training Epoch: 17 [20096/50048]	Loss: 0.8745
Training Epoch: 17 [20224/50048]	Loss: 1.0234
Training Epoch: 17 [20352/50048]	Loss: 1.1297
Training Epoch: 17 [20480/50048]	Loss: 1.1659
Training Epoch: 17 [20608/50048]	Loss: 1.3811
Training Epoch: 17 [20736/50048]	Loss: 1.2570
Training Epoch: 17 [20864/50048]	Loss: 0.9614
Training Epoch: 17 [20992/50048]	Loss: 1.3579
Training Epoch: 17 [21120/50048]	Loss: 1.0740
Training Epoch: 17 [21248/50048]	Loss: 1.1174
Training Epoch: 17 [21376/50048]	Loss: 1.0015
Training Epoch: 17 [21504/50048]	Loss: 1.2127
Training Epoch: 17 [21632/50048]	Loss: 1.2677
Training Epoch: 17 [21760/50048]	Loss: 1.2089
Training Epoch: 17 [21888/50048]	Loss: 1.1864
Training Epoch: 17 [22016/50048]	Loss: 1.2073
Training Epoch: 17 [22144/50048]	Loss: 1.0851
Training Epoch: 17 [22272/50048]	Loss: 1.1033
Training Epoch: 17 [22400/50048]	Loss: 1.0465
Training Epoch: 17 [22528/50048]	Loss: 1.4485
Training Epoch: 17 [22656/50048]	Loss: 1.0533
Training Epoch: 17 [22784/50048]	Loss: 1.1094
Training Epoch: 17 [22912/50048]	Loss: 1.1489
Training Epoch: 17 [23040/50048]	Loss: 1.0996
Training Epoch: 17 [23168/50048]	Loss: 1.1031
Training Epoch: 17 [23296/50048]	Loss: 1.1261
Training Epoch: 17 [23424/50048]	Loss: 1.1550
Training Epoch: 17 [23552/50048]	Loss: 1.1788
Training Epoch: 17 [23680/50048]	Loss: 1.1434
Training Epoch: 17 [23808/50048]	Loss: 1.3364
Training Epoch: 17 [23936/50048]	Loss: 1.1737
Training Epoch: 17 [24064/50048]	Loss: 1.3058
Training Epoch: 17 [24192/50048]	Loss: 1.2663
Training Epoch: 17 [24320/50048]	Loss: 1.3740
Training Epoch: 17 [24448/50048]	Loss: 1.1536
Training Epoch: 17 [24576/50048]	Loss: 1.1218
Training Epoch: 17 [24704/50048]	Loss: 1.0092
Training Epoch: 17 [24832/50048]	Loss: 1.1616
Training Epoch: 17 [24960/50048]	Loss: 1.0869
Training Epoch: 17 [25088/50048]	Loss: 1.2972
Training Epoch: 17 [25216/50048]	Loss: 1.1270
Training Epoch: 17 [25344/50048]	Loss: 1.3123
Training Epoch: 17 [25472/50048]	Loss: 1.0466
Training Epoch: 17 [25600/50048]	Loss: 1.1510
Training Epoch: 17 [25728/50048]	Loss: 1.1032
Training Epoch: 17 [25856/50048]	Loss: 1.2031
Training Epoch: 17 [25984/50048]	Loss: 1.2633
Training Epoch: 17 [26112/50048]	Loss: 1.1907
Training Epoch: 17 [26240/50048]	Loss: 1.1452
Training Epoch: 17 [26368/50048]	Loss: 1.1304
Training Epoch: 17 [26496/50048]	Loss: 1.1083
Training Epoch: 17 [26624/50048]	Loss: 1.0779
Training Epoch: 17 [26752/50048]	Loss: 1.2512
Training Epoch: 17 [26880/50048]	Loss: 1.1951
Training Epoch: 17 [27008/50048]	Loss: 0.9333
Training Epoch: 17 [27136/50048]	Loss: 0.9535
Training Epoch: 17 [27264/50048]	Loss: 1.1731
Training Epoch: 17 [27392/50048]	Loss: 0.9372
Training Epoch: 17 [27520/50048]	Loss: 1.2715
Training Epoch: 17 [27648/50048]	Loss: 1.0680
Training Epoch: 17 [27776/50048]	Loss: 1.2310
Training Epoch: 17 [27904/50048]	Loss: 1.0939
Training Epoch: 17 [28032/50048]	Loss: 1.2287
Training Epoch: 17 [28160/50048]	Loss: 1.2140
Training Epoch: 17 [28288/50048]	Loss: 1.1445
Training Epoch: 17 [28416/50048]	Loss: 1.0734
Training Epoch: 17 [28544/50048]	Loss: 1.2335
Training Epoch: 17 [28672/50048]	Loss: 1.2299
Training Epoch: 17 [28800/50048]	Loss: 1.3844
Training Epoch: 17 [28928/50048]	Loss: 1.2452
Training Epoch: 17 [29056/50048]	Loss: 1.1068
Training Epoch: 17 [29184/50048]	Loss: 1.2242
Training Epoch: 17 [29312/50048]	Loss: 1.2373
Training Epoch: 17 [29440/50048]	Loss: 0.9375
Training Epoch: 17 [29568/50048]	Loss: 1.1301
Training Epoch: 17 [29696/50048]	Loss: 1.1614
Training Epoch: 17 [29824/50048]	Loss: 1.3189
Training Epoch: 17 [29952/50048]	Loss: 1.4382
Training Epoch: 17 [30080/50048]	Loss: 1.3991
Training Epoch: 17 [30208/50048]	Loss: 1.3636
Training Epoch: 17 [30336/50048]	Loss: 1.2693
Training Epoch: 17 [30464/50048]	Loss: 1.1569
Training Epoch: 17 [30592/50048]	Loss: 1.2147
Training Epoch: 17 [30720/50048]	Loss: 1.1377
Training Epoch: 17 [30848/50048]	Loss: 1.1769
Training Epoch: 17 [30976/50048]	Loss: 0.9873
Training Epoch: 17 [31104/50048]	Loss: 1.1428
Training Epoch: 17 [31232/50048]	Loss: 1.2051
Training Epoch: 17 [31360/50048]	Loss: 1.2694
Training Epoch: 17 [31488/50048]	Loss: 1.3432
Training Epoch: 17 [31616/50048]	Loss: 1.3682
Training Epoch: 17 [31744/50048]	Loss: 1.0801
Training Epoch: 17 [31872/50048]	Loss: 1.0772
Training Epoch: 17 [32000/50048]	Loss: 1.1782
Training Epoch: 17 [32128/50048]	Loss: 1.3567
Training Epoch: 17 [32256/50048]	Loss: 1.0841
Training Epoch: 17 [32384/50048]	Loss: 1.1553
Training Epoch: 17 [32512/50048]	Loss: 1.1933
Training Epoch: 17 [32640/50048]	Loss: 1.1569
Training Epoch: 17 [32768/50048]	Loss: 1.0072
Training Epoch: 17 [32896/50048]	Loss: 1.1976
Training Epoch: 17 [33024/50048]	Loss: 1.3091
Training Epoch: 17 [33152/50048]	Loss: 0.9850
Training Epoch: 17 [33280/50048]	Loss: 1.1256
Training Epoch: 17 [33408/50048]	Loss: 1.1265
Training Epoch: 17 [33536/50048]	Loss: 1.1556
Training Epoch: 17 [33664/50048]	Loss: 1.0875
Training Epoch: 17 [33792/50048]	Loss: 0.9870
Training Epoch: 17 [33920/50048]	Loss: 0.9224
Training Epoch: 17 [34048/50048]	Loss: 0.9579
Training Epoch: 17 [34176/50048]	Loss: 1.3333
Training Epoch: 17 [34304/50048]	Loss: 1.2193
Training Epoch: 17 [34432/50048]	Loss: 1.2929
Training Epoch: 17 [34560/50048]	Loss: 1.4293
Training Epoch: 17 [34688/50048]	Loss: 1.2346
Training Epoch: 17 [34816/50048]	Loss: 1.0656
Training Epoch: 17 [34944/50048]	Loss: 1.1081
Training Epoch: 17 [35072/50048]	Loss: 1.0532
Training Epoch: 17 [35200/50048]	Loss: 1.3558
Training Epoch: 17 [35328/50048]	Loss: 1.0623
Training Epoch: 17 [35456/50048]	Loss: 1.0713
Training Epoch: 17 [35584/50048]	Loss: 1.1022
Training Epoch: 17 [35712/50048]	Loss: 1.0199
Training Epoch: 17 [35840/50048]	Loss: 1.2331
Training Epoch: 17 [35968/50048]	Loss: 1.1777
Training Epoch: 17 [36096/50048]	Loss: 1.2548
Training Epoch: 17 [36224/50048]	Loss: 1.3892
Training Epoch: 17 [36352/50048]	Loss: 1.1366
Training Epoch: 17 [36480/50048]	Loss: 1.1672
Training Epoch: 17 [36608/50048]	Loss: 1.1118
Training Epoch: 17 [36736/50048]	Loss: 1.2041
Training Epoch: 17 [36864/50048]	Loss: 1.3803
Training Epoch: 17 [36992/50048]	Loss: 1.1986
Training Epoch: 17 [37120/50048]	Loss: 1.2540
Training Epoch: 17 [37248/50048]	Loss: 1.3449
Training Epoch: 17 [37376/50048]	Loss: 1.0482
Training Epoch: 17 [37504/50048]	Loss: 1.1653
Training Epoch: 17 [37632/50048]	Loss: 0.9777
Training Epoch: 17 [37760/50048]	Loss: 1.1474
Training Epoch: 17 [37888/50048]	Loss: 1.1072
Training Epoch: 17 [38016/50048]	Loss: 1.2041
Training Epoch: 17 [38144/50048]	Loss: 1.2324
Training Epoch: 17 [38272/50048]	Loss: 1.0943
Training Epoch: 17 [38400/50048]	Loss: 1.2484
Training Epoch: 17 [38528/50048]	Loss: 1.1319
Training Epoch: 17 [38656/50048]	Loss: 1.2186
Training Epoch: 17 [38784/50048]	Loss: 1.0877
Training Epoch: 17 [38912/50048]	Loss: 1.1743
Training Epoch: 17 [39040/50048]	Loss: 1.4069
Training Epoch: 17 [39168/50048]	Loss: 1.2500
Training Epoch: 17 [39296/50048]	Loss: 1.2530
Training Epoch: 17 [39424/50048]	Loss: 0.8666
Training Epoch: 17 [39552/50048]	Loss: 1.3206
Training Epoch: 17 [39680/50048]	Loss: 1.2164
Training Epoch: 17 [39808/50048]	Loss: 1.0630
Training Epoch: 17 [39936/50048]	Loss: 0.9997
Training Epoch: 17 [40064/50048]	Loss: 1.0554
Training Epoch: 17 [40192/50048]	Loss: 1.1784
Training Epoch: 17 [40320/50048]	Loss: 1.1600
Training Epoch: 17 [40448/50048]	Loss: 1.2236
Training Epoch: 17 [40576/50048]	Loss: 1.1511
Training Epoch: 17 [40704/50048]	Loss: 1.3294
Training Epoch: 17 [40832/50048]	Loss: 1.0328
Training Epoch: 17 [40960/50048]	Loss: 1.0590
Training Epoch: 17 [41088/50048]	Loss: 1.2847
Training Epoch: 17 [41216/50048]	Loss: 1.1660
Training Epoch: 17 [41344/50048]	Loss: 1.3110
Training Epoch: 17 [41472/50048]	Loss: 1.1977
Training Epoch: 17 [41600/50048]	Loss: 1.1582
Training Epoch: 17 [41728/50048]	Loss: 1.0373
Training Epoch: 17 [41856/50048]	Loss: 1.2922
Training Epoch: 17 [41984/50048]	Loss: 1.2455
Training Epoch: 17 [42112/50048]	Loss: 1.0009
Training Epoch: 17 [42240/50048]	Loss: 1.3610
Training Epoch: 17 [42368/50048]	Loss: 1.3559
Training Epoch: 17 [42496/50048]	Loss: 1.2463
Training Epoch: 17 [42624/50048]	Loss: 1.2421
Training Epoch: 17 [42752/50048]	Loss: 1.0828
Training Epoch: 17 [42880/50048]	Loss: 1.0262
Training Epoch: 17 [43008/50048]	Loss: 0.9562
Training Epoch: 17 [43136/50048]	Loss: 1.1504
Training Epoch: 17 [43264/50048]	Loss: 1.1683
Training Epoch: 17 [43392/50048]	Loss: 1.2319
Training Epoch: 17 [43520/50048]	Loss: 1.1755
Training Epoch: 17 [43648/50048]	Loss: 1.2378
Training Epoch: 17 [43776/50048]	Loss: 1.2177
Training Epoch: 17 [43904/50048]	Loss: 1.2963
Training Epoch: 17 [44032/50048]	Loss: 1.4267
Training Epoch: 17 [44160/50048]	Loss: 1.0095
Training Epoch: 17 [44288/50048]	Loss: 1.2055
Training Epoch: 17 [44416/50048]	Loss: 1.1941
Training Epoch: 17 [44544/50048]	Loss: 1.1233
Training Epoch: 17 [44672/50048]	Loss: 0.8470
Training Epoch: 17 [44800/50048]	Loss: 1.3642
Training Epoch: 17 [44928/50048]	Loss: 1.2139
Training Epoch: 17 [45056/50048]	Loss: 1.2341
Training Epoch: 17 [45184/50048]	Loss: 1.1904
Training Epoch: 17 [45312/50048]	Loss: 0.9691
Training Epoch: 17 [45440/50048]	Loss: 0.9225
Training Epoch: 17 [45568/50048]	Loss: 1.3357
Training Epoch: 17 [45696/50048]	Loss: 0.9845
2022-12-06 06:34:54,044 [ZeusDataLoader(train)] train epoch 18 done: time=86.43 energy=10491.87
2022-12-06 06:34:54,046 [ZeusDataLoader(eval)] Epoch 18 begin.
Training Epoch: 17 [45824/50048]	Loss: 1.0293
Training Epoch: 17 [45952/50048]	Loss: 1.2645
Training Epoch: 17 [46080/50048]	Loss: 1.0597
Training Epoch: 17 [46208/50048]	Loss: 1.0697
Training Epoch: 17 [46336/50048]	Loss: 1.0415
Training Epoch: 17 [46464/50048]	Loss: 1.3083
Training Epoch: 17 [46592/50048]	Loss: 1.0340
Training Epoch: 17 [46720/50048]	Loss: 1.1522
Training Epoch: 17 [46848/50048]	Loss: 1.1180
Training Epoch: 17 [46976/50048]	Loss: 1.1826
Training Epoch: 17 [47104/50048]	Loss: 1.0657
Training Epoch: 17 [47232/50048]	Loss: 0.9068
Training Epoch: 17 [47360/50048]	Loss: 1.2616
Training Epoch: 17 [47488/50048]	Loss: 1.4815
Training Epoch: 17 [47616/50048]	Loss: 1.2516
Training Epoch: 17 [47744/50048]	Loss: 1.1888
Training Epoch: 17 [47872/50048]	Loss: 1.0886
Training Epoch: 17 [48000/50048]	Loss: 1.1104
Training Epoch: 17 [48128/50048]	Loss: 1.0944
Training Epoch: 17 [48256/50048]	Loss: 1.2764
Training Epoch: 17 [48384/50048]	Loss: 1.0289
Training Epoch: 17 [48512/50048]	Loss: 1.2836
Training Epoch: 17 [48640/50048]	Loss: 1.1388
Training Epoch: 17 [48768/50048]	Loss: 1.0580
Training Epoch: 17 [48896/50048]	Loss: 1.0084
Training Epoch: 17 [49024/50048]	Loss: 1.5341
Training Epoch: 17 [49152/50048]	Loss: 1.1175
Training Epoch: 17 [49280/50048]	Loss: 1.2640
Training Epoch: 17 [49408/50048]	Loss: 1.2477
Training Epoch: 17 [49536/50048]	Loss: 1.1775
Training Epoch: 17 [49664/50048]	Loss: 1.2696
Training Epoch: 17 [49792/50048]	Loss: 1.3287
Training Epoch: 17 [49920/50048]	Loss: 1.1415
Training Epoch: 17 [50048/50048]	Loss: 1.2610
2022-12-06 11:34:57.739 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:34:57,751 [ZeusDataLoader(eval)] eval epoch 18 done: time=3.70 energy=453.82
2022-12-06 06:34:57,751 [ZeusDataLoader(train)] Up to epoch 18: time=1623.75, energy=197095.04, cost=240625.48
2022-12-06 06:34:57,751 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:34:57,751 [ZeusDataLoader(train)] Expected next epoch: time=1713.55, energy=207893.05, cost=253881.86
2022-12-06 06:34:57,752 [ZeusDataLoader(train)] Epoch 19 begin.
Validation Epoch: 17, Average loss: 0.0120, Accuracy: 0.5832
2022-12-06 06:34:57,946 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:34:57,947 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:34:57.949 [ZeusMonitor] Monitor started.
2022-12-06 11:34:57.949 [ZeusMonitor] Running indefinitely. 2022-12-06 11:34:57.949 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:34:57.949 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e19+gpu0.power.log
Training Epoch: 18 [128/50048]	Loss: 1.0027
Training Epoch: 18 [256/50048]	Loss: 1.0768
Training Epoch: 18 [384/50048]	Loss: 1.0225
Training Epoch: 18 [512/50048]	Loss: 0.7874
Training Epoch: 18 [640/50048]	Loss: 1.1243
Training Epoch: 18 [768/50048]	Loss: 1.1663
Training Epoch: 18 [896/50048]	Loss: 0.9474
Training Epoch: 18 [1024/50048]	Loss: 1.0076
Training Epoch: 18 [1152/50048]	Loss: 1.0754
Training Epoch: 18 [1280/50048]	Loss: 1.1512
Training Epoch: 18 [1408/50048]	Loss: 1.0367
Training Epoch: 18 [1536/50048]	Loss: 1.3142
Training Epoch: 18 [1664/50048]	Loss: 1.1383
Training Epoch: 18 [1792/50048]	Loss: 1.2802
Training Epoch: 18 [1920/50048]	Loss: 1.0815
Training Epoch: 18 [2048/50048]	Loss: 1.0296
Training Epoch: 18 [2176/50048]	Loss: 1.1358
Training Epoch: 18 [2304/50048]	Loss: 0.9462
Training Epoch: 18 [2432/50048]	Loss: 0.9778
Training Epoch: 18 [2560/50048]	Loss: 1.2391
Training Epoch: 18 [2688/50048]	Loss: 1.0118
Training Epoch: 18 [2816/50048]	Loss: 1.1564
Training Epoch: 18 [2944/50048]	Loss: 1.1173
Training Epoch: 18 [3072/50048]	Loss: 0.9200
Training Epoch: 18 [3200/50048]	Loss: 1.0583
Training Epoch: 18 [3328/50048]	Loss: 1.0447
Training Epoch: 18 [3456/50048]	Loss: 1.0475
Training Epoch: 18 [3584/50048]	Loss: 1.1140
Training Epoch: 18 [3712/50048]	Loss: 1.1969
Training Epoch: 18 [3840/50048]	Loss: 1.0032
Training Epoch: 18 [3968/50048]	Loss: 1.0648
Training Epoch: 18 [4096/50048]	Loss: 0.9174
Training Epoch: 18 [4224/50048]	Loss: 0.9436
Training Epoch: 18 [4352/50048]	Loss: 1.0265
Training Epoch: 18 [4480/50048]	Loss: 1.1509
Training Epoch: 18 [4608/50048]	Loss: 1.1002
Training Epoch: 18 [4736/50048]	Loss: 1.0984
Training Epoch: 18 [4864/50048]	Loss: 1.0726
Training Epoch: 18 [4992/50048]	Loss: 0.9516
Training Epoch: 18 [5120/50048]	Loss: 0.9816
Training Epoch: 18 [5248/50048]	Loss: 1.0086
Training Epoch: 18 [5376/50048]	Loss: 1.1354
Training Epoch: 18 [5504/50048]	Loss: 1.0952
Training Epoch: 18 [5632/50048]	Loss: 1.1326
Training Epoch: 18 [5760/50048]	Loss: 1.0867
Training Epoch: 18 [5888/50048]	Loss: 0.9799
Training Epoch: 18 [6016/50048]	Loss: 1.2394
Training Epoch: 18 [6144/50048]	Loss: 1.0703
Training Epoch: 18 [6272/50048]	Loss: 0.8931
Training Epoch: 18 [6400/50048]	Loss: 0.7741
Training Epoch: 18 [6528/50048]	Loss: 1.1671
Training Epoch: 18 [6656/50048]	Loss: 0.9234
Training Epoch: 18 [6784/50048]	Loss: 1.0756
Training Epoch: 18 [6912/50048]	Loss: 1.1395
Training Epoch: 18 [7040/50048]	Loss: 1.2226
Training Epoch: 18 [7168/50048]	Loss: 0.9112
Training Epoch: 18 [7296/50048]	Loss: 1.0906
Training Epoch: 18 [7424/50048]	Loss: 1.0649
Training Epoch: 18 [7552/50048]	Loss: 1.1170
Training Epoch: 18 [7680/50048]	Loss: 1.0248
Training Epoch: 18 [7808/50048]	Loss: 1.2058
Training Epoch: 18 [7936/50048]	Loss: 0.9933
Training Epoch: 18 [8064/50048]	Loss: 1.3032
Training Epoch: 18 [8192/50048]	Loss: 0.9438
Training Epoch: 18 [8320/50048]	Loss: 1.1480
Training Epoch: 18 [8448/50048]	Loss: 1.0184
Training Epoch: 18 [8576/50048]	Loss: 1.1200
Training Epoch: 18 [8704/50048]	Loss: 0.9899
Training Epoch: 18 [8832/50048]	Loss: 1.2079
Training Epoch: 18 [8960/50048]	Loss: 0.9484
Training Epoch: 18 [9088/50048]	Loss: 1.0350
Training Epoch: 18 [9216/50048]	Loss: 0.8089
Training Epoch: 18 [9344/50048]	Loss: 1.2281
Training Epoch: 18 [9472/50048]	Loss: 1.1704
Training Epoch: 18 [9600/50048]	Loss: 1.0982
Training Epoch: 18 [9728/50048]	Loss: 0.8928
Training Epoch: 18 [9856/50048]	Loss: 0.9463
Training Epoch: 18 [9984/50048]	Loss: 1.0728
Training Epoch: 18 [10112/50048]	Loss: 1.1582
Training Epoch: 18 [10240/50048]	Loss: 1.1743
Training Epoch: 18 [10368/50048]	Loss: 1.2114
Training Epoch: 18 [10496/50048]	Loss: 0.9716
Training Epoch: 18 [10624/50048]	Loss: 0.9964
Training Epoch: 18 [10752/50048]	Loss: 1.1317
Training Epoch: 18 [10880/50048]	Loss: 0.9121
Training Epoch: 18 [11008/50048]	Loss: 1.1076
Training Epoch: 18 [11136/50048]	Loss: 1.1651
Training Epoch: 18 [11264/50048]	Loss: 1.1384
Training Epoch: 18 [11392/50048]	Loss: 1.1490
Training Epoch: 18 [11520/50048]	Loss: 1.0187
Training Epoch: 18 [11648/50048]	Loss: 1.0953
Training Epoch: 18 [11776/50048]	Loss: 1.1529
Training Epoch: 18 [11904/50048]	Loss: 1.1583
Training Epoch: 18 [12032/50048]	Loss: 1.1583
Training Epoch: 18 [12160/50048]	Loss: 0.9390
Training Epoch: 18 [12288/50048]	Loss: 0.9108
Training Epoch: 18 [12416/50048]	Loss: 1.2259
Training Epoch: 18 [12544/50048]	Loss: 1.1211
Training Epoch: 18 [12672/50048]	Loss: 1.1154
Training Epoch: 18 [12800/50048]	Loss: 1.1403
Training Epoch: 18 [12928/50048]	Loss: 1.0098
Training Epoch: 18 [13056/50048]	Loss: 1.1678
Training Epoch: 18 [13184/50048]	Loss: 1.1527
Training Epoch: 18 [13312/50048]	Loss: 0.8897
Training Epoch: 18 [13440/50048]	Loss: 1.2790
Training Epoch: 18 [13568/50048]	Loss: 1.1898
Training Epoch: 18 [13696/50048]	Loss: 1.0243
Training Epoch: 18 [13824/50048]	Loss: 0.9904
Training Epoch: 18 [13952/50048]	Loss: 1.2244
Training Epoch: 18 [14080/50048]	Loss: 1.1088
Training Epoch: 18 [14208/50048]	Loss: 0.8773
Training Epoch: 18 [14336/50048]	Loss: 0.9170
Training Epoch: 18 [14464/50048]	Loss: 1.1968
Training Epoch: 18 [14592/50048]	Loss: 1.3838
Training Epoch: 18 [14720/50048]	Loss: 0.9376
Training Epoch: 18 [14848/50048]	Loss: 1.3885
Training Epoch: 18 [14976/50048]	Loss: 0.8215
Training Epoch: 18 [15104/50048]	Loss: 1.3856
Training Epoch: 18 [15232/50048]	Loss: 1.1009
Training Epoch: 18 [15360/50048]	Loss: 1.2277
Training Epoch: 18 [15488/50048]	Loss: 1.1493
Training Epoch: 18 [15616/50048]	Loss: 1.1556
Training Epoch: 18 [15744/50048]	Loss: 1.2227
Training Epoch: 18 [15872/50048]	Loss: 1.2689
Training Epoch: 18 [16000/50048]	Loss: 1.0234
Training Epoch: 18 [16128/50048]	Loss: 0.9016
Training Epoch: 18 [16256/50048]	Loss: 1.0683
Training Epoch: 18 [16384/50048]	Loss: 1.1712
Training Epoch: 18 [16512/50048]	Loss: 1.0671
Training Epoch: 18 [16640/50048]	Loss: 1.2386
Training Epoch: 18 [16768/50048]	Loss: 1.0863
Training Epoch: 18 [16896/50048]	Loss: 1.1883
Training Epoch: 18 [17024/50048]	Loss: 1.3275
Training Epoch: 18 [17152/50048]	Loss: 1.0074
Training Epoch: 18 [17280/50048]	Loss: 0.9536
Training Epoch: 18 [17408/50048]	Loss: 1.0656
Training Epoch: 18 [17536/50048]	Loss: 1.1247
Training Epoch: 18 [17664/50048]	Loss: 0.9988
Training Epoch: 18 [17792/50048]	Loss: 1.1574
Training Epoch: 18 [17920/50048]	Loss: 1.0799
Training Epoch: 18 [18048/50048]	Loss: 1.1090
Training Epoch: 18 [18176/50048]	Loss: 1.2307
Training Epoch: 18 [18304/50048]	Loss: 1.2224
Training Epoch: 18 [18432/50048]	Loss: 1.1996
Training Epoch: 18 [18560/50048]	Loss: 1.1181
Training Epoch: 18 [18688/50048]	Loss: 1.2897
Training Epoch: 18 [18816/50048]	Loss: 0.9194
Training Epoch: 18 [18944/50048]	Loss: 1.0425
Training Epoch: 18 [19072/50048]	Loss: 1.3151
Training Epoch: 18 [19200/50048]	Loss: 1.1579
Training Epoch: 18 [19328/50048]	Loss: 1.0012
Training Epoch: 18 [19456/50048]	Loss: 0.9295
Training Epoch: 18 [19584/50048]	Loss: 1.0566
Training Epoch: 18 [19712/50048]	Loss: 1.0497
Training Epoch: 18 [19840/50048]	Loss: 1.1078
Training Epoch: 18 [19968/50048]	Loss: 1.1218
Training Epoch: 18 [20096/50048]	Loss: 0.9378
Training Epoch: 18 [20224/50048]	Loss: 1.0635
Training Epoch: 18 [20352/50048]	Loss: 1.1057
Training Epoch: 18 [20480/50048]	Loss: 1.1435
Training Epoch: 18 [20608/50048]	Loss: 1.0024
Training Epoch: 18 [20736/50048]	Loss: 1.0229
Training Epoch: 18 [20864/50048]	Loss: 1.3004
Training Epoch: 18 [20992/50048]	Loss: 1.0199
Training Epoch: 18 [21120/50048]	Loss: 0.9767
Training Epoch: 18 [21248/50048]	Loss: 1.3866
Training Epoch: 18 [21376/50048]	Loss: 1.0945
Training Epoch: 18 [21504/50048]	Loss: 1.1340
Training Epoch: 18 [21632/50048]	Loss: 1.0952
Training Epoch: 18 [21760/50048]	Loss: 1.1074
Training Epoch: 18 [21888/50048]	Loss: 1.3032
Training Epoch: 18 [22016/50048]	Loss: 1.1106
Training Epoch: 18 [22144/50048]	Loss: 0.8102
Training Epoch: 18 [22272/50048]	Loss: 1.0591
Training Epoch: 18 [22400/50048]	Loss: 1.3441
Training Epoch: 18 [22528/50048]	Loss: 1.1176
Training Epoch: 18 [22656/50048]	Loss: 0.9844
Training Epoch: 18 [22784/50048]	Loss: 0.8729
Training Epoch: 18 [22912/50048]	Loss: 1.0856
Training Epoch: 18 [23040/50048]	Loss: 1.1162
Training Epoch: 18 [23168/50048]	Loss: 1.2376
Training Epoch: 18 [23296/50048]	Loss: 1.1736
Training Epoch: 18 [23424/50048]	Loss: 0.7756
Training Epoch: 18 [23552/50048]	Loss: 0.9237
Training Epoch: 18 [23680/50048]	Loss: 1.2115
Training Epoch: 18 [23808/50048]	Loss: 1.0281
Training Epoch: 18 [23936/50048]	Loss: 1.0272
Training Epoch: 18 [24064/50048]	Loss: 1.0416
Training Epoch: 18 [24192/50048]	Loss: 0.9370
Training Epoch: 18 [24320/50048]	Loss: 0.7867
Training Epoch: 18 [24448/50048]	Loss: 1.0248
Training Epoch: 18 [24576/50048]	Loss: 1.1926
Training Epoch: 18 [24704/50048]	Loss: 1.1016
Training Epoch: 18 [24832/50048]	Loss: 0.9333
Training Epoch: 18 [24960/50048]	Loss: 1.3518
Training Epoch: 18 [25088/50048]	Loss: 1.3279
Training Epoch: 18 [25216/50048]	Loss: 0.8396
Training Epoch: 18 [25344/50048]	Loss: 1.0789
Training Epoch: 18 [25472/50048]	Loss: 0.8336
Training Epoch: 18 [25600/50048]	Loss: 1.0125
Training Epoch: 18 [25728/50048]	Loss: 1.2718
Training Epoch: 18 [25856/50048]	Loss: 0.7336
Training Epoch: 18 [25984/50048]	Loss: 1.1254
Training Epoch: 18 [26112/50048]	Loss: 1.0591
Training Epoch: 18 [26240/50048]	Loss: 1.0817
Training Epoch: 18 [26368/50048]	Loss: 1.0385
Training Epoch: 18 [26496/50048]	Loss: 1.2321
Training Epoch: 18 [26624/50048]	Loss: 1.1081
Training Epoch: 18 [26752/50048]	Loss: 1.0924
Training Epoch: 18 [26880/50048]	Loss: 1.1077
Training Epoch: 18 [27008/50048]	Loss: 0.9499
Training Epoch: 18 [27136/50048]	Loss: 1.0889
Training Epoch: 18 [27264/50048]	Loss: 1.0845
Training Epoch: 18 [27392/50048]	Loss: 1.2991
Training Epoch: 18 [27520/50048]	Loss: 1.0144
Training Epoch: 18 [27648/50048]	Loss: 1.0512
Training Epoch: 18 [27776/50048]	Loss: 1.0740
Training Epoch: 18 [27904/50048]	Loss: 1.0413
Training Epoch: 18 [28032/50048]	Loss: 1.0718
Training Epoch: 18 [28160/50048]	Loss: 1.0974
Training Epoch: 18 [28288/50048]	Loss: 1.1086
Training Epoch: 18 [28416/50048]	Loss: 1.3374
Training Epoch: 18 [28544/50048]	Loss: 1.0132
Training Epoch: 18 [28672/50048]	Loss: 1.0796
Training Epoch: 18 [28800/50048]	Loss: 1.4289
Training Epoch: 18 [28928/50048]	Loss: 0.9834
Training Epoch: 18 [29056/50048]	Loss: 1.1294
Training Epoch: 18 [29184/50048]	Loss: 0.9774
Training Epoch: 18 [29312/50048]	Loss: 1.0934
Training Epoch: 18 [29440/50048]	Loss: 1.0429
Training Epoch: 18 [29568/50048]	Loss: 1.2374
Training Epoch: 18 [29696/50048]	Loss: 1.1321
Training Epoch: 18 [29824/50048]	Loss: 1.0239
Training Epoch: 18 [29952/50048]	Loss: 1.2093
Training Epoch: 18 [30080/50048]	Loss: 1.3557
Training Epoch: 18 [30208/50048]	Loss: 1.0331
Training Epoch: 18 [30336/50048]	Loss: 1.0473
Training Epoch: 18 [30464/50048]	Loss: 1.0237
Training Epoch: 18 [30592/50048]	Loss: 1.1806
Training Epoch: 18 [30720/50048]	Loss: 0.9967
Training Epoch: 18 [30848/50048]	Loss: 1.1629
Training Epoch: 18 [30976/50048]	Loss: 1.0852
Training Epoch: 18 [31104/50048]	Loss: 1.2930
Training Epoch: 18 [31232/50048]	Loss: 1.2068
Training Epoch: 18 [31360/50048]	Loss: 1.1391
Training Epoch: 18 [31488/50048]	Loss: 1.2353
Training Epoch: 18 [31616/50048]	Loss: 1.2221
Training Epoch: 18 [31744/50048]	Loss: 1.3009
Training Epoch: 18 [31872/50048]	Loss: 1.1159
Training Epoch: 18 [32000/50048]	Loss: 1.1595
Training Epoch: 18 [32128/50048]	Loss: 1.1255
Training Epoch: 18 [32256/50048]	Loss: 0.9444
Training Epoch: 18 [32384/50048]	Loss: 1.1413
Training Epoch: 18 [32512/50048]	Loss: 0.6812
Training Epoch: 18 [32640/50048]	Loss: 1.1059
Training Epoch: 18 [32768/50048]	Loss: 1.0381
Training Epoch: 18 [32896/50048]	Loss: 0.9646
Training Epoch: 18 [33024/50048]	Loss: 1.0971
Training Epoch: 18 [33152/50048]	Loss: 1.1525
Training Epoch: 18 [33280/50048]	Loss: 1.1322
Training Epoch: 18 [33408/50048]	Loss: 0.8857
Training Epoch: 18 [33536/50048]	Loss: 1.2340
Training Epoch: 18 [33664/50048]	Loss: 1.1442
Training Epoch: 18 [33792/50048]	Loss: 1.0463
Training Epoch: 18 [33920/50048]	Loss: 1.2678
Training Epoch: 18 [34048/50048]	Loss: 1.0124
Training Epoch: 18 [34176/50048]	Loss: 1.2189
Training Epoch: 18 [34304/50048]	Loss: 0.9592
Training Epoch: 18 [34432/50048]	Loss: 0.8725
Training Epoch: 18 [34560/50048]	Loss: 1.1199
Training Epoch: 18 [34688/50048]	Loss: 1.3266
Training Epoch: 18 [34816/50048]	Loss: 1.0440
Training Epoch: 18 [34944/50048]	Loss: 1.0536
Training Epoch: 18 [35072/50048]	Loss: 1.2750
Training Epoch: 18 [35200/50048]	Loss: 1.0483
Training Epoch: 18 [35328/50048]	Loss: 1.0947
Training Epoch: 18 [35456/50048]	Loss: 1.2094
Training Epoch: 18 [35584/50048]	Loss: 1.2463
Training Epoch: 18 [35712/50048]	Loss: 1.0379
Training Epoch: 18 [35840/50048]	Loss: 0.9645
Training Epoch: 18 [35968/50048]	Loss: 0.9497
Training Epoch: 18 [36096/50048]	Loss: 1.1319
Training Epoch: 18 [36224/50048]	Loss: 1.1586
Training Epoch: 18 [36352/50048]	Loss: 1.3738
Training Epoch: 18 [36480/50048]	Loss: 1.2088
Training Epoch: 18 [36608/50048]	Loss: 1.1493
Training Epoch: 18 [36736/50048]	Loss: 0.9611
Training Epoch: 18 [36864/50048]	Loss: 1.3185
Training Epoch: 18 [36992/50048]	Loss: 1.2090
Training Epoch: 18 [37120/50048]	Loss: 1.0567
Training Epoch: 18 [37248/50048]	Loss: 1.0852
Training Epoch: 18 [37376/50048]	Loss: 1.1123
Training Epoch: 18 [37504/50048]	Loss: 1.0372
Training Epoch: 18 [37632/50048]	Loss: 1.1184
Training Epoch: 18 [37760/50048]	Loss: 1.1360
Training Epoch: 18 [37888/50048]	Loss: 1.0967
Training Epoch: 18 [38016/50048]	Loss: 1.0900
Training Epoch: 18 [38144/50048]	Loss: 1.3973
Training Epoch: 18 [38272/50048]	Loss: 1.1165
Training Epoch: 18 [38400/50048]	Loss: 1.2187
Training Epoch: 18 [38528/50048]	Loss: 1.0745
Training Epoch: 18 [38656/50048]	Loss: 1.3191
Training Epoch: 18 [38784/50048]	Loss: 1.0597
Training Epoch: 18 [38912/50048]	Loss: 1.1246
Training Epoch: 18 [39040/50048]	Loss: 1.0581
Training Epoch: 18 [39168/50048]	Loss: 1.1678
Training Epoch: 18 [39296/50048]	Loss: 1.2167
Training Epoch: 18 [39424/50048]	Loss: 1.3812
Training Epoch: 18 [39552/50048]	Loss: 1.0727
Training Epoch: 18 [39680/50048]	Loss: 1.1264
Training Epoch: 18 [39808/50048]	Loss: 1.2475
Training Epoch: 18 [39936/50048]	Loss: 0.9251
Training Epoch: 18 [40064/50048]	Loss: 1.0000
Training Epoch: 18 [40192/50048]	Loss: 0.9066
Training Epoch: 18 [40320/50048]	Loss: 1.1588
Training Epoch: 18 [40448/50048]	Loss: 0.9957
Training Epoch: 18 [40576/50048]	Loss: 1.1282
Training Epoch: 18 [40704/50048]	Loss: 1.0096
Training Epoch: 18 [40832/50048]	Loss: 1.0576
Training Epoch: 18 [40960/50048]	Loss: 1.4016
Training Epoch: 18 [41088/50048]	Loss: 1.1637
Training Epoch: 18 [41216/50048]	Loss: 1.0408
Training Epoch: 18 [41344/50048]	Loss: 1.3680
Training Epoch: 18 [41472/50048]	Loss: 1.1323
Training Epoch: 18 [41600/50048]	Loss: 1.0378
Training Epoch: 18 [41728/50048]	Loss: 1.1288
Training Epoch: 18 [41856/50048]	Loss: 1.0305
Training Epoch: 18 [41984/50048]	Loss: 1.2880
Training Epoch: 18 [42112/50048]	Loss: 1.1835
Training Epoch: 18 [42240/50048]	Loss: 1.2544
Training Epoch: 18 [42368/50048]	Loss: 0.9795
Training Epoch: 18 [42496/50048]	Loss: 1.0883
Training Epoch: 18 [42624/50048]	Loss: 1.3554
Training Epoch: 18 [42752/50048]	Loss: 1.0485
Training Epoch: 18 [42880/50048]	Loss: 0.9892
Training Epoch: 18 [43008/50048]	Loss: 1.2607
Training Epoch: 18 [43136/50048]	Loss: 1.2113
Training Epoch: 18 [43264/50048]	Loss: 1.1794
Training Epoch: 18 [43392/50048]	Loss: 1.1800
Training Epoch: 18 [43520/50048]	Loss: 1.0226
Training Epoch: 18 [43648/50048]	Loss: 1.0850
Training Epoch: 18 [43776/50048]	Loss: 1.3028
Training Epoch: 18 [43904/50048]	Loss: 1.2870
Training Epoch: 18 [44032/50048]	Loss: 0.9875
Training Epoch: 18 [44160/50048]	Loss: 1.1265
Training Epoch: 18 [44288/50048]	Loss: 0.9417
Training Epoch: 18 [44416/50048]	Loss: 0.9733
Training Epoch: 18 [44544/50048]	Loss: 1.1347
Training Epoch: 18 [44672/50048]	Loss: 1.1972
Training Epoch: 18 [44800/50048]	Loss: 1.2705
Training Epoch: 18 [44928/50048]	Loss: 1.1819
Training Epoch: 18 [45056/50048]	Loss: 1.0908
Training Epoch: 18 [45184/50048]	Loss: 1.0968
Training Epoch: 18 [45312/50048]	Loss: 1.1689
Training Epoch: 18 [45440/50048]	Loss: 1.1129
Training Epoch: 18 [45568/50048]	Loss: 1.2031
Training Epoch: 18 [45696/50048]	Loss: 0.8582
2022-12-06 06:36:24,112 [ZeusDataLoader(train)] train epoch 19 done: time=86.35 energy=10487.21
2022-12-06 06:36:24,114 [ZeusDataLoader(eval)] Epoch 19 begin.
Training Epoch: 18 [45824/50048]	Loss: 1.3463
Training Epoch: 18 [45952/50048]	Loss: 1.1429
Training Epoch: 18 [46080/50048]	Loss: 1.1824
Training Epoch: 18 [46208/50048]	Loss: 1.1037
Training Epoch: 18 [46336/50048]	Loss: 1.0944
Training Epoch: 18 [46464/50048]	Loss: 1.1465
Training Epoch: 18 [46592/50048]	Loss: 1.0375
Training Epoch: 18 [46720/50048]	Loss: 1.0892
Training Epoch: 18 [46848/50048]	Loss: 1.1887
Training Epoch: 18 [46976/50048]	Loss: 1.2721
Training Epoch: 18 [47104/50048]	Loss: 0.9525
Training Epoch: 18 [47232/50048]	Loss: 1.2311
Training Epoch: 18 [47360/50048]	Loss: 1.0413
Training Epoch: 18 [47488/50048]	Loss: 1.2698
Training Epoch: 18 [47616/50048]	Loss: 0.8648
Training Epoch: 18 [47744/50048]	Loss: 1.2270
Training Epoch: 18 [47872/50048]	Loss: 1.0877
Training Epoch: 18 [48000/50048]	Loss: 1.0702
Training Epoch: 18 [48128/50048]	Loss: 1.0414
Training Epoch: 18 [48256/50048]	Loss: 1.0905
Training Epoch: 18 [48384/50048]	Loss: 1.2363
Training Epoch: 18 [48512/50048]	Loss: 1.0552
Training Epoch: 18 [48640/50048]	Loss: 0.8724
Training Epoch: 18 [48768/50048]	Loss: 0.8551
Training Epoch: 18 [48896/50048]	Loss: 1.1169
Training Epoch: 18 [49024/50048]	Loss: 0.9363
Training Epoch: 18 [49152/50048]	Loss: 1.3164
Training Epoch: 18 [49280/50048]	Loss: 1.2434
Training Epoch: 18 [49408/50048]	Loss: 1.0890
Training Epoch: 18 [49536/50048]	Loss: 1.0029
Training Epoch: 18 [49664/50048]	Loss: 1.1904
Training Epoch: 18 [49792/50048]	Loss: 1.2998
Training Epoch: 18 [49920/50048]	Loss: 1.1510
Training Epoch: 18 [50048/50048]	Loss: 1.4421
2022-12-06 11:36:27.803 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:36:27,856 [ZeusDataLoader(eval)] eval epoch 19 done: time=3.73 energy=453.47
2022-12-06 06:36:27,856 [ZeusDataLoader(train)] Up to epoch 19: time=1713.83, energy=208035.72, cost=253978.01
2022-12-06 06:36:27,856 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:36:27,856 [ZeusDataLoader(train)] Expected next epoch: time=1803.63, energy=218833.73, cost=267234.39
2022-12-06 06:36:27,857 [ZeusDataLoader(train)] Epoch 20 begin.
Validation Epoch: 18, Average loss: 0.0120, Accuracy: 0.5847
2022-12-06 06:36:28,049 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:36:28,050 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:36:28.052 [ZeusMonitor] Monitor started.
2022-12-06 11:36:28.052 [ZeusMonitor] Running indefinitely. 2022-12-06 11:36:28.052 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:36:28.052 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e20+gpu0.power.log
Training Epoch: 19 [128/50048]	Loss: 1.0639
Training Epoch: 19 [256/50048]	Loss: 0.9964
Training Epoch: 19 [384/50048]	Loss: 0.8384
Training Epoch: 19 [512/50048]	Loss: 0.8077
Training Epoch: 19 [640/50048]	Loss: 1.0176
Training Epoch: 19 [768/50048]	Loss: 0.9202
Training Epoch: 19 [896/50048]	Loss: 1.1716
Training Epoch: 19 [1024/50048]	Loss: 0.9100
Training Epoch: 19 [1152/50048]	Loss: 1.0747
Training Epoch: 19 [1280/50048]	Loss: 1.2552
Training Epoch: 19 [1408/50048]	Loss: 0.9893
Training Epoch: 19 [1536/50048]	Loss: 1.0487
Training Epoch: 19 [1664/50048]	Loss: 1.1598
Training Epoch: 19 [1792/50048]	Loss: 1.0982
Training Epoch: 19 [1920/50048]	Loss: 0.9881
Training Epoch: 19 [2048/50048]	Loss: 1.0197
Training Epoch: 19 [2176/50048]	Loss: 0.8409
Training Epoch: 19 [2304/50048]	Loss: 1.0399
Training Epoch: 19 [2432/50048]	Loss: 0.9269
Training Epoch: 19 [2560/50048]	Loss: 1.0246
Training Epoch: 19 [2688/50048]	Loss: 0.9727
Training Epoch: 19 [2816/50048]	Loss: 0.9425
Training Epoch: 19 [2944/50048]	Loss: 0.9702
Training Epoch: 19 [3072/50048]	Loss: 0.7515
Training Epoch: 19 [3200/50048]	Loss: 1.0432
Training Epoch: 19 [3328/50048]	Loss: 1.2174
Training Epoch: 19 [3456/50048]	Loss: 1.2523
Training Epoch: 19 [3584/50048]	Loss: 0.9894
Training Epoch: 19 [3712/50048]	Loss: 1.0699
Training Epoch: 19 [3840/50048]	Loss: 0.8250
Training Epoch: 19 [3968/50048]	Loss: 1.1869
Training Epoch: 19 [4096/50048]	Loss: 1.0955
Training Epoch: 19 [4224/50048]	Loss: 0.9571
Training Epoch: 19 [4352/50048]	Loss: 1.0605
Training Epoch: 19 [4480/50048]	Loss: 0.9730
Training Epoch: 19 [4608/50048]	Loss: 1.0928
Training Epoch: 19 [4736/50048]	Loss: 0.6685
Training Epoch: 19 [4864/50048]	Loss: 1.1663
Training Epoch: 19 [4992/50048]	Loss: 1.0184
Training Epoch: 19 [5120/50048]	Loss: 1.2243
Training Epoch: 19 [5248/50048]	Loss: 0.8315
Training Epoch: 19 [5376/50048]	Loss: 0.9507
Training Epoch: 19 [5504/50048]	Loss: 0.8632
Training Epoch: 19 [5632/50048]	Loss: 1.2796
Training Epoch: 19 [5760/50048]	Loss: 0.9710
Training Epoch: 19 [5888/50048]	Loss: 1.2418
Training Epoch: 19 [6016/50048]	Loss: 1.0811
Training Epoch: 19 [6144/50048]	Loss: 1.0233
Training Epoch: 19 [6272/50048]	Loss: 1.0076
Training Epoch: 19 [6400/50048]	Loss: 1.0144
Training Epoch: 19 [6528/50048]	Loss: 1.0216
Training Epoch: 19 [6656/50048]	Loss: 0.9864
Training Epoch: 19 [6784/50048]	Loss: 0.9266
Training Epoch: 19 [6912/50048]	Loss: 0.9292
Training Epoch: 19 [7040/50048]	Loss: 1.0503
Training Epoch: 19 [7168/50048]	Loss: 0.9440
Training Epoch: 19 [7296/50048]	Loss: 0.8916
Training Epoch: 19 [7424/50048]	Loss: 0.9959
Training Epoch: 19 [7552/50048]	Loss: 1.1913
Training Epoch: 19 [7680/50048]	Loss: 0.9956
Training Epoch: 19 [7808/50048]	Loss: 1.1576
Training Epoch: 19 [7936/50048]	Loss: 0.9591
Training Epoch: 19 [8064/50048]	Loss: 0.8868
Training Epoch: 19 [8192/50048]	Loss: 0.8959
Training Epoch: 19 [8320/50048]	Loss: 1.0740
Training Epoch: 19 [8448/50048]	Loss: 0.7403
Training Epoch: 19 [8576/50048]	Loss: 1.0002
Training Epoch: 19 [8704/50048]	Loss: 1.0371
Training Epoch: 19 [8832/50048]	Loss: 1.1309
Training Epoch: 19 [8960/50048]	Loss: 1.0205
Training Epoch: 19 [9088/50048]	Loss: 1.0756
Training Epoch: 19 [9216/50048]	Loss: 1.0599
Training Epoch: 19 [9344/50048]	Loss: 1.0725
Training Epoch: 19 [9472/50048]	Loss: 1.1254
Training Epoch: 19 [9600/50048]	Loss: 0.9161
Training Epoch: 19 [9728/50048]	Loss: 1.0767
Training Epoch: 19 [9856/50048]	Loss: 1.0459
Training Epoch: 19 [9984/50048]	Loss: 1.0207
Training Epoch: 19 [10112/50048]	Loss: 1.1420
Training Epoch: 19 [10240/50048]	Loss: 1.0539
Training Epoch: 19 [10368/50048]	Loss: 1.1767
Training Epoch: 19 [10496/50048]	Loss: 0.8127
Training Epoch: 19 [10624/50048]	Loss: 1.1413
Training Epoch: 19 [10752/50048]	Loss: 1.1640
Training Epoch: 19 [10880/50048]	Loss: 0.9686
Training Epoch: 19 [11008/50048]	Loss: 0.9324
Training Epoch: 19 [11136/50048]	Loss: 1.1089
Training Epoch: 19 [11264/50048]	Loss: 1.0595
Training Epoch: 19 [11392/50048]	Loss: 0.9051
Training Epoch: 19 [11520/50048]	Loss: 0.9993
Training Epoch: 19 [11648/50048]	Loss: 1.0637
Training Epoch: 19 [11776/50048]	Loss: 0.9754
Training Epoch: 19 [11904/50048]	Loss: 0.8730
Training Epoch: 19 [12032/50048]	Loss: 1.1585
Training Epoch: 19 [12160/50048]	Loss: 1.0103
Training Epoch: 19 [12288/50048]	Loss: 1.0609
Training Epoch: 19 [12416/50048]	Loss: 1.2287
Training Epoch: 19 [12544/50048]	Loss: 1.0621
Training Epoch: 19 [12672/50048]	Loss: 0.9783
Training Epoch: 19 [12800/50048]	Loss: 1.1182
Training Epoch: 19 [12928/50048]	Loss: 0.9772
Training Epoch: 19 [13056/50048]	Loss: 1.1899
Training Epoch: 19 [13184/50048]	Loss: 0.9610
Training Epoch: 19 [13312/50048]	Loss: 0.9881
Training Epoch: 19 [13440/50048]	Loss: 1.0703
Training Epoch: 19 [13568/50048]	Loss: 0.9742
Training Epoch: 19 [13696/50048]	Loss: 1.1217
Training Epoch: 19 [13824/50048]	Loss: 1.0655
Training Epoch: 19 [13952/50048]	Loss: 1.0415
Training Epoch: 19 [14080/50048]	Loss: 1.1798
Training Epoch: 19 [14208/50048]	Loss: 1.0152
Training Epoch: 19 [14336/50048]	Loss: 1.0691
Training Epoch: 19 [14464/50048]	Loss: 0.9450
Training Epoch: 19 [14592/50048]	Loss: 1.0943
Training Epoch: 19 [14720/50048]	Loss: 1.2419
Training Epoch: 19 [14848/50048]	Loss: 1.1822
Training Epoch: 19 [14976/50048]	Loss: 0.9370
Training Epoch: 19 [15104/50048]	Loss: 1.1172
Training Epoch: 19 [15232/50048]	Loss: 1.0256
Training Epoch: 19 [15360/50048]	Loss: 1.1848
Training Epoch: 19 [15488/50048]	Loss: 1.2082
Training Epoch: 19 [15616/50048]	Loss: 0.8369
Training Epoch: 19 [15744/50048]	Loss: 0.9409
Training Epoch: 19 [15872/50048]	Loss: 1.3035
Training Epoch: 19 [16000/50048]	Loss: 1.0472
Training Epoch: 19 [16128/50048]	Loss: 0.9462
Training Epoch: 19 [16256/50048]	Loss: 1.0466
Training Epoch: 19 [16384/50048]	Loss: 0.9825
Training Epoch: 19 [16512/50048]	Loss: 1.2060
Training Epoch: 19 [16640/50048]	Loss: 1.1065
Training Epoch: 19 [16768/50048]	Loss: 0.9966
Training Epoch: 19 [16896/50048]	Loss: 0.9298
Training Epoch: 19 [17024/50048]	Loss: 1.1291
Training Epoch: 19 [17152/50048]	Loss: 1.1814
Training Epoch: 19 [17280/50048]	Loss: 1.0232
Training Epoch: 19 [17408/50048]	Loss: 1.1373
Training Epoch: 19 [17536/50048]	Loss: 0.8602
Training Epoch: 19 [17664/50048]	Loss: 1.0960
Training Epoch: 19 [17792/50048]	Loss: 0.9600
Training Epoch: 19 [17920/50048]	Loss: 1.0558
Training Epoch: 19 [18048/50048]	Loss: 0.8605
Training Epoch: 19 [18176/50048]	Loss: 1.1532
Training Epoch: 19 [18304/50048]	Loss: 1.1348
Training Epoch: 19 [18432/50048]	Loss: 1.2409
Training Epoch: 19 [18560/50048]	Loss: 0.9114
Training Epoch: 19 [18688/50048]	Loss: 1.1859
Training Epoch: 19 [18816/50048]	Loss: 0.9756
Training Epoch: 19 [18944/50048]	Loss: 1.0414
Training Epoch: 19 [19072/50048]	Loss: 0.9158
Training Epoch: 19 [19200/50048]	Loss: 1.0129
Training Epoch: 19 [19328/50048]	Loss: 0.8740
Training Epoch: 19 [19456/50048]	Loss: 0.9580
Training Epoch: 19 [19584/50048]	Loss: 1.1178
Training Epoch: 19 [19712/50048]	Loss: 0.9874
Training Epoch: 19 [19840/50048]	Loss: 1.1164
Training Epoch: 19 [19968/50048]	Loss: 1.3982
Training Epoch: 19 [20096/50048]	Loss: 1.1698
Training Epoch: 19 [20224/50048]	Loss: 1.2148
Training Epoch: 19 [20352/50048]	Loss: 1.0198
Training Epoch: 19 [20480/50048]	Loss: 0.9158
Training Epoch: 19 [20608/50048]	Loss: 1.1111
Training Epoch: 19 [20736/50048]	Loss: 0.8951
Training Epoch: 19 [20864/50048]	Loss: 0.9701
Training Epoch: 19 [20992/50048]	Loss: 1.0157
Training Epoch: 19 [21120/50048]	Loss: 0.8619
Training Epoch: 19 [21248/50048]	Loss: 0.9968
Training Epoch: 19 [21376/50048]	Loss: 1.0801
Training Epoch: 19 [21504/50048]	Loss: 1.1232
Training Epoch: 19 [21632/50048]	Loss: 1.4133
Training Epoch: 19 [21760/50048]	Loss: 1.1417
Training Epoch: 19 [21888/50048]	Loss: 1.2526
Training Epoch: 19 [22016/50048]	Loss: 1.2551
Training Epoch: 19 [22144/50048]	Loss: 1.0594
Training Epoch: 19 [22272/50048]	Loss: 0.9951
Training Epoch: 19 [22400/50048]	Loss: 0.9368
Training Epoch: 19 [22528/50048]	Loss: 0.9724
Training Epoch: 19 [22656/50048]	Loss: 1.3544
Training Epoch: 19 [22784/50048]	Loss: 1.0226
Training Epoch: 19 [22912/50048]	Loss: 1.0807
Training Epoch: 19 [23040/50048]	Loss: 1.0981
Training Epoch: 19 [23168/50048]	Loss: 1.0925
Training Epoch: 19 [23296/50048]	Loss: 1.1961
Training Epoch: 19 [23424/50048]	Loss: 1.4081
Training Epoch: 19 [23552/50048]	Loss: 1.1134
Training Epoch: 19 [23680/50048]	Loss: 0.8645
Training Epoch: 19 [23808/50048]	Loss: 1.0359
Training Epoch: 19 [23936/50048]	Loss: 0.8877
Training Epoch: 19 [24064/50048]	Loss: 1.0806
Training Epoch: 19 [24192/50048]	Loss: 1.0296
Training Epoch: 19 [24320/50048]	Loss: 1.0226
Training Epoch: 19 [24448/50048]	Loss: 1.0448
Training Epoch: 19 [24576/50048]	Loss: 0.8000
Training Epoch: 19 [24704/50048]	Loss: 0.9462
Training Epoch: 19 [24832/50048]	Loss: 0.9656
Training Epoch: 19 [24960/50048]	Loss: 1.0277
Training Epoch: 19 [25088/50048]	Loss: 1.1756
Training Epoch: 19 [25216/50048]	Loss: 1.1989
Training Epoch: 19 [25344/50048]	Loss: 0.8527
Training Epoch: 19 [25472/50048]	Loss: 0.9578
Training Epoch: 19 [25600/50048]	Loss: 1.0417
Training Epoch: 19 [25728/50048]	Loss: 1.2952
Training Epoch: 19 [25856/50048]	Loss: 0.9035
Training Epoch: 19 [25984/50048]	Loss: 1.1152
Training Epoch: 19 [26112/50048]	Loss: 1.1444
Training Epoch: 19 [26240/50048]	Loss: 1.1497
Training Epoch: 19 [26368/50048]	Loss: 0.9542
Training Epoch: 19 [26496/50048]	Loss: 1.0339
Training Epoch: 19 [26624/50048]	Loss: 1.0984
Training Epoch: 19 [26752/50048]	Loss: 0.8673
Training Epoch: 19 [26880/50048]	Loss: 1.2048
Training Epoch: 19 [27008/50048]	Loss: 0.9863
Training Epoch: 19 [27136/50048]	Loss: 1.0437
Training Epoch: 19 [27264/50048]	Loss: 0.7836
Training Epoch: 19 [27392/50048]	Loss: 1.2277
Training Epoch: 19 [27520/50048]	Loss: 0.9615
Training Epoch: 19 [27648/50048]	Loss: 1.0560
Training Epoch: 19 [27776/50048]	Loss: 1.1572
Training Epoch: 19 [27904/50048]	Loss: 1.0600
Training Epoch: 19 [28032/50048]	Loss: 1.2106
Training Epoch: 19 [28160/50048]	Loss: 1.1218
Training Epoch: 19 [28288/50048]	Loss: 0.9423
Training Epoch: 19 [28416/50048]	Loss: 0.7753
Training Epoch: 19 [28544/50048]	Loss: 1.1401
Training Epoch: 19 [28672/50048]	Loss: 1.0275
Training Epoch: 19 [28800/50048]	Loss: 0.9875
Training Epoch: 19 [28928/50048]	Loss: 1.1255
Training Epoch: 19 [29056/50048]	Loss: 1.1239
Training Epoch: 19 [29184/50048]	Loss: 1.1408
Training Epoch: 19 [29312/50048]	Loss: 1.1472
Training Epoch: 19 [29440/50048]	Loss: 1.0837
Training Epoch: 19 [29568/50048]	Loss: 0.9632
Training Epoch: 19 [29696/50048]	Loss: 1.1360
Training Epoch: 19 [29824/50048]	Loss: 1.3341
Training Epoch: 19 [29952/50048]	Loss: 1.0734
Training Epoch: 19 [30080/50048]	Loss: 1.0229
Training Epoch: 19 [30208/50048]	Loss: 1.2604
Training Epoch: 19 [30336/50048]	Loss: 1.0446
Training Epoch: 19 [30464/50048]	Loss: 1.0463
Training Epoch: 19 [30592/50048]	Loss: 0.9211
Training Epoch: 19 [30720/50048]	Loss: 0.9547
Training Epoch: 19 [30848/50048]	Loss: 0.9906
Training Epoch: 19 [30976/50048]	Loss: 1.0566
Training Epoch: 19 [31104/50048]	Loss: 1.3689
Training Epoch: 19 [31232/50048]	Loss: 0.8919
Training Epoch: 19 [31360/50048]	Loss: 1.1192
Training Epoch: 19 [31488/50048]	Loss: 0.9149
Training Epoch: 19 [31616/50048]	Loss: 1.1886
Training Epoch: 19 [31744/50048]	Loss: 1.0982
Training Epoch: 19 [31872/50048]	Loss: 1.2561
Training Epoch: 19 [32000/50048]	Loss: 1.0654
Training Epoch: 19 [32128/50048]	Loss: 1.0466
Training Epoch: 19 [32256/50048]	Loss: 1.0484
Training Epoch: 19 [32384/50048]	Loss: 0.9916
Training Epoch: 19 [32512/50048]	Loss: 1.3970
Training Epoch: 19 [32640/50048]	Loss: 1.2725
Training Epoch: 19 [32768/50048]	Loss: 1.1595
Training Epoch: 19 [32896/50048]	Loss: 1.0100
Training Epoch: 19 [33024/50048]	Loss: 1.3684
Training Epoch: 19 [33152/50048]	Loss: 1.1612
Training Epoch: 19 [33280/50048]	Loss: 1.1289
Training Epoch: 19 [33408/50048]	Loss: 0.9195
Training Epoch: 19 [33536/50048]	Loss: 0.7534
Training Epoch: 19 [33664/50048]	Loss: 1.1707
Training Epoch: 19 [33792/50048]	Loss: 0.8578
Training Epoch: 19 [33920/50048]	Loss: 0.8901
Training Epoch: 19 [34048/50048]	Loss: 1.2538
Training Epoch: 19 [34176/50048]	Loss: 1.3022
Training Epoch: 19 [34304/50048]	Loss: 1.1747
Training Epoch: 19 [34432/50048]	Loss: 0.8393
Training Epoch: 19 [34560/50048]	Loss: 0.9415
Training Epoch: 19 [34688/50048]	Loss: 1.0005
Training Epoch: 19 [34816/50048]	Loss: 1.1216
Training Epoch: 19 [34944/50048]	Loss: 0.9215
Training Epoch: 19 [35072/50048]	Loss: 0.9981
Training Epoch: 19 [35200/50048]	Loss: 1.0594
Training Epoch: 19 [35328/50048]	Loss: 1.0411
Training Epoch: 19 [35456/50048]	Loss: 1.1372
Training Epoch: 19 [35584/50048]	Loss: 1.0767
Training Epoch: 19 [35712/50048]	Loss: 0.9803
Training Epoch: 19 [35840/50048]	Loss: 1.0308
Training Epoch: 19 [35968/50048]	Loss: 1.0768
Training Epoch: 19 [36096/50048]	Loss: 0.9686
Training Epoch: 19 [36224/50048]	Loss: 1.1690
Training Epoch: 19 [36352/50048]	Loss: 1.1249
Training Epoch: 19 [36480/50048]	Loss: 1.0821
Training Epoch: 19 [36608/50048]	Loss: 1.1221
Training Epoch: 19 [36736/50048]	Loss: 1.0188
Training Epoch: 19 [36864/50048]	Loss: 1.1095
Training Epoch: 19 [36992/50048]	Loss: 1.1128
Training Epoch: 19 [37120/50048]	Loss: 1.0296
Training Epoch: 19 [37248/50048]	Loss: 0.9542
Training Epoch: 19 [37376/50048]	Loss: 1.0027
Training Epoch: 19 [37504/50048]	Loss: 0.8655
Training Epoch: 19 [37632/50048]	Loss: 1.0412
Training Epoch: 19 [37760/50048]	Loss: 1.3537
Training Epoch: 19 [37888/50048]	Loss: 1.0439
Training Epoch: 19 [38016/50048]	Loss: 1.2712
Training Epoch: 19 [38144/50048]	Loss: 1.2941
Training Epoch: 19 [38272/50048]	Loss: 0.9064
Training Epoch: 19 [38400/50048]	Loss: 1.0766
Training Epoch: 19 [38528/50048]	Loss: 1.2934
Training Epoch: 19 [38656/50048]	Loss: 0.9053
Training Epoch: 19 [38784/50048]	Loss: 1.3088
Training Epoch: 19 [38912/50048]	Loss: 1.1774
Training Epoch: 19 [39040/50048]	Loss: 1.0297
Training Epoch: 19 [39168/50048]	Loss: 0.8940
Training Epoch: 19 [39296/50048]	Loss: 0.8384
Training Epoch: 19 [39424/50048]	Loss: 1.2463
Training Epoch: 19 [39552/50048]	Loss: 0.9599
Training Epoch: 19 [39680/50048]	Loss: 0.9683
Training Epoch: 19 [39808/50048]	Loss: 1.0252
Training Epoch: 19 [39936/50048]	Loss: 1.1753
Training Epoch: 19 [40064/50048]	Loss: 1.0251
Training Epoch: 19 [40192/50048]	Loss: 1.0113
Training Epoch: 19 [40320/50048]	Loss: 0.9666
Training Epoch: 19 [40448/50048]	Loss: 1.0192
Training Epoch: 19 [40576/50048]	Loss: 1.0829
Training Epoch: 19 [40704/50048]	Loss: 0.9489
Training Epoch: 19 [40832/50048]	Loss: 0.8560
Training Epoch: 19 [40960/50048]	Loss: 1.0945
Training Epoch: 19 [41088/50048]	Loss: 1.1884
Training Epoch: 19 [41216/50048]	Loss: 1.0694
Training Epoch: 19 [41344/50048]	Loss: 0.9392
Training Epoch: 19 [41472/50048]	Loss: 1.1002
Training Epoch: 19 [41600/50048]	Loss: 1.0450
Training Epoch: 19 [41728/50048]	Loss: 0.9656
Training Epoch: 19 [41856/50048]	Loss: 1.2134
Training Epoch: 19 [41984/50048]	Loss: 1.1664
Training Epoch: 19 [42112/50048]	Loss: 1.3477
Training Epoch: 19 [42240/50048]	Loss: 1.2088
Training Epoch: 19 [42368/50048]	Loss: 1.0858
Training Epoch: 19 [42496/50048]	Loss: 1.1361
Training Epoch: 19 [42624/50048]	Loss: 0.8971
Training Epoch: 19 [42752/50048]	Loss: 1.0704
Training Epoch: 19 [42880/50048]	Loss: 1.0378
Training Epoch: 19 [43008/50048]	Loss: 0.9773
Training Epoch: 19 [43136/50048]	Loss: 0.9457
Training Epoch: 19 [43264/50048]	Loss: 1.2463
Training Epoch: 19 [43392/50048]	Loss: 1.0501
Training Epoch: 19 [43520/50048]	Loss: 1.0020
Training Epoch: 19 [43648/50048]	Loss: 1.3125
Training Epoch: 19 [43776/50048]	Loss: 0.9854
Training Epoch: 19 [43904/50048]	Loss: 1.0287
Training Epoch: 19 [44032/50048]	Loss: 1.2929
Training Epoch: 19 [44160/50048]	Loss: 0.9358
Training Epoch: 19 [44288/50048]	Loss: 1.1038
Training Epoch: 19 [44416/50048]	Loss: 0.9739
Training Epoch: 19 [44544/50048]	Loss: 0.8950
Training Epoch: 19 [44672/50048]	Loss: 1.2207
Training Epoch: 19 [44800/50048]	Loss: 1.0491
Training Epoch: 19 [44928/50048]	Loss: 0.9485
Training Epoch: 19 [45056/50048]	Loss: 1.2892
Training Epoch: 19 [45184/50048]	Loss: 0.9718
Training Epoch: 19 [45312/50048]	Loss: 1.1175
Training Epoch: 19 [45440/50048]	Loss: 0.9919
Training Epoch: 19 [45568/50048]	Loss: 1.1493
Training Epoch: 19 [45696/50048]	Loss: 0.9397
2022-12-06 06:37:54,337 [ZeusDataLoader(train)] train epoch 20 done: time=86.47 energy=10502.68
2022-12-06 06:37:54,338 [ZeusDataLoader(eval)] Epoch 20 begin.
Training Epoch: 19 [45824/50048]	Loss: 1.1781
Training Epoch: 19 [45952/50048]	Loss: 1.0456
Training Epoch: 19 [46080/50048]	Loss: 1.0289
Training Epoch: 19 [46208/50048]	Loss: 1.1005
Training Epoch: 19 [46336/50048]	Loss: 1.0300
Training Epoch: 19 [46464/50048]	Loss: 1.1979
Training Epoch: 19 [46592/50048]	Loss: 1.2984
Training Epoch: 19 [46720/50048]	Loss: 0.9942
Training Epoch: 19 [46848/50048]	Loss: 1.0660
Training Epoch: 19 [46976/50048]	Loss: 1.1650
Training Epoch: 19 [47104/50048]	Loss: 1.2590
Training Epoch: 19 [47232/50048]	Loss: 0.8846
Training Epoch: 19 [47360/50048]	Loss: 0.9787
Training Epoch: 19 [47488/50048]	Loss: 1.0747
Training Epoch: 19 [47616/50048]	Loss: 1.1020
Training Epoch: 19 [47744/50048]	Loss: 1.0044
Training Epoch: 19 [47872/50048]	Loss: 0.9194
Training Epoch: 19 [48000/50048]	Loss: 0.8833
Training Epoch: 19 [48128/50048]	Loss: 1.2780
Training Epoch: 19 [48256/50048]	Loss: 1.0220
Training Epoch: 19 [48384/50048]	Loss: 1.0279
Training Epoch: 19 [48512/50048]	Loss: 0.9325
Training Epoch: 19 [48640/50048]	Loss: 1.0376
Training Epoch: 19 [48768/50048]	Loss: 1.2779
Training Epoch: 19 [48896/50048]	Loss: 1.1495
Training Epoch: 19 [49024/50048]	Loss: 1.1080
Training Epoch: 19 [49152/50048]	Loss: 1.0906
Training Epoch: 19 [49280/50048]	Loss: 1.1524
Training Epoch: 19 [49408/50048]	Loss: 1.0393
Training Epoch: 19 [49536/50048]	Loss: 0.8909
Training Epoch: 19 [49664/50048]	Loss: 1.1172
Training Epoch: 19 [49792/50048]	Loss: 0.9340
Training Epoch: 19 [49920/50048]	Loss: 0.9658
Training Epoch: 19 [50048/50048]	Loss: 0.9925
2022-12-06 11:37:58.077 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:37:58,099 [ZeusDataLoader(eval)] eval epoch 20 done: time=3.75 energy=453.73
2022-12-06 06:37:58,100 [ZeusDataLoader(train)] Up to epoch 20: time=1804.05, energy=218992.12, cost=267350.62
2022-12-06 06:37:58,100 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:37:58,100 [ZeusDataLoader(train)] Expected next epoch: time=1893.85, energy=229790.13, cost=280607.00
2022-12-06 06:37:58,101 [ZeusDataLoader(train)] Epoch 21 begin.
Validation Epoch: 19, Average loss: 0.0121, Accuracy: 0.5743
2022-12-06 06:37:58,249 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:37:58,250 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:37:58.253 [ZeusMonitor] Monitor started.
2022-12-06 11:37:58.253 [ZeusMonitor] Running indefinitely. 2022-12-06 11:37:58.253 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:37:58.253 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e21+gpu0.power.log
Training Epoch: 20 [128/50048]	Loss: 0.9062
Training Epoch: 20 [256/50048]	Loss: 1.0429
Training Epoch: 20 [384/50048]	Loss: 0.8705
Training Epoch: 20 [512/50048]	Loss: 0.7877
Training Epoch: 20 [640/50048]	Loss: 0.8185
Training Epoch: 20 [768/50048]	Loss: 1.0301
Training Epoch: 20 [896/50048]	Loss: 0.9863
Training Epoch: 20 [1024/50048]	Loss: 0.9150
Training Epoch: 20 [1152/50048]	Loss: 0.9990
Training Epoch: 20 [1280/50048]	Loss: 1.0681
Training Epoch: 20 [1408/50048]	Loss: 0.9276
Training Epoch: 20 [1536/50048]	Loss: 0.9469
Training Epoch: 20 [1664/50048]	Loss: 1.0840
Training Epoch: 20 [1792/50048]	Loss: 0.8695
Training Epoch: 20 [1920/50048]	Loss: 1.1185
Training Epoch: 20 [2048/50048]	Loss: 1.0743
Training Epoch: 20 [2176/50048]	Loss: 0.8760
Training Epoch: 20 [2304/50048]	Loss: 1.1143
Training Epoch: 20 [2432/50048]	Loss: 0.8161
Training Epoch: 20 [2560/50048]	Loss: 0.8782
Training Epoch: 20 [2688/50048]	Loss: 1.1138
Training Epoch: 20 [2816/50048]	Loss: 1.1231
Training Epoch: 20 [2944/50048]	Loss: 0.9897
Training Epoch: 20 [3072/50048]	Loss: 1.0526
Training Epoch: 20 [3200/50048]	Loss: 1.0228
Training Epoch: 20 [3328/50048]	Loss: 1.0273
Training Epoch: 20 [3456/50048]	Loss: 0.9594
Training Epoch: 20 [3584/50048]	Loss: 0.9112
Training Epoch: 20 [3712/50048]	Loss: 1.0354
Training Epoch: 20 [3840/50048]	Loss: 0.7929
Training Epoch: 20 [3968/50048]	Loss: 0.8887
Training Epoch: 20 [4096/50048]	Loss: 0.8876
Training Epoch: 20 [4224/50048]	Loss: 0.8746
Training Epoch: 20 [4352/50048]	Loss: 0.9623
Training Epoch: 20 [4480/50048]	Loss: 0.9092
Training Epoch: 20 [4608/50048]	Loss: 0.9450
Training Epoch: 20 [4736/50048]	Loss: 1.3522
Training Epoch: 20 [4864/50048]	Loss: 0.9794
Training Epoch: 20 [4992/50048]	Loss: 0.9327
Training Epoch: 20 [5120/50048]	Loss: 0.7802
Training Epoch: 20 [5248/50048]	Loss: 0.9361
Training Epoch: 20 [5376/50048]	Loss: 0.8798
Training Epoch: 20 [5504/50048]	Loss: 0.9598
Training Epoch: 20 [5632/50048]	Loss: 1.2781
Training Epoch: 20 [5760/50048]	Loss: 1.1085
Training Epoch: 20 [5888/50048]	Loss: 0.8762
Training Epoch: 20 [6016/50048]	Loss: 1.0272
Training Epoch: 20 [6144/50048]	Loss: 0.9488
Training Epoch: 20 [6272/50048]	Loss: 0.7996
Training Epoch: 20 [6400/50048]	Loss: 0.8024
Training Epoch: 20 [6528/50048]	Loss: 1.1686
Training Epoch: 20 [6656/50048]	Loss: 0.9818
Training Epoch: 20 [6784/50048]	Loss: 0.8874
Training Epoch: 20 [6912/50048]	Loss: 1.1530
Training Epoch: 20 [7040/50048]	Loss: 0.9686
Training Epoch: 20 [7168/50048]	Loss: 0.9157
Training Epoch: 20 [7296/50048]	Loss: 1.0152
Training Epoch: 20 [7424/50048]	Loss: 0.8793
Training Epoch: 20 [7552/50048]	Loss: 0.9522
Training Epoch: 20 [7680/50048]	Loss: 0.7668
Training Epoch: 20 [7808/50048]	Loss: 1.0549
Training Epoch: 20 [7936/50048]	Loss: 1.2470
Training Epoch: 20 [8064/50048]	Loss: 0.9477
Training Epoch: 20 [8192/50048]	Loss: 0.8344
Training Epoch: 20 [8320/50048]	Loss: 1.0945
Training Epoch: 20 [8448/50048]	Loss: 1.0215
Training Epoch: 20 [8576/50048]	Loss: 1.1495
Training Epoch: 20 [8704/50048]	Loss: 0.8560
Training Epoch: 20 [8832/50048]	Loss: 1.0622
Training Epoch: 20 [8960/50048]	Loss: 0.8649
Training Epoch: 20 [9088/50048]	Loss: 0.9508
Training Epoch: 20 [9216/50048]	Loss: 1.0561
Training Epoch: 20 [9344/50048]	Loss: 0.9884
Training Epoch: 20 [9472/50048]	Loss: 0.9759
Training Epoch: 20 [9600/50048]	Loss: 0.9871
Training Epoch: 20 [9728/50048]	Loss: 0.8805
Training Epoch: 20 [9856/50048]	Loss: 0.9305
Training Epoch: 20 [9984/50048]	Loss: 0.9757
Training Epoch: 20 [10112/50048]	Loss: 1.0014
Training Epoch: 20 [10240/50048]	Loss: 0.8623
Training Epoch: 20 [10368/50048]	Loss: 0.9316
Training Epoch: 20 [10496/50048]	Loss: 0.9665
Training Epoch: 20 [10624/50048]	Loss: 0.9695
Training Epoch: 20 [10752/50048]	Loss: 0.9627
Training Epoch: 20 [10880/50048]	Loss: 1.1845
Training Epoch: 20 [11008/50048]	Loss: 1.0201
Training Epoch: 20 [11136/50048]	Loss: 0.8341
Training Epoch: 20 [11264/50048]	Loss: 0.8203
Training Epoch: 20 [11392/50048]	Loss: 1.0836
Training Epoch: 20 [11520/50048]	Loss: 0.9485
Training Epoch: 20 [11648/50048]	Loss: 0.8435
Training Epoch: 20 [11776/50048]	Loss: 0.8595
Training Epoch: 20 [11904/50048]	Loss: 0.8564
Training Epoch: 20 [12032/50048]	Loss: 1.2001
Training Epoch: 20 [12160/50048]	Loss: 0.9190
Training Epoch: 20 [12288/50048]	Loss: 0.9733
Training Epoch: 20 [12416/50048]	Loss: 1.1173
Training Epoch: 20 [12544/50048]	Loss: 1.0399
Training Epoch: 20 [12672/50048]	Loss: 0.8696
Training Epoch: 20 [12800/50048]	Loss: 0.9807
Training Epoch: 20 [12928/50048]	Loss: 1.0191
Training Epoch: 20 [13056/50048]	Loss: 0.8370
Training Epoch: 20 [13184/50048]	Loss: 0.9797
Training Epoch: 20 [13312/50048]	Loss: 0.8883
Training Epoch: 20 [13440/50048]	Loss: 1.0441
Training Epoch: 20 [13568/50048]	Loss: 0.9581
Training Epoch: 20 [13696/50048]	Loss: 0.8153
Training Epoch: 20 [13824/50048]	Loss: 0.9343
Training Epoch: 20 [13952/50048]	Loss: 1.1904
Training Epoch: 20 [14080/50048]	Loss: 1.0907
Training Epoch: 20 [14208/50048]	Loss: 0.7974
Training Epoch: 20 [14336/50048]	Loss: 0.9424
Training Epoch: 20 [14464/50048]	Loss: 0.9664
Training Epoch: 20 [14592/50048]	Loss: 0.8503
Training Epoch: 20 [14720/50048]	Loss: 0.9012
Training Epoch: 20 [14848/50048]	Loss: 0.8754
Training Epoch: 20 [14976/50048]	Loss: 1.2945
Training Epoch: 20 [15104/50048]	Loss: 1.0994
Training Epoch: 20 [15232/50048]	Loss: 1.0557
Training Epoch: 20 [15360/50048]	Loss: 1.1803
Training Epoch: 20 [15488/50048]	Loss: 1.2697
Training Epoch: 20 [15616/50048]	Loss: 1.1448
Training Epoch: 20 [15744/50048]	Loss: 0.9814
Training Epoch: 20 [15872/50048]	Loss: 1.0711
Training Epoch: 20 [16000/50048]	Loss: 0.8537
Training Epoch: 20 [16128/50048]	Loss: 1.0953
Training Epoch: 20 [16256/50048]	Loss: 1.0823
Training Epoch: 20 [16384/50048]	Loss: 1.0826
Training Epoch: 20 [16512/50048]	Loss: 0.9257
Training Epoch: 20 [16640/50048]	Loss: 0.9140
Training Epoch: 20 [16768/50048]	Loss: 1.1738
Training Epoch: 20 [16896/50048]	Loss: 1.0054
Training Epoch: 20 [17024/50048]	Loss: 1.1242
Training Epoch: 20 [17152/50048]	Loss: 1.1022
Training Epoch: 20 [17280/50048]	Loss: 0.8572
Training Epoch: 20 [17408/50048]	Loss: 0.9129
Training Epoch: 20 [17536/50048]	Loss: 0.9295
Training Epoch: 20 [17664/50048]	Loss: 1.0301
Training Epoch: 20 [17792/50048]	Loss: 0.8601
Training Epoch: 20 [17920/50048]	Loss: 1.2011
Training Epoch: 20 [18048/50048]	Loss: 0.9655
Training Epoch: 20 [18176/50048]	Loss: 1.0638
Training Epoch: 20 [18304/50048]	Loss: 1.0090
Training Epoch: 20 [18432/50048]	Loss: 0.9266
Training Epoch: 20 [18560/50048]	Loss: 0.9478
Training Epoch: 20 [18688/50048]	Loss: 0.9123
Training Epoch: 20 [18816/50048]	Loss: 1.0446
Training Epoch: 20 [18944/50048]	Loss: 1.1203
Training Epoch: 20 [19072/50048]	Loss: 1.1170
Training Epoch: 20 [19200/50048]	Loss: 1.1499
Training Epoch: 20 [19328/50048]	Loss: 0.9693
Training Epoch: 20 [19456/50048]	Loss: 1.2628
Training Epoch: 20 [19584/50048]	Loss: 1.2604
Training Epoch: 20 [19712/50048]	Loss: 1.2177
Training Epoch: 20 [19840/50048]	Loss: 0.8851
Training Epoch: 20 [19968/50048]	Loss: 0.8360
Training Epoch: 20 [20096/50048]	Loss: 0.8005
Training Epoch: 20 [20224/50048]	Loss: 0.9712
Training Epoch: 20 [20352/50048]	Loss: 1.0358
Training Epoch: 20 [20480/50048]	Loss: 0.8032
Training Epoch: 20 [20608/50048]	Loss: 1.0985
Training Epoch: 20 [20736/50048]	Loss: 0.9642
Training Epoch: 20 [20864/50048]	Loss: 1.3642
Training Epoch: 20 [20992/50048]	Loss: 1.0085
Training Epoch: 20 [21120/50048]	Loss: 1.0356
Training Epoch: 20 [21248/50048]	Loss: 1.0260
Training Epoch: 20 [21376/50048]	Loss: 1.0040
Training Epoch: 20 [21504/50048]	Loss: 1.0567
Training Epoch: 20 [21632/50048]	Loss: 1.0164
Training Epoch: 20 [21760/50048]	Loss: 1.0012
Training Epoch: 20 [21888/50048]	Loss: 1.0183
Training Epoch: 20 [22016/50048]	Loss: 1.0202
Training Epoch: 20 [22144/50048]	Loss: 0.8835
Training Epoch: 20 [22272/50048]	Loss: 1.2065
Training Epoch: 20 [22400/50048]	Loss: 1.1192
Training Epoch: 20 [22528/50048]	Loss: 0.9722
Training Epoch: 20 [22656/50048]	Loss: 0.8093
Training Epoch: 20 [22784/50048]	Loss: 0.9053
Training Epoch: 20 [22912/50048]	Loss: 0.9345
Training Epoch: 20 [23040/50048]	Loss: 0.9769
Training Epoch: 20 [23168/50048]	Loss: 1.1173
Training Epoch: 20 [23296/50048]	Loss: 1.0181
Training Epoch: 20 [23424/50048]	Loss: 0.7653
Training Epoch: 20 [23552/50048]	Loss: 0.9949
Training Epoch: 20 [23680/50048]	Loss: 0.9618
Training Epoch: 20 [23808/50048]	Loss: 0.8333
Training Epoch: 20 [23936/50048]	Loss: 1.0646
Training Epoch: 20 [24064/50048]	Loss: 1.1159
Training Epoch: 20 [24192/50048]	Loss: 1.1718
Training Epoch: 20 [24320/50048]	Loss: 1.0648
Training Epoch: 20 [24448/50048]	Loss: 0.9568
Training Epoch: 20 [24576/50048]	Loss: 0.9411
Training Epoch: 20 [24704/50048]	Loss: 0.8096
Training Epoch: 20 [24832/50048]	Loss: 1.2875
Training Epoch: 20 [24960/50048]	Loss: 1.0375
Training Epoch: 20 [25088/50048]	Loss: 1.1095
Training Epoch: 20 [25216/50048]	Loss: 0.9475
Training Epoch: 20 [25344/50048]	Loss: 1.2342
Training Epoch: 20 [25472/50048]	Loss: 1.0037
Training Epoch: 20 [25600/50048]	Loss: 0.9180
Training Epoch: 20 [25728/50048]	Loss: 0.9683
Training Epoch: 20 [25856/50048]	Loss: 1.1209
Training Epoch: 20 [25984/50048]	Loss: 0.9655
Training Epoch: 20 [26112/50048]	Loss: 1.1246
Training Epoch: 20 [26240/50048]	Loss: 1.1198
Training Epoch: 20 [26368/50048]	Loss: 1.0046
Training Epoch: 20 [26496/50048]	Loss: 0.9656
Training Epoch: 20 [26624/50048]	Loss: 0.8338
Training Epoch: 20 [26752/50048]	Loss: 0.8689
Training Epoch: 20 [26880/50048]	Loss: 1.0318
Training Epoch: 20 [27008/50048]	Loss: 1.0064
Training Epoch: 20 [27136/50048]	Loss: 1.0905
Training Epoch: 20 [27264/50048]	Loss: 0.8182
Training Epoch: 20 [27392/50048]	Loss: 1.0409
Training Epoch: 20 [27520/50048]	Loss: 1.0817
Training Epoch: 20 [27648/50048]	Loss: 1.1804
Training Epoch: 20 [27776/50048]	Loss: 1.1963
Training Epoch: 20 [27904/50048]	Loss: 0.9540
Training Epoch: 20 [28032/50048]	Loss: 1.0568
Training Epoch: 20 [28160/50048]	Loss: 1.1142
Training Epoch: 20 [28288/50048]	Loss: 0.9774
Training Epoch: 20 [28416/50048]	Loss: 1.1167
Training Epoch: 20 [28544/50048]	Loss: 1.1643
Training Epoch: 20 [28672/50048]	Loss: 1.0463
Training Epoch: 20 [28800/50048]	Loss: 1.2195
Training Epoch: 20 [28928/50048]	Loss: 1.0795
Training Epoch: 20 [29056/50048]	Loss: 1.1477
Training Epoch: 20 [29184/50048]	Loss: 0.8398
Training Epoch: 20 [29312/50048]	Loss: 0.9814
Training Epoch: 20 [29440/50048]	Loss: 1.0831
Training Epoch: 20 [29568/50048]	Loss: 1.0335
Training Epoch: 20 [29696/50048]	Loss: 1.0087
Training Epoch: 20 [29824/50048]	Loss: 0.9118
Training Epoch: 20 [29952/50048]	Loss: 0.9764
Training Epoch: 20 [30080/50048]	Loss: 0.7953
Training Epoch: 20 [30208/50048]	Loss: 1.1361
Training Epoch: 20 [30336/50048]	Loss: 1.2768
Training Epoch: 20 [30464/50048]	Loss: 1.0274
Training Epoch: 20 [30592/50048]	Loss: 0.9200
Training Epoch: 20 [30720/50048]	Loss: 1.1739
Training Epoch: 20 [30848/50048]	Loss: 0.8981
Training Epoch: 20 [30976/50048]	Loss: 0.9885
Training Epoch: 20 [31104/50048]	Loss: 0.8811
Training Epoch: 20 [31232/50048]	Loss: 0.8720
Training Epoch: 20 [31360/50048]	Loss: 1.3349
Training Epoch: 20 [31488/50048]	Loss: 1.0053
Training Epoch: 20 [31616/50048]	Loss: 0.9269
Training Epoch: 20 [31744/50048]	Loss: 1.0422
Training Epoch: 20 [31872/50048]	Loss: 0.9818
Training Epoch: 20 [32000/50048]	Loss: 1.0918
Training Epoch: 20 [32128/50048]	Loss: 0.9825
Training Epoch: 20 [32256/50048]	Loss: 1.0025
Training Epoch: 20 [32384/50048]	Loss: 1.1127
Training Epoch: 20 [32512/50048]	Loss: 1.0109
Training Epoch: 20 [32640/50048]	Loss: 0.9478
Training Epoch: 20 [32768/50048]	Loss: 1.0285
Training Epoch: 20 [32896/50048]	Loss: 0.8273
Training Epoch: 20 [33024/50048]	Loss: 1.1007
Training Epoch: 20 [33152/50048]	Loss: 1.2896
Training Epoch: 20 [33280/50048]	Loss: 1.0857
Training Epoch: 20 [33408/50048]	Loss: 1.0223
Training Epoch: 20 [33536/50048]	Loss: 0.9288
Training Epoch: 20 [33664/50048]	Loss: 0.9101
Training Epoch: 20 [33792/50048]	Loss: 0.9563
Training Epoch: 20 [33920/50048]	Loss: 1.2191
Training Epoch: 20 [34048/50048]	Loss: 0.8589
Training Epoch: 20 [34176/50048]	Loss: 1.1088
Training Epoch: 20 [34304/50048]	Loss: 0.9731
Training Epoch: 20 [34432/50048]	Loss: 0.8773
Training Epoch: 20 [34560/50048]	Loss: 1.1517
Training Epoch: 20 [34688/50048]	Loss: 1.1242
Training Epoch: 20 [34816/50048]	Loss: 1.1742
Training Epoch: 20 [34944/50048]	Loss: 1.1916
Training Epoch: 20 [35072/50048]	Loss: 0.8438
Training Epoch: 20 [35200/50048]	Loss: 0.9796
Training Epoch: 20 [35328/50048]	Loss: 0.9074
Training Epoch: 20 [35456/50048]	Loss: 1.0166
Training Epoch: 20 [35584/50048]	Loss: 1.0687
Training Epoch: 20 [35712/50048]	Loss: 0.9007
Training Epoch: 20 [35840/50048]	Loss: 0.9990
Training Epoch: 20 [35968/50048]	Loss: 1.0729
Training Epoch: 20 [36096/50048]	Loss: 1.0998
Training Epoch: 20 [36224/50048]	Loss: 1.0699
Training Epoch: 20 [36352/50048]	Loss: 1.0165
Training Epoch: 20 [36480/50048]	Loss: 0.9995
Training Epoch: 20 [36608/50048]	Loss: 1.1660
Training Epoch: 20 [36736/50048]	Loss: 1.1231
Training Epoch: 20 [36864/50048]	Loss: 1.3290
Training Epoch: 20 [36992/50048]	Loss: 1.1014
Training Epoch: 20 [37120/50048]	Loss: 1.0944
Training Epoch: 20 [37248/50048]	Loss: 0.8410
Training Epoch: 20 [37376/50048]	Loss: 1.1353
Training Epoch: 20 [37504/50048]	Loss: 0.9722
Training Epoch: 20 [37632/50048]	Loss: 0.8890
Training Epoch: 20 [37760/50048]	Loss: 0.9399
Training Epoch: 20 [37888/50048]	Loss: 1.1354
Training Epoch: 20 [38016/50048]	Loss: 0.9967
Training Epoch: 20 [38144/50048]	Loss: 0.9201
Training Epoch: 20 [38272/50048]	Loss: 1.0110
Training Epoch: 20 [38400/50048]	Loss: 1.2036
Training Epoch: 20 [38528/50048]	Loss: 1.0690
Training Epoch: 20 [38656/50048]	Loss: 0.7557
Training Epoch: 20 [38784/50048]	Loss: 1.0542
Training Epoch: 20 [38912/50048]	Loss: 1.1114
Training Epoch: 20 [39040/50048]	Loss: 0.9214
Training Epoch: 20 [39168/50048]	Loss: 1.2439
Training Epoch: 20 [39296/50048]	Loss: 0.9802
Training Epoch: 20 [39424/50048]	Loss: 1.2039
Training Epoch: 20 [39552/50048]	Loss: 1.0960
Training Epoch: 20 [39680/50048]	Loss: 1.0502
Training Epoch: 20 [39808/50048]	Loss: 1.0554
Training Epoch: 20 [39936/50048]	Loss: 1.1990
Training Epoch: 20 [40064/50048]	Loss: 1.2729
Training Epoch: 20 [40192/50048]	Loss: 0.8590
Training Epoch: 20 [40320/50048]	Loss: 1.2435
Training Epoch: 20 [40448/50048]	Loss: 1.1089
Training Epoch: 20 [40576/50048]	Loss: 1.0278
Training Epoch: 20 [40704/50048]	Loss: 1.1900
Training Epoch: 20 [40832/50048]	Loss: 1.0652
Training Epoch: 20 [40960/50048]	Loss: 0.7956
Training Epoch: 20 [41088/50048]	Loss: 1.1040
Training Epoch: 20 [41216/50048]	Loss: 1.0733
Training Epoch: 20 [41344/50048]	Loss: 0.9652
Training Epoch: 20 [41472/50048]	Loss: 1.0821
Training Epoch: 20 [41600/50048]	Loss: 1.0391
Training Epoch: 20 [41728/50048]	Loss: 1.1662
Training Epoch: 20 [41856/50048]	Loss: 0.8570
Training Epoch: 20 [41984/50048]	Loss: 1.0851
Training Epoch: 20 [42112/50048]	Loss: 1.1738
Training Epoch: 20 [42240/50048]	Loss: 1.0476
Training Epoch: 20 [42368/50048]	Loss: 0.9018
Training Epoch: 20 [42496/50048]	Loss: 1.0755
Training Epoch: 20 [42624/50048]	Loss: 0.9610
Training Epoch: 20 [42752/50048]	Loss: 0.9712
Training Epoch: 20 [42880/50048]	Loss: 1.1315
Training Epoch: 20 [43008/50048]	Loss: 1.3143
Training Epoch: 20 [43136/50048]	Loss: 0.7184
Training Epoch: 20 [43264/50048]	Loss: 1.2329
Training Epoch: 20 [43392/50048]	Loss: 0.9292
Training Epoch: 20 [43520/50048]	Loss: 0.8436
Training Epoch: 20 [43648/50048]	Loss: 1.2408
Training Epoch: 20 [43776/50048]	Loss: 1.0892
Training Epoch: 20 [43904/50048]	Loss: 0.8754
Training Epoch: 20 [44032/50048]	Loss: 0.9893
Training Epoch: 20 [44160/50048]	Loss: 1.0905
Training Epoch: 20 [44288/50048]	Loss: 1.3527
Training Epoch: 20 [44416/50048]	Loss: 0.9118
Training Epoch: 20 [44544/50048]	Loss: 0.9383
Training Epoch: 20 [44672/50048]	Loss: 0.9192
Training Epoch: 20 [44800/50048]	Loss: 1.0290
Training Epoch: 20 [44928/50048]	Loss: 1.0830
Training Epoch: 20 [45056/50048]	Loss: 0.9548
Training Epoch: 20 [45184/50048]	Loss: 1.0562
Training Epoch: 20 [45312/50048]	Loss: 1.0088
Training Epoch: 20 [45440/50048]	Loss: 1.0793
Training Epoch: 20 [45568/50048]	Loss: 0.8543
Training Epoch: 20 [45696/50048]	Loss: 0.9221
2022-12-06 06:39:24,650 [ZeusDataLoader(train)] train epoch 21 done: time=86.54 energy=10502.10
2022-12-06 06:39:24,651 [ZeusDataLoader(eval)] Epoch 21 begin.
Training Epoch: 20 [45824/50048]	Loss: 0.8994
Training Epoch: 20 [45952/50048]	Loss: 0.9416
Training Epoch: 20 [46080/50048]	Loss: 1.0125
Training Epoch: 20 [46208/50048]	Loss: 1.0902
Training Epoch: 20 [46336/50048]	Loss: 1.1923
Training Epoch: 20 [46464/50048]	Loss: 0.9804
Training Epoch: 20 [46592/50048]	Loss: 1.1214
Training Epoch: 20 [46720/50048]	Loss: 1.0946
Training Epoch: 20 [46848/50048]	Loss: 0.9673
Training Epoch: 20 [46976/50048]	Loss: 0.8566
Training Epoch: 20 [47104/50048]	Loss: 1.0653
Training Epoch: 20 [47232/50048]	Loss: 1.2316
Training Epoch: 20 [47360/50048]	Loss: 0.8359
Training Epoch: 20 [47488/50048]	Loss: 0.9615
Training Epoch: 20 [47616/50048]	Loss: 0.9254
Training Epoch: 20 [47744/50048]	Loss: 0.9817
Training Epoch: 20 [47872/50048]	Loss: 1.2677
Training Epoch: 20 [48000/50048]	Loss: 1.0669
Training Epoch: 20 [48128/50048]	Loss: 0.9563
Training Epoch: 20 [48256/50048]	Loss: 0.9801
Training Epoch: 20 [48384/50048]	Loss: 1.0394
Training Epoch: 20 [48512/50048]	Loss: 0.9557
Training Epoch: 20 [48640/50048]	Loss: 0.9308
Training Epoch: 20 [48768/50048]	Loss: 1.0718
Training Epoch: 20 [48896/50048]	Loss: 1.3096
Training Epoch: 20 [49024/50048]	Loss: 1.0675
Training Epoch: 20 [49152/50048]	Loss: 1.1373
Training Epoch: 20 [49280/50048]	Loss: 1.1001
Training Epoch: 20 [49408/50048]	Loss: 1.0011
Training Epoch: 20 [49536/50048]	Loss: 0.9749
Training Epoch: 20 [49664/50048]	Loss: 1.0387
Training Epoch: 20 [49792/50048]	Loss: 1.2317
Training Epoch: 20 [49920/50048]	Loss: 1.0961
Training Epoch: 20 [50048/50048]	Loss: 1.1034
2022-12-06 11:39:28.364 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:39:28,422 [ZeusDataLoader(eval)] eval epoch 21 done: time=3.76 energy=452.72
2022-12-06 06:39:28,422 [ZeusDataLoader(train)] Up to epoch 21: time=1894.35, energy=229946.93, cost=280729.32
2022-12-06 06:39:28,422 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:39:28,422 [ZeusDataLoader(train)] Expected next epoch: time=1984.15, energy=240744.95, cost=293985.70
2022-12-06 06:39:28,423 [ZeusDataLoader(train)] Epoch 22 begin.
Validation Epoch: 20, Average loss: 0.0119, Accuracy: 0.5920
2022-12-06 06:39:28,608 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:39:28,609 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:39:28.610 [ZeusMonitor] Monitor started.
2022-12-06 11:39:28.610 [ZeusMonitor] Running indefinitely. 2022-12-06 11:39:28.610 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:39:28.610 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e22+gpu0.power.log
Training Epoch: 21 [128/50048]	Loss: 0.8747
Training Epoch: 21 [256/50048]	Loss: 0.9694
Training Epoch: 21 [384/50048]	Loss: 0.9709
Training Epoch: 21 [512/50048]	Loss: 0.9970
Training Epoch: 21 [640/50048]	Loss: 1.0453
Training Epoch: 21 [768/50048]	Loss: 0.8837
Training Epoch: 21 [896/50048]	Loss: 0.9876
Training Epoch: 21 [1024/50048]	Loss: 0.9445
Training Epoch: 21 [1152/50048]	Loss: 0.8349
Training Epoch: 21 [1280/50048]	Loss: 0.7704
Training Epoch: 21 [1408/50048]	Loss: 0.8284
Training Epoch: 21 [1536/50048]	Loss: 0.8385
Training Epoch: 21 [1664/50048]	Loss: 0.9906
Training Epoch: 21 [1792/50048]	Loss: 1.0163
Training Epoch: 21 [1920/50048]	Loss: 0.8759
Training Epoch: 21 [2048/50048]	Loss: 0.9858
Training Epoch: 21 [2176/50048]	Loss: 0.7506
Training Epoch: 21 [2304/50048]	Loss: 0.8499
Training Epoch: 21 [2432/50048]	Loss: 1.0364
Training Epoch: 21 [2560/50048]	Loss: 0.7493
Training Epoch: 21 [2688/50048]	Loss: 0.8692
Training Epoch: 21 [2816/50048]	Loss: 0.9178
Training Epoch: 21 [2944/50048]	Loss: 0.9175
Training Epoch: 21 [3072/50048]	Loss: 0.8754
Training Epoch: 21 [3200/50048]	Loss: 1.1470
Training Epoch: 21 [3328/50048]	Loss: 0.8181
Training Epoch: 21 [3456/50048]	Loss: 0.8345
Training Epoch: 21 [3584/50048]	Loss: 0.9084
Training Epoch: 21 [3712/50048]	Loss: 1.0332
Training Epoch: 21 [3840/50048]	Loss: 0.9013
Training Epoch: 21 [3968/50048]	Loss: 0.7540
Training Epoch: 21 [4096/50048]	Loss: 1.0583
Training Epoch: 21 [4224/50048]	Loss: 0.8412
Training Epoch: 21 [4352/50048]	Loss: 0.9535
Training Epoch: 21 [4480/50048]	Loss: 0.9066
Training Epoch: 21 [4608/50048]	Loss: 0.9143
Training Epoch: 21 [4736/50048]	Loss: 0.8038
Training Epoch: 21 [4864/50048]	Loss: 0.6550
Training Epoch: 21 [4992/50048]	Loss: 0.7221
Training Epoch: 21 [5120/50048]	Loss: 1.0515
Training Epoch: 21 [5248/50048]	Loss: 0.9047
Training Epoch: 21 [5376/50048]	Loss: 0.7019
Training Epoch: 21 [5504/50048]	Loss: 0.9661
Training Epoch: 21 [5632/50048]	Loss: 0.9011
Training Epoch: 21 [5760/50048]	Loss: 0.8981
Training Epoch: 21 [5888/50048]	Loss: 0.9089
Training Epoch: 21 [6016/50048]	Loss: 0.7706
Training Epoch: 21 [6144/50048]	Loss: 0.8744
Training Epoch: 21 [6272/50048]	Loss: 0.7306
Training Epoch: 21 [6400/50048]	Loss: 0.8694
Training Epoch: 21 [6528/50048]	Loss: 0.8660
Training Epoch: 21 [6656/50048]	Loss: 0.8141
Training Epoch: 21 [6784/50048]	Loss: 0.7731
Training Epoch: 21 [6912/50048]	Loss: 0.7624
Training Epoch: 21 [7040/50048]	Loss: 0.9754
Training Epoch: 21 [7168/50048]	Loss: 0.8080
Training Epoch: 21 [7296/50048]	Loss: 0.8587
Training Epoch: 21 [7424/50048]	Loss: 0.8950
Training Epoch: 21 [7552/50048]	Loss: 0.8965
Training Epoch: 21 [7680/50048]	Loss: 0.8975
Training Epoch: 21 [7808/50048]	Loss: 1.1185
Training Epoch: 21 [7936/50048]	Loss: 1.0593
Training Epoch: 21 [8064/50048]	Loss: 0.8081
Training Epoch: 21 [8192/50048]	Loss: 1.0899
Training Epoch: 21 [8320/50048]	Loss: 1.0350
Training Epoch: 21 [8448/50048]	Loss: 1.0208
Training Epoch: 21 [8576/50048]	Loss: 0.8703
Training Epoch: 21 [8704/50048]	Loss: 1.0258
Training Epoch: 21 [8832/50048]	Loss: 0.9856
Training Epoch: 21 [8960/50048]	Loss: 1.0019
Training Epoch: 21 [9088/50048]	Loss: 0.8639
Training Epoch: 21 [9216/50048]	Loss: 0.9515
Training Epoch: 21 [9344/50048]	Loss: 0.9682
Training Epoch: 21 [9472/50048]	Loss: 0.7139
Training Epoch: 21 [9600/50048]	Loss: 0.9121
Training Epoch: 21 [9728/50048]	Loss: 1.0242
Training Epoch: 21 [9856/50048]	Loss: 1.1041
Training Epoch: 21 [9984/50048]	Loss: 0.9363
Training Epoch: 21 [10112/50048]	Loss: 1.1640
Training Epoch: 21 [10240/50048]	Loss: 0.8406
Training Epoch: 21 [10368/50048]	Loss: 0.8052
Training Epoch: 21 [10496/50048]	Loss: 0.9230
Training Epoch: 21 [10624/50048]	Loss: 0.9404
Training Epoch: 21 [10752/50048]	Loss: 0.9482
Training Epoch: 21 [10880/50048]	Loss: 0.9602
Training Epoch: 21 [11008/50048]	Loss: 1.0033
Training Epoch: 21 [11136/50048]	Loss: 0.8984
Training Epoch: 21 [11264/50048]	Loss: 1.0285
Training Epoch: 21 [11392/50048]	Loss: 0.8185
Training Epoch: 21 [11520/50048]	Loss: 0.8341
Training Epoch: 21 [11648/50048]	Loss: 1.1463
Training Epoch: 21 [11776/50048]	Loss: 0.9912
Training Epoch: 21 [11904/50048]	Loss: 0.7981
Training Epoch: 21 [12032/50048]	Loss: 0.9349
Training Epoch: 21 [12160/50048]	Loss: 0.8824
Training Epoch: 21 [12288/50048]	Loss: 0.8888
Training Epoch: 21 [12416/50048]	Loss: 0.9543
Training Epoch: 21 [12544/50048]	Loss: 0.9932
Training Epoch: 21 [12672/50048]	Loss: 0.9811
Training Epoch: 21 [12800/50048]	Loss: 1.1579
Training Epoch: 21 [12928/50048]	Loss: 0.8547
Training Epoch: 21 [13056/50048]	Loss: 0.8949
Training Epoch: 21 [13184/50048]	Loss: 0.8884
Training Epoch: 21 [13312/50048]	Loss: 0.8391
Training Epoch: 21 [13440/50048]	Loss: 1.0670
Training Epoch: 21 [13568/50048]	Loss: 1.1013
Training Epoch: 21 [13696/50048]	Loss: 0.9123
Training Epoch: 21 [13824/50048]	Loss: 0.9182
Training Epoch: 21 [13952/50048]	Loss: 0.9570
Training Epoch: 21 [14080/50048]	Loss: 1.0783
Training Epoch: 21 [14208/50048]	Loss: 1.1281
Training Epoch: 21 [14336/50048]	Loss: 0.9839
Training Epoch: 21 [14464/50048]	Loss: 0.9693
Training Epoch: 21 [14592/50048]	Loss: 0.8649
Training Epoch: 21 [14720/50048]	Loss: 0.9319
Training Epoch: 21 [14848/50048]	Loss: 0.9340
Training Epoch: 21 [14976/50048]	Loss: 0.9867
Training Epoch: 21 [15104/50048]	Loss: 1.1068
Training Epoch: 21 [15232/50048]	Loss: 0.8453
Training Epoch: 21 [15360/50048]	Loss: 0.9369
Training Epoch: 21 [15488/50048]	Loss: 0.9361
Training Epoch: 21 [15616/50048]	Loss: 0.8607
Training Epoch: 21 [15744/50048]	Loss: 1.0820
Training Epoch: 21 [15872/50048]	Loss: 1.0296
Training Epoch: 21 [16000/50048]	Loss: 1.2514
Training Epoch: 21 [16128/50048]	Loss: 0.7735
Training Epoch: 21 [16256/50048]	Loss: 1.0807
Training Epoch: 21 [16384/50048]	Loss: 0.9121
Training Epoch: 21 [16512/50048]	Loss: 0.9695
Training Epoch: 21 [16640/50048]	Loss: 0.8611
Training Epoch: 21 [16768/50048]	Loss: 1.0496
Training Epoch: 21 [16896/50048]	Loss: 0.8082
Training Epoch: 21 [17024/50048]	Loss: 0.8987
Training Epoch: 21 [17152/50048]	Loss: 0.8875
Training Epoch: 21 [17280/50048]	Loss: 1.0809
Training Epoch: 21 [17408/50048]	Loss: 0.9415
Training Epoch: 21 [17536/50048]	Loss: 0.7659
Training Epoch: 21 [17664/50048]	Loss: 1.0726
Training Epoch: 21 [17792/50048]	Loss: 0.9010
Training Epoch: 21 [17920/50048]	Loss: 0.8637
Training Epoch: 21 [18048/50048]	Loss: 0.9746
Training Epoch: 21 [18176/50048]	Loss: 0.9503
Training Epoch: 21 [18304/50048]	Loss: 0.7591
Training Epoch: 21 [18432/50048]	Loss: 0.9164
Training Epoch: 21 [18560/50048]	Loss: 1.0267
Training Epoch: 21 [18688/50048]	Loss: 0.7921
Training Epoch: 21 [18816/50048]	Loss: 1.0931
Training Epoch: 21 [18944/50048]	Loss: 0.9389
Training Epoch: 21 [19072/50048]	Loss: 1.0296
Training Epoch: 21 [19200/50048]	Loss: 0.9619
Training Epoch: 21 [19328/50048]	Loss: 1.0822
Training Epoch: 21 [19456/50048]	Loss: 0.9419
Training Epoch: 21 [19584/50048]	Loss: 0.9049
Training Epoch: 21 [19712/50048]	Loss: 0.9927
Training Epoch: 21 [19840/50048]	Loss: 1.0117
Training Epoch: 21 [19968/50048]	Loss: 0.9758
Training Epoch: 21 [20096/50048]	Loss: 0.9485
Training Epoch: 21 [20224/50048]	Loss: 0.9597
Training Epoch: 21 [20352/50048]	Loss: 0.9712
Training Epoch: 21 [20480/50048]	Loss: 1.1160
Training Epoch: 21 [20608/50048]	Loss: 1.1116
Training Epoch: 21 [20736/50048]	Loss: 0.9627
Training Epoch: 21 [20864/50048]	Loss: 1.0982
Training Epoch: 21 [20992/50048]	Loss: 0.9627
Training Epoch: 21 [21120/50048]	Loss: 1.0808
Training Epoch: 21 [21248/50048]	Loss: 0.8351
Training Epoch: 21 [21376/50048]	Loss: 0.9158
Training Epoch: 21 [21504/50048]	Loss: 1.1765
Training Epoch: 21 [21632/50048]	Loss: 0.8781
Training Epoch: 21 [21760/50048]	Loss: 1.2036
Training Epoch: 21 [21888/50048]	Loss: 1.1433
Training Epoch: 21 [22016/50048]	Loss: 1.0865
Training Epoch: 21 [22144/50048]	Loss: 1.0973
Training Epoch: 21 [22272/50048]	Loss: 0.9488
Training Epoch: 21 [22400/50048]	Loss: 0.7436
Training Epoch: 21 [22528/50048]	Loss: 0.8436
Training Epoch: 21 [22656/50048]	Loss: 0.9421
Training Epoch: 21 [22784/50048]	Loss: 0.7855
Training Epoch: 21 [22912/50048]	Loss: 0.9935
Training Epoch: 21 [23040/50048]	Loss: 1.0263
Training Epoch: 21 [23168/50048]	Loss: 1.0458
Training Epoch: 21 [23296/50048]	Loss: 0.8173
Training Epoch: 21 [23424/50048]	Loss: 0.8111
Training Epoch: 21 [23552/50048]	Loss: 0.8829
Training Epoch: 21 [23680/50048]	Loss: 1.0469
Training Epoch: 21 [23808/50048]	Loss: 0.9387
Training Epoch: 21 [23936/50048]	Loss: 0.8299
Training Epoch: 21 [24064/50048]	Loss: 0.8631
Training Epoch: 21 [24192/50048]	Loss: 0.9585
Training Epoch: 21 [24320/50048]	Loss: 0.8824
Training Epoch: 21 [24448/50048]	Loss: 0.9835
Training Epoch: 21 [24576/50048]	Loss: 1.0451
Training Epoch: 21 [24704/50048]	Loss: 1.1352
Training Epoch: 21 [24832/50048]	Loss: 1.0559
Training Epoch: 21 [24960/50048]	Loss: 0.9495
Training Epoch: 21 [25088/50048]	Loss: 0.8279
Training Epoch: 21 [25216/50048]	Loss: 0.9079
Training Epoch: 21 [25344/50048]	Loss: 0.9135
Training Epoch: 21 [25472/50048]	Loss: 1.0856
Training Epoch: 21 [25600/50048]	Loss: 0.9317
Training Epoch: 21 [25728/50048]	Loss: 0.9218
Training Epoch: 21 [25856/50048]	Loss: 1.1517
Training Epoch: 21 [25984/50048]	Loss: 0.9189
Training Epoch: 21 [26112/50048]	Loss: 1.2060
Training Epoch: 21 [26240/50048]	Loss: 0.9028
Training Epoch: 21 [26368/50048]	Loss: 0.8569
Training Epoch: 21 [26496/50048]	Loss: 0.9029
Training Epoch: 21 [26624/50048]	Loss: 0.7526
Training Epoch: 21 [26752/50048]	Loss: 0.9758
Training Epoch: 21 [26880/50048]	Loss: 1.0371
Training Epoch: 21 [27008/50048]	Loss: 0.9960
Training Epoch: 21 [27136/50048]	Loss: 0.7648
Training Epoch: 21 [27264/50048]	Loss: 1.0561
Training Epoch: 21 [27392/50048]	Loss: 1.1438
Training Epoch: 21 [27520/50048]	Loss: 0.9067
Training Epoch: 21 [27648/50048]	Loss: 1.1443
Training Epoch: 21 [27776/50048]	Loss: 0.8898
Training Epoch: 21 [27904/50048]	Loss: 1.0109
Training Epoch: 21 [28032/50048]	Loss: 0.8725
Training Epoch: 21 [28160/50048]	Loss: 0.9525
Training Epoch: 21 [28288/50048]	Loss: 0.8412
Training Epoch: 21 [28416/50048]	Loss: 0.8316
Training Epoch: 21 [28544/50048]	Loss: 1.1294
Training Epoch: 21 [28672/50048]	Loss: 1.1764
Training Epoch: 21 [28800/50048]	Loss: 1.2649
Training Epoch: 21 [28928/50048]	Loss: 0.8369
Training Epoch: 21 [29056/50048]	Loss: 1.0968
Training Epoch: 21 [29184/50048]	Loss: 1.1580
Training Epoch: 21 [29312/50048]	Loss: 0.8369
Training Epoch: 21 [29440/50048]	Loss: 1.0149
Training Epoch: 21 [29568/50048]	Loss: 0.9275
Training Epoch: 21 [29696/50048]	Loss: 1.2535
Training Epoch: 21 [29824/50048]	Loss: 1.0267
Training Epoch: 21 [29952/50048]	Loss: 1.2527
Training Epoch: 21 [30080/50048]	Loss: 0.8721
Training Epoch: 21 [30208/50048]	Loss: 0.9487
Training Epoch: 21 [30336/50048]	Loss: 0.8401
Training Epoch: 21 [30464/50048]	Loss: 0.8703
Training Epoch: 21 [30592/50048]	Loss: 0.9596
Training Epoch: 21 [30720/50048]	Loss: 1.0581
Training Epoch: 21 [30848/50048]	Loss: 0.9765
Training Epoch: 21 [30976/50048]	Loss: 1.1619
Training Epoch: 21 [31104/50048]	Loss: 1.1204
Training Epoch: 21 [31232/50048]	Loss: 1.0260
Training Epoch: 21 [31360/50048]	Loss: 1.1321
Training Epoch: 21 [31488/50048]	Loss: 0.8723
Training Epoch: 21 [31616/50048]	Loss: 0.9020
Training Epoch: 21 [31744/50048]	Loss: 1.0544
Training Epoch: 21 [31872/50048]	Loss: 0.9554
Training Epoch: 21 [32000/50048]	Loss: 1.1490
Training Epoch: 21 [32128/50048]	Loss: 1.0206
Training Epoch: 21 [32256/50048]	Loss: 1.0990
Training Epoch: 21 [32384/50048]	Loss: 1.0045
Training Epoch: 21 [32512/50048]	Loss: 1.1786
Training Epoch: 21 [32640/50048]	Loss: 0.9417
Training Epoch: 21 [32768/50048]	Loss: 0.8812
Training Epoch: 21 [32896/50048]	Loss: 0.7925
Training Epoch: 21 [33024/50048]	Loss: 0.8617
Training Epoch: 21 [33152/50048]	Loss: 1.1674
Training Epoch: 21 [33280/50048]	Loss: 0.9132
Training Epoch: 21 [33408/50048]	Loss: 0.8862
Training Epoch: 21 [33536/50048]	Loss: 0.8716
Training Epoch: 21 [33664/50048]	Loss: 0.9313
Training Epoch: 21 [33792/50048]	Loss: 1.0222
Training Epoch: 21 [33920/50048]	Loss: 0.9209
Training Epoch: 21 [34048/50048]	Loss: 0.9339
Training Epoch: 21 [34176/50048]	Loss: 1.0547
Training Epoch: 21 [34304/50048]	Loss: 0.8612
Training Epoch: 21 [34432/50048]	Loss: 0.7735
Training Epoch: 21 [34560/50048]	Loss: 0.8845
Training Epoch: 21 [34688/50048]	Loss: 0.8792
Training Epoch: 21 [34816/50048]	Loss: 0.9817
Training Epoch: 21 [34944/50048]	Loss: 0.8644
Training Epoch: 21 [35072/50048]	Loss: 1.0648
Training Epoch: 21 [35200/50048]	Loss: 1.0456
Training Epoch: 21 [35328/50048]	Loss: 0.8032
Training Epoch: 21 [35456/50048]	Loss: 1.1058
Training Epoch: 21 [35584/50048]	Loss: 0.9403
Training Epoch: 21 [35712/50048]	Loss: 0.8890
Training Epoch: 21 [35840/50048]	Loss: 1.0107
Training Epoch: 21 [35968/50048]	Loss: 0.8071
Training Epoch: 21 [36096/50048]	Loss: 0.9368
Training Epoch: 21 [36224/50048]	Loss: 1.0340
Training Epoch: 21 [36352/50048]	Loss: 0.9574
Training Epoch: 21 [36480/50048]	Loss: 0.8801
Training Epoch: 21 [36608/50048]	Loss: 0.9483
Training Epoch: 21 [36736/50048]	Loss: 1.0702
Training Epoch: 21 [36864/50048]	Loss: 0.9149
Training Epoch: 21 [36992/50048]	Loss: 1.0670
Training Epoch: 21 [37120/50048]	Loss: 1.2486
Training Epoch: 21 [37248/50048]	Loss: 1.0640
Training Epoch: 21 [37376/50048]	Loss: 1.0599
Training Epoch: 21 [37504/50048]	Loss: 1.0164
Training Epoch: 21 [37632/50048]	Loss: 1.1641
Training Epoch: 21 [37760/50048]	Loss: 1.0820
Training Epoch: 21 [37888/50048]	Loss: 0.9066
Training Epoch: 21 [38016/50048]	Loss: 0.9094
Training Epoch: 21 [38144/50048]	Loss: 1.0502
Training Epoch: 21 [38272/50048]	Loss: 0.8578
Training Epoch: 21 [38400/50048]	Loss: 1.0876
Training Epoch: 21 [38528/50048]	Loss: 0.9883
Training Epoch: 21 [38656/50048]	Loss: 1.0264
Training Epoch: 21 [38784/50048]	Loss: 0.9493
Training Epoch: 21 [38912/50048]	Loss: 1.2120
Training Epoch: 21 [39040/50048]	Loss: 0.9189
Training Epoch: 21 [39168/50048]	Loss: 0.9533
Training Epoch: 21 [39296/50048]	Loss: 1.0956
Training Epoch: 21 [39424/50048]	Loss: 1.1165
Training Epoch: 21 [39552/50048]	Loss: 1.0644
Training Epoch: 21 [39680/50048]	Loss: 0.9875
Training Epoch: 21 [39808/50048]	Loss: 0.9876
Training Epoch: 21 [39936/50048]	Loss: 1.0975
Training Epoch: 21 [40064/50048]	Loss: 1.0420
Training Epoch: 21 [40192/50048]	Loss: 0.9941
Training Epoch: 21 [40320/50048]	Loss: 0.6889
Training Epoch: 21 [40448/50048]	Loss: 0.8659
Training Epoch: 21 [40576/50048]	Loss: 1.1563
Training Epoch: 21 [40704/50048]	Loss: 0.9426
Training Epoch: 21 [40832/50048]	Loss: 0.9480
Training Epoch: 21 [40960/50048]	Loss: 1.0393
Training Epoch: 21 [41088/50048]	Loss: 1.1253
Training Epoch: 21 [41216/50048]	Loss: 0.9401
Training Epoch: 21 [41344/50048]	Loss: 1.0792
Training Epoch: 21 [41472/50048]	Loss: 0.9118
Training Epoch: 21 [41600/50048]	Loss: 1.0473
Training Epoch: 21 [41728/50048]	Loss: 0.9537
Training Epoch: 21 [41856/50048]	Loss: 1.1935
Training Epoch: 21 [41984/50048]	Loss: 1.0997
Training Epoch: 21 [42112/50048]	Loss: 0.9841
Training Epoch: 21 [42240/50048]	Loss: 1.0732
Training Epoch: 21 [42368/50048]	Loss: 0.9994
Training Epoch: 21 [42496/50048]	Loss: 0.9664
Training Epoch: 21 [42624/50048]	Loss: 0.8784
Training Epoch: 21 [42752/50048]	Loss: 0.9471
Training Epoch: 21 [42880/50048]	Loss: 0.8939
Training Epoch: 21 [43008/50048]	Loss: 1.2841
Training Epoch: 21 [43136/50048]	Loss: 1.2562
Training Epoch: 21 [43264/50048]	Loss: 1.0539
Training Epoch: 21 [43392/50048]	Loss: 1.1266
Training Epoch: 21 [43520/50048]	Loss: 0.9816
Training Epoch: 21 [43648/50048]	Loss: 1.0535
Training Epoch: 21 [43776/50048]	Loss: 1.0041
Training Epoch: 21 [43904/50048]	Loss: 0.8158
Training Epoch: 21 [44032/50048]	Loss: 1.1165
Training Epoch: 21 [44160/50048]	Loss: 1.1761
Training Epoch: 21 [44288/50048]	Loss: 0.8828
Training Epoch: 21 [44416/50048]	Loss: 1.0235
Training Epoch: 21 [44544/50048]	Loss: 0.9134
Training Epoch: 21 [44672/50048]	Loss: 0.9929
Training Epoch: 21 [44800/50048]	Loss: 1.0462
Training Epoch: 21 [44928/50048]	Loss: 0.8884
Training Epoch: 21 [45056/50048]	Loss: 1.0954
Training Epoch: 21 [45184/50048]	Loss: 0.8131
Training Epoch: 21 [45312/50048]	Loss: 1.0220
Training Epoch: 21 [45440/50048]	Loss: 0.9453
Training Epoch: 21 [45568/50048]	Loss: 0.9684
Training Epoch: 21 [45696/50048]	Loss: 0.9512
2022-12-06 06:40:54,923 [ZeusDataLoader(train)] train epoch 22 done: time=86.49 energy=10504.12
2022-12-06 06:40:54,924 [ZeusDataLoader(eval)] Epoch 22 begin.
Training Epoch: 21 [45824/50048]	Loss: 0.9492
Training Epoch: 21 [45952/50048]	Loss: 0.7694
Training Epoch: 21 [46080/50048]	Loss: 1.1030
Training Epoch: 21 [46208/50048]	Loss: 1.0299
Training Epoch: 21 [46336/50048]	Loss: 0.8724
Training Epoch: 21 [46464/50048]	Loss: 0.8913
Training Epoch: 21 [46592/50048]	Loss: 1.1665
Training Epoch: 21 [46720/50048]	Loss: 1.0698
Training Epoch: 21 [46848/50048]	Loss: 1.0253
Training Epoch: 21 [46976/50048]	Loss: 1.0668
Training Epoch: 21 [47104/50048]	Loss: 1.0717
Training Epoch: 21 [47232/50048]	Loss: 1.0236
Training Epoch: 21 [47360/50048]	Loss: 1.0466
Training Epoch: 21 [47488/50048]	Loss: 0.9009
Training Epoch: 21 [47616/50048]	Loss: 0.9801
Training Epoch: 21 [47744/50048]	Loss: 1.0668
Training Epoch: 21 [47872/50048]	Loss: 1.2074
Training Epoch: 21 [48000/50048]	Loss: 0.7999
Training Epoch: 21 [48128/50048]	Loss: 0.9061
Training Epoch: 21 [48256/50048]	Loss: 1.0512
Training Epoch: 21 [48384/50048]	Loss: 0.8197
Training Epoch: 21 [48512/50048]	Loss: 0.9470
Training Epoch: 21 [48640/50048]	Loss: 0.9452
Training Epoch: 21 [48768/50048]	Loss: 1.1723
Training Epoch: 21 [48896/50048]	Loss: 0.9338
Training Epoch: 21 [49024/50048]	Loss: 0.9890
Training Epoch: 21 [49152/50048]	Loss: 1.0610
Training Epoch: 21 [49280/50048]	Loss: 1.2556
Training Epoch: 21 [49408/50048]	Loss: 0.8934
Training Epoch: 21 [49536/50048]	Loss: 1.0191
Training Epoch: 21 [49664/50048]	Loss: 1.0322
Training Epoch: 21 [49792/50048]	Loss: 1.0103
Training Epoch: 21 [49920/50048]	Loss: 0.9890
Training Epoch: 21 [50048/50048]	Loss: 0.8062
2022-12-06 11:40:58.568 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:40:58,588 [ZeusDataLoader(eval)] eval epoch 22 done: time=3.66 energy=442.80
2022-12-06 06:40:58,589 [ZeusDataLoader(train)] Up to epoch 22: time=1984.50, energy=240893.85, cost=294090.43
2022-12-06 06:40:58,589 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:40:58,589 [ZeusDataLoader(train)] Expected next epoch: time=2074.30, energy=251691.86, cost=307346.81
2022-12-06 06:40:58,590 [ZeusDataLoader(train)] Epoch 23 begin.
Validation Epoch: 21, Average loss: 0.0118, Accuracy: 0.6056
2022-12-06 06:40:58,781 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:40:58,781 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:40:58.783 [ZeusMonitor] Monitor started.
2022-12-06 11:40:58.783 [ZeusMonitor] Running indefinitely. 2022-12-06 11:40:58.783 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:40:58.783 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e23+gpu0.power.log
Training Epoch: 22 [128/50048]	Loss: 0.9287
Training Epoch: 22 [256/50048]	Loss: 0.9646
Training Epoch: 22 [384/50048]	Loss: 0.8838
Training Epoch: 22 [512/50048]	Loss: 0.9866
Training Epoch: 22 [640/50048]	Loss: 0.7129
Training Epoch: 22 [768/50048]	Loss: 1.0014
Training Epoch: 22 [896/50048]	Loss: 0.8969
Training Epoch: 22 [1024/50048]	Loss: 0.7749
Training Epoch: 22 [1152/50048]	Loss: 0.9380
Training Epoch: 22 [1280/50048]	Loss: 0.9068
Training Epoch: 22 [1408/50048]	Loss: 0.8817
Training Epoch: 22 [1536/50048]	Loss: 0.8021
Training Epoch: 22 [1664/50048]	Loss: 0.8609
Training Epoch: 22 [1792/50048]	Loss: 1.0140
Training Epoch: 22 [1920/50048]	Loss: 0.9375
Training Epoch: 22 [2048/50048]	Loss: 0.8039
Training Epoch: 22 [2176/50048]	Loss: 0.7546
Training Epoch: 22 [2304/50048]	Loss: 1.0179
Training Epoch: 22 [2432/50048]	Loss: 0.8940
Training Epoch: 22 [2560/50048]	Loss: 1.1978
Training Epoch: 22 [2688/50048]	Loss: 0.8483
Training Epoch: 22 [2816/50048]	Loss: 0.9925
Training Epoch: 22 [2944/50048]	Loss: 0.9272
Training Epoch: 22 [3072/50048]	Loss: 0.6813
Training Epoch: 22 [3200/50048]	Loss: 0.6162
Training Epoch: 22 [3328/50048]	Loss: 0.7730
Training Epoch: 22 [3456/50048]	Loss: 0.8440
Training Epoch: 22 [3584/50048]	Loss: 0.9315
Training Epoch: 22 [3712/50048]	Loss: 0.9678
Training Epoch: 22 [3840/50048]	Loss: 0.8130
Training Epoch: 22 [3968/50048]	Loss: 0.8716
Training Epoch: 22 [4096/50048]	Loss: 0.7822
Training Epoch: 22 [4224/50048]	Loss: 0.8335
Training Epoch: 22 [4352/50048]	Loss: 0.8191
Training Epoch: 22 [4480/50048]	Loss: 0.8275
Training Epoch: 22 [4608/50048]	Loss: 0.8522
Training Epoch: 22 [4736/50048]	Loss: 0.9522
Training Epoch: 22 [4864/50048]	Loss: 0.6967
Training Epoch: 22 [4992/50048]	Loss: 1.0799
Training Epoch: 22 [5120/50048]	Loss: 0.8487
Training Epoch: 22 [5248/50048]	Loss: 0.8268
Training Epoch: 22 [5376/50048]	Loss: 0.9633
Training Epoch: 22 [5504/50048]	Loss: 0.9242
Training Epoch: 22 [5632/50048]	Loss: 0.8855
Training Epoch: 22 [5760/50048]	Loss: 0.6928
Training Epoch: 22 [5888/50048]	Loss: 1.2021
Training Epoch: 22 [6016/50048]	Loss: 0.8319
Training Epoch: 22 [6144/50048]	Loss: 0.8516
Training Epoch: 22 [6272/50048]	Loss: 0.7288
Training Epoch: 22 [6400/50048]	Loss: 0.8424
Training Epoch: 22 [6528/50048]	Loss: 1.1025
Training Epoch: 22 [6656/50048]	Loss: 0.9285
Training Epoch: 22 [6784/50048]	Loss: 0.9402
Training Epoch: 22 [6912/50048]	Loss: 0.9860
Training Epoch: 22 [7040/50048]	Loss: 0.7782
Training Epoch: 22 [7168/50048]	Loss: 1.0198
Training Epoch: 22 [7296/50048]	Loss: 1.0203
Training Epoch: 22 [7424/50048]	Loss: 0.7322
Training Epoch: 22 [7552/50048]	Loss: 0.8325
Training Epoch: 22 [7680/50048]	Loss: 1.0953
Training Epoch: 22 [7808/50048]	Loss: 1.1726
Training Epoch: 22 [7936/50048]	Loss: 0.9655
Training Epoch: 22 [8064/50048]	Loss: 0.9918
Training Epoch: 22 [8192/50048]	Loss: 1.0813
Training Epoch: 22 [8320/50048]	Loss: 1.0055
Training Epoch: 22 [8448/50048]	Loss: 0.8752
Training Epoch: 22 [8576/50048]	Loss: 1.0242
Training Epoch: 22 [8704/50048]	Loss: 0.9219
Training Epoch: 22 [8832/50048]	Loss: 0.9086
Training Epoch: 22 [8960/50048]	Loss: 0.8742
Training Epoch: 22 [9088/50048]	Loss: 0.9885
Training Epoch: 22 [9216/50048]	Loss: 0.8886
Training Epoch: 22 [9344/50048]	Loss: 0.6831
Training Epoch: 22 [9472/50048]	Loss: 0.7549
Training Epoch: 22 [9600/50048]	Loss: 0.6661
Training Epoch: 22 [9728/50048]	Loss: 0.8530
Training Epoch: 22 [9856/50048]	Loss: 0.9361
Training Epoch: 22 [9984/50048]	Loss: 0.9708
Training Epoch: 22 [10112/50048]	Loss: 0.5835
Training Epoch: 22 [10240/50048]	Loss: 0.7523
Training Epoch: 22 [10368/50048]	Loss: 1.0535
Training Epoch: 22 [10496/50048]	Loss: 1.0801
Training Epoch: 22 [10624/50048]	Loss: 0.9928
Training Epoch: 22 [10752/50048]	Loss: 0.9044
Training Epoch: 22 [10880/50048]	Loss: 1.0123
Training Epoch: 22 [11008/50048]	Loss: 0.8191
Training Epoch: 22 [11136/50048]	Loss: 0.9026
Training Epoch: 22 [11264/50048]	Loss: 0.8845
Training Epoch: 22 [11392/50048]	Loss: 0.8344
Training Epoch: 22 [11520/50048]	Loss: 0.8056
Training Epoch: 22 [11648/50048]	Loss: 1.0780
Training Epoch: 22 [11776/50048]	Loss: 0.7664
Training Epoch: 22 [11904/50048]	Loss: 0.9400
Training Epoch: 22 [12032/50048]	Loss: 1.0108
Training Epoch: 22 [12160/50048]	Loss: 0.9646
Training Epoch: 22 [12288/50048]	Loss: 0.6800
Training Epoch: 22 [12416/50048]	Loss: 0.8767
Training Epoch: 22 [12544/50048]	Loss: 0.8569
Training Epoch: 22 [12672/50048]	Loss: 1.0033
Training Epoch: 22 [12800/50048]	Loss: 0.8464
Training Epoch: 22 [12928/50048]	Loss: 0.8294
Training Epoch: 22 [13056/50048]	Loss: 0.8363
Training Epoch: 22 [13184/50048]	Loss: 0.7707
Training Epoch: 22 [13312/50048]	Loss: 1.0490
Training Epoch: 22 [13440/50048]	Loss: 0.9363
Training Epoch: 22 [13568/50048]	Loss: 0.9198
Training Epoch: 22 [13696/50048]	Loss: 1.0220
Training Epoch: 22 [13824/50048]	Loss: 1.1187
Training Epoch: 22 [13952/50048]	Loss: 0.8471
Training Epoch: 22 [14080/50048]	Loss: 0.9299
Training Epoch: 22 [14208/50048]	Loss: 0.9214
Training Epoch: 22 [14336/50048]	Loss: 0.7207
Training Epoch: 22 [14464/50048]	Loss: 0.8929
Training Epoch: 22 [14592/50048]	Loss: 0.9351
Training Epoch: 22 [14720/50048]	Loss: 0.9456
Training Epoch: 22 [14848/50048]	Loss: 0.8657
Training Epoch: 22 [14976/50048]	Loss: 0.8118
Training Epoch: 22 [15104/50048]	Loss: 1.3305
Training Epoch: 22 [15232/50048]	Loss: 0.8944
Training Epoch: 22 [15360/50048]	Loss: 0.8835
Training Epoch: 22 [15488/50048]	Loss: 0.8545
Training Epoch: 22 [15616/50048]	Loss: 0.9764
Training Epoch: 22 [15744/50048]	Loss: 0.7518
Training Epoch: 22 [15872/50048]	Loss: 0.8784
Training Epoch: 22 [16000/50048]	Loss: 0.9166
Training Epoch: 22 [16128/50048]	Loss: 0.8923
Training Epoch: 22 [16256/50048]	Loss: 0.9265
Training Epoch: 22 [16384/50048]	Loss: 0.8889
Training Epoch: 22 [16512/50048]	Loss: 0.6960
Training Epoch: 22 [16640/50048]	Loss: 0.7969
Training Epoch: 22 [16768/50048]	Loss: 1.1163
Training Epoch: 22 [16896/50048]	Loss: 0.8499
Training Epoch: 22 [17024/50048]	Loss: 0.9059
Training Epoch: 22 [17152/50048]	Loss: 0.8195
Training Epoch: 22 [17280/50048]	Loss: 0.9751
Training Epoch: 22 [17408/50048]	Loss: 0.8218
Training Epoch: 22 [17536/50048]	Loss: 0.8322
Training Epoch: 22 [17664/50048]	Loss: 1.0000
Training Epoch: 22 [17792/50048]	Loss: 0.9521
Training Epoch: 22 [17920/50048]	Loss: 0.9619
Training Epoch: 22 [18048/50048]	Loss: 0.8995
Training Epoch: 22 [18176/50048]	Loss: 0.8368
Training Epoch: 22 [18304/50048]	Loss: 1.0404
Training Epoch: 22 [18432/50048]	Loss: 0.9758
Training Epoch: 22 [18560/50048]	Loss: 0.9555
Training Epoch: 22 [18688/50048]	Loss: 0.9657
Training Epoch: 22 [18816/50048]	Loss: 1.0970
Training Epoch: 22 [18944/50048]	Loss: 0.7564
Training Epoch: 22 [19072/50048]	Loss: 0.9658
Training Epoch: 22 [19200/50048]	Loss: 0.9080
Training Epoch: 22 [19328/50048]	Loss: 0.9533
Training Epoch: 22 [19456/50048]	Loss: 0.9364
Training Epoch: 22 [19584/50048]	Loss: 0.9843
Training Epoch: 22 [19712/50048]	Loss: 0.8031
Training Epoch: 22 [19840/50048]	Loss: 1.0455
Training Epoch: 22 [19968/50048]	Loss: 0.9293
Training Epoch: 22 [20096/50048]	Loss: 0.9627
Training Epoch: 22 [20224/50048]	Loss: 0.9764
Training Epoch: 22 [20352/50048]	Loss: 0.6792
Training Epoch: 22 [20480/50048]	Loss: 1.0402
Training Epoch: 22 [20608/50048]	Loss: 0.9022
Training Epoch: 22 [20736/50048]	Loss: 0.7621
Training Epoch: 22 [20864/50048]	Loss: 0.8947
Training Epoch: 22 [20992/50048]	Loss: 1.0574
Training Epoch: 22 [21120/50048]	Loss: 0.7002
Training Epoch: 22 [21248/50048]	Loss: 1.0043
Training Epoch: 22 [21376/50048]	Loss: 1.0160
Training Epoch: 22 [21504/50048]	Loss: 1.0397
Training Epoch: 22 [21632/50048]	Loss: 0.8685
Training Epoch: 22 [21760/50048]	Loss: 1.0583
Training Epoch: 22 [21888/50048]	Loss: 0.9162
Training Epoch: 22 [22016/50048]	Loss: 0.7893
Training Epoch: 22 [22144/50048]	Loss: 1.1741
Training Epoch: 22 [22272/50048]	Loss: 0.9186
Training Epoch: 22 [22400/50048]	Loss: 0.9189
Training Epoch: 22 [22528/50048]	Loss: 1.2014
Training Epoch: 22 [22656/50048]	Loss: 1.0751
Training Epoch: 22 [22784/50048]	Loss: 1.0313
Training Epoch: 22 [22912/50048]	Loss: 0.8811
Training Epoch: 22 [23040/50048]	Loss: 0.8554
Training Epoch: 22 [23168/50048]	Loss: 0.8778
Training Epoch: 22 [23296/50048]	Loss: 0.9696
Training Epoch: 22 [23424/50048]	Loss: 1.0036
Training Epoch: 22 [23552/50048]	Loss: 0.9064
Training Epoch: 22 [23680/50048]	Loss: 0.7700
Training Epoch: 22 [23808/50048]	Loss: 1.0820
Training Epoch: 22 [23936/50048]	Loss: 0.9879
Training Epoch: 22 [24064/50048]	Loss: 0.8752
Training Epoch: 22 [24192/50048]	Loss: 1.1897
Training Epoch: 22 [24320/50048]	Loss: 1.1900
Training Epoch: 22 [24448/50048]	Loss: 0.8680
Training Epoch: 22 [24576/50048]	Loss: 0.9645
Training Epoch: 22 [24704/50048]	Loss: 0.9204
Training Epoch: 22 [24832/50048]	Loss: 0.8589
Training Epoch: 22 [24960/50048]	Loss: 1.0344
Training Epoch: 22 [25088/50048]	Loss: 0.8926
Training Epoch: 22 [25216/50048]	Loss: 0.8238
Training Epoch: 22 [25344/50048]	Loss: 1.0230
Training Epoch: 22 [25472/50048]	Loss: 0.9953
Training Epoch: 22 [25600/50048]	Loss: 0.9008
Training Epoch: 22 [25728/50048]	Loss: 1.0762
Training Epoch: 22 [25856/50048]	Loss: 0.9100
Training Epoch: 22 [25984/50048]	Loss: 1.0546
Training Epoch: 22 [26112/50048]	Loss: 0.9552
Training Epoch: 22 [26240/50048]	Loss: 1.2988
Training Epoch: 22 [26368/50048]	Loss: 1.0227
Training Epoch: 22 [26496/50048]	Loss: 0.8041
Training Epoch: 22 [26624/50048]	Loss: 1.0285
Training Epoch: 22 [26752/50048]	Loss: 0.8235
Training Epoch: 22 [26880/50048]	Loss: 0.9767
Training Epoch: 22 [27008/50048]	Loss: 0.9651
Training Epoch: 22 [27136/50048]	Loss: 0.8018
Training Epoch: 22 [27264/50048]	Loss: 1.1691
Training Epoch: 22 [27392/50048]	Loss: 1.0298
Training Epoch: 22 [27520/50048]	Loss: 0.7607
Training Epoch: 22 [27648/50048]	Loss: 0.8603
Training Epoch: 22 [27776/50048]	Loss: 0.9295
Training Epoch: 22 [27904/50048]	Loss: 0.8422
Training Epoch: 22 [28032/50048]	Loss: 1.1500
Training Epoch: 22 [28160/50048]	Loss: 1.0709
Training Epoch: 22 [28288/50048]	Loss: 0.8529
Training Epoch: 22 [28416/50048]	Loss: 1.0410
Training Epoch: 22 [28544/50048]	Loss: 0.8849
Training Epoch: 22 [28672/50048]	Loss: 0.9572
Training Epoch: 22 [28800/50048]	Loss: 1.2219
Training Epoch: 22 [28928/50048]	Loss: 0.9915
Training Epoch: 22 [29056/50048]	Loss: 0.7835
Training Epoch: 22 [29184/50048]	Loss: 1.0524
Training Epoch: 22 [29312/50048]	Loss: 0.7654
Training Epoch: 22 [29440/50048]	Loss: 0.6489
Training Epoch: 22 [29568/50048]	Loss: 0.9638
Training Epoch: 22 [29696/50048]	Loss: 0.9439
Training Epoch: 22 [29824/50048]	Loss: 0.7873
Training Epoch: 22 [29952/50048]	Loss: 0.7883
Training Epoch: 22 [30080/50048]	Loss: 0.8061
Training Epoch: 22 [30208/50048]	Loss: 0.8542
Training Epoch: 22 [30336/50048]	Loss: 0.8590
Training Epoch: 22 [30464/50048]	Loss: 0.9083
Training Epoch: 22 [30592/50048]	Loss: 0.9759
Training Epoch: 22 [30720/50048]	Loss: 0.9017
Training Epoch: 22 [30848/50048]	Loss: 1.1514
Training Epoch: 22 [30976/50048]	Loss: 1.0033
Training Epoch: 22 [31104/50048]	Loss: 1.0405
Training Epoch: 22 [31232/50048]	Loss: 0.9080
Training Epoch: 22 [31360/50048]	Loss: 0.8414
Training Epoch: 22 [31488/50048]	Loss: 1.1909
Training Epoch: 22 [31616/50048]	Loss: 1.0893
Training Epoch: 22 [31744/50048]	Loss: 1.0401
Training Epoch: 22 [31872/50048]	Loss: 0.8316
Training Epoch: 22 [32000/50048]	Loss: 0.8236
Training Epoch: 22 [32128/50048]	Loss: 0.9187
Training Epoch: 22 [32256/50048]	Loss: 1.0096
Training Epoch: 22 [32384/50048]	Loss: 0.7430
Training Epoch: 22 [32512/50048]	Loss: 0.8822
Training Epoch: 22 [32640/50048]	Loss: 0.8647
Training Epoch: 22 [32768/50048]	Loss: 0.9173
Training Epoch: 22 [32896/50048]	Loss: 0.8164
Training Epoch: 22 [33024/50048]	Loss: 0.8515
Training Epoch: 22 [33152/50048]	Loss: 0.9397
Training Epoch: 22 [33280/50048]	Loss: 1.0822
Training Epoch: 22 [33408/50048]	Loss: 1.1052
Training Epoch: 22 [33536/50048]	Loss: 0.8422
Training Epoch: 22 [33664/50048]	Loss: 1.0432
Training Epoch: 22 [33792/50048]	Loss: 1.0137
Training Epoch: 22 [33920/50048]	Loss: 1.0712
Training Epoch: 22 [34048/50048]	Loss: 0.8165
Training Epoch: 22 [34176/50048]	Loss: 0.9489
Training Epoch: 22 [34304/50048]	Loss: 1.0299
Training Epoch: 22 [34432/50048]	Loss: 1.0373
Training Epoch: 22 [34560/50048]	Loss: 1.0369
Training Epoch: 22 [34688/50048]	Loss: 0.9496
Training Epoch: 22 [34816/50048]	Loss: 0.8440
Training Epoch: 22 [34944/50048]	Loss: 0.8373
Training Epoch: 22 [35072/50048]	Loss: 0.9027
Training Epoch: 22 [35200/50048]	Loss: 0.8658
Training Epoch: 22 [35328/50048]	Loss: 0.8737
Training Epoch: 22 [35456/50048]	Loss: 1.0015
Training Epoch: 22 [35584/50048]	Loss: 0.9636
Training Epoch: 22 [35712/50048]	Loss: 0.9815
Training Epoch: 22 [35840/50048]	Loss: 0.9062
Training Epoch: 22 [35968/50048]	Loss: 0.9495
Training Epoch: 22 [36096/50048]	Loss: 1.1275
Training Epoch: 22 [36224/50048]	Loss: 1.2421
Training Epoch: 22 [36352/50048]	Loss: 1.0459
Training Epoch: 22 [36480/50048]	Loss: 0.7653
Training Epoch: 22 [36608/50048]	Loss: 0.7474
Training Epoch: 22 [36736/50048]	Loss: 0.9352
Training Epoch: 22 [36864/50048]	Loss: 1.0035
Training Epoch: 22 [36992/50048]	Loss: 0.9678
Training Epoch: 22 [37120/50048]	Loss: 1.0210
Training Epoch: 22 [37248/50048]	Loss: 1.1083
Training Epoch: 22 [37376/50048]	Loss: 0.7349
Training Epoch: 22 [37504/50048]	Loss: 0.7866
Training Epoch: 22 [37632/50048]	Loss: 0.9996
Training Epoch: 22 [37760/50048]	Loss: 0.9092
Training Epoch: 22 [37888/50048]	Loss: 1.1538
Training Epoch: 22 [38016/50048]	Loss: 0.8193
Training Epoch: 22 [38144/50048]	Loss: 0.8421
Training Epoch: 22 [38272/50048]	Loss: 0.9282
Training Epoch: 22 [38400/50048]	Loss: 0.8871
Training Epoch: 22 [38528/50048]	Loss: 0.9137
Training Epoch: 22 [38656/50048]	Loss: 1.0411
Training Epoch: 22 [38784/50048]	Loss: 0.7238
Training Epoch: 22 [38912/50048]	Loss: 0.8612
Training Epoch: 22 [39040/50048]	Loss: 1.1468
Training Epoch: 22 [39168/50048]	Loss: 0.8390
Training Epoch: 22 [39296/50048]	Loss: 0.7933
Training Epoch: 22 [39424/50048]	Loss: 0.8413
Training Epoch: 22 [39552/50048]	Loss: 0.9411
Training Epoch: 22 [39680/50048]	Loss: 1.0796
Training Epoch: 22 [39808/50048]	Loss: 0.9295
Training Epoch: 22 [39936/50048]	Loss: 0.9257
Training Epoch: 22 [40064/50048]	Loss: 0.8521
Training Epoch: 22 [40192/50048]	Loss: 0.9915
Training Epoch: 22 [40320/50048]	Loss: 0.8678
Training Epoch: 22 [40448/50048]	Loss: 0.8846
Training Epoch: 22 [40576/50048]	Loss: 1.1246
Training Epoch: 22 [40704/50048]	Loss: 0.7165
Training Epoch: 22 [40832/50048]	Loss: 0.8737
Training Epoch: 22 [40960/50048]	Loss: 0.9767
Training Epoch: 22 [41088/50048]	Loss: 0.8253
Training Epoch: 22 [41216/50048]	Loss: 1.0950
Training Epoch: 22 [41344/50048]	Loss: 1.0354
Training Epoch: 22 [41472/50048]	Loss: 0.8384
Training Epoch: 22 [41600/50048]	Loss: 0.9932
Training Epoch: 22 [41728/50048]	Loss: 0.8747
Training Epoch: 22 [41856/50048]	Loss: 0.9375
Training Epoch: 22 [41984/50048]	Loss: 1.0934
Training Epoch: 22 [42112/50048]	Loss: 0.7810
Training Epoch: 22 [42240/50048]	Loss: 1.1251
Training Epoch: 22 [42368/50048]	Loss: 0.7178
Training Epoch: 22 [42496/50048]	Loss: 0.9216
Training Epoch: 22 [42624/50048]	Loss: 1.1532
Training Epoch: 22 [42752/50048]	Loss: 0.7206
Training Epoch: 22 [42880/50048]	Loss: 1.0527
Training Epoch: 22 [43008/50048]	Loss: 0.7882
Training Epoch: 22 [43136/50048]	Loss: 1.0016
Training Epoch: 22 [43264/50048]	Loss: 0.8385
Training Epoch: 22 [43392/50048]	Loss: 0.9475
Training Epoch: 22 [43520/50048]	Loss: 0.8610
Training Epoch: 22 [43648/50048]	Loss: 0.8297
Training Epoch: 22 [43776/50048]	Loss: 0.9832
Training Epoch: 22 [43904/50048]	Loss: 0.8794
Training Epoch: 22 [44032/50048]	Loss: 0.8732
Training Epoch: 22 [44160/50048]	Loss: 1.2578
Training Epoch: 22 [44288/50048]	Loss: 0.9710
Training Epoch: 22 [44416/50048]	Loss: 0.9370
Training Epoch: 22 [44544/50048]	Loss: 0.9883
Training Epoch: 22 [44672/50048]	Loss: 0.8154
Training Epoch: 22 [44800/50048]	Loss: 1.0706
Training Epoch: 22 [44928/50048]	Loss: 0.9781
Training Epoch: 22 [45056/50048]	Loss: 0.8939
Training Epoch: 22 [45184/50048]	Loss: 1.0069
Training Epoch: 22 [45312/50048]	Loss: 0.7665
Training Epoch: 22 [45440/50048]	Loss: 1.1273
Training Epoch: 22 [45568/50048]	Loss: 0.8819
Training Epoch: 22 [45696/50048]	Loss: 0.9371
2022-12-06 06:42:24,943 [ZeusDataLoader(train)] train epoch 23 done: time=86.34 energy=10496.99
2022-12-06 06:42:24,945 [ZeusDataLoader(eval)] Epoch 23 begin.
Training Epoch: 22 [45824/50048]	Loss: 0.8170
Training Epoch: 22 [45952/50048]	Loss: 0.9588
Training Epoch: 22 [46080/50048]	Loss: 1.0976
Training Epoch: 22 [46208/50048]	Loss: 1.0177
Training Epoch: 22 [46336/50048]	Loss: 0.9718
Training Epoch: 22 [46464/50048]	Loss: 1.0061
Training Epoch: 22 [46592/50048]	Loss: 0.8365
Training Epoch: 22 [46720/50048]	Loss: 0.8799
Training Epoch: 22 [46848/50048]	Loss: 0.9789
Training Epoch: 22 [46976/50048]	Loss: 0.8617
Training Epoch: 22 [47104/50048]	Loss: 1.0268
Training Epoch: 22 [47232/50048]	Loss: 1.2215
Training Epoch: 22 [47360/50048]	Loss: 1.1271
Training Epoch: 22 [47488/50048]	Loss: 0.9289
Training Epoch: 22 [47616/50048]	Loss: 1.2737
Training Epoch: 22 [47744/50048]	Loss: 1.1548
Training Epoch: 22 [47872/50048]	Loss: 0.8460
Training Epoch: 22 [48000/50048]	Loss: 1.0600
Training Epoch: 22 [48128/50048]	Loss: 0.9653
Training Epoch: 22 [48256/50048]	Loss: 0.9115
Training Epoch: 22 [48384/50048]	Loss: 1.1055
Training Epoch: 22 [48512/50048]	Loss: 0.9181
Training Epoch: 22 [48640/50048]	Loss: 1.0844
Training Epoch: 22 [48768/50048]	Loss: 1.0611
Training Epoch: 22 [48896/50048]	Loss: 1.0338
Training Epoch: 22 [49024/50048]	Loss: 1.1178
Training Epoch: 22 [49152/50048]	Loss: 0.9739
Training Epoch: 22 [49280/50048]	Loss: 0.8736
Training Epoch: 22 [49408/50048]	Loss: 0.9546
Training Epoch: 22 [49536/50048]	Loss: 0.7624
Training Epoch: 22 [49664/50048]	Loss: 1.0217
Training Epoch: 22 [49792/50048]	Loss: 1.0012
Training Epoch: 22 [49920/50048]	Loss: 0.8059
Training Epoch: 22 [50048/50048]	Loss: 0.8846
2022-12-06 11:42:28.600 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:42:28,621 [ZeusDataLoader(eval)] eval epoch 23 done: time=3.67 energy=439.86
2022-12-06 06:42:28,621 [ZeusDataLoader(train)] Up to epoch 23: time=2074.51, energy=251830.70, cost=307434.75
2022-12-06 06:42:28,621 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:42:28,621 [ZeusDataLoader(train)] Expected next epoch: time=2164.31, energy=262628.71, cost=320691.13
2022-12-06 06:42:28,622 [ZeusDataLoader(train)] Epoch 24 begin.
Validation Epoch: 22, Average loss: 0.0118, Accuracy: 0.6036
2022-12-06 06:42:28,805 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:42:28,806 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:42:28.807 [ZeusMonitor] Monitor started.
2022-12-06 11:42:28.807 [ZeusMonitor] Running indefinitely. 2022-12-06 11:42:28.807 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:42:28.807 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e24+gpu0.power.log
Training Epoch: 23 [128/50048]	Loss: 0.8680
Training Epoch: 23 [256/50048]	Loss: 0.8513
Training Epoch: 23 [384/50048]	Loss: 0.8164
Training Epoch: 23 [512/50048]	Loss: 1.0613
Training Epoch: 23 [640/50048]	Loss: 0.9924
Training Epoch: 23 [768/50048]	Loss: 0.7591
Training Epoch: 23 [896/50048]	Loss: 0.8009
Training Epoch: 23 [1024/50048]	Loss: 0.6650
Training Epoch: 23 [1152/50048]	Loss: 0.9405
Training Epoch: 23 [1280/50048]	Loss: 0.9517
Training Epoch: 23 [1408/50048]	Loss: 0.8332
Training Epoch: 23 [1536/50048]	Loss: 0.9178
Training Epoch: 23 [1664/50048]	Loss: 0.8066
Training Epoch: 23 [1792/50048]	Loss: 0.8229
Training Epoch: 23 [1920/50048]	Loss: 0.7639
Training Epoch: 23 [2048/50048]	Loss: 0.8056
Training Epoch: 23 [2176/50048]	Loss: 0.8543
Training Epoch: 23 [2304/50048]	Loss: 0.8018
Training Epoch: 23 [2432/50048]	Loss: 0.6738
Training Epoch: 23 [2560/50048]	Loss: 0.9627
Training Epoch: 23 [2688/50048]	Loss: 0.7303
Training Epoch: 23 [2816/50048]	Loss: 0.9187
Training Epoch: 23 [2944/50048]	Loss: 0.9824
Training Epoch: 23 [3072/50048]	Loss: 0.9170
Training Epoch: 23 [3200/50048]	Loss: 0.7597
Training Epoch: 23 [3328/50048]	Loss: 1.0085
Training Epoch: 23 [3456/50048]	Loss: 0.8378
Training Epoch: 23 [3584/50048]	Loss: 0.8123
Training Epoch: 23 [3712/50048]	Loss: 0.8308
Training Epoch: 23 [3840/50048]	Loss: 0.7048
Training Epoch: 23 [3968/50048]	Loss: 0.7253
Training Epoch: 23 [4096/50048]	Loss: 0.7430
Training Epoch: 23 [4224/50048]	Loss: 1.0006
Training Epoch: 23 [4352/50048]	Loss: 0.8303
Training Epoch: 23 [4480/50048]	Loss: 0.9090
Training Epoch: 23 [4608/50048]	Loss: 0.7842
Training Epoch: 23 [4736/50048]	Loss: 0.8289
Training Epoch: 23 [4864/50048]	Loss: 0.9899
Training Epoch: 23 [4992/50048]	Loss: 0.6308
Training Epoch: 23 [5120/50048]	Loss: 0.8577
Training Epoch: 23 [5248/50048]	Loss: 0.8049
Training Epoch: 23 [5376/50048]	Loss: 0.9757
Training Epoch: 23 [5504/50048]	Loss: 0.9627
Training Epoch: 23 [5632/50048]	Loss: 0.8229
Training Epoch: 23 [5760/50048]	Loss: 0.7744
Training Epoch: 23 [5888/50048]	Loss: 0.7901
Training Epoch: 23 [6016/50048]	Loss: 0.8024
Training Epoch: 23 [6144/50048]	Loss: 0.7730
Training Epoch: 23 [6272/50048]	Loss: 0.7026
Training Epoch: 23 [6400/50048]	Loss: 0.8812
Training Epoch: 23 [6528/50048]	Loss: 0.9368
Training Epoch: 23 [6656/50048]	Loss: 0.7861
Training Epoch: 23 [6784/50048]	Loss: 0.8502
Training Epoch: 23 [6912/50048]	Loss: 1.0183
Training Epoch: 23 [7040/50048]	Loss: 0.7857
Training Epoch: 23 [7168/50048]	Loss: 1.0322
Training Epoch: 23 [7296/50048]	Loss: 0.9578
Training Epoch: 23 [7424/50048]	Loss: 0.8177
Training Epoch: 23 [7552/50048]	Loss: 0.7462
Training Epoch: 23 [7680/50048]	Loss: 0.8344
Training Epoch: 23 [7808/50048]	Loss: 0.6532
Training Epoch: 23 [7936/50048]	Loss: 0.7576
Training Epoch: 23 [8064/50048]	Loss: 0.7853
Training Epoch: 23 [8192/50048]	Loss: 0.8751
Training Epoch: 23 [8320/50048]	Loss: 0.9253
Training Epoch: 23 [8448/50048]	Loss: 0.8431
Training Epoch: 23 [8576/50048]	Loss: 0.9646
Training Epoch: 23 [8704/50048]	Loss: 0.7680
Training Epoch: 23 [8832/50048]	Loss: 0.7850
Training Epoch: 23 [8960/50048]	Loss: 0.7550
Training Epoch: 23 [9088/50048]	Loss: 0.7831
Training Epoch: 23 [9216/50048]	Loss: 0.8063
Training Epoch: 23 [9344/50048]	Loss: 0.6817
Training Epoch: 23 [9472/50048]	Loss: 1.0094
Training Epoch: 23 [9600/50048]	Loss: 0.7181
Training Epoch: 23 [9728/50048]	Loss: 0.8205
Training Epoch: 23 [9856/50048]	Loss: 0.8154
Training Epoch: 23 [9984/50048]	Loss: 0.9125
Training Epoch: 23 [10112/50048]	Loss: 0.8461
Training Epoch: 23 [10240/50048]	Loss: 0.8104
Training Epoch: 23 [10368/50048]	Loss: 0.6976
Training Epoch: 23 [10496/50048]	Loss: 0.9160
Training Epoch: 23 [10624/50048]	Loss: 0.9496
Training Epoch: 23 [10752/50048]	Loss: 1.0101
Training Epoch: 23 [10880/50048]	Loss: 0.7892
Training Epoch: 23 [11008/50048]	Loss: 0.9747
Training Epoch: 23 [11136/50048]	Loss: 1.0300
Training Epoch: 23 [11264/50048]	Loss: 0.9238
Training Epoch: 23 [11392/50048]	Loss: 0.5333
Training Epoch: 23 [11520/50048]	Loss: 0.9029
Training Epoch: 23 [11648/50048]	Loss: 0.6986
Training Epoch: 23 [11776/50048]	Loss: 0.9022
Training Epoch: 23 [11904/50048]	Loss: 0.8819
Training Epoch: 23 [12032/50048]	Loss: 0.8567
Training Epoch: 23 [12160/50048]	Loss: 0.9110
Training Epoch: 23 [12288/50048]	Loss: 0.7283
Training Epoch: 23 [12416/50048]	Loss: 0.9080
Training Epoch: 23 [12544/50048]	Loss: 0.9441
Training Epoch: 23 [12672/50048]	Loss: 0.9163
Training Epoch: 23 [12800/50048]	Loss: 1.1382
Training Epoch: 23 [12928/50048]	Loss: 0.6943
Training Epoch: 23 [13056/50048]	Loss: 0.8594
Training Epoch: 23 [13184/50048]	Loss: 0.7841
Training Epoch: 23 [13312/50048]	Loss: 0.9591
Training Epoch: 23 [13440/50048]	Loss: 0.8929
Training Epoch: 23 [13568/50048]	Loss: 0.7641
Training Epoch: 23 [13696/50048]	Loss: 0.8662
Training Epoch: 23 [13824/50048]	Loss: 0.7729
Training Epoch: 23 [13952/50048]	Loss: 0.8692
Training Epoch: 23 [14080/50048]	Loss: 0.6824
Training Epoch: 23 [14208/50048]	Loss: 0.8495
Training Epoch: 23 [14336/50048]	Loss: 0.8882
Training Epoch: 23 [14464/50048]	Loss: 0.8928
Training Epoch: 23 [14592/50048]	Loss: 1.0313
Training Epoch: 23 [14720/50048]	Loss: 0.8376
Training Epoch: 23 [14848/50048]	Loss: 0.7118
Training Epoch: 23 [14976/50048]	Loss: 0.8992
Training Epoch: 23 [15104/50048]	Loss: 0.8466
Training Epoch: 23 [15232/50048]	Loss: 0.8121
Training Epoch: 23 [15360/50048]	Loss: 0.9398
Training Epoch: 23 [15488/50048]	Loss: 0.8006
Training Epoch: 23 [15616/50048]	Loss: 0.9509
Training Epoch: 23 [15744/50048]	Loss: 0.9750
Training Epoch: 23 [15872/50048]	Loss: 1.0719
Training Epoch: 23 [16000/50048]	Loss: 0.8319
Training Epoch: 23 [16128/50048]	Loss: 0.8420
Training Epoch: 23 [16256/50048]	Loss: 0.7782
Training Epoch: 23 [16384/50048]	Loss: 0.9389
Training Epoch: 23 [16512/50048]	Loss: 1.0317
Training Epoch: 23 [16640/50048]	Loss: 0.7344
Training Epoch: 23 [16768/50048]	Loss: 0.8284
Training Epoch: 23 [16896/50048]	Loss: 0.9323
Training Epoch: 23 [17024/50048]	Loss: 0.9723
Training Epoch: 23 [17152/50048]	Loss: 0.7481
Training Epoch: 23 [17280/50048]	Loss: 0.9345
Training Epoch: 23 [17408/50048]	Loss: 0.8017
Training Epoch: 23 [17536/50048]	Loss: 0.8316
Training Epoch: 23 [17664/50048]	Loss: 0.9391
Training Epoch: 23 [17792/50048]	Loss: 0.6927
Training Epoch: 23 [17920/50048]	Loss: 0.7425
Training Epoch: 23 [18048/50048]	Loss: 0.9125
Training Epoch: 23 [18176/50048]	Loss: 0.8738
Training Epoch: 23 [18304/50048]	Loss: 0.7573
Training Epoch: 23 [18432/50048]	Loss: 1.0744
Training Epoch: 23 [18560/50048]	Loss: 0.7795
Training Epoch: 23 [18688/50048]	Loss: 0.8747
Training Epoch: 23 [18816/50048]	Loss: 0.7230
Training Epoch: 23 [18944/50048]	Loss: 1.0351
Training Epoch: 23 [19072/50048]	Loss: 0.7945
Training Epoch: 23 [19200/50048]	Loss: 1.0749
Training Epoch: 23 [19328/50048]	Loss: 0.8086
Training Epoch: 23 [19456/50048]	Loss: 0.8405
Training Epoch: 23 [19584/50048]	Loss: 0.9422
Training Epoch: 23 [19712/50048]	Loss: 0.8001
Training Epoch: 23 [19840/50048]	Loss: 0.7702
Training Epoch: 23 [19968/50048]	Loss: 0.6501
Training Epoch: 23 [20096/50048]	Loss: 0.7792
Training Epoch: 23 [20224/50048]	Loss: 0.7308
Training Epoch: 23 [20352/50048]	Loss: 0.9884
Training Epoch: 23 [20480/50048]	Loss: 1.1449
Training Epoch: 23 [20608/50048]	Loss: 0.8483
Training Epoch: 23 [20736/50048]	Loss: 1.0591
Training Epoch: 23 [20864/50048]	Loss: 0.9829
Training Epoch: 23 [20992/50048]	Loss: 0.7966
Training Epoch: 23 [21120/50048]	Loss: 0.7412
Training Epoch: 23 [21248/50048]	Loss: 0.9259
Training Epoch: 23 [21376/50048]	Loss: 0.8708
Training Epoch: 23 [21504/50048]	Loss: 0.9359
Training Epoch: 23 [21632/50048]	Loss: 0.7605
Training Epoch: 23 [21760/50048]	Loss: 0.8399
Training Epoch: 23 [21888/50048]	Loss: 0.6475
Training Epoch: 23 [22016/50048]	Loss: 0.8763
Training Epoch: 23 [22144/50048]	Loss: 0.9096
Training Epoch: 23 [22272/50048]	Loss: 1.1101
Training Epoch: 23 [22400/50048]	Loss: 0.8532
Training Epoch: 23 [22528/50048]	Loss: 0.9175
Training Epoch: 23 [22656/50048]	Loss: 0.9627
Training Epoch: 23 [22784/50048]	Loss: 0.7334
Training Epoch: 23 [22912/50048]	Loss: 0.8600
Training Epoch: 23 [23040/50048]	Loss: 0.8627
Training Epoch: 23 [23168/50048]	Loss: 0.7989
Training Epoch: 23 [23296/50048]	Loss: 0.9572
Training Epoch: 23 [23424/50048]	Loss: 0.7477
Training Epoch: 23 [23552/50048]	Loss: 0.8169
Training Epoch: 23 [23680/50048]	Loss: 0.7740
Training Epoch: 23 [23808/50048]	Loss: 0.8104
Training Epoch: 23 [23936/50048]	Loss: 0.8896
Training Epoch: 23 [24064/50048]	Loss: 0.9387
Training Epoch: 23 [24192/50048]	Loss: 1.0411
Training Epoch: 23 [24320/50048]	Loss: 0.9268
Training Epoch: 23 [24448/50048]	Loss: 0.9247
Training Epoch: 23 [24576/50048]	Loss: 0.9792
Training Epoch: 23 [24704/50048]	Loss: 0.8251
Training Epoch: 23 [24832/50048]	Loss: 0.7893
Training Epoch: 23 [24960/50048]	Loss: 0.9455
Training Epoch: 23 [25088/50048]	Loss: 0.7465
Training Epoch: 23 [25216/50048]	Loss: 0.9300
Training Epoch: 23 [25344/50048]	Loss: 0.8262
Training Epoch: 23 [25472/50048]	Loss: 1.0402
Training Epoch: 23 [25600/50048]	Loss: 1.0105
Training Epoch: 23 [25728/50048]	Loss: 0.9558
Training Epoch: 23 [25856/50048]	Loss: 0.8846
Training Epoch: 23 [25984/50048]	Loss: 0.9116
Training Epoch: 23 [26112/50048]	Loss: 0.8753
Training Epoch: 23 [26240/50048]	Loss: 1.0194
Training Epoch: 23 [26368/50048]	Loss: 0.8733
Training Epoch: 23 [26496/50048]	Loss: 1.0205
Training Epoch: 23 [26624/50048]	Loss: 0.9146
Training Epoch: 23 [26752/50048]	Loss: 0.8327
Training Epoch: 23 [26880/50048]	Loss: 0.9947
Training Epoch: 23 [27008/50048]	Loss: 0.9435
Training Epoch: 23 [27136/50048]	Loss: 0.8277
Training Epoch: 23 [27264/50048]	Loss: 0.9276
Training Epoch: 23 [27392/50048]	Loss: 0.8777
Training Epoch: 23 [27520/50048]	Loss: 0.7890
Training Epoch: 23 [27648/50048]	Loss: 0.9859
Training Epoch: 23 [27776/50048]	Loss: 1.1174
Training Epoch: 23 [27904/50048]	Loss: 0.7815
Training Epoch: 23 [28032/50048]	Loss: 0.8730
Training Epoch: 23 [28160/50048]	Loss: 0.9935
Training Epoch: 23 [28288/50048]	Loss: 0.9387
Training Epoch: 23 [28416/50048]	Loss: 0.9457
Training Epoch: 23 [28544/50048]	Loss: 0.8704
Training Epoch: 23 [28672/50048]	Loss: 0.7845
Training Epoch: 23 [28800/50048]	Loss: 1.0270
Training Epoch: 23 [28928/50048]	Loss: 1.0930
Training Epoch: 23 [29056/50048]	Loss: 0.9299
Training Epoch: 23 [29184/50048]	Loss: 0.8144
Training Epoch: 23 [29312/50048]	Loss: 0.7182
Training Epoch: 23 [29440/50048]	Loss: 1.1584
Training Epoch: 23 [29568/50048]	Loss: 0.8653
Training Epoch: 23 [29696/50048]	Loss: 0.7148
Training Epoch: 23 [29824/50048]	Loss: 1.0208
Training Epoch: 23 [29952/50048]	Loss: 0.9797
Training Epoch: 23 [30080/50048]	Loss: 0.9479
Training Epoch: 23 [30208/50048]	Loss: 0.8988
Training Epoch: 23 [30336/50048]	Loss: 0.9280
Training Epoch: 23 [30464/50048]	Loss: 1.1300
Training Epoch: 23 [30592/50048]	Loss: 0.9218
Training Epoch: 23 [30720/50048]	Loss: 0.8723
Training Epoch: 23 [30848/50048]	Loss: 0.9191
Training Epoch: 23 [30976/50048]	Loss: 0.8761
Training Epoch: 23 [31104/50048]	Loss: 1.0955
Training Epoch: 23 [31232/50048]	Loss: 1.0147
Training Epoch: 23 [31360/50048]	Loss: 0.7090
Training Epoch: 23 [31488/50048]	Loss: 1.0281
Training Epoch: 23 [31616/50048]	Loss: 0.9253
Training Epoch: 23 [31744/50048]	Loss: 0.9176
Training Epoch: 23 [31872/50048]	Loss: 0.9859
Training Epoch: 23 [32000/50048]	Loss: 0.8759
Training Epoch: 23 [32128/50048]	Loss: 0.9163
Training Epoch: 23 [32256/50048]	Loss: 0.8276
Training Epoch: 23 [32384/50048]	Loss: 0.8568
Training Epoch: 23 [32512/50048]	Loss: 0.9498
Training Epoch: 23 [32640/50048]	Loss: 1.1768
Training Epoch: 23 [32768/50048]	Loss: 0.7303
Training Epoch: 23 [32896/50048]	Loss: 0.8389
Training Epoch: 23 [33024/50048]	Loss: 0.7811
Training Epoch: 23 [33152/50048]	Loss: 0.9375
Training Epoch: 23 [33280/50048]	Loss: 0.9654
Training Epoch: 23 [33408/50048]	Loss: 0.8401
Training Epoch: 23 [33536/50048]	Loss: 0.8363
Training Epoch: 23 [33664/50048]	Loss: 0.8656
Training Epoch: 23 [33792/50048]	Loss: 0.8969
Training Epoch: 23 [33920/50048]	Loss: 0.9895
Training Epoch: 23 [34048/50048]	Loss: 0.9736
Training Epoch: 23 [34176/50048]	Loss: 1.0490
Training Epoch: 23 [34304/50048]	Loss: 0.9823
Training Epoch: 23 [34432/50048]	Loss: 0.9969
Training Epoch: 23 [34560/50048]	Loss: 0.7982
Training Epoch: 23 [34688/50048]	Loss: 0.6914
Training Epoch: 23 [34816/50048]	Loss: 1.0372
Training Epoch: 23 [34944/50048]	Loss: 0.8529
Training Epoch: 23 [35072/50048]	Loss: 0.9682
Training Epoch: 23 [35200/50048]	Loss: 0.8949
Training Epoch: 23 [35328/50048]	Loss: 0.9969
Training Epoch: 23 [35456/50048]	Loss: 0.7681
Training Epoch: 23 [35584/50048]	Loss: 0.6884
Training Epoch: 23 [35712/50048]	Loss: 0.9310
Training Epoch: 23 [35840/50048]	Loss: 1.0502
Training Epoch: 23 [35968/50048]	Loss: 1.0560
Training Epoch: 23 [36096/50048]	Loss: 0.9921
Training Epoch: 23 [36224/50048]	Loss: 0.9662
Training Epoch: 23 [36352/50048]	Loss: 1.0397
Training Epoch: 23 [36480/50048]	Loss: 0.7803
Training Epoch: 23 [36608/50048]	Loss: 0.9473
Training Epoch: 23 [36736/50048]	Loss: 0.7365
Training Epoch: 23 [36864/50048]	Loss: 0.9887
Training Epoch: 23 [36992/50048]	Loss: 0.6992
Training Epoch: 23 [37120/50048]	Loss: 0.9847
Training Epoch: 23 [37248/50048]	Loss: 0.8061
Training Epoch: 23 [37376/50048]	Loss: 1.1669
Training Epoch: 23 [37504/50048]	Loss: 0.7777
Training Epoch: 23 [37632/50048]	Loss: 0.9281
Training Epoch: 23 [37760/50048]	Loss: 1.0230
Training Epoch: 23 [37888/50048]	Loss: 0.8755
Training Epoch: 23 [38016/50048]	Loss: 1.0331
Training Epoch: 23 [38144/50048]	Loss: 0.7022
Training Epoch: 23 [38272/50048]	Loss: 0.9280
Training Epoch: 23 [38400/50048]	Loss: 1.0235
Training Epoch: 23 [38528/50048]	Loss: 1.2324
Training Epoch: 23 [38656/50048]	Loss: 0.8346
Training Epoch: 23 [38784/50048]	Loss: 0.7470
Training Epoch: 23 [38912/50048]	Loss: 0.9094
Training Epoch: 23 [39040/50048]	Loss: 1.0241
Training Epoch: 23 [39168/50048]	Loss: 0.8957
Training Epoch: 23 [39296/50048]	Loss: 1.0092
Training Epoch: 23 [39424/50048]	Loss: 0.8911
Training Epoch: 23 [39552/50048]	Loss: 0.9628
Training Epoch: 23 [39680/50048]	Loss: 0.8091
Training Epoch: 23 [39808/50048]	Loss: 0.7961
Training Epoch: 23 [39936/50048]	Loss: 1.1410
Training Epoch: 23 [40064/50048]	Loss: 1.0406
Training Epoch: 23 [40192/50048]	Loss: 0.8337
Training Epoch: 23 [40320/50048]	Loss: 0.8218
Training Epoch: 23 [40448/50048]	Loss: 0.8238
Training Epoch: 23 [40576/50048]	Loss: 0.9497
Training Epoch: 23 [40704/50048]	Loss: 0.8960
Training Epoch: 23 [40832/50048]	Loss: 1.2418
Training Epoch: 23 [40960/50048]	Loss: 1.0464
Training Epoch: 23 [41088/50048]	Loss: 1.0398
Training Epoch: 23 [41216/50048]	Loss: 0.9215
Training Epoch: 23 [41344/50048]	Loss: 0.8774
Training Epoch: 23 [41472/50048]	Loss: 0.8155
Training Epoch: 23 [41600/50048]	Loss: 0.8772
Training Epoch: 23 [41728/50048]	Loss: 1.1487
Training Epoch: 23 [41856/50048]	Loss: 0.9638
Training Epoch: 23 [41984/50048]	Loss: 0.9926
Training Epoch: 23 [42112/50048]	Loss: 0.7220
Training Epoch: 23 [42240/50048]	Loss: 1.0420
Training Epoch: 23 [42368/50048]	Loss: 1.0792
Training Epoch: 23 [42496/50048]	Loss: 1.1642
Training Epoch: 23 [42624/50048]	Loss: 1.0319
Training Epoch: 23 [42752/50048]	Loss: 0.9285
Training Epoch: 23 [42880/50048]	Loss: 0.8931
Training Epoch: 23 [43008/50048]	Loss: 0.8533
Training Epoch: 23 [43136/50048]	Loss: 1.0814
Training Epoch: 23 [43264/50048]	Loss: 0.8241
Training Epoch: 23 [43392/50048]	Loss: 1.0240
Training Epoch: 23 [43520/50048]	Loss: 0.7522
Training Epoch: 23 [43648/50048]	Loss: 1.0617
Training Epoch: 23 [43776/50048]	Loss: 0.8972
Training Epoch: 23 [43904/50048]	Loss: 1.1619
Training Epoch: 23 [44032/50048]	Loss: 0.8504
Training Epoch: 23 [44160/50048]	Loss: 0.8762
Training Epoch: 23 [44288/50048]	Loss: 0.7764
Training Epoch: 23 [44416/50048]	Loss: 0.9810
Training Epoch: 23 [44544/50048]	Loss: 0.9505
Training Epoch: 23 [44672/50048]	Loss: 1.0829
Training Epoch: 23 [44800/50048]	Loss: 0.9866
Training Epoch: 23 [44928/50048]	Loss: 1.0065
Training Epoch: 23 [45056/50048]	Loss: 1.0652
Training Epoch: 23 [45184/50048]	Loss: 0.9508
Training Epoch: 23 [45312/50048]	Loss: 0.8441
Training Epoch: 23 [45440/50048]	Loss: 1.0234
Training Epoch: 23 [45568/50048]	Loss: 0.7481
Training Epoch: 23 [45696/50048]	Loss: 1.0180
2022-12-06 06:43:55,146 [ZeusDataLoader(train)] train epoch 24 done: time=86.51 energy=10509.71
2022-12-06 06:43:55,147 [ZeusDataLoader(eval)] Epoch 24 begin.
Training Epoch: 23 [45824/50048]	Loss: 0.8233
Training Epoch: 23 [45952/50048]	Loss: 1.0683
Training Epoch: 23 [46080/50048]	Loss: 1.1380
Training Epoch: 23 [46208/50048]	Loss: 0.8497
Training Epoch: 23 [46336/50048]	Loss: 1.0823
Training Epoch: 23 [46464/50048]	Loss: 0.8325
Training Epoch: 23 [46592/50048]	Loss: 0.8713
Training Epoch: 23 [46720/50048]	Loss: 1.1571
Training Epoch: 23 [46848/50048]	Loss: 0.9034
Training Epoch: 23 [46976/50048]	Loss: 0.9278
Training Epoch: 23 [47104/50048]	Loss: 0.8595
Training Epoch: 23 [47232/50048]	Loss: 0.7484
Training Epoch: 23 [47360/50048]	Loss: 1.0011
Training Epoch: 23 [47488/50048]	Loss: 0.8648
Training Epoch: 23 [47616/50048]	Loss: 0.9474
Training Epoch: 23 [47744/50048]	Loss: 0.8772
Training Epoch: 23 [47872/50048]	Loss: 0.9843
Training Epoch: 23 [48000/50048]	Loss: 1.1391
Training Epoch: 23 [48128/50048]	Loss: 0.7985
Training Epoch: 23 [48256/50048]	Loss: 1.0756
Training Epoch: 23 [48384/50048]	Loss: 0.9052
Training Epoch: 23 [48512/50048]	Loss: 0.9989
Training Epoch: 23 [48640/50048]	Loss: 0.8862
Training Epoch: 23 [48768/50048]	Loss: 0.9444
Training Epoch: 23 [48896/50048]	Loss: 0.9205
Training Epoch: 23 [49024/50048]	Loss: 0.9627
Training Epoch: 23 [49152/50048]	Loss: 0.8337
Training Epoch: 23 [49280/50048]	Loss: 0.7900
Training Epoch: 23 [49408/50048]	Loss: 0.8048
Training Epoch: 23 [49536/50048]	Loss: 0.9850
Training Epoch: 23 [49664/50048]	Loss: 0.8934
Training Epoch: 23 [49792/50048]	Loss: 0.8017
Training Epoch: 23 [49920/50048]	Loss: 1.0260
Training Epoch: 23 [50048/50048]	Loss: 1.0630
2022-12-06 11:43:58.802 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:43:58,825 [ZeusDataLoader(eval)] eval epoch 24 done: time=3.67 energy=441.02
2022-12-06 06:43:58,825 [ZeusDataLoader(train)] Up to epoch 24: time=2164.69, energy=262781.43, cost=320801.06
2022-12-06 06:43:58,825 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:43:58,826 [ZeusDataLoader(train)] Expected next epoch: time=2254.49, energy=273579.45, cost=334057.44
2022-12-06 06:43:58,826 [ZeusDataLoader(train)] Epoch 25 begin.
Validation Epoch: 23, Average loss: 0.0119, Accuracy: 0.6097
2022-12-06 06:43:59,018 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:43:59,019 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:43:59.021 [ZeusMonitor] Monitor started.
2022-12-06 11:43:59.021 [ZeusMonitor] Running indefinitely. 2022-12-06 11:43:59.021 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:43:59.021 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e25+gpu0.power.log
Training Epoch: 24 [128/50048]	Loss: 0.5876
Training Epoch: 24 [256/50048]	Loss: 0.7936
Training Epoch: 24 [384/50048]	Loss: 0.6225
Training Epoch: 24 [512/50048]	Loss: 0.7511
Training Epoch: 24 [640/50048]	Loss: 0.7713
Training Epoch: 24 [768/50048]	Loss: 0.7270
Training Epoch: 24 [896/50048]	Loss: 0.6943
Training Epoch: 24 [1024/50048]	Loss: 0.8274
Training Epoch: 24 [1152/50048]	Loss: 0.8610
Training Epoch: 24 [1280/50048]	Loss: 0.9387
Training Epoch: 24 [1408/50048]	Loss: 0.5889
Training Epoch: 24 [1536/50048]	Loss: 0.5361
Training Epoch: 24 [1664/50048]	Loss: 0.6671
Training Epoch: 24 [1792/50048]	Loss: 0.9674
Training Epoch: 24 [1920/50048]	Loss: 0.8837
Training Epoch: 24 [2048/50048]	Loss: 0.7000
Training Epoch: 24 [2176/50048]	Loss: 0.8161
Training Epoch: 24 [2304/50048]	Loss: 0.7929
Training Epoch: 24 [2432/50048]	Loss: 0.7801
Training Epoch: 24 [2560/50048]	Loss: 0.7331
Training Epoch: 24 [2688/50048]	Loss: 0.8094
Training Epoch: 24 [2816/50048]	Loss: 0.9693
Training Epoch: 24 [2944/50048]	Loss: 0.9461
Training Epoch: 24 [3072/50048]	Loss: 0.8293
Training Epoch: 24 [3200/50048]	Loss: 0.8068
Training Epoch: 24 [3328/50048]	Loss: 0.7543
Training Epoch: 24 [3456/50048]	Loss: 0.6355
Training Epoch: 24 [3584/50048]	Loss: 0.7381
Training Epoch: 24 [3712/50048]	Loss: 0.9111
Training Epoch: 24 [3840/50048]	Loss: 0.7974
Training Epoch: 24 [3968/50048]	Loss: 0.7479
Training Epoch: 24 [4096/50048]	Loss: 0.7115
Training Epoch: 24 [4224/50048]	Loss: 0.9854
Training Epoch: 24 [4352/50048]	Loss: 0.7940
Training Epoch: 24 [4480/50048]	Loss: 0.6823
Training Epoch: 24 [4608/50048]	Loss: 0.8638
Training Epoch: 24 [4736/50048]	Loss: 0.7467
Training Epoch: 24 [4864/50048]	Loss: 0.9016
Training Epoch: 24 [4992/50048]	Loss: 1.0045
Training Epoch: 24 [5120/50048]	Loss: 0.7983
Training Epoch: 24 [5248/50048]	Loss: 0.5863
Training Epoch: 24 [5376/50048]	Loss: 0.8572
Training Epoch: 24 [5504/50048]	Loss: 0.8467
Training Epoch: 24 [5632/50048]	Loss: 0.7415
Training Epoch: 24 [5760/50048]	Loss: 0.8163
Training Epoch: 24 [5888/50048]	Loss: 0.8302
Training Epoch: 24 [6016/50048]	Loss: 0.7741
Training Epoch: 24 [6144/50048]	Loss: 0.8237
Training Epoch: 24 [6272/50048]	Loss: 0.8060
Training Epoch: 24 [6400/50048]	Loss: 0.6234
Training Epoch: 24 [6528/50048]	Loss: 0.8965
Training Epoch: 24 [6656/50048]	Loss: 0.9928
Training Epoch: 24 [6784/50048]	Loss: 0.8912
Training Epoch: 24 [6912/50048]	Loss: 0.8348
Training Epoch: 24 [7040/50048]	Loss: 0.9805
Training Epoch: 24 [7168/50048]	Loss: 0.7878
Training Epoch: 24 [7296/50048]	Loss: 0.8154
Training Epoch: 24 [7424/50048]	Loss: 0.7486
Training Epoch: 24 [7552/50048]	Loss: 0.9486
Training Epoch: 24 [7680/50048]	Loss: 0.7560
Training Epoch: 24 [7808/50048]	Loss: 0.8792
Training Epoch: 24 [7936/50048]	Loss: 0.7476
Training Epoch: 24 [8064/50048]	Loss: 0.9814
Training Epoch: 24 [8192/50048]	Loss: 0.7913
Training Epoch: 24 [8320/50048]	Loss: 0.8859
Training Epoch: 24 [8448/50048]	Loss: 0.7745
Training Epoch: 24 [8576/50048]	Loss: 0.8551
Training Epoch: 24 [8704/50048]	Loss: 0.8287
Training Epoch: 24 [8832/50048]	Loss: 0.8723
Training Epoch: 24 [8960/50048]	Loss: 0.6312
Training Epoch: 24 [9088/50048]	Loss: 0.7067
Training Epoch: 24 [9216/50048]	Loss: 1.0102
Training Epoch: 24 [9344/50048]	Loss: 0.9084
Training Epoch: 24 [9472/50048]	Loss: 1.0475
Training Epoch: 24 [9600/50048]	Loss: 0.6598
Training Epoch: 24 [9728/50048]	Loss: 0.9840
Training Epoch: 24 [9856/50048]	Loss: 0.9015
Training Epoch: 24 [9984/50048]	Loss: 0.5900
Training Epoch: 24 [10112/50048]	Loss: 0.5627
Training Epoch: 24 [10240/50048]	Loss: 0.9779
Training Epoch: 24 [10368/50048]	Loss: 0.8425
Training Epoch: 24 [10496/50048]	Loss: 0.7358
Training Epoch: 24 [10624/50048]	Loss: 0.8386
Training Epoch: 24 [10752/50048]	Loss: 0.7519
Training Epoch: 24 [10880/50048]	Loss: 0.7596
Training Epoch: 24 [11008/50048]	Loss: 0.8914
Training Epoch: 24 [11136/50048]	Loss: 0.8867
Training Epoch: 24 [11264/50048]	Loss: 0.8175
Training Epoch: 24 [11392/50048]	Loss: 0.8711
Training Epoch: 24 [11520/50048]	Loss: 0.7153
Training Epoch: 24 [11648/50048]	Loss: 0.8538
Training Epoch: 24 [11776/50048]	Loss: 1.1323
Training Epoch: 24 [11904/50048]	Loss: 0.7594
Training Epoch: 24 [12032/50048]	Loss: 0.6456
Training Epoch: 24 [12160/50048]	Loss: 0.7733
Training Epoch: 24 [12288/50048]	Loss: 0.8753
Training Epoch: 24 [12416/50048]	Loss: 0.8933
Training Epoch: 24 [12544/50048]	Loss: 0.7255
Training Epoch: 24 [12672/50048]	Loss: 0.7834
Training Epoch: 24 [12800/50048]	Loss: 0.9181
Training Epoch: 24 [12928/50048]	Loss: 0.9448
Training Epoch: 24 [13056/50048]	Loss: 0.7659
Training Epoch: 24 [13184/50048]	Loss: 0.7122
Training Epoch: 24 [13312/50048]	Loss: 0.6987
Training Epoch: 24 [13440/50048]	Loss: 0.8403
Training Epoch: 24 [13568/50048]	Loss: 0.9362
Training Epoch: 24 [13696/50048]	Loss: 0.8112
Training Epoch: 24 [13824/50048]	Loss: 0.9552
Training Epoch: 24 [13952/50048]	Loss: 0.8556
Training Epoch: 24 [14080/50048]	Loss: 0.9376
Training Epoch: 24 [14208/50048]	Loss: 0.7292
Training Epoch: 24 [14336/50048]	Loss: 1.0152
Training Epoch: 24 [14464/50048]	Loss: 0.7483
Training Epoch: 24 [14592/50048]	Loss: 0.8650
Training Epoch: 24 [14720/50048]	Loss: 0.9158
Training Epoch: 24 [14848/50048]	Loss: 0.6017
Training Epoch: 24 [14976/50048]	Loss: 0.8219
Training Epoch: 24 [15104/50048]	Loss: 0.7844
Training Epoch: 24 [15232/50048]	Loss: 0.9884
Training Epoch: 24 [15360/50048]	Loss: 0.8291
Training Epoch: 24 [15488/50048]	Loss: 0.7776
Training Epoch: 24 [15616/50048]	Loss: 0.8196
Training Epoch: 24 [15744/50048]	Loss: 0.6618
Training Epoch: 24 [15872/50048]	Loss: 0.6621
Training Epoch: 24 [16000/50048]	Loss: 0.9550
Training Epoch: 24 [16128/50048]	Loss: 0.6828
Training Epoch: 24 [16256/50048]	Loss: 0.8070
Training Epoch: 24 [16384/50048]	Loss: 0.9991
Training Epoch: 24 [16512/50048]	Loss: 0.9172
Training Epoch: 24 [16640/50048]	Loss: 0.9193
Training Epoch: 24 [16768/50048]	Loss: 0.8208
Training Epoch: 24 [16896/50048]	Loss: 0.8720
Training Epoch: 24 [17024/50048]	Loss: 0.9336
Training Epoch: 24 [17152/50048]	Loss: 0.9805
Training Epoch: 24 [17280/50048]	Loss: 0.9235
Training Epoch: 24 [17408/50048]	Loss: 0.7506
Training Epoch: 24 [17536/50048]	Loss: 0.9672
Training Epoch: 24 [17664/50048]	Loss: 0.9781
Training Epoch: 24 [17792/50048]	Loss: 0.9013
Training Epoch: 24 [17920/50048]	Loss: 0.8469
Training Epoch: 24 [18048/50048]	Loss: 0.8563
Training Epoch: 24 [18176/50048]	Loss: 1.0022
Training Epoch: 24 [18304/50048]	Loss: 0.9649
Training Epoch: 24 [18432/50048]	Loss: 0.7569
Training Epoch: 24 [18560/50048]	Loss: 0.8408
Training Epoch: 24 [18688/50048]	Loss: 0.8789
Training Epoch: 24 [18816/50048]	Loss: 0.8541
Training Epoch: 24 [18944/50048]	Loss: 0.9516
Training Epoch: 24 [19072/50048]	Loss: 0.9482
Training Epoch: 24 [19200/50048]	Loss: 0.8394
Training Epoch: 24 [19328/50048]	Loss: 0.8118
Training Epoch: 24 [19456/50048]	Loss: 0.8519
Training Epoch: 24 [19584/50048]	Loss: 1.2203
Training Epoch: 24 [19712/50048]	Loss: 0.9162
Training Epoch: 24 [19840/50048]	Loss: 0.8656
Training Epoch: 24 [19968/50048]	Loss: 0.7020
Training Epoch: 24 [20096/50048]	Loss: 0.8380
Training Epoch: 24 [20224/50048]	Loss: 0.7735
Training Epoch: 24 [20352/50048]	Loss: 0.7337
Training Epoch: 24 [20480/50048]	Loss: 0.8336
Training Epoch: 24 [20608/50048]	Loss: 0.7361
Training Epoch: 24 [20736/50048]	Loss: 0.7173
Training Epoch: 24 [20864/50048]	Loss: 0.9218
Training Epoch: 24 [20992/50048]	Loss: 0.8748
Training Epoch: 24 [21120/50048]	Loss: 0.9651
Training Epoch: 24 [21248/50048]	Loss: 0.8417
Training Epoch: 24 [21376/50048]	Loss: 1.0467
Training Epoch: 24 [21504/50048]	Loss: 1.2515
Training Epoch: 24 [21632/50048]	Loss: 0.8091
Training Epoch: 24 [21760/50048]	Loss: 0.9869
Training Epoch: 24 [21888/50048]	Loss: 0.7282
Training Epoch: 24 [22016/50048]	Loss: 0.7227
Training Epoch: 24 [22144/50048]	Loss: 0.8030
Training Epoch: 24 [22272/50048]	Loss: 0.9085
Training Epoch: 24 [22400/50048]	Loss: 0.7267
Training Epoch: 24 [22528/50048]	Loss: 0.9161
Training Epoch: 24 [22656/50048]	Loss: 0.7861
Training Epoch: 24 [22784/50048]	Loss: 1.1179
Training Epoch: 24 [22912/50048]	Loss: 0.8866
Training Epoch: 24 [23040/50048]	Loss: 0.8009
Training Epoch: 24 [23168/50048]	Loss: 0.8980
Training Epoch: 24 [23296/50048]	Loss: 1.0489
Training Epoch: 24 [23424/50048]	Loss: 0.9024
Training Epoch: 24 [23552/50048]	Loss: 1.0731
Training Epoch: 24 [23680/50048]	Loss: 0.7858
Training Epoch: 24 [23808/50048]	Loss: 0.9972
Training Epoch: 24 [23936/50048]	Loss: 0.6651
Training Epoch: 24 [24064/50048]	Loss: 0.9211
Training Epoch: 24 [24192/50048]	Loss: 0.8436
Training Epoch: 24 [24320/50048]	Loss: 0.7508
Training Epoch: 24 [24448/50048]	Loss: 0.8052
Training Epoch: 24 [24576/50048]	Loss: 0.7213
Training Epoch: 24 [24704/50048]	Loss: 0.7389
Training Epoch: 24 [24832/50048]	Loss: 0.8499
Training Epoch: 24 [24960/50048]	Loss: 1.0899
Training Epoch: 24 [25088/50048]	Loss: 0.9345
Training Epoch: 24 [25216/50048]	Loss: 0.7765
Training Epoch: 24 [25344/50048]	Loss: 0.8983
Training Epoch: 24 [25472/50048]	Loss: 0.9854
Training Epoch: 24 [25600/50048]	Loss: 0.7343
Training Epoch: 24 [25728/50048]	Loss: 0.7625
Training Epoch: 24 [25856/50048]	Loss: 0.8022
Training Epoch: 24 [25984/50048]	Loss: 0.8946
Training Epoch: 24 [26112/50048]	Loss: 1.0584
Training Epoch: 24 [26240/50048]	Loss: 1.1077
Training Epoch: 24 [26368/50048]	Loss: 0.7923
Training Epoch: 24 [26496/50048]	Loss: 0.9041
Training Epoch: 24 [26624/50048]	Loss: 0.8894
Training Epoch: 24 [26752/50048]	Loss: 1.0223
Training Epoch: 24 [26880/50048]	Loss: 0.7468
Training Epoch: 24 [27008/50048]	Loss: 0.8822
Training Epoch: 24 [27136/50048]	Loss: 0.8543
Training Epoch: 24 [27264/50048]	Loss: 0.7802
Training Epoch: 24 [27392/50048]	Loss: 0.8843
Training Epoch: 24 [27520/50048]	Loss: 0.9372
Training Epoch: 24 [27648/50048]	Loss: 0.9219
Training Epoch: 24 [27776/50048]	Loss: 1.0206
Training Epoch: 24 [27904/50048]	Loss: 0.7797
Training Epoch: 24 [28032/50048]	Loss: 0.9445
Training Epoch: 24 [28160/50048]	Loss: 0.9572
Training Epoch: 24 [28288/50048]	Loss: 0.8300
Training Epoch: 24 [28416/50048]	Loss: 0.8042
Training Epoch: 24 [28544/50048]	Loss: 0.7497
Training Epoch: 24 [28672/50048]	Loss: 0.8024
Training Epoch: 24 [28800/50048]	Loss: 0.6849
Training Epoch: 24 [28928/50048]	Loss: 0.7800
Training Epoch: 24 [29056/50048]	Loss: 0.7488
Training Epoch: 24 [29184/50048]	Loss: 0.7708
Training Epoch: 24 [29312/50048]	Loss: 0.8444
Training Epoch: 24 [29440/50048]	Loss: 0.9758
Training Epoch: 24 [29568/50048]	Loss: 0.7273
Training Epoch: 24 [29696/50048]	Loss: 0.9940
Training Epoch: 24 [29824/50048]	Loss: 0.9136
Training Epoch: 24 [29952/50048]	Loss: 0.8968
Training Epoch: 24 [30080/50048]	Loss: 0.7872
Training Epoch: 24 [30208/50048]	Loss: 0.8338
Training Epoch: 24 [30336/50048]	Loss: 0.6193
Training Epoch: 24 [30464/50048]	Loss: 1.1156
Training Epoch: 24 [30592/50048]	Loss: 0.8911
Training Epoch: 24 [30720/50048]	Loss: 0.9595
Training Epoch: 24 [30848/50048]	Loss: 0.7632
Training Epoch: 24 [30976/50048]	Loss: 0.8890
Training Epoch: 24 [31104/50048]	Loss: 0.9656
Training Epoch: 24 [31232/50048]	Loss: 0.8753
Training Epoch: 24 [31360/50048]	Loss: 0.9073
Training Epoch: 24 [31488/50048]	Loss: 1.0283
Training Epoch: 24 [31616/50048]	Loss: 0.9206
Training Epoch: 24 [31744/50048]	Loss: 0.8951
Training Epoch: 24 [31872/50048]	Loss: 0.8845
Training Epoch: 24 [32000/50048]	Loss: 0.9160
Training Epoch: 24 [32128/50048]	Loss: 0.7498
Training Epoch: 24 [32256/50048]	Loss: 0.7047
Training Epoch: 24 [32384/50048]	Loss: 0.9555
Training Epoch: 24 [32512/50048]	Loss: 0.8733
Training Epoch: 24 [32640/50048]	Loss: 0.9922
Training Epoch: 24 [32768/50048]	Loss: 0.9341
Training Epoch: 24 [32896/50048]	Loss: 0.8818
Training Epoch: 24 [33024/50048]	Loss: 0.7301
Training Epoch: 24 [33152/50048]	Loss: 0.9362
Training Epoch: 24 [33280/50048]	Loss: 0.9148
Training Epoch: 24 [33408/50048]	Loss: 1.0115
Training Epoch: 24 [33536/50048]	Loss: 1.0296
Training Epoch: 24 [33664/50048]	Loss: 0.7286
Training Epoch: 24 [33792/50048]	Loss: 0.8558
Training Epoch: 24 [33920/50048]	Loss: 0.9824
Training Epoch: 24 [34048/50048]	Loss: 0.9448
Training Epoch: 24 [34176/50048]	Loss: 0.7569
Training Epoch: 24 [34304/50048]	Loss: 1.0501
Training Epoch: 24 [34432/50048]	Loss: 1.0783
Training Epoch: 24 [34560/50048]	Loss: 0.9597
Training Epoch: 24 [34688/50048]	Loss: 0.7598
Training Epoch: 24 [34816/50048]	Loss: 1.0378
Training Epoch: 24 [34944/50048]	Loss: 0.8381
Training Epoch: 24 [35072/50048]	Loss: 0.9815
Training Epoch: 24 [35200/50048]	Loss: 0.8047
Training Epoch: 24 [35328/50048]	Loss: 1.1525
Training Epoch: 24 [35456/50048]	Loss: 0.8943
Training Epoch: 24 [35584/50048]	Loss: 1.0379
Training Epoch: 24 [35712/50048]	Loss: 0.7849
Training Epoch: 24 [35840/50048]	Loss: 0.9116
Training Epoch: 24 [35968/50048]	Loss: 0.9441
Training Epoch: 24 [36096/50048]	Loss: 0.9302
Training Epoch: 24 [36224/50048]	Loss: 0.8063
Training Epoch: 24 [36352/50048]	Loss: 0.8678
Training Epoch: 24 [36480/50048]	Loss: 0.8658
Training Epoch: 24 [36608/50048]	Loss: 0.7701
Training Epoch: 24 [36736/50048]	Loss: 0.7547
Training Epoch: 24 [36864/50048]	Loss: 0.9795
Training Epoch: 24 [36992/50048]	Loss: 0.9919
Training Epoch: 24 [37120/50048]	Loss: 0.9696
Training Epoch: 24 [37248/50048]	Loss: 0.9680
Training Epoch: 24 [37376/50048]	Loss: 0.9386
Training Epoch: 24 [37504/50048]	Loss: 0.8597
Training Epoch: 24 [37632/50048]	Loss: 0.7987
Training Epoch: 24 [37760/50048]	Loss: 0.7736
Training Epoch: 24 [37888/50048]	Loss: 0.9322
Training Epoch: 24 [38016/50048]	Loss: 1.0292
Training Epoch: 24 [38144/50048]	Loss: 0.9816
Training Epoch: 24 [38272/50048]	Loss: 0.7628
Training Epoch: 24 [38400/50048]	Loss: 0.8301
Training Epoch: 24 [38528/50048]	Loss: 0.8125
Training Epoch: 24 [38656/50048]	Loss: 0.9086
Training Epoch: 24 [38784/50048]	Loss: 1.0211
Training Epoch: 24 [38912/50048]	Loss: 0.9707
Training Epoch: 24 [39040/50048]	Loss: 0.8189
Training Epoch: 24 [39168/50048]	Loss: 0.7493
Training Epoch: 24 [39296/50048]	Loss: 0.7963
Training Epoch: 24 [39424/50048]	Loss: 0.6724
Training Epoch: 24 [39552/50048]	Loss: 0.9930
Training Epoch: 24 [39680/50048]	Loss: 1.1430
Training Epoch: 24 [39808/50048]	Loss: 0.7982
Training Epoch: 24 [39936/50048]	Loss: 0.7276
Training Epoch: 24 [40064/50048]	Loss: 0.7478
Training Epoch: 24 [40192/50048]	Loss: 0.7564
Training Epoch: 24 [40320/50048]	Loss: 0.9851
Training Epoch: 24 [40448/50048]	Loss: 0.7371
Training Epoch: 24 [40576/50048]	Loss: 0.7116
Training Epoch: 24 [40704/50048]	Loss: 0.8392
Training Epoch: 24 [40832/50048]	Loss: 0.8250
Training Epoch: 24 [40960/50048]	Loss: 0.9537
Training Epoch: 24 [41088/50048]	Loss: 0.9416
Training Epoch: 24 [41216/50048]	Loss: 0.8129
Training Epoch: 24 [41344/50048]	Loss: 0.7324
Training Epoch: 24 [41472/50048]	Loss: 0.8904
Training Epoch: 24 [41600/50048]	Loss: 1.0778
Training Epoch: 24 [41728/50048]	Loss: 0.8803
Training Epoch: 24 [41856/50048]	Loss: 1.1332
Training Epoch: 24 [41984/50048]	Loss: 0.7402
Training Epoch: 24 [42112/50048]	Loss: 1.0374
Training Epoch: 24 [42240/50048]	Loss: 0.9665
Training Epoch: 24 [42368/50048]	Loss: 0.7886
Training Epoch: 24 [42496/50048]	Loss: 0.8636
Training Epoch: 24 [42624/50048]	Loss: 1.1403
Training Epoch: 24 [42752/50048]	Loss: 0.9738
Training Epoch: 24 [42880/50048]	Loss: 0.9833
Training Epoch: 24 [43008/50048]	Loss: 0.8582
Training Epoch: 24 [43136/50048]	Loss: 0.9449
Training Epoch: 24 [43264/50048]	Loss: 0.8724
Training Epoch: 24 [43392/50048]	Loss: 0.6615
Training Epoch: 24 [43520/50048]	Loss: 0.8310
Training Epoch: 24 [43648/50048]	Loss: 0.8826
Training Epoch: 24 [43776/50048]	Loss: 0.5121
Training Epoch: 24 [43904/50048]	Loss: 1.0095
Training Epoch: 24 [44032/50048]	Loss: 0.7407
Training Epoch: 24 [44160/50048]	Loss: 1.0537
Training Epoch: 24 [44288/50048]	Loss: 0.6900
Training Epoch: 24 [44416/50048]	Loss: 0.8088
Training Epoch: 24 [44544/50048]	Loss: 0.7325
Training Epoch: 24 [44672/50048]	Loss: 0.9870
Training Epoch: 24 [44800/50048]	Loss: 0.9375
Training Epoch: 24 [44928/50048]	Loss: 1.0803
Training Epoch: 24 [45056/50048]	Loss: 0.8044
Training Epoch: 24 [45184/50048]	Loss: 0.7618
Training Epoch: 24 [45312/50048]	Loss: 0.7052
Training Epoch: 24 [45440/50048]	Loss: 0.8960
Training Epoch: 24 [45568/50048]	Loss: 1.0643
Training Epoch: 24 [45696/50048]	Loss: 1.0305
2022-12-06 06:45:25,270 [ZeusDataLoader(train)] train epoch 25 done: time=86.43 energy=10484.48
2022-12-06 06:45:25,272 [ZeusDataLoader(eval)] Epoch 25 begin.
Training Epoch: 24 [45824/50048]	Loss: 0.7630
Training Epoch: 24 [45952/50048]	Loss: 0.8500
Training Epoch: 24 [46080/50048]	Loss: 0.8822
Training Epoch: 24 [46208/50048]	Loss: 1.0180
Training Epoch: 24 [46336/50048]	Loss: 1.1950
Training Epoch: 24 [46464/50048]	Loss: 1.0065
Training Epoch: 24 [46592/50048]	Loss: 0.7501
Training Epoch: 24 [46720/50048]	Loss: 0.8685
Training Epoch: 24 [46848/50048]	Loss: 1.0906
Training Epoch: 24 [46976/50048]	Loss: 0.9785
Training Epoch: 24 [47104/50048]	Loss: 0.8449
Training Epoch: 24 [47232/50048]	Loss: 1.0408
Training Epoch: 24 [47360/50048]	Loss: 1.0427
Training Epoch: 24 [47488/50048]	Loss: 1.0466
Training Epoch: 24 [47616/50048]	Loss: 0.8938
Training Epoch: 24 [47744/50048]	Loss: 1.2397
Training Epoch: 24 [47872/50048]	Loss: 1.1147
Training Epoch: 24 [48000/50048]	Loss: 0.9477
Training Epoch: 24 [48128/50048]	Loss: 0.7954
Training Epoch: 24 [48256/50048]	Loss: 0.8417
Training Epoch: 24 [48384/50048]	Loss: 0.8230
Training Epoch: 24 [48512/50048]	Loss: 0.7652
Training Epoch: 24 [48640/50048]	Loss: 0.9938
Training Epoch: 24 [48768/50048]	Loss: 0.8086
Training Epoch: 24 [48896/50048]	Loss: 1.0276
Training Epoch: 24 [49024/50048]	Loss: 0.9270
Training Epoch: 24 [49152/50048]	Loss: 1.0168
Training Epoch: 24 [49280/50048]	Loss: 0.8233
Training Epoch: 24 [49408/50048]	Loss: 0.9606
Training Epoch: 24 [49536/50048]	Loss: 0.7429
Training Epoch: 24 [49664/50048]	Loss: 1.1399
Training Epoch: 24 [49792/50048]	Loss: 0.8997
Training Epoch: 24 [49920/50048]	Loss: 0.8823
Training Epoch: 24 [50048/50048]	Loss: 0.9477
2022-12-06 11:45:28.966 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:45:29,011 [ZeusDataLoader(eval)] eval epoch 25 done: time=3.73 energy=454.57
2022-12-06 06:45:29,012 [ZeusDataLoader(train)] Up to epoch 25: time=2254.85, energy=273720.49, cost=334159.97
2022-12-06 06:45:29,012 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:45:29,012 [ZeusDataLoader(train)] Expected next epoch: time=2344.65, energy=284518.50, cost=347416.36
2022-12-06 06:45:29,013 [ZeusDataLoader(train)] Epoch 26 begin.
Validation Epoch: 24, Average loss: 0.0120, Accuracy: 0.6104
2022-12-06 06:45:29,206 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:45:29,207 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:45:29.209 [ZeusMonitor] Monitor started.
2022-12-06 11:45:29.209 [ZeusMonitor] Running indefinitely. 2022-12-06 11:45:29.209 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:45:29.209 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e26+gpu0.power.log
Training Epoch: 25 [128/50048]	Loss: 0.7344
Training Epoch: 25 [256/50048]	Loss: 0.8527
Training Epoch: 25 [384/50048]	Loss: 0.6366
Training Epoch: 25 [512/50048]	Loss: 0.6675
Training Epoch: 25 [640/50048]	Loss: 0.6670
Training Epoch: 25 [768/50048]	Loss: 0.7469
Training Epoch: 25 [896/50048]	Loss: 0.7543
Training Epoch: 25 [1024/50048]	Loss: 0.9567
Training Epoch: 25 [1152/50048]	Loss: 0.6861
Training Epoch: 25 [1280/50048]	Loss: 0.6439
Training Epoch: 25 [1408/50048]	Loss: 0.6381
Training Epoch: 25 [1536/50048]	Loss: 0.8189
Training Epoch: 25 [1664/50048]	Loss: 0.7491
Training Epoch: 25 [1792/50048]	Loss: 0.6287
Training Epoch: 25 [1920/50048]	Loss: 0.8486
Training Epoch: 25 [2048/50048]	Loss: 0.7162
Training Epoch: 25 [2176/50048]	Loss: 0.9041
Training Epoch: 25 [2304/50048]	Loss: 0.8833
Training Epoch: 25 [2432/50048]	Loss: 0.5637
Training Epoch: 25 [2560/50048]	Loss: 0.7738
Training Epoch: 25 [2688/50048]	Loss: 0.8314
Training Epoch: 25 [2816/50048]	Loss: 0.7983
Training Epoch: 25 [2944/50048]	Loss: 0.6753
Training Epoch: 25 [3072/50048]	Loss: 1.0322
Training Epoch: 25 [3200/50048]	Loss: 0.8881
Training Epoch: 25 [3328/50048]	Loss: 0.8816
Training Epoch: 25 [3456/50048]	Loss: 0.8559
Training Epoch: 25 [3584/50048]	Loss: 0.6326
Training Epoch: 25 [3712/50048]	Loss: 0.6195
Training Epoch: 25 [3840/50048]	Loss: 0.8057
Training Epoch: 25 [3968/50048]	Loss: 0.7026
Training Epoch: 25 [4096/50048]	Loss: 0.7022
Training Epoch: 25 [4224/50048]	Loss: 0.7125
Training Epoch: 25 [4352/50048]	Loss: 0.7281
Training Epoch: 25 [4480/50048]	Loss: 0.7475
Training Epoch: 25 [4608/50048]	Loss: 0.8437
Training Epoch: 25 [4736/50048]	Loss: 0.7982
Training Epoch: 25 [4864/50048]	Loss: 0.6898
Training Epoch: 25 [4992/50048]	Loss: 0.7396
Training Epoch: 25 [5120/50048]	Loss: 0.8284
Training Epoch: 25 [5248/50048]	Loss: 0.8649
Training Epoch: 25 [5376/50048]	Loss: 0.8956
Training Epoch: 25 [5504/50048]	Loss: 0.8212
Training Epoch: 25 [5632/50048]	Loss: 0.7102
Training Epoch: 25 [5760/50048]	Loss: 0.7988
Training Epoch: 25 [5888/50048]	Loss: 0.7490
Training Epoch: 25 [6016/50048]	Loss: 0.7988
Training Epoch: 25 [6144/50048]	Loss: 0.7819
Training Epoch: 25 [6272/50048]	Loss: 0.6083
Training Epoch: 25 [6400/50048]	Loss: 0.7388
Training Epoch: 25 [6528/50048]	Loss: 0.6786
Training Epoch: 25 [6656/50048]	Loss: 0.6869
Training Epoch: 25 [6784/50048]	Loss: 0.9297
Training Epoch: 25 [6912/50048]	Loss: 0.7377
Training Epoch: 25 [7040/50048]	Loss: 0.7686
Training Epoch: 25 [7168/50048]	Loss: 0.7457
Training Epoch: 25 [7296/50048]	Loss: 0.7453
Training Epoch: 25 [7424/50048]	Loss: 0.8376
Training Epoch: 25 [7552/50048]	Loss: 0.7124
Training Epoch: 25 [7680/50048]	Loss: 0.8067
Training Epoch: 25 [7808/50048]	Loss: 0.8784
Training Epoch: 25 [7936/50048]	Loss: 0.6258
Training Epoch: 25 [8064/50048]	Loss: 0.7724
Training Epoch: 25 [8192/50048]	Loss: 0.8783
Training Epoch: 25 [8320/50048]	Loss: 1.0267
Training Epoch: 25 [8448/50048]	Loss: 0.8193
Training Epoch: 25 [8576/50048]	Loss: 0.8193
Training Epoch: 25 [8704/50048]	Loss: 0.9176
Training Epoch: 25 [8832/50048]	Loss: 0.8127
Training Epoch: 25 [8960/50048]	Loss: 0.9465
Training Epoch: 25 [9088/50048]	Loss: 0.7776
Training Epoch: 25 [9216/50048]	Loss: 0.6625
Training Epoch: 25 [9344/50048]	Loss: 0.8369
Training Epoch: 25 [9472/50048]	Loss: 0.7599
Training Epoch: 25 [9600/50048]	Loss: 0.9577
Training Epoch: 25 [9728/50048]	Loss: 0.8328
Training Epoch: 25 [9856/50048]	Loss: 0.6442
Training Epoch: 25 [9984/50048]	Loss: 0.9113
Training Epoch: 25 [10112/50048]	Loss: 0.6887
Training Epoch: 25 [10240/50048]	Loss: 0.8006
Training Epoch: 25 [10368/50048]	Loss: 0.7343
Training Epoch: 25 [10496/50048]	Loss: 0.7367
Training Epoch: 25 [10624/50048]	Loss: 0.8756
Training Epoch: 25 [10752/50048]	Loss: 0.7065
Training Epoch: 25 [10880/50048]	Loss: 0.7702
Training Epoch: 25 [11008/50048]	Loss: 0.8903
Training Epoch: 25 [11136/50048]	Loss: 0.8141
Training Epoch: 25 [11264/50048]	Loss: 0.8553
Training Epoch: 25 [11392/50048]	Loss: 0.5477
Training Epoch: 25 [11520/50048]	Loss: 0.7755
Training Epoch: 25 [11648/50048]	Loss: 0.7350
Training Epoch: 25 [11776/50048]	Loss: 0.7003
Training Epoch: 25 [11904/50048]	Loss: 0.8034
Training Epoch: 25 [12032/50048]	Loss: 0.8199
Training Epoch: 25 [12160/50048]	Loss: 0.7405
Training Epoch: 25 [12288/50048]	Loss: 0.7764
Training Epoch: 25 [12416/50048]	Loss: 0.8477
Training Epoch: 25 [12544/50048]	Loss: 0.7644
Training Epoch: 25 [12672/50048]	Loss: 0.6953
Training Epoch: 25 [12800/50048]	Loss: 0.8016
Training Epoch: 25 [12928/50048]	Loss: 0.7779
Training Epoch: 25 [13056/50048]	Loss: 0.8253
Training Epoch: 25 [13184/50048]	Loss: 0.8362
Training Epoch: 25 [13312/50048]	Loss: 0.6728
Training Epoch: 25 [13440/50048]	Loss: 0.7385
Training Epoch: 25 [13568/50048]	Loss: 0.9390
Training Epoch: 25 [13696/50048]	Loss: 0.7516
Training Epoch: 25 [13824/50048]	Loss: 0.8389
Training Epoch: 25 [13952/50048]	Loss: 0.7390
Training Epoch: 25 [14080/50048]	Loss: 0.8997
Training Epoch: 25 [14208/50048]	Loss: 0.7283
Training Epoch: 25 [14336/50048]	Loss: 0.7289
Training Epoch: 25 [14464/50048]	Loss: 0.6637
Training Epoch: 25 [14592/50048]	Loss: 0.8596
Training Epoch: 25 [14720/50048]	Loss: 0.8066
Training Epoch: 25 [14848/50048]	Loss: 0.7570
Training Epoch: 25 [14976/50048]	Loss: 0.6394
Training Epoch: 25 [15104/50048]	Loss: 0.8180
Training Epoch: 25 [15232/50048]	Loss: 0.9237
Training Epoch: 25 [15360/50048]	Loss: 0.5741
Training Epoch: 25 [15488/50048]	Loss: 0.7237
Training Epoch: 25 [15616/50048]	Loss: 1.1204
Training Epoch: 25 [15744/50048]	Loss: 0.8196
Training Epoch: 25 [15872/50048]	Loss: 0.9721
Training Epoch: 25 [16000/50048]	Loss: 0.8799
Training Epoch: 25 [16128/50048]	Loss: 0.8718
Training Epoch: 25 [16256/50048]	Loss: 0.6738
Training Epoch: 25 [16384/50048]	Loss: 0.7426
Training Epoch: 25 [16512/50048]	Loss: 0.8025
Training Epoch: 25 [16640/50048]	Loss: 0.9399
Training Epoch: 25 [16768/50048]	Loss: 0.8320
Training Epoch: 25 [16896/50048]	Loss: 0.8247
Training Epoch: 25 [17024/50048]	Loss: 0.6905
Training Epoch: 25 [17152/50048]	Loss: 0.6553
Training Epoch: 25 [17280/50048]	Loss: 0.9087
Training Epoch: 25 [17408/50048]	Loss: 0.7231
Training Epoch: 25 [17536/50048]	Loss: 1.0678
Training Epoch: 25 [17664/50048]	Loss: 0.8770
Training Epoch: 25 [17792/50048]	Loss: 0.8676
Training Epoch: 25 [17920/50048]	Loss: 0.8941
Training Epoch: 25 [18048/50048]	Loss: 0.7753
Training Epoch: 25 [18176/50048]	Loss: 0.5642
Training Epoch: 25 [18304/50048]	Loss: 0.6679
Training Epoch: 25 [18432/50048]	Loss: 0.8431
Training Epoch: 25 [18560/50048]	Loss: 0.7545
Training Epoch: 25 [18688/50048]	Loss: 0.5621
Training Epoch: 25 [18816/50048]	Loss: 0.8101
Training Epoch: 25 [18944/50048]	Loss: 0.8133
Training Epoch: 25 [19072/50048]	Loss: 0.5556
Training Epoch: 25 [19200/50048]	Loss: 0.7666
Training Epoch: 25 [19328/50048]	Loss: 0.8744
Training Epoch: 25 [19456/50048]	Loss: 0.8229
Training Epoch: 25 [19584/50048]	Loss: 0.7389
Training Epoch: 25 [19712/50048]	Loss: 0.7940
Training Epoch: 25 [19840/50048]	Loss: 0.7915
Training Epoch: 25 [19968/50048]	Loss: 0.7320
Training Epoch: 25 [20096/50048]	Loss: 0.7506
Training Epoch: 25 [20224/50048]	Loss: 0.8309
Training Epoch: 25 [20352/50048]	Loss: 0.6570
Training Epoch: 25 [20480/50048]	Loss: 0.7947
Training Epoch: 25 [20608/50048]	Loss: 0.7108
Training Epoch: 25 [20736/50048]	Loss: 0.7326
Training Epoch: 25 [20864/50048]	Loss: 0.5495
Training Epoch: 25 [20992/50048]	Loss: 0.8612
Training Epoch: 25 [21120/50048]	Loss: 0.7119
Training Epoch: 25 [21248/50048]	Loss: 0.7502
Training Epoch: 25 [21376/50048]	Loss: 0.8791
Training Epoch: 25 [21504/50048]	Loss: 0.8021
Training Epoch: 25 [21632/50048]	Loss: 0.9306
Training Epoch: 25 [21760/50048]	Loss: 0.9906
Training Epoch: 25 [21888/50048]	Loss: 0.8963
Training Epoch: 25 [22016/50048]	Loss: 0.8986
Training Epoch: 25 [22144/50048]	Loss: 0.6594
Training Epoch: 25 [22272/50048]	Loss: 0.9140
Training Epoch: 25 [22400/50048]	Loss: 0.9400
Training Epoch: 25 [22528/50048]	Loss: 0.9966
Training Epoch: 25 [22656/50048]	Loss: 0.9527
Training Epoch: 25 [22784/50048]	Loss: 0.6477
Training Epoch: 25 [22912/50048]	Loss: 0.7718
Training Epoch: 25 [23040/50048]	Loss: 0.8585
Training Epoch: 25 [23168/50048]	Loss: 0.6761
Training Epoch: 25 [23296/50048]	Loss: 0.7995
Training Epoch: 25 [23424/50048]	Loss: 0.7995
Training Epoch: 25 [23552/50048]	Loss: 1.0210
Training Epoch: 25 [23680/50048]	Loss: 0.8563
Training Epoch: 25 [23808/50048]	Loss: 0.9879
Training Epoch: 25 [23936/50048]	Loss: 0.7277
Training Epoch: 25 [24064/50048]	Loss: 0.7183
Training Epoch: 25 [24192/50048]	Loss: 0.7788
Training Epoch: 25 [24320/50048]	Loss: 0.8513
Training Epoch: 25 [24448/50048]	Loss: 0.5804
Training Epoch: 25 [24576/50048]	Loss: 0.7674
Training Epoch: 25 [24704/50048]	Loss: 0.7046
Training Epoch: 25 [24832/50048]	Loss: 0.8094
Training Epoch: 25 [24960/50048]	Loss: 0.6546
Training Epoch: 25 [25088/50048]	Loss: 0.8670
Training Epoch: 25 [25216/50048]	Loss: 1.0508
Training Epoch: 25 [25344/50048]	Loss: 0.7621
Training Epoch: 25 [25472/50048]	Loss: 0.9417
Training Epoch: 25 [25600/50048]	Loss: 0.5837
Training Epoch: 25 [25728/50048]	Loss: 0.7971
Training Epoch: 25 [25856/50048]	Loss: 0.9626
Training Epoch: 25 [25984/50048]	Loss: 0.8643
Training Epoch: 25 [26112/50048]	Loss: 0.9577
Training Epoch: 25 [26240/50048]	Loss: 0.9253
Training Epoch: 25 [26368/50048]	Loss: 0.6886
Training Epoch: 25 [26496/50048]	Loss: 0.9816
Training Epoch: 25 [26624/50048]	Loss: 1.0075
Training Epoch: 25 [26752/50048]	Loss: 0.8697
Training Epoch: 25 [26880/50048]	Loss: 0.8040
Training Epoch: 25 [27008/50048]	Loss: 0.8354
Training Epoch: 25 [27136/50048]	Loss: 0.7153
Training Epoch: 25 [27264/50048]	Loss: 0.9746
Training Epoch: 25 [27392/50048]	Loss: 0.9028
Training Epoch: 25 [27520/50048]	Loss: 0.7994
Training Epoch: 25 [27648/50048]	Loss: 1.1354
Training Epoch: 25 [27776/50048]	Loss: 1.0035
Training Epoch: 25 [27904/50048]	Loss: 1.0541
Training Epoch: 25 [28032/50048]	Loss: 0.6346
Training Epoch: 25 [28160/50048]	Loss: 0.8457
Training Epoch: 25 [28288/50048]	Loss: 0.9752
Training Epoch: 25 [28416/50048]	Loss: 0.8255
Training Epoch: 25 [28544/50048]	Loss: 1.0300
Training Epoch: 25 [28672/50048]	Loss: 0.7610
Training Epoch: 25 [28800/50048]	Loss: 0.9876
Training Epoch: 25 [28928/50048]	Loss: 0.6136
Training Epoch: 25 [29056/50048]	Loss: 1.1266
Training Epoch: 25 [29184/50048]	Loss: 0.8511
Training Epoch: 25 [29312/50048]	Loss: 0.8311
Training Epoch: 25 [29440/50048]	Loss: 0.6897
Training Epoch: 25 [29568/50048]	Loss: 1.0063
Training Epoch: 25 [29696/50048]	Loss: 0.9519
Training Epoch: 25 [29824/50048]	Loss: 0.7470
Training Epoch: 25 [29952/50048]	Loss: 0.9733
Training Epoch: 25 [30080/50048]	Loss: 0.9307
Training Epoch: 25 [30208/50048]	Loss: 0.7106
Training Epoch: 25 [30336/50048]	Loss: 1.0570
Training Epoch: 25 [30464/50048]	Loss: 0.7432
Training Epoch: 25 [30592/50048]	Loss: 0.9474
Training Epoch: 25 [30720/50048]	Loss: 0.8192
Training Epoch: 25 [30848/50048]	Loss: 0.6680
Training Epoch: 25 [30976/50048]	Loss: 0.7343
Training Epoch: 25 [31104/50048]	Loss: 0.7056
Training Epoch: 25 [31232/50048]	Loss: 0.8951
Training Epoch: 25 [31360/50048]	Loss: 0.9894
Training Epoch: 25 [31488/50048]	Loss: 0.7211
Training Epoch: 25 [31616/50048]	Loss: 1.0163
Training Epoch: 25 [31744/50048]	Loss: 0.8537
Training Epoch: 25 [31872/50048]	Loss: 0.8534
Training Epoch: 25 [32000/50048]	Loss: 0.7839
Training Epoch: 25 [32128/50048]	Loss: 0.8577
Training Epoch: 25 [32256/50048]	Loss: 0.9125
Training Epoch: 25 [32384/50048]	Loss: 0.7234
Training Epoch: 25 [32512/50048]	Loss: 0.9190
Training Epoch: 25 [32640/50048]	Loss: 0.8477
Training Epoch: 25 [32768/50048]	Loss: 0.8487
Training Epoch: 25 [32896/50048]	Loss: 0.6950
Training Epoch: 25 [33024/50048]	Loss: 0.8573
Training Epoch: 25 [33152/50048]	Loss: 0.8446
Training Epoch: 25 [33280/50048]	Loss: 0.8153
Training Epoch: 25 [33408/50048]	Loss: 0.8876
Training Epoch: 25 [33536/50048]	Loss: 0.7748
Training Epoch: 25 [33664/50048]	Loss: 0.8264
Training Epoch: 25 [33792/50048]	Loss: 0.7899
Training Epoch: 25 [33920/50048]	Loss: 0.7533
Training Epoch: 25 [34048/50048]	Loss: 0.6676
Training Epoch: 25 [34176/50048]	Loss: 1.0818
Training Epoch: 25 [34304/50048]	Loss: 0.8251
Training Epoch: 25 [34432/50048]	Loss: 0.8701
Training Epoch: 25 [34560/50048]	Loss: 0.8689
Training Epoch: 25 [34688/50048]	Loss: 1.0696
Training Epoch: 25 [34816/50048]	Loss: 0.7360
Training Epoch: 25 [34944/50048]	Loss: 0.8227
Training Epoch: 25 [35072/50048]	Loss: 0.8732
Training Epoch: 25 [35200/50048]	Loss: 0.6910
Training Epoch: 25 [35328/50048]	Loss: 0.8090
Training Epoch: 25 [35456/50048]	Loss: 0.7918
Training Epoch: 25 [35584/50048]	Loss: 0.8630
Training Epoch: 25 [35712/50048]	Loss: 1.0489
Training Epoch: 25 [35840/50048]	Loss: 0.9517
Training Epoch: 25 [35968/50048]	Loss: 0.8808
Training Epoch: 25 [36096/50048]	Loss: 1.0424
Training Epoch: 25 [36224/50048]	Loss: 0.8036
Training Epoch: 25 [36352/50048]	Loss: 0.7848
Training Epoch: 25 [36480/50048]	Loss: 0.8151
Training Epoch: 25 [36608/50048]	Loss: 1.0121
Training Epoch: 25 [36736/50048]	Loss: 0.8967
Training Epoch: 25 [36864/50048]	Loss: 0.6390
Training Epoch: 25 [36992/50048]	Loss: 0.7312
Training Epoch: 25 [37120/50048]	Loss: 0.9551
Training Epoch: 25 [37248/50048]	Loss: 0.7581
Training Epoch: 25 [37376/50048]	Loss: 0.8096
Training Epoch: 25 [37504/50048]	Loss: 0.8768
Training Epoch: 25 [37632/50048]	Loss: 0.6810
Training Epoch: 25 [37760/50048]	Loss: 1.0336
Training Epoch: 25 [37888/50048]	Loss: 0.8686
Training Epoch: 25 [38016/50048]	Loss: 0.7476
Training Epoch: 25 [38144/50048]	Loss: 0.6895
Training Epoch: 25 [38272/50048]	Loss: 0.8801
Training Epoch: 25 [38400/50048]	Loss: 0.8855
Training Epoch: 25 [38528/50048]	Loss: 0.8678
Training Epoch: 25 [38656/50048]	Loss: 0.9386
Training Epoch: 25 [38784/50048]	Loss: 0.8236
Training Epoch: 25 [38912/50048]	Loss: 0.6953
Training Epoch: 25 [39040/50048]	Loss: 0.7839
Training Epoch: 25 [39168/50048]	Loss: 0.9623
Training Epoch: 25 [39296/50048]	Loss: 0.7054
Training Epoch: 25 [39424/50048]	Loss: 0.7448
Training Epoch: 25 [39552/50048]	Loss: 0.9474
Training Epoch: 25 [39680/50048]	Loss: 0.9229
Training Epoch: 25 [39808/50048]	Loss: 0.9539
Training Epoch: 25 [39936/50048]	Loss: 0.7106
Training Epoch: 25 [40064/50048]	Loss: 0.8415
Training Epoch: 25 [40192/50048]	Loss: 0.7939
Training Epoch: 25 [40320/50048]	Loss: 0.7307
Training Epoch: 25 [40448/50048]	Loss: 0.6419
Training Epoch: 25 [40576/50048]	Loss: 0.9182
Training Epoch: 25 [40704/50048]	Loss: 0.9078
Training Epoch: 25 [40832/50048]	Loss: 0.7201
Training Epoch: 25 [40960/50048]	Loss: 0.8321
Training Epoch: 25 [41088/50048]	Loss: 0.8469
Training Epoch: 25 [41216/50048]	Loss: 0.8750
Training Epoch: 25 [41344/50048]	Loss: 0.8175
Training Epoch: 25 [41472/50048]	Loss: 0.8176
Training Epoch: 25 [41600/50048]	Loss: 0.9145
Training Epoch: 25 [41728/50048]	Loss: 0.5766
Training Epoch: 25 [41856/50048]	Loss: 0.8641
Training Epoch: 25 [41984/50048]	Loss: 1.0267
Training Epoch: 25 [42112/50048]	Loss: 0.9226
Training Epoch: 25 [42240/50048]	Loss: 1.1710
Training Epoch: 25 [42368/50048]	Loss: 0.9624
Training Epoch: 25 [42496/50048]	Loss: 0.7870
Training Epoch: 25 [42624/50048]	Loss: 0.8569
Training Epoch: 25 [42752/50048]	Loss: 0.6931
Training Epoch: 25 [42880/50048]	Loss: 0.9161
Training Epoch: 25 [43008/50048]	Loss: 1.0029
Training Epoch: 25 [43136/50048]	Loss: 0.6609
Training Epoch: 25 [43264/50048]	Loss: 0.7000
Training Epoch: 25 [43392/50048]	Loss: 0.8424
Training Epoch: 25 [43520/50048]	Loss: 0.8982
Training Epoch: 25 [43648/50048]	Loss: 0.8341
Training Epoch: 25 [43776/50048]	Loss: 0.9628
Training Epoch: 25 [43904/50048]	Loss: 0.9592
Training Epoch: 25 [44032/50048]	Loss: 1.0914
Training Epoch: 25 [44160/50048]	Loss: 0.6979
Training Epoch: 25 [44288/50048]	Loss: 0.8620
Training Epoch: 25 [44416/50048]	Loss: 0.6079
Training Epoch: 25 [44544/50048]	Loss: 0.9538
Training Epoch: 25 [44672/50048]	Loss: 0.9545
Training Epoch: 25 [44800/50048]	Loss: 1.0020
Training Epoch: 25 [44928/50048]	Loss: 1.1233
Training Epoch: 25 [45056/50048]	Loss: 1.0735
Training Epoch: 25 [45184/50048]	Loss: 0.8172
Training Epoch: 25 [45312/50048]	Loss: 1.0064
Training Epoch: 25 [45440/50048]	Loss: 0.7728
Training Epoch: 25 [45568/50048]	Loss: 0.6857
Training Epoch: 25 [45696/50048]	Loss: 0.8182
2022-12-06 06:46:55,495 [ZeusDataLoader(train)] train epoch 26 done: time=86.47 energy=10501.48
2022-12-06 06:46:55,496 [ZeusDataLoader(eval)] Epoch 26 begin.
Training Epoch: 25 [45824/50048]	Loss: 0.8507
Training Epoch: 25 [45952/50048]	Loss: 0.8642
Training Epoch: 25 [46080/50048]	Loss: 0.8813
Training Epoch: 25 [46208/50048]	Loss: 0.8919
Training Epoch: 25 [46336/50048]	Loss: 0.9288
Training Epoch: 25 [46464/50048]	Loss: 0.7882
Training Epoch: 25 [46592/50048]	Loss: 0.8834
Training Epoch: 25 [46720/50048]	Loss: 0.9052
Training Epoch: 25 [46848/50048]	Loss: 0.9507
Training Epoch: 25 [46976/50048]	Loss: 0.9385
Training Epoch: 25 [47104/50048]	Loss: 0.7974
Training Epoch: 25 [47232/50048]	Loss: 0.7808
Training Epoch: 25 [47360/50048]	Loss: 0.9024
Training Epoch: 25 [47488/50048]	Loss: 0.7587
Training Epoch: 25 [47616/50048]	Loss: 1.1381
Training Epoch: 25 [47744/50048]	Loss: 1.1094
Training Epoch: 25 [47872/50048]	Loss: 1.0605
Training Epoch: 25 [48000/50048]	Loss: 0.6378
Training Epoch: 25 [48128/50048]	Loss: 0.9140
Training Epoch: 25 [48256/50048]	Loss: 0.5333
Training Epoch: 25 [48384/50048]	Loss: 0.8839
Training Epoch: 25 [48512/50048]	Loss: 0.7188
Training Epoch: 25 [48640/50048]	Loss: 0.8489
Training Epoch: 25 [48768/50048]	Loss: 1.0243
Training Epoch: 25 [48896/50048]	Loss: 1.1321
Training Epoch: 25 [49024/50048]	Loss: 0.8021
Training Epoch: 25 [49152/50048]	Loss: 0.9701
Training Epoch: 25 [49280/50048]	Loss: 0.6934
Training Epoch: 25 [49408/50048]	Loss: 0.8900
Training Epoch: 25 [49536/50048]	Loss: 1.0396
Training Epoch: 25 [49664/50048]	Loss: 0.9179
Training Epoch: 25 [49792/50048]	Loss: 0.9090
Training Epoch: 25 [49920/50048]	Loss: 0.9314
Training Epoch: 25 [50048/50048]	Loss: 0.8585
2022-12-06 11:46:59.166 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:46:59,179 [ZeusDataLoader(eval)] eval epoch 26 done: time=3.67 energy=439.65
2022-12-06 06:46:59,179 [ZeusDataLoader(train)] Up to epoch 26: time=2345.00, energy=284661.62, cost=347518.20
2022-12-06 06:46:59,179 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:46:59,179 [ZeusDataLoader(train)] Expected next epoch: time=2434.80, energy=295459.64, cost=360774.59
2022-12-06 06:46:59,180 [ZeusDataLoader(train)] Epoch 27 begin.
Validation Epoch: 25, Average loss: 0.0119, Accuracy: 0.6123
2022-12-06 06:46:59,361 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:46:59,362 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:46:59.371 [ZeusMonitor] Monitor started.
2022-12-06 11:46:59.371 [ZeusMonitor] Running indefinitely. 2022-12-06 11:46:59.371 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:46:59.371 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e27+gpu0.power.log
Training Epoch: 26 [128/50048]	Loss: 0.6021
Training Epoch: 26 [256/50048]	Loss: 0.5678
Training Epoch: 26 [384/50048]	Loss: 0.6553
Training Epoch: 26 [512/50048]	Loss: 0.7389
Training Epoch: 26 [640/50048]	Loss: 0.6592
Training Epoch: 26 [768/50048]	Loss: 0.7761
Training Epoch: 26 [896/50048]	Loss: 0.5787
Training Epoch: 26 [1024/50048]	Loss: 0.9362
Training Epoch: 26 [1152/50048]	Loss: 0.7445
Training Epoch: 26 [1280/50048]	Loss: 0.7060
Training Epoch: 26 [1408/50048]	Loss: 0.6806
Training Epoch: 26 [1536/50048]	Loss: 0.7850
Training Epoch: 26 [1664/50048]	Loss: 0.7322
Training Epoch: 26 [1792/50048]	Loss: 0.7198
Training Epoch: 26 [1920/50048]	Loss: 0.6995
Training Epoch: 26 [2048/50048]	Loss: 0.8036
Training Epoch: 26 [2176/50048]	Loss: 0.7256
Training Epoch: 26 [2304/50048]	Loss: 0.8086
Training Epoch: 26 [2432/50048]	Loss: 0.8557
Training Epoch: 26 [2560/50048]	Loss: 0.8946
Training Epoch: 26 [2688/50048]	Loss: 0.5163
Training Epoch: 26 [2816/50048]	Loss: 0.7683
Training Epoch: 26 [2944/50048]	Loss: 0.6795
Training Epoch: 26 [3072/50048]	Loss: 0.9840
Training Epoch: 26 [3200/50048]	Loss: 0.9244
Training Epoch: 26 [3328/50048]	Loss: 0.7027
Training Epoch: 26 [3456/50048]	Loss: 0.6793
Training Epoch: 26 [3584/50048]	Loss: 0.6788
Training Epoch: 26 [3712/50048]	Loss: 0.6634
Training Epoch: 26 [3840/50048]	Loss: 0.7644
Training Epoch: 26 [3968/50048]	Loss: 0.6938
Training Epoch: 26 [4096/50048]	Loss: 0.7062
Training Epoch: 26 [4224/50048]	Loss: 0.7382
Training Epoch: 26 [4352/50048]	Loss: 0.9545
Training Epoch: 26 [4480/50048]	Loss: 0.7761
Training Epoch: 26 [4608/50048]	Loss: 0.9744
Training Epoch: 26 [4736/50048]	Loss: 0.8719
Training Epoch: 26 [4864/50048]	Loss: 0.4708
Training Epoch: 26 [4992/50048]	Loss: 0.9556
Training Epoch: 26 [5120/50048]	Loss: 0.7654
Training Epoch: 26 [5248/50048]	Loss: 0.7080
Training Epoch: 26 [5376/50048]	Loss: 0.8118
Training Epoch: 26 [5504/50048]	Loss: 0.7642
Training Epoch: 26 [5632/50048]	Loss: 0.7216
Training Epoch: 26 [5760/50048]	Loss: 0.7060
Training Epoch: 26 [5888/50048]	Loss: 0.9271
Training Epoch: 26 [6016/50048]	Loss: 0.8663
Training Epoch: 26 [6144/50048]	Loss: 0.7741
Training Epoch: 26 [6272/50048]	Loss: 0.7586
Training Epoch: 26 [6400/50048]	Loss: 0.7169
Training Epoch: 26 [6528/50048]	Loss: 0.6912
Training Epoch: 26 [6656/50048]	Loss: 0.9626
Training Epoch: 26 [6784/50048]	Loss: 0.7139
Training Epoch: 26 [6912/50048]	Loss: 0.7677
Training Epoch: 26 [7040/50048]	Loss: 0.8980
Training Epoch: 26 [7168/50048]	Loss: 0.8552
Training Epoch: 26 [7296/50048]	Loss: 0.7962
Training Epoch: 26 [7424/50048]	Loss: 0.8743
Training Epoch: 26 [7552/50048]	Loss: 0.8883
Training Epoch: 26 [7680/50048]	Loss: 0.7619
Training Epoch: 26 [7808/50048]	Loss: 0.7197
Training Epoch: 26 [7936/50048]	Loss: 0.8259
Training Epoch: 26 [8064/50048]	Loss: 0.6933
Training Epoch: 26 [8192/50048]	Loss: 0.6753
Training Epoch: 26 [8320/50048]	Loss: 0.6723
Training Epoch: 26 [8448/50048]	Loss: 0.6822
Training Epoch: 26 [8576/50048]	Loss: 0.7650
Training Epoch: 26 [8704/50048]	Loss: 0.5530
Training Epoch: 26 [8832/50048]	Loss: 0.7792
Training Epoch: 26 [8960/50048]	Loss: 0.8358
Training Epoch: 26 [9088/50048]	Loss: 0.7153
Training Epoch: 26 [9216/50048]	Loss: 0.6804
Training Epoch: 26 [9344/50048]	Loss: 0.5924
Training Epoch: 26 [9472/50048]	Loss: 0.6313
Training Epoch: 26 [9600/50048]	Loss: 0.8829
Training Epoch: 26 [9728/50048]	Loss: 0.7647
Training Epoch: 26 [9856/50048]	Loss: 0.7196
Training Epoch: 26 [9984/50048]	Loss: 0.7827
Training Epoch: 26 [10112/50048]	Loss: 0.9222
Training Epoch: 26 [10240/50048]	Loss: 0.7861
Training Epoch: 26 [10368/50048]	Loss: 0.5047
Training Epoch: 26 [10496/50048]	Loss: 0.8113
Training Epoch: 26 [10624/50048]	Loss: 1.0167
Training Epoch: 26 [10752/50048]	Loss: 0.6528
Training Epoch: 26 [10880/50048]	Loss: 0.7902
Training Epoch: 26 [11008/50048]	Loss: 0.7490
Training Epoch: 26 [11136/50048]	Loss: 0.6231
Training Epoch: 26 [11264/50048]	Loss: 0.7739
Training Epoch: 26 [11392/50048]	Loss: 0.8020
Training Epoch: 26 [11520/50048]	Loss: 0.9569
Training Epoch: 26 [11648/50048]	Loss: 0.7190
Training Epoch: 26 [11776/50048]	Loss: 0.6537
Training Epoch: 26 [11904/50048]	Loss: 0.7271
Training Epoch: 26 [12032/50048]	Loss: 0.6606
Training Epoch: 26 [12160/50048]	Loss: 0.7541
Training Epoch: 26 [12288/50048]	Loss: 0.7393
Training Epoch: 26 [12416/50048]	Loss: 0.8027
Training Epoch: 26 [12544/50048]	Loss: 0.7743
Training Epoch: 26 [12672/50048]	Loss: 0.7098
Training Epoch: 26 [12800/50048]	Loss: 0.5610
Training Epoch: 26 [12928/50048]	Loss: 0.8549
Training Epoch: 26 [13056/50048]	Loss: 0.6162
Training Epoch: 26 [13184/50048]	Loss: 0.8363
Training Epoch: 26 [13312/50048]	Loss: 0.6145
Training Epoch: 26 [13440/50048]	Loss: 0.6999
Training Epoch: 26 [13568/50048]	Loss: 0.6525
Training Epoch: 26 [13696/50048]	Loss: 0.8397
Training Epoch: 26 [13824/50048]	Loss: 0.8124
Training Epoch: 26 [13952/50048]	Loss: 0.8151
Training Epoch: 26 [14080/50048]	Loss: 0.7347
Training Epoch: 26 [14208/50048]	Loss: 0.7988
Training Epoch: 26 [14336/50048]	Loss: 0.6669
Training Epoch: 26 [14464/50048]	Loss: 0.7316
Training Epoch: 26 [14592/50048]	Loss: 0.7044
Training Epoch: 26 [14720/50048]	Loss: 0.7797
Training Epoch: 26 [14848/50048]	Loss: 0.6485
Training Epoch: 26 [14976/50048]	Loss: 0.6433
Training Epoch: 26 [15104/50048]	Loss: 0.8016
Training Epoch: 26 [15232/50048]	Loss: 0.7433
Training Epoch: 26 [15360/50048]	Loss: 0.7532
Training Epoch: 26 [15488/50048]	Loss: 0.8507
Training Epoch: 26 [15616/50048]	Loss: 0.7295
Training Epoch: 26 [15744/50048]	Loss: 0.7594
Training Epoch: 26 [15872/50048]	Loss: 0.8233
Training Epoch: 26 [16000/50048]	Loss: 0.6517
Training Epoch: 26 [16128/50048]	Loss: 0.8512
Training Epoch: 26 [16256/50048]	Loss: 0.6901
Training Epoch: 26 [16384/50048]	Loss: 0.6194
Training Epoch: 26 [16512/50048]	Loss: 0.7924
Training Epoch: 26 [16640/50048]	Loss: 0.8789
Training Epoch: 26 [16768/50048]	Loss: 0.6856
Training Epoch: 26 [16896/50048]	Loss: 0.8616
Training Epoch: 26 [17024/50048]	Loss: 0.8004
Training Epoch: 26 [17152/50048]	Loss: 1.0373
Training Epoch: 26 [17280/50048]	Loss: 0.6976
Training Epoch: 26 [17408/50048]	Loss: 0.9186
Training Epoch: 26 [17536/50048]	Loss: 0.8072
Training Epoch: 26 [17664/50048]	Loss: 0.8420
Training Epoch: 26 [17792/50048]	Loss: 0.6572
Training Epoch: 26 [17920/50048]	Loss: 0.9523
Training Epoch: 26 [18048/50048]	Loss: 0.6714
Training Epoch: 26 [18176/50048]	Loss: 0.7424
Training Epoch: 26 [18304/50048]	Loss: 0.7177
Training Epoch: 26 [18432/50048]	Loss: 0.7274
Training Epoch: 26 [18560/50048]	Loss: 0.6848
Training Epoch: 26 [18688/50048]	Loss: 0.9216
Training Epoch: 26 [18816/50048]	Loss: 0.8359
Training Epoch: 26 [18944/50048]	Loss: 0.8323
Training Epoch: 26 [19072/50048]	Loss: 0.8311
Training Epoch: 26 [19200/50048]	Loss: 0.8250
Training Epoch: 26 [19328/50048]	Loss: 0.6604
Training Epoch: 26 [19456/50048]	Loss: 0.6796
Training Epoch: 26 [19584/50048]	Loss: 0.8080
Training Epoch: 26 [19712/50048]	Loss: 0.7680
Training Epoch: 26 [19840/50048]	Loss: 0.9358
Training Epoch: 26 [19968/50048]	Loss: 0.7106
Training Epoch: 26 [20096/50048]	Loss: 0.7093
Training Epoch: 26 [20224/50048]	Loss: 0.8045
Training Epoch: 26 [20352/50048]	Loss: 0.8154
Training Epoch: 26 [20480/50048]	Loss: 0.7306
Training Epoch: 26 [20608/50048]	Loss: 0.7046
Training Epoch: 26 [20736/50048]	Loss: 0.7229
Training Epoch: 26 [20864/50048]	Loss: 0.6306
Training Epoch: 26 [20992/50048]	Loss: 0.9623
Training Epoch: 26 [21120/50048]	Loss: 0.7428
Training Epoch: 26 [21248/50048]	Loss: 0.8212
Training Epoch: 26 [21376/50048]	Loss: 0.7673
Training Epoch: 26 [21504/50048]	Loss: 0.7439
Training Epoch: 26 [21632/50048]	Loss: 0.6338
Training Epoch: 26 [21760/50048]	Loss: 0.7355
Training Epoch: 26 [21888/50048]	Loss: 0.8544
Training Epoch: 26 [22016/50048]	Loss: 0.7682
Training Epoch: 26 [22144/50048]	Loss: 0.9092
Training Epoch: 26 [22272/50048]	Loss: 0.6812
Training Epoch: 26 [22400/50048]	Loss: 0.7469
Training Epoch: 26 [22528/50048]	Loss: 0.7583
Training Epoch: 26 [22656/50048]	Loss: 0.7046
Training Epoch: 26 [22784/50048]	Loss: 0.6754
Training Epoch: 26 [22912/50048]	Loss: 1.0180
Training Epoch: 26 [23040/50048]	Loss: 0.8845
Training Epoch: 26 [23168/50048]	Loss: 0.6701
Training Epoch: 26 [23296/50048]	Loss: 0.8607
Training Epoch: 26 [23424/50048]	Loss: 0.8028
Training Epoch: 26 [23552/50048]	Loss: 0.7433
Training Epoch: 26 [23680/50048]	Loss: 0.6661
Training Epoch: 26 [23808/50048]	Loss: 0.8983
Training Epoch: 26 [23936/50048]	Loss: 0.6980
Training Epoch: 26 [24064/50048]	Loss: 0.7012
Training Epoch: 26 [24192/50048]	Loss: 0.8034
Training Epoch: 26 [24320/50048]	Loss: 0.7560
Training Epoch: 26 [24448/50048]	Loss: 0.7666
Training Epoch: 26 [24576/50048]	Loss: 0.8111
Training Epoch: 26 [24704/50048]	Loss: 0.7710
Training Epoch: 26 [24832/50048]	Loss: 0.9087
Training Epoch: 26 [24960/50048]	Loss: 0.7876
Training Epoch: 26 [25088/50048]	Loss: 0.7929
Training Epoch: 26 [25216/50048]	Loss: 0.6801
Training Epoch: 26 [25344/50048]	Loss: 0.8755
Training Epoch: 26 [25472/50048]	Loss: 0.6304
Training Epoch: 26 [25600/50048]	Loss: 0.9101
Training Epoch: 26 [25728/50048]	Loss: 0.8918
Training Epoch: 26 [25856/50048]	Loss: 0.8601
Training Epoch: 26 [25984/50048]	Loss: 0.8982
Training Epoch: 26 [26112/50048]	Loss: 0.7912
Training Epoch: 26 [26240/50048]	Loss: 0.7679
Training Epoch: 26 [26368/50048]	Loss: 0.7088
Training Epoch: 26 [26496/50048]	Loss: 0.7961
Training Epoch: 26 [26624/50048]	Loss: 0.8291
Training Epoch: 26 [26752/50048]	Loss: 0.7659
Training Epoch: 26 [26880/50048]	Loss: 0.6365
Training Epoch: 26 [27008/50048]	Loss: 0.8155
Training Epoch: 26 [27136/50048]	Loss: 1.0034
Training Epoch: 26 [27264/50048]	Loss: 0.8781
Training Epoch: 26 [27392/50048]	Loss: 0.9944
Training Epoch: 26 [27520/50048]	Loss: 0.6167
Training Epoch: 26 [27648/50048]	Loss: 0.7967
Training Epoch: 26 [27776/50048]	Loss: 0.7396
Training Epoch: 26 [27904/50048]	Loss: 0.7792
Training Epoch: 26 [28032/50048]	Loss: 0.7316
Training Epoch: 26 [28160/50048]	Loss: 0.8286
Training Epoch: 26 [28288/50048]	Loss: 0.7929
Training Epoch: 26 [28416/50048]	Loss: 0.8966
Training Epoch: 26 [28544/50048]	Loss: 0.6918
Training Epoch: 26 [28672/50048]	Loss: 0.5979
Training Epoch: 26 [28800/50048]	Loss: 0.5692
Training Epoch: 26 [28928/50048]	Loss: 0.8205
Training Epoch: 26 [29056/50048]	Loss: 1.0010
Training Epoch: 26 [29184/50048]	Loss: 0.7694
Training Epoch: 26 [29312/50048]	Loss: 0.7506
Training Epoch: 26 [29440/50048]	Loss: 0.9840
Training Epoch: 26 [29568/50048]	Loss: 0.7739
Training Epoch: 26 [29696/50048]	Loss: 0.7552
Training Epoch: 26 [29824/50048]	Loss: 0.6821
Training Epoch: 26 [29952/50048]	Loss: 0.7154
Training Epoch: 26 [30080/50048]	Loss: 0.9216
Training Epoch: 26 [30208/50048]	Loss: 0.8644
Training Epoch: 26 [30336/50048]	Loss: 0.8810
Training Epoch: 26 [30464/50048]	Loss: 0.7118
Training Epoch: 26 [30592/50048]	Loss: 0.9863
Training Epoch: 26 [30720/50048]	Loss: 1.0653
Training Epoch: 26 [30848/50048]	Loss: 0.6750
Training Epoch: 26 [30976/50048]	Loss: 0.8427
Training Epoch: 26 [31104/50048]	Loss: 0.9025
Training Epoch: 26 [31232/50048]	Loss: 0.7712
Training Epoch: 26 [31360/50048]	Loss: 0.7303
Training Epoch: 26 [31488/50048]	Loss: 0.7534
Training Epoch: 26 [31616/50048]	Loss: 0.7892
Training Epoch: 26 [31744/50048]	Loss: 0.8530
Training Epoch: 26 [31872/50048]	Loss: 0.7410
Training Epoch: 26 [32000/50048]	Loss: 0.8689
Training Epoch: 26 [32128/50048]	Loss: 0.7888
Training Epoch: 26 [32256/50048]	Loss: 0.7799
Training Epoch: 26 [32384/50048]	Loss: 0.6548
Training Epoch: 26 [32512/50048]	Loss: 0.7470
Training Epoch: 26 [32640/50048]	Loss: 0.9759
Training Epoch: 26 [32768/50048]	Loss: 0.9628
Training Epoch: 26 [32896/50048]	Loss: 0.8096
Training Epoch: 26 [33024/50048]	Loss: 0.6849
Training Epoch: 26 [33152/50048]	Loss: 0.8740
Training Epoch: 26 [33280/50048]	Loss: 0.9146
Training Epoch: 26 [33408/50048]	Loss: 0.8075
Training Epoch: 26 [33536/50048]	Loss: 0.7759
Training Epoch: 26 [33664/50048]	Loss: 0.6570
Training Epoch: 26 [33792/50048]	Loss: 0.7157
Training Epoch: 26 [33920/50048]	Loss: 0.7995
Training Epoch: 26 [34048/50048]	Loss: 0.7452
Training Epoch: 26 [34176/50048]	Loss: 0.9892
Training Epoch: 26 [34304/50048]	Loss: 0.8224
Training Epoch: 26 [34432/50048]	Loss: 0.9586
Training Epoch: 26 [34560/50048]	Loss: 0.8128
Training Epoch: 26 [34688/50048]	Loss: 0.8516
Training Epoch: 26 [34816/50048]	Loss: 0.9848
Training Epoch: 26 [34944/50048]	Loss: 0.7105
Training Epoch: 26 [35072/50048]	Loss: 0.7780
Training Epoch: 26 [35200/50048]	Loss: 0.7881
Training Epoch: 26 [35328/50048]	Loss: 0.7762
Training Epoch: 26 [35456/50048]	Loss: 0.6896
Training Epoch: 26 [35584/50048]	Loss: 0.8786
Training Epoch: 26 [35712/50048]	Loss: 0.8870
Training Epoch: 26 [35840/50048]	Loss: 0.8815
Training Epoch: 26 [35968/50048]	Loss: 0.9597
Training Epoch: 26 [36096/50048]	Loss: 0.7641
Training Epoch: 26 [36224/50048]	Loss: 0.5719
Training Epoch: 26 [36352/50048]	Loss: 0.9759
Training Epoch: 26 [36480/50048]	Loss: 0.7765
Training Epoch: 26 [36608/50048]	Loss: 0.5412
Training Epoch: 26 [36736/50048]	Loss: 0.8868
Training Epoch: 26 [36864/50048]	Loss: 1.0276
Training Epoch: 26 [36992/50048]	Loss: 0.6936
Training Epoch: 26 [37120/50048]	Loss: 0.9152
Training Epoch: 26 [37248/50048]	Loss: 0.6889
Training Epoch: 26 [37376/50048]	Loss: 0.8698
Training Epoch: 26 [37504/50048]	Loss: 0.7918
Training Epoch: 26 [37632/50048]	Loss: 0.9138
Training Epoch: 26 [37760/50048]	Loss: 0.8704
Training Epoch: 26 [37888/50048]	Loss: 0.8424
Training Epoch: 26 [38016/50048]	Loss: 0.6764
Training Epoch: 26 [38144/50048]	Loss: 0.8464
Training Epoch: 26 [38272/50048]	Loss: 0.9573
Training Epoch: 26 [38400/50048]	Loss: 0.7905
Training Epoch: 26 [38528/50048]	Loss: 0.8187
Training Epoch: 26 [38656/50048]	Loss: 0.7668
Training Epoch: 26 [38784/50048]	Loss: 0.7668
Training Epoch: 26 [38912/50048]	Loss: 0.7919
Training Epoch: 26 [39040/50048]	Loss: 0.8959
Training Epoch: 26 [39168/50048]	Loss: 0.8531
Training Epoch: 26 [39296/50048]	Loss: 0.7182
Training Epoch: 26 [39424/50048]	Loss: 0.8564
Training Epoch: 26 [39552/50048]	Loss: 0.7925
Training Epoch: 26 [39680/50048]	Loss: 0.8129
Training Epoch: 26 [39808/50048]	Loss: 0.6778
Training Epoch: 26 [39936/50048]	Loss: 0.7507
Training Epoch: 26 [40064/50048]	Loss: 0.6977
Training Epoch: 26 [40192/50048]	Loss: 0.8146
Training Epoch: 26 [40320/50048]	Loss: 0.7946
Training Epoch: 26 [40448/50048]	Loss: 0.8703
Training Epoch: 26 [40576/50048]	Loss: 0.7034
Training Epoch: 26 [40704/50048]	Loss: 0.7515
Training Epoch: 26 [40832/50048]	Loss: 0.9598
Training Epoch: 26 [40960/50048]	Loss: 0.9777
Training Epoch: 26 [41088/50048]	Loss: 0.8985
Training Epoch: 26 [41216/50048]	Loss: 0.8551
Training Epoch: 26 [41344/50048]	Loss: 0.7824
Training Epoch: 26 [41472/50048]	Loss: 0.8965
Training Epoch: 26 [41600/50048]	Loss: 0.8981
Training Epoch: 26 [41728/50048]	Loss: 0.7520
Training Epoch: 26 [41856/50048]	Loss: 0.9946
Training Epoch: 26 [41984/50048]	Loss: 0.9181
Training Epoch: 26 [42112/50048]	Loss: 0.8787
Training Epoch: 26 [42240/50048]	Loss: 0.8160
Training Epoch: 26 [42368/50048]	Loss: 0.8168
Training Epoch: 26 [42496/50048]	Loss: 0.8254
Training Epoch: 26 [42624/50048]	Loss: 0.8363
Training Epoch: 26 [42752/50048]	Loss: 0.9005
Training Epoch: 26 [42880/50048]	Loss: 0.7491
Training Epoch: 26 [43008/50048]	Loss: 0.8351
Training Epoch: 26 [43136/50048]	Loss: 0.7346
Training Epoch: 26 [43264/50048]	Loss: 0.7657
Training Epoch: 26 [43392/50048]	Loss: 0.5949
Training Epoch: 26 [43520/50048]	Loss: 0.8098
Training Epoch: 26 [43648/50048]	Loss: 0.7591
Training Epoch: 26 [43776/50048]	Loss: 0.6607
Training Epoch: 26 [43904/50048]	Loss: 0.7978
Training Epoch: 26 [44032/50048]	Loss: 0.7868
Training Epoch: 26 [44160/50048]	Loss: 0.8612
Training Epoch: 26 [44288/50048]	Loss: 0.9767
Training Epoch: 26 [44416/50048]	Loss: 0.8350
Training Epoch: 26 [44544/50048]	Loss: 0.6871
Training Epoch: 26 [44672/50048]	Loss: 0.7362
Training Epoch: 26 [44800/50048]	Loss: 0.8317
Training Epoch: 26 [44928/50048]	Loss: 0.8718
Training Epoch: 26 [45056/50048]	Loss: 0.7033
Training Epoch: 26 [45184/50048]	Loss: 0.7319
Training Epoch: 26 [45312/50048]	Loss: 0.7974
Training Epoch: 26 [45440/50048]	Loss: 0.8248
Training Epoch: 26 [45568/50048]	Loss: 0.8066
Training Epoch: 26 [45696/50048]	Loss: 0.6912
2022-12-06 06:48:25,624 [ZeusDataLoader(train)] train epoch 27 done: time=86.43 energy=10505.05
2022-12-06 06:48:25,626 [ZeusDataLoader(eval)] Epoch 27 begin.
Training Epoch: 26 [45824/50048]	Loss: 1.1164
Training Epoch: 26 [45952/50048]	Loss: 0.7205
Training Epoch: 26 [46080/50048]	Loss: 0.8842
Training Epoch: 26 [46208/50048]	Loss: 0.9875
Training Epoch: 26 [46336/50048]	Loss: 0.6468
Training Epoch: 26 [46464/50048]	Loss: 0.7894
Training Epoch: 26 [46592/50048]	Loss: 0.8130
Training Epoch: 26 [46720/50048]	Loss: 0.9069
Training Epoch: 26 [46848/50048]	Loss: 0.6704
Training Epoch: 26 [46976/50048]	Loss: 0.9693
Training Epoch: 26 [47104/50048]	Loss: 0.8657
Training Epoch: 26 [47232/50048]	Loss: 0.7947
Training Epoch: 26 [47360/50048]	Loss: 0.9015
Training Epoch: 26 [47488/50048]	Loss: 0.7490
Training Epoch: 26 [47616/50048]	Loss: 0.7601
Training Epoch: 26 [47744/50048]	Loss: 0.9323
Training Epoch: 26 [47872/50048]	Loss: 0.6671
Training Epoch: 26 [48000/50048]	Loss: 0.9179
Training Epoch: 26 [48128/50048]	Loss: 0.8417
Training Epoch: 26 [48256/50048]	Loss: 0.8172
Training Epoch: 26 [48384/50048]	Loss: 0.6911
Training Epoch: 26 [48512/50048]	Loss: 0.6809
Training Epoch: 26 [48640/50048]	Loss: 0.7474
Training Epoch: 26 [48768/50048]	Loss: 0.8859
Training Epoch: 26 [48896/50048]	Loss: 0.8821
Training Epoch: 26 [49024/50048]	Loss: 1.1434
Training Epoch: 26 [49152/50048]	Loss: 0.7564
Training Epoch: 26 [49280/50048]	Loss: 0.7419
Training Epoch: 26 [49408/50048]	Loss: 0.9359
Training Epoch: 26 [49536/50048]	Loss: 0.8753
Training Epoch: 26 [49664/50048]	Loss: 0.7934
Training Epoch: 26 [49792/50048]	Loss: 0.9075
Training Epoch: 26 [49920/50048]	Loss: 0.8968
Training Epoch: 26 [50048/50048]	Loss: 0.8561
2022-12-06 11:48:29.294 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:48:29,308 [ZeusDataLoader(eval)] eval epoch 27 done: time=3.67 energy=443.34
2022-12-06 06:48:29,309 [ZeusDataLoader(train)] Up to epoch 27: time=2435.11, energy=295610.02, cost=360876.80
2022-12-06 06:48:29,309 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:48:29,309 [ZeusDataLoader(train)] Expected next epoch: time=2524.90, energy=306408.04, cost=374133.18
2022-12-06 06:48:29,310 [ZeusDataLoader(train)] Epoch 28 begin.
Validation Epoch: 26, Average loss: 0.0123, Accuracy: 0.6030
2022-12-06 06:48:29,458 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:48:29,458 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:48:29.462 [ZeusMonitor] Monitor started.
2022-12-06 11:48:29.462 [ZeusMonitor] Running indefinitely. 2022-12-06 11:48:29.462 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:48:29.462 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e28+gpu0.power.log
Training Epoch: 27 [128/50048]	Loss: 0.6395
Training Epoch: 27 [256/50048]	Loss: 0.6168
Training Epoch: 27 [384/50048]	Loss: 0.6589
Training Epoch: 27 [512/50048]	Loss: 0.6915
Training Epoch: 27 [640/50048]	Loss: 0.5430
Training Epoch: 27 [768/50048]	Loss: 0.7813
Training Epoch: 27 [896/50048]	Loss: 0.8336
Training Epoch: 27 [1024/50048]	Loss: 0.7454
Training Epoch: 27 [1152/50048]	Loss: 0.6011
Training Epoch: 27 [1280/50048]	Loss: 0.6370
Training Epoch: 27 [1408/50048]	Loss: 0.7912
Training Epoch: 27 [1536/50048]	Loss: 0.7484
Training Epoch: 27 [1664/50048]	Loss: 0.7163
Training Epoch: 27 [1792/50048]	Loss: 0.7889
Training Epoch: 27 [1920/50048]	Loss: 0.6653
Training Epoch: 27 [2048/50048]	Loss: 0.7290
Training Epoch: 27 [2176/50048]	Loss: 0.5195
Training Epoch: 27 [2304/50048]	Loss: 0.7295
Training Epoch: 27 [2432/50048]	Loss: 0.5427
Training Epoch: 27 [2560/50048]	Loss: 0.4984
Training Epoch: 27 [2688/50048]	Loss: 0.7531
Training Epoch: 27 [2816/50048]	Loss: 0.6033
Training Epoch: 27 [2944/50048]	Loss: 0.7253
Training Epoch: 27 [3072/50048]	Loss: 0.7388
Training Epoch: 27 [3200/50048]	Loss: 0.8522
Training Epoch: 27 [3328/50048]	Loss: 0.7380
Training Epoch: 27 [3456/50048]	Loss: 0.7565
Training Epoch: 27 [3584/50048]	Loss: 0.6344
Training Epoch: 27 [3712/50048]	Loss: 0.6755
Training Epoch: 27 [3840/50048]	Loss: 0.7434
Training Epoch: 27 [3968/50048]	Loss: 0.8421
Training Epoch: 27 [4096/50048]	Loss: 0.8475
Training Epoch: 27 [4224/50048]	Loss: 0.6749
Training Epoch: 27 [4352/50048]	Loss: 0.8266
Training Epoch: 27 [4480/50048]	Loss: 0.6869
Training Epoch: 27 [4608/50048]	Loss: 0.8427
Training Epoch: 27 [4736/50048]	Loss: 0.8090
Training Epoch: 27 [4864/50048]	Loss: 0.6864
Training Epoch: 27 [4992/50048]	Loss: 0.5643
Training Epoch: 27 [5120/50048]	Loss: 0.6262
Training Epoch: 27 [5248/50048]	Loss: 0.6765
Training Epoch: 27 [5376/50048]	Loss: 0.7155
Training Epoch: 27 [5504/50048]	Loss: 0.6260
Training Epoch: 27 [5632/50048]	Loss: 0.7099
Training Epoch: 27 [5760/50048]	Loss: 0.5822
Training Epoch: 27 [5888/50048]	Loss: 0.6701
Training Epoch: 27 [6016/50048]	Loss: 0.7200
Training Epoch: 27 [6144/50048]	Loss: 0.7466
Training Epoch: 27 [6272/50048]	Loss: 0.7949
Training Epoch: 27 [6400/50048]	Loss: 0.7332
Training Epoch: 27 [6528/50048]	Loss: 0.6600
Training Epoch: 27 [6656/50048]	Loss: 0.6672
Training Epoch: 27 [6784/50048]	Loss: 0.7256
Training Epoch: 27 [6912/50048]	Loss: 0.7207
Training Epoch: 27 [7040/50048]	Loss: 0.6909
Training Epoch: 27 [7168/50048]	Loss: 0.5821
Training Epoch: 27 [7296/50048]	Loss: 0.6997
Training Epoch: 27 [7424/50048]	Loss: 0.7452
Training Epoch: 27 [7552/50048]	Loss: 0.6106
Training Epoch: 27 [7680/50048]	Loss: 0.6340
Training Epoch: 27 [7808/50048]	Loss: 0.6664
Training Epoch: 27 [7936/50048]	Loss: 0.7538
Training Epoch: 27 [8064/50048]	Loss: 0.7120
Training Epoch: 27 [8192/50048]	Loss: 0.6785
Training Epoch: 27 [8320/50048]	Loss: 0.6730
Training Epoch: 27 [8448/50048]	Loss: 0.6475
Training Epoch: 27 [8576/50048]	Loss: 0.6301
Training Epoch: 27 [8704/50048]	Loss: 0.6449
Training Epoch: 27 [8832/50048]	Loss: 0.8388
Training Epoch: 27 [8960/50048]	Loss: 0.5766
Training Epoch: 27 [9088/50048]	Loss: 0.6666
Training Epoch: 27 [9216/50048]	Loss: 0.8034
Training Epoch: 27 [9344/50048]	Loss: 0.6762
Training Epoch: 27 [9472/50048]	Loss: 0.5762
Training Epoch: 27 [9600/50048]	Loss: 0.9691
Training Epoch: 27 [9728/50048]	Loss: 0.8909
Training Epoch: 27 [9856/50048]	Loss: 0.5272
Training Epoch: 27 [9984/50048]	Loss: 0.9604
Training Epoch: 27 [10112/50048]	Loss: 0.5847
Training Epoch: 27 [10240/50048]	Loss: 0.8119
Training Epoch: 27 [10368/50048]	Loss: 0.7861
Training Epoch: 27 [10496/50048]	Loss: 0.7095
Training Epoch: 27 [10624/50048]	Loss: 0.6211
Training Epoch: 27 [10752/50048]	Loss: 0.7067
Training Epoch: 27 [10880/50048]	Loss: 0.6594
Training Epoch: 27 [11008/50048]	Loss: 0.7439
Training Epoch: 27 [11136/50048]	Loss: 0.7871
Training Epoch: 27 [11264/50048]	Loss: 0.7964
Training Epoch: 27 [11392/50048]	Loss: 0.6556
Training Epoch: 27 [11520/50048]	Loss: 0.6014
Training Epoch: 27 [11648/50048]	Loss: 0.9046
Training Epoch: 27 [11776/50048]	Loss: 0.7621
Training Epoch: 27 [11904/50048]	Loss: 0.6906
Training Epoch: 27 [12032/50048]	Loss: 0.7582
Training Epoch: 27 [12160/50048]	Loss: 0.6163
Training Epoch: 27 [12288/50048]	Loss: 0.5075
Training Epoch: 27 [12416/50048]	Loss: 0.9399
Training Epoch: 27 [12544/50048]	Loss: 0.6725
Training Epoch: 27 [12672/50048]	Loss: 0.7930
Training Epoch: 27 [12800/50048]	Loss: 0.5588
Training Epoch: 27 [12928/50048]	Loss: 0.7178
Training Epoch: 27 [13056/50048]	Loss: 0.7274
Training Epoch: 27 [13184/50048]	Loss: 0.5557
Training Epoch: 27 [13312/50048]	Loss: 0.6548
Training Epoch: 27 [13440/50048]	Loss: 0.5329
Training Epoch: 27 [13568/50048]	Loss: 0.5887
Training Epoch: 27 [13696/50048]	Loss: 0.5740
Training Epoch: 27 [13824/50048]	Loss: 0.5522
Training Epoch: 27 [13952/50048]	Loss: 0.6698
Training Epoch: 27 [14080/50048]	Loss: 1.0143
Training Epoch: 27 [14208/50048]	Loss: 0.6226
Training Epoch: 27 [14336/50048]	Loss: 0.8282
Training Epoch: 27 [14464/50048]	Loss: 0.7895
Training Epoch: 27 [14592/50048]	Loss: 0.8475
Training Epoch: 27 [14720/50048]	Loss: 0.7045
Training Epoch: 27 [14848/50048]	Loss: 0.9504
Training Epoch: 27 [14976/50048]	Loss: 0.9170
Training Epoch: 27 [15104/50048]	Loss: 0.7043
Training Epoch: 27 [15232/50048]	Loss: 0.6088
Training Epoch: 27 [15360/50048]	Loss: 0.8538
Training Epoch: 27 [15488/50048]	Loss: 0.8794
Training Epoch: 27 [15616/50048]	Loss: 0.7470
Training Epoch: 27 [15744/50048]	Loss: 0.7029
Training Epoch: 27 [15872/50048]	Loss: 0.7430
Training Epoch: 27 [16000/50048]	Loss: 0.7346
Training Epoch: 27 [16128/50048]	Loss: 0.6367
Training Epoch: 27 [16256/50048]	Loss: 0.8385
Training Epoch: 27 [16384/50048]	Loss: 0.6515
Training Epoch: 27 [16512/50048]	Loss: 0.7067
Training Epoch: 27 [16640/50048]	Loss: 0.8720
Training Epoch: 27 [16768/50048]	Loss: 0.5974
Training Epoch: 27 [16896/50048]	Loss: 1.0296
Training Epoch: 27 [17024/50048]	Loss: 0.6581
Training Epoch: 27 [17152/50048]	Loss: 0.4959
Training Epoch: 27 [17280/50048]	Loss: 0.9708
Training Epoch: 27 [17408/50048]	Loss: 0.8402
Training Epoch: 27 [17536/50048]	Loss: 0.9490
Training Epoch: 27 [17664/50048]	Loss: 0.6042
Training Epoch: 27 [17792/50048]	Loss: 0.8067
Training Epoch: 27 [17920/50048]	Loss: 0.9858
Training Epoch: 27 [18048/50048]	Loss: 0.8261
Training Epoch: 27 [18176/50048]	Loss: 0.7852
Training Epoch: 27 [18304/50048]	Loss: 0.7843
Training Epoch: 27 [18432/50048]	Loss: 0.7415
Training Epoch: 27 [18560/50048]	Loss: 0.6495
Training Epoch: 27 [18688/50048]	Loss: 0.7980
Training Epoch: 27 [18816/50048]	Loss: 0.7488
Training Epoch: 27 [18944/50048]	Loss: 0.8374
Training Epoch: 27 [19072/50048]	Loss: 0.6839
Training Epoch: 27 [19200/50048]	Loss: 0.8566
Training Epoch: 27 [19328/50048]	Loss: 0.7282
Training Epoch: 27 [19456/50048]	Loss: 0.8522
Training Epoch: 27 [19584/50048]	Loss: 0.5069
Training Epoch: 27 [19712/50048]	Loss: 0.8280
Training Epoch: 27 [19840/50048]	Loss: 0.8000
Training Epoch: 27 [19968/50048]	Loss: 0.7234
Training Epoch: 27 [20096/50048]	Loss: 0.5835
Training Epoch: 27 [20224/50048]	Loss: 0.8076
Training Epoch: 27 [20352/50048]	Loss: 0.7737
Training Epoch: 27 [20480/50048]	Loss: 0.8082
Training Epoch: 27 [20608/50048]	Loss: 0.6699
Training Epoch: 27 [20736/50048]	Loss: 0.7688
Training Epoch: 27 [20864/50048]	Loss: 0.8480
Training Epoch: 27 [20992/50048]	Loss: 0.7181
Training Epoch: 27 [21120/50048]	Loss: 0.6592
Training Epoch: 27 [21248/50048]	Loss: 0.7892
Training Epoch: 27 [21376/50048]	Loss: 0.9206
Training Epoch: 27 [21504/50048]	Loss: 0.7720
Training Epoch: 27 [21632/50048]	Loss: 0.6754
Training Epoch: 27 [21760/50048]	Loss: 0.7888
Training Epoch: 27 [21888/50048]	Loss: 0.7581
Training Epoch: 27 [22016/50048]	Loss: 0.6419
Training Epoch: 27 [22144/50048]	Loss: 0.8823
Training Epoch: 27 [22272/50048]	Loss: 0.7060
Training Epoch: 27 [22400/50048]	Loss: 0.8708
Training Epoch: 27 [22528/50048]	Loss: 0.5907
Training Epoch: 27 [22656/50048]	Loss: 0.6327
Training Epoch: 27 [22784/50048]	Loss: 0.6451
Training Epoch: 27 [22912/50048]	Loss: 0.6564
Training Epoch: 27 [23040/50048]	Loss: 0.7248
Training Epoch: 27 [23168/50048]	Loss: 0.9237
Training Epoch: 27 [23296/50048]	Loss: 0.5997
Training Epoch: 27 [23424/50048]	Loss: 0.8469
Training Epoch: 27 [23552/50048]	Loss: 0.7501
Training Epoch: 27 [23680/50048]	Loss: 0.5979
Training Epoch: 27 [23808/50048]	Loss: 0.6884
Training Epoch: 27 [23936/50048]	Loss: 0.7088
Training Epoch: 27 [24064/50048]	Loss: 0.6821
Training Epoch: 27 [24192/50048]	Loss: 0.7440
Training Epoch: 27 [24320/50048]	Loss: 0.7201
Training Epoch: 27 [24448/50048]	Loss: 0.7515
Training Epoch: 27 [24576/50048]	Loss: 0.7502
Training Epoch: 27 [24704/50048]	Loss: 0.7045
Training Epoch: 27 [24832/50048]	Loss: 0.6548
Training Epoch: 27 [24960/50048]	Loss: 0.8613
Training Epoch: 27 [25088/50048]	Loss: 0.7843
Training Epoch: 27 [25216/50048]	Loss: 0.8740
Training Epoch: 27 [25344/50048]	Loss: 0.7786
Training Epoch: 27 [25472/50048]	Loss: 0.8039
Training Epoch: 27 [25600/50048]	Loss: 0.8646
Training Epoch: 27 [25728/50048]	Loss: 0.6569
Training Epoch: 27 [25856/50048]	Loss: 0.7451
Training Epoch: 27 [25984/50048]	Loss: 0.8296
Training Epoch: 27 [26112/50048]	Loss: 0.7844
Training Epoch: 27 [26240/50048]	Loss: 0.6586
Training Epoch: 27 [26368/50048]	Loss: 0.8773
Training Epoch: 27 [26496/50048]	Loss: 0.8271
Training Epoch: 27 [26624/50048]	Loss: 0.8383
Training Epoch: 27 [26752/50048]	Loss: 0.8782
Training Epoch: 27 [26880/50048]	Loss: 0.8407
Training Epoch: 27 [27008/50048]	Loss: 0.7853
Training Epoch: 27 [27136/50048]	Loss: 0.7994
Training Epoch: 27 [27264/50048]	Loss: 0.7084
Training Epoch: 27 [27392/50048]	Loss: 0.7960
Training Epoch: 27 [27520/50048]	Loss: 0.7393
Training Epoch: 27 [27648/50048]	Loss: 0.9348
Training Epoch: 27 [27776/50048]	Loss: 0.8191
Training Epoch: 27 [27904/50048]	Loss: 0.8123
Training Epoch: 27 [28032/50048]	Loss: 0.8407
Training Epoch: 27 [28160/50048]	Loss: 0.9311
Training Epoch: 27 [28288/50048]	Loss: 0.5650
Training Epoch: 27 [28416/50048]	Loss: 0.7751
Training Epoch: 27 [28544/50048]	Loss: 0.8303
Training Epoch: 27 [28672/50048]	Loss: 0.6995
Training Epoch: 27 [28800/50048]	Loss: 0.6491
Training Epoch: 27 [28928/50048]	Loss: 0.6888
Training Epoch: 27 [29056/50048]	Loss: 0.7033
Training Epoch: 27 [29184/50048]	Loss: 0.8268
Training Epoch: 27 [29312/50048]	Loss: 0.7890
Training Epoch: 27 [29440/50048]	Loss: 1.0578
Training Epoch: 27 [29568/50048]	Loss: 1.1610
Training Epoch: 27 [29696/50048]	Loss: 0.7570
Training Epoch: 27 [29824/50048]	Loss: 0.7444
Training Epoch: 27 [29952/50048]	Loss: 0.8627
Training Epoch: 27 [30080/50048]	Loss: 0.5821
Training Epoch: 27 [30208/50048]	Loss: 0.9835
Training Epoch: 27 [30336/50048]	Loss: 0.9791
Training Epoch: 27 [30464/50048]	Loss: 0.6707
Training Epoch: 27 [30592/50048]	Loss: 0.6358
Training Epoch: 27 [30720/50048]	Loss: 0.5775
Training Epoch: 27 [30848/50048]	Loss: 0.9680
Training Epoch: 27 [30976/50048]	Loss: 1.0636
Training Epoch: 27 [31104/50048]	Loss: 0.7268
Training Epoch: 27 [31232/50048]	Loss: 0.6852
Training Epoch: 27 [31360/50048]	Loss: 0.9256
Training Epoch: 27 [31488/50048]	Loss: 0.7904
Training Epoch: 27 [31616/50048]	Loss: 0.7220
Training Epoch: 27 [31744/50048]	Loss: 0.6911
Training Epoch: 27 [31872/50048]	Loss: 0.8703
Training Epoch: 27 [32000/50048]	Loss: 0.7934
Training Epoch: 27 [32128/50048]	Loss: 0.7333
Training Epoch: 27 [32256/50048]	Loss: 0.6003
Training Epoch: 27 [32384/50048]	Loss: 0.8157
Training Epoch: 27 [32512/50048]	Loss: 0.7507
Training Epoch: 27 [32640/50048]	Loss: 0.6068
Training Epoch: 27 [32768/50048]	Loss: 0.7918
Training Epoch: 27 [32896/50048]	Loss: 0.6058
Training Epoch: 27 [33024/50048]	Loss: 0.7254
Training Epoch: 27 [33152/50048]	Loss: 0.7183
Training Epoch: 27 [33280/50048]	Loss: 0.7388
Training Epoch: 27 [33408/50048]	Loss: 0.6286
Training Epoch: 27 [33536/50048]	Loss: 0.5688
Training Epoch: 27 [33664/50048]	Loss: 0.7269
Training Epoch: 27 [33792/50048]	Loss: 0.7813
Training Epoch: 27 [33920/50048]	Loss: 0.8063
Training Epoch: 27 [34048/50048]	Loss: 0.9546
Training Epoch: 27 [34176/50048]	Loss: 0.8445
Training Epoch: 27 [34304/50048]	Loss: 0.8150
Training Epoch: 27 [34432/50048]	Loss: 0.7433
Training Epoch: 27 [34560/50048]	Loss: 0.6305
Training Epoch: 27 [34688/50048]	Loss: 1.0750
Training Epoch: 27 [34816/50048]	Loss: 0.8688
Training Epoch: 27 [34944/50048]	Loss: 0.7807
Training Epoch: 27 [35072/50048]	Loss: 0.6301
Training Epoch: 27 [35200/50048]	Loss: 0.8650
Training Epoch: 27 [35328/50048]	Loss: 0.8000
Training Epoch: 27 [35456/50048]	Loss: 0.8184
Training Epoch: 27 [35584/50048]	Loss: 0.8786
Training Epoch: 27 [35712/50048]	Loss: 0.6775
Training Epoch: 27 [35840/50048]	Loss: 0.7455
Training Epoch: 27 [35968/50048]	Loss: 0.5553
Training Epoch: 27 [36096/50048]	Loss: 0.7269
Training Epoch: 27 [36224/50048]	Loss: 0.7657
Training Epoch: 27 [36352/50048]	Loss: 0.8263
Training Epoch: 27 [36480/50048]	Loss: 0.9411
Training Epoch: 27 [36608/50048]	Loss: 0.9333
Training Epoch: 27 [36736/50048]	Loss: 0.8278
Training Epoch: 27 [36864/50048]	Loss: 0.7961
Training Epoch: 27 [36992/50048]	Loss: 0.8620
Training Epoch: 27 [37120/50048]	Loss: 0.8901
Training Epoch: 27 [37248/50048]	Loss: 0.7151
Training Epoch: 27 [37376/50048]	Loss: 0.7858
Training Epoch: 27 [37504/50048]	Loss: 0.6436
Training Epoch: 27 [37632/50048]	Loss: 0.7717
Training Epoch: 27 [37760/50048]	Loss: 0.7497
Training Epoch: 27 [37888/50048]	Loss: 0.7211
Training Epoch: 27 [38016/50048]	Loss: 0.5855
Training Epoch: 27 [38144/50048]	Loss: 0.5902
Training Epoch: 27 [38272/50048]	Loss: 0.6517
Training Epoch: 27 [38400/50048]	Loss: 0.7393
Training Epoch: 27 [38528/50048]	Loss: 0.7593
Training Epoch: 27 [38656/50048]	Loss: 0.8051
Training Epoch: 27 [38784/50048]	Loss: 0.8554
Training Epoch: 27 [38912/50048]	Loss: 0.8653
Training Epoch: 27 [39040/50048]	Loss: 0.9040
Training Epoch: 27 [39168/50048]	Loss: 0.6724
Training Epoch: 27 [39296/50048]	Loss: 0.7945
Training Epoch: 27 [39424/50048]	Loss: 0.7658
Training Epoch: 27 [39552/50048]	Loss: 0.8357
Training Epoch: 27 [39680/50048]	Loss: 0.7673
Training Epoch: 27 [39808/50048]	Loss: 0.7272
Training Epoch: 27 [39936/50048]	Loss: 0.7353
Training Epoch: 27 [40064/50048]	Loss: 0.6768
Training Epoch: 27 [40192/50048]	Loss: 0.7973
Training Epoch: 27 [40320/50048]	Loss: 0.9352
Training Epoch: 27 [40448/50048]	Loss: 0.9461
Training Epoch: 27 [40576/50048]	Loss: 0.8010
Training Epoch: 27 [40704/50048]	Loss: 0.8484
Training Epoch: 27 [40832/50048]	Loss: 0.6800
Training Epoch: 27 [40960/50048]	Loss: 0.6923
Training Epoch: 27 [41088/50048]	Loss: 0.9329
Training Epoch: 27 [41216/50048]	Loss: 0.8048
Training Epoch: 27 [41344/50048]	Loss: 0.7904
Training Epoch: 27 [41472/50048]	Loss: 0.7389
Training Epoch: 27 [41600/50048]	Loss: 0.7165
Training Epoch: 27 [41728/50048]	Loss: 0.7282
Training Epoch: 27 [41856/50048]	Loss: 0.7537
Training Epoch: 27 [41984/50048]	Loss: 0.7792
Training Epoch: 27 [42112/50048]	Loss: 0.7529
Training Epoch: 27 [42240/50048]	Loss: 0.8054
Training Epoch: 27 [42368/50048]	Loss: 0.8697
Training Epoch: 27 [42496/50048]	Loss: 0.6514
Training Epoch: 27 [42624/50048]	Loss: 0.7180
Training Epoch: 27 [42752/50048]	Loss: 0.7126
Training Epoch: 27 [42880/50048]	Loss: 0.8289
Training Epoch: 27 [43008/50048]	Loss: 0.6923
Training Epoch: 27 [43136/50048]	Loss: 0.7693
Training Epoch: 27 [43264/50048]	Loss: 0.7534
Training Epoch: 27 [43392/50048]	Loss: 0.6680
Training Epoch: 27 [43520/50048]	Loss: 1.0071
Training Epoch: 27 [43648/50048]	Loss: 0.7675
Training Epoch: 27 [43776/50048]	Loss: 0.9350
Training Epoch: 27 [43904/50048]	Loss: 0.9652
Training Epoch: 27 [44032/50048]	Loss: 0.6237
Training Epoch: 27 [44160/50048]	Loss: 0.8114
Training Epoch: 27 [44288/50048]	Loss: 0.9643
Training Epoch: 27 [44416/50048]	Loss: 0.6691
Training Epoch: 27 [44544/50048]	Loss: 0.7970
Training Epoch: 27 [44672/50048]	Loss: 0.8492
Training Epoch: 27 [44800/50048]	Loss: 0.6241
Training Epoch: 27 [44928/50048]	Loss: 0.8792
Training Epoch: 27 [45056/50048]	Loss: 0.5979
Training Epoch: 27 [45184/50048]	Loss: 0.7283
Training Epoch: 27 [45312/50048]	Loss: 0.7279
Training Epoch: 27 [45440/50048]	Loss: 0.5100
Training Epoch: 27 [45568/50048]	Loss: 0.6500
Training Epoch: 27 [45696/50048]	Loss: 0.8055
2022-12-06 06:49:55,820 [ZeusDataLoader(train)] train epoch 28 done: time=86.50 energy=10503.30
2022-12-06 06:49:55,822 [ZeusDataLoader(eval)] Epoch 28 begin.
Training Epoch: 27 [45824/50048]	Loss: 0.8346
Training Epoch: 27 [45952/50048]	Loss: 0.9511
Training Epoch: 27 [46080/50048]	Loss: 0.8725
Training Epoch: 27 [46208/50048]	Loss: 0.8264
Training Epoch: 27 [46336/50048]	Loss: 0.7843
Training Epoch: 27 [46464/50048]	Loss: 0.8904
Training Epoch: 27 [46592/50048]	Loss: 0.8858
Training Epoch: 27 [46720/50048]	Loss: 0.6842
Training Epoch: 27 [46848/50048]	Loss: 0.7534
Training Epoch: 27 [46976/50048]	Loss: 0.8252
Training Epoch: 27 [47104/50048]	Loss: 0.8066
Training Epoch: 27 [47232/50048]	Loss: 0.9743
Training Epoch: 27 [47360/50048]	Loss: 0.6427
Training Epoch: 27 [47488/50048]	Loss: 0.9145
Training Epoch: 27 [47616/50048]	Loss: 0.5947
Training Epoch: 27 [47744/50048]	Loss: 0.7212
Training Epoch: 27 [47872/50048]	Loss: 0.8440
Training Epoch: 27 [48000/50048]	Loss: 0.9573
Training Epoch: 27 [48128/50048]	Loss: 0.5033
Training Epoch: 27 [48256/50048]	Loss: 0.6617
Training Epoch: 27 [48384/50048]	Loss: 0.8335
Training Epoch: 27 [48512/50048]	Loss: 0.8360
Training Epoch: 27 [48640/50048]	Loss: 0.7711
Training Epoch: 27 [48768/50048]	Loss: 0.7372
Training Epoch: 27 [48896/50048]	Loss: 1.0348
Training Epoch: 27 [49024/50048]	Loss: 0.7403
Training Epoch: 27 [49152/50048]	Loss: 0.8556
Training Epoch: 27 [49280/50048]	Loss: 0.7283
Training Epoch: 27 [49408/50048]	Loss: 0.8690
Training Epoch: 27 [49536/50048]	Loss: 0.7171
Training Epoch: 27 [49664/50048]	Loss: 0.8386
Training Epoch: 27 [49792/50048]	Loss: 0.6456
Training Epoch: 27 [49920/50048]	Loss: 0.8360
Training Epoch: 27 [50048/50048]	Loss: 1.1003
2022-12-06 11:49:59.530 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:49:59,583 [ZeusDataLoader(eval)] eval epoch 28 done: time=3.75 energy=453.11
2022-12-06 06:49:59,583 [ZeusDataLoader(train)] Up to epoch 28: time=2525.36, energy=306566.44, cost=374252.08
2022-12-06 06:49:59,583 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:49:59,583 [ZeusDataLoader(train)] Expected next epoch: time=2615.16, energy=317364.45, cost=387508.46
2022-12-06 06:49:59,584 [ZeusDataLoader(train)] Epoch 29 begin.
Validation Epoch: 27, Average loss: 0.0117, Accuracy: 0.6205
2022-12-06 06:49:59,777 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:49:59,778 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:49:59.779 [ZeusMonitor] Monitor started.
2022-12-06 11:49:59.779 [ZeusMonitor] Running indefinitely. 2022-12-06 11:49:59.779 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:49:59.779 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e29+gpu0.power.log
Training Epoch: 28 [128/50048]	Loss: 0.6225
Training Epoch: 28 [256/50048]	Loss: 0.6249
Training Epoch: 28 [384/50048]	Loss: 0.6949
Training Epoch: 28 [512/50048]	Loss: 0.6015
Training Epoch: 28 [640/50048]	Loss: 0.6316
Training Epoch: 28 [768/50048]	Loss: 0.5729
Training Epoch: 28 [896/50048]	Loss: 0.6139
Training Epoch: 28 [1024/50048]	Loss: 0.5736
Training Epoch: 28 [1152/50048]	Loss: 0.7055
Training Epoch: 28 [1280/50048]	Loss: 0.8162
Training Epoch: 28 [1408/50048]	Loss: 0.5390
Training Epoch: 28 [1536/50048]	Loss: 0.7137
Training Epoch: 28 [1664/50048]	Loss: 0.6868
Training Epoch: 28 [1792/50048]	Loss: 0.6532
Training Epoch: 28 [1920/50048]	Loss: 0.6314
Training Epoch: 28 [2048/50048]	Loss: 0.7764
Training Epoch: 28 [2176/50048]	Loss: 0.5451
Training Epoch: 28 [2304/50048]	Loss: 0.5164
Training Epoch: 28 [2432/50048]	Loss: 0.6503
Training Epoch: 28 [2560/50048]	Loss: 0.6344
Training Epoch: 28 [2688/50048]	Loss: 0.6900
Training Epoch: 28 [2816/50048]	Loss: 0.7300
Training Epoch: 28 [2944/50048]	Loss: 0.6433
Training Epoch: 28 [3072/50048]	Loss: 0.7107
Training Epoch: 28 [3200/50048]	Loss: 0.6304
Training Epoch: 28 [3328/50048]	Loss: 0.4388
Training Epoch: 28 [3456/50048]	Loss: 0.6414
Training Epoch: 28 [3584/50048]	Loss: 0.7048
Training Epoch: 28 [3712/50048]	Loss: 0.6055
Training Epoch: 28 [3840/50048]	Loss: 0.7429
Training Epoch: 28 [3968/50048]	Loss: 0.8965
Training Epoch: 28 [4096/50048]	Loss: 0.8830
Training Epoch: 28 [4224/50048]	Loss: 0.8076
Training Epoch: 28 [4352/50048]	Loss: 0.5943
Training Epoch: 28 [4480/50048]	Loss: 0.6101
Training Epoch: 28 [4608/50048]	Loss: 0.6258
Training Epoch: 28 [4736/50048]	Loss: 0.7573
Training Epoch: 28 [4864/50048]	Loss: 0.6477
Training Epoch: 28 [4992/50048]	Loss: 0.6097
Training Epoch: 28 [5120/50048]	Loss: 0.6012
Training Epoch: 28 [5248/50048]	Loss: 0.4953
Training Epoch: 28 [5376/50048]	Loss: 0.6926
Training Epoch: 28 [5504/50048]	Loss: 0.7005
Training Epoch: 28 [5632/50048]	Loss: 0.6417
Training Epoch: 28 [5760/50048]	Loss: 0.6274
Training Epoch: 28 [5888/50048]	Loss: 0.7254
Training Epoch: 28 [6016/50048]	Loss: 0.4939
Training Epoch: 28 [6144/50048]	Loss: 0.6407
Training Epoch: 28 [6272/50048]	Loss: 0.9081
Training Epoch: 28 [6400/50048]	Loss: 0.6094
Training Epoch: 28 [6528/50048]	Loss: 0.6176
Training Epoch: 28 [6656/50048]	Loss: 0.7238
Training Epoch: 28 [6784/50048]	Loss: 0.7625
Training Epoch: 28 [6912/50048]	Loss: 0.7216
Training Epoch: 28 [7040/50048]	Loss: 0.5984
Training Epoch: 28 [7168/50048]	Loss: 0.5665
Training Epoch: 28 [7296/50048]	Loss: 0.7876
Training Epoch: 28 [7424/50048]	Loss: 0.5688
Training Epoch: 28 [7552/50048]	Loss: 0.7405
Training Epoch: 28 [7680/50048]	Loss: 0.5067
Training Epoch: 28 [7808/50048]	Loss: 0.5530
Training Epoch: 28 [7936/50048]	Loss: 0.7318
Training Epoch: 28 [8064/50048]	Loss: 0.6594
Training Epoch: 28 [8192/50048]	Loss: 0.8437
Training Epoch: 28 [8320/50048]	Loss: 0.5942
Training Epoch: 28 [8448/50048]	Loss: 0.6818
Training Epoch: 28 [8576/50048]	Loss: 0.6223
Training Epoch: 28 [8704/50048]	Loss: 0.7470
Training Epoch: 28 [8832/50048]	Loss: 0.7062
Training Epoch: 28 [8960/50048]	Loss: 0.7083
Training Epoch: 28 [9088/50048]	Loss: 0.8225
Training Epoch: 28 [9216/50048]	Loss: 0.7796
Training Epoch: 28 [9344/50048]	Loss: 0.6646
Training Epoch: 28 [9472/50048]	Loss: 0.9664
Training Epoch: 28 [9600/50048]	Loss: 0.6828
Training Epoch: 28 [9728/50048]	Loss: 0.6813
Training Epoch: 28 [9856/50048]	Loss: 0.9237
Training Epoch: 28 [9984/50048]	Loss: 0.6260
Training Epoch: 28 [10112/50048]	Loss: 0.5234
Training Epoch: 28 [10240/50048]	Loss: 0.7092
Training Epoch: 28 [10368/50048]	Loss: 0.5635
Training Epoch: 28 [10496/50048]	Loss: 0.7068
Training Epoch: 28 [10624/50048]	Loss: 0.5765
Training Epoch: 28 [10752/50048]	Loss: 0.6137
Training Epoch: 28 [10880/50048]	Loss: 0.8398
Training Epoch: 28 [11008/50048]	Loss: 0.6844
Training Epoch: 28 [11136/50048]	Loss: 0.7226
Training Epoch: 28 [11264/50048]	Loss: 0.6568
Training Epoch: 28 [11392/50048]	Loss: 0.6667
Training Epoch: 28 [11520/50048]	Loss: 0.5950
Training Epoch: 28 [11648/50048]	Loss: 0.7239
Training Epoch: 28 [11776/50048]	Loss: 0.7787
Training Epoch: 28 [11904/50048]	Loss: 0.7155
Training Epoch: 28 [12032/50048]	Loss: 0.6603
Training Epoch: 28 [12160/50048]	Loss: 0.6590
Training Epoch: 28 [12288/50048]	Loss: 0.5758
Training Epoch: 28 [12416/50048]	Loss: 0.6869
Training Epoch: 28 [12544/50048]	Loss: 0.6883
Training Epoch: 28 [12672/50048]	Loss: 0.6540
Training Epoch: 28 [12800/50048]	Loss: 0.8112
Training Epoch: 28 [12928/50048]	Loss: 0.6218
Training Epoch: 28 [13056/50048]	Loss: 0.4715
Training Epoch: 28 [13184/50048]	Loss: 0.6557
Training Epoch: 28 [13312/50048]	Loss: 0.7744
Training Epoch: 28 [13440/50048]	Loss: 0.8107
Training Epoch: 28 [13568/50048]	Loss: 0.6124
Training Epoch: 28 [13696/50048]	Loss: 0.8354
Training Epoch: 28 [13824/50048]	Loss: 0.5870
Training Epoch: 28 [13952/50048]	Loss: 0.7170
Training Epoch: 28 [14080/50048]	Loss: 0.7053
Training Epoch: 28 [14208/50048]	Loss: 0.8755
Training Epoch: 28 [14336/50048]	Loss: 0.8637
Training Epoch: 28 [14464/50048]	Loss: 0.5878
Training Epoch: 28 [14592/50048]	Loss: 0.7802
Training Epoch: 28 [14720/50048]	Loss: 0.6766
Training Epoch: 28 [14848/50048]	Loss: 0.7368
Training Epoch: 28 [14976/50048]	Loss: 0.6848
Training Epoch: 28 [15104/50048]	Loss: 0.9568
Training Epoch: 28 [15232/50048]	Loss: 0.7307
Training Epoch: 28 [15360/50048]	Loss: 0.6345
Training Epoch: 28 [15488/50048]	Loss: 0.5255
Training Epoch: 28 [15616/50048]	Loss: 0.7806
Training Epoch: 28 [15744/50048]	Loss: 0.6077
Training Epoch: 28 [15872/50048]	Loss: 0.8493
Training Epoch: 28 [16000/50048]	Loss: 0.7201
Training Epoch: 28 [16128/50048]	Loss: 0.6568
Training Epoch: 28 [16256/50048]	Loss: 0.7148
Training Epoch: 28 [16384/50048]	Loss: 0.7832
Training Epoch: 28 [16512/50048]	Loss: 0.8255
Training Epoch: 28 [16640/50048]	Loss: 0.7447
Training Epoch: 28 [16768/50048]	Loss: 0.5366
Training Epoch: 28 [16896/50048]	Loss: 0.8525
Training Epoch: 28 [17024/50048]	Loss: 0.7739
Training Epoch: 28 [17152/50048]	Loss: 0.9450
Training Epoch: 28 [17280/50048]	Loss: 0.7198
Training Epoch: 28 [17408/50048]	Loss: 0.7467
Training Epoch: 28 [17536/50048]	Loss: 0.6624
Training Epoch: 28 [17664/50048]	Loss: 0.7458
Training Epoch: 28 [17792/50048]	Loss: 0.5586
Training Epoch: 28 [17920/50048]	Loss: 0.6976
Training Epoch: 28 [18048/50048]	Loss: 0.7898
Training Epoch: 28 [18176/50048]	Loss: 0.6254
Training Epoch: 28 [18304/50048]	Loss: 0.7069
Training Epoch: 28 [18432/50048]	Loss: 0.7102
Training Epoch: 28 [18560/50048]	Loss: 0.9493
Training Epoch: 28 [18688/50048]	Loss: 0.6289
Training Epoch: 28 [18816/50048]	Loss: 0.6073
Training Epoch: 28 [18944/50048]	Loss: 0.8963
Training Epoch: 28 [19072/50048]	Loss: 0.6521
Training Epoch: 28 [19200/50048]	Loss: 0.6272
Training Epoch: 28 [19328/50048]	Loss: 0.7920
Training Epoch: 28 [19456/50048]	Loss: 0.8408
Training Epoch: 28 [19584/50048]	Loss: 0.6999
Training Epoch: 28 [19712/50048]	Loss: 0.7670
Training Epoch: 28 [19840/50048]	Loss: 0.6735
Training Epoch: 28 [19968/50048]	Loss: 0.4768
Training Epoch: 28 [20096/50048]	Loss: 0.7110
Training Epoch: 28 [20224/50048]	Loss: 0.7430
Training Epoch: 28 [20352/50048]	Loss: 0.6482
Training Epoch: 28 [20480/50048]	Loss: 0.6146
Training Epoch: 28 [20608/50048]	Loss: 0.7202
Training Epoch: 28 [20736/50048]	Loss: 0.8454
Training Epoch: 28 [20864/50048]	Loss: 0.8246
Training Epoch: 28 [20992/50048]	Loss: 0.6298
Training Epoch: 28 [21120/50048]	Loss: 0.7454
Training Epoch: 28 [21248/50048]	Loss: 0.7785
Training Epoch: 28 [21376/50048]	Loss: 0.6910
Training Epoch: 28 [21504/50048]	Loss: 0.7153
Training Epoch: 28 [21632/50048]	Loss: 0.6918
Training Epoch: 28 [21760/50048]	Loss: 0.6461
Training Epoch: 28 [21888/50048]	Loss: 0.6351
Training Epoch: 28 [22016/50048]	Loss: 0.7271
Training Epoch: 28 [22144/50048]	Loss: 0.7960
Training Epoch: 28 [22272/50048]	Loss: 0.6454
Training Epoch: 28 [22400/50048]	Loss: 0.6443
Training Epoch: 28 [22528/50048]	Loss: 0.7407
Training Epoch: 28 [22656/50048]	Loss: 0.7850
Training Epoch: 28 [22784/50048]	Loss: 0.7323
Training Epoch: 28 [22912/50048]	Loss: 0.8997
Training Epoch: 28 [23040/50048]	Loss: 0.6337
Training Epoch: 28 [23168/50048]	Loss: 0.7496
Training Epoch: 28 [23296/50048]	Loss: 0.7276
Training Epoch: 28 [23424/50048]	Loss: 0.6300
Training Epoch: 28 [23552/50048]	Loss: 0.8544
Training Epoch: 28 [23680/50048]	Loss: 0.6803
Training Epoch: 28 [23808/50048]	Loss: 0.5541
Training Epoch: 28 [23936/50048]	Loss: 0.5592
Training Epoch: 28 [24064/50048]	Loss: 0.7905
Training Epoch: 28 [24192/50048]	Loss: 0.5645
Training Epoch: 28 [24320/50048]	Loss: 0.8244
Training Epoch: 28 [24448/50048]	Loss: 0.8427
Training Epoch: 28 [24576/50048]	Loss: 0.6716
Training Epoch: 28 [24704/50048]	Loss: 0.6968
Training Epoch: 28 [24832/50048]	Loss: 0.7365
Training Epoch: 28 [24960/50048]	Loss: 0.6937
Training Epoch: 28 [25088/50048]	Loss: 0.7564
Training Epoch: 28 [25216/50048]	Loss: 0.7814
Training Epoch: 28 [25344/50048]	Loss: 0.7602
Training Epoch: 28 [25472/50048]	Loss: 0.5409
Training Epoch: 28 [25600/50048]	Loss: 0.7999
Training Epoch: 28 [25728/50048]	Loss: 0.8204
Training Epoch: 28 [25856/50048]	Loss: 0.5350
Training Epoch: 28 [25984/50048]	Loss: 0.6615
Training Epoch: 28 [26112/50048]	Loss: 0.6975
Training Epoch: 28 [26240/50048]	Loss: 0.6364
Training Epoch: 28 [26368/50048]	Loss: 0.6705
Training Epoch: 28 [26496/50048]	Loss: 0.7643
Training Epoch: 28 [26624/50048]	Loss: 0.6590
Training Epoch: 28 [26752/50048]	Loss: 0.8004
Training Epoch: 28 [26880/50048]	Loss: 0.7671
Training Epoch: 28 [27008/50048]	Loss: 0.7019
Training Epoch: 28 [27136/50048]	Loss: 0.5900
Training Epoch: 28 [27264/50048]	Loss: 0.7867
Training Epoch: 28 [27392/50048]	Loss: 0.8380
Training Epoch: 28 [27520/50048]	Loss: 0.7499
Training Epoch: 28 [27648/50048]	Loss: 0.6903
Training Epoch: 28 [27776/50048]	Loss: 0.8996
Training Epoch: 28 [27904/50048]	Loss: 0.6966
Training Epoch: 28 [28032/50048]	Loss: 0.5892
Training Epoch: 28 [28160/50048]	Loss: 0.8467
Training Epoch: 28 [28288/50048]	Loss: 0.6823
Training Epoch: 28 [28416/50048]	Loss: 0.5951
Training Epoch: 28 [28544/50048]	Loss: 0.9798
Training Epoch: 28 [28672/50048]	Loss: 0.7021
Training Epoch: 28 [28800/50048]	Loss: 0.5322
Training Epoch: 28 [28928/50048]	Loss: 0.5802
Training Epoch: 28 [29056/50048]	Loss: 0.7110
Training Epoch: 28 [29184/50048]	Loss: 0.8183
Training Epoch: 28 [29312/50048]	Loss: 0.5418
Training Epoch: 28 [29440/50048]	Loss: 0.8538
Training Epoch: 28 [29568/50048]	Loss: 0.6099
Training Epoch: 28 [29696/50048]	Loss: 0.6923
Training Epoch: 28 [29824/50048]	Loss: 0.5382
Training Epoch: 28 [29952/50048]	Loss: 0.7430
Training Epoch: 28 [30080/50048]	Loss: 0.7348
Training Epoch: 28 [30208/50048]	Loss: 0.8225
Training Epoch: 28 [30336/50048]	Loss: 0.5973
Training Epoch: 28 [30464/50048]	Loss: 0.6284
Training Epoch: 28 [30592/50048]	Loss: 0.7842
Training Epoch: 28 [30720/50048]	Loss: 0.5056
Training Epoch: 28 [30848/50048]	Loss: 0.9666
Training Epoch: 28 [30976/50048]	Loss: 0.6170
Training Epoch: 28 [31104/50048]	Loss: 0.7881
Training Epoch: 28 [31232/50048]	Loss: 0.6325
Training Epoch: 28 [31360/50048]	Loss: 0.8071
Training Epoch: 28 [31488/50048]	Loss: 0.7699
Training Epoch: 28 [31616/50048]	Loss: 0.8273
Training Epoch: 28 [31744/50048]	Loss: 0.8192
Training Epoch: 28 [31872/50048]	Loss: 0.6543
Training Epoch: 28 [32000/50048]	Loss: 0.8266
Training Epoch: 28 [32128/50048]	Loss: 0.8869
Training Epoch: 28 [32256/50048]	Loss: 0.6236
Training Epoch: 28 [32384/50048]	Loss: 0.6465
Training Epoch: 28 [32512/50048]	Loss: 0.7148
Training Epoch: 28 [32640/50048]	Loss: 0.6967
Training Epoch: 28 [32768/50048]	Loss: 0.6884
Training Epoch: 28 [32896/50048]	Loss: 0.6999
Training Epoch: 28 [33024/50048]	Loss: 0.5659
Training Epoch: 28 [33152/50048]	Loss: 0.6845
Training Epoch: 28 [33280/50048]	Loss: 0.9025
Training Epoch: 28 [33408/50048]	Loss: 0.8214
Training Epoch: 28 [33536/50048]	Loss: 0.7844
Training Epoch: 28 [33664/50048]	Loss: 0.6005
Training Epoch: 28 [33792/50048]	Loss: 0.5884
Training Epoch: 28 [33920/50048]	Loss: 0.8785
Training Epoch: 28 [34048/50048]	Loss: 0.4681
Training Epoch: 28 [34176/50048]	Loss: 0.7513
Training Epoch: 28 [34304/50048]	Loss: 0.7973
Training Epoch: 28 [34432/50048]	Loss: 0.6563
Training Epoch: 28 [34560/50048]	Loss: 0.7905
Training Epoch: 28 [34688/50048]	Loss: 0.7432
Training Epoch: 28 [34816/50048]	Loss: 0.7299
Training Epoch: 28 [34944/50048]	Loss: 0.8998
Training Epoch: 28 [35072/50048]	Loss: 0.8535
Training Epoch: 28 [35200/50048]	Loss: 0.6759
Training Epoch: 28 [35328/50048]	Loss: 0.6525
Training Epoch: 28 [35456/50048]	Loss: 0.7430
Training Epoch: 28 [35584/50048]	Loss: 0.7926
Training Epoch: 28 [35712/50048]	Loss: 0.8397
Training Epoch: 28 [35840/50048]	Loss: 0.6185
Training Epoch: 28 [35968/50048]	Loss: 0.6864
Training Epoch: 28 [36096/50048]	Loss: 0.5396
Training Epoch: 28 [36224/50048]	Loss: 0.9154
Training Epoch: 28 [36352/50048]	Loss: 0.8167
Training Epoch: 28 [36480/50048]	Loss: 0.8131
Training Epoch: 28 [36608/50048]	Loss: 0.7286
Training Epoch: 28 [36736/50048]	Loss: 0.8214
Training Epoch: 28 [36864/50048]	Loss: 0.8519
Training Epoch: 28 [36992/50048]	Loss: 0.7885
Training Epoch: 28 [37120/50048]	Loss: 0.7314
Training Epoch: 28 [37248/50048]	Loss: 0.7974
Training Epoch: 28 [37376/50048]	Loss: 0.8946
Training Epoch: 28 [37504/50048]	Loss: 0.6265
Training Epoch: 28 [37632/50048]	Loss: 0.7947
Training Epoch: 28 [37760/50048]	Loss: 0.6232
Training Epoch: 28 [37888/50048]	Loss: 0.8416
Training Epoch: 28 [38016/50048]	Loss: 0.6960
Training Epoch: 28 [38144/50048]	Loss: 0.8172
Training Epoch: 28 [38272/50048]	Loss: 0.7135
Training Epoch: 28 [38400/50048]	Loss: 0.7318
Training Epoch: 28 [38528/50048]	Loss: 0.8398
Training Epoch: 28 [38656/50048]	Loss: 0.8465
Training Epoch: 28 [38784/50048]	Loss: 0.7638
Training Epoch: 28 [38912/50048]	Loss: 0.8432
Training Epoch: 28 [39040/50048]	Loss: 0.7434
Training Epoch: 28 [39168/50048]	Loss: 0.7317
Training Epoch: 28 [39296/50048]	Loss: 0.6799
Training Epoch: 28 [39424/50048]	Loss: 0.8301
Training Epoch: 28 [39552/50048]	Loss: 0.7078
Training Epoch: 28 [39680/50048]	Loss: 0.8784
Training Epoch: 28 [39808/50048]	Loss: 0.6955
Training Epoch: 28 [39936/50048]	Loss: 0.7675
Training Epoch: 28 [40064/50048]	Loss: 0.9302
Training Epoch: 28 [40192/50048]	Loss: 0.5029
Training Epoch: 28 [40320/50048]	Loss: 0.8362
Training Epoch: 28 [40448/50048]	Loss: 0.7704
Training Epoch: 28 [40576/50048]	Loss: 0.7832
Training Epoch: 28 [40704/50048]	Loss: 0.9369
Training Epoch: 28 [40832/50048]	Loss: 0.9887
Training Epoch: 28 [40960/50048]	Loss: 0.8724
Training Epoch: 28 [41088/50048]	Loss: 0.7836
Training Epoch: 28 [41216/50048]	Loss: 0.8621
Training Epoch: 28 [41344/50048]	Loss: 0.8904
Training Epoch: 28 [41472/50048]	Loss: 0.7392
Training Epoch: 28 [41600/50048]	Loss: 0.7984
Training Epoch: 28 [41728/50048]	Loss: 0.6291
Training Epoch: 28 [41856/50048]	Loss: 0.5372
Training Epoch: 28 [41984/50048]	Loss: 0.6530
Training Epoch: 28 [42112/50048]	Loss: 0.7636
Training Epoch: 28 [42240/50048]	Loss: 0.5868
Training Epoch: 28 [42368/50048]	Loss: 0.7302
Training Epoch: 28 [42496/50048]	Loss: 0.6975
Training Epoch: 28 [42624/50048]	Loss: 0.7346
Training Epoch: 28 [42752/50048]	Loss: 0.7225
Training Epoch: 28 [42880/50048]	Loss: 0.8658
Training Epoch: 28 [43008/50048]	Loss: 0.7224
Training Epoch: 28 [43136/50048]	Loss: 0.6039
Training Epoch: 28 [43264/50048]	Loss: 0.6110
Training Epoch: 28 [43392/50048]	Loss: 0.6566
Training Epoch: 28 [43520/50048]	Loss: 0.6926
Training Epoch: 28 [43648/50048]	Loss: 0.8453
Training Epoch: 28 [43776/50048]	Loss: 0.6693
Training Epoch: 28 [43904/50048]	Loss: 0.7135
Training Epoch: 28 [44032/50048]	Loss: 0.7309
Training Epoch: 28 [44160/50048]	Loss: 0.9039
Training Epoch: 28 [44288/50048]	Loss: 0.7610
Training Epoch: 28 [44416/50048]	Loss: 0.8642
Training Epoch: 28 [44544/50048]	Loss: 0.9237
Training Epoch: 28 [44672/50048]	Loss: 0.8137
Training Epoch: 28 [44800/50048]	Loss: 1.0079
Training Epoch: 28 [44928/50048]	Loss: 0.9498
Training Epoch: 28 [45056/50048]	Loss: 0.6988
Training Epoch: 28 [45184/50048]	Loss: 0.9454
Training Epoch: 28 [45312/50048]	Loss: 0.6331
Training Epoch: 28 [45440/50048]	Loss: 0.6804
Training Epoch: 28 [45568/50048]	Loss: 0.6397
Training Epoch: 28 [45696/50048]	Loss: 0.7961
2022-12-06 06:51:26,068 [ZeusDataLoader(train)] train epoch 29 done: time=86.47 energy=10501.96
2022-12-06 06:51:26,070 [ZeusDataLoader(eval)] Epoch 29 begin.
Training Epoch: 28 [45824/50048]	Loss: 0.8499
Training Epoch: 28 [45952/50048]	Loss: 0.8685
Training Epoch: 28 [46080/50048]	Loss: 0.6862
Training Epoch: 28 [46208/50048]	Loss: 0.6263
Training Epoch: 28 [46336/50048]	Loss: 0.8614
Training Epoch: 28 [46464/50048]	Loss: 0.6258
Training Epoch: 28 [46592/50048]	Loss: 0.8088
Training Epoch: 28 [46720/50048]	Loss: 0.7591
Training Epoch: 28 [46848/50048]	Loss: 0.7790
Training Epoch: 28 [46976/50048]	Loss: 0.8895
Training Epoch: 28 [47104/50048]	Loss: 0.6136
Training Epoch: 28 [47232/50048]	Loss: 0.7469
Training Epoch: 28 [47360/50048]	Loss: 0.8126
Training Epoch: 28 [47488/50048]	Loss: 0.6544
Training Epoch: 28 [47616/50048]	Loss: 0.7456
Training Epoch: 28 [47744/50048]	Loss: 0.8277
Training Epoch: 28 [47872/50048]	Loss: 0.6928
Training Epoch: 28 [48000/50048]	Loss: 0.6402
Training Epoch: 28 [48128/50048]	Loss: 0.7782
Training Epoch: 28 [48256/50048]	Loss: 0.8332
Training Epoch: 28 [48384/50048]	Loss: 0.8592
Training Epoch: 28 [48512/50048]	Loss: 0.6481
Training Epoch: 28 [48640/50048]	Loss: 0.8860
Training Epoch: 28 [48768/50048]	Loss: 0.7724
Training Epoch: 28 [48896/50048]	Loss: 0.7586
Training Epoch: 28 [49024/50048]	Loss: 0.7700
Training Epoch: 28 [49152/50048]	Loss: 0.8046
Training Epoch: 28 [49280/50048]	Loss: 0.9602
Training Epoch: 28 [49408/50048]	Loss: 0.7369
Training Epoch: 28 [49536/50048]	Loss: 0.5536
Training Epoch: 28 [49664/50048]	Loss: 0.9114
Training Epoch: 28 [49792/50048]	Loss: 0.8289
Training Epoch: 28 [49920/50048]	Loss: 0.6721
Training Epoch: 28 [50048/50048]	Loss: 0.9170
2022-12-06 11:51:29.753 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:51:29,770 [ZeusDataLoader(eval)] eval epoch 29 done: time=3.69 energy=452.83
2022-12-06 06:51:29,770 [ZeusDataLoader(train)] Up to epoch 29: time=2615.52, energy=317521.22, cost=387618.94
2022-12-06 06:51:29,770 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:51:29,770 [ZeusDataLoader(train)] Expected next epoch: time=2705.32, energy=328319.24, cost=400875.32
2022-12-06 06:51:29,771 [ZeusDataLoader(train)] Epoch 30 begin.
Validation Epoch: 28, Average loss: 0.0121, Accuracy: 0.6182
2022-12-06 06:51:29,960 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:51:29,960 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:51:29.962 [ZeusMonitor] Monitor started.
2022-12-06 11:51:29.962 [ZeusMonitor] Running indefinitely. 2022-12-06 11:51:29.962 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:51:29.962 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e30+gpu0.power.log
Training Epoch: 29 [128/50048]	Loss: 0.5454
Training Epoch: 29 [256/50048]	Loss: 0.6030
Training Epoch: 29 [384/50048]	Loss: 0.6715
Training Epoch: 29 [512/50048]	Loss: 0.7029
Training Epoch: 29 [640/50048]	Loss: 0.4945
Training Epoch: 29 [768/50048]	Loss: 0.6485
Training Epoch: 29 [896/50048]	Loss: 0.7004
Training Epoch: 29 [1024/50048]	Loss: 0.5841
Training Epoch: 29 [1152/50048]	Loss: 0.4763
Training Epoch: 29 [1280/50048]	Loss: 0.5936
Training Epoch: 29 [1408/50048]	Loss: 0.7199
Training Epoch: 29 [1536/50048]	Loss: 0.6247
Training Epoch: 29 [1664/50048]	Loss: 0.7491
Training Epoch: 29 [1792/50048]	Loss: 0.6188
Training Epoch: 29 [1920/50048]	Loss: 0.9201
Training Epoch: 29 [2048/50048]	Loss: 0.5289
Training Epoch: 29 [2176/50048]	Loss: 0.5972
Training Epoch: 29 [2304/50048]	Loss: 0.6399
Training Epoch: 29 [2432/50048]	Loss: 0.6687
Training Epoch: 29 [2560/50048]	Loss: 0.5988
Training Epoch: 29 [2688/50048]	Loss: 0.6007
Training Epoch: 29 [2816/50048]	Loss: 0.5337
Training Epoch: 29 [2944/50048]	Loss: 0.7898
Training Epoch: 29 [3072/50048]	Loss: 0.6785
Training Epoch: 29 [3200/50048]	Loss: 0.5099
Training Epoch: 29 [3328/50048]	Loss: 0.7530
Training Epoch: 29 [3456/50048]	Loss: 0.8122
Training Epoch: 29 [3584/50048]	Loss: 0.7613
Training Epoch: 29 [3712/50048]	Loss: 0.6294
Training Epoch: 29 [3840/50048]	Loss: 0.4905
Training Epoch: 29 [3968/50048]	Loss: 0.6552
Training Epoch: 29 [4096/50048]	Loss: 0.7225
Training Epoch: 29 [4224/50048]	Loss: 0.5164
Training Epoch: 29 [4352/50048]	Loss: 0.7277
Training Epoch: 29 [4480/50048]	Loss: 0.5592
Training Epoch: 29 [4608/50048]	Loss: 0.7588
Training Epoch: 29 [4736/50048]	Loss: 0.8910
Training Epoch: 29 [4864/50048]	Loss: 0.6940
Training Epoch: 29 [4992/50048]	Loss: 0.5756
Training Epoch: 29 [5120/50048]	Loss: 0.6767
Training Epoch: 29 [5248/50048]	Loss: 0.5834
Training Epoch: 29 [5376/50048]	Loss: 0.6478
Training Epoch: 29 [5504/50048]	Loss: 0.5274
Training Epoch: 29 [5632/50048]	Loss: 0.6604
Training Epoch: 29 [5760/50048]	Loss: 0.6942
Training Epoch: 29 [5888/50048]	Loss: 0.7132
Training Epoch: 29 [6016/50048]	Loss: 0.5869
Training Epoch: 29 [6144/50048]	Loss: 0.7530
Training Epoch: 29 [6272/50048]	Loss: 0.5488
Training Epoch: 29 [6400/50048]	Loss: 0.7948
Training Epoch: 29 [6528/50048]	Loss: 0.5895
Training Epoch: 29 [6656/50048]	Loss: 0.7212
Training Epoch: 29 [6784/50048]	Loss: 0.6013
Training Epoch: 29 [6912/50048]	Loss: 0.6184
Training Epoch: 29 [7040/50048]	Loss: 0.7931
Training Epoch: 29 [7168/50048]	Loss: 0.5931
Training Epoch: 29 [7296/50048]	Loss: 0.5671
Training Epoch: 29 [7424/50048]	Loss: 0.5381
Training Epoch: 29 [7552/50048]	Loss: 0.7325
Training Epoch: 29 [7680/50048]	Loss: 0.6687
Training Epoch: 29 [7808/50048]	Loss: 0.7421
Training Epoch: 29 [7936/50048]	Loss: 0.5883
Training Epoch: 29 [8064/50048]	Loss: 0.7345
Training Epoch: 29 [8192/50048]	Loss: 0.5496
Training Epoch: 29 [8320/50048]	Loss: 0.6269
Training Epoch: 29 [8448/50048]	Loss: 0.6568
Training Epoch: 29 [8576/50048]	Loss: 0.6594
Training Epoch: 29 [8704/50048]	Loss: 0.8701
Training Epoch: 29 [8832/50048]	Loss: 0.6862
Training Epoch: 29 [8960/50048]	Loss: 0.6898
Training Epoch: 29 [9088/50048]	Loss: 0.7039
Training Epoch: 29 [9216/50048]	Loss: 0.7139
Training Epoch: 29 [9344/50048]	Loss: 0.6645
Training Epoch: 29 [9472/50048]	Loss: 0.7177
Training Epoch: 29 [9600/50048]	Loss: 0.7171
Training Epoch: 29 [9728/50048]	Loss: 0.7678
Training Epoch: 29 [9856/50048]	Loss: 0.5243
Training Epoch: 29 [9984/50048]	Loss: 0.6966
Training Epoch: 29 [10112/50048]	Loss: 0.6030
Training Epoch: 29 [10240/50048]	Loss: 0.6822
Training Epoch: 29 [10368/50048]	Loss: 0.7232
Training Epoch: 29 [10496/50048]	Loss: 0.7550
Training Epoch: 29 [10624/50048]	Loss: 0.7659
Training Epoch: 29 [10752/50048]	Loss: 0.7002
Training Epoch: 29 [10880/50048]	Loss: 0.7546
Training Epoch: 29 [11008/50048]	Loss: 0.8635
Training Epoch: 29 [11136/50048]	Loss: 0.6730
Training Epoch: 29 [11264/50048]	Loss: 0.7049
Training Epoch: 29 [11392/50048]	Loss: 0.6566
Training Epoch: 29 [11520/50048]	Loss: 0.6672
Training Epoch: 29 [11648/50048]	Loss: 0.6528
Training Epoch: 29 [11776/50048]	Loss: 0.7949
Training Epoch: 29 [11904/50048]	Loss: 0.8641
Training Epoch: 29 [12032/50048]	Loss: 0.8188
Training Epoch: 29 [12160/50048]	Loss: 0.6766
Training Epoch: 29 [12288/50048]	Loss: 0.6636
Training Epoch: 29 [12416/50048]	Loss: 0.5459
Training Epoch: 29 [12544/50048]	Loss: 0.4966
Training Epoch: 29 [12672/50048]	Loss: 0.7059
Training Epoch: 29 [12800/50048]	Loss: 0.8711
Training Epoch: 29 [12928/50048]	Loss: 0.6927
Training Epoch: 29 [13056/50048]	Loss: 0.6644
Training Epoch: 29 [13184/50048]	Loss: 0.4969
Training Epoch: 29 [13312/50048]	Loss: 0.6865
Training Epoch: 29 [13440/50048]	Loss: 0.4999
Training Epoch: 29 [13568/50048]	Loss: 0.5677
Training Epoch: 29 [13696/50048]	Loss: 0.7056
Training Epoch: 29 [13824/50048]	Loss: 0.6062
Training Epoch: 29 [13952/50048]	Loss: 0.6903
Training Epoch: 29 [14080/50048]	Loss: 0.4789
Training Epoch: 29 [14208/50048]	Loss: 0.7739
Training Epoch: 29 [14336/50048]	Loss: 0.6506
Training Epoch: 29 [14464/50048]	Loss: 0.7220
Training Epoch: 29 [14592/50048]	Loss: 0.7052
Training Epoch: 29 [14720/50048]	Loss: 0.6048
Training Epoch: 29 [14848/50048]	Loss: 0.6717
Training Epoch: 29 [14976/50048]	Loss: 0.6882
Training Epoch: 29 [15104/50048]	Loss: 0.7875
Training Epoch: 29 [15232/50048]	Loss: 0.7672
Training Epoch: 29 [15360/50048]	Loss: 0.7262
Training Epoch: 29 [15488/50048]	Loss: 0.6372
Training Epoch: 29 [15616/50048]	Loss: 0.5698
Training Epoch: 29 [15744/50048]	Loss: 0.7153
Training Epoch: 29 [15872/50048]	Loss: 0.6792
Training Epoch: 29 [16000/50048]	Loss: 0.7333
Training Epoch: 29 [16128/50048]	Loss: 0.5885
Training Epoch: 29 [16256/50048]	Loss: 0.6411
Training Epoch: 29 [16384/50048]	Loss: 0.8023
Training Epoch: 29 [16512/50048]	Loss: 0.6995
Training Epoch: 29 [16640/50048]	Loss: 0.7372
Training Epoch: 29 [16768/50048]	Loss: 0.7688
Training Epoch: 29 [16896/50048]	Loss: 0.7422
Training Epoch: 29 [17024/50048]	Loss: 0.5421
Training Epoch: 29 [17152/50048]	Loss: 0.8306
Training Epoch: 29 [17280/50048]	Loss: 0.5588
Training Epoch: 29 [17408/50048]	Loss: 0.6286
Training Epoch: 29 [17536/50048]	Loss: 0.6207
Training Epoch: 29 [17664/50048]	Loss: 0.7155
Training Epoch: 29 [17792/50048]	Loss: 0.5637
Training Epoch: 29 [17920/50048]	Loss: 0.5936
Training Epoch: 29 [18048/50048]	Loss: 0.6429
Training Epoch: 29 [18176/50048]	Loss: 0.6416
Training Epoch: 29 [18304/50048]	Loss: 0.7463
Training Epoch: 29 [18432/50048]	Loss: 0.7153
Training Epoch: 29 [18560/50048]	Loss: 0.7260
Training Epoch: 29 [18688/50048]	Loss: 0.5702
Training Epoch: 29 [18816/50048]	Loss: 0.7980
Training Epoch: 29 [18944/50048]	Loss: 0.9190
Training Epoch: 29 [19072/50048]	Loss: 0.7932
Training Epoch: 29 [19200/50048]	Loss: 0.6638
Training Epoch: 29 [19328/50048]	Loss: 0.7386
Training Epoch: 29 [19456/50048]	Loss: 0.8391
Training Epoch: 29 [19584/50048]	Loss: 0.5497
Training Epoch: 29 [19712/50048]	Loss: 0.7008
Training Epoch: 29 [19840/50048]	Loss: 0.5127
Training Epoch: 29 [19968/50048]	Loss: 0.7148
Training Epoch: 29 [20096/50048]	Loss: 0.8577
Training Epoch: 29 [20224/50048]	Loss: 0.6695
Training Epoch: 29 [20352/50048]	Loss: 0.7269
Training Epoch: 29 [20480/50048]	Loss: 0.8505
Training Epoch: 29 [20608/50048]	Loss: 0.5696
Training Epoch: 29 [20736/50048]	Loss: 0.7091
Training Epoch: 29 [20864/50048]	Loss: 0.9405
Training Epoch: 29 [20992/50048]	Loss: 0.8943
Training Epoch: 29 [21120/50048]	Loss: 0.5082
Training Epoch: 29 [21248/50048]	Loss: 0.8054
Training Epoch: 29 [21376/50048]	Loss: 0.7542
Training Epoch: 29 [21504/50048]	Loss: 0.7488
Training Epoch: 29 [21632/50048]	Loss: 0.5959
Training Epoch: 29 [21760/50048]	Loss: 0.5848
Training Epoch: 29 [21888/50048]	Loss: 0.6667
Training Epoch: 29 [22016/50048]	Loss: 0.6634
Training Epoch: 29 [22144/50048]	Loss: 0.6649
Training Epoch: 29 [22272/50048]	Loss: 0.6696
Training Epoch: 29 [22400/50048]	Loss: 0.6958
Training Epoch: 29 [22528/50048]	Loss: 0.6211
Training Epoch: 29 [22656/50048]	Loss: 0.7113
Training Epoch: 29 [22784/50048]	Loss: 0.6710
Training Epoch: 29 [22912/50048]	Loss: 0.6534
Training Epoch: 29 [23040/50048]	Loss: 0.5018
Training Epoch: 29 [23168/50048]	Loss: 0.5552
Training Epoch: 29 [23296/50048]	Loss: 0.7706
Training Epoch: 29 [23424/50048]	Loss: 0.7197
Training Epoch: 29 [23552/50048]	Loss: 0.7403
Training Epoch: 29 [23680/50048]	Loss: 0.6554
Training Epoch: 29 [23808/50048]	Loss: 0.7466
Training Epoch: 29 [23936/50048]	Loss: 0.5319
Training Epoch: 29 [24064/50048]	Loss: 0.7163
Training Epoch: 29 [24192/50048]	Loss: 0.5358
Training Epoch: 29 [24320/50048]	Loss: 0.7742
Training Epoch: 29 [24448/50048]	Loss: 0.6683
Training Epoch: 29 [24576/50048]	Loss: 0.7275
Training Epoch: 29 [24704/50048]	Loss: 0.6101
Training Epoch: 29 [24832/50048]	Loss: 0.6705
Training Epoch: 29 [24960/50048]	Loss: 0.6695
Training Epoch: 29 [25088/50048]	Loss: 0.8138
Training Epoch: 29 [25216/50048]	Loss: 0.5595
Training Epoch: 29 [25344/50048]	Loss: 0.6704
Training Epoch: 29 [25472/50048]	Loss: 0.6537
Training Epoch: 29 [25600/50048]	Loss: 0.7276
Training Epoch: 29 [25728/50048]	Loss: 0.7566
Training Epoch: 29 [25856/50048]	Loss: 0.5787
Training Epoch: 29 [25984/50048]	Loss: 0.4933
Training Epoch: 29 [26112/50048]	Loss: 0.6030
Training Epoch: 29 [26240/50048]	Loss: 0.9168
Training Epoch: 29 [26368/50048]	Loss: 0.4960
Training Epoch: 29 [26496/50048]	Loss: 0.6607
Training Epoch: 29 [26624/50048]	Loss: 0.7669
Training Epoch: 29 [26752/50048]	Loss: 0.6633
Training Epoch: 29 [26880/50048]	Loss: 0.8413
Training Epoch: 29 [27008/50048]	Loss: 0.6794
Training Epoch: 29 [27136/50048]	Loss: 0.7974
Training Epoch: 29 [27264/50048]	Loss: 0.6534
Training Epoch: 29 [27392/50048]	Loss: 0.6446
Training Epoch: 29 [27520/50048]	Loss: 0.7065
Training Epoch: 29 [27648/50048]	Loss: 0.8007
Training Epoch: 29 [27776/50048]	Loss: 0.6124
Training Epoch: 29 [27904/50048]	Loss: 0.6049
Training Epoch: 29 [28032/50048]	Loss: 0.5671
Training Epoch: 29 [28160/50048]	Loss: 0.6935
Training Epoch: 29 [28288/50048]	Loss: 0.6858
Training Epoch: 29 [28416/50048]	Loss: 0.7538
Training Epoch: 29 [28544/50048]	Loss: 0.5751
Training Epoch: 29 [28672/50048]	Loss: 0.5781
Training Epoch: 29 [28800/50048]	Loss: 0.7621
Training Epoch: 29 [28928/50048]	Loss: 0.7197
Training Epoch: 29 [29056/50048]	Loss: 0.7122
Training Epoch: 29 [29184/50048]	Loss: 0.4856
Training Epoch: 29 [29312/50048]	Loss: 0.7759
Training Epoch: 29 [29440/50048]	Loss: 0.5785
Training Epoch: 29 [29568/50048]	Loss: 0.5811
Training Epoch: 29 [29696/50048]	Loss: 0.7448
Training Epoch: 29 [29824/50048]	Loss: 0.7569
Training Epoch: 29 [29952/50048]	Loss: 0.6272
Training Epoch: 29 [30080/50048]	Loss: 0.7025
Training Epoch: 29 [30208/50048]	Loss: 0.5208
Training Epoch: 29 [30336/50048]	Loss: 0.6366
Training Epoch: 29 [30464/50048]	Loss: 0.7441
Training Epoch: 29 [30592/50048]	Loss: 0.6273
Training Epoch: 29 [30720/50048]	Loss: 0.6990
Training Epoch: 29 [30848/50048]	Loss: 0.7905
Training Epoch: 29 [30976/50048]	Loss: 0.7684
Training Epoch: 29 [31104/50048]	Loss: 0.7318
Training Epoch: 29 [31232/50048]	Loss: 0.6016
Training Epoch: 29 [31360/50048]	Loss: 0.7103
Training Epoch: 29 [31488/50048]	Loss: 0.6197
Training Epoch: 29 [31616/50048]	Loss: 0.8376
Training Epoch: 29 [31744/50048]	Loss: 0.7125
Training Epoch: 29 [31872/50048]	Loss: 0.6421
Training Epoch: 29 [32000/50048]	Loss: 0.9442
Training Epoch: 29 [32128/50048]	Loss: 0.8068
Training Epoch: 29 [32256/50048]	Loss: 0.8510
Training Epoch: 29 [32384/50048]	Loss: 0.9545
Training Epoch: 29 [32512/50048]	Loss: 0.6533
Training Epoch: 29 [32640/50048]	Loss: 0.5558
Training Epoch: 29 [32768/50048]	Loss: 0.6853
Training Epoch: 29 [32896/50048]	Loss: 0.7548
Training Epoch: 29 [33024/50048]	Loss: 0.6406
Training Epoch: 29 [33152/50048]	Loss: 0.7363
Training Epoch: 29 [33280/50048]	Loss: 0.7050
Training Epoch: 29 [33408/50048]	Loss: 0.6841
Training Epoch: 29 [33536/50048]	Loss: 0.7969
Training Epoch: 29 [33664/50048]	Loss: 0.6713
Training Epoch: 29 [33792/50048]	Loss: 0.7086
Training Epoch: 29 [33920/50048]	Loss: 0.8986
Training Epoch: 29 [34048/50048]	Loss: 0.6839
Training Epoch: 29 [34176/50048]	Loss: 0.7428
Training Epoch: 29 [34304/50048]	Loss: 0.7834
Training Epoch: 29 [34432/50048]	Loss: 0.7748
Training Epoch: 29 [34560/50048]	Loss: 0.7896
Training Epoch: 29 [34688/50048]	Loss: 0.8853
Training Epoch: 29 [34816/50048]	Loss: 0.6075
Training Epoch: 29 [34944/50048]	Loss: 0.6597
Training Epoch: 29 [35072/50048]	Loss: 0.6623
Training Epoch: 29 [35200/50048]	Loss: 0.5331
Training Epoch: 29 [35328/50048]	Loss: 0.6818
Training Epoch: 29 [35456/50048]	Loss: 0.5538
Training Epoch: 29 [35584/50048]	Loss: 0.6808
Training Epoch: 29 [35712/50048]	Loss: 0.7788
Training Epoch: 29 [35840/50048]	Loss: 0.7069
Training Epoch: 29 [35968/50048]	Loss: 0.5968
Training Epoch: 29 [36096/50048]	Loss: 0.8389
Training Epoch: 29 [36224/50048]	Loss: 0.7069
Training Epoch: 29 [36352/50048]	Loss: 0.5991
Training Epoch: 29 [36480/50048]	Loss: 0.5647
Training Epoch: 29 [36608/50048]	Loss: 0.8341
Training Epoch: 29 [36736/50048]	Loss: 0.6001
Training Epoch: 29 [36864/50048]	Loss: 0.9124
Training Epoch: 29 [36992/50048]	Loss: 0.6874
Training Epoch: 29 [37120/50048]	Loss: 0.7056
Training Epoch: 29 [37248/50048]	Loss: 0.6615
Training Epoch: 29 [37376/50048]	Loss: 0.5446
Training Epoch: 29 [37504/50048]	Loss: 0.5898
Training Epoch: 29 [37632/50048]	Loss: 0.6932
Training Epoch: 29 [37760/50048]	Loss: 0.7837
Training Epoch: 29 [37888/50048]	Loss: 0.7230
Training Epoch: 29 [38016/50048]	Loss: 0.7253
Training Epoch: 29 [38144/50048]	Loss: 0.7403
Training Epoch: 29 [38272/50048]	Loss: 0.6204
Training Epoch: 29 [38400/50048]	Loss: 0.7271
Training Epoch: 29 [38528/50048]	Loss: 0.6071
Training Epoch: 29 [38656/50048]	Loss: 0.7367
Training Epoch: 29 [38784/50048]	Loss: 0.7278
Training Epoch: 29 [38912/50048]	Loss: 0.6356
Training Epoch: 29 [39040/50048]	Loss: 0.6307
Training Epoch: 29 [39168/50048]	Loss: 0.6887
Training Epoch: 29 [39296/50048]	Loss: 0.6664
Training Epoch: 29 [39424/50048]	Loss: 0.7678
Training Epoch: 29 [39552/50048]	Loss: 0.9589
Training Epoch: 29 [39680/50048]	Loss: 0.9299
Training Epoch: 29 [39808/50048]	Loss: 0.7227
Training Epoch: 29 [39936/50048]	Loss: 0.7275
Training Epoch: 29 [40064/50048]	Loss: 0.7737
Training Epoch: 29 [40192/50048]	Loss: 0.6991
Training Epoch: 29 [40320/50048]	Loss: 0.7740
Training Epoch: 29 [40448/50048]	Loss: 0.6290
Training Epoch: 29 [40576/50048]	Loss: 0.8334
Training Epoch: 29 [40704/50048]	Loss: 0.8102
Training Epoch: 29 [40832/50048]	Loss: 0.8269
Training Epoch: 29 [40960/50048]	Loss: 0.6200
Training Epoch: 29 [41088/50048]	Loss: 0.7525
Training Epoch: 29 [41216/50048]	Loss: 0.7250
Training Epoch: 29 [41344/50048]	Loss: 0.5928
Training Epoch: 29 [41472/50048]	Loss: 0.6994
Training Epoch: 29 [41600/50048]	Loss: 0.7537
Training Epoch: 29 [41728/50048]	Loss: 0.8192
Training Epoch: 29 [41856/50048]	Loss: 0.6446
Training Epoch: 29 [41984/50048]	Loss: 0.8398
Training Epoch: 29 [42112/50048]	Loss: 0.7495
Training Epoch: 29 [42240/50048]	Loss: 0.6929
Training Epoch: 29 [42368/50048]	Loss: 0.7109
Training Epoch: 29 [42496/50048]	Loss: 0.8502
Training Epoch: 29 [42624/50048]	Loss: 0.6317
Training Epoch: 29 [42752/50048]	Loss: 0.8200
Training Epoch: 29 [42880/50048]	Loss: 0.5699
Training Epoch: 29 [43008/50048]	Loss: 0.9039
Training Epoch: 29 [43136/50048]	Loss: 0.6168
Training Epoch: 29 [43264/50048]	Loss: 0.7314
Training Epoch: 29 [43392/50048]	Loss: 0.5906
Training Epoch: 29 [43520/50048]	Loss: 0.7852
Training Epoch: 29 [43648/50048]	Loss: 0.6041
Training Epoch: 29 [43776/50048]	Loss: 0.5718
Training Epoch: 29 [43904/50048]	Loss: 0.8420
Training Epoch: 29 [44032/50048]	Loss: 0.5826
Training Epoch: 29 [44160/50048]	Loss: 0.5933
Training Epoch: 29 [44288/50048]	Loss: 0.5991
Training Epoch: 29 [44416/50048]	Loss: 0.7612
Training Epoch: 29 [44544/50048]	Loss: 0.7296
Training Epoch: 29 [44672/50048]	Loss: 0.6847
Training Epoch: 29 [44800/50048]	Loss: 0.7671
Training Epoch: 29 [44928/50048]	Loss: 0.5511
Training Epoch: 29 [45056/50048]	Loss: 0.7737
Training Epoch: 29 [45184/50048]	Loss: 0.7233
Training Epoch: 29 [45312/50048]	Loss: 0.7492
Training Epoch: 29 [45440/50048]	Loss: 0.8768
Training Epoch: 29 [45568/50048]	Loss: 0.6560
Training Epoch: 29 [45696/50048]	Loss: 0.8474
2022-12-06 06:52:56,233 [ZeusDataLoader(train)] train epoch 30 done: time=86.45 energy=10505.45
2022-12-06 06:52:56,234 [ZeusDataLoader(eval)] Epoch 30 begin.
Training Epoch: 29 [45824/50048]	Loss: 0.6047
Training Epoch: 29 [45952/50048]	Loss: 0.8226
Training Epoch: 29 [46080/50048]	Loss: 0.6518
Training Epoch: 29 [46208/50048]	Loss: 0.6535
Training Epoch: 29 [46336/50048]	Loss: 0.6642
Training Epoch: 29 [46464/50048]	Loss: 0.8736
Training Epoch: 29 [46592/50048]	Loss: 0.7924
Training Epoch: 29 [46720/50048]	Loss: 0.6675
Training Epoch: 29 [46848/50048]	Loss: 0.8860
Training Epoch: 29 [46976/50048]	Loss: 0.7189
Training Epoch: 29 [47104/50048]	Loss: 0.5786
Training Epoch: 29 [47232/50048]	Loss: 0.7372
Training Epoch: 29 [47360/50048]	Loss: 0.6806
Training Epoch: 29 [47488/50048]	Loss: 0.6082
Training Epoch: 29 [47616/50048]	Loss: 0.8137
Training Epoch: 29 [47744/50048]	Loss: 0.6227
Training Epoch: 29 [47872/50048]	Loss: 0.7752
Training Epoch: 29 [48000/50048]	Loss: 0.8156
Training Epoch: 29 [48128/50048]	Loss: 0.6862
Training Epoch: 29 [48256/50048]	Loss: 0.6987
Training Epoch: 29 [48384/50048]	Loss: 0.5946
Training Epoch: 29 [48512/50048]	Loss: 0.7668
Training Epoch: 29 [48640/50048]	Loss: 0.8211
Training Epoch: 29 [48768/50048]	Loss: 0.7187
Training Epoch: 29 [48896/50048]	Loss: 0.7267
Training Epoch: 29 [49024/50048]	Loss: 0.7010
Training Epoch: 29 [49152/50048]	Loss: 0.6977
Training Epoch: 29 [49280/50048]	Loss: 0.7707
Training Epoch: 29 [49408/50048]	Loss: 0.8838
Training Epoch: 29 [49536/50048]	Loss: 0.6896
Training Epoch: 29 [49664/50048]	Loss: 0.8172
Training Epoch: 29 [49792/50048]	Loss: 0.7213
Training Epoch: 29 [49920/50048]	Loss: 0.9513
Training Epoch: 29 [50048/50048]	Loss: 0.8097
2022-12-06 11:52:59.892 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:52:59,913 [ZeusDataLoader(eval)] eval epoch 30 done: time=3.67 energy=441.01
2022-12-06 06:52:59,913 [ZeusDataLoader(train)] Up to epoch 30: time=2705.64, energy=328467.68, cost=400977.73
2022-12-06 06:52:59,913 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:52:59,913 [ZeusDataLoader(train)] Expected next epoch: time=2795.44, energy=339265.70, cost=414234.12
2022-12-06 06:52:59,914 [ZeusDataLoader(train)] Epoch 31 begin.
Validation Epoch: 29, Average loss: 0.0122, Accuracy: 0.6181
2022-12-06 06:53:00,108 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:53:00,109 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:53:00.110 [ZeusMonitor] Monitor started.
2022-12-06 11:53:00.111 [ZeusMonitor] Running indefinitely. 2022-12-06 11:53:00.111 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:53:00.111 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e31+gpu0.power.log
Training Epoch: 30 [128/50048]	Loss: 0.5421
Training Epoch: 30 [256/50048]	Loss: 0.6661
Training Epoch: 30 [384/50048]	Loss: 0.6779
Training Epoch: 30 [512/50048]	Loss: 0.5468
Training Epoch: 30 [640/50048]	Loss: 0.7839
Training Epoch: 30 [768/50048]	Loss: 0.5923
Training Epoch: 30 [896/50048]	Loss: 0.6770
Training Epoch: 30 [1024/50048]	Loss: 0.5568
Training Epoch: 30 [1152/50048]	Loss: 0.7145
Training Epoch: 30 [1280/50048]	Loss: 0.5871
Training Epoch: 30 [1408/50048]	Loss: 0.7131
Training Epoch: 30 [1536/50048]	Loss: 0.8565
Training Epoch: 30 [1664/50048]	Loss: 0.5274
Training Epoch: 30 [1792/50048]	Loss: 0.6188
Training Epoch: 30 [1920/50048]	Loss: 0.4574
Training Epoch: 30 [2048/50048]	Loss: 0.6496
Training Epoch: 30 [2176/50048]	Loss: 0.6462
Training Epoch: 30 [2304/50048]	Loss: 0.6097
Training Epoch: 30 [2432/50048]	Loss: 0.6310
Training Epoch: 30 [2560/50048]	Loss: 0.4413
Training Epoch: 30 [2688/50048]	Loss: 0.6507
Training Epoch: 30 [2816/50048]	Loss: 0.6949
Training Epoch: 30 [2944/50048]	Loss: 0.7852
Training Epoch: 30 [3072/50048]	Loss: 0.5101
Training Epoch: 30 [3200/50048]	Loss: 0.6640
Training Epoch: 30 [3328/50048]	Loss: 0.6889
Training Epoch: 30 [3456/50048]	Loss: 0.5819
Training Epoch: 30 [3584/50048]	Loss: 0.5134
Training Epoch: 30 [3712/50048]	Loss: 0.7432
Training Epoch: 30 [3840/50048]	Loss: 0.4617
Training Epoch: 30 [3968/50048]	Loss: 0.6532
Training Epoch: 30 [4096/50048]	Loss: 0.4238
Training Epoch: 30 [4224/50048]	Loss: 0.6802
Training Epoch: 30 [4352/50048]	Loss: 0.5783
Training Epoch: 30 [4480/50048]	Loss: 0.4796
Training Epoch: 30 [4608/50048]	Loss: 0.6683
Training Epoch: 30 [4736/50048]	Loss: 0.5858
Training Epoch: 30 [4864/50048]	Loss: 0.8024
Training Epoch: 30 [4992/50048]	Loss: 0.6885
Training Epoch: 30 [5120/50048]	Loss: 0.5660
Training Epoch: 30 [5248/50048]	Loss: 0.6134
Training Epoch: 30 [5376/50048]	Loss: 0.5561
Training Epoch: 30 [5504/50048]	Loss: 0.7107
Training Epoch: 30 [5632/50048]	Loss: 0.6501
Training Epoch: 30 [5760/50048]	Loss: 0.6006
Training Epoch: 30 [5888/50048]	Loss: 0.7102
Training Epoch: 30 [6016/50048]	Loss: 0.7186
Training Epoch: 30 [6144/50048]	Loss: 0.6371
Training Epoch: 30 [6272/50048]	Loss: 0.5758
Training Epoch: 30 [6400/50048]	Loss: 0.7573
Training Epoch: 30 [6528/50048]	Loss: 0.5943
Training Epoch: 30 [6656/50048]	Loss: 0.5714
Training Epoch: 30 [6784/50048]	Loss: 0.6693
Training Epoch: 30 [6912/50048]	Loss: 0.8085
Training Epoch: 30 [7040/50048]	Loss: 0.6540
Training Epoch: 30 [7168/50048]	Loss: 0.7970
Training Epoch: 30 [7296/50048]	Loss: 0.6793
Training Epoch: 30 [7424/50048]	Loss: 0.8282
Training Epoch: 30 [7552/50048]	Loss: 0.5070
Training Epoch: 30 [7680/50048]	Loss: 0.8059
Training Epoch: 30 [7808/50048]	Loss: 0.7965
Training Epoch: 30 [7936/50048]	Loss: 0.6477
Training Epoch: 30 [8064/50048]	Loss: 0.6873
Training Epoch: 30 [8192/50048]	Loss: 0.6028
Training Epoch: 30 [8320/50048]	Loss: 0.6551
Training Epoch: 30 [8448/50048]	Loss: 0.8459
Training Epoch: 30 [8576/50048]	Loss: 0.6420
Training Epoch: 30 [8704/50048]	Loss: 0.6161
Training Epoch: 30 [8832/50048]	Loss: 0.8177
Training Epoch: 30 [8960/50048]	Loss: 0.6858
Training Epoch: 30 [9088/50048]	Loss: 0.8696
Training Epoch: 30 [9216/50048]	Loss: 0.4563
Training Epoch: 30 [9344/50048]	Loss: 0.4276
Training Epoch: 30 [9472/50048]	Loss: 0.6361
Training Epoch: 30 [9600/50048]	Loss: 0.5976
Training Epoch: 30 [9728/50048]	Loss: 0.6054
Training Epoch: 30 [9856/50048]	Loss: 0.7963
Training Epoch: 30 [9984/50048]	Loss: 0.6331
Training Epoch: 30 [10112/50048]	Loss: 0.5671
Training Epoch: 30 [10240/50048]	Loss: 0.5529
Training Epoch: 30 [10368/50048]	Loss: 0.5956
Training Epoch: 30 [10496/50048]	Loss: 0.7303
Training Epoch: 30 [10624/50048]	Loss: 0.6575
Training Epoch: 30 [10752/50048]	Loss: 0.7665
Training Epoch: 30 [10880/50048]	Loss: 0.4690
Training Epoch: 30 [11008/50048]	Loss: 0.7248
Training Epoch: 30 [11136/50048]	Loss: 0.6589
Training Epoch: 30 [11264/50048]	Loss: 0.6631
Training Epoch: 30 [11392/50048]	Loss: 0.7294
Training Epoch: 30 [11520/50048]	Loss: 0.6009
Training Epoch: 30 [11648/50048]	Loss: 0.7436
Training Epoch: 30 [11776/50048]	Loss: 0.5646
Training Epoch: 30 [11904/50048]	Loss: 0.6267
Training Epoch: 30 [12032/50048]	Loss: 0.5757
Training Epoch: 30 [12160/50048]	Loss: 0.8732
Training Epoch: 30 [12288/50048]	Loss: 0.6644
Training Epoch: 30 [12416/50048]	Loss: 0.7585
Training Epoch: 30 [12544/50048]	Loss: 0.4972
Training Epoch: 30 [12672/50048]	Loss: 0.6308
Training Epoch: 30 [12800/50048]	Loss: 0.6219
Training Epoch: 30 [12928/50048]	Loss: 0.6659
Training Epoch: 30 [13056/50048]	Loss: 0.6342
Training Epoch: 30 [13184/50048]	Loss: 0.4877
Training Epoch: 30 [13312/50048]	Loss: 0.5972
Training Epoch: 30 [13440/50048]	Loss: 0.6545
Training Epoch: 30 [13568/50048]	Loss: 0.5166
Training Epoch: 30 [13696/50048]	Loss: 0.6474
Training Epoch: 30 [13824/50048]	Loss: 0.5909
Training Epoch: 30 [13952/50048]	Loss: 0.7764
Training Epoch: 30 [14080/50048]	Loss: 0.4625
Training Epoch: 30 [14208/50048]	Loss: 0.5905
Training Epoch: 30 [14336/50048]	Loss: 0.5398
Training Epoch: 30 [14464/50048]	Loss: 0.8063
Training Epoch: 30 [14592/50048]	Loss: 0.4553
Training Epoch: 30 [14720/50048]	Loss: 0.6423
Training Epoch: 30 [14848/50048]	Loss: 0.5995
Training Epoch: 30 [14976/50048]	Loss: 0.7967
Training Epoch: 30 [15104/50048]	Loss: 0.7098
Training Epoch: 30 [15232/50048]	Loss: 0.7025
Training Epoch: 30 [15360/50048]	Loss: 0.8078
Training Epoch: 30 [15488/50048]	Loss: 0.7904
Training Epoch: 30 [15616/50048]	Loss: 0.6816
Training Epoch: 30 [15744/50048]	Loss: 0.5808
Training Epoch: 30 [15872/50048]	Loss: 0.6712
Training Epoch: 30 [16000/50048]	Loss: 0.6899
Training Epoch: 30 [16128/50048]	Loss: 0.6591
Training Epoch: 30 [16256/50048]	Loss: 0.7372
Training Epoch: 30 [16384/50048]	Loss: 0.6620
Training Epoch: 30 [16512/50048]	Loss: 0.6802
Training Epoch: 30 [16640/50048]	Loss: 0.7382
Training Epoch: 30 [16768/50048]	Loss: 1.1187
Training Epoch: 30 [16896/50048]	Loss: 0.6752
Training Epoch: 30 [17024/50048]	Loss: 0.5830
Training Epoch: 30 [17152/50048]	Loss: 0.6188
Training Epoch: 30 [17280/50048]	Loss: 0.6047
Training Epoch: 30 [17408/50048]	Loss: 0.6419
Training Epoch: 30 [17536/50048]	Loss: 0.5227
Training Epoch: 30 [17664/50048]	Loss: 0.7414
Training Epoch: 30 [17792/50048]	Loss: 0.6403
Training Epoch: 30 [17920/50048]	Loss: 0.4939
Training Epoch: 30 [18048/50048]	Loss: 0.5132
Training Epoch: 30 [18176/50048]	Loss: 0.5031
Training Epoch: 30 [18304/50048]	Loss: 0.6411
Training Epoch: 30 [18432/50048]	Loss: 0.5980
Training Epoch: 30 [18560/50048]	Loss: 0.5992
Training Epoch: 30 [18688/50048]	Loss: 0.6055
Training Epoch: 30 [18816/50048]	Loss: 0.6150
Training Epoch: 30 [18944/50048]	Loss: 0.5304
Training Epoch: 30 [19072/50048]	Loss: 0.5991
Training Epoch: 30 [19200/50048]	Loss: 0.6440
Training Epoch: 30 [19328/50048]	Loss: 0.6577
Training Epoch: 30 [19456/50048]	Loss: 0.5531
Training Epoch: 30 [19584/50048]	Loss: 0.8070
Training Epoch: 30 [19712/50048]	Loss: 0.6500
Training Epoch: 30 [19840/50048]	Loss: 0.6766
Training Epoch: 30 [19968/50048]	Loss: 0.6960
Training Epoch: 30 [20096/50048]	Loss: 0.6677
Training Epoch: 30 [20224/50048]	Loss: 0.5858
Training Epoch: 30 [20352/50048]	Loss: 0.7264
Training Epoch: 30 [20480/50048]	Loss: 0.6635
Training Epoch: 30 [20608/50048]	Loss: 0.6191
Training Epoch: 30 [20736/50048]	Loss: 0.5410
Training Epoch: 30 [20864/50048]	Loss: 0.7025
Training Epoch: 30 [20992/50048]	Loss: 0.5754
Training Epoch: 30 [21120/50048]	Loss: 0.6375
Training Epoch: 30 [21248/50048]	Loss: 0.5367
Training Epoch: 30 [21376/50048]	Loss: 0.5873
Training Epoch: 30 [21504/50048]	Loss: 0.7111
Training Epoch: 30 [21632/50048]	Loss: 0.7058
Training Epoch: 30 [21760/50048]	Loss: 0.6627
Training Epoch: 30 [21888/50048]	Loss: 0.5301
Training Epoch: 30 [22016/50048]	Loss: 0.7918
Training Epoch: 30 [22144/50048]	Loss: 0.5437
Training Epoch: 30 [22272/50048]	Loss: 0.6087
Training Epoch: 30 [22400/50048]	Loss: 0.5669
Training Epoch: 30 [22528/50048]	Loss: 0.5203
Training Epoch: 30 [22656/50048]	Loss: 0.6004
Training Epoch: 30 [22784/50048]	Loss: 0.7253
Training Epoch: 30 [22912/50048]	Loss: 0.6549
Training Epoch: 30 [23040/50048]	Loss: 0.7759
Training Epoch: 30 [23168/50048]	Loss: 0.5312
Training Epoch: 30 [23296/50048]	Loss: 0.5618
Training Epoch: 30 [23424/50048]	Loss: 0.6657
Training Epoch: 30 [23552/50048]	Loss: 0.7801
Training Epoch: 30 [23680/50048]	Loss: 0.6848
Training Epoch: 30 [23808/50048]	Loss: 0.7217
Training Epoch: 30 [23936/50048]	Loss: 0.5863
Training Epoch: 30 [24064/50048]	Loss: 0.7905
Training Epoch: 30 [24192/50048]	Loss: 0.6722
Training Epoch: 30 [24320/50048]	Loss: 0.4841
Training Epoch: 30 [24448/50048]	Loss: 0.6459
Training Epoch: 30 [24576/50048]	Loss: 0.5883
Training Epoch: 30 [24704/50048]	Loss: 0.7087
Training Epoch: 30 [24832/50048]	Loss: 0.5385
Training Epoch: 30 [24960/50048]	Loss: 0.6820
Training Epoch: 30 [25088/50048]	Loss: 0.8119
Training Epoch: 30 [25216/50048]	Loss: 0.7357
Training Epoch: 30 [25344/50048]	Loss: 0.8653
Training Epoch: 30 [25472/50048]	Loss: 0.5914
Training Epoch: 30 [25600/50048]	Loss: 0.7115
Training Epoch: 30 [25728/50048]	Loss: 0.7227
Training Epoch: 30 [25856/50048]	Loss: 0.6374
Training Epoch: 30 [25984/50048]	Loss: 0.5604
Training Epoch: 30 [26112/50048]	Loss: 0.4939
Training Epoch: 30 [26240/50048]	Loss: 0.6488
Training Epoch: 30 [26368/50048]	Loss: 0.6829
Training Epoch: 30 [26496/50048]	Loss: 0.7176
Training Epoch: 30 [26624/50048]	Loss: 0.6233
Training Epoch: 30 [26752/50048]	Loss: 0.8523
Training Epoch: 30 [26880/50048]	Loss: 0.7981
Training Epoch: 30 [27008/50048]	Loss: 0.8733
Training Epoch: 30 [27136/50048]	Loss: 0.4783
Training Epoch: 30 [27264/50048]	Loss: 0.6406
Training Epoch: 30 [27392/50048]	Loss: 0.7533
Training Epoch: 30 [27520/50048]	Loss: 0.9110
Training Epoch: 30 [27648/50048]	Loss: 0.5249
Training Epoch: 30 [27776/50048]	Loss: 0.7809
Training Epoch: 30 [27904/50048]	Loss: 0.6671
Training Epoch: 30 [28032/50048]	Loss: 0.7276
Training Epoch: 30 [28160/50048]	Loss: 0.6145
Training Epoch: 30 [28288/50048]	Loss: 0.7311
Training Epoch: 30 [28416/50048]	Loss: 0.9008
Training Epoch: 30 [28544/50048]	Loss: 0.5787
Training Epoch: 30 [28672/50048]	Loss: 0.6679
Training Epoch: 30 [28800/50048]	Loss: 0.5843
Training Epoch: 30 [28928/50048]	Loss: 0.8567
Training Epoch: 30 [29056/50048]	Loss: 0.6039
Training Epoch: 30 [29184/50048]	Loss: 0.5711
Training Epoch: 30 [29312/50048]	Loss: 0.7159
Training Epoch: 30 [29440/50048]	Loss: 0.6011
Training Epoch: 30 [29568/50048]	Loss: 0.6418
Training Epoch: 30 [29696/50048]	Loss: 0.6495
Training Epoch: 30 [29824/50048]	Loss: 0.4870
Training Epoch: 30 [29952/50048]	Loss: 0.6988
Training Epoch: 30 [30080/50048]	Loss: 0.5739
Training Epoch: 30 [30208/50048]	Loss: 0.8255
Training Epoch: 30 [30336/50048]	Loss: 0.8381
Training Epoch: 30 [30464/50048]	Loss: 0.6713
Training Epoch: 30 [30592/50048]	Loss: 0.5947
Training Epoch: 30 [30720/50048]	Loss: 0.7860
Training Epoch: 30 [30848/50048]	Loss: 0.6417
Training Epoch: 30 [30976/50048]	Loss: 0.5475
Training Epoch: 30 [31104/50048]	Loss: 0.5594
Training Epoch: 30 [31232/50048]	Loss: 0.6691
Training Epoch: 30 [31360/50048]	Loss: 0.5359
Training Epoch: 30 [31488/50048]	Loss: 0.7197
Training Epoch: 30 [31616/50048]	Loss: 0.7791
Training Epoch: 30 [31744/50048]	Loss: 0.6523
Training Epoch: 30 [31872/50048]	Loss: 0.4593
Training Epoch: 30 [32000/50048]	Loss: 0.7500
Training Epoch: 30 [32128/50048]	Loss: 0.4667
Training Epoch: 30 [32256/50048]	Loss: 0.6365
Training Epoch: 30 [32384/50048]	Loss: 0.6938
Training Epoch: 30 [32512/50048]	Loss: 0.6466
Training Epoch: 30 [32640/50048]	Loss: 0.5855
Training Epoch: 30 [32768/50048]	Loss: 0.7419
Training Epoch: 30 [32896/50048]	Loss: 0.6492
Training Epoch: 30 [33024/50048]	Loss: 0.5980
Training Epoch: 30 [33152/50048]	Loss: 0.6779
Training Epoch: 30 [33280/50048]	Loss: 0.6602
Training Epoch: 30 [33408/50048]	Loss: 0.7250
Training Epoch: 30 [33536/50048]	Loss: 0.7599
Training Epoch: 30 [33664/50048]	Loss: 0.5583
Training Epoch: 30 [33792/50048]	Loss: 0.7082
Training Epoch: 30 [33920/50048]	Loss: 0.7369
Training Epoch: 30 [34048/50048]	Loss: 0.6591
Training Epoch: 30 [34176/50048]	Loss: 0.6014
Training Epoch: 30 [34304/50048]	Loss: 0.6424
Training Epoch: 30 [34432/50048]	Loss: 0.6424
Training Epoch: 30 [34560/50048]	Loss: 0.8559
Training Epoch: 30 [34688/50048]	Loss: 0.8249
Training Epoch: 30 [34816/50048]	Loss: 0.8609
Training Epoch: 30 [34944/50048]	Loss: 0.7048
Training Epoch: 30 [35072/50048]	Loss: 0.7447
Training Epoch: 30 [35200/50048]	Loss: 0.6596
Training Epoch: 30 [35328/50048]	Loss: 0.6297
Training Epoch: 30 [35456/50048]	Loss: 0.6816
Training Epoch: 30 [35584/50048]	Loss: 0.6483
Training Epoch: 30 [35712/50048]	Loss: 0.8065
Training Epoch: 30 [35840/50048]	Loss: 0.6755
Training Epoch: 30 [35968/50048]	Loss: 0.7340
Training Epoch: 30 [36096/50048]	Loss: 0.7591
Training Epoch: 30 [36224/50048]	Loss: 0.7747
Training Epoch: 30 [36352/50048]	Loss: 0.8803
Training Epoch: 30 [36480/50048]	Loss: 0.7376
Training Epoch: 30 [36608/50048]	Loss: 0.6470
Training Epoch: 30 [36736/50048]	Loss: 0.5929
Training Epoch: 30 [36864/50048]	Loss: 0.8232
Training Epoch: 30 [36992/50048]	Loss: 0.6260
Training Epoch: 30 [37120/50048]	Loss: 0.5355
Training Epoch: 30 [37248/50048]	Loss: 0.6099
Training Epoch: 30 [37376/50048]	Loss: 0.6854
Training Epoch: 30 [37504/50048]	Loss: 0.7060
Training Epoch: 30 [37632/50048]	Loss: 0.5792
Training Epoch: 30 [37760/50048]	Loss: 0.5376
Training Epoch: 30 [37888/50048]	Loss: 0.6873
Training Epoch: 30 [38016/50048]	Loss: 0.7161
Training Epoch: 30 [38144/50048]	Loss: 0.6999
Training Epoch: 30 [38272/50048]	Loss: 0.6001
Training Epoch: 30 [38400/50048]	Loss: 0.7346
Training Epoch: 30 [38528/50048]	Loss: 0.7700
Training Epoch: 30 [38656/50048]	Loss: 0.7657
Training Epoch: 30 [38784/50048]	Loss: 0.8501
Training Epoch: 30 [38912/50048]	Loss: 0.7865
Training Epoch: 30 [39040/50048]	Loss: 0.5933
Training Epoch: 30 [39168/50048]	Loss: 0.7648
Training Epoch: 30 [39296/50048]	Loss: 0.6165
Training Epoch: 30 [39424/50048]	Loss: 0.6538
Training Epoch: 30 [39552/50048]	Loss: 0.4766
Training Epoch: 30 [39680/50048]	Loss: 0.8599
Training Epoch: 30 [39808/50048]	Loss: 0.7861
Training Epoch: 30 [39936/50048]	Loss: 0.6656
Training Epoch: 30 [40064/50048]	Loss: 0.5977
Training Epoch: 30 [40192/50048]	Loss: 0.7236
Training Epoch: 30 [40320/50048]	Loss: 0.6179
Training Epoch: 30 [40448/50048]	Loss: 0.6329
Training Epoch: 30 [40576/50048]	Loss: 0.5788
Training Epoch: 30 [40704/50048]	Loss: 0.8640
Training Epoch: 30 [40832/50048]	Loss: 0.7904
Training Epoch: 30 [40960/50048]	Loss: 0.6233
Training Epoch: 30 [41088/50048]	Loss: 0.6119
Training Epoch: 30 [41216/50048]	Loss: 0.6438
Training Epoch: 30 [41344/50048]	Loss: 0.7720
Training Epoch: 30 [41472/50048]	Loss: 0.8989
Training Epoch: 30 [41600/50048]	Loss: 0.6151
Training Epoch: 30 [41728/50048]	Loss: 0.7842
Training Epoch: 30 [41856/50048]	Loss: 0.5816
Training Epoch: 30 [41984/50048]	Loss: 0.6439
Training Epoch: 30 [42112/50048]	Loss: 0.7022
Training Epoch: 30 [42240/50048]	Loss: 0.9196
Training Epoch: 30 [42368/50048]	Loss: 0.5522
Training Epoch: 30 [42496/50048]	Loss: 0.6931
Training Epoch: 30 [42624/50048]	Loss: 0.5853
Training Epoch: 30 [42752/50048]	Loss: 0.6870
Training Epoch: 30 [42880/50048]	Loss: 0.6731
Training Epoch: 30 [43008/50048]	Loss: 0.7313
Training Epoch: 30 [43136/50048]	Loss: 0.8439
Training Epoch: 30 [43264/50048]	Loss: 0.6469
Training Epoch: 30 [43392/50048]	Loss: 0.7900
Training Epoch: 30 [43520/50048]	Loss: 0.7927
Training Epoch: 30 [43648/50048]	Loss: 0.4701
Training Epoch: 30 [43776/50048]	Loss: 0.7886
Training Epoch: 30 [43904/50048]	Loss: 0.6693
Training Epoch: 30 [44032/50048]	Loss: 0.5038
Training Epoch: 30 [44160/50048]	Loss: 0.5898
Training Epoch: 30 [44288/50048]	Loss: 0.7817
Training Epoch: 30 [44416/50048]	Loss: 0.6039
Training Epoch: 30 [44544/50048]	Loss: 0.7686
Training Epoch: 30 [44672/50048]	Loss: 0.7230
Training Epoch: 30 [44800/50048]	Loss: 0.6050
Training Epoch: 30 [44928/50048]	Loss: 0.7156
Training Epoch: 30 [45056/50048]	Loss: 0.7664
Training Epoch: 30 [45184/50048]	Loss: 0.6728
Training Epoch: 30 [45312/50048]	Loss: 0.7194
Training Epoch: 30 [45440/50048]	Loss: 0.7476
Training Epoch: 30 [45568/50048]	Loss: 0.7255
Training Epoch: 30 [45696/50048]	Loss: 0.5621
2022-12-06 06:54:26,404 [ZeusDataLoader(train)] train epoch 31 done: time=86.48 energy=10496.43
2022-12-06 06:54:26,406 [ZeusDataLoader(eval)] Epoch 31 begin.
Training Epoch: 30 [45824/50048]	Loss: 0.5755
Training Epoch: 30 [45952/50048]	Loss: 0.7273
Training Epoch: 30 [46080/50048]	Loss: 0.7283
Training Epoch: 30 [46208/50048]	Loss: 0.9282
Training Epoch: 30 [46336/50048]	Loss: 0.6271
Training Epoch: 30 [46464/50048]	Loss: 0.6236
Training Epoch: 30 [46592/50048]	Loss: 0.6117
Training Epoch: 30 [46720/50048]	Loss: 0.9497
Training Epoch: 30 [46848/50048]	Loss: 0.6739
Training Epoch: 30 [46976/50048]	Loss: 0.6376
Training Epoch: 30 [47104/50048]	Loss: 0.6591
Training Epoch: 30 [47232/50048]	Loss: 0.6947
Training Epoch: 30 [47360/50048]	Loss: 0.6544
Training Epoch: 30 [47488/50048]	Loss: 0.7120
Training Epoch: 30 [47616/50048]	Loss: 0.5909
Training Epoch: 30 [47744/50048]	Loss: 0.7819
Training Epoch: 30 [47872/50048]	Loss: 0.7479
Training Epoch: 30 [48000/50048]	Loss: 0.7232
Training Epoch: 30 [48128/50048]	Loss: 0.8737
Training Epoch: 30 [48256/50048]	Loss: 0.9185
Training Epoch: 30 [48384/50048]	Loss: 0.7095
Training Epoch: 30 [48512/50048]	Loss: 0.7059
Training Epoch: 30 [48640/50048]	Loss: 0.7906
Training Epoch: 30 [48768/50048]	Loss: 0.6842
Training Epoch: 30 [48896/50048]	Loss: 0.7651
Training Epoch: 30 [49024/50048]	Loss: 0.7517
Training Epoch: 30 [49152/50048]	Loss: 0.6728
Training Epoch: 30 [49280/50048]	Loss: 0.7499
Training Epoch: 30 [49408/50048]	Loss: 0.6588
Training Epoch: 30 [49536/50048]	Loss: 0.7632
Training Epoch: 30 [49664/50048]	Loss: 0.6015
Training Epoch: 30 [49792/50048]	Loss: 0.6904
Training Epoch: 30 [49920/50048]	Loss: 1.0076
Training Epoch: 30 [50048/50048]	Loss: 0.8383
2022-12-06 11:54:30.075 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:54:30,113 [ZeusDataLoader(eval)] eval epoch 31 done: time=3.70 energy=453.53
2022-12-06 06:54:30,113 [ZeusDataLoader(train)] Up to epoch 31: time=2795.82, energy=339417.64, cost=414343.27
2022-12-06 06:54:30,113 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:54:30,113 [ZeusDataLoader(train)] Expected next epoch: time=2885.62, energy=350215.65, cost=427599.65
2022-12-06 06:54:30,114 [ZeusDataLoader(train)] Epoch 32 begin.
Validation Epoch: 30, Average loss: 0.0125, Accuracy: 0.6113
2022-12-06 06:54:30,301 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:54:30,302 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:54:30.304 [ZeusMonitor] Monitor started.
2022-12-06 11:54:30.304 [ZeusMonitor] Running indefinitely. 2022-12-06 11:54:30.304 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:54:30.304 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e32+gpu0.power.log
Training Epoch: 31 [128/50048]	Loss: 0.6070
Training Epoch: 31 [256/50048]	Loss: 0.5971
Training Epoch: 31 [384/50048]	Loss: 0.5969
Training Epoch: 31 [512/50048]	Loss: 0.4199
Training Epoch: 31 [640/50048]	Loss: 0.5408
Training Epoch: 31 [768/50048]	Loss: 0.5584
Training Epoch: 31 [896/50048]	Loss: 0.5441
Training Epoch: 31 [1024/50048]	Loss: 0.7939
Training Epoch: 31 [1152/50048]	Loss: 0.4795
Training Epoch: 31 [1280/50048]	Loss: 0.4820
Training Epoch: 31 [1408/50048]	Loss: 0.6414
Training Epoch: 31 [1536/50048]	Loss: 0.5307
Training Epoch: 31 [1664/50048]	Loss: 0.6202
Training Epoch: 31 [1792/50048]	Loss: 0.5076
Training Epoch: 31 [1920/50048]	Loss: 0.5355
Training Epoch: 31 [2048/50048]	Loss: 0.6120
Training Epoch: 31 [2176/50048]	Loss: 0.4648
Training Epoch: 31 [2304/50048]	Loss: 0.6395
Training Epoch: 31 [2432/50048]	Loss: 0.5690
Training Epoch: 31 [2560/50048]	Loss: 0.6337
Training Epoch: 31 [2688/50048]	Loss: 0.6028
Training Epoch: 31 [2816/50048]	Loss: 0.5766
Training Epoch: 31 [2944/50048]	Loss: 0.5976
Training Epoch: 31 [3072/50048]	Loss: 0.5368
Training Epoch: 31 [3200/50048]	Loss: 0.5239
Training Epoch: 31 [3328/50048]	Loss: 0.6663
Training Epoch: 31 [3456/50048]	Loss: 0.5764
Training Epoch: 31 [3584/50048]	Loss: 0.4773
Training Epoch: 31 [3712/50048]	Loss: 0.5550
Training Epoch: 31 [3840/50048]	Loss: 0.5470
Training Epoch: 31 [3968/50048]	Loss: 0.6138
Training Epoch: 31 [4096/50048]	Loss: 0.6256
Training Epoch: 31 [4224/50048]	Loss: 0.5198
Training Epoch: 31 [4352/50048]	Loss: 0.4703
Training Epoch: 31 [4480/50048]	Loss: 0.6168
Training Epoch: 31 [4608/50048]	Loss: 0.4722
Training Epoch: 31 [4736/50048]	Loss: 0.6512
Training Epoch: 31 [4864/50048]	Loss: 0.7325
Training Epoch: 31 [4992/50048]	Loss: 0.5448
Training Epoch: 31 [5120/50048]	Loss: 0.7124
Training Epoch: 31 [5248/50048]	Loss: 0.5113
Training Epoch: 31 [5376/50048]	Loss: 0.5463
Training Epoch: 31 [5504/50048]	Loss: 0.6203
Training Epoch: 31 [5632/50048]	Loss: 0.5581
Training Epoch: 31 [5760/50048]	Loss: 0.5043
Training Epoch: 31 [5888/50048]	Loss: 0.5555
Training Epoch: 31 [6016/50048]	Loss: 0.5799
Training Epoch: 31 [6144/50048]	Loss: 0.6065
Training Epoch: 31 [6272/50048]	Loss: 0.7057
Training Epoch: 31 [6400/50048]	Loss: 0.7672
Training Epoch: 31 [6528/50048]	Loss: 0.6850
Training Epoch: 31 [6656/50048]	Loss: 0.6018
Training Epoch: 31 [6784/50048]	Loss: 0.4885
Training Epoch: 31 [6912/50048]	Loss: 0.6187
Training Epoch: 31 [7040/50048]	Loss: 0.4995
Training Epoch: 31 [7168/50048]	Loss: 0.5153
Training Epoch: 31 [7296/50048]	Loss: 0.6294
Training Epoch: 31 [7424/50048]	Loss: 0.6694
Training Epoch: 31 [7552/50048]	Loss: 0.5674
Training Epoch: 31 [7680/50048]	Loss: 0.6459
Training Epoch: 31 [7808/50048]	Loss: 0.6648
Training Epoch: 31 [7936/50048]	Loss: 0.6093
Training Epoch: 31 [8064/50048]	Loss: 0.6316
Training Epoch: 31 [8192/50048]	Loss: 0.5819
Training Epoch: 31 [8320/50048]	Loss: 0.5810
Training Epoch: 31 [8448/50048]	Loss: 0.6806
Training Epoch: 31 [8576/50048]	Loss: 0.6838
Training Epoch: 31 [8704/50048]	Loss: 0.6008
Training Epoch: 31 [8832/50048]	Loss: 0.5738
Training Epoch: 31 [8960/50048]	Loss: 0.4962
Training Epoch: 31 [9088/50048]	Loss: 0.4977
Training Epoch: 31 [9216/50048]	Loss: 0.5404
Training Epoch: 31 [9344/50048]	Loss: 0.6696
Training Epoch: 31 [9472/50048]	Loss: 0.5567
Training Epoch: 31 [9600/50048]	Loss: 0.5210
Training Epoch: 31 [9728/50048]	Loss: 0.7342
Training Epoch: 31 [9856/50048]	Loss: 0.6454
Training Epoch: 31 [9984/50048]	Loss: 0.7343
Training Epoch: 31 [10112/50048]	Loss: 0.5501
Training Epoch: 31 [10240/50048]	Loss: 0.6247
Training Epoch: 31 [10368/50048]	Loss: 0.6483
Training Epoch: 31 [10496/50048]	Loss: 0.4961
Training Epoch: 31 [10624/50048]	Loss: 0.5610
Training Epoch: 31 [10752/50048]	Loss: 0.7752
Training Epoch: 31 [10880/50048]	Loss: 0.5355
Training Epoch: 31 [11008/50048]	Loss: 0.5384
Training Epoch: 31 [11136/50048]	Loss: 0.5986
Training Epoch: 31 [11264/50048]	Loss: 0.5926
Training Epoch: 31 [11392/50048]	Loss: 0.5234
Training Epoch: 31 [11520/50048]	Loss: 0.6350
Training Epoch: 31 [11648/50048]	Loss: 0.6617
Training Epoch: 31 [11776/50048]	Loss: 0.6904
Training Epoch: 31 [11904/50048]	Loss: 0.6389
Training Epoch: 31 [12032/50048]	Loss: 0.7150
Training Epoch: 31 [12160/50048]	Loss: 0.5995
Training Epoch: 31 [12288/50048]	Loss: 0.6847
Training Epoch: 31 [12416/50048]	Loss: 0.6704
Training Epoch: 31 [12544/50048]	Loss: 0.6531
Training Epoch: 31 [12672/50048]	Loss: 0.7651
Training Epoch: 31 [12800/50048]	Loss: 0.6148
Training Epoch: 31 [12928/50048]	Loss: 0.6200
Training Epoch: 31 [13056/50048]	Loss: 0.6980
Training Epoch: 31 [13184/50048]	Loss: 0.7672
Training Epoch: 31 [13312/50048]	Loss: 0.7173
Training Epoch: 31 [13440/50048]	Loss: 0.5641
Training Epoch: 31 [13568/50048]	Loss: 0.6409
Training Epoch: 31 [13696/50048]	Loss: 0.6911
Training Epoch: 31 [13824/50048]	Loss: 0.5532
Training Epoch: 31 [13952/50048]	Loss: 0.4116
Training Epoch: 31 [14080/50048]	Loss: 0.5826
Training Epoch: 31 [14208/50048]	Loss: 0.4857
Training Epoch: 31 [14336/50048]	Loss: 0.6578
Training Epoch: 31 [14464/50048]	Loss: 0.6261
Training Epoch: 31 [14592/50048]	Loss: 0.6022
Training Epoch: 31 [14720/50048]	Loss: 0.5483
Training Epoch: 31 [14848/50048]	Loss: 0.3425
Training Epoch: 31 [14976/50048]	Loss: 0.5539
Training Epoch: 31 [15104/50048]	Loss: 0.5919
Training Epoch: 31 [15232/50048]	Loss: 0.4405
Training Epoch: 31 [15360/50048]	Loss: 0.5476
Training Epoch: 31 [15488/50048]	Loss: 0.6718
Training Epoch: 31 [15616/50048]	Loss: 0.6629
Training Epoch: 31 [15744/50048]	Loss: 0.6035
Training Epoch: 31 [15872/50048]	Loss: 0.7269
Training Epoch: 31 [16000/50048]	Loss: 0.7139
Training Epoch: 31 [16128/50048]	Loss: 0.3922
Training Epoch: 31 [16256/50048]	Loss: 0.7082
Training Epoch: 31 [16384/50048]	Loss: 0.5183
Training Epoch: 31 [16512/50048]	Loss: 0.5570
Training Epoch: 31 [16640/50048]	Loss: 0.5611
Training Epoch: 31 [16768/50048]	Loss: 0.4897
Training Epoch: 31 [16896/50048]	Loss: 0.5102
Training Epoch: 31 [17024/50048]	Loss: 0.7390
Training Epoch: 31 [17152/50048]	Loss: 0.5437
Training Epoch: 31 [17280/50048]	Loss: 0.5977
Training Epoch: 31 [17408/50048]	Loss: 0.6775
Training Epoch: 31 [17536/50048]	Loss: 0.8024
Training Epoch: 31 [17664/50048]	Loss: 0.6833
Training Epoch: 31 [17792/50048]	Loss: 0.7200
Training Epoch: 31 [17920/50048]	Loss: 0.5954
Training Epoch: 31 [18048/50048]	Loss: 0.5764
Training Epoch: 31 [18176/50048]	Loss: 0.7564
Training Epoch: 31 [18304/50048]	Loss: 0.5237
Training Epoch: 31 [18432/50048]	Loss: 0.6414
Training Epoch: 31 [18560/50048]	Loss: 0.7353
Training Epoch: 31 [18688/50048]	Loss: 0.5020
Training Epoch: 31 [18816/50048]	Loss: 0.6291
Training Epoch: 31 [18944/50048]	Loss: 0.5773
Training Epoch: 31 [19072/50048]	Loss: 0.8598
Training Epoch: 31 [19200/50048]	Loss: 0.5498
Training Epoch: 31 [19328/50048]	Loss: 0.8674
Training Epoch: 31 [19456/50048]	Loss: 0.8248
Training Epoch: 31 [19584/50048]	Loss: 0.6128
Training Epoch: 31 [19712/50048]	Loss: 0.6361
Training Epoch: 31 [19840/50048]	Loss: 0.6319
Training Epoch: 31 [19968/50048]	Loss: 0.6414
Training Epoch: 31 [20096/50048]	Loss: 0.5674
Training Epoch: 31 [20224/50048]	Loss: 0.8004
Training Epoch: 31 [20352/50048]	Loss: 0.4911
Training Epoch: 31 [20480/50048]	Loss: 0.6239
Training Epoch: 31 [20608/50048]	Loss: 0.7340
Training Epoch: 31 [20736/50048]	Loss: 0.6806
Training Epoch: 31 [20864/50048]	Loss: 0.6427
Training Epoch: 31 [20992/50048]	Loss: 0.5004
Training Epoch: 31 [21120/50048]	Loss: 0.8713
Training Epoch: 31 [21248/50048]	Loss: 0.6834
Training Epoch: 31 [21376/50048]	Loss: 0.6100
Training Epoch: 31 [21504/50048]	Loss: 0.6693
Training Epoch: 31 [21632/50048]	Loss: 0.7239
Training Epoch: 31 [21760/50048]	Loss: 0.5937
Training Epoch: 31 [21888/50048]	Loss: 0.5110
Training Epoch: 31 [22016/50048]	Loss: 0.4471
Training Epoch: 31 [22144/50048]	Loss: 0.4968
Training Epoch: 31 [22272/50048]	Loss: 0.7107
Training Epoch: 31 [22400/50048]	Loss: 0.7569
Training Epoch: 31 [22528/50048]	Loss: 0.7042
Training Epoch: 31 [22656/50048]	Loss: 0.8064
Training Epoch: 31 [22784/50048]	Loss: 0.6793
Training Epoch: 31 [22912/50048]	Loss: 0.7905
Training Epoch: 31 [23040/50048]	Loss: 0.6360
Training Epoch: 31 [23168/50048]	Loss: 0.6767
Training Epoch: 31 [23296/50048]	Loss: 0.7147
Training Epoch: 31 [23424/50048]	Loss: 0.6642
Training Epoch: 31 [23552/50048]	Loss: 0.6676
Training Epoch: 31 [23680/50048]	Loss: 0.7159
Training Epoch: 31 [23808/50048]	Loss: 0.6007
Training Epoch: 31 [23936/50048]	Loss: 0.7663
Training Epoch: 31 [24064/50048]	Loss: 0.5833
Training Epoch: 31 [24192/50048]	Loss: 0.5801
Training Epoch: 31 [24320/50048]	Loss: 0.6380
Training Epoch: 31 [24448/50048]	Loss: 0.7604
Training Epoch: 31 [24576/50048]	Loss: 0.5692
Training Epoch: 31 [24704/50048]	Loss: 0.5753
Training Epoch: 31 [24832/50048]	Loss: 0.5704
Training Epoch: 31 [24960/50048]	Loss: 0.5081
Training Epoch: 31 [25088/50048]	Loss: 0.7043
Training Epoch: 31 [25216/50048]	Loss: 0.5732
Training Epoch: 31 [25344/50048]	Loss: 0.3912
Training Epoch: 31 [25472/50048]	Loss: 0.7664
Training Epoch: 31 [25600/50048]	Loss: 0.5410
Training Epoch: 31 [25728/50048]	Loss: 0.4916
Training Epoch: 31 [25856/50048]	Loss: 0.8393
Training Epoch: 31 [25984/50048]	Loss: 0.6330
Training Epoch: 31 [26112/50048]	Loss: 0.6616
Training Epoch: 31 [26240/50048]	Loss: 0.6372
Training Epoch: 31 [26368/50048]	Loss: 0.6506
Training Epoch: 31 [26496/50048]	Loss: 0.6196
Training Epoch: 31 [26624/50048]	Loss: 0.4785
Training Epoch: 31 [26752/50048]	Loss: 0.6118
Training Epoch: 31 [26880/50048]	Loss: 0.5921
Training Epoch: 31 [27008/50048]	Loss: 0.5861
Training Epoch: 31 [27136/50048]	Loss: 0.5100
Training Epoch: 31 [27264/50048]	Loss: 0.8802
Training Epoch: 31 [27392/50048]	Loss: 0.4522
Training Epoch: 31 [27520/50048]	Loss: 0.6677
Training Epoch: 31 [27648/50048]	Loss: 0.6456
Training Epoch: 31 [27776/50048]	Loss: 0.5921
Training Epoch: 31 [27904/50048]	Loss: 0.8923
Training Epoch: 31 [28032/50048]	Loss: 0.5735
Training Epoch: 31 [28160/50048]	Loss: 0.6037
Training Epoch: 31 [28288/50048]	Loss: 0.7537
Training Epoch: 31 [28416/50048]	Loss: 0.6515
Training Epoch: 31 [28544/50048]	Loss: 0.6136
Training Epoch: 31 [28672/50048]	Loss: 0.6646
Training Epoch: 31 [28800/50048]	Loss: 0.7556
Training Epoch: 31 [28928/50048]	Loss: 0.7532
Training Epoch: 31 [29056/50048]	Loss: 0.7387
Training Epoch: 31 [29184/50048]	Loss: 0.6126
Training Epoch: 31 [29312/50048]	Loss: 0.6965
Training Epoch: 31 [29440/50048]	Loss: 0.6754
Training Epoch: 31 [29568/50048]	Loss: 0.6118
Training Epoch: 31 [29696/50048]	Loss: 0.7618
Training Epoch: 31 [29824/50048]	Loss: 0.4552
Training Epoch: 31 [29952/50048]	Loss: 0.5484
Training Epoch: 31 [30080/50048]	Loss: 0.5584
Training Epoch: 31 [30208/50048]	Loss: 0.5755
Training Epoch: 31 [30336/50048]	Loss: 0.6750
Training Epoch: 31 [30464/50048]	Loss: 0.7279
Training Epoch: 31 [30592/50048]	Loss: 0.6084
Training Epoch: 31 [30720/50048]	Loss: 0.6414
Training Epoch: 31 [30848/50048]	Loss: 0.5603
Training Epoch: 31 [30976/50048]	Loss: 0.5380
Training Epoch: 31 [31104/50048]	Loss: 0.7542
Training Epoch: 31 [31232/50048]	Loss: 0.6176
Training Epoch: 31 [31360/50048]	Loss: 0.7911
Training Epoch: 31 [31488/50048]	Loss: 0.6506
Training Epoch: 31 [31616/50048]	Loss: 0.7933
Training Epoch: 31 [31744/50048]	Loss: 0.6626
Training Epoch: 31 [31872/50048]	Loss: 0.5066
Training Epoch: 31 [32000/50048]	Loss: 0.6104
Training Epoch: 31 [32128/50048]	Loss: 0.6511
Training Epoch: 31 [32256/50048]	Loss: 1.0408
Training Epoch: 31 [32384/50048]	Loss: 0.7383
Training Epoch: 31 [32512/50048]	Loss: 0.6561
Training Epoch: 31 [32640/50048]	Loss: 0.5701
Training Epoch: 31 [32768/50048]	Loss: 0.8764
Training Epoch: 31 [32896/50048]	Loss: 0.5300
Training Epoch: 31 [33024/50048]	Loss: 0.5189
Training Epoch: 31 [33152/50048]	Loss: 0.5433
Training Epoch: 31 [33280/50048]	Loss: 0.7175
Training Epoch: 31 [33408/50048]	Loss: 0.5698
Training Epoch: 31 [33536/50048]	Loss: 0.5698
Training Epoch: 31 [33664/50048]	Loss: 0.7232
Training Epoch: 31 [33792/50048]	Loss: 0.7032
Training Epoch: 31 [33920/50048]	Loss: 0.6843
Training Epoch: 31 [34048/50048]	Loss: 0.6309
Training Epoch: 31 [34176/50048]	Loss: 0.7186
Training Epoch: 31 [34304/50048]	Loss: 0.6543
Training Epoch: 31 [34432/50048]	Loss: 0.8354
Training Epoch: 31 [34560/50048]	Loss: 0.6146
Training Epoch: 31 [34688/50048]	Loss: 0.6047
Training Epoch: 31 [34816/50048]	Loss: 0.6412
Training Epoch: 31 [34944/50048]	Loss: 0.5553
Training Epoch: 31 [35072/50048]	Loss: 0.5981
Training Epoch: 31 [35200/50048]	Loss: 0.5679
Training Epoch: 31 [35328/50048]	Loss: 0.5560
Training Epoch: 31 [35456/50048]	Loss: 0.7811
Training Epoch: 31 [35584/50048]	Loss: 0.7584
Training Epoch: 31 [35712/50048]	Loss: 0.6114
Training Epoch: 31 [35840/50048]	Loss: 0.5806
Training Epoch: 31 [35968/50048]	Loss: 0.8339
Training Epoch: 31 [36096/50048]	Loss: 0.6926
Training Epoch: 31 [36224/50048]	Loss: 0.6270
Training Epoch: 31 [36352/50048]	Loss: 0.7314
Training Epoch: 31 [36480/50048]	Loss: 0.8542
Training Epoch: 31 [36608/50048]	Loss: 0.6029
Training Epoch: 31 [36736/50048]	Loss: 0.7796
Training Epoch: 31 [36864/50048]	Loss: 0.7028
Training Epoch: 31 [36992/50048]	Loss: 0.6444
Training Epoch: 31 [37120/50048]	Loss: 0.8552
Training Epoch: 31 [37248/50048]	Loss: 0.7671
Training Epoch: 31 [37376/50048]	Loss: 0.6015
Training Epoch: 31 [37504/50048]	Loss: 0.6694
Training Epoch: 31 [37632/50048]	Loss: 0.5853
Training Epoch: 31 [37760/50048]	Loss: 0.9086
Training Epoch: 31 [37888/50048]	Loss: 0.7864
Training Epoch: 31 [38016/50048]	Loss: 0.5837
Training Epoch: 31 [38144/50048]	Loss: 0.5260
Training Epoch: 31 [38272/50048]	Loss: 0.7889
Training Epoch: 31 [38400/50048]	Loss: 0.6413
Training Epoch: 31 [38528/50048]	Loss: 0.5378
Training Epoch: 31 [38656/50048]	Loss: 0.6028
Training Epoch: 31 [38784/50048]	Loss: 0.7629
Training Epoch: 31 [38912/50048]	Loss: 0.8059
Training Epoch: 31 [39040/50048]	Loss: 0.6978
Training Epoch: 31 [39168/50048]	Loss: 0.7000
Training Epoch: 31 [39296/50048]	Loss: 0.6668
Training Epoch: 31 [39424/50048]	Loss: 0.8860
Training Epoch: 31 [39552/50048]	Loss: 0.5682
Training Epoch: 31 [39680/50048]	Loss: 0.6550
Training Epoch: 31 [39808/50048]	Loss: 0.9313
Training Epoch: 31 [39936/50048]	Loss: 0.6656
Training Epoch: 31 [40064/50048]	Loss: 0.9206
Training Epoch: 31 [40192/50048]	Loss: 0.4918
Training Epoch: 31 [40320/50048]	Loss: 0.6555
Training Epoch: 31 [40448/50048]	Loss: 0.6807
Training Epoch: 31 [40576/50048]	Loss: 0.5682
Training Epoch: 31 [40704/50048]	Loss: 0.6012
Training Epoch: 31 [40832/50048]	Loss: 0.7094
Training Epoch: 31 [40960/50048]	Loss: 0.6065
Training Epoch: 31 [41088/50048]	Loss: 0.5613
Training Epoch: 31 [41216/50048]	Loss: 0.4650
Training Epoch: 31 [41344/50048]	Loss: 0.6084
Training Epoch: 31 [41472/50048]	Loss: 0.6886
Training Epoch: 31 [41600/50048]	Loss: 0.7035
Training Epoch: 31 [41728/50048]	Loss: 0.5888
Training Epoch: 31 [41856/50048]	Loss: 0.5952
Training Epoch: 31 [41984/50048]	Loss: 0.5815
Training Epoch: 31 [42112/50048]	Loss: 0.5936
Training Epoch: 31 [42240/50048]	Loss: 0.4307
Training Epoch: 31 [42368/50048]	Loss: 0.5645
Training Epoch: 31 [42496/50048]	Loss: 0.6414
Training Epoch: 31 [42624/50048]	Loss: 0.6188
Training Epoch: 31 [42752/50048]	Loss: 0.6105
Training Epoch: 31 [42880/50048]	Loss: 0.4507
Training Epoch: 31 [43008/50048]	Loss: 0.5683
Training Epoch: 31 [43136/50048]	Loss: 0.5501
Training Epoch: 31 [43264/50048]	Loss: 0.5831
Training Epoch: 31 [43392/50048]	Loss: 0.6491
Training Epoch: 31 [43520/50048]	Loss: 0.6523
Training Epoch: 31 [43648/50048]	Loss: 0.6420
Training Epoch: 31 [43776/50048]	Loss: 0.6820
Training Epoch: 31 [43904/50048]	Loss: 0.5886
Training Epoch: 31 [44032/50048]	Loss: 0.6973
Training Epoch: 31 [44160/50048]	Loss: 0.8102
Training Epoch: 31 [44288/50048]	Loss: 0.9814
Training Epoch: 31 [44416/50048]	Loss: 0.8624
Training Epoch: 31 [44544/50048]	Loss: 0.7731
Training Epoch: 31 [44672/50048]	Loss: 0.5468
Training Epoch: 31 [44800/50048]	Loss: 0.6118
Training Epoch: 31 [44928/50048]	Loss: 0.9137
Training Epoch: 31 [45056/50048]	Loss: 0.8353
Training Epoch: 31 [45184/50048]	Loss: 0.7369
Training Epoch: 31 [45312/50048]	Loss: 0.7567
Training Epoch: 31 [45440/50048]	Loss: 0.9614
Training Epoch: 31 [45568/50048]	Loss: 0.6551
Training Epoch: 31 [45696/50048]	Loss: 0.4845
2022-12-06 06:55:56,728 [ZeusDataLoader(train)] train epoch 32 done: time=86.60 energy=10508.84
2022-12-06 06:55:56,729 [ZeusDataLoader(eval)] Epoch 32 begin.
Training Epoch: 31 [45824/50048]	Loss: 0.7346
Training Epoch: 31 [45952/50048]	Loss: 0.8710
Training Epoch: 31 [46080/50048]	Loss: 0.7924
Training Epoch: 31 [46208/50048]	Loss: 0.6915
Training Epoch: 31 [46336/50048]	Loss: 0.7090
Training Epoch: 31 [46464/50048]	Loss: 0.6985
Training Epoch: 31 [46592/50048]	Loss: 0.5972
Training Epoch: 31 [46720/50048]	Loss: 0.6391
Training Epoch: 31 [46848/50048]	Loss: 0.6787
Training Epoch: 31 [46976/50048]	Loss: 0.6472
Training Epoch: 31 [47104/50048]	Loss: 0.6423
Training Epoch: 31 [47232/50048]	Loss: 0.8716
Training Epoch: 31 [47360/50048]	Loss: 0.9286
Training Epoch: 31 [47488/50048]	Loss: 0.7839
Training Epoch: 31 [47616/50048]	Loss: 0.5222
Training Epoch: 31 [47744/50048]	Loss: 0.7039
Training Epoch: 31 [47872/50048]	Loss: 0.5971
Training Epoch: 31 [48000/50048]	Loss: 0.5990
Training Epoch: 31 [48128/50048]	Loss: 0.6236
Training Epoch: 31 [48256/50048]	Loss: 0.7140
Training Epoch: 31 [48384/50048]	Loss: 0.7315
Training Epoch: 31 [48512/50048]	Loss: 0.6965
Training Epoch: 31 [48640/50048]	Loss: 0.7321
Training Epoch: 31 [48768/50048]	Loss: 0.6886
Training Epoch: 31 [48896/50048]	Loss: 0.6321
Training Epoch: 31 [49024/50048]	Loss: 0.6903
Training Epoch: 31 [49152/50048]	Loss: 0.6767
Training Epoch: 31 [49280/50048]	Loss: 0.6578
Training Epoch: 31 [49408/50048]	Loss: 0.5412
Training Epoch: 31 [49536/50048]	Loss: 0.7031
Training Epoch: 31 [49664/50048]	Loss: 0.6524
Training Epoch: 31 [49792/50048]	Loss: 0.8023
Training Epoch: 31 [49920/50048]	Loss: 0.6066
Training Epoch: 31 [50048/50048]	Loss: 0.6931
2022-12-06 11:56:00.458 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:56:00,507 [ZeusDataLoader(eval)] eval epoch 32 done: time=3.77 energy=453.43
2022-12-06 06:56:00,508 [ZeusDataLoader(train)] Up to epoch 32: time=2886.19, energy=350379.91, cost=427731.97
2022-12-06 06:56:00,508 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:56:00,508 [ZeusDataLoader(train)] Expected next epoch: time=2975.99, energy=361177.92, cost=440988.36
2022-12-06 06:56:00,509 [ZeusDataLoader(train)] Epoch 33 begin.
Validation Epoch: 31, Average loss: 0.0123, Accuracy: 0.6208
2022-12-06 06:56:00,704 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:56:00,705 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:56:00.707 [ZeusMonitor] Monitor started.
2022-12-06 11:56:00.707 [ZeusMonitor] Running indefinitely. 2022-12-06 11:56:00.707 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:56:00.707 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e33+gpu0.power.log
Training Epoch: 32 [128/50048]	Loss: 0.5745
Training Epoch: 32 [256/50048]	Loss: 0.4997
Training Epoch: 32 [384/50048]	Loss: 0.6196
Training Epoch: 32 [512/50048]	Loss: 0.7820
Training Epoch: 32 [640/50048]	Loss: 0.4458
Training Epoch: 32 [768/50048]	Loss: 0.4984
Training Epoch: 32 [896/50048]	Loss: 0.4267
Training Epoch: 32 [1024/50048]	Loss: 0.5489
Training Epoch: 32 [1152/50048]	Loss: 0.4375
Training Epoch: 32 [1280/50048]	Loss: 0.5054
Training Epoch: 32 [1408/50048]	Loss: 0.4961
Training Epoch: 32 [1536/50048]	Loss: 0.5444
Training Epoch: 32 [1664/50048]	Loss: 0.5221
Training Epoch: 32 [1792/50048]	Loss: 0.5791
Training Epoch: 32 [1920/50048]	Loss: 0.6550
Training Epoch: 32 [2048/50048]	Loss: 0.6099
Training Epoch: 32 [2176/50048]	Loss: 0.6849
Training Epoch: 32 [2304/50048]	Loss: 0.7034
Training Epoch: 32 [2432/50048]	Loss: 0.6190
Training Epoch: 32 [2560/50048]	Loss: 0.5844
Training Epoch: 32 [2688/50048]	Loss: 0.6196
Training Epoch: 32 [2816/50048]	Loss: 0.6311
Training Epoch: 32 [2944/50048]	Loss: 0.5791
Training Epoch: 32 [3072/50048]	Loss: 0.6305
Training Epoch: 32 [3200/50048]	Loss: 0.7255
Training Epoch: 32 [3328/50048]	Loss: 0.7469
Training Epoch: 32 [3456/50048]	Loss: 0.3921
Training Epoch: 32 [3584/50048]	Loss: 0.5956
Training Epoch: 32 [3712/50048]	Loss: 0.5304
Training Epoch: 32 [3840/50048]	Loss: 0.7016
Training Epoch: 32 [3968/50048]	Loss: 0.5765
Training Epoch: 32 [4096/50048]	Loss: 0.5363
Training Epoch: 32 [4224/50048]	Loss: 0.5466
Training Epoch: 32 [4352/50048]	Loss: 0.5176
Training Epoch: 32 [4480/50048]	Loss: 0.5445
Training Epoch: 32 [4608/50048]	Loss: 0.6414
Training Epoch: 32 [4736/50048]	Loss: 0.5388
Training Epoch: 32 [4864/50048]	Loss: 0.5490
Training Epoch: 32 [4992/50048]	Loss: 0.7928
Training Epoch: 32 [5120/50048]	Loss: 0.6757
Training Epoch: 32 [5248/50048]	Loss: 0.6066
Training Epoch: 32 [5376/50048]	Loss: 0.5370
Training Epoch: 32 [5504/50048]	Loss: 0.6490
Training Epoch: 32 [5632/50048]	Loss: 0.5714
Training Epoch: 32 [5760/50048]	Loss: 0.7621
Training Epoch: 32 [5888/50048]	Loss: 0.4220
Training Epoch: 32 [6016/50048]	Loss: 0.3908
Training Epoch: 32 [6144/50048]	Loss: 0.5316
Training Epoch: 32 [6272/50048]	Loss: 0.6496
Training Epoch: 32 [6400/50048]	Loss: 0.5893
Training Epoch: 32 [6528/50048]	Loss: 0.6298
Training Epoch: 32 [6656/50048]	Loss: 0.5814
Training Epoch: 32 [6784/50048]	Loss: 0.5693
Training Epoch: 32 [6912/50048]	Loss: 0.5541
Training Epoch: 32 [7040/50048]	Loss: 0.7388
Training Epoch: 32 [7168/50048]	Loss: 0.4256
Training Epoch: 32 [7296/50048]	Loss: 0.5324
Training Epoch: 32 [7424/50048]	Loss: 0.6027
Training Epoch: 32 [7552/50048]	Loss: 0.5853
Training Epoch: 32 [7680/50048]	Loss: 0.5822
Training Epoch: 32 [7808/50048]	Loss: 0.5256
Training Epoch: 32 [7936/50048]	Loss: 0.6994
Training Epoch: 32 [8064/50048]	Loss: 0.5653
Training Epoch: 32 [8192/50048]	Loss: 0.6747
Training Epoch: 32 [8320/50048]	Loss: 0.5711
Training Epoch: 32 [8448/50048]	Loss: 0.5789
Training Epoch: 32 [8576/50048]	Loss: 0.5305
Training Epoch: 32 [8704/50048]	Loss: 0.4328
Training Epoch: 32 [8832/50048]	Loss: 0.5320
Training Epoch: 32 [8960/50048]	Loss: 0.5682
Training Epoch: 32 [9088/50048]	Loss: 0.5879
Training Epoch: 32 [9216/50048]	Loss: 0.6235
Training Epoch: 32 [9344/50048]	Loss: 0.5439
Training Epoch: 32 [9472/50048]	Loss: 0.4187
Training Epoch: 32 [9600/50048]	Loss: 0.6107
Training Epoch: 32 [9728/50048]	Loss: 0.5616
Training Epoch: 32 [9856/50048]	Loss: 0.5787
Training Epoch: 32 [9984/50048]	Loss: 0.5612
Training Epoch: 32 [10112/50048]	Loss: 0.4126
Training Epoch: 32 [10240/50048]	Loss: 0.6171
Training Epoch: 32 [10368/50048]	Loss: 0.5992
Training Epoch: 32 [10496/50048]	Loss: 0.5766
Training Epoch: 32 [10624/50048]	Loss: 0.3472
Training Epoch: 32 [10752/50048]	Loss: 0.5210
Training Epoch: 32 [10880/50048]	Loss: 0.5683
Training Epoch: 32 [11008/50048]	Loss: 0.5946
Training Epoch: 32 [11136/50048]	Loss: 0.7502
Training Epoch: 32 [11264/50048]	Loss: 0.5520
Training Epoch: 32 [11392/50048]	Loss: 0.5134
Training Epoch: 32 [11520/50048]	Loss: 0.6498
Training Epoch: 32 [11648/50048]	Loss: 0.5545
Training Epoch: 32 [11776/50048]	Loss: 0.7247
Training Epoch: 32 [11904/50048]	Loss: 0.6140
Training Epoch: 32 [12032/50048]	Loss: 0.6463
Training Epoch: 32 [12160/50048]	Loss: 0.4579
Training Epoch: 32 [12288/50048]	Loss: 0.6470
Training Epoch: 32 [12416/50048]	Loss: 0.6641
Training Epoch: 32 [12544/50048]	Loss: 0.5559
Training Epoch: 32 [12672/50048]	Loss: 0.4987
Training Epoch: 32 [12800/50048]	Loss: 0.6881
Training Epoch: 32 [12928/50048]	Loss: 0.6029
Training Epoch: 32 [13056/50048]	Loss: 0.6868
Training Epoch: 32 [13184/50048]	Loss: 0.4821
Training Epoch: 32 [13312/50048]	Loss: 0.5684
Training Epoch: 32 [13440/50048]	Loss: 0.7304
Training Epoch: 32 [13568/50048]	Loss: 0.7334
Training Epoch: 32 [13696/50048]	Loss: 0.7351
Training Epoch: 32 [13824/50048]	Loss: 0.5687
Training Epoch: 32 [13952/50048]	Loss: 0.5118
Training Epoch: 32 [14080/50048]	Loss: 0.5531
Training Epoch: 32 [14208/50048]	Loss: 0.4755
Training Epoch: 32 [14336/50048]	Loss: 0.5915
Training Epoch: 32 [14464/50048]	Loss: 0.7289
Training Epoch: 32 [14592/50048]	Loss: 0.4661
Training Epoch: 32 [14720/50048]	Loss: 0.4982
Training Epoch: 32 [14848/50048]	Loss: 0.5958
Training Epoch: 32 [14976/50048]	Loss: 0.5617
Training Epoch: 32 [15104/50048]	Loss: 0.4923
Training Epoch: 32 [15232/50048]	Loss: 0.4592
Training Epoch: 32 [15360/50048]	Loss: 0.4195
Training Epoch: 32 [15488/50048]	Loss: 0.6623
Training Epoch: 32 [15616/50048]	Loss: 0.5777
Training Epoch: 32 [15744/50048]	Loss: 0.7243
Training Epoch: 32 [15872/50048]	Loss: 0.4829
Training Epoch: 32 [16000/50048]	Loss: 0.5074
Training Epoch: 32 [16128/50048]	Loss: 0.4524
Training Epoch: 32 [16256/50048]	Loss: 0.5313
Training Epoch: 32 [16384/50048]	Loss: 0.4758
Training Epoch: 32 [16512/50048]	Loss: 0.6476
Training Epoch: 32 [16640/50048]	Loss: 0.5670
Training Epoch: 32 [16768/50048]	Loss: 0.4740
Training Epoch: 32 [16896/50048]	Loss: 0.6536
Training Epoch: 32 [17024/50048]	Loss: 0.6035
Training Epoch: 32 [17152/50048]	Loss: 0.6986
Training Epoch: 32 [17280/50048]	Loss: 0.5215
Training Epoch: 32 [17408/50048]	Loss: 0.6791
Training Epoch: 32 [17536/50048]	Loss: 0.7021
Training Epoch: 32 [17664/50048]	Loss: 0.4606
Training Epoch: 32 [17792/50048]	Loss: 0.5626
Training Epoch: 32 [17920/50048]	Loss: 0.4795
Training Epoch: 32 [18048/50048]	Loss: 0.6507
Training Epoch: 32 [18176/50048]	Loss: 0.7155
Training Epoch: 32 [18304/50048]	Loss: 0.6705
Training Epoch: 32 [18432/50048]	Loss: 0.4291
Training Epoch: 32 [18560/50048]	Loss: 0.6701
Training Epoch: 32 [18688/50048]	Loss: 0.4662
Training Epoch: 32 [18816/50048]	Loss: 0.6588
Training Epoch: 32 [18944/50048]	Loss: 0.5088
Training Epoch: 32 [19072/50048]	Loss: 0.6580
Training Epoch: 32 [19200/50048]	Loss: 0.5062
Training Epoch: 32 [19328/50048]	Loss: 0.5778
Training Epoch: 32 [19456/50048]	Loss: 0.5731
Training Epoch: 32 [19584/50048]	Loss: 0.6968
Training Epoch: 32 [19712/50048]	Loss: 0.6761
Training Epoch: 32 [19840/50048]	Loss: 0.4959
Training Epoch: 32 [19968/50048]	Loss: 0.7233
Training Epoch: 32 [20096/50048]	Loss: 0.5452
Training Epoch: 32 [20224/50048]	Loss: 0.4652
Training Epoch: 32 [20352/50048]	Loss: 0.6116
Training Epoch: 32 [20480/50048]	Loss: 0.4806
Training Epoch: 32 [20608/50048]	Loss: 0.4566
Training Epoch: 32 [20736/50048]	Loss: 0.5648
Training Epoch: 32 [20864/50048]	Loss: 0.6278
Training Epoch: 32 [20992/50048]	Loss: 0.7050
Training Epoch: 32 [21120/50048]	Loss: 0.7130
Training Epoch: 32 [21248/50048]	Loss: 0.7188
Training Epoch: 32 [21376/50048]	Loss: 0.6221
Training Epoch: 32 [21504/50048]	Loss: 0.7004
Training Epoch: 32 [21632/50048]	Loss: 0.6923
Training Epoch: 32 [21760/50048]	Loss: 0.6440
Training Epoch: 32 [21888/50048]	Loss: 0.6745
Training Epoch: 32 [22016/50048]	Loss: 0.5479
Training Epoch: 32 [22144/50048]	Loss: 0.6343
Training Epoch: 32 [22272/50048]	Loss: 0.7357
Training Epoch: 32 [22400/50048]	Loss: 0.7864
Training Epoch: 32 [22528/50048]	Loss: 0.5682
Training Epoch: 32 [22656/50048]	Loss: 0.5642
Training Epoch: 32 [22784/50048]	Loss: 0.5687
Training Epoch: 32 [22912/50048]	Loss: 0.7968
Training Epoch: 32 [23040/50048]	Loss: 0.4332
Training Epoch: 32 [23168/50048]	Loss: 0.6114
Training Epoch: 32 [23296/50048]	Loss: 0.6757
Training Epoch: 32 [23424/50048]	Loss: 0.6114
Training Epoch: 32 [23552/50048]	Loss: 0.7706
Training Epoch: 32 [23680/50048]	Loss: 0.5505
Training Epoch: 32 [23808/50048]	Loss: 0.5728
Training Epoch: 32 [23936/50048]	Loss: 0.4919
Training Epoch: 32 [24064/50048]	Loss: 0.6036
Training Epoch: 32 [24192/50048]	Loss: 0.8148
Training Epoch: 32 [24320/50048]	Loss: 0.5881
Training Epoch: 32 [24448/50048]	Loss: 0.4915
Training Epoch: 32 [24576/50048]	Loss: 0.5429
Training Epoch: 32 [24704/50048]	Loss: 0.5463
Training Epoch: 32 [24832/50048]	Loss: 0.5684
Training Epoch: 32 [24960/50048]	Loss: 0.6859
Training Epoch: 32 [25088/50048]	Loss: 0.6929
Training Epoch: 32 [25216/50048]	Loss: 0.6876
Training Epoch: 32 [25344/50048]	Loss: 0.5668
Training Epoch: 32 [25472/50048]	Loss: 0.6992
Training Epoch: 32 [25600/50048]	Loss: 0.5944
Training Epoch: 32 [25728/50048]	Loss: 0.5257
Training Epoch: 32 [25856/50048]	Loss: 0.5785
Training Epoch: 32 [25984/50048]	Loss: 0.8350
Training Epoch: 32 [26112/50048]	Loss: 0.4867
Training Epoch: 32 [26240/50048]	Loss: 0.6733
Training Epoch: 32 [26368/50048]	Loss: 0.6039
Training Epoch: 32 [26496/50048]	Loss: 0.6528
Training Epoch: 32 [26624/50048]	Loss: 0.5258
Training Epoch: 32 [26752/50048]	Loss: 0.5933
Training Epoch: 32 [26880/50048]	Loss: 0.5868
Training Epoch: 32 [27008/50048]	Loss: 0.5342
Training Epoch: 32 [27136/50048]	Loss: 0.4569
Training Epoch: 32 [27264/50048]	Loss: 0.6857
Training Epoch: 32 [27392/50048]	Loss: 0.6258
Training Epoch: 32 [27520/50048]	Loss: 0.5982
Training Epoch: 32 [27648/50048]	Loss: 0.6726
Training Epoch: 32 [27776/50048]	Loss: 0.7569
Training Epoch: 32 [27904/50048]	Loss: 0.6093
Training Epoch: 32 [28032/50048]	Loss: 0.4648
Training Epoch: 32 [28160/50048]	Loss: 0.4960
Training Epoch: 32 [28288/50048]	Loss: 0.7290
Training Epoch: 32 [28416/50048]	Loss: 0.5456
Training Epoch: 32 [28544/50048]	Loss: 0.6198
Training Epoch: 32 [28672/50048]	Loss: 0.6001
Training Epoch: 32 [28800/50048]	Loss: 0.5626
Training Epoch: 32 [28928/50048]	Loss: 0.4992
Training Epoch: 32 [29056/50048]	Loss: 0.5618
Training Epoch: 32 [29184/50048]	Loss: 0.7400
Training Epoch: 32 [29312/50048]	Loss: 0.4019
Training Epoch: 32 [29440/50048]	Loss: 0.6336
Training Epoch: 32 [29568/50048]	Loss: 0.6350
Training Epoch: 32 [29696/50048]	Loss: 0.6958
Training Epoch: 32 [29824/50048]	Loss: 0.6512
Training Epoch: 32 [29952/50048]	Loss: 0.6957
Training Epoch: 32 [30080/50048]	Loss: 0.7013
Training Epoch: 32 [30208/50048]	Loss: 0.5243
Training Epoch: 32 [30336/50048]	Loss: 0.7254
Training Epoch: 32 [30464/50048]	Loss: 0.5972
Training Epoch: 32 [30592/50048]	Loss: 0.4991
Training Epoch: 32 [30720/50048]	Loss: 0.4954
Training Epoch: 32 [30848/50048]	Loss: 0.6347
Training Epoch: 32 [30976/50048]	Loss: 0.6185
Training Epoch: 32 [31104/50048]	Loss: 0.6280
Training Epoch: 32 [31232/50048]	Loss: 0.7611
Training Epoch: 32 [31360/50048]	Loss: 0.7764
Training Epoch: 32 [31488/50048]	Loss: 0.6245
Training Epoch: 32 [31616/50048]	Loss: 0.7387
Training Epoch: 32 [31744/50048]	Loss: 0.7188
Training Epoch: 32 [31872/50048]	Loss: 0.5856
Training Epoch: 32 [32000/50048]	Loss: 0.5988
Training Epoch: 32 [32128/50048]	Loss: 0.7434
Training Epoch: 32 [32256/50048]	Loss: 0.7513
Training Epoch: 32 [32384/50048]	Loss: 0.4787
Training Epoch: 32 [32512/50048]	Loss: 0.5636
Training Epoch: 32 [32640/50048]	Loss: 0.5835
Training Epoch: 32 [32768/50048]	Loss: 0.7453
Training Epoch: 32 [32896/50048]	Loss: 0.7235
Training Epoch: 32 [33024/50048]	Loss: 0.7052
Training Epoch: 32 [33152/50048]	Loss: 0.6079
Training Epoch: 32 [33280/50048]	Loss: 0.7011
Training Epoch: 32 [33408/50048]	Loss: 0.4873
Training Epoch: 32 [33536/50048]	Loss: 0.5629
Training Epoch: 32 [33664/50048]	Loss: 0.7431
Training Epoch: 32 [33792/50048]	Loss: 0.5045
Training Epoch: 32 [33920/50048]	Loss: 0.7138
Training Epoch: 32 [34048/50048]	Loss: 0.6823
Training Epoch: 32 [34176/50048]	Loss: 0.7817
Training Epoch: 32 [34304/50048]	Loss: 0.5487
Training Epoch: 32 [34432/50048]	Loss: 0.6004
Training Epoch: 32 [34560/50048]	Loss: 0.6296
Training Epoch: 32 [34688/50048]	Loss: 0.6505
Training Epoch: 32 [34816/50048]	Loss: 0.6336
Training Epoch: 32 [34944/50048]	Loss: 0.7859
Training Epoch: 32 [35072/50048]	Loss: 0.5422
Training Epoch: 32 [35200/50048]	Loss: 0.6617
Training Epoch: 32 [35328/50048]	Loss: 0.6812
Training Epoch: 32 [35456/50048]	Loss: 0.5001
Training Epoch: 32 [35584/50048]	Loss: 0.7260
Training Epoch: 32 [35712/50048]	Loss: 0.7248
Training Epoch: 32 [35840/50048]	Loss: 0.5023
Training Epoch: 32 [35968/50048]	Loss: 0.6995
Training Epoch: 32 [36096/50048]	Loss: 0.7305
Training Epoch: 32 [36224/50048]	Loss: 0.6011
Training Epoch: 32 [36352/50048]	Loss: 0.7705
Training Epoch: 32 [36480/50048]	Loss: 0.8120
Training Epoch: 32 [36608/50048]	Loss: 0.5617
Training Epoch: 32 [36736/50048]	Loss: 0.5409
Training Epoch: 32 [36864/50048]	Loss: 0.5922
Training Epoch: 32 [36992/50048]	Loss: 0.7450
Training Epoch: 32 [37120/50048]	Loss: 0.6067
Training Epoch: 32 [37248/50048]	Loss: 0.7133
Training Epoch: 32 [37376/50048]	Loss: 0.5555
Training Epoch: 32 [37504/50048]	Loss: 0.5763
Training Epoch: 32 [37632/50048]	Loss: 0.6063
Training Epoch: 32 [37760/50048]	Loss: 0.6093
Training Epoch: 32 [37888/50048]	Loss: 0.5556
Training Epoch: 32 [38016/50048]	Loss: 0.5634
Training Epoch: 32 [38144/50048]	Loss: 0.6186
Training Epoch: 32 [38272/50048]	Loss: 0.4510
Training Epoch: 32 [38400/50048]	Loss: 0.5832
Training Epoch: 32 [38528/50048]	Loss: 0.6201
Training Epoch: 32 [38656/50048]	Loss: 0.7087
Training Epoch: 32 [38784/50048]	Loss: 0.6672
Training Epoch: 32 [38912/50048]	Loss: 0.7904
Training Epoch: 32 [39040/50048]	Loss: 0.9674
Training Epoch: 32 [39168/50048]	Loss: 0.5203
Training Epoch: 32 [39296/50048]	Loss: 0.6719
Training Epoch: 32 [39424/50048]	Loss: 0.6091
Training Epoch: 32 [39552/50048]	Loss: 0.5150
Training Epoch: 32 [39680/50048]	Loss: 0.7480
Training Epoch: 32 [39808/50048]	Loss: 0.6144
Training Epoch: 32 [39936/50048]	Loss: 0.7168
Training Epoch: 32 [40064/50048]	Loss: 0.5999
Training Epoch: 32 [40192/50048]	Loss: 0.6446
Training Epoch: 32 [40320/50048]	Loss: 0.6150
Training Epoch: 32 [40448/50048]	Loss: 0.4709
Training Epoch: 32 [40576/50048]	Loss: 0.5517
Training Epoch: 32 [40704/50048]	Loss: 0.6881
Training Epoch: 32 [40832/50048]	Loss: 0.8357
Training Epoch: 32 [40960/50048]	Loss: 0.6366
Training Epoch: 32 [41088/50048]	Loss: 0.5422
Training Epoch: 32 [41216/50048]	Loss: 0.8132
Training Epoch: 32 [41344/50048]	Loss: 0.5961
Training Epoch: 32 [41472/50048]	Loss: 0.6405
Training Epoch: 32 [41600/50048]	Loss: 0.6210
Training Epoch: 32 [41728/50048]	Loss: 0.6165
Training Epoch: 32 [41856/50048]	Loss: 0.6122
Training Epoch: 32 [41984/50048]	Loss: 0.7387
Training Epoch: 32 [42112/50048]	Loss: 0.5680
Training Epoch: 32 [42240/50048]	Loss: 0.7277
Training Epoch: 32 [42368/50048]	Loss: 0.5368
Training Epoch: 32 [42496/50048]	Loss: 0.5905
Training Epoch: 32 [42624/50048]	Loss: 0.8436
Training Epoch: 32 [42752/50048]	Loss: 0.5922
Training Epoch: 32 [42880/50048]	Loss: 0.5925
Training Epoch: 32 [43008/50048]	Loss: 0.7004
Training Epoch: 32 [43136/50048]	Loss: 0.7642
Training Epoch: 32 [43264/50048]	Loss: 0.6102
Training Epoch: 32 [43392/50048]	Loss: 0.5220
Training Epoch: 32 [43520/50048]	Loss: 0.5311
Training Epoch: 32 [43648/50048]	Loss: 0.7088
Training Epoch: 32 [43776/50048]	Loss: 0.6600
Training Epoch: 32 [43904/50048]	Loss: 0.7260
Training Epoch: 32 [44032/50048]	Loss: 0.6156
Training Epoch: 32 [44160/50048]	Loss: 0.5802
Training Epoch: 32 [44288/50048]	Loss: 0.5519
Training Epoch: 32 [44416/50048]	Loss: 0.6239
Training Epoch: 32 [44544/50048]	Loss: 0.7133
Training Epoch: 32 [44672/50048]	Loss: 0.6139
Training Epoch: 32 [44800/50048]	Loss: 0.6614
Training Epoch: 32 [44928/50048]	Loss: 0.4220
Training Epoch: 32 [45056/50048]	Loss: 0.5735
Training Epoch: 32 [45184/50048]	Loss: 0.6118
Training Epoch: 32 [45312/50048]	Loss: 0.5631
Training Epoch: 32 [45440/50048]	Loss: 0.6695
Training Epoch: 32 [45568/50048]	Loss: 0.7564
Training Epoch: 32 [45696/50048]	Loss: 0.6035
2022-12-06 06:57:26,976 [ZeusDataLoader(train)] train epoch 33 done: time=86.46 energy=10499.18
2022-12-06 06:57:26,978 [ZeusDataLoader(eval)] Epoch 33 begin.
Training Epoch: 32 [45824/50048]	Loss: 0.7817
Training Epoch: 32 [45952/50048]	Loss: 0.6795
Training Epoch: 32 [46080/50048]	Loss: 0.6900
Training Epoch: 32 [46208/50048]	Loss: 0.7682
Training Epoch: 32 [46336/50048]	Loss: 0.5278
Training Epoch: 32 [46464/50048]	Loss: 0.7434
Training Epoch: 32 [46592/50048]	Loss: 0.5856
Training Epoch: 32 [46720/50048]	Loss: 0.5697
Training Epoch: 32 [46848/50048]	Loss: 0.5070
Training Epoch: 32 [46976/50048]	Loss: 0.6015
Training Epoch: 32 [47104/50048]	Loss: 0.5447
Training Epoch: 32 [47232/50048]	Loss: 0.6271
Training Epoch: 32 [47360/50048]	Loss: 0.4696
Training Epoch: 32 [47488/50048]	Loss: 0.6782
Training Epoch: 32 [47616/50048]	Loss: 0.5224
Training Epoch: 32 [47744/50048]	Loss: 0.8355
Training Epoch: 32 [47872/50048]	Loss: 0.7521
Training Epoch: 32 [48000/50048]	Loss: 0.5958
Training Epoch: 32 [48128/50048]	Loss: 0.7107
Training Epoch: 32 [48256/50048]	Loss: 0.6402
Training Epoch: 32 [48384/50048]	Loss: 0.6322
Training Epoch: 32 [48512/50048]	Loss: 0.4856
Training Epoch: 32 [48640/50048]	Loss: 0.6914
Training Epoch: 32 [48768/50048]	Loss: 0.8230
Training Epoch: 32 [48896/50048]	Loss: 0.6911
Training Epoch: 32 [49024/50048]	Loss: 0.7303
Training Epoch: 32 [49152/50048]	Loss: 0.5924
Training Epoch: 32 [49280/50048]	Loss: 0.5217
Training Epoch: 32 [49408/50048]	Loss: 0.8469
Training Epoch: 32 [49536/50048]	Loss: 0.6446
Training Epoch: 32 [49664/50048]	Loss: 0.6015
Training Epoch: 32 [49792/50048]	Loss: 0.7226
Training Epoch: 32 [49920/50048]	Loss: 0.5994
Training Epoch: 32 [50048/50048]	Loss: 0.7676
2022-12-06 11:57:30.619 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:57:30,633 [ZeusDataLoader(eval)] eval epoch 33 done: time=3.65 energy=441.21
2022-12-06 06:57:30,633 [ZeusDataLoader(train)] Up to epoch 33: time=2976.30, energy=361320.30, cost=441086.21
2022-12-06 06:57:30,633 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:57:30,633 [ZeusDataLoader(train)] Expected next epoch: time=3066.10, energy=372118.31, cost=454342.59
2022-12-06 06:57:30,634 [ZeusDataLoader(train)] Epoch 34 begin.
Validation Epoch: 32, Average loss: 0.0126, Accuracy: 0.6194
2022-12-06 06:57:30,830 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:57:30,831 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:57:30.833 [ZeusMonitor] Monitor started.
2022-12-06 11:57:30.833 [ZeusMonitor] Running indefinitely. 2022-12-06 11:57:30.833 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:57:30.833 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e34+gpu0.power.log
Training Epoch: 33 [128/50048]	Loss: 0.6283
Training Epoch: 33 [256/50048]	Loss: 0.5291
Training Epoch: 33 [384/50048]	Loss: 0.5232
Training Epoch: 33 [512/50048]	Loss: 0.4840
Training Epoch: 33 [640/50048]	Loss: 0.6371
Training Epoch: 33 [768/50048]	Loss: 0.5725
Training Epoch: 33 [896/50048]	Loss: 0.5991
Training Epoch: 33 [1024/50048]	Loss: 0.4185
Training Epoch: 33 [1152/50048]	Loss: 0.5655
Training Epoch: 33 [1280/50048]	Loss: 0.4643
Training Epoch: 33 [1408/50048]	Loss: 0.5010
Training Epoch: 33 [1536/50048]	Loss: 0.5029
Training Epoch: 33 [1664/50048]	Loss: 0.6001
Training Epoch: 33 [1792/50048]	Loss: 0.6309
Training Epoch: 33 [1920/50048]	Loss: 0.4696
Training Epoch: 33 [2048/50048]	Loss: 0.5082
Training Epoch: 33 [2176/50048]	Loss: 0.5134
Training Epoch: 33 [2304/50048]	Loss: 0.5581
Training Epoch: 33 [2432/50048]	Loss: 0.4928
Training Epoch: 33 [2560/50048]	Loss: 0.5322
Training Epoch: 33 [2688/50048]	Loss: 0.6953
Training Epoch: 33 [2816/50048]	Loss: 0.5055
Training Epoch: 33 [2944/50048]	Loss: 0.5011
Training Epoch: 33 [3072/50048]	Loss: 0.4915
Training Epoch: 33 [3200/50048]	Loss: 0.4565
Training Epoch: 33 [3328/50048]	Loss: 0.4740
Training Epoch: 33 [3456/50048]	Loss: 0.5421
Training Epoch: 33 [3584/50048]	Loss: 0.6173
Training Epoch: 33 [3712/50048]	Loss: 0.6190
Training Epoch: 33 [3840/50048]	Loss: 0.5632
Training Epoch: 33 [3968/50048]	Loss: 0.5923
Training Epoch: 33 [4096/50048]	Loss: 0.6029
Training Epoch: 33 [4224/50048]	Loss: 0.5897
Training Epoch: 33 [4352/50048]	Loss: 0.5136
Training Epoch: 33 [4480/50048]	Loss: 0.4594
Training Epoch: 33 [4608/50048]	Loss: 0.5736
Training Epoch: 33 [4736/50048]	Loss: 0.7158
Training Epoch: 33 [4864/50048]	Loss: 0.4467
Training Epoch: 33 [4992/50048]	Loss: 0.6058
Training Epoch: 33 [5120/50048]	Loss: 0.5272
Training Epoch: 33 [5248/50048]	Loss: 0.5143
Training Epoch: 33 [5376/50048]	Loss: 0.5082
Training Epoch: 33 [5504/50048]	Loss: 0.4277
Training Epoch: 33 [5632/50048]	Loss: 0.4485
Training Epoch: 33 [5760/50048]	Loss: 0.3968
Training Epoch: 33 [5888/50048]	Loss: 0.4870
Training Epoch: 33 [6016/50048]	Loss: 0.6477
Training Epoch: 33 [6144/50048]	Loss: 0.5879
Training Epoch: 33 [6272/50048]	Loss: 0.6378
Training Epoch: 33 [6400/50048]	Loss: 0.6188
Training Epoch: 33 [6528/50048]	Loss: 0.5919
Training Epoch: 33 [6656/50048]	Loss: 0.5762
Training Epoch: 33 [6784/50048]	Loss: 0.5920
Training Epoch: 33 [6912/50048]	Loss: 0.5450
Training Epoch: 33 [7040/50048]	Loss: 0.5946
Training Epoch: 33 [7168/50048]	Loss: 0.5166
Training Epoch: 33 [7296/50048]	Loss: 0.5414
Training Epoch: 33 [7424/50048]	Loss: 0.5520
Training Epoch: 33 [7552/50048]	Loss: 0.5014
Training Epoch: 33 [7680/50048]	Loss: 0.4892
Training Epoch: 33 [7808/50048]	Loss: 0.4816
Training Epoch: 33 [7936/50048]	Loss: 0.5634
Training Epoch: 33 [8064/50048]	Loss: 0.4583
Training Epoch: 33 [8192/50048]	Loss: 0.6273
Training Epoch: 33 [8320/50048]	Loss: 0.3985
Training Epoch: 33 [8448/50048]	Loss: 0.4258
Training Epoch: 33 [8576/50048]	Loss: 0.5642
Training Epoch: 33 [8704/50048]	Loss: 0.6133
Training Epoch: 33 [8832/50048]	Loss: 0.4770
Training Epoch: 33 [8960/50048]	Loss: 0.5597
Training Epoch: 33 [9088/50048]	Loss: 0.6667
Training Epoch: 33 [9216/50048]	Loss: 0.6145
Training Epoch: 33 [9344/50048]	Loss: 0.5191
Training Epoch: 33 [9472/50048]	Loss: 0.6315
Training Epoch: 33 [9600/50048]	Loss: 0.7061
Training Epoch: 33 [9728/50048]	Loss: 0.6179
Training Epoch: 33 [9856/50048]	Loss: 0.6426
Training Epoch: 33 [9984/50048]	Loss: 0.5146
Training Epoch: 33 [10112/50048]	Loss: 0.4796
Training Epoch: 33 [10240/50048]	Loss: 0.5470
Training Epoch: 33 [10368/50048]	Loss: 0.6281
Training Epoch: 33 [10496/50048]	Loss: 0.5192
Training Epoch: 33 [10624/50048]	Loss: 0.5590
Training Epoch: 33 [10752/50048]	Loss: 0.6406
Training Epoch: 33 [10880/50048]	Loss: 0.5377
Training Epoch: 33 [11008/50048]	Loss: 0.4861
Training Epoch: 33 [11136/50048]	Loss: 0.6438
Training Epoch: 33 [11264/50048]	Loss: 0.5988
Training Epoch: 33 [11392/50048]	Loss: 0.6571
Training Epoch: 33 [11520/50048]	Loss: 0.5729
Training Epoch: 33 [11648/50048]	Loss: 0.6675
Training Epoch: 33 [11776/50048]	Loss: 0.4514
Training Epoch: 33 [11904/50048]	Loss: 0.5458
Training Epoch: 33 [12032/50048]	Loss: 0.4991
Training Epoch: 33 [12160/50048]	Loss: 0.6666
Training Epoch: 33 [12288/50048]	Loss: 0.5302
Training Epoch: 33 [12416/50048]	Loss: 0.4735
Training Epoch: 33 [12544/50048]	Loss: 0.6208
Training Epoch: 33 [12672/50048]	Loss: 0.4619
Training Epoch: 33 [12800/50048]	Loss: 0.5069
Training Epoch: 33 [12928/50048]	Loss: 0.5004
Training Epoch: 33 [13056/50048]	Loss: 0.7314
Training Epoch: 33 [13184/50048]	Loss: 0.6346
Training Epoch: 33 [13312/50048]	Loss: 0.5616
Training Epoch: 33 [13440/50048]	Loss: 0.4534
Training Epoch: 33 [13568/50048]	Loss: 0.5071
Training Epoch: 33 [13696/50048]	Loss: 0.4337
Training Epoch: 33 [13824/50048]	Loss: 0.6163
Training Epoch: 33 [13952/50048]	Loss: 0.6321
Training Epoch: 33 [14080/50048]	Loss: 0.5525
Training Epoch: 33 [14208/50048]	Loss: 0.6466
Training Epoch: 33 [14336/50048]	Loss: 0.6918
Training Epoch: 33 [14464/50048]	Loss: 0.5823
Training Epoch: 33 [14592/50048]	Loss: 0.4068
Training Epoch: 33 [14720/50048]	Loss: 0.5310
Training Epoch: 33 [14848/50048]	Loss: 0.6484
Training Epoch: 33 [14976/50048]	Loss: 0.6266
Training Epoch: 33 [15104/50048]	Loss: 0.5506
Training Epoch: 33 [15232/50048]	Loss: 0.4878
Training Epoch: 33 [15360/50048]	Loss: 0.5598
Training Epoch: 33 [15488/50048]	Loss: 0.4827
Training Epoch: 33 [15616/50048]	Loss: 0.6158
Training Epoch: 33 [15744/50048]	Loss: 0.6232
Training Epoch: 33 [15872/50048]	Loss: 0.7115
Training Epoch: 33 [16000/50048]	Loss: 0.5616
Training Epoch: 33 [16128/50048]	Loss: 0.6871
Training Epoch: 33 [16256/50048]	Loss: 0.4913
Training Epoch: 33 [16384/50048]	Loss: 0.5774
Training Epoch: 33 [16512/50048]	Loss: 0.6314
Training Epoch: 33 [16640/50048]	Loss: 0.6590
Training Epoch: 33 [16768/50048]	Loss: 0.5460
Training Epoch: 33 [16896/50048]	Loss: 0.5877
Training Epoch: 33 [17024/50048]	Loss: 0.3495
Training Epoch: 33 [17152/50048]	Loss: 0.3750
Training Epoch: 33 [17280/50048]	Loss: 0.5802
Training Epoch: 33 [17408/50048]	Loss: 0.4921
Training Epoch: 33 [17536/50048]	Loss: 0.5759
Training Epoch: 33 [17664/50048]	Loss: 0.4394
Training Epoch: 33 [17792/50048]	Loss: 0.4800
Training Epoch: 33 [17920/50048]	Loss: 0.6795
Training Epoch: 33 [18048/50048]	Loss: 0.6034
Training Epoch: 33 [18176/50048]	Loss: 0.5384
Training Epoch: 33 [18304/50048]	Loss: 0.4935
Training Epoch: 33 [18432/50048]	Loss: 0.6418
Training Epoch: 33 [18560/50048]	Loss: 0.6794
Training Epoch: 33 [18688/50048]	Loss: 0.6251
Training Epoch: 33 [18816/50048]	Loss: 0.5610
Training Epoch: 33 [18944/50048]	Loss: 0.4925
Training Epoch: 33 [19072/50048]	Loss: 0.6330
Training Epoch: 33 [19200/50048]	Loss: 0.6595
Training Epoch: 33 [19328/50048]	Loss: 0.5425
Training Epoch: 33 [19456/50048]	Loss: 0.5640
Training Epoch: 33 [19584/50048]	Loss: 0.5388
Training Epoch: 33 [19712/50048]	Loss: 0.5529
Training Epoch: 33 [19840/50048]	Loss: 0.7019
Training Epoch: 33 [19968/50048]	Loss: 0.4385
Training Epoch: 33 [20096/50048]	Loss: 0.4896
Training Epoch: 33 [20224/50048]	Loss: 0.4993
Training Epoch: 33 [20352/50048]	Loss: 0.5540
Training Epoch: 33 [20480/50048]	Loss: 0.4264
Training Epoch: 33 [20608/50048]	Loss: 0.5604
Training Epoch: 33 [20736/50048]	Loss: 0.7428
Training Epoch: 33 [20864/50048]	Loss: 0.5995
Training Epoch: 33 [20992/50048]	Loss: 0.4676
Training Epoch: 33 [21120/50048]	Loss: 0.5847
Training Epoch: 33 [21248/50048]	Loss: 0.4982
Training Epoch: 33 [21376/50048]	Loss: 0.6574
Training Epoch: 33 [21504/50048]	Loss: 0.7283
Training Epoch: 33 [21632/50048]	Loss: 0.5809
Training Epoch: 33 [21760/50048]	Loss: 0.6098
Training Epoch: 33 [21888/50048]	Loss: 0.7761
Training Epoch: 33 [22016/50048]	Loss: 0.5046
Training Epoch: 33 [22144/50048]	Loss: 0.5632
Training Epoch: 33 [22272/50048]	Loss: 0.6335
Training Epoch: 33 [22400/50048]	Loss: 0.4728
Training Epoch: 33 [22528/50048]	Loss: 0.5750
Training Epoch: 33 [22656/50048]	Loss: 0.5078
Training Epoch: 33 [22784/50048]	Loss: 0.5794
Training Epoch: 33 [22912/50048]	Loss: 0.5117
Training Epoch: 33 [23040/50048]	Loss: 0.6309
Training Epoch: 33 [23168/50048]	Loss: 0.5255
Training Epoch: 33 [23296/50048]	Loss: 0.5472
Training Epoch: 33 [23424/50048]	Loss: 0.6464
Training Epoch: 33 [23552/50048]	Loss: 0.5733
Training Epoch: 33 [23680/50048]	Loss: 0.4488
Training Epoch: 33 [23808/50048]	Loss: 0.7104
Training Epoch: 33 [23936/50048]	Loss: 0.5281
Training Epoch: 33 [24064/50048]	Loss: 0.5754
Training Epoch: 33 [24192/50048]	Loss: 0.6649
Training Epoch: 33 [24320/50048]	Loss: 0.7532
Training Epoch: 33 [24448/50048]	Loss: 0.6154
Training Epoch: 33 [24576/50048]	Loss: 0.6193
Training Epoch: 33 [24704/50048]	Loss: 0.4041
Training Epoch: 33 [24832/50048]	Loss: 0.4351
Training Epoch: 33 [24960/50048]	Loss: 0.6674
Training Epoch: 33 [25088/50048]	Loss: 0.3740
Training Epoch: 33 [25216/50048]	Loss: 0.7057
Training Epoch: 33 [25344/50048]	Loss: 0.6175
Training Epoch: 33 [25472/50048]	Loss: 0.7599
Training Epoch: 33 [25600/50048]	Loss: 0.6204
Training Epoch: 33 [25728/50048]	Loss: 0.6318
Training Epoch: 33 [25856/50048]	Loss: 0.6280
Training Epoch: 33 [25984/50048]	Loss: 0.5786
Training Epoch: 33 [26112/50048]	Loss: 0.5663
Training Epoch: 33 [26240/50048]	Loss: 0.5997
Training Epoch: 33 [26368/50048]	Loss: 0.4020
Training Epoch: 33 [26496/50048]	Loss: 0.5832
Training Epoch: 33 [26624/50048]	Loss: 0.5211
Training Epoch: 33 [26752/50048]	Loss: 0.4209
Training Epoch: 33 [26880/50048]	Loss: 0.4127
Training Epoch: 33 [27008/50048]	Loss: 0.6816
Training Epoch: 33 [27136/50048]	Loss: 0.5628
Training Epoch: 33 [27264/50048]	Loss: 0.4566
Training Epoch: 33 [27392/50048]	Loss: 0.5383
Training Epoch: 33 [27520/50048]	Loss: 0.5872
Training Epoch: 33 [27648/50048]	Loss: 0.4692
Training Epoch: 33 [27776/50048]	Loss: 0.5877
Training Epoch: 33 [27904/50048]	Loss: 0.6286
Training Epoch: 33 [28032/50048]	Loss: 0.5104
Training Epoch: 33 [28160/50048]	Loss: 0.6298
Training Epoch: 33 [28288/50048]	Loss: 0.4191
Training Epoch: 33 [28416/50048]	Loss: 0.6519
Training Epoch: 33 [28544/50048]	Loss: 0.6357
Training Epoch: 33 [28672/50048]	Loss: 0.4623
Training Epoch: 33 [28800/50048]	Loss: 0.4752
Training Epoch: 33 [28928/50048]	Loss: 0.6221
Training Epoch: 33 [29056/50048]	Loss: 0.7584
Training Epoch: 33 [29184/50048]	Loss: 0.4867
Training Epoch: 33 [29312/50048]	Loss: 0.5878
Training Epoch: 33 [29440/50048]	Loss: 0.6265
Training Epoch: 33 [29568/50048]	Loss: 0.7070
Training Epoch: 33 [29696/50048]	Loss: 0.6449
Training Epoch: 33 [29824/50048]	Loss: 0.6912
Training Epoch: 33 [29952/50048]	Loss: 0.5862
Training Epoch: 33 [30080/50048]	Loss: 0.5923
Training Epoch: 33 [30208/50048]	Loss: 0.5689
Training Epoch: 33 [30336/50048]	Loss: 0.6707
Training Epoch: 33 [30464/50048]	Loss: 0.5996
Training Epoch: 33 [30592/50048]	Loss: 0.7749
Training Epoch: 33 [30720/50048]	Loss: 0.4829
Training Epoch: 33 [30848/50048]	Loss: 0.6644
Training Epoch: 33 [30976/50048]	Loss: 0.6227
Training Epoch: 33 [31104/50048]	Loss: 0.6877
Training Epoch: 33 [31232/50048]	Loss: 0.7350
Training Epoch: 33 [31360/50048]	Loss: 0.5580
Training Epoch: 33 [31488/50048]	Loss: 0.5805
Training Epoch: 33 [31616/50048]	Loss: 0.5617
Training Epoch: 33 [31744/50048]	Loss: 0.6626
Training Epoch: 33 [31872/50048]	Loss: 0.6410
Training Epoch: 33 [32000/50048]	Loss: 0.6025
Training Epoch: 33 [32128/50048]	Loss: 0.5511
Training Epoch: 33 [32256/50048]	Loss: 0.3960
Training Epoch: 33 [32384/50048]	Loss: 0.5023
Training Epoch: 33 [32512/50048]	Loss: 0.7531
Training Epoch: 33 [32640/50048]	Loss: 0.6393
Training Epoch: 33 [32768/50048]	Loss: 0.6729
Training Epoch: 33 [32896/50048]	Loss: 0.5935
Training Epoch: 33 [33024/50048]	Loss: 0.6182
Training Epoch: 33 [33152/50048]	Loss: 0.6985
Training Epoch: 33 [33280/50048]	Loss: 0.6507
Training Epoch: 33 [33408/50048]	Loss: 0.6635
Training Epoch: 33 [33536/50048]	Loss: 0.7897
Training Epoch: 33 [33664/50048]	Loss: 0.4788
Training Epoch: 33 [33792/50048]	Loss: 0.6156
Training Epoch: 33 [33920/50048]	Loss: 0.6200
Training Epoch: 33 [34048/50048]	Loss: 0.5574
Training Epoch: 33 [34176/50048]	Loss: 0.6848
Training Epoch: 33 [34304/50048]	Loss: 0.5295
Training Epoch: 33 [34432/50048]	Loss: 0.5124
Training Epoch: 33 [34560/50048]	Loss: 0.5235
Training Epoch: 33 [34688/50048]	Loss: 0.6369
Training Epoch: 33 [34816/50048]	Loss: 0.5113
Training Epoch: 33 [34944/50048]	Loss: 0.5764
Training Epoch: 33 [35072/50048]	Loss: 0.5564
Training Epoch: 33 [35200/50048]	Loss: 0.5222
Training Epoch: 33 [35328/50048]	Loss: 0.6058
Training Epoch: 33 [35456/50048]	Loss: 0.4191
Training Epoch: 33 [35584/50048]	Loss: 0.6008
Training Epoch: 33 [35712/50048]	Loss: 0.5891
Training Epoch: 33 [35840/50048]	Loss: 0.5382
Training Epoch: 33 [35968/50048]	Loss: 0.7196
Training Epoch: 33 [36096/50048]	Loss: 0.6386
Training Epoch: 33 [36224/50048]	Loss: 0.4179
Training Epoch: 33 [36352/50048]	Loss: 0.6013
Training Epoch: 33 [36480/50048]	Loss: 0.5583
Training Epoch: 33 [36608/50048]	Loss: 0.6120
Training Epoch: 33 [36736/50048]	Loss: 0.6232
Training Epoch: 33 [36864/50048]	Loss: 0.5769
Training Epoch: 33 [36992/50048]	Loss: 0.5434
Training Epoch: 33 [37120/50048]	Loss: 0.7150
Training Epoch: 33 [37248/50048]	Loss: 0.6623
Training Epoch: 33 [37376/50048]	Loss: 0.8245
Training Epoch: 33 [37504/50048]	Loss: 0.5978
Training Epoch: 33 [37632/50048]	Loss: 0.3853
Training Epoch: 33 [37760/50048]	Loss: 0.7482
Training Epoch: 33 [37888/50048]	Loss: 0.5845
Training Epoch: 33 [38016/50048]	Loss: 0.5533
Training Epoch: 33 [38144/50048]	Loss: 0.5698
Training Epoch: 33 [38272/50048]	Loss: 0.4158
Training Epoch: 33 [38400/50048]	Loss: 0.4587
Training Epoch: 33 [38528/50048]	Loss: 0.4982
Training Epoch: 33 [38656/50048]	Loss: 0.4834
Training Epoch: 33 [38784/50048]	Loss: 0.6048
Training Epoch: 33 [38912/50048]	Loss: 0.4657
Training Epoch: 33 [39040/50048]	Loss: 0.4872
Training Epoch: 33 [39168/50048]	Loss: 0.5076
Training Epoch: 33 [39296/50048]	Loss: 0.4911
Training Epoch: 33 [39424/50048]	Loss: 0.6955
Training Epoch: 33 [39552/50048]	Loss: 0.6647
Training Epoch: 33 [39680/50048]	Loss: 0.6396
Training Epoch: 33 [39808/50048]	Loss: 0.6874
Training Epoch: 33 [39936/50048]	Loss: 0.5385
Training Epoch: 33 [40064/50048]	Loss: 0.7558
Training Epoch: 33 [40192/50048]	Loss: 0.6283
Training Epoch: 33 [40320/50048]	Loss: 0.4995
Training Epoch: 33 [40448/50048]	Loss: 0.5535
Training Epoch: 33 [40576/50048]	Loss: 0.6583
Training Epoch: 33 [40704/50048]	Loss: 0.6380
Training Epoch: 33 [40832/50048]	Loss: 0.5147
Training Epoch: 33 [40960/50048]	Loss: 0.5913
Training Epoch: 33 [41088/50048]	Loss: 0.5557
Training Epoch: 33 [41216/50048]	Loss: 0.6630
Training Epoch: 33 [41344/50048]	Loss: 0.5322
Training Epoch: 33 [41472/50048]	Loss: 0.7406
Training Epoch: 33 [41600/50048]	Loss: 0.6855
Training Epoch: 33 [41728/50048]	Loss: 0.8095
Training Epoch: 33 [41856/50048]	Loss: 0.5501
Training Epoch: 33 [41984/50048]	Loss: 0.5755
Training Epoch: 33 [42112/50048]	Loss: 0.5201
Training Epoch: 33 [42240/50048]	Loss: 0.6934
Training Epoch: 33 [42368/50048]	Loss: 0.4420
Training Epoch: 33 [42496/50048]	Loss: 0.5634
Training Epoch: 33 [42624/50048]	Loss: 0.5757
Training Epoch: 33 [42752/50048]	Loss: 0.7026
Training Epoch: 33 [42880/50048]	Loss: 0.6820
Training Epoch: 33 [43008/50048]	Loss: 0.5596
Training Epoch: 33 [43136/50048]	Loss: 0.5296
Training Epoch: 33 [43264/50048]	Loss: 0.5970
Training Epoch: 33 [43392/50048]	Loss: 0.6091
Training Epoch: 33 [43520/50048]	Loss: 0.5851
Training Epoch: 33 [43648/50048]	Loss: 0.6120
Training Epoch: 33 [43776/50048]	Loss: 0.6439
Training Epoch: 33 [43904/50048]	Loss: 0.5854
Training Epoch: 33 [44032/50048]	Loss: 0.7193
Training Epoch: 33 [44160/50048]	Loss: 0.6257
Training Epoch: 33 [44288/50048]	Loss: 0.7501
Training Epoch: 33 [44416/50048]	Loss: 0.7243
Training Epoch: 33 [44544/50048]	Loss: 0.5566
Training Epoch: 33 [44672/50048]	Loss: 0.7078
Training Epoch: 33 [44800/50048]	Loss: 0.5977
Training Epoch: 33 [44928/50048]	Loss: 0.5139
Training Epoch: 33 [45056/50048]	Loss: 0.6466
Training Epoch: 33 [45184/50048]	Loss: 0.5521
Training Epoch: 33 [45312/50048]	Loss: 0.6598
Training Epoch: 33 [45440/50048]	Loss: 0.6514
Training Epoch: 33 [45568/50048]	Loss: 0.5137
Training Epoch: 33 [45696/50048]	Loss: 0.4326
2022-12-06 06:58:57,137 [ZeusDataLoader(train)] train epoch 34 done: time=86.49 energy=10504.08
2022-12-06 06:58:57,139 [ZeusDataLoader(eval)] Epoch 34 begin.
Training Epoch: 33 [45824/50048]	Loss: 0.4176
Training Epoch: 33 [45952/50048]	Loss: 0.6497
Training Epoch: 33 [46080/50048]	Loss: 0.5985
Training Epoch: 33 [46208/50048]	Loss: 0.6878
Training Epoch: 33 [46336/50048]	Loss: 0.6728
Training Epoch: 33 [46464/50048]	Loss: 0.6507
Training Epoch: 33 [46592/50048]	Loss: 0.5518
Training Epoch: 33 [46720/50048]	Loss: 0.5260
Training Epoch: 33 [46848/50048]	Loss: 0.6216
Training Epoch: 33 [46976/50048]	Loss: 0.6343
Training Epoch: 33 [47104/50048]	Loss: 0.8298
Training Epoch: 33 [47232/50048]	Loss: 0.5953
Training Epoch: 33 [47360/50048]	Loss: 0.6276
Training Epoch: 33 [47488/50048]	Loss: 0.5628
Training Epoch: 33 [47616/50048]	Loss: 0.7089
Training Epoch: 33 [47744/50048]	Loss: 0.6467
Training Epoch: 33 [47872/50048]	Loss: 0.5848
Training Epoch: 33 [48000/50048]	Loss: 0.4144
Training Epoch: 33 [48128/50048]	Loss: 0.6410
Training Epoch: 33 [48256/50048]	Loss: 0.4365
Training Epoch: 33 [48384/50048]	Loss: 0.5577
Training Epoch: 33 [48512/50048]	Loss: 0.8118
Training Epoch: 33 [48640/50048]	Loss: 0.6688
Training Epoch: 33 [48768/50048]	Loss: 0.7593
Training Epoch: 33 [48896/50048]	Loss: 0.5741
Training Epoch: 33 [49024/50048]	Loss: 0.5790
Training Epoch: 33 [49152/50048]	Loss: 0.6487
Training Epoch: 33 [49280/50048]	Loss: 0.6087
Training Epoch: 33 [49408/50048]	Loss: 0.4213
Training Epoch: 33 [49536/50048]	Loss: 0.6954
Training Epoch: 33 [49664/50048]	Loss: 0.5787
Training Epoch: 33 [49792/50048]	Loss: 0.7330
Training Epoch: 33 [49920/50048]	Loss: 0.6935
Training Epoch: 33 [50048/50048]	Loss: 0.6266
2022-12-06 11:59:00.855 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 06:59:00,912 [ZeusDataLoader(eval)] eval epoch 34 done: time=3.76 energy=452.30
2022-12-06 06:59:00,913 [ZeusDataLoader(train)] Up to epoch 34: time=3066.55, energy=372276.68, cost=454461.89
2022-12-06 06:59:00,913 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 06:59:00,913 [ZeusDataLoader(train)] Expected next epoch: time=3156.35, energy=383074.69, cost=467718.27
2022-12-06 06:59:00,914 [ZeusDataLoader(train)] Epoch 35 begin.
Validation Epoch: 33, Average loss: 0.0127, Accuracy: 0.6241
2022-12-06 06:59:01,116 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 06:59:01,117 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 11:59:01.119 [ZeusMonitor] Monitor started.
2022-12-06 11:59:01.127 [ZeusMonitor] Running indefinitely. 2022-12-06 11:59:01.127 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 11:59:01.127 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e35+gpu0.power.log
Training Epoch: 34 [128/50048]	Loss: 0.4652
Training Epoch: 34 [256/50048]	Loss: 0.5397
Training Epoch: 34 [384/50048]	Loss: 0.4571
Training Epoch: 34 [512/50048]	Loss: 0.5314
Training Epoch: 34 [640/50048]	Loss: 0.5000
Training Epoch: 34 [768/50048]	Loss: 0.3593
Training Epoch: 34 [896/50048]	Loss: 0.5127
Training Epoch: 34 [1024/50048]	Loss: 0.5541
Training Epoch: 34 [1152/50048]	Loss: 0.6206
Training Epoch: 34 [1280/50048]	Loss: 0.7096
Training Epoch: 34 [1408/50048]	Loss: 0.4542
Training Epoch: 34 [1536/50048]	Loss: 0.4884
Training Epoch: 34 [1664/50048]	Loss: 0.3859
Training Epoch: 34 [1792/50048]	Loss: 0.5195
Training Epoch: 34 [1920/50048]	Loss: 0.4263
Training Epoch: 34 [2048/50048]	Loss: 0.4020
Training Epoch: 34 [2176/50048]	Loss: 0.5492
Training Epoch: 34 [2304/50048]	Loss: 0.4260
Training Epoch: 34 [2432/50048]	Loss: 0.4829
Training Epoch: 34 [2560/50048]	Loss: 0.6386
Training Epoch: 34 [2688/50048]	Loss: 0.4770
Training Epoch: 34 [2816/50048]	Loss: 0.5186
Training Epoch: 34 [2944/50048]	Loss: 0.6343
Training Epoch: 34 [3072/50048]	Loss: 0.5665
Training Epoch: 34 [3200/50048]	Loss: 0.4771
Training Epoch: 34 [3328/50048]	Loss: 0.5208
Training Epoch: 34 [3456/50048]	Loss: 0.4894
Training Epoch: 34 [3584/50048]	Loss: 0.5381
Training Epoch: 34 [3712/50048]	Loss: 0.5877
Training Epoch: 34 [3840/50048]	Loss: 0.3932
Training Epoch: 34 [3968/50048]	Loss: 0.4555
Training Epoch: 34 [4096/50048]	Loss: 0.5740
Training Epoch: 34 [4224/50048]	Loss: 0.4568
Training Epoch: 34 [4352/50048]	Loss: 0.6011
Training Epoch: 34 [4480/50048]	Loss: 0.5575
Training Epoch: 34 [4608/50048]	Loss: 0.5359
Training Epoch: 34 [4736/50048]	Loss: 0.4925
Training Epoch: 34 [4864/50048]	Loss: 0.5717
Training Epoch: 34 [4992/50048]	Loss: 0.4803
Training Epoch: 34 [5120/50048]	Loss: 0.4490
Training Epoch: 34 [5248/50048]	Loss: 0.4606
Training Epoch: 34 [5376/50048]	Loss: 0.4780
Training Epoch: 34 [5504/50048]	Loss: 0.4426
Training Epoch: 34 [5632/50048]	Loss: 0.4794
Training Epoch: 34 [5760/50048]	Loss: 0.5459
Training Epoch: 34 [5888/50048]	Loss: 0.6420
Training Epoch: 34 [6016/50048]	Loss: 0.4403
Training Epoch: 34 [6144/50048]	Loss: 0.5088
Training Epoch: 34 [6272/50048]	Loss: 0.4906
Training Epoch: 34 [6400/50048]	Loss: 0.4955
Training Epoch: 34 [6528/50048]	Loss: 0.4906
Training Epoch: 34 [6656/50048]	Loss: 0.4686
Training Epoch: 34 [6784/50048]	Loss: 0.3881
Training Epoch: 34 [6912/50048]	Loss: 0.6198
Training Epoch: 34 [7040/50048]	Loss: 0.4000
Training Epoch: 34 [7168/50048]	Loss: 0.4288
Training Epoch: 34 [7296/50048]	Loss: 0.6432
Training Epoch: 34 [7424/50048]	Loss: 0.4675
Training Epoch: 34 [7552/50048]	Loss: 0.4741
Training Epoch: 34 [7680/50048]	Loss: 0.5531
Training Epoch: 34 [7808/50048]	Loss: 0.4309
Training Epoch: 34 [7936/50048]	Loss: 0.4813
Training Epoch: 34 [8064/50048]	Loss: 0.4692
Training Epoch: 34 [8192/50048]	Loss: 0.5309
Training Epoch: 34 [8320/50048]	Loss: 0.5389
Training Epoch: 34 [8448/50048]	Loss: 0.5428
Training Epoch: 34 [8576/50048]	Loss: 0.4052
Training Epoch: 34 [8704/50048]	Loss: 0.4039
Training Epoch: 34 [8832/50048]	Loss: 0.6000
Training Epoch: 34 [8960/50048]	Loss: 0.4111
Training Epoch: 34 [9088/50048]	Loss: 0.5397
Training Epoch: 34 [9216/50048]	Loss: 0.4687
Training Epoch: 34 [9344/50048]	Loss: 0.5612
Training Epoch: 34 [9472/50048]	Loss: 0.5430
Training Epoch: 34 [9600/50048]	Loss: 0.4802
Training Epoch: 34 [9728/50048]	Loss: 0.6243
Training Epoch: 34 [9856/50048]	Loss: 0.5618
Training Epoch: 34 [9984/50048]	Loss: 0.7192
Training Epoch: 34 [10112/50048]	Loss: 0.5816
Training Epoch: 34 [10240/50048]	Loss: 0.3862
Training Epoch: 34 [10368/50048]	Loss: 0.4293
Training Epoch: 34 [10496/50048]	Loss: 0.3694
Training Epoch: 34 [10624/50048]	Loss: 0.4357
Training Epoch: 34 [10752/50048]	Loss: 0.4282
Training Epoch: 34 [10880/50048]	Loss: 0.6178
Training Epoch: 34 [11008/50048]	Loss: 0.6577
Training Epoch: 34 [11136/50048]	Loss: 0.5270
Training Epoch: 34 [11264/50048]	Loss: 0.4989
Training Epoch: 34 [11392/50048]	Loss: 0.5450
Training Epoch: 34 [11520/50048]	Loss: 0.5555
Training Epoch: 34 [11648/50048]	Loss: 0.6186
Training Epoch: 34 [11776/50048]	Loss: 0.5122
Training Epoch: 34 [11904/50048]	Loss: 0.4974
Training Epoch: 34 [12032/50048]	Loss: 0.5586
Training Epoch: 34 [12160/50048]	Loss: 0.4754
Training Epoch: 34 [12288/50048]	Loss: 0.5229
Training Epoch: 34 [12416/50048]	Loss: 0.5660
Training Epoch: 34 [12544/50048]	Loss: 0.5411
Training Epoch: 34 [12672/50048]	Loss: 0.6832
Training Epoch: 34 [12800/50048]	Loss: 0.3862
Training Epoch: 34 [12928/50048]	Loss: 0.5888
Training Epoch: 34 [13056/50048]	Loss: 0.5893
Training Epoch: 34 [13184/50048]	Loss: 0.6143
Training Epoch: 34 [13312/50048]	Loss: 0.4543
Training Epoch: 34 [13440/50048]	Loss: 0.4779
Training Epoch: 34 [13568/50048]	Loss: 0.5442
Training Epoch: 34 [13696/50048]	Loss: 0.5076
Training Epoch: 34 [13824/50048]	Loss: 0.5385
Training Epoch: 34 [13952/50048]	Loss: 0.3874
Training Epoch: 34 [14080/50048]	Loss: 0.5456
Training Epoch: 34 [14208/50048]	Loss: 0.5994
Training Epoch: 34 [14336/50048]	Loss: 0.4704
Training Epoch: 34 [14464/50048]	Loss: 0.6223
Training Epoch: 34 [14592/50048]	Loss: 0.5529
Training Epoch: 34 [14720/50048]	Loss: 0.5447
Training Epoch: 34 [14848/50048]	Loss: 0.5623
Training Epoch: 34 [14976/50048]	Loss: 0.5139
Training Epoch: 34 [15104/50048]	Loss: 0.5959
Training Epoch: 34 [15232/50048]	Loss: 0.4174
Training Epoch: 34 [15360/50048]	Loss: 0.5217
Training Epoch: 34 [15488/50048]	Loss: 0.6263
Training Epoch: 34 [15616/50048]	Loss: 0.6875
Training Epoch: 34 [15744/50048]	Loss: 0.6979
Training Epoch: 34 [15872/50048]	Loss: 0.4601
Training Epoch: 34 [16000/50048]	Loss: 0.5183
Training Epoch: 34 [16128/50048]	Loss: 0.6340
Training Epoch: 34 [16256/50048]	Loss: 0.6714
Training Epoch: 34 [16384/50048]	Loss: 0.4506
Training Epoch: 34 [16512/50048]	Loss: 0.6231
Training Epoch: 34 [16640/50048]	Loss: 0.4462
Training Epoch: 34 [16768/50048]	Loss: 0.3969
Training Epoch: 34 [16896/50048]	Loss: 0.5863
Training Epoch: 34 [17024/50048]	Loss: 0.6002
Training Epoch: 34 [17152/50048]	Loss: 0.5089
Training Epoch: 34 [17280/50048]	Loss: 0.4744
Training Epoch: 34 [17408/50048]	Loss: 0.6246
Training Epoch: 34 [17536/50048]	Loss: 0.5427
Training Epoch: 34 [17664/50048]	Loss: 0.3768
Training Epoch: 34 [17792/50048]	Loss: 0.5020
Training Epoch: 34 [17920/50048]	Loss: 0.5588
Training Epoch: 34 [18048/50048]	Loss: 0.6819
Training Epoch: 34 [18176/50048]	Loss: 0.5024
Training Epoch: 34 [18304/50048]	Loss: 0.4914
Training Epoch: 34 [18432/50048]	Loss: 0.7695
Training Epoch: 34 [18560/50048]	Loss: 0.4958
Training Epoch: 34 [18688/50048]	Loss: 0.5602
Training Epoch: 34 [18816/50048]	Loss: 0.5502
Training Epoch: 34 [18944/50048]	Loss: 0.4323
Training Epoch: 34 [19072/50048]	Loss: 0.4708
Training Epoch: 34 [19200/50048]	Loss: 0.4404
Training Epoch: 34 [19328/50048]	Loss: 0.4633
Training Epoch: 34 [19456/50048]	Loss: 0.4275
Training Epoch: 34 [19584/50048]	Loss: 0.7719
Training Epoch: 34 [19712/50048]	Loss: 0.5595
Training Epoch: 34 [19840/50048]	Loss: 0.4883
Training Epoch: 34 [19968/50048]	Loss: 0.4515
Training Epoch: 34 [20096/50048]	Loss: 0.6964
Training Epoch: 34 [20224/50048]	Loss: 0.5272
Training Epoch: 34 [20352/50048]	Loss: 0.5137
Training Epoch: 34 [20480/50048]	Loss: 0.7026
Training Epoch: 34 [20608/50048]	Loss: 0.5327
Training Epoch: 34 [20736/50048]	Loss: 0.5480
Training Epoch: 34 [20864/50048]	Loss: 0.7568
Training Epoch: 34 [20992/50048]	Loss: 0.6121
Training Epoch: 34 [21120/50048]	Loss: 0.5883
Training Epoch: 34 [21248/50048]	Loss: 0.6163
Training Epoch: 34 [21376/50048]	Loss: 0.6623
Training Epoch: 34 [21504/50048]	Loss: 0.5993
Training Epoch: 34 [21632/50048]	Loss: 0.6380
Training Epoch: 34 [21760/50048]	Loss: 0.6458
Training Epoch: 34 [21888/50048]	Loss: 0.4621
Training Epoch: 34 [22016/50048]	Loss: 0.3307
Training Epoch: 34 [22144/50048]	Loss: 0.5198
Training Epoch: 34 [22272/50048]	Loss: 0.5170
Training Epoch: 34 [22400/50048]	Loss: 0.5763
Training Epoch: 34 [22528/50048]	Loss: 0.6016
Training Epoch: 34 [22656/50048]	Loss: 0.6088
Training Epoch: 34 [22784/50048]	Loss: 0.4707
Training Epoch: 34 [22912/50048]	Loss: 0.6535
Training Epoch: 34 [23040/50048]	Loss: 0.5060
Training Epoch: 34 [23168/50048]	Loss: 0.4862
Training Epoch: 34 [23296/50048]	Loss: 0.5766
Training Epoch: 34 [23424/50048]	Loss: 0.6107
Training Epoch: 34 [23552/50048]	Loss: 0.4832
Training Epoch: 34 [23680/50048]	Loss: 0.5838
Training Epoch: 34 [23808/50048]	Loss: 0.6305
Training Epoch: 34 [23936/50048]	Loss: 0.4029
Training Epoch: 34 [24064/50048]	Loss: 0.4560
Training Epoch: 34 [24192/50048]	Loss: 0.4968
Training Epoch: 34 [24320/50048]	Loss: 0.5656
Training Epoch: 34 [24448/50048]	Loss: 0.5624
Training Epoch: 34 [24576/50048]	Loss: 0.5248
Training Epoch: 34 [24704/50048]	Loss: 0.5861
Training Epoch: 34 [24832/50048]	Loss: 0.4998
Training Epoch: 34 [24960/50048]	Loss: 0.5589
Training Epoch: 34 [25088/50048]	Loss: 0.5127
Training Epoch: 34 [25216/50048]	Loss: 0.4460
Training Epoch: 34 [25344/50048]	Loss: 0.4607
Training Epoch: 34 [25472/50048]	Loss: 0.5740
Training Epoch: 34 [25600/50048]	Loss: 0.4940
Training Epoch: 34 [25728/50048]	Loss: 0.4129
Training Epoch: 34 [25856/50048]	Loss: 0.5041
Training Epoch: 34 [25984/50048]	Loss: 0.7410
Training Epoch: 34 [26112/50048]	Loss: 0.4607
Training Epoch: 34 [26240/50048]	Loss: 0.4719
Training Epoch: 34 [26368/50048]	Loss: 0.7307
Training Epoch: 34 [26496/50048]	Loss: 0.6375
Training Epoch: 34 [26624/50048]	Loss: 0.6464
Training Epoch: 34 [26752/50048]	Loss: 0.6567
Training Epoch: 34 [26880/50048]	Loss: 0.6477
Training Epoch: 34 [27008/50048]	Loss: 0.6116
Training Epoch: 34 [27136/50048]	Loss: 0.6107
Training Epoch: 34 [27264/50048]	Loss: 0.4789
Training Epoch: 34 [27392/50048]	Loss: 0.6812
Training Epoch: 34 [27520/50048]	Loss: 0.5143
Training Epoch: 34 [27648/50048]	Loss: 0.6785
Training Epoch: 34 [27776/50048]	Loss: 0.3389
Training Epoch: 34 [27904/50048]	Loss: 0.6208
Training Epoch: 34 [28032/50048]	Loss: 0.3645
Training Epoch: 34 [28160/50048]	Loss: 0.6588
Training Epoch: 34 [28288/50048]	Loss: 0.6799
Training Epoch: 34 [28416/50048]	Loss: 0.5570
Training Epoch: 34 [28544/50048]	Loss: 0.6017
Training Epoch: 34 [28672/50048]	Loss: 0.5902
Training Epoch: 34 [28800/50048]	Loss: 0.3457
Training Epoch: 34 [28928/50048]	Loss: 0.6934
Training Epoch: 34 [29056/50048]	Loss: 0.6502
Training Epoch: 34 [29184/50048]	Loss: 0.5306
Training Epoch: 34 [29312/50048]	Loss: 0.4801
Training Epoch: 34 [29440/50048]	Loss: 0.6094
Training Epoch: 34 [29568/50048]	Loss: 0.7217
Training Epoch: 34 [29696/50048]	Loss: 0.6040
Training Epoch: 34 [29824/50048]	Loss: 0.6641
Training Epoch: 34 [29952/50048]	Loss: 0.5414
Training Epoch: 34 [30080/50048]	Loss: 0.6460
Training Epoch: 34 [30208/50048]	Loss: 0.5269
Training Epoch: 34 [30336/50048]	Loss: 0.7213
Training Epoch: 34 [30464/50048]	Loss: 0.4808
Training Epoch: 34 [30592/50048]	Loss: 0.6630
Training Epoch: 34 [30720/50048]	Loss: 0.5881
Training Epoch: 34 [30848/50048]	Loss: 0.7192
Training Epoch: 34 [30976/50048]	Loss: 0.4998
Training Epoch: 34 [31104/50048]	Loss: 0.6109
Training Epoch: 34 [31232/50048]	Loss: 0.8167
Training Epoch: 34 [31360/50048]	Loss: 0.4708
Training Epoch: 34 [31488/50048]	Loss: 0.6344
Training Epoch: 34 [31616/50048]	Loss: 0.6130
Training Epoch: 34 [31744/50048]	Loss: 0.4801
Training Epoch: 34 [31872/50048]	Loss: 0.6277
Training Epoch: 34 [32000/50048]	Loss: 0.5292
Training Epoch: 34 [32128/50048]	Loss: 0.6667
Training Epoch: 34 [32256/50048]	Loss: 0.6266
Training Epoch: 34 [32384/50048]	Loss: 0.7976
Training Epoch: 34 [32512/50048]	Loss: 0.5733
Training Epoch: 34 [32640/50048]	Loss: 0.6167
Training Epoch: 34 [32768/50048]	Loss: 0.6560
Training Epoch: 34 [32896/50048]	Loss: 0.5065
Training Epoch: 34 [33024/50048]	Loss: 0.5892
Training Epoch: 34 [33152/50048]	Loss: 0.4848
Training Epoch: 34 [33280/50048]	Loss: 0.7442
Training Epoch: 34 [33408/50048]	Loss: 0.7443
Training Epoch: 34 [33536/50048]	Loss: 0.5730
Training Epoch: 34 [33664/50048]	Loss: 0.4220
Training Epoch: 34 [33792/50048]	Loss: 0.4904
Training Epoch: 34 [33920/50048]	Loss: 0.5924
Training Epoch: 34 [34048/50048]	Loss: 0.5539
Training Epoch: 34 [34176/50048]	Loss: 0.6657
Training Epoch: 34 [34304/50048]	Loss: 0.5241
Training Epoch: 34 [34432/50048]	Loss: 0.6144
Training Epoch: 34 [34560/50048]	Loss: 0.5137
Training Epoch: 34 [34688/50048]	Loss: 0.6942
Training Epoch: 34 [34816/50048]	Loss: 0.6179
Training Epoch: 34 [34944/50048]	Loss: 0.4620
Training Epoch: 34 [35072/50048]	Loss: 0.5724
Training Epoch: 34 [35200/50048]	Loss: 0.7351
Training Epoch: 34 [35328/50048]	Loss: 0.6051
Training Epoch: 34 [35456/50048]	Loss: 0.5154
Training Epoch: 34 [35584/50048]	Loss: 0.6133
Training Epoch: 34 [35712/50048]	Loss: 0.7050
Training Epoch: 34 [35840/50048]	Loss: 0.4783
Training Epoch: 34 [35968/50048]	Loss: 0.6069
Training Epoch: 34 [36096/50048]	Loss: 0.5902
Training Epoch: 34 [36224/50048]	Loss: 0.4572
Training Epoch: 34 [36352/50048]	Loss: 0.5837
Training Epoch: 34 [36480/50048]	Loss: 0.7656
Training Epoch: 34 [36608/50048]	Loss: 0.5682
Training Epoch: 34 [36736/50048]	Loss: 0.5795
Training Epoch: 34 [36864/50048]	Loss: 0.6337
Training Epoch: 34 [36992/50048]	Loss: 0.6182
Training Epoch: 34 [37120/50048]	Loss: 0.5816
Training Epoch: 34 [37248/50048]	Loss: 0.4583
Training Epoch: 34 [37376/50048]	Loss: 0.6776
Training Epoch: 34 [37504/50048]	Loss: 0.4789
Training Epoch: 34 [37632/50048]	Loss: 0.6426
Training Epoch: 34 [37760/50048]	Loss: 0.5476
Training Epoch: 34 [37888/50048]	Loss: 0.5754
Training Epoch: 34 [38016/50048]	Loss: 0.6112
Training Epoch: 34 [38144/50048]	Loss: 0.5480
Training Epoch: 34 [38272/50048]	Loss: 0.5406
Training Epoch: 34 [38400/50048]	Loss: 0.5669
Training Epoch: 34 [38528/50048]	Loss: 0.4431
Training Epoch: 34 [38656/50048]	Loss: 0.5923
Training Epoch: 34 [38784/50048]	Loss: 0.6625
Training Epoch: 34 [38912/50048]	Loss: 0.5455
Training Epoch: 34 [39040/50048]	Loss: 0.5584
Training Epoch: 34 [39168/50048]	Loss: 0.8676
Training Epoch: 34 [39296/50048]	Loss: 0.8040
Training Epoch: 34 [39424/50048]	Loss: 0.6888
Training Epoch: 34 [39552/50048]	Loss: 0.6440
Training Epoch: 34 [39680/50048]	Loss: 0.4992
Training Epoch: 34 [39808/50048]	Loss: 0.5714
Training Epoch: 34 [39936/50048]	Loss: 0.6092
Training Epoch: 34 [40064/50048]	Loss: 0.5466
Training Epoch: 34 [40192/50048]	Loss: 0.6012
Training Epoch: 34 [40320/50048]	Loss: 0.7748
Training Epoch: 34 [40448/50048]	Loss: 0.8320
Training Epoch: 34 [40576/50048]	Loss: 0.6017
Training Epoch: 34 [40704/50048]	Loss: 0.6211
Training Epoch: 34 [40832/50048]	Loss: 0.6465
Training Epoch: 34 [40960/50048]	Loss: 0.5728
Training Epoch: 34 [41088/50048]	Loss: 0.5262
Training Epoch: 34 [41216/50048]	Loss: 0.4995
Training Epoch: 34 [41344/50048]	Loss: 0.5521
Training Epoch: 34 [41472/50048]	Loss: 0.5065
Training Epoch: 34 [41600/50048]	Loss: 0.5810
Training Epoch: 34 [41728/50048]	Loss: 0.7093
Training Epoch: 34 [41856/50048]	Loss: 0.7006
Training Epoch: 34 [41984/50048]	Loss: 0.4989
Training Epoch: 34 [42112/50048]	Loss: 0.5161
Training Epoch: 34 [42240/50048]	Loss: 0.5165
Training Epoch: 34 [42368/50048]	Loss: 0.5317
Training Epoch: 34 [42496/50048]	Loss: 0.5431
Training Epoch: 34 [42624/50048]	Loss: 0.5129
Training Epoch: 34 [42752/50048]	Loss: 0.5845
Training Epoch: 34 [42880/50048]	Loss: 0.5087
Training Epoch: 34 [43008/50048]	Loss: 0.7057
Training Epoch: 34 [43136/50048]	Loss: 0.4809
Training Epoch: 34 [43264/50048]	Loss: 0.6548
Training Epoch: 34 [43392/50048]	Loss: 0.5090
Training Epoch: 34 [43520/50048]	Loss: 0.7282
Training Epoch: 34 [43648/50048]	Loss: 0.7880
Training Epoch: 34 [43776/50048]	Loss: 0.4987
Training Epoch: 34 [43904/50048]	Loss: 0.5624
Training Epoch: 34 [44032/50048]	Loss: 0.5627
Training Epoch: 34 [44160/50048]	Loss: 0.6513
Training Epoch: 34 [44288/50048]	Loss: 0.7190
Training Epoch: 34 [44416/50048]	Loss: 0.6728
Training Epoch: 34 [44544/50048]	Loss: 0.5890
Training Epoch: 34 [44672/50048]	Loss: 0.5788
Training Epoch: 34 [44800/50048]	Loss: 0.6035
Training Epoch: 34 [44928/50048]	Loss: 0.5894
Training Epoch: 34 [45056/50048]	Loss: 0.5904
Training Epoch: 34 [45184/50048]	Loss: 0.6678
Training Epoch: 34 [45312/50048]	Loss: 0.5128
Training Epoch: 34 [45440/50048]	Loss: 0.4171
Training Epoch: 34 [45568/50048]	Loss: 0.7840
Training Epoch: 34 [45696/50048]	Loss: 0.5614
2022-12-06 07:00:27,375 [ZeusDataLoader(train)] train epoch 35 done: time=86.45 energy=10497.95
2022-12-06 07:00:27,377 [ZeusDataLoader(eval)] Epoch 35 begin.
Training Epoch: 34 [45824/50048]	Loss: 0.6537
Training Epoch: 34 [45952/50048]	Loss: 0.5839
Training Epoch: 34 [46080/50048]	Loss: 0.5869
Training Epoch: 34 [46208/50048]	Loss: 0.5844
Training Epoch: 34 [46336/50048]	Loss: 0.5624
Training Epoch: 34 [46464/50048]	Loss: 0.5412
Training Epoch: 34 [46592/50048]	Loss: 0.8093
Training Epoch: 34 [46720/50048]	Loss: 0.6054
Training Epoch: 34 [46848/50048]	Loss: 0.6458
Training Epoch: 34 [46976/50048]	Loss: 0.6141
Training Epoch: 34 [47104/50048]	Loss: 0.7728
Training Epoch: 34 [47232/50048]	Loss: 0.6667
Training Epoch: 34 [47360/50048]	Loss: 0.6147
Training Epoch: 34 [47488/50048]	Loss: 0.4171
Training Epoch: 34 [47616/50048]	Loss: 0.4310
Training Epoch: 34 [47744/50048]	Loss: 0.5788
Training Epoch: 34 [47872/50048]	Loss: 0.6984
Training Epoch: 34 [48000/50048]	Loss: 0.6157
Training Epoch: 34 [48128/50048]	Loss: 0.7652
Training Epoch: 34 [48256/50048]	Loss: 0.7838
Training Epoch: 34 [48384/50048]	Loss: 0.4795
Training Epoch: 34 [48512/50048]	Loss: 0.5883
Training Epoch: 34 [48640/50048]	Loss: 0.7711
Training Epoch: 34 [48768/50048]	Loss: 0.5015
Training Epoch: 34 [48896/50048]	Loss: 0.4269
Training Epoch: 34 [49024/50048]	Loss: 0.4699
Training Epoch: 34 [49152/50048]	Loss: 0.5701
Training Epoch: 34 [49280/50048]	Loss: 0.7358
Training Epoch: 34 [49408/50048]	Loss: 0.5290
Training Epoch: 34 [49536/50048]	Loss: 0.4681
Training Epoch: 34 [49664/50048]	Loss: 0.6670
Training Epoch: 34 [49792/50048]	Loss: 0.6275
Training Epoch: 34 [49920/50048]	Loss: 0.6701
Training Epoch: 34 [50048/50048]	Loss: 0.5260
2022-12-06 12:00:31.021 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:00:31,034 [ZeusDataLoader(eval)] eval epoch 35 done: time=3.65 energy=442.45
2022-12-06 07:00:31,034 [ZeusDataLoader(train)] Up to epoch 35: time=3156.65, energy=383217.07, cost=467815.75
2022-12-06 07:00:31,034 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:00:31,034 [ZeusDataLoader(train)] Expected next epoch: time=3246.45, energy=394015.09, cost=481072.14
2022-12-06 07:00:31,035 [ZeusDataLoader(train)] Epoch 36 begin.
Validation Epoch: 34, Average loss: 0.0126, Accuracy: 0.6282
2022-12-06 07:00:31,226 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:00:31,227 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:00:31.229 [ZeusMonitor] Monitor started.
2022-12-06 12:00:31.229 [ZeusMonitor] Running indefinitely. 2022-12-06 12:00:31.229 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:00:31.229 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e36+gpu0.power.log
Training Epoch: 35 [128/50048]	Loss: 0.3738
Training Epoch: 35 [256/50048]	Loss: 0.5053
Training Epoch: 35 [384/50048]	Loss: 0.4467
Training Epoch: 35 [512/50048]	Loss: 0.4251
Training Epoch: 35 [640/50048]	Loss: 0.5906
Training Epoch: 35 [768/50048]	Loss: 0.4095
Training Epoch: 35 [896/50048]	Loss: 0.3367
Training Epoch: 35 [1024/50048]	Loss: 0.6090
Training Epoch: 35 [1152/50048]	Loss: 0.4163
Training Epoch: 35 [1280/50048]	Loss: 0.4985
Training Epoch: 35 [1408/50048]	Loss: 0.4271
Training Epoch: 35 [1536/50048]	Loss: 0.5887
Training Epoch: 35 [1664/50048]	Loss: 0.4223
Training Epoch: 35 [1792/50048]	Loss: 0.4926
Training Epoch: 35 [1920/50048]	Loss: 0.4829
Training Epoch: 35 [2048/50048]	Loss: 0.4614
Training Epoch: 35 [2176/50048]	Loss: 0.4421
Training Epoch: 35 [2304/50048]	Loss: 0.3874
Training Epoch: 35 [2432/50048]	Loss: 0.5055
Training Epoch: 35 [2560/50048]	Loss: 0.5138
Training Epoch: 35 [2688/50048]	Loss: 0.5807
Training Epoch: 35 [2816/50048]	Loss: 0.5127
Training Epoch: 35 [2944/50048]	Loss: 0.5215
Training Epoch: 35 [3072/50048]	Loss: 0.5161
Training Epoch: 35 [3200/50048]	Loss: 0.3713
Training Epoch: 35 [3328/50048]	Loss: 0.4083
Training Epoch: 35 [3456/50048]	Loss: 0.3999
Training Epoch: 35 [3584/50048]	Loss: 0.4087
Training Epoch: 35 [3712/50048]	Loss: 0.5383
Training Epoch: 35 [3840/50048]	Loss: 0.5512
Training Epoch: 35 [3968/50048]	Loss: 0.5664
Training Epoch: 35 [4096/50048]	Loss: 0.5269
Training Epoch: 35 [4224/50048]	Loss: 0.5757
Training Epoch: 35 [4352/50048]	Loss: 0.4155
Training Epoch: 35 [4480/50048]	Loss: 0.2681
Training Epoch: 35 [4608/50048]	Loss: 0.5744
Training Epoch: 35 [4736/50048]	Loss: 0.6960
Training Epoch: 35 [4864/50048]	Loss: 0.4000
Training Epoch: 35 [4992/50048]	Loss: 0.5684
Training Epoch: 35 [5120/50048]	Loss: 0.4587
Training Epoch: 35 [5248/50048]	Loss: 0.4537
Training Epoch: 35 [5376/50048]	Loss: 0.5539
Training Epoch: 35 [5504/50048]	Loss: 0.4776
Training Epoch: 35 [5632/50048]	Loss: 0.3797
Training Epoch: 35 [5760/50048]	Loss: 0.5042
Training Epoch: 35 [5888/50048]	Loss: 0.3420
Training Epoch: 35 [6016/50048]	Loss: 0.5507
Training Epoch: 35 [6144/50048]	Loss: 0.4772
Training Epoch: 35 [6272/50048]	Loss: 0.5132
Training Epoch: 35 [6400/50048]	Loss: 0.6717
Training Epoch: 35 [6528/50048]	Loss: 0.3663
Training Epoch: 35 [6656/50048]	Loss: 0.6943
Training Epoch: 35 [6784/50048]	Loss: 0.5469
Training Epoch: 35 [6912/50048]	Loss: 0.4834
Training Epoch: 35 [7040/50048]	Loss: 0.3813
Training Epoch: 35 [7168/50048]	Loss: 0.5970
Training Epoch: 35 [7296/50048]	Loss: 0.3309
Training Epoch: 35 [7424/50048]	Loss: 0.5130
Training Epoch: 35 [7552/50048]	Loss: 0.3234
Training Epoch: 35 [7680/50048]	Loss: 0.2948
Training Epoch: 35 [7808/50048]	Loss: 0.5339
Training Epoch: 35 [7936/50048]	Loss: 0.4075
Training Epoch: 35 [8064/50048]	Loss: 0.4320
Training Epoch: 35 [8192/50048]	Loss: 0.5723
Training Epoch: 35 [8320/50048]	Loss: 0.5078
Training Epoch: 35 [8448/50048]	Loss: 0.3927
Training Epoch: 35 [8576/50048]	Loss: 0.5441
Training Epoch: 35 [8704/50048]	Loss: 0.6434
Training Epoch: 35 [8832/50048]	Loss: 0.4546
Training Epoch: 35 [8960/50048]	Loss: 0.4989
Training Epoch: 35 [9088/50048]	Loss: 0.4276
Training Epoch: 35 [9216/50048]	Loss: 0.4013
Training Epoch: 35 [9344/50048]	Loss: 0.6072
Training Epoch: 35 [9472/50048]	Loss: 0.6005
Training Epoch: 35 [9600/50048]	Loss: 0.5025
Training Epoch: 35 [9728/50048]	Loss: 0.4554
Training Epoch: 35 [9856/50048]	Loss: 0.5276
Training Epoch: 35 [9984/50048]	Loss: 0.4118
Training Epoch: 35 [10112/50048]	Loss: 0.5290
Training Epoch: 35 [10240/50048]	Loss: 0.4154
Training Epoch: 35 [10368/50048]	Loss: 0.5011
Training Epoch: 35 [10496/50048]	Loss: 0.6714
Training Epoch: 35 [10624/50048]	Loss: 0.5082
Training Epoch: 35 [10752/50048]	Loss: 0.5431
Training Epoch: 35 [10880/50048]	Loss: 0.7519
Training Epoch: 35 [11008/50048]	Loss: 0.5490
Training Epoch: 35 [11136/50048]	Loss: 0.3814
Training Epoch: 35 [11264/50048]	Loss: 0.4087
Training Epoch: 35 [11392/50048]	Loss: 0.4777
Training Epoch: 35 [11520/50048]	Loss: 0.5497
Training Epoch: 35 [11648/50048]	Loss: 0.5300
Training Epoch: 35 [11776/50048]	Loss: 0.5768
Training Epoch: 35 [11904/50048]	Loss: 0.4874
Training Epoch: 35 [12032/50048]	Loss: 0.4984
Training Epoch: 35 [12160/50048]	Loss: 0.4600
Training Epoch: 35 [12288/50048]	Loss: 0.4370
Training Epoch: 35 [12416/50048]	Loss: 0.3602
Training Epoch: 35 [12544/50048]	Loss: 0.4799
Training Epoch: 35 [12672/50048]	Loss: 0.5377
Training Epoch: 35 [12800/50048]	Loss: 0.4595
Training Epoch: 35 [12928/50048]	Loss: 0.3267
Training Epoch: 35 [13056/50048]	Loss: 0.6401
Training Epoch: 35 [13184/50048]	Loss: 0.5826
Training Epoch: 35 [13312/50048]	Loss: 0.4868
Training Epoch: 35 [13440/50048]	Loss: 0.3830
Training Epoch: 35 [13568/50048]	Loss: 0.5895
Training Epoch: 35 [13696/50048]	Loss: 0.5502
Training Epoch: 35 [13824/50048]	Loss: 0.4135
Training Epoch: 35 [13952/50048]	Loss: 0.5611
Training Epoch: 35 [14080/50048]	Loss: 0.5841
Training Epoch: 35 [14208/50048]	Loss: 0.5190
Training Epoch: 35 [14336/50048]	Loss: 0.4505
Training Epoch: 35 [14464/50048]	Loss: 0.4491
Training Epoch: 35 [14592/50048]	Loss: 0.4328
Training Epoch: 35 [14720/50048]	Loss: 0.5979
Training Epoch: 35 [14848/50048]	Loss: 0.5708
Training Epoch: 35 [14976/50048]	Loss: 0.4808
Training Epoch: 35 [15104/50048]	Loss: 0.3980
Training Epoch: 35 [15232/50048]	Loss: 0.4950
Training Epoch: 35 [15360/50048]	Loss: 0.3945
Training Epoch: 35 [15488/50048]	Loss: 0.5097
Training Epoch: 35 [15616/50048]	Loss: 0.5439
Training Epoch: 35 [15744/50048]	Loss: 0.5826
Training Epoch: 35 [15872/50048]	Loss: 0.6251
Training Epoch: 35 [16000/50048]	Loss: 0.2824
Training Epoch: 35 [16128/50048]	Loss: 0.3534
Training Epoch: 35 [16256/50048]	Loss: 0.5883
Training Epoch: 35 [16384/50048]	Loss: 0.4743
Training Epoch: 35 [16512/50048]	Loss: 0.6151
Training Epoch: 35 [16640/50048]	Loss: 0.3955
Training Epoch: 35 [16768/50048]	Loss: 0.5437
Training Epoch: 35 [16896/50048]	Loss: 0.4741
Training Epoch: 35 [17024/50048]	Loss: 0.5202
Training Epoch: 35 [17152/50048]	Loss: 0.5811
Training Epoch: 35 [17280/50048]	Loss: 0.4510
Training Epoch: 35 [17408/50048]	Loss: 0.5959
Training Epoch: 35 [17536/50048]	Loss: 0.6343
Training Epoch: 35 [17664/50048]	Loss: 0.4425
Training Epoch: 35 [17792/50048]	Loss: 0.5252
Training Epoch: 35 [17920/50048]	Loss: 0.6242
Training Epoch: 35 [18048/50048]	Loss: 0.6251
Training Epoch: 35 [18176/50048]	Loss: 0.3492
Training Epoch: 35 [18304/50048]	Loss: 0.4963
Training Epoch: 35 [18432/50048]	Loss: 0.5289
Training Epoch: 35 [18560/50048]	Loss: 0.5349
Training Epoch: 35 [18688/50048]	Loss: 0.4486
Training Epoch: 35 [18816/50048]	Loss: 0.4905
Training Epoch: 35 [18944/50048]	Loss: 0.6272
Training Epoch: 35 [19072/50048]	Loss: 0.5317
Training Epoch: 35 [19200/50048]	Loss: 0.4436
Training Epoch: 35 [19328/50048]	Loss: 0.5319
Training Epoch: 35 [19456/50048]	Loss: 0.5908
Training Epoch: 35 [19584/50048]	Loss: 0.3796
Training Epoch: 35 [19712/50048]	Loss: 0.4555
Training Epoch: 35 [19840/50048]	Loss: 0.5820
Training Epoch: 35 [19968/50048]	Loss: 0.5133
Training Epoch: 35 [20096/50048]	Loss: 0.4573
Training Epoch: 35 [20224/50048]	Loss: 0.5097
Training Epoch: 35 [20352/50048]	Loss: 0.5273
Training Epoch: 35 [20480/50048]	Loss: 0.4593
Training Epoch: 35 [20608/50048]	Loss: 0.5882
Training Epoch: 35 [20736/50048]	Loss: 0.4726
Training Epoch: 35 [20864/50048]	Loss: 0.4420
Training Epoch: 35 [20992/50048]	Loss: 0.3964
Training Epoch: 35 [21120/50048]	Loss: 0.5849
Training Epoch: 35 [21248/50048]	Loss: 0.5587
Training Epoch: 35 [21376/50048]	Loss: 0.4644
Training Epoch: 35 [21504/50048]	Loss: 0.7088
Training Epoch: 35 [21632/50048]	Loss: 0.6882
Training Epoch: 35 [21760/50048]	Loss: 0.3948
Training Epoch: 35 [21888/50048]	Loss: 0.4880
Training Epoch: 35 [22016/50048]	Loss: 0.5558
Training Epoch: 35 [22144/50048]	Loss: 0.4668
Training Epoch: 35 [22272/50048]	Loss: 0.6352
Training Epoch: 35 [22400/50048]	Loss: 0.5950
Training Epoch: 35 [22528/50048]	Loss: 0.6692
Training Epoch: 35 [22656/50048]	Loss: 0.5546
Training Epoch: 35 [22784/50048]	Loss: 0.5714
Training Epoch: 35 [22912/50048]	Loss: 0.4335
Training Epoch: 35 [23040/50048]	Loss: 0.5083
Training Epoch: 35 [23168/50048]	Loss: 0.5537
Training Epoch: 35 [23296/50048]	Loss: 0.5820
Training Epoch: 35 [23424/50048]	Loss: 0.5224
Training Epoch: 35 [23552/50048]	Loss: 0.6326
Training Epoch: 35 [23680/50048]	Loss: 0.5734
Training Epoch: 35 [23808/50048]	Loss: 0.6070
Training Epoch: 35 [23936/50048]	Loss: 0.6932
Training Epoch: 35 [24064/50048]	Loss: 0.5142
Training Epoch: 35 [24192/50048]	Loss: 0.5071
Training Epoch: 35 [24320/50048]	Loss: 0.4625
Training Epoch: 35 [24448/50048]	Loss: 0.5921
Training Epoch: 35 [24576/50048]	Loss: 0.5961
Training Epoch: 35 [24704/50048]	Loss: 0.6124
Training Epoch: 35 [24832/50048]	Loss: 0.6585
Training Epoch: 35 [24960/50048]	Loss: 0.5726
Training Epoch: 35 [25088/50048]	Loss: 0.5240
Training Epoch: 35 [25216/50048]	Loss: 0.5411
Training Epoch: 35 [25344/50048]	Loss: 0.4912
Training Epoch: 35 [25472/50048]	Loss: 0.6065
Training Epoch: 35 [25600/50048]	Loss: 0.4931
Training Epoch: 35 [25728/50048]	Loss: 0.4950
Training Epoch: 35 [25856/50048]	Loss: 0.6595
Training Epoch: 35 [25984/50048]	Loss: 0.4609
Training Epoch: 35 [26112/50048]	Loss: 0.4565
Training Epoch: 35 [26240/50048]	Loss: 0.4557
Training Epoch: 35 [26368/50048]	Loss: 0.6714
Training Epoch: 35 [26496/50048]	Loss: 0.4489
Training Epoch: 35 [26624/50048]	Loss: 0.5439
Training Epoch: 35 [26752/50048]	Loss: 0.5988
Training Epoch: 35 [26880/50048]	Loss: 0.5323
Training Epoch: 35 [27008/50048]	Loss: 0.4768
Training Epoch: 35 [27136/50048]	Loss: 0.6313
Training Epoch: 35 [27264/50048]	Loss: 0.5727
Training Epoch: 35 [27392/50048]	Loss: 0.4609
Training Epoch: 35 [27520/50048]	Loss: 0.5477
Training Epoch: 35 [27648/50048]	Loss: 0.5414
Training Epoch: 35 [27776/50048]	Loss: 0.5310
Training Epoch: 35 [27904/50048]	Loss: 0.5863
Training Epoch: 35 [28032/50048]	Loss: 0.6142
Training Epoch: 35 [28160/50048]	Loss: 0.4603
Training Epoch: 35 [28288/50048]	Loss: 0.6495
Training Epoch: 35 [28416/50048]	Loss: 0.7292
Training Epoch: 35 [28544/50048]	Loss: 0.7314
Training Epoch: 35 [28672/50048]	Loss: 0.5691
Training Epoch: 35 [28800/50048]	Loss: 0.4824
Training Epoch: 35 [28928/50048]	Loss: 0.4166
Training Epoch: 35 [29056/50048]	Loss: 0.5109
Training Epoch: 35 [29184/50048]	Loss: 0.5856
Training Epoch: 35 [29312/50048]	Loss: 0.4635
Training Epoch: 35 [29440/50048]	Loss: 0.4615
Training Epoch: 35 [29568/50048]	Loss: 0.5084
Training Epoch: 35 [29696/50048]	Loss: 0.6718
Training Epoch: 35 [29824/50048]	Loss: 0.5256
Training Epoch: 35 [29952/50048]	Loss: 0.3881
Training Epoch: 35 [30080/50048]	Loss: 0.4282
Training Epoch: 35 [30208/50048]	Loss: 0.5333
Training Epoch: 35 [30336/50048]	Loss: 0.6150
Training Epoch: 35 [30464/50048]	Loss: 0.7318
Training Epoch: 35 [30592/50048]	Loss: 0.3815
Training Epoch: 35 [30720/50048]	Loss: 0.4505
Training Epoch: 35 [30848/50048]	Loss: 0.4872
Training Epoch: 35 [30976/50048]	Loss: 0.4239
Training Epoch: 35 [31104/50048]	Loss: 0.4099
Training Epoch: 35 [31232/50048]	Loss: 0.3982
Training Epoch: 35 [31360/50048]	Loss: 0.4525
Training Epoch: 35 [31488/50048]	Loss: 0.5672
Training Epoch: 35 [31616/50048]	Loss: 0.5902
Training Epoch: 35 [31744/50048]	Loss: 0.4328
Training Epoch: 35 [31872/50048]	Loss: 0.3482
Training Epoch: 35 [32000/50048]	Loss: 0.5673
Training Epoch: 35 [32128/50048]	Loss: 0.5999
Training Epoch: 35 [32256/50048]	Loss: 0.5282
Training Epoch: 35 [32384/50048]	Loss: 0.4844
Training Epoch: 35 [32512/50048]	Loss: 0.5282
Training Epoch: 35 [32640/50048]	Loss: 0.4649
Training Epoch: 35 [32768/50048]	Loss: 0.5658
Training Epoch: 35 [32896/50048]	Loss: 0.4292
Training Epoch: 35 [33024/50048]	Loss: 0.5363
Training Epoch: 35 [33152/50048]	Loss: 0.6417
Training Epoch: 35 [33280/50048]	Loss: 0.4259
Training Epoch: 35 [33408/50048]	Loss: 0.7064
Training Epoch: 35 [33536/50048]	Loss: 0.3699
Training Epoch: 35 [33664/50048]	Loss: 0.3837
Training Epoch: 35 [33792/50048]	Loss: 0.5196
Training Epoch: 35 [33920/50048]	Loss: 0.5498
Training Epoch: 35 [34048/50048]	Loss: 0.5327
Training Epoch: 35 [34176/50048]	Loss: 0.5751
Training Epoch: 35 [34304/50048]	Loss: 0.5229
Training Epoch: 35 [34432/50048]	Loss: 0.6073
Training Epoch: 35 [34560/50048]	Loss: 0.4799
Training Epoch: 35 [34688/50048]	Loss: 0.6063
Training Epoch: 35 [34816/50048]	Loss: 0.5303
Training Epoch: 35 [34944/50048]	Loss: 0.4490
Training Epoch: 35 [35072/50048]	Loss: 0.5303
Training Epoch: 35 [35200/50048]	Loss: 0.4790
Training Epoch: 35 [35328/50048]	Loss: 0.6056
Training Epoch: 35 [35456/50048]	Loss: 0.7153
Training Epoch: 35 [35584/50048]	Loss: 0.5147
Training Epoch: 35 [35712/50048]	Loss: 0.3868
Training Epoch: 35 [35840/50048]	Loss: 0.5320
Training Epoch: 35 [35968/50048]	Loss: 0.5532
Training Epoch: 35 [36096/50048]	Loss: 0.5421
Training Epoch: 35 [36224/50048]	Loss: 0.6258
Training Epoch: 35 [36352/50048]	Loss: 0.4889
Training Epoch: 35 [36480/50048]	Loss: 0.6704
Training Epoch: 35 [36608/50048]	Loss: 0.5935
Training Epoch: 35 [36736/50048]	Loss: 0.4795
Training Epoch: 35 [36864/50048]	Loss: 0.6415
Training Epoch: 35 [36992/50048]	Loss: 0.6076
Training Epoch: 35 [37120/50048]	Loss: 0.5269
Training Epoch: 35 [37248/50048]	Loss: 0.6567
Training Epoch: 35 [37376/50048]	Loss: 0.4998
Training Epoch: 35 [37504/50048]	Loss: 0.7008
Training Epoch: 35 [37632/50048]	Loss: 0.4672
Training Epoch: 35 [37760/50048]	Loss: 0.4356
Training Epoch: 35 [37888/50048]	Loss: 0.5341
Training Epoch: 35 [38016/50048]	Loss: 0.5800
Training Epoch: 35 [38144/50048]	Loss: 0.5054
Training Epoch: 35 [38272/50048]	Loss: 0.5901
Training Epoch: 35 [38400/50048]	Loss: 0.5952
Training Epoch: 35 [38528/50048]	Loss: 0.5189
Training Epoch: 35 [38656/50048]	Loss: 0.6133
Training Epoch: 35 [38784/50048]	Loss: 0.6447
Training Epoch: 35 [38912/50048]	Loss: 0.4364
Training Epoch: 35 [39040/50048]	Loss: 0.6075
Training Epoch: 35 [39168/50048]	Loss: 0.6555
Training Epoch: 35 [39296/50048]	Loss: 0.6352
Training Epoch: 35 [39424/50048]	Loss: 0.5282
Training Epoch: 35 [39552/50048]	Loss: 0.5051
Training Epoch: 35 [39680/50048]	Loss: 0.4199
Training Epoch: 35 [39808/50048]	Loss: 0.6783
Training Epoch: 35 [39936/50048]	Loss: 0.4969
Training Epoch: 35 [40064/50048]	Loss: 0.4693
Training Epoch: 35 [40192/50048]	Loss: 0.5592
Training Epoch: 35 [40320/50048]	Loss: 0.4311
Training Epoch: 35 [40448/50048]	Loss: 0.4186
Training Epoch: 35 [40576/50048]	Loss: 0.4953
Training Epoch: 35 [40704/50048]	Loss: 0.5266
Training Epoch: 35 [40832/50048]	Loss: 0.6986
Training Epoch: 35 [40960/50048]	Loss: 0.4857
Training Epoch: 35 [41088/50048]	Loss: 0.5424
Training Epoch: 35 [41216/50048]	Loss: 0.7367
Training Epoch: 35 [41344/50048]	Loss: 0.6918
Training Epoch: 35 [41472/50048]	Loss: 0.6270
Training Epoch: 35 [41600/50048]	Loss: 0.4839
Training Epoch: 35 [41728/50048]	Loss: 0.4923
Training Epoch: 35 [41856/50048]	Loss: 0.4270
Training Epoch: 35 [41984/50048]	Loss: 0.4851
Training Epoch: 35 [42112/50048]	Loss: 0.5129
Training Epoch: 35 [42240/50048]	Loss: 0.5183
Training Epoch: 35 [42368/50048]	Loss: 0.5356
Training Epoch: 35 [42496/50048]	Loss: 0.4941
Training Epoch: 35 [42624/50048]	Loss: 0.8404
Training Epoch: 35 [42752/50048]	Loss: 0.6071
Training Epoch: 35 [42880/50048]	Loss: 0.5395
Training Epoch: 35 [43008/50048]	Loss: 0.5500
Training Epoch: 35 [43136/50048]	Loss: 0.4994
Training Epoch: 35 [43264/50048]	Loss: 0.4623
Training Epoch: 35 [43392/50048]	Loss: 0.6701
Training Epoch: 35 [43520/50048]	Loss: 0.5280
Training Epoch: 35 [43648/50048]	Loss: 0.6624
Training Epoch: 35 [43776/50048]	Loss: 0.5256
Training Epoch: 35 [43904/50048]	Loss: 0.7240
Training Epoch: 35 [44032/50048]	Loss: 0.4555
Training Epoch: 35 [44160/50048]	Loss: 0.6066
Training Epoch: 35 [44288/50048]	Loss: 0.5305
Training Epoch: 35 [44416/50048]	Loss: 0.6830
Training Epoch: 35 [44544/50048]	Loss: 0.4718
Training Epoch: 35 [44672/50048]	Loss: 0.5989
Training Epoch: 35 [44800/50048]	Loss: 0.6142
Training Epoch: 35 [44928/50048]	Loss: 0.6148
Training Epoch: 35 [45056/50048]	Loss: 0.6535
Training Epoch: 35 [45184/50048]	Loss: 0.6673
Training Epoch: 35 [45312/50048]	Loss: 0.5151
Training Epoch: 35 [45440/50048]	Loss: 0.5673
Training Epoch: 35 [45568/50048]	Loss: 0.5831
Training Epoch: 35 [45696/50048]	Loss: 0.4868
2022-12-06 07:01:57,575 [ZeusDataLoader(train)] train epoch 36 done: time=86.53 energy=10505.76
2022-12-06 07:01:57,577 [ZeusDataLoader(eval)] Epoch 36 begin.
Training Epoch: 35 [45824/50048]	Loss: 0.7114
Training Epoch: 35 [45952/50048]	Loss: 0.8126
Training Epoch: 35 [46080/50048]	Loss: 0.6258
Training Epoch: 35 [46208/50048]	Loss: 0.6528
Training Epoch: 35 [46336/50048]	Loss: 0.5814
Training Epoch: 35 [46464/50048]	Loss: 0.6321
Training Epoch: 35 [46592/50048]	Loss: 0.6005
Training Epoch: 35 [46720/50048]	Loss: 0.4058
Training Epoch: 35 [46848/50048]	Loss: 0.6535
Training Epoch: 35 [46976/50048]	Loss: 0.3841
Training Epoch: 35 [47104/50048]	Loss: 0.6757
Training Epoch: 35 [47232/50048]	Loss: 0.5186
Training Epoch: 35 [47360/50048]	Loss: 0.7083
Training Epoch: 35 [47488/50048]	Loss: 0.6526
Training Epoch: 35 [47616/50048]	Loss: 0.6926
Training Epoch: 35 [47744/50048]	Loss: 0.5841
Training Epoch: 35 [47872/50048]	Loss: 0.5449
Training Epoch: 35 [48000/50048]	Loss: 0.4991
Training Epoch: 35 [48128/50048]	Loss: 0.5760
Training Epoch: 35 [48256/50048]	Loss: 0.6333
Training Epoch: 35 [48384/50048]	Loss: 0.6058
Training Epoch: 35 [48512/50048]	Loss: 0.5988
Training Epoch: 35 [48640/50048]	Loss: 0.8782
Training Epoch: 35 [48768/50048]	Loss: 0.6653
Training Epoch: 35 [48896/50048]	Loss: 0.5987
Training Epoch: 35 [49024/50048]	Loss: 0.6377
Training Epoch: 35 [49152/50048]	Loss: 0.5324
Training Epoch: 35 [49280/50048]	Loss: 0.5933
Training Epoch: 35 [49408/50048]	Loss: 0.5702
Training Epoch: 35 [49536/50048]	Loss: 0.5667
Training Epoch: 35 [49664/50048]	Loss: 0.5138
Training Epoch: 35 [49792/50048]	Loss: 0.4530
Training Epoch: 35 [49920/50048]	Loss: 0.5613
Training Epoch: 35 [50048/50048]	Loss: 0.5506
2022-12-06 12:02:01.241 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:02:01,262 [ZeusDataLoader(eval)] eval epoch 36 done: time=3.68 energy=443.35
2022-12-06 07:02:01,262 [ZeusDataLoader(train)] Up to epoch 36: time=3246.86, energy=394166.18, cost=481183.24
2022-12-06 07:02:01,262 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:02:01,262 [ZeusDataLoader(train)] Expected next epoch: time=3336.66, energy=404964.20, cost=494439.63
2022-12-06 07:02:01,263 [ZeusDataLoader(train)] Epoch 37 begin.
Validation Epoch: 35, Average loss: 0.0130, Accuracy: 0.6218
2022-12-06 07:02:01,454 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:02:01,455 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:02:01.465 [ZeusMonitor] Monitor started.
2022-12-06 12:02:01.465 [ZeusMonitor] Running indefinitely. 2022-12-06 12:02:01.465 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:02:01.465 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e37+gpu0.power.log
Training Epoch: 36 [128/50048]	Loss: 0.6193
Training Epoch: 36 [256/50048]	Loss: 0.3774
Training Epoch: 36 [384/50048]	Loss: 0.4063
Training Epoch: 36 [512/50048]	Loss: 0.3936
Training Epoch: 36 [640/50048]	Loss: 0.4147
Training Epoch: 36 [768/50048]	Loss: 0.4023
Training Epoch: 36 [896/50048]	Loss: 0.5163
Training Epoch: 36 [1024/50048]	Loss: 0.4343
Training Epoch: 36 [1152/50048]	Loss: 0.3807
Training Epoch: 36 [1280/50048]	Loss: 0.4089
Training Epoch: 36 [1408/50048]	Loss: 0.4880
Training Epoch: 36 [1536/50048]	Loss: 0.4435
Training Epoch: 36 [1664/50048]	Loss: 0.4950
Training Epoch: 36 [1792/50048]	Loss: 0.4882
Training Epoch: 36 [1920/50048]	Loss: 0.5895
Training Epoch: 36 [2048/50048]	Loss: 0.4254
Training Epoch: 36 [2176/50048]	Loss: 0.4408
Training Epoch: 36 [2304/50048]	Loss: 0.5194
Training Epoch: 36 [2432/50048]	Loss: 0.4397
Training Epoch: 36 [2560/50048]	Loss: 0.3469
Training Epoch: 36 [2688/50048]	Loss: 0.3968
Training Epoch: 36 [2816/50048]	Loss: 0.4749
Training Epoch: 36 [2944/50048]	Loss: 0.4870
Training Epoch: 36 [3072/50048]	Loss: 0.3998
Training Epoch: 36 [3200/50048]	Loss: 0.4148
Training Epoch: 36 [3328/50048]	Loss: 0.5206
Training Epoch: 36 [3456/50048]	Loss: 0.3721
Training Epoch: 36 [3584/50048]	Loss: 0.4453
Training Epoch: 36 [3712/50048]	Loss: 0.3879
Training Epoch: 36 [3840/50048]	Loss: 0.4859
Training Epoch: 36 [3968/50048]	Loss: 0.5695
Training Epoch: 36 [4096/50048]	Loss: 0.5661
Training Epoch: 36 [4224/50048]	Loss: 0.5229
Training Epoch: 36 [4352/50048]	Loss: 0.3892
Training Epoch: 36 [4480/50048]	Loss: 0.4233
Training Epoch: 36 [4608/50048]	Loss: 0.5337
Training Epoch: 36 [4736/50048]	Loss: 0.4444
Training Epoch: 36 [4864/50048]	Loss: 0.4164
Training Epoch: 36 [4992/50048]	Loss: 0.5033
Training Epoch: 36 [5120/50048]	Loss: 0.5144
Training Epoch: 36 [5248/50048]	Loss: 0.4828
Training Epoch: 36 [5376/50048]	Loss: 0.4449
Training Epoch: 36 [5504/50048]	Loss: 0.4902
Training Epoch: 36 [5632/50048]	Loss: 0.5216
Training Epoch: 36 [5760/50048]	Loss: 0.4164
Training Epoch: 36 [5888/50048]	Loss: 0.4157
Training Epoch: 36 [6016/50048]	Loss: 0.5090
Training Epoch: 36 [6144/50048]	Loss: 0.4498
Training Epoch: 36 [6272/50048]	Loss: 0.4911
Training Epoch: 36 [6400/50048]	Loss: 0.4323
Training Epoch: 36 [6528/50048]	Loss: 0.5556
Training Epoch: 36 [6656/50048]	Loss: 0.4644
Training Epoch: 36 [6784/50048]	Loss: 0.4735
Training Epoch: 36 [6912/50048]	Loss: 0.4597
Training Epoch: 36 [7040/50048]	Loss: 0.4143
Training Epoch: 36 [7168/50048]	Loss: 0.3717
Training Epoch: 36 [7296/50048]	Loss: 0.5164
Training Epoch: 36 [7424/50048]	Loss: 0.5233
Training Epoch: 36 [7552/50048]	Loss: 0.4677
Training Epoch: 36 [7680/50048]	Loss: 0.3821
Training Epoch: 36 [7808/50048]	Loss: 0.4107
Training Epoch: 36 [7936/50048]	Loss: 0.5116
Training Epoch: 36 [8064/50048]	Loss: 0.4922
Training Epoch: 36 [8192/50048]	Loss: 0.3952
Training Epoch: 36 [8320/50048]	Loss: 0.3663
Training Epoch: 36 [8448/50048]	Loss: 0.4066
Training Epoch: 36 [8576/50048]	Loss: 0.5100
Training Epoch: 36 [8704/50048]	Loss: 0.5762
Training Epoch: 36 [8832/50048]	Loss: 0.5376
Training Epoch: 36 [8960/50048]	Loss: 0.4698
Training Epoch: 36 [9088/50048]	Loss: 0.5697
Training Epoch: 36 [9216/50048]	Loss: 0.4437
Training Epoch: 36 [9344/50048]	Loss: 0.3969
Training Epoch: 36 [9472/50048]	Loss: 0.5872
Training Epoch: 36 [9600/50048]	Loss: 0.4554
Training Epoch: 36 [9728/50048]	Loss: 0.5607
Training Epoch: 36 [9856/50048]	Loss: 0.6192
Training Epoch: 36 [9984/50048]	Loss: 0.6053
Training Epoch: 36 [10112/50048]	Loss: 0.5606
Training Epoch: 36 [10240/50048]	Loss: 0.4833
Training Epoch: 36 [10368/50048]	Loss: 0.4566
Training Epoch: 36 [10496/50048]	Loss: 0.5186
Training Epoch: 36 [10624/50048]	Loss: 0.5449
Training Epoch: 36 [10752/50048]	Loss: 0.6393
Training Epoch: 36 [10880/50048]	Loss: 0.4620
Training Epoch: 36 [11008/50048]	Loss: 0.5017
Training Epoch: 36 [11136/50048]	Loss: 0.4261
Training Epoch: 36 [11264/50048]	Loss: 0.4627
Training Epoch: 36 [11392/50048]	Loss: 0.6412
Training Epoch: 36 [11520/50048]	Loss: 0.5085
Training Epoch: 36 [11648/50048]	Loss: 0.5250
Training Epoch: 36 [11776/50048]	Loss: 0.4533
Training Epoch: 36 [11904/50048]	Loss: 0.4650
Training Epoch: 36 [12032/50048]	Loss: 0.2749
Training Epoch: 36 [12160/50048]	Loss: 0.4605
Training Epoch: 36 [12288/50048]	Loss: 0.3952
Training Epoch: 36 [12416/50048]	Loss: 0.4177
Training Epoch: 36 [12544/50048]	Loss: 0.4843
Training Epoch: 36 [12672/50048]	Loss: 0.4198
Training Epoch: 36 [12800/50048]	Loss: 0.5355
Training Epoch: 36 [12928/50048]	Loss: 0.4485
Training Epoch: 36 [13056/50048]	Loss: 0.4278
Training Epoch: 36 [13184/50048]	Loss: 0.7622
Training Epoch: 36 [13312/50048]	Loss: 0.5717
Training Epoch: 36 [13440/50048]	Loss: 0.5546
Training Epoch: 36 [13568/50048]	Loss: 0.3840
Training Epoch: 36 [13696/50048]	Loss: 0.3163
Training Epoch: 36 [13824/50048]	Loss: 0.4776
Training Epoch: 36 [13952/50048]	Loss: 0.5666
Training Epoch: 36 [14080/50048]	Loss: 0.4039
Training Epoch: 36 [14208/50048]	Loss: 0.4547
Training Epoch: 36 [14336/50048]	Loss: 0.4204
Training Epoch: 36 [14464/50048]	Loss: 0.5515
Training Epoch: 36 [14592/50048]	Loss: 0.4220
Training Epoch: 36 [14720/50048]	Loss: 0.6463
Training Epoch: 36 [14848/50048]	Loss: 0.4354
Training Epoch: 36 [14976/50048]	Loss: 0.5305
Training Epoch: 36 [15104/50048]	Loss: 0.5230
Training Epoch: 36 [15232/50048]	Loss: 0.4998
Training Epoch: 36 [15360/50048]	Loss: 0.5612
Training Epoch: 36 [15488/50048]	Loss: 0.4579
Training Epoch: 36 [15616/50048]	Loss: 0.4041
Training Epoch: 36 [15744/50048]	Loss: 0.4708
Training Epoch: 36 [15872/50048]	Loss: 0.4588
Training Epoch: 36 [16000/50048]	Loss: 0.4290
Training Epoch: 36 [16128/50048]	Loss: 0.4161
Training Epoch: 36 [16256/50048]	Loss: 0.5347
Training Epoch: 36 [16384/50048]	Loss: 0.5040
Training Epoch: 36 [16512/50048]	Loss: 0.4584
Training Epoch: 36 [16640/50048]	Loss: 0.5759
Training Epoch: 36 [16768/50048]	Loss: 0.4384
Training Epoch: 36 [16896/50048]	Loss: 0.4150
Training Epoch: 36 [17024/50048]	Loss: 0.5666
Training Epoch: 36 [17152/50048]	Loss: 0.4371
Training Epoch: 36 [17280/50048]	Loss: 0.5380
Training Epoch: 36 [17408/50048]	Loss: 0.5438
Training Epoch: 36 [17536/50048]	Loss: 0.4887
Training Epoch: 36 [17664/50048]	Loss: 0.5200
Training Epoch: 36 [17792/50048]	Loss: 0.3666
Training Epoch: 36 [17920/50048]	Loss: 0.3824
Training Epoch: 36 [18048/50048]	Loss: 0.5277
Training Epoch: 36 [18176/50048]	Loss: 0.4639
Training Epoch: 36 [18304/50048]	Loss: 0.6302
Training Epoch: 36 [18432/50048]	Loss: 0.5205
Training Epoch: 36 [18560/50048]	Loss: 0.5529
Training Epoch: 36 [18688/50048]	Loss: 0.4853
Training Epoch: 36 [18816/50048]	Loss: 0.4252
Training Epoch: 36 [18944/50048]	Loss: 0.4069
Training Epoch: 36 [19072/50048]	Loss: 0.6603
Training Epoch: 36 [19200/50048]	Loss: 0.5770
Training Epoch: 36 [19328/50048]	Loss: 0.4632
Training Epoch: 36 [19456/50048]	Loss: 0.4743
Training Epoch: 36 [19584/50048]	Loss: 0.5318
Training Epoch: 36 [19712/50048]	Loss: 0.4707
Training Epoch: 36 [19840/50048]	Loss: 0.5918
Training Epoch: 36 [19968/50048]	Loss: 0.4968
Training Epoch: 36 [20096/50048]	Loss: 0.3887
Training Epoch: 36 [20224/50048]	Loss: 0.5145
Training Epoch: 36 [20352/50048]	Loss: 0.5107
Training Epoch: 36 [20480/50048]	Loss: 0.4265
Training Epoch: 36 [20608/50048]	Loss: 0.4239
Training Epoch: 36 [20736/50048]	Loss: 0.3820
Training Epoch: 36 [20864/50048]	Loss: 0.7062
Training Epoch: 36 [20992/50048]	Loss: 0.6879
Training Epoch: 36 [21120/50048]	Loss: 0.3862
Training Epoch: 36 [21248/50048]	Loss: 0.4236
Training Epoch: 36 [21376/50048]	Loss: 0.5152
Training Epoch: 36 [21504/50048]	Loss: 0.4793
Training Epoch: 36 [21632/50048]	Loss: 0.5612
Training Epoch: 36 [21760/50048]	Loss: 0.6317
Training Epoch: 36 [21888/50048]	Loss: 0.4945
Training Epoch: 36 [22016/50048]	Loss: 0.6856
Training Epoch: 36 [22144/50048]	Loss: 0.6020
Training Epoch: 36 [22272/50048]	Loss: 0.5435
Training Epoch: 36 [22400/50048]	Loss: 0.4412
Training Epoch: 36 [22528/50048]	Loss: 0.6168
Training Epoch: 36 [22656/50048]	Loss: 0.5546
Training Epoch: 36 [22784/50048]	Loss: 0.3808
Training Epoch: 36 [22912/50048]	Loss: 0.4954
Training Epoch: 36 [23040/50048]	Loss: 0.4674
Training Epoch: 36 [23168/50048]	Loss: 0.3141
Training Epoch: 36 [23296/50048]	Loss: 0.5687
Training Epoch: 36 [23424/50048]	Loss: 0.4559
Training Epoch: 36 [23552/50048]	Loss: 0.3865
Training Epoch: 36 [23680/50048]	Loss: 0.4157
Training Epoch: 36 [23808/50048]	Loss: 0.4486
Training Epoch: 36 [23936/50048]	Loss: 0.5771
Training Epoch: 36 [24064/50048]	Loss: 0.5276
Training Epoch: 36 [24192/50048]	Loss: 0.6671
Training Epoch: 36 [24320/50048]	Loss: 0.5564
Training Epoch: 36 [24448/50048]	Loss: 0.5815
Training Epoch: 36 [24576/50048]	Loss: 0.5350
Training Epoch: 36 [24704/50048]	Loss: 0.4984
Training Epoch: 36 [24832/50048]	Loss: 0.6494
Training Epoch: 36 [24960/50048]	Loss: 0.5570
Training Epoch: 36 [25088/50048]	Loss: 0.5574
Training Epoch: 36 [25216/50048]	Loss: 0.5131
Training Epoch: 36 [25344/50048]	Loss: 0.6938
Training Epoch: 36 [25472/50048]	Loss: 0.5826
Training Epoch: 36 [25600/50048]	Loss: 0.5019
Training Epoch: 36 [25728/50048]	Loss: 0.4493
Training Epoch: 36 [25856/50048]	Loss: 0.4411
Training Epoch: 36 [25984/50048]	Loss: 0.5255
Training Epoch: 36 [26112/50048]	Loss: 0.6704
Training Epoch: 36 [26240/50048]	Loss: 0.5496
Training Epoch: 36 [26368/50048]	Loss: 0.6099
Training Epoch: 36 [26496/50048]	Loss: 0.5375
Training Epoch: 36 [26624/50048]	Loss: 0.5157
Training Epoch: 36 [26752/50048]	Loss: 0.4603
Training Epoch: 36 [26880/50048]	Loss: 0.6052
Training Epoch: 36 [27008/50048]	Loss: 0.6546
Training Epoch: 36 [27136/50048]	Loss: 0.4513
Training Epoch: 36 [27264/50048]	Loss: 0.5559
Training Epoch: 36 [27392/50048]	Loss: 0.5390
Training Epoch: 36 [27520/50048]	Loss: 0.4989
Training Epoch: 36 [27648/50048]	Loss: 0.6221
Training Epoch: 36 [27776/50048]	Loss: 0.7434
Training Epoch: 36 [27904/50048]	Loss: 0.4768
Training Epoch: 36 [28032/50048]	Loss: 0.4550
Training Epoch: 36 [28160/50048]	Loss: 0.4505
Training Epoch: 36 [28288/50048]	Loss: 0.5190
Training Epoch: 36 [28416/50048]	Loss: 0.4995
Training Epoch: 36 [28544/50048]	Loss: 0.4983
Training Epoch: 36 [28672/50048]	Loss: 0.5949
Training Epoch: 36 [28800/50048]	Loss: 0.5405
Training Epoch: 36 [28928/50048]	Loss: 0.4861
Training Epoch: 36 [29056/50048]	Loss: 0.5075
Training Epoch: 36 [29184/50048]	Loss: 0.5417
Training Epoch: 36 [29312/50048]	Loss: 0.5699
Training Epoch: 36 [29440/50048]	Loss: 0.6439
Training Epoch: 36 [29568/50048]	Loss: 0.7575
Training Epoch: 36 [29696/50048]	Loss: 0.3402
Training Epoch: 36 [29824/50048]	Loss: 0.4661
Training Epoch: 36 [29952/50048]	Loss: 0.6232
Training Epoch: 36 [30080/50048]	Loss: 0.5201
Training Epoch: 36 [30208/50048]	Loss: 0.6757
Training Epoch: 36 [30336/50048]	Loss: 0.5294
Training Epoch: 36 [30464/50048]	Loss: 0.4468
Training Epoch: 36 [30592/50048]	Loss: 0.5739
Training Epoch: 36 [30720/50048]	Loss: 0.6243
Training Epoch: 36 [30848/50048]	Loss: 0.4618
Training Epoch: 36 [30976/50048]	Loss: 0.4950
Training Epoch: 36 [31104/50048]	Loss: 0.5581
Training Epoch: 36 [31232/50048]	Loss: 0.4928
Training Epoch: 36 [31360/50048]	Loss: 0.5526
Training Epoch: 36 [31488/50048]	Loss: 0.6527
Training Epoch: 36 [31616/50048]	Loss: 0.5938
Training Epoch: 36 [31744/50048]	Loss: 0.6562
Training Epoch: 36 [31872/50048]	Loss: 0.6221
Training Epoch: 36 [32000/50048]	Loss: 0.5771
Training Epoch: 36 [32128/50048]	Loss: 0.5896
Training Epoch: 36 [32256/50048]	Loss: 0.6222
Training Epoch: 36 [32384/50048]	Loss: 0.5693
Training Epoch: 36 [32512/50048]	Loss: 0.5644
Training Epoch: 36 [32640/50048]	Loss: 0.6391
Training Epoch: 36 [32768/50048]	Loss: 0.5486
Training Epoch: 36 [32896/50048]	Loss: 0.4722
Training Epoch: 36 [33024/50048]	Loss: 0.5353
Training Epoch: 36 [33152/50048]	Loss: 0.5556
Training Epoch: 36 [33280/50048]	Loss: 0.6667
Training Epoch: 36 [33408/50048]	Loss: 0.5457
Training Epoch: 36 [33536/50048]	Loss: 0.5223
Training Epoch: 36 [33664/50048]	Loss: 0.3916
Training Epoch: 36 [33792/50048]	Loss: 0.5228
Training Epoch: 36 [33920/50048]	Loss: 0.5930
Training Epoch: 36 [34048/50048]	Loss: 0.5118
Training Epoch: 36 [34176/50048]	Loss: 0.6183
Training Epoch: 36 [34304/50048]	Loss: 0.4745
Training Epoch: 36 [34432/50048]	Loss: 0.4705
Training Epoch: 36 [34560/50048]	Loss: 0.5755
Training Epoch: 36 [34688/50048]	Loss: 0.6260
Training Epoch: 36 [34816/50048]	Loss: 0.4703
Training Epoch: 36 [34944/50048]	Loss: 0.4103
Training Epoch: 36 [35072/50048]	Loss: 0.4322
Training Epoch: 36 [35200/50048]	Loss: 0.5432
Training Epoch: 36 [35328/50048]	Loss: 0.5103
Training Epoch: 36 [35456/50048]	Loss: 0.6147
Training Epoch: 36 [35584/50048]	Loss: 0.6269
Training Epoch: 36 [35712/50048]	Loss: 0.5023
Training Epoch: 36 [35840/50048]	Loss: 0.6248
Training Epoch: 36 [35968/50048]	Loss: 0.4760
Training Epoch: 36 [36096/50048]	Loss: 0.3985
Training Epoch: 36 [36224/50048]	Loss: 0.4599
Training Epoch: 36 [36352/50048]	Loss: 0.4603
Training Epoch: 36 [36480/50048]	Loss: 0.4898
Training Epoch: 36 [36608/50048]	Loss: 0.5412
Training Epoch: 36 [36736/50048]	Loss: 0.3657
Training Epoch: 36 [36864/50048]	Loss: 0.4761
Training Epoch: 36 [36992/50048]	Loss: 0.4136
Training Epoch: 36 [37120/50048]	Loss: 0.4451
Training Epoch: 36 [37248/50048]	Loss: 0.5259
Training Epoch: 36 [37376/50048]	Loss: 0.4101
Training Epoch: 36 [37504/50048]	Loss: 0.6137
Training Epoch: 36 [37632/50048]	Loss: 0.5951
Training Epoch: 36 [37760/50048]	Loss: 0.8429
Training Epoch: 36 [37888/50048]	Loss: 0.4771
Training Epoch: 36 [38016/50048]	Loss: 0.5987
Training Epoch: 36 [38144/50048]	Loss: 0.6334
Training Epoch: 36 [38272/50048]	Loss: 0.5218
Training Epoch: 36 [38400/50048]	Loss: 0.5071
Training Epoch: 36 [38528/50048]	Loss: 0.6244
Training Epoch: 36 [38656/50048]	Loss: 0.4745
Training Epoch: 36 [38784/50048]	Loss: 0.6487
Training Epoch: 36 [38912/50048]	Loss: 0.4627
Training Epoch: 36 [39040/50048]	Loss: 0.5081
Training Epoch: 36 [39168/50048]	Loss: 0.5044
Training Epoch: 36 [39296/50048]	Loss: 0.6674
Training Epoch: 36 [39424/50048]	Loss: 0.6801
Training Epoch: 36 [39552/50048]	Loss: 0.6379
Training Epoch: 36 [39680/50048]	Loss: 0.3712
Training Epoch: 36 [39808/50048]	Loss: 0.5898
Training Epoch: 36 [39936/50048]	Loss: 0.5457
Training Epoch: 36 [40064/50048]	Loss: 0.5663
Training Epoch: 36 [40192/50048]	Loss: 0.5119
Training Epoch: 36 [40320/50048]	Loss: 0.6339
Training Epoch: 36 [40448/50048]	Loss: 0.5060
Training Epoch: 36 [40576/50048]	Loss: 0.6058
Training Epoch: 36 [40704/50048]	Loss: 0.5354
Training Epoch: 36 [40832/50048]	Loss: 0.4330
Training Epoch: 36 [40960/50048]	Loss: 0.6076
Training Epoch: 36 [41088/50048]	Loss: 0.6015
Training Epoch: 36 [41216/50048]	Loss: 0.4341
Training Epoch: 36 [41344/50048]	Loss: 0.4884
Training Epoch: 36 [41472/50048]	Loss: 0.4744
Training Epoch: 36 [41600/50048]	Loss: 0.6315
Training Epoch: 36 [41728/50048]	Loss: 0.5431
Training Epoch: 36 [41856/50048]	Loss: 0.6560
Training Epoch: 36 [41984/50048]	Loss: 0.5720
Training Epoch: 36 [42112/50048]	Loss: 0.4225
Training Epoch: 36 [42240/50048]	Loss: 0.4576
Training Epoch: 36 [42368/50048]	Loss: 0.5853
Training Epoch: 36 [42496/50048]	Loss: 0.4671
Training Epoch: 36 [42624/50048]	Loss: 0.7142
Training Epoch: 36 [42752/50048]	Loss: 0.4711
Training Epoch: 36 [42880/50048]	Loss: 0.7013
Training Epoch: 36 [43008/50048]	Loss: 0.6271
Training Epoch: 36 [43136/50048]	Loss: 0.4128
Training Epoch: 36 [43264/50048]	Loss: 0.4116
Training Epoch: 36 [43392/50048]	Loss: 0.6744
Training Epoch: 36 [43520/50048]	Loss: 0.5585
Training Epoch: 36 [43648/50048]	Loss: 0.5304
Training Epoch: 36 [43776/50048]	Loss: 0.5997
Training Epoch: 36 [43904/50048]	Loss: 0.5528
Training Epoch: 36 [44032/50048]	Loss: 0.7299
Training Epoch: 36 [44160/50048]	Loss: 0.5640
Training Epoch: 36 [44288/50048]	Loss: 0.4614
Training Epoch: 36 [44416/50048]	Loss: 0.5243
Training Epoch: 36 [44544/50048]	Loss: 0.6634
Training Epoch: 36 [44672/50048]	Loss: 0.4471
Training Epoch: 36 [44800/50048]	Loss: 0.6013
Training Epoch: 36 [44928/50048]	Loss: 0.6143
Training Epoch: 36 [45056/50048]	Loss: 0.6161
Training Epoch: 36 [45184/50048]	Loss: 0.4710
Training Epoch: 36 [45312/50048]	Loss: 0.4426
Training Epoch: 36 [45440/50048]	Loss: 0.6186
Training Epoch: 36 [45568/50048]	Loss: 0.5017
Training Epoch: 36 [45696/50048]	Loss: 0.4958
2022-12-06 07:03:27,792 [ZeusDataLoader(train)] train epoch 37 done: time=86.52 energy=10501.10
2022-12-06 07:03:27,793 [ZeusDataLoader(eval)] Epoch 37 begin.
Training Epoch: 36 [45824/50048]	Loss: 0.5356
Training Epoch: 36 [45952/50048]	Loss: 0.6221
Training Epoch: 36 [46080/50048]	Loss: 0.4991
Training Epoch: 36 [46208/50048]	Loss: 0.7141
Training Epoch: 36 [46336/50048]	Loss: 0.6554
Training Epoch: 36 [46464/50048]	Loss: 0.6145
Training Epoch: 36 [46592/50048]	Loss: 0.6510
Training Epoch: 36 [46720/50048]	Loss: 0.6995
Training Epoch: 36 [46848/50048]	Loss: 0.5962
Training Epoch: 36 [46976/50048]	Loss: 0.5701
Training Epoch: 36 [47104/50048]	Loss: 0.4618
Training Epoch: 36 [47232/50048]	Loss: 0.5970
Training Epoch: 36 [47360/50048]	Loss: 0.4486
Training Epoch: 36 [47488/50048]	Loss: 0.5288
Training Epoch: 36 [47616/50048]	Loss: 0.7709
Training Epoch: 36 [47744/50048]	Loss: 0.5828
Training Epoch: 36 [47872/50048]	Loss: 0.4451
Training Epoch: 36 [48000/50048]	Loss: 0.5973
Training Epoch: 36 [48128/50048]	Loss: 0.5443
Training Epoch: 36 [48256/50048]	Loss: 0.4353
Training Epoch: 36 [48384/50048]	Loss: 0.5308
Training Epoch: 36 [48512/50048]	Loss: 0.5193
Training Epoch: 36 [48640/50048]	Loss: 0.5269
Training Epoch: 36 [48768/50048]	Loss: 0.5452
Training Epoch: 36 [48896/50048]	Loss: 0.5245
Training Epoch: 36 [49024/50048]	Loss: 0.5229
Training Epoch: 36 [49152/50048]	Loss: 0.5205
Training Epoch: 36 [49280/50048]	Loss: 0.4634
Training Epoch: 36 [49408/50048]	Loss: 0.5497
Training Epoch: 36 [49536/50048]	Loss: 0.5123
Training Epoch: 36 [49664/50048]	Loss: 0.5567
Training Epoch: 36 [49792/50048]	Loss: 0.5653
Training Epoch: 36 [49920/50048]	Loss: 0.4620
Training Epoch: 36 [50048/50048]	Loss: 0.7183
2022-12-06 12:03:31.502 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:03:31,513 [ZeusDataLoader(eval)] eval epoch 37 done: time=3.71 energy=453.19
2022-12-06 07:03:31,513 [ZeusDataLoader(train)] Up to epoch 37: time=3337.09, energy=405120.48, cost=494555.41
2022-12-06 07:03:31,513 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:03:31,513 [ZeusDataLoader(train)] Expected next epoch: time=3426.89, energy=415918.49, cost=507811.79
2022-12-06 07:03:31,514 [ZeusDataLoader(train)] Epoch 38 begin.
Validation Epoch: 36, Average loss: 0.0130, Accuracy: 0.6257
2022-12-06 07:03:31,720 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:03:31,721 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:03:31.723 [ZeusMonitor] Monitor started.
2022-12-06 12:03:31.723 [ZeusMonitor] Running indefinitely. 2022-12-06 12:03:31.723 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:03:31.723 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e38+gpu0.power.log
Training Epoch: 37 [128/50048]	Loss: 0.5510
Training Epoch: 37 [256/50048]	Loss: 0.4686
Training Epoch: 37 [384/50048]	Loss: 0.3699
Training Epoch: 37 [512/50048]	Loss: 0.4334
Training Epoch: 37 [640/50048]	Loss: 0.3723
Training Epoch: 37 [768/50048]	Loss: 0.4976
Training Epoch: 37 [896/50048]	Loss: 0.5109
Training Epoch: 37 [1024/50048]	Loss: 0.3026
Training Epoch: 37 [1152/50048]	Loss: 0.3184
Training Epoch: 37 [1280/50048]	Loss: 0.4471
Training Epoch: 37 [1408/50048]	Loss: 0.5009
Training Epoch: 37 [1536/50048]	Loss: 0.5239
Training Epoch: 37 [1664/50048]	Loss: 0.3706
Training Epoch: 37 [1792/50048]	Loss: 0.4411
Training Epoch: 37 [1920/50048]	Loss: 0.4494
Training Epoch: 37 [2048/50048]	Loss: 0.4458
Training Epoch: 37 [2176/50048]	Loss: 0.3427
Training Epoch: 37 [2304/50048]	Loss: 0.3503
Training Epoch: 37 [2432/50048]	Loss: 0.3689
Training Epoch: 37 [2560/50048]	Loss: 0.4937
Training Epoch: 37 [2688/50048]	Loss: 0.4183
Training Epoch: 37 [2816/50048]	Loss: 0.3046
Training Epoch: 37 [2944/50048]	Loss: 0.5053
Training Epoch: 37 [3072/50048]	Loss: 0.5078
Training Epoch: 37 [3200/50048]	Loss: 0.4333
Training Epoch: 37 [3328/50048]	Loss: 0.5572
Training Epoch: 37 [3456/50048]	Loss: 0.4399
Training Epoch: 37 [3584/50048]	Loss: 0.4924
Training Epoch: 37 [3712/50048]	Loss: 0.4698
Training Epoch: 37 [3840/50048]	Loss: 0.4494
Training Epoch: 37 [3968/50048]	Loss: 0.4012
Training Epoch: 37 [4096/50048]	Loss: 0.3489
Training Epoch: 37 [4224/50048]	Loss: 0.4897
Training Epoch: 37 [4352/50048]	Loss: 0.3941
Training Epoch: 37 [4480/50048]	Loss: 0.4844
Training Epoch: 37 [4608/50048]	Loss: 0.3828
Training Epoch: 37 [4736/50048]	Loss: 0.3644
Training Epoch: 37 [4864/50048]	Loss: 0.3681
Training Epoch: 37 [4992/50048]	Loss: 0.3623
Training Epoch: 37 [5120/50048]	Loss: 0.3982
Training Epoch: 37 [5248/50048]	Loss: 0.3256
Training Epoch: 37 [5376/50048]	Loss: 0.4001
Training Epoch: 37 [5504/50048]	Loss: 0.5893
Training Epoch: 37 [5632/50048]	Loss: 0.4951
Training Epoch: 37 [5760/50048]	Loss: 0.4996
Training Epoch: 37 [5888/50048]	Loss: 0.3762
Training Epoch: 37 [6016/50048]	Loss: 0.4257
Training Epoch: 37 [6144/50048]	Loss: 0.4401
Training Epoch: 37 [6272/50048]	Loss: 0.4446
Training Epoch: 37 [6400/50048]	Loss: 0.6050
Training Epoch: 37 [6528/50048]	Loss: 0.3903
Training Epoch: 37 [6656/50048]	Loss: 0.4387
Training Epoch: 37 [6784/50048]	Loss: 0.3587
Training Epoch: 37 [6912/50048]	Loss: 0.4605
Training Epoch: 37 [7040/50048]	Loss: 0.4132
Training Epoch: 37 [7168/50048]	Loss: 0.3442
Training Epoch: 37 [7296/50048]	Loss: 0.4042
Training Epoch: 37 [7424/50048]	Loss: 0.4295
Training Epoch: 37 [7552/50048]	Loss: 0.5097
Training Epoch: 37 [7680/50048]	Loss: 0.5069
Training Epoch: 37 [7808/50048]	Loss: 0.4484
Training Epoch: 37 [7936/50048]	Loss: 0.3313
Training Epoch: 37 [8064/50048]	Loss: 0.4108
Training Epoch: 37 [8192/50048]	Loss: 0.4970
Training Epoch: 37 [8320/50048]	Loss: 0.4058
Training Epoch: 37 [8448/50048]	Loss: 0.5231
Training Epoch: 37 [8576/50048]	Loss: 0.4152
Training Epoch: 37 [8704/50048]	Loss: 0.3689
Training Epoch: 37 [8832/50048]	Loss: 0.4488
Training Epoch: 37 [8960/50048]	Loss: 0.4474
Training Epoch: 37 [9088/50048]	Loss: 0.3756
Training Epoch: 37 [9216/50048]	Loss: 0.3694
Training Epoch: 37 [9344/50048]	Loss: 0.4048
Training Epoch: 37 [9472/50048]	Loss: 0.4732
Training Epoch: 37 [9600/50048]	Loss: 0.4511
Training Epoch: 37 [9728/50048]	Loss: 0.5243
Training Epoch: 37 [9856/50048]	Loss: 0.4194
Training Epoch: 37 [9984/50048]	Loss: 0.4106
Training Epoch: 37 [10112/50048]	Loss: 0.5936
Training Epoch: 37 [10240/50048]	Loss: 0.5955
Training Epoch: 37 [10368/50048]	Loss: 0.5648
Training Epoch: 37 [10496/50048]	Loss: 0.3901
Training Epoch: 37 [10624/50048]	Loss: 0.5066
Training Epoch: 37 [10752/50048]	Loss: 0.3770
Training Epoch: 37 [10880/50048]	Loss: 0.4600
Training Epoch: 37 [11008/50048]	Loss: 0.5553
Training Epoch: 37 [11136/50048]	Loss: 0.5896
Training Epoch: 37 [11264/50048]	Loss: 0.3873
Training Epoch: 37 [11392/50048]	Loss: 0.5712
Training Epoch: 37 [11520/50048]	Loss: 0.5831
Training Epoch: 37 [11648/50048]	Loss: 0.4805
Training Epoch: 37 [11776/50048]	Loss: 0.3497
Training Epoch: 37 [11904/50048]	Loss: 0.5158
Training Epoch: 37 [12032/50048]	Loss: 0.5140
Training Epoch: 37 [12160/50048]	Loss: 0.4859
Training Epoch: 37 [12288/50048]	Loss: 0.4827
Training Epoch: 37 [12416/50048]	Loss: 0.5270
Training Epoch: 37 [12544/50048]	Loss: 0.4342
Training Epoch: 37 [12672/50048]	Loss: 0.3923
Training Epoch: 37 [12800/50048]	Loss: 0.3665
Training Epoch: 37 [12928/50048]	Loss: 0.4667
Training Epoch: 37 [13056/50048]	Loss: 0.3277
Training Epoch: 37 [13184/50048]	Loss: 0.4862
Training Epoch: 37 [13312/50048]	Loss: 0.6603
Training Epoch: 37 [13440/50048]	Loss: 0.4821
Training Epoch: 37 [13568/50048]	Loss: 0.5073
Training Epoch: 37 [13696/50048]	Loss: 0.4378
Training Epoch: 37 [13824/50048]	Loss: 0.3927
Training Epoch: 37 [13952/50048]	Loss: 0.4819
Training Epoch: 37 [14080/50048]	Loss: 0.3894
Training Epoch: 37 [14208/50048]	Loss: 0.3412
Training Epoch: 37 [14336/50048]	Loss: 0.4918
Training Epoch: 37 [14464/50048]	Loss: 0.4309
Training Epoch: 37 [14592/50048]	Loss: 0.5420
Training Epoch: 37 [14720/50048]	Loss: 0.4934
Training Epoch: 37 [14848/50048]	Loss: 0.5233
Training Epoch: 37 [14976/50048]	Loss: 0.6088
Training Epoch: 37 [15104/50048]	Loss: 0.4351
Training Epoch: 37 [15232/50048]	Loss: 0.3969
Training Epoch: 37 [15360/50048]	Loss: 0.4589
Training Epoch: 37 [15488/50048]	Loss: 0.5970
Training Epoch: 37 [15616/50048]	Loss: 0.5363
Training Epoch: 37 [15744/50048]	Loss: 0.3411
Training Epoch: 37 [15872/50048]	Loss: 0.4066
Training Epoch: 37 [16000/50048]	Loss: 0.4004
Training Epoch: 37 [16128/50048]	Loss: 0.4470
Training Epoch: 37 [16256/50048]	Loss: 0.5017
Training Epoch: 37 [16384/50048]	Loss: 0.6536
Training Epoch: 37 [16512/50048]	Loss: 0.5778
Training Epoch: 37 [16640/50048]	Loss: 0.4915
Training Epoch: 37 [16768/50048]	Loss: 0.4915
Training Epoch: 37 [16896/50048]	Loss: 0.4544
Training Epoch: 37 [17024/50048]	Loss: 0.4819
Training Epoch: 37 [17152/50048]	Loss: 0.6158
Training Epoch: 37 [17280/50048]	Loss: 0.3983
Training Epoch: 37 [17408/50048]	Loss: 0.3810
Training Epoch: 37 [17536/50048]	Loss: 0.4664
Training Epoch: 37 [17664/50048]	Loss: 0.4962
Training Epoch: 37 [17792/50048]	Loss: 0.4228
Training Epoch: 37 [17920/50048]	Loss: 0.5183
Training Epoch: 37 [18048/50048]	Loss: 0.3918
Training Epoch: 37 [18176/50048]	Loss: 0.4726
Training Epoch: 37 [18304/50048]	Loss: 0.4797
Training Epoch: 37 [18432/50048]	Loss: 0.3840
Training Epoch: 37 [18560/50048]	Loss: 0.4188
Training Epoch: 37 [18688/50048]	Loss: 0.4841
Training Epoch: 37 [18816/50048]	Loss: 0.5725
Training Epoch: 37 [18944/50048]	Loss: 0.4823
Training Epoch: 37 [19072/50048]	Loss: 0.4791
Training Epoch: 37 [19200/50048]	Loss: 0.5360
Training Epoch: 37 [19328/50048]	Loss: 0.5562
Training Epoch: 37 [19456/50048]	Loss: 0.4778
Training Epoch: 37 [19584/50048]	Loss: 0.6772
Training Epoch: 37 [19712/50048]	Loss: 0.5084
Training Epoch: 37 [19840/50048]	Loss: 0.6105
Training Epoch: 37 [19968/50048]	Loss: 0.2957
Training Epoch: 37 [20096/50048]	Loss: 0.3673
Training Epoch: 37 [20224/50048]	Loss: 0.4163
Training Epoch: 37 [20352/50048]	Loss: 0.5898
Training Epoch: 37 [20480/50048]	Loss: 0.5422
Training Epoch: 37 [20608/50048]	Loss: 0.4243
Training Epoch: 37 [20736/50048]	Loss: 0.5248
Training Epoch: 37 [20864/50048]	Loss: 0.5935
Training Epoch: 37 [20992/50048]	Loss: 0.6299
Training Epoch: 37 [21120/50048]	Loss: 0.5431
Training Epoch: 37 [21248/50048]	Loss: 0.5117
Training Epoch: 37 [21376/50048]	Loss: 0.6091
Training Epoch: 37 [21504/50048]	Loss: 0.4914
Training Epoch: 37 [21632/50048]	Loss: 0.5826
Training Epoch: 37 [21760/50048]	Loss: 0.5872
Training Epoch: 37 [21888/50048]	Loss: 0.4373
Training Epoch: 37 [22016/50048]	Loss: 0.5773
Training Epoch: 37 [22144/50048]	Loss: 0.2673
Training Epoch: 37 [22272/50048]	Loss: 0.4874
Training Epoch: 37 [22400/50048]	Loss: 0.4132
Training Epoch: 37 [22528/50048]	Loss: 0.4924
Training Epoch: 37 [22656/50048]	Loss: 0.4933
Training Epoch: 37 [22784/50048]	Loss: 0.5650
Training Epoch: 37 [22912/50048]	Loss: 0.6374
Training Epoch: 37 [23040/50048]	Loss: 0.4042
Training Epoch: 37 [23168/50048]	Loss: 0.3907
Training Epoch: 37 [23296/50048]	Loss: 0.6628
Training Epoch: 37 [23424/50048]	Loss: 0.5082
Training Epoch: 37 [23552/50048]	Loss: 0.4075
Training Epoch: 37 [23680/50048]	Loss: 0.3673
Training Epoch: 37 [23808/50048]	Loss: 0.4066
Training Epoch: 37 [23936/50048]	Loss: 0.3722
Training Epoch: 37 [24064/50048]	Loss: 0.3930
Training Epoch: 37 [24192/50048]	Loss: 0.5055
Training Epoch: 37 [24320/50048]	Loss: 0.4201
Training Epoch: 37 [24448/50048]	Loss: 0.6069
Training Epoch: 37 [24576/50048]	Loss: 0.4622
Training Epoch: 37 [24704/50048]	Loss: 0.4017
Training Epoch: 37 [24832/50048]	Loss: 0.4423
Training Epoch: 37 [24960/50048]	Loss: 0.3864
Training Epoch: 37 [25088/50048]	Loss: 0.7222
Training Epoch: 37 [25216/50048]	Loss: 0.4264
Training Epoch: 37 [25344/50048]	Loss: 0.5308
Training Epoch: 37 [25472/50048]	Loss: 0.4279
Training Epoch: 37 [25600/50048]	Loss: 0.4379
Training Epoch: 37 [25728/50048]	Loss: 0.5779
Training Epoch: 37 [25856/50048]	Loss: 0.6416
Training Epoch: 37 [25984/50048]	Loss: 0.5285
Training Epoch: 37 [26112/50048]	Loss: 0.4807
Training Epoch: 37 [26240/50048]	Loss: 0.4195
Training Epoch: 37 [26368/50048]	Loss: 0.6374
Training Epoch: 37 [26496/50048]	Loss: 0.4490
Training Epoch: 37 [26624/50048]	Loss: 0.5321
Training Epoch: 37 [26752/50048]	Loss: 0.4161
Training Epoch: 37 [26880/50048]	Loss: 0.4291
Training Epoch: 37 [27008/50048]	Loss: 0.5409
Training Epoch: 37 [27136/50048]	Loss: 0.7062
Training Epoch: 37 [27264/50048]	Loss: 0.4534
Training Epoch: 37 [27392/50048]	Loss: 0.4145
Training Epoch: 37 [27520/50048]	Loss: 0.4641
Training Epoch: 37 [27648/50048]	Loss: 0.5750
Training Epoch: 37 [27776/50048]	Loss: 0.4818
Training Epoch: 37 [27904/50048]	Loss: 0.5617
Training Epoch: 37 [28032/50048]	Loss: 0.4738
Training Epoch: 37 [28160/50048]	Loss: 0.3887
Training Epoch: 37 [28288/50048]	Loss: 0.4543
Training Epoch: 37 [28416/50048]	Loss: 0.6505
Training Epoch: 37 [28544/50048]	Loss: 0.4698
Training Epoch: 37 [28672/50048]	Loss: 0.4307
Training Epoch: 37 [28800/50048]	Loss: 0.4896
Training Epoch: 37 [28928/50048]	Loss: 0.5151
Training Epoch: 37 [29056/50048]	Loss: 0.5624
Training Epoch: 37 [29184/50048]	Loss: 0.5412
Training Epoch: 37 [29312/50048]	Loss: 0.5629
Training Epoch: 37 [29440/50048]	Loss: 0.5275
Training Epoch: 37 [29568/50048]	Loss: 0.4376
Training Epoch: 37 [29696/50048]	Loss: 0.4813
Training Epoch: 37 [29824/50048]	Loss: 0.4966
Training Epoch: 37 [29952/50048]	Loss: 0.4780
Training Epoch: 37 [30080/50048]	Loss: 0.4177
Training Epoch: 37 [30208/50048]	Loss: 0.4767
Training Epoch: 37 [30336/50048]	Loss: 0.4186
Training Epoch: 37 [30464/50048]	Loss: 0.6155
Training Epoch: 37 [30592/50048]	Loss: 0.5644
Training Epoch: 37 [30720/50048]	Loss: 0.5376
Training Epoch: 37 [30848/50048]	Loss: 0.4342
Training Epoch: 37 [30976/50048]	Loss: 0.3356
Training Epoch: 37 [31104/50048]	Loss: 0.5862
Training Epoch: 37 [31232/50048]	Loss: 0.4483
Training Epoch: 37 [31360/50048]	Loss: 0.4815
Training Epoch: 37 [31488/50048]	Loss: 0.3851
Training Epoch: 37 [31616/50048]	Loss: 0.4004
Training Epoch: 37 [31744/50048]	Loss: 0.5707
Training Epoch: 37 [31872/50048]	Loss: 0.5153
Training Epoch: 37 [32000/50048]	Loss: 0.4571
Training Epoch: 37 [32128/50048]	Loss: 0.6243
Training Epoch: 37 [32256/50048]	Loss: 0.8010
Training Epoch: 37 [32384/50048]	Loss: 0.5190
Training Epoch: 37 [32512/50048]	Loss: 0.5055
Training Epoch: 37 [32640/50048]	Loss: 0.4998
Training Epoch: 37 [32768/50048]	Loss: 0.4196
Training Epoch: 37 [32896/50048]	Loss: 0.5365
Training Epoch: 37 [33024/50048]	Loss: 0.4857
Training Epoch: 37 [33152/50048]	Loss: 0.4251
Training Epoch: 37 [33280/50048]	Loss: 0.5838
Training Epoch: 37 [33408/50048]	Loss: 0.5152
Training Epoch: 37 [33536/50048]	Loss: 0.4134
Training Epoch: 37 [33664/50048]	Loss: 0.5156
Training Epoch: 37 [33792/50048]	Loss: 0.5233
Training Epoch: 37 [33920/50048]	Loss: 0.6415
Training Epoch: 37 [34048/50048]	Loss: 0.6924
Training Epoch: 37 [34176/50048]	Loss: 0.4126
Training Epoch: 37 [34304/50048]	Loss: 0.4557
Training Epoch: 37 [34432/50048]	Loss: 0.3647
Training Epoch: 37 [34560/50048]	Loss: 0.4943
Training Epoch: 37 [34688/50048]	Loss: 0.5407
Training Epoch: 37 [34816/50048]	Loss: 0.5113
Training Epoch: 37 [34944/50048]	Loss: 0.4649
Training Epoch: 37 [35072/50048]	Loss: 0.4528
Training Epoch: 37 [35200/50048]	Loss: 0.5135
Training Epoch: 37 [35328/50048]	Loss: 0.5578
Training Epoch: 37 [35456/50048]	Loss: 0.4476
Training Epoch: 37 [35584/50048]	Loss: 0.6386
Training Epoch: 37 [35712/50048]	Loss: 0.4851
Training Epoch: 37 [35840/50048]	Loss: 0.5384
Training Epoch: 37 [35968/50048]	Loss: 0.6422
Training Epoch: 37 [36096/50048]	Loss: 0.4420
Training Epoch: 37 [36224/50048]	Loss: 0.4625
Training Epoch: 37 [36352/50048]	Loss: 0.5198
Training Epoch: 37 [36480/50048]	Loss: 0.5273
Training Epoch: 37 [36608/50048]	Loss: 0.3949
Training Epoch: 37 [36736/50048]	Loss: 0.4355
Training Epoch: 37 [36864/50048]	Loss: 0.5631
Training Epoch: 37 [36992/50048]	Loss: 0.4700
Training Epoch: 37 [37120/50048]	Loss: 0.4861
Training Epoch: 37 [37248/50048]	Loss: 0.5388
Training Epoch: 37 [37376/50048]	Loss: 0.4639
Training Epoch: 37 [37504/50048]	Loss: 0.4834
Training Epoch: 37 [37632/50048]	Loss: 0.5955
Training Epoch: 37 [37760/50048]	Loss: 0.5906
Training Epoch: 37 [37888/50048]	Loss: 0.5216
Training Epoch: 37 [38016/50048]	Loss: 0.4592
Training Epoch: 37 [38144/50048]	Loss: 0.4594
Training Epoch: 37 [38272/50048]	Loss: 0.5089
Training Epoch: 37 [38400/50048]	Loss: 0.5827
Training Epoch: 37 [38528/50048]	Loss: 0.4609
Training Epoch: 37 [38656/50048]	Loss: 0.5469
Training Epoch: 37 [38784/50048]	Loss: 0.2943
Training Epoch: 37 [38912/50048]	Loss: 0.5696
Training Epoch: 37 [39040/50048]	Loss: 0.4114
Training Epoch: 37 [39168/50048]	Loss: 0.4691
Training Epoch: 37 [39296/50048]	Loss: 0.5290
Training Epoch: 37 [39424/50048]	Loss: 0.4103
Training Epoch: 37 [39552/50048]	Loss: 0.4356
Training Epoch: 37 [39680/50048]	Loss: 0.3989
Training Epoch: 37 [39808/50048]	Loss: 0.5402
Training Epoch: 37 [39936/50048]	Loss: 0.6504
Training Epoch: 37 [40064/50048]	Loss: 0.6180
Training Epoch: 37 [40192/50048]	Loss: 0.3897
Training Epoch: 37 [40320/50048]	Loss: 0.5704
Training Epoch: 37 [40448/50048]	Loss: 0.3686
Training Epoch: 37 [40576/50048]	Loss: 0.6043
Training Epoch: 37 [40704/50048]	Loss: 0.7009
Training Epoch: 37 [40832/50048]	Loss: 0.4436
Training Epoch: 37 [40960/50048]	Loss: 0.5589
Training Epoch: 37 [41088/50048]	Loss: 0.4027
Training Epoch: 37 [41216/50048]	Loss: 0.3784
Training Epoch: 37 [41344/50048]	Loss: 0.5354
Training Epoch: 37 [41472/50048]	Loss: 0.6934
Training Epoch: 37 [41600/50048]	Loss: 0.5222
Training Epoch: 37 [41728/50048]	Loss: 0.5347
Training Epoch: 37 [41856/50048]	Loss: 0.4227
Training Epoch: 37 [41984/50048]	Loss: 0.5731
Training Epoch: 37 [42112/50048]	Loss: 0.5815
Training Epoch: 37 [42240/50048]	Loss: 0.7049
Training Epoch: 37 [42368/50048]	Loss: 0.6141
Training Epoch: 37 [42496/50048]	Loss: 0.5751
Training Epoch: 37 [42624/50048]	Loss: 0.5648
Training Epoch: 37 [42752/50048]	Loss: 0.4248
Training Epoch: 37 [42880/50048]	Loss: 0.4820
Training Epoch: 37 [43008/50048]	Loss: 0.5956
Training Epoch: 37 [43136/50048]	Loss: 0.6317
Training Epoch: 37 [43264/50048]	Loss: 0.5472
Training Epoch: 37 [43392/50048]	Loss: 0.4522
Training Epoch: 37 [43520/50048]	Loss: 0.5438
Training Epoch: 37 [43648/50048]	Loss: 0.5842
Training Epoch: 37 [43776/50048]	Loss: 0.4227
Training Epoch: 37 [43904/50048]	Loss: 0.4412
Training Epoch: 37 [44032/50048]	Loss: 0.5878
Training Epoch: 37 [44160/50048]	Loss: 0.4890
Training Epoch: 37 [44288/50048]	Loss: 0.5829
Training Epoch: 37 [44416/50048]	Loss: 0.5694
Training Epoch: 37 [44544/50048]	Loss: 0.4665
Training Epoch: 37 [44672/50048]	Loss: 0.5156
Training Epoch: 37 [44800/50048]	Loss: 0.6405
Training Epoch: 37 [44928/50048]	Loss: 0.5541
Training Epoch: 37 [45056/50048]	Loss: 0.5093
Training Epoch: 37 [45184/50048]	Loss: 0.5346
Training Epoch: 37 [45312/50048]	Loss: 0.5666
Training Epoch: 37 [45440/50048]	Loss: 0.4137
Training Epoch: 37 [45568/50048]	Loss: 0.5297
Training Epoch: 37 [45696/50048]	Loss: 0.4839
2022-12-06 07:04:57,979 [ZeusDataLoader(train)] train epoch 38 done: time=86.45 energy=10485.52
2022-12-06 07:04:57,980 [ZeusDataLoader(eval)] Epoch 38 begin.
Training Epoch: 37 [45824/50048]	Loss: 0.6063
Training Epoch: 37 [45952/50048]	Loss: 0.4323
Training Epoch: 37 [46080/50048]	Loss: 0.5386
Training Epoch: 37 [46208/50048]	Loss: 0.6107
Training Epoch: 37 [46336/50048]	Loss: 0.5811
Training Epoch: 37 [46464/50048]	Loss: 0.5296
Training Epoch: 37 [46592/50048]	Loss: 0.3883
Training Epoch: 37 [46720/50048]	Loss: 0.5167
Training Epoch: 37 [46848/50048]	Loss: 0.6020
Training Epoch: 37 [46976/50048]	Loss: 0.4885
Training Epoch: 37 [47104/50048]	Loss: 0.5551
Training Epoch: 37 [47232/50048]	Loss: 0.5653
Training Epoch: 37 [47360/50048]	Loss: 0.4533
Training Epoch: 37 [47488/50048]	Loss: 0.5987
Training Epoch: 37 [47616/50048]	Loss: 0.7098
Training Epoch: 37 [47744/50048]	Loss: 0.5138
Training Epoch: 37 [47872/50048]	Loss: 0.6242
Training Epoch: 37 [48000/50048]	Loss: 0.6392
Training Epoch: 37 [48128/50048]	Loss: 0.4060
Training Epoch: 37 [48256/50048]	Loss: 0.3677
Training Epoch: 37 [48384/50048]	Loss: 0.4087
Training Epoch: 37 [48512/50048]	Loss: 0.3998
Training Epoch: 37 [48640/50048]	Loss: 0.4028
Training Epoch: 37 [48768/50048]	Loss: 0.5423
Training Epoch: 37 [48896/50048]	Loss: 0.3167
Training Epoch: 37 [49024/50048]	Loss: 0.5748
Training Epoch: 37 [49152/50048]	Loss: 0.6010
Training Epoch: 37 [49280/50048]	Loss: 0.5199
Training Epoch: 37 [49408/50048]	Loss: 0.5008
Training Epoch: 37 [49536/50048]	Loss: 0.5859
Training Epoch: 37 [49664/50048]	Loss: 0.5915
Training Epoch: 37 [49792/50048]	Loss: 0.4929
Training Epoch: 37 [49920/50048]	Loss: 0.6242
Training Epoch: 37 [50048/50048]	Loss: 0.7095
2022-12-06 12:05:01.659 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:05:01,674 [ZeusDataLoader(eval)] eval epoch 38 done: time=3.69 energy=439.94
2022-12-06 07:05:01,675 [ZeusDataLoader(train)] Up to epoch 38: time=3427.23, energy=416045.93, cost=507905.30
2022-12-06 07:05:01,675 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:05:01,675 [ZeusDataLoader(train)] Expected next epoch: time=3517.03, energy=426843.95, cost=521161.68
2022-12-06 07:05:01,676 [ZeusDataLoader(train)] Epoch 39 begin.
Validation Epoch: 37, Average loss: 0.0133, Accuracy: 0.6267
2022-12-06 07:05:01,877 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:05:01,878 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:05:01.880 [ZeusMonitor] Monitor started.
2022-12-06 12:05:01.880 [ZeusMonitor] Running indefinitely. 2022-12-06 12:05:01.880 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:05:01.880 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e39+gpu0.power.log
Training Epoch: 38 [128/50048]	Loss: 0.4716
Training Epoch: 38 [256/50048]	Loss: 0.4668
Training Epoch: 38 [384/50048]	Loss: 0.4134
Training Epoch: 38 [512/50048]	Loss: 0.6569
Training Epoch: 38 [640/50048]	Loss: 0.4571
Training Epoch: 38 [768/50048]	Loss: 0.4148
Training Epoch: 38 [896/50048]	Loss: 0.5282
Training Epoch: 38 [1024/50048]	Loss: 0.4437
Training Epoch: 38 [1152/50048]	Loss: 0.2958
Training Epoch: 38 [1280/50048]	Loss: 0.3897
Training Epoch: 38 [1408/50048]	Loss: 0.5111
Training Epoch: 38 [1536/50048]	Loss: 0.3564
Training Epoch: 38 [1664/50048]	Loss: 0.3753
Training Epoch: 38 [1792/50048]	Loss: 0.4455
Training Epoch: 38 [1920/50048]	Loss: 0.4720
Training Epoch: 38 [2048/50048]	Loss: 0.3790
Training Epoch: 38 [2176/50048]	Loss: 0.5093
Training Epoch: 38 [2304/50048]	Loss: 0.4755
Training Epoch: 38 [2432/50048]	Loss: 0.6069
Training Epoch: 38 [2560/50048]	Loss: 0.4949
Training Epoch: 38 [2688/50048]	Loss: 0.4938
Training Epoch: 38 [2816/50048]	Loss: 0.3751
Training Epoch: 38 [2944/50048]	Loss: 0.3799
Training Epoch: 38 [3072/50048]	Loss: 0.4370
Training Epoch: 38 [3200/50048]	Loss: 0.3277
Training Epoch: 38 [3328/50048]	Loss: 0.4665
Training Epoch: 38 [3456/50048]	Loss: 0.4166
Training Epoch: 38 [3584/50048]	Loss: 0.4940
Training Epoch: 38 [3712/50048]	Loss: 0.3880
Training Epoch: 38 [3840/50048]	Loss: 0.4221
Training Epoch: 38 [3968/50048]	Loss: 0.2817
Training Epoch: 38 [4096/50048]	Loss: 0.4208
Training Epoch: 38 [4224/50048]	Loss: 0.4115
Training Epoch: 38 [4352/50048]	Loss: 0.5398
Training Epoch: 38 [4480/50048]	Loss: 0.5949
Training Epoch: 38 [4608/50048]	Loss: 0.5337
Training Epoch: 38 [4736/50048]	Loss: 0.4484
Training Epoch: 38 [4864/50048]	Loss: 0.5679
Training Epoch: 38 [4992/50048]	Loss: 0.3541
Training Epoch: 38 [5120/50048]	Loss: 0.3567
Training Epoch: 38 [5248/50048]	Loss: 0.2942
Training Epoch: 38 [5376/50048]	Loss: 0.3552
Training Epoch: 38 [5504/50048]	Loss: 0.5512
Training Epoch: 38 [5632/50048]	Loss: 0.3429
Training Epoch: 38 [5760/50048]	Loss: 0.3374
Training Epoch: 38 [5888/50048]	Loss: 0.4195
Training Epoch: 38 [6016/50048]	Loss: 0.5184
Training Epoch: 38 [6144/50048]	Loss: 0.4706
Training Epoch: 38 [6272/50048]	Loss: 0.4466
Training Epoch: 38 [6400/50048]	Loss: 0.4923
Training Epoch: 38 [6528/50048]	Loss: 0.4582
Training Epoch: 38 [6656/50048]	Loss: 0.4849
Training Epoch: 38 [6784/50048]	Loss: 0.3666
Training Epoch: 38 [6912/50048]	Loss: 0.4048
Training Epoch: 38 [7040/50048]	Loss: 0.4727
Training Epoch: 38 [7168/50048]	Loss: 0.4008
Training Epoch: 38 [7296/50048]	Loss: 0.3698
Training Epoch: 38 [7424/50048]	Loss: 0.3829
Training Epoch: 38 [7552/50048]	Loss: 0.4241
Training Epoch: 38 [7680/50048]	Loss: 0.4041
Training Epoch: 38 [7808/50048]	Loss: 0.3993
Training Epoch: 38 [7936/50048]	Loss: 0.5581
Training Epoch: 38 [8064/50048]	Loss: 0.4689
Training Epoch: 38 [8192/50048]	Loss: 0.2858
Training Epoch: 38 [8320/50048]	Loss: 0.4511
Training Epoch: 38 [8448/50048]	Loss: 0.4390
Training Epoch: 38 [8576/50048]	Loss: 0.3837
Training Epoch: 38 [8704/50048]	Loss: 0.4588
Training Epoch: 38 [8832/50048]	Loss: 0.3561
Training Epoch: 38 [8960/50048]	Loss: 0.4272
Training Epoch: 38 [9088/50048]	Loss: 0.4697
Training Epoch: 38 [9216/50048]	Loss: 0.5723
Training Epoch: 38 [9344/50048]	Loss: 0.3432
Training Epoch: 38 [9472/50048]	Loss: 0.5087
Training Epoch: 38 [9600/50048]	Loss: 0.5074
Training Epoch: 38 [9728/50048]	Loss: 0.3570
Training Epoch: 38 [9856/50048]	Loss: 0.5218
Training Epoch: 38 [9984/50048]	Loss: 0.4219
Training Epoch: 38 [10112/50048]	Loss: 0.5737
Training Epoch: 38 [10240/50048]	Loss: 0.2358
Training Epoch: 38 [10368/50048]	Loss: 0.4527
Training Epoch: 38 [10496/50048]	Loss: 0.3157
Training Epoch: 38 [10624/50048]	Loss: 0.4717
Training Epoch: 38 [10752/50048]	Loss: 0.5056
Training Epoch: 38 [10880/50048]	Loss: 0.4613
Training Epoch: 38 [11008/50048]	Loss: 0.3236
Training Epoch: 38 [11136/50048]	Loss: 0.6447
Training Epoch: 38 [11264/50048]	Loss: 0.5371
Training Epoch: 38 [11392/50048]	Loss: 0.4731
Training Epoch: 38 [11520/50048]	Loss: 0.4112
Training Epoch: 38 [11648/50048]	Loss: 0.3406
Training Epoch: 38 [11776/50048]	Loss: 0.4832
Training Epoch: 38 [11904/50048]	Loss: 0.4725
Training Epoch: 38 [12032/50048]	Loss: 0.5324
Training Epoch: 38 [12160/50048]	Loss: 0.4882
Training Epoch: 38 [12288/50048]	Loss: 0.4197
Training Epoch: 38 [12416/50048]	Loss: 0.5069
Training Epoch: 38 [12544/50048]	Loss: 0.3249
Training Epoch: 38 [12672/50048]	Loss: 0.5362
Training Epoch: 38 [12800/50048]	Loss: 0.5416
Training Epoch: 38 [12928/50048]	Loss: 0.3450
Training Epoch: 38 [13056/50048]	Loss: 0.3595
Training Epoch: 38 [13184/50048]	Loss: 0.4373
Training Epoch: 38 [13312/50048]	Loss: 0.6159
Training Epoch: 38 [13440/50048]	Loss: 0.4278
Training Epoch: 38 [13568/50048]	Loss: 0.4563
Training Epoch: 38 [13696/50048]	Loss: 0.5044
Training Epoch: 38 [13824/50048]	Loss: 0.4568
Training Epoch: 38 [13952/50048]	Loss: 0.4437
Training Epoch: 38 [14080/50048]	Loss: 0.4784
Training Epoch: 38 [14208/50048]	Loss: 0.5568
Training Epoch: 38 [14336/50048]	Loss: 0.5154
Training Epoch: 38 [14464/50048]	Loss: 0.3741
Training Epoch: 38 [14592/50048]	Loss: 0.3652
Training Epoch: 38 [14720/50048]	Loss: 0.5003
Training Epoch: 38 [14848/50048]	Loss: 0.3000
Training Epoch: 38 [14976/50048]	Loss: 0.4920
Training Epoch: 38 [15104/50048]	Loss: 0.4835
Training Epoch: 38 [15232/50048]	Loss: 0.4810
Training Epoch: 38 [15360/50048]	Loss: 0.5443
Training Epoch: 38 [15488/50048]	Loss: 0.4986
Training Epoch: 38 [15616/50048]	Loss: 0.4858
Training Epoch: 38 [15744/50048]	Loss: 0.4094
Training Epoch: 38 [15872/50048]	Loss: 0.3826
Training Epoch: 38 [16000/50048]	Loss: 0.4067
Training Epoch: 38 [16128/50048]	Loss: 0.2972
Training Epoch: 38 [16256/50048]	Loss: 0.4858
Training Epoch: 38 [16384/50048]	Loss: 0.4120
Training Epoch: 38 [16512/50048]	Loss: 0.5273
Training Epoch: 38 [16640/50048]	Loss: 0.3661
Training Epoch: 38 [16768/50048]	Loss: 0.5050
Training Epoch: 38 [16896/50048]	Loss: 0.4816
Training Epoch: 38 [17024/50048]	Loss: 0.3415
Training Epoch: 38 [17152/50048]	Loss: 0.4369
Training Epoch: 38 [17280/50048]	Loss: 0.4344
Training Epoch: 38 [17408/50048]	Loss: 0.4161
Training Epoch: 38 [17536/50048]	Loss: 0.5138
Training Epoch: 38 [17664/50048]	Loss: 0.4677
Training Epoch: 38 [17792/50048]	Loss: 0.7065
Training Epoch: 38 [17920/50048]	Loss: 0.5039
Training Epoch: 38 [18048/50048]	Loss: 0.3588
Training Epoch: 38 [18176/50048]	Loss: 0.4455
Training Epoch: 38 [18304/50048]	Loss: 0.5815
Training Epoch: 38 [18432/50048]	Loss: 0.4638
Training Epoch: 38 [18560/50048]	Loss: 0.5494
Training Epoch: 38 [18688/50048]	Loss: 0.5084
Training Epoch: 38 [18816/50048]	Loss: 0.5830
Training Epoch: 38 [18944/50048]	Loss: 0.3977
Training Epoch: 38 [19072/50048]	Loss: 0.6193
Training Epoch: 38 [19200/50048]	Loss: 0.4938
Training Epoch: 38 [19328/50048]	Loss: 0.4757
Training Epoch: 38 [19456/50048]	Loss: 0.3708
Training Epoch: 38 [19584/50048]	Loss: 0.3138
Training Epoch: 38 [19712/50048]	Loss: 0.4783
Training Epoch: 38 [19840/50048]	Loss: 0.6362
Training Epoch: 38 [19968/50048]	Loss: 0.5228
Training Epoch: 38 [20096/50048]	Loss: 0.4017
Training Epoch: 38 [20224/50048]	Loss: 0.5333
Training Epoch: 38 [20352/50048]	Loss: 0.5583
Training Epoch: 38 [20480/50048]	Loss: 0.3980
Training Epoch: 38 [20608/50048]	Loss: 0.4612
Training Epoch: 38 [20736/50048]	Loss: 0.4314
Training Epoch: 38 [20864/50048]	Loss: 0.5753
Training Epoch: 38 [20992/50048]	Loss: 0.4878
Training Epoch: 38 [21120/50048]	Loss: 0.3769
Training Epoch: 38 [21248/50048]	Loss: 0.4960
Training Epoch: 38 [21376/50048]	Loss: 0.3646
Training Epoch: 38 [21504/50048]	Loss: 0.3926
Training Epoch: 38 [21632/50048]	Loss: 0.4720
Training Epoch: 38 [21760/50048]	Loss: 0.4568
Training Epoch: 38 [21888/50048]	Loss: 0.5148
Training Epoch: 38 [22016/50048]	Loss: 0.4402
Training Epoch: 38 [22144/50048]	Loss: 0.3838
Training Epoch: 38 [22272/50048]	Loss: 0.4135
Training Epoch: 38 [22400/50048]	Loss: 0.4274
Training Epoch: 38 [22528/50048]	Loss: 0.4709
Training Epoch: 38 [22656/50048]	Loss: 0.4745
Training Epoch: 38 [22784/50048]	Loss: 0.4350
Training Epoch: 38 [22912/50048]	Loss: 0.3669
Training Epoch: 38 [23040/50048]	Loss: 0.4233
Training Epoch: 38 [23168/50048]	Loss: 0.5126
Training Epoch: 38 [23296/50048]	Loss: 0.5029
Training Epoch: 38 [23424/50048]	Loss: 0.5117
Training Epoch: 38 [23552/50048]	Loss: 0.6677
Training Epoch: 38 [23680/50048]	Loss: 0.4863
Training Epoch: 38 [23808/50048]	Loss: 0.3656
Training Epoch: 38 [23936/50048]	Loss: 0.6743
Training Epoch: 38 [24064/50048]	Loss: 0.4800
Training Epoch: 38 [24192/50048]	Loss: 0.4499
Training Epoch: 38 [24320/50048]	Loss: 0.5644
Training Epoch: 38 [24448/50048]	Loss: 0.5538
Training Epoch: 38 [24576/50048]	Loss: 0.3594
Training Epoch: 38 [24704/50048]	Loss: 0.5390
Training Epoch: 38 [24832/50048]	Loss: 0.5860
Training Epoch: 38 [24960/50048]	Loss: 0.5411
Training Epoch: 38 [25088/50048]	Loss: 0.3132
Training Epoch: 38 [25216/50048]	Loss: 0.4073
Training Epoch: 38 [25344/50048]	Loss: 0.2704
Training Epoch: 38 [25472/50048]	Loss: 0.6001
Training Epoch: 38 [25600/50048]	Loss: 0.4228
Training Epoch: 38 [25728/50048]	Loss: 0.5636
Training Epoch: 38 [25856/50048]	Loss: 0.4753
Training Epoch: 38 [25984/50048]	Loss: 0.4500
Training Epoch: 38 [26112/50048]	Loss: 0.4761
Training Epoch: 38 [26240/50048]	Loss: 0.4692
Training Epoch: 38 [26368/50048]	Loss: 0.5962
Training Epoch: 38 [26496/50048]	Loss: 0.4505
Training Epoch: 38 [26624/50048]	Loss: 0.4105
Training Epoch: 38 [26752/50048]	Loss: 0.4499
Training Epoch: 38 [26880/50048]	Loss: 0.3505
Training Epoch: 38 [27008/50048]	Loss: 0.4393
Training Epoch: 38 [27136/50048]	Loss: 0.3963
Training Epoch: 38 [27264/50048]	Loss: 0.4780
Training Epoch: 38 [27392/50048]	Loss: 0.5540
Training Epoch: 38 [27520/50048]	Loss: 0.3352
Training Epoch: 38 [27648/50048]	Loss: 0.5044
Training Epoch: 38 [27776/50048]	Loss: 0.5192
Training Epoch: 38 [27904/50048]	Loss: 0.6310
Training Epoch: 38 [28032/50048]	Loss: 0.5386
Training Epoch: 38 [28160/50048]	Loss: 0.5234
Training Epoch: 38 [28288/50048]	Loss: 0.6323
Training Epoch: 38 [28416/50048]	Loss: 0.5974
Training Epoch: 38 [28544/50048]	Loss: 0.4382
Training Epoch: 38 [28672/50048]	Loss: 0.5094
Training Epoch: 38 [28800/50048]	Loss: 0.4125
Training Epoch: 38 [28928/50048]	Loss: 0.3742
Training Epoch: 38 [29056/50048]	Loss: 0.5163
Training Epoch: 38 [29184/50048]	Loss: 0.5269
Training Epoch: 38 [29312/50048]	Loss: 0.5075
Training Epoch: 38 [29440/50048]	Loss: 0.3488
Training Epoch: 38 [29568/50048]	Loss: 0.2951
Training Epoch: 38 [29696/50048]	Loss: 0.3842
Training Epoch: 38 [29824/50048]	Loss: 0.4679
Training Epoch: 38 [29952/50048]	Loss: 0.3740
Training Epoch: 38 [30080/50048]	Loss: 0.4058
Training Epoch: 38 [30208/50048]	Loss: 0.3914
Training Epoch: 38 [30336/50048]	Loss: 0.3872
Training Epoch: 38 [30464/50048]	Loss: 0.4193
Training Epoch: 38 [30592/50048]	Loss: 0.5346
Training Epoch: 38 [30720/50048]	Loss: 0.5101
Training Epoch: 38 [30848/50048]	Loss: 0.4752
Training Epoch: 38 [30976/50048]	Loss: 0.5592
Training Epoch: 38 [31104/50048]	Loss: 0.4564
Training Epoch: 38 [31232/50048]	Loss: 0.6068
Training Epoch: 38 [31360/50048]	Loss: 0.4088
Training Epoch: 38 [31488/50048]	Loss: 0.5940
Training Epoch: 38 [31616/50048]	Loss: 0.4482
Training Epoch: 38 [31744/50048]	Loss: 0.4599
Training Epoch: 38 [31872/50048]	Loss: 0.3283
Training Epoch: 38 [32000/50048]	Loss: 0.4687
Training Epoch: 38 [32128/50048]	Loss: 0.5645
Training Epoch: 38 [32256/50048]	Loss: 0.4456
Training Epoch: 38 [32384/50048]	Loss: 0.4446
Training Epoch: 38 [32512/50048]	Loss: 0.3601
Training Epoch: 38 [32640/50048]	Loss: 0.4241
Training Epoch: 38 [32768/50048]	Loss: 0.4889
Training Epoch: 38 [32896/50048]	Loss: 0.5366
Training Epoch: 38 [33024/50048]	Loss: 0.4380
Training Epoch: 38 [33152/50048]	Loss: 0.4807
Training Epoch: 38 [33280/50048]	Loss: 0.5027
Training Epoch: 38 [33408/50048]	Loss: 0.5419
Training Epoch: 38 [33536/50048]	Loss: 0.4970
Training Epoch: 38 [33664/50048]	Loss: 0.4742
Training Epoch: 38 [33792/50048]	Loss: 0.4885
Training Epoch: 38 [33920/50048]	Loss: 0.4586
Training Epoch: 38 [34048/50048]	Loss: 0.5161
Training Epoch: 38 [34176/50048]	Loss: 0.4404
Training Epoch: 38 [34304/50048]	Loss: 0.6220
Training Epoch: 38 [34432/50048]	Loss: 0.4257
Training Epoch: 38 [34560/50048]	Loss: 0.4967
Training Epoch: 38 [34688/50048]	Loss: 0.4671
Training Epoch: 38 [34816/50048]	Loss: 0.7463
Training Epoch: 38 [34944/50048]	Loss: 0.3912
Training Epoch: 38 [35072/50048]	Loss: 0.4367
Training Epoch: 38 [35200/50048]	Loss: 0.4787
Training Epoch: 38 [35328/50048]	Loss: 0.4057
Training Epoch: 38 [35456/50048]	Loss: 0.5852
Training Epoch: 38 [35584/50048]	Loss: 0.3868
Training Epoch: 38 [35712/50048]	Loss: 0.3842
Training Epoch: 38 [35840/50048]	Loss: 0.5131
Training Epoch: 38 [35968/50048]	Loss: 0.6292
Training Epoch: 38 [36096/50048]	Loss: 0.3839
Training Epoch: 38 [36224/50048]	Loss: 0.4332
Training Epoch: 38 [36352/50048]	Loss: 0.3617
Training Epoch: 38 [36480/50048]	Loss: 0.5237
Training Epoch: 38 [36608/50048]	Loss: 0.4852
Training Epoch: 38 [36736/50048]	Loss: 0.5847
Training Epoch: 38 [36864/50048]	Loss: 0.5429
Training Epoch: 38 [36992/50048]	Loss: 0.4985
Training Epoch: 38 [37120/50048]	Loss: 0.5435
Training Epoch: 38 [37248/50048]	Loss: 0.4265
Training Epoch: 38 [37376/50048]	Loss: 0.4443
Training Epoch: 38 [37504/50048]	Loss: 0.5080
Training Epoch: 38 [37632/50048]	Loss: 0.5272
Training Epoch: 38 [37760/50048]	Loss: 0.5290
Training Epoch: 38 [37888/50048]	Loss: 0.3756
Training Epoch: 38 [38016/50048]	Loss: 0.4809
Training Epoch: 38 [38144/50048]	Loss: 0.5048
Training Epoch: 38 [38272/50048]	Loss: 0.4798
Training Epoch: 38 [38400/50048]	Loss: 0.4154
Training Epoch: 38 [38528/50048]	Loss: 0.4095
Training Epoch: 38 [38656/50048]	Loss: 0.3561
Training Epoch: 38 [38784/50048]	Loss: 0.4364
Training Epoch: 38 [38912/50048]	Loss: 0.3792
Training Epoch: 38 [39040/50048]	Loss: 0.5083
Training Epoch: 38 [39168/50048]	Loss: 0.5303
Training Epoch: 38 [39296/50048]	Loss: 0.5288
Training Epoch: 38 [39424/50048]	Loss: 0.4230
Training Epoch: 38 [39552/50048]	Loss: 0.4431
Training Epoch: 38 [39680/50048]	Loss: 0.5262
Training Epoch: 38 [39808/50048]	Loss: 0.5562
Training Epoch: 38 [39936/50048]	Loss: 0.5044
Training Epoch: 38 [40064/50048]	Loss: 0.5931
Training Epoch: 38 [40192/50048]	Loss: 0.5553
Training Epoch: 38 [40320/50048]	Loss: 0.4359
Training Epoch: 38 [40448/50048]	Loss: 0.6015
Training Epoch: 38 [40576/50048]	Loss: 0.4058
Training Epoch: 38 [40704/50048]	Loss: 0.3848
Training Epoch: 38 [40832/50048]	Loss: 0.3397
Training Epoch: 38 [40960/50048]	Loss: 0.4655
Training Epoch: 38 [41088/50048]	Loss: 0.5503
Training Epoch: 38 [41216/50048]	Loss: 0.4224
Training Epoch: 38 [41344/50048]	Loss: 0.4016
Training Epoch: 38 [41472/50048]	Loss: 0.6002
Training Epoch: 38 [41600/50048]	Loss: 0.4524
Training Epoch: 38 [41728/50048]	Loss: 0.6523
Training Epoch: 38 [41856/50048]	Loss: 0.5012
Training Epoch: 38 [41984/50048]	Loss: 0.4923
Training Epoch: 38 [42112/50048]	Loss: 0.4781
Training Epoch: 38 [42240/50048]	Loss: 0.4687
Training Epoch: 38 [42368/50048]	Loss: 0.6107
Training Epoch: 38 [42496/50048]	Loss: 0.6337
Training Epoch: 38 [42624/50048]	Loss: 0.6605
Training Epoch: 38 [42752/50048]	Loss: 0.5307
Training Epoch: 38 [42880/50048]	Loss: 0.3870
Training Epoch: 38 [43008/50048]	Loss: 0.5631
Training Epoch: 38 [43136/50048]	Loss: 0.5270
Training Epoch: 38 [43264/50048]	Loss: 0.4037
Training Epoch: 38 [43392/50048]	Loss: 0.4076
Training Epoch: 38 [43520/50048]	Loss: 0.4990
Training Epoch: 38 [43648/50048]	Loss: 0.4427
Training Epoch: 38 [43776/50048]	Loss: 0.5091
Training Epoch: 38 [43904/50048]	Loss: 0.6989
Training Epoch: 38 [44032/50048]	Loss: 0.5210
Training Epoch: 38 [44160/50048]	Loss: 0.4158
Training Epoch: 38 [44288/50048]	Loss: 0.5728
Training Epoch: 38 [44416/50048]	Loss: 0.3782
Training Epoch: 38 [44544/50048]	Loss: 0.5622
Training Epoch: 38 [44672/50048]	Loss: 0.5602
Training Epoch: 38 [44800/50048]	Loss: 0.6374
Training Epoch: 38 [44928/50048]	Loss: 0.3585
Training Epoch: 38 [45056/50048]	Loss: 0.3249
Training Epoch: 38 [45184/50048]	Loss: 0.3819
Training Epoch: 38 [45312/50048]	Loss: 0.5206
Training Epoch: 38 [45440/50048]	Loss: 0.5497
Training Epoch: 38 [45568/50048]	Loss: 0.5939
Training Epoch: 38 [45696/50048]	Loss: 0.5176
2022-12-06 07:06:28,132 [ZeusDataLoader(train)] train epoch 39 done: time=86.45 energy=10490.15
2022-12-06 07:06:28,134 [ZeusDataLoader(eval)] Epoch 39 begin.
Training Epoch: 38 [45824/50048]	Loss: 0.3311
Training Epoch: 38 [45952/50048]	Loss: 0.6031
Training Epoch: 38 [46080/50048]	Loss: 0.3915
Training Epoch: 38 [46208/50048]	Loss: 0.3900
Training Epoch: 38 [46336/50048]	Loss: 0.6021
Training Epoch: 38 [46464/50048]	Loss: 0.4525
Training Epoch: 38 [46592/50048]	Loss: 0.4692
Training Epoch: 38 [46720/50048]	Loss: 0.6170
Training Epoch: 38 [46848/50048]	Loss: 0.5241
Training Epoch: 38 [46976/50048]	Loss: 0.4833
Training Epoch: 38 [47104/50048]	Loss: 0.4791
Training Epoch: 38 [47232/50048]	Loss: 0.6855
Training Epoch: 38 [47360/50048]	Loss: 0.3846
Training Epoch: 38 [47488/50048]	Loss: 0.5028
Training Epoch: 38 [47616/50048]	Loss: 0.5759
Training Epoch: 38 [47744/50048]	Loss: 0.5887
Training Epoch: 38 [47872/50048]	Loss: 0.4007
Training Epoch: 38 [48000/50048]	Loss: 0.4982
Training Epoch: 38 [48128/50048]	Loss: 0.5991
Training Epoch: 38 [48256/50048]	Loss: 0.5563
Training Epoch: 38 [48384/50048]	Loss: 0.5290
Training Epoch: 38 [48512/50048]	Loss: 0.5026
Training Epoch: 38 [48640/50048]	Loss: 0.6559
Training Epoch: 38 [48768/50048]	Loss: 0.4571
Training Epoch: 38 [48896/50048]	Loss: 0.4796
Training Epoch: 38 [49024/50048]	Loss: 0.5202
Training Epoch: 38 [49152/50048]	Loss: 0.6768
Training Epoch: 38 [49280/50048]	Loss: 0.4412
Training Epoch: 38 [49408/50048]	Loss: 0.5320
Training Epoch: 38 [49536/50048]	Loss: 0.5906
Training Epoch: 38 [49664/50048]	Loss: 0.3957
Training Epoch: 38 [49792/50048]	Loss: 0.5408
Training Epoch: 38 [49920/50048]	Loss: 0.5763
Training Epoch: 38 [50048/50048]	Loss: 0.3813
2022-12-06 12:06:31.811 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:06:31,850 [ZeusDataLoader(eval)] eval epoch 39 done: time=3.71 energy=453.27
2022-12-06 07:06:31,851 [ZeusDataLoader(train)] Up to epoch 39: time=3517.38, energy=426989.35, cost=521265.36
2022-12-06 07:06:31,851 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:06:31,851 [ZeusDataLoader(train)] Expected next epoch: time=3607.18, energy=437787.37, cost=534521.74
2022-12-06 07:06:31,852 [ZeusDataLoader(train)] Epoch 40 begin.
Validation Epoch: 38, Average loss: 0.0136, Accuracy: 0.6188
2022-12-06 07:06:32,052 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:06:32,052 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:06:32.054 [ZeusMonitor] Monitor started.
2022-12-06 12:06:32.054 [ZeusMonitor] Running indefinitely. 2022-12-06 12:06:32.054 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:06:32.054 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e40+gpu0.power.log
Training Epoch: 39 [128/50048]	Loss: 0.3218
Training Epoch: 39 [256/50048]	Loss: 0.3574
Training Epoch: 39 [384/50048]	Loss: 0.3741
Training Epoch: 39 [512/50048]	Loss: 0.3799
Training Epoch: 39 [640/50048]	Loss: 0.3976
Training Epoch: 39 [768/50048]	Loss: 0.3751
Training Epoch: 39 [896/50048]	Loss: 0.4640
Training Epoch: 39 [1024/50048]	Loss: 0.4359
Training Epoch: 39 [1152/50048]	Loss: 0.3373
Training Epoch: 39 [1280/50048]	Loss: 0.3095
Training Epoch: 39 [1408/50048]	Loss: 0.4141
Training Epoch: 39 [1536/50048]	Loss: 0.4298
Training Epoch: 39 [1664/50048]	Loss: 0.4026
Training Epoch: 39 [1792/50048]	Loss: 0.4478
Training Epoch: 39 [1920/50048]	Loss: 0.3669
Training Epoch: 39 [2048/50048]	Loss: 0.4260
Training Epoch: 39 [2176/50048]	Loss: 0.2998
Training Epoch: 39 [2304/50048]	Loss: 0.2894
Training Epoch: 39 [2432/50048]	Loss: 0.4717
Training Epoch: 39 [2560/50048]	Loss: 0.4778
Training Epoch: 39 [2688/50048]	Loss: 0.4655
Training Epoch: 39 [2816/50048]	Loss: 0.3220
Training Epoch: 39 [2944/50048]	Loss: 0.3513
Training Epoch: 39 [3072/50048]	Loss: 0.3946
Training Epoch: 39 [3200/50048]	Loss: 0.4037
Training Epoch: 39 [3328/50048]	Loss: 0.3796
Training Epoch: 39 [3456/50048]	Loss: 0.4147
Training Epoch: 39 [3584/50048]	Loss: 0.2785
Training Epoch: 39 [3712/50048]	Loss: 0.3753
Training Epoch: 39 [3840/50048]	Loss: 0.3990
Training Epoch: 39 [3968/50048]	Loss: 0.3835
Training Epoch: 39 [4096/50048]	Loss: 0.3620
Training Epoch: 39 [4224/50048]	Loss: 0.3450
Training Epoch: 39 [4352/50048]	Loss: 0.4146
Training Epoch: 39 [4480/50048]	Loss: 0.4914
Training Epoch: 39 [4608/50048]	Loss: 0.4269
Training Epoch: 39 [4736/50048]	Loss: 0.3674
Training Epoch: 39 [4864/50048]	Loss: 0.4049
Training Epoch: 39 [4992/50048]	Loss: 0.5530
Training Epoch: 39 [5120/50048]	Loss: 0.4186
Training Epoch: 39 [5248/50048]	Loss: 0.3678
Training Epoch: 39 [5376/50048]	Loss: 0.4421
Training Epoch: 39 [5504/50048]	Loss: 0.3970
Training Epoch: 39 [5632/50048]	Loss: 0.3893
Training Epoch: 39 [5760/50048]	Loss: 0.5129
Training Epoch: 39 [5888/50048]	Loss: 0.4252
Training Epoch: 39 [6016/50048]	Loss: 0.3656
Training Epoch: 39 [6144/50048]	Loss: 0.4915
Training Epoch: 39 [6272/50048]	Loss: 0.3941
Training Epoch: 39 [6400/50048]	Loss: 0.3314
Training Epoch: 39 [6528/50048]	Loss: 0.3671
Training Epoch: 39 [6656/50048]	Loss: 0.3778
Training Epoch: 39 [6784/50048]	Loss: 0.6449
Training Epoch: 39 [6912/50048]	Loss: 0.4086
Training Epoch: 39 [7040/50048]	Loss: 0.4555
Training Epoch: 39 [7168/50048]	Loss: 0.3583
Training Epoch: 39 [7296/50048]	Loss: 0.5193
Training Epoch: 39 [7424/50048]	Loss: 0.4797
Training Epoch: 39 [7552/50048]	Loss: 0.3394
Training Epoch: 39 [7680/50048]	Loss: 0.4941
Training Epoch: 39 [7808/50048]	Loss: 0.4228
Training Epoch: 39 [7936/50048]	Loss: 0.5624
Training Epoch: 39 [8064/50048]	Loss: 0.3883
Training Epoch: 39 [8192/50048]	Loss: 0.5360
Training Epoch: 39 [8320/50048]	Loss: 0.4681
Training Epoch: 39 [8448/50048]	Loss: 0.3858
Training Epoch: 39 [8576/50048]	Loss: 0.4285
Training Epoch: 39 [8704/50048]	Loss: 0.4258
Training Epoch: 39 [8832/50048]	Loss: 0.3562
Training Epoch: 39 [8960/50048]	Loss: 0.4174
Training Epoch: 39 [9088/50048]	Loss: 0.4021
Training Epoch: 39 [9216/50048]	Loss: 0.3751
Training Epoch: 39 [9344/50048]	Loss: 0.4603
Training Epoch: 39 [9472/50048]	Loss: 0.4343
Training Epoch: 39 [9600/50048]	Loss: 0.3011
Training Epoch: 39 [9728/50048]	Loss: 0.4131
Training Epoch: 39 [9856/50048]	Loss: 0.4132
Training Epoch: 39 [9984/50048]	Loss: 0.3756
Training Epoch: 39 [10112/50048]	Loss: 0.3756
Training Epoch: 39 [10240/50048]	Loss: 0.4915
Training Epoch: 39 [10368/50048]	Loss: 0.4494
Training Epoch: 39 [10496/50048]	Loss: 0.4682
Training Epoch: 39 [10624/50048]	Loss: 0.3457
Training Epoch: 39 [10752/50048]	Loss: 0.4209
Training Epoch: 39 [10880/50048]	Loss: 0.4168
Training Epoch: 39 [11008/50048]	Loss: 0.4522
Training Epoch: 39 [11136/50048]	Loss: 0.4168
Training Epoch: 39 [11264/50048]	Loss: 0.5673
Training Epoch: 39 [11392/50048]	Loss: 0.3068
Training Epoch: 39 [11520/50048]	Loss: 0.3544
Training Epoch: 39 [11648/50048]	Loss: 0.3567
Training Epoch: 39 [11776/50048]	Loss: 0.3507
Training Epoch: 39 [11904/50048]	Loss: 0.3808
Training Epoch: 39 [12032/50048]	Loss: 0.2766
Training Epoch: 39 [12160/50048]	Loss: 0.4623
Training Epoch: 39 [12288/50048]	Loss: 0.3928
Training Epoch: 39 [12416/50048]	Loss: 0.4156
Training Epoch: 39 [12544/50048]	Loss: 0.4359
Training Epoch: 39 [12672/50048]	Loss: 0.3597
Training Epoch: 39 [12800/50048]	Loss: 0.3296
Training Epoch: 39 [12928/50048]	Loss: 0.5129
Training Epoch: 39 [13056/50048]	Loss: 0.4721
Training Epoch: 39 [13184/50048]	Loss: 0.2733
Training Epoch: 39 [13312/50048]	Loss: 0.3669
Training Epoch: 39 [13440/50048]	Loss: 0.4490
Training Epoch: 39 [13568/50048]	Loss: 0.3683
Training Epoch: 39 [13696/50048]	Loss: 0.4557
Training Epoch: 39 [13824/50048]	Loss: 0.4316
Training Epoch: 39 [13952/50048]	Loss: 0.4213
Training Epoch: 39 [14080/50048]	Loss: 0.3991
Training Epoch: 39 [14208/50048]	Loss: 0.4320
Training Epoch: 39 [14336/50048]	Loss: 0.4891
Training Epoch: 39 [14464/50048]	Loss: 0.4774
Training Epoch: 39 [14592/50048]	Loss: 0.4709
Training Epoch: 39 [14720/50048]	Loss: 0.2811
Training Epoch: 39 [14848/50048]	Loss: 0.4444
Training Epoch: 39 [14976/50048]	Loss: 0.4716
Training Epoch: 39 [15104/50048]	Loss: 0.3917
Training Epoch: 39 [15232/50048]	Loss: 0.5066
Training Epoch: 39 [15360/50048]	Loss: 0.3132
Training Epoch: 39 [15488/50048]	Loss: 0.3662
Training Epoch: 39 [15616/50048]	Loss: 0.3176
Training Epoch: 39 [15744/50048]	Loss: 0.2619
Training Epoch: 39 [15872/50048]	Loss: 0.3389
Training Epoch: 39 [16000/50048]	Loss: 0.3885
Training Epoch: 39 [16128/50048]	Loss: 0.4991
Training Epoch: 39 [16256/50048]	Loss: 0.4133
Training Epoch: 39 [16384/50048]	Loss: 0.4088
Training Epoch: 39 [16512/50048]	Loss: 0.3828
Training Epoch: 39 [16640/50048]	Loss: 0.4519
Training Epoch: 39 [16768/50048]	Loss: 0.5038
Training Epoch: 39 [16896/50048]	Loss: 0.4511
Training Epoch: 39 [17024/50048]	Loss: 0.3922
Training Epoch: 39 [17152/50048]	Loss: 0.3719
Training Epoch: 39 [17280/50048]	Loss: 0.3662
Training Epoch: 39 [17408/50048]	Loss: 0.4822
Training Epoch: 39 [17536/50048]	Loss: 0.3599
Training Epoch: 39 [17664/50048]	Loss: 0.3231
Training Epoch: 39 [17792/50048]	Loss: 0.3638
Training Epoch: 39 [17920/50048]	Loss: 0.3944
Training Epoch: 39 [18048/50048]	Loss: 0.5883
Training Epoch: 39 [18176/50048]	Loss: 0.3993
Training Epoch: 39 [18304/50048]	Loss: 0.4361
Training Epoch: 39 [18432/50048]	Loss: 0.4496
Training Epoch: 39 [18560/50048]	Loss: 0.4211
Training Epoch: 39 [18688/50048]	Loss: 0.3276
Training Epoch: 39 [18816/50048]	Loss: 0.4520
Training Epoch: 39 [18944/50048]	Loss: 0.4374
Training Epoch: 39 [19072/50048]	Loss: 0.4137
Training Epoch: 39 [19200/50048]	Loss: 0.3687
Training Epoch: 39 [19328/50048]	Loss: 0.4073
Training Epoch: 39 [19456/50048]	Loss: 0.4914
Training Epoch: 39 [19584/50048]	Loss: 0.4524
Training Epoch: 39 [19712/50048]	Loss: 0.4187
Training Epoch: 39 [19840/50048]	Loss: 0.5366
Training Epoch: 39 [19968/50048]	Loss: 0.4075
Training Epoch: 39 [20096/50048]	Loss: 0.3882
Training Epoch: 39 [20224/50048]	Loss: 0.5453
Training Epoch: 39 [20352/50048]	Loss: 0.5039
Training Epoch: 39 [20480/50048]	Loss: 0.4964
Training Epoch: 39 [20608/50048]	Loss: 0.4892
Training Epoch: 39 [20736/50048]	Loss: 0.3621
Training Epoch: 39 [20864/50048]	Loss: 0.4955
Training Epoch: 39 [20992/50048]	Loss: 0.4890
Training Epoch: 39 [21120/50048]	Loss: 0.4865
Training Epoch: 39 [21248/50048]	Loss: 0.5628
Training Epoch: 39 [21376/50048]	Loss: 0.4495
Training Epoch: 39 [21504/50048]	Loss: 0.5315
Training Epoch: 39 [21632/50048]	Loss: 0.3398
Training Epoch: 39 [21760/50048]	Loss: 0.3748
Training Epoch: 39 [21888/50048]	Loss: 0.6099
Training Epoch: 39 [22016/50048]	Loss: 0.4705
Training Epoch: 39 [22144/50048]	Loss: 0.4276
Training Epoch: 39 [22272/50048]	Loss: 0.6542
Training Epoch: 39 [22400/50048]	Loss: 0.4890
Training Epoch: 39 [22528/50048]	Loss: 0.4044
Training Epoch: 39 [22656/50048]	Loss: 0.3667
Training Epoch: 39 [22784/50048]	Loss: 0.6505
Training Epoch: 39 [22912/50048]	Loss: 0.5157
Training Epoch: 39 [23040/50048]	Loss: 0.5215
Training Epoch: 39 [23168/50048]	Loss: 0.3771
Training Epoch: 39 [23296/50048]	Loss: 0.5507
Training Epoch: 39 [23424/50048]	Loss: 0.5372
Training Epoch: 39 [23552/50048]	Loss: 0.3297
Training Epoch: 39 [23680/50048]	Loss: 0.3848
Training Epoch: 39 [23808/50048]	Loss: 0.4733
Training Epoch: 39 [23936/50048]	Loss: 0.4359
Training Epoch: 39 [24064/50048]	Loss: 0.4934
Training Epoch: 39 [24192/50048]	Loss: 0.4621
Training Epoch: 39 [24320/50048]	Loss: 0.2694
Training Epoch: 39 [24448/50048]	Loss: 0.4702
Training Epoch: 39 [24576/50048]	Loss: 0.4689
Training Epoch: 39 [24704/50048]	Loss: 0.4896
Training Epoch: 39 [24832/50048]	Loss: 0.5668
Training Epoch: 39 [24960/50048]	Loss: 0.5156
Training Epoch: 39 [25088/50048]	Loss: 0.6585
Training Epoch: 39 [25216/50048]	Loss: 0.3847
Training Epoch: 39 [25344/50048]	Loss: 0.4024
Training Epoch: 39 [25472/50048]	Loss: 0.3745
Training Epoch: 39 [25600/50048]	Loss: 0.4516
Training Epoch: 39 [25728/50048]	Loss: 0.3533
Training Epoch: 39 [25856/50048]	Loss: 0.3676
Training Epoch: 39 [25984/50048]	Loss: 0.3271
Training Epoch: 39 [26112/50048]	Loss: 0.4257
Training Epoch: 39 [26240/50048]	Loss: 0.3620
Training Epoch: 39 [26368/50048]	Loss: 0.2964
Training Epoch: 39 [26496/50048]	Loss: 0.5767
Training Epoch: 39 [26624/50048]	Loss: 0.5256
Training Epoch: 39 [26752/50048]	Loss: 0.5310
Training Epoch: 39 [26880/50048]	Loss: 0.3728
Training Epoch: 39 [27008/50048]	Loss: 0.4495
Training Epoch: 39 [27136/50048]	Loss: 0.4872
Training Epoch: 39 [27264/50048]	Loss: 0.4051
Training Epoch: 39 [27392/50048]	Loss: 0.4525
Training Epoch: 39 [27520/50048]	Loss: 0.5087
Training Epoch: 39 [27648/50048]	Loss: 0.4447
Training Epoch: 39 [27776/50048]	Loss: 0.4899
Training Epoch: 39 [27904/50048]	Loss: 0.3771
Training Epoch: 39 [28032/50048]	Loss: 0.5110
Training Epoch: 39 [28160/50048]	Loss: 0.4484
Training Epoch: 39 [28288/50048]	Loss: 0.3865
Training Epoch: 39 [28416/50048]	Loss: 0.3624
Training Epoch: 39 [28544/50048]	Loss: 0.5015
Training Epoch: 39 [28672/50048]	Loss: 0.3250
Training Epoch: 39 [28800/50048]	Loss: 0.3962
Training Epoch: 39 [28928/50048]	Loss: 0.3473
Training Epoch: 39 [29056/50048]	Loss: 0.4675
Training Epoch: 39 [29184/50048]	Loss: 0.5194
Training Epoch: 39 [29312/50048]	Loss: 0.4780
Training Epoch: 39 [29440/50048]	Loss: 0.5429
Training Epoch: 39 [29568/50048]	Loss: 0.5145
Training Epoch: 39 [29696/50048]	Loss: 0.5276
Training Epoch: 39 [29824/50048]	Loss: 0.4613
Training Epoch: 39 [29952/50048]	Loss: 0.5941
Training Epoch: 39 [30080/50048]	Loss: 0.4072
Training Epoch: 39 [30208/50048]	Loss: 0.3467
Training Epoch: 39 [30336/50048]	Loss: 0.3907
Training Epoch: 39 [30464/50048]	Loss: 0.3473
Training Epoch: 39 [30592/50048]	Loss: 0.3926
Training Epoch: 39 [30720/50048]	Loss: 0.6791
Training Epoch: 39 [30848/50048]	Loss: 0.4448
Training Epoch: 39 [30976/50048]	Loss: 0.5885
Training Epoch: 39 [31104/50048]	Loss: 0.3610
Training Epoch: 39 [31232/50048]	Loss: 0.3840
Training Epoch: 39 [31360/50048]	Loss: 0.3996
Training Epoch: 39 [31488/50048]	Loss: 0.4973
Training Epoch: 39 [31616/50048]	Loss: 0.4651
Training Epoch: 39 [31744/50048]	Loss: 0.4617
Training Epoch: 39 [31872/50048]	Loss: 0.4477
Training Epoch: 39 [32000/50048]	Loss: 0.4846
Training Epoch: 39 [32128/50048]	Loss: 0.4569
Training Epoch: 39 [32256/50048]	Loss: 0.4666
Training Epoch: 39 [32384/50048]	Loss: 0.3774
Training Epoch: 39 [32512/50048]	Loss: 0.3992
Training Epoch: 39 [32640/50048]	Loss: 0.5292
Training Epoch: 39 [32768/50048]	Loss: 0.4697
Training Epoch: 39 [32896/50048]	Loss: 0.5866
Training Epoch: 39 [33024/50048]	Loss: 0.4763
Training Epoch: 39 [33152/50048]	Loss: 0.5506
Training Epoch: 39 [33280/50048]	Loss: 0.4024
Training Epoch: 39 [33408/50048]	Loss: 0.3948
Training Epoch: 39 [33536/50048]	Loss: 0.4338
Training Epoch: 39 [33664/50048]	Loss: 0.4290
Training Epoch: 39 [33792/50048]	Loss: 0.4771
Training Epoch: 39 [33920/50048]	Loss: 0.3685
Training Epoch: 39 [34048/50048]	Loss: 0.3991
Training Epoch: 39 [34176/50048]	Loss: 0.4566
Training Epoch: 39 [34304/50048]	Loss: 0.4485
Training Epoch: 39 [34432/50048]	Loss: 0.4173
Training Epoch: 39 [34560/50048]	Loss: 0.4996
Training Epoch: 39 [34688/50048]	Loss: 0.4462
Training Epoch: 39 [34816/50048]	Loss: 0.5498
Training Epoch: 39 [34944/50048]	Loss: 0.6419
Training Epoch: 39 [35072/50048]	Loss: 0.4182
Training Epoch: 39 [35200/50048]	Loss: 0.3920
Training Epoch: 39 [35328/50048]	Loss: 0.3300
Training Epoch: 39 [35456/50048]	Loss: 0.5077
Training Epoch: 39 [35584/50048]	Loss: 0.4797
Training Epoch: 39 [35712/50048]	Loss: 0.6092
Training Epoch: 39 [35840/50048]	Loss: 0.3790
Training Epoch: 39 [35968/50048]	Loss: 0.4694
Training Epoch: 39 [36096/50048]	Loss: 0.5137
Training Epoch: 39 [36224/50048]	Loss: 0.4160
Training Epoch: 39 [36352/50048]	Loss: 0.4728
Training Epoch: 39 [36480/50048]	Loss: 0.5273
Training Epoch: 39 [36608/50048]	Loss: 0.3806
Training Epoch: 39 [36736/50048]	Loss: 0.3408
Training Epoch: 39 [36864/50048]	Loss: 0.4866
Training Epoch: 39 [36992/50048]	Loss: 0.5045
Training Epoch: 39 [37120/50048]	Loss: 0.5437
Training Epoch: 39 [37248/50048]	Loss: 0.4738
Training Epoch: 39 [37376/50048]	Loss: 0.5307
Training Epoch: 39 [37504/50048]	Loss: 0.3960
Training Epoch: 39 [37632/50048]	Loss: 0.4698
Training Epoch: 39 [37760/50048]	Loss: 0.3560
Training Epoch: 39 [37888/50048]	Loss: 0.5248
Training Epoch: 39 [38016/50048]	Loss: 0.4997
Training Epoch: 39 [38144/50048]	Loss: 0.4413
Training Epoch: 39 [38272/50048]	Loss: 0.4761
Training Epoch: 39 [38400/50048]	Loss: 0.5614
Training Epoch: 39 [38528/50048]	Loss: 0.4609
Training Epoch: 39 [38656/50048]	Loss: 0.4878
Training Epoch: 39 [38784/50048]	Loss: 0.5028
Training Epoch: 39 [38912/50048]	Loss: 0.4407
Training Epoch: 39 [39040/50048]	Loss: 0.6258
Training Epoch: 39 [39168/50048]	Loss: 0.4354
Training Epoch: 39 [39296/50048]	Loss: 0.6045
Training Epoch: 39 [39424/50048]	Loss: 0.4886
Training Epoch: 39 [39552/50048]	Loss: 0.5896
Training Epoch: 39 [39680/50048]	Loss: 0.5451
Training Epoch: 39 [39808/50048]	Loss: 0.5134
Training Epoch: 39 [39936/50048]	Loss: 0.5603
Training Epoch: 39 [40064/50048]	Loss: 0.4278
Training Epoch: 39 [40192/50048]	Loss: 0.4198
Training Epoch: 39 [40320/50048]	Loss: 0.3951
Training Epoch: 39 [40448/50048]	Loss: 0.3729
Training Epoch: 39 [40576/50048]	Loss: 0.3800
Training Epoch: 39 [40704/50048]	Loss: 0.4224
Training Epoch: 39 [40832/50048]	Loss: 0.5010
Training Epoch: 39 [40960/50048]	Loss: 0.4819
Training Epoch: 39 [41088/50048]	Loss: 0.4284
Training Epoch: 39 [41216/50048]	Loss: 0.4485
Training Epoch: 39 [41344/50048]	Loss: 0.5318
Training Epoch: 39 [41472/50048]	Loss: 0.4857
Training Epoch: 39 [41600/50048]	Loss: 0.5683
Training Epoch: 39 [41728/50048]	Loss: 0.5559
Training Epoch: 39 [41856/50048]	Loss: 0.3958
Training Epoch: 39 [41984/50048]	Loss: 0.4395
Training Epoch: 39 [42112/50048]	Loss: 0.5937
Training Epoch: 39 [42240/50048]	Loss: 0.4641
Training Epoch: 39 [42368/50048]	Loss: 0.3343
Training Epoch: 39 [42496/50048]	Loss: 0.3353
Training Epoch: 39 [42624/50048]	Loss: 0.4448
Training Epoch: 39 [42752/50048]	Loss: 0.6212
Training Epoch: 39 [42880/50048]	Loss: 0.3960
Training Epoch: 39 [43008/50048]	Loss: 0.4687
Training Epoch: 39 [43136/50048]	Loss: 0.4810
Training Epoch: 39 [43264/50048]	Loss: 0.5138
Training Epoch: 39 [43392/50048]	Loss: 0.4936
Training Epoch: 39 [43520/50048]	Loss: 0.5364
Training Epoch: 39 [43648/50048]	Loss: 0.3441
Training Epoch: 39 [43776/50048]	Loss: 0.4769
Training Epoch: 39 [43904/50048]	Loss: 0.4572
Training Epoch: 39 [44032/50048]	Loss: 0.3850
Training Epoch: 39 [44160/50048]	Loss: 0.5089
Training Epoch: 39 [44288/50048]	Loss: 0.6071
Training Epoch: 39 [44416/50048]	Loss: 0.2775
Training Epoch: 39 [44544/50048]	Loss: 0.4959
Training Epoch: 39 [44672/50048]	Loss: 0.5303
Training Epoch: 39 [44800/50048]	Loss: 0.4980
Training Epoch: 39 [44928/50048]	Loss: 0.6313
Training Epoch: 39 [45056/50048]	Loss: 0.5187
Training Epoch: 39 [45184/50048]	Loss: 0.6117
Training Epoch: 39 [45312/50048]	Loss: 0.5423
Training Epoch: 39 [45440/50048]	Loss: 0.4944
Training Epoch: 39 [45568/50048]	Loss: 0.3503
Training Epoch: 39 [45696/50048]	Loss: 0.3541
2022-12-06 07:07:58,353 [ZeusDataLoader(train)] train epoch 40 done: time=86.49 energy=10504.81
2022-12-06 07:07:58,354 [ZeusDataLoader(eval)] Epoch 40 begin.
Training Epoch: 39 [45824/50048]	Loss: 0.5213
Training Epoch: 39 [45952/50048]	Loss: 0.6265
Training Epoch: 39 [46080/50048]	Loss: 0.4708
Training Epoch: 39 [46208/50048]	Loss: 0.5054
Training Epoch: 39 [46336/50048]	Loss: 0.5278
Training Epoch: 39 [46464/50048]	Loss: 0.4365
Training Epoch: 39 [46592/50048]	Loss: 0.7174
Training Epoch: 39 [46720/50048]	Loss: 0.4050
Training Epoch: 39 [46848/50048]	Loss: 0.4794
Training Epoch: 39 [46976/50048]	Loss: 0.6256
Training Epoch: 39 [47104/50048]	Loss: 0.3903
Training Epoch: 39 [47232/50048]	Loss: 0.4042
Training Epoch: 39 [47360/50048]	Loss: 0.5343
Training Epoch: 39 [47488/50048]	Loss: 0.5507
Training Epoch: 39 [47616/50048]	Loss: 0.3491
Training Epoch: 39 [47744/50048]	Loss: 0.3507
Training Epoch: 39 [47872/50048]	Loss: 0.3272
Training Epoch: 39 [48000/50048]	Loss: 0.4688
Training Epoch: 39 [48128/50048]	Loss: 0.5926
Training Epoch: 39 [48256/50048]	Loss: 0.5965
Training Epoch: 39 [48384/50048]	Loss: 0.4416
Training Epoch: 39 [48512/50048]	Loss: 0.3911
Training Epoch: 39 [48640/50048]	Loss: 0.3610
Training Epoch: 39 [48768/50048]	Loss: 0.5734
Training Epoch: 39 [48896/50048]	Loss: 0.5132
Training Epoch: 39 [49024/50048]	Loss: 0.4611
Training Epoch: 39 [49152/50048]	Loss: 0.4031
Training Epoch: 39 [49280/50048]	Loss: 0.5036
Training Epoch: 39 [49408/50048]	Loss: 0.5749
Training Epoch: 39 [49536/50048]	Loss: 0.4964
Training Epoch: 39 [49664/50048]	Loss: 0.5222
Training Epoch: 39 [49792/50048]	Loss: 0.5799
Training Epoch: 39 [49920/50048]	Loss: 0.4331
Training Epoch: 39 [50048/50048]	Loss: 0.6437
2022-12-06 12:08:02.103 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:08:02,146 [ZeusDataLoader(eval)] eval epoch 40 done: time=3.78 energy=451.96
2022-12-06 07:08:02,146 [ZeusDataLoader(train)] Up to epoch 40: time=3607.65, energy=437946.12, cost=534642.65
2022-12-06 07:08:02,146 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:08:02,147 [ZeusDataLoader(train)] Expected next epoch: time=3697.45, energy=448744.13, cost=547899.03
2022-12-06 07:08:02,147 [ZeusDataLoader(train)] Epoch 41 begin.
Validation Epoch: 39, Average loss: 0.0138, Accuracy: 0.6174
2022-12-06 07:08:02,352 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:08:02,353 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:08:02.354 [ZeusMonitor] Monitor started.
2022-12-06 12:08:02.355 [ZeusMonitor] Running indefinitely. 2022-12-06 12:08:02.355 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:08:02.355 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e41+gpu0.power.log
Training Epoch: 40 [128/50048]	Loss: 0.3949
Training Epoch: 40 [256/50048]	Loss: 0.3212
Training Epoch: 40 [384/50048]	Loss: 0.3336
Training Epoch: 40 [512/50048]	Loss: 0.4839
Training Epoch: 40 [640/50048]	Loss: 0.4573
Training Epoch: 40 [768/50048]	Loss: 0.3398
Training Epoch: 40 [896/50048]	Loss: 0.3181
Training Epoch: 40 [1024/50048]	Loss: 0.3534
Training Epoch: 40 [1152/50048]	Loss: 0.3629
Training Epoch: 40 [1280/50048]	Loss: 0.4023
Training Epoch: 40 [1408/50048]	Loss: 0.3947
Training Epoch: 40 [1536/50048]	Loss: 0.3363
Training Epoch: 40 [1664/50048]	Loss: 0.4112
Training Epoch: 40 [1792/50048]	Loss: 0.3374
Training Epoch: 40 [1920/50048]	Loss: 0.3446
Training Epoch: 40 [2048/50048]	Loss: 0.3444
Training Epoch: 40 [2176/50048]	Loss: 0.3619
Training Epoch: 40 [2304/50048]	Loss: 0.4749
Training Epoch: 40 [2432/50048]	Loss: 0.4382
Training Epoch: 40 [2560/50048]	Loss: 0.4100
Training Epoch: 40 [2688/50048]	Loss: 0.3077
Training Epoch: 40 [2816/50048]	Loss: 0.4288
Training Epoch: 40 [2944/50048]	Loss: 0.4454
Training Epoch: 40 [3072/50048]	Loss: 0.4099
Training Epoch: 40 [3200/50048]	Loss: 0.5240
Training Epoch: 40 [3328/50048]	Loss: 0.3419
Training Epoch: 40 [3456/50048]	Loss: 0.4413
Training Epoch: 40 [3584/50048]	Loss: 0.4200
Training Epoch: 40 [3712/50048]	Loss: 0.4836
Training Epoch: 40 [3840/50048]	Loss: 0.4279
Training Epoch: 40 [3968/50048]	Loss: 0.2959
Training Epoch: 40 [4096/50048]	Loss: 0.3507
Training Epoch: 40 [4224/50048]	Loss: 0.2557
Training Epoch: 40 [4352/50048]	Loss: 0.3598
Training Epoch: 40 [4480/50048]	Loss: 0.3185
Training Epoch: 40 [4608/50048]	Loss: 0.3887
Training Epoch: 40 [4736/50048]	Loss: 0.4709
Training Epoch: 40 [4864/50048]	Loss: 0.5074
Training Epoch: 40 [4992/50048]	Loss: 0.4419
Training Epoch: 40 [5120/50048]	Loss: 0.4186
Training Epoch: 40 [5248/50048]	Loss: 0.5294
Training Epoch: 40 [5376/50048]	Loss: 0.3650
Training Epoch: 40 [5504/50048]	Loss: 0.4562
Training Epoch: 40 [5632/50048]	Loss: 0.3021
Training Epoch: 40 [5760/50048]	Loss: 0.3455
Training Epoch: 40 [5888/50048]	Loss: 0.3738
Training Epoch: 40 [6016/50048]	Loss: 0.3786
Training Epoch: 40 [6144/50048]	Loss: 0.4178
Training Epoch: 40 [6272/50048]	Loss: 0.4561
Training Epoch: 40 [6400/50048]	Loss: 0.4123
Training Epoch: 40 [6528/50048]	Loss: 0.2979
Training Epoch: 40 [6656/50048]	Loss: 0.5058
Training Epoch: 40 [6784/50048]	Loss: 0.4589
Training Epoch: 40 [6912/50048]	Loss: 0.2881
Training Epoch: 40 [7040/50048]	Loss: 0.4135
Training Epoch: 40 [7168/50048]	Loss: 0.3456
Training Epoch: 40 [7296/50048]	Loss: 0.2705
Training Epoch: 40 [7424/50048]	Loss: 0.3767
Training Epoch: 40 [7552/50048]	Loss: 0.3444
Training Epoch: 40 [7680/50048]	Loss: 0.4279
Training Epoch: 40 [7808/50048]	Loss: 0.4753
Training Epoch: 40 [7936/50048]	Loss: 0.3282
Training Epoch: 40 [8064/50048]	Loss: 0.4197
Training Epoch: 40 [8192/50048]	Loss: 0.4263
Training Epoch: 40 [8320/50048]	Loss: 0.3466
Training Epoch: 40 [8448/50048]	Loss: 0.4004
Training Epoch: 40 [8576/50048]	Loss: 0.3929
Training Epoch: 40 [8704/50048]	Loss: 0.4273
Training Epoch: 40 [8832/50048]	Loss: 0.5461
Training Epoch: 40 [8960/50048]	Loss: 0.4348
Training Epoch: 40 [9088/50048]	Loss: 0.2993
Training Epoch: 40 [9216/50048]	Loss: 0.3137
Training Epoch: 40 [9344/50048]	Loss: 0.5198
Training Epoch: 40 [9472/50048]	Loss: 0.3700
Training Epoch: 40 [9600/50048]	Loss: 0.5301
Training Epoch: 40 [9728/50048]	Loss: 0.4768
Training Epoch: 40 [9856/50048]	Loss: 0.4704
Training Epoch: 40 [9984/50048]	Loss: 0.5608
Training Epoch: 40 [10112/50048]	Loss: 0.4400
Training Epoch: 40 [10240/50048]	Loss: 0.3828
Training Epoch: 40 [10368/50048]	Loss: 0.3702
Training Epoch: 40 [10496/50048]	Loss: 0.4436
Training Epoch: 40 [10624/50048]	Loss: 0.3756
Training Epoch: 40 [10752/50048]	Loss: 0.3742
Training Epoch: 40 [10880/50048]	Loss: 0.3542
Training Epoch: 40 [11008/50048]	Loss: 0.4629
Training Epoch: 40 [11136/50048]	Loss: 0.3812
Training Epoch: 40 [11264/50048]	Loss: 0.3638
Training Epoch: 40 [11392/50048]	Loss: 0.3595
Training Epoch: 40 [11520/50048]	Loss: 0.3894
Training Epoch: 40 [11648/50048]	Loss: 0.3921
Training Epoch: 40 [11776/50048]	Loss: 0.3935
Training Epoch: 40 [11904/50048]	Loss: 0.2886
Training Epoch: 40 [12032/50048]	Loss: 0.5113
Training Epoch: 40 [12160/50048]	Loss: 0.3357
Training Epoch: 40 [12288/50048]	Loss: 0.3823
Training Epoch: 40 [12416/50048]	Loss: 0.4336
Training Epoch: 40 [12544/50048]	Loss: 0.5362
Training Epoch: 40 [12672/50048]	Loss: 0.3139
Training Epoch: 40 [12800/50048]	Loss: 0.3094
Training Epoch: 40 [12928/50048]	Loss: 0.3766
Training Epoch: 40 [13056/50048]	Loss: 0.5374
Training Epoch: 40 [13184/50048]	Loss: 0.2722
Training Epoch: 40 [13312/50048]	Loss: 0.4265
Training Epoch: 40 [13440/50048]	Loss: 0.4195
Training Epoch: 40 [13568/50048]	Loss: 0.4557
Training Epoch: 40 [13696/50048]	Loss: 0.3563
Training Epoch: 40 [13824/50048]	Loss: 0.3237
Training Epoch: 40 [13952/50048]	Loss: 0.2684
Training Epoch: 40 [14080/50048]	Loss: 0.4425
Training Epoch: 40 [14208/50048]	Loss: 0.4652
Training Epoch: 40 [14336/50048]	Loss: 0.3791
Training Epoch: 40 [14464/50048]	Loss: 0.3458
Training Epoch: 40 [14592/50048]	Loss: 0.5448
Training Epoch: 40 [14720/50048]	Loss: 0.3924
Training Epoch: 40 [14848/50048]	Loss: 0.5058
Training Epoch: 40 [14976/50048]	Loss: 0.3676
Training Epoch: 40 [15104/50048]	Loss: 0.4315
Training Epoch: 40 [15232/50048]	Loss: 0.3495
Training Epoch: 40 [15360/50048]	Loss: 0.3276
Training Epoch: 40 [15488/50048]	Loss: 0.3884
Training Epoch: 40 [15616/50048]	Loss: 0.3002
Training Epoch: 40 [15744/50048]	Loss: 0.4094
Training Epoch: 40 [15872/50048]	Loss: 0.3506
Training Epoch: 40 [16000/50048]	Loss: 0.4850
Training Epoch: 40 [16128/50048]	Loss: 0.3846
Training Epoch: 40 [16256/50048]	Loss: 0.2976
Training Epoch: 40 [16384/50048]	Loss: 0.2349
Training Epoch: 40 [16512/50048]	Loss: 0.3930
Training Epoch: 40 [16640/50048]	Loss: 0.3499
Training Epoch: 40 [16768/50048]	Loss: 0.3438
Training Epoch: 40 [16896/50048]	Loss: 0.4602
Training Epoch: 40 [17024/50048]	Loss: 0.4707
Training Epoch: 40 [17152/50048]	Loss: 0.3944
Training Epoch: 40 [17280/50048]	Loss: 0.4295
Training Epoch: 40 [17408/50048]	Loss: 0.4236
Training Epoch: 40 [17536/50048]	Loss: 0.5505
Training Epoch: 40 [17664/50048]	Loss: 0.5434
Training Epoch: 40 [17792/50048]	Loss: 0.3527
Training Epoch: 40 [17920/50048]	Loss: 0.3652
Training Epoch: 40 [18048/50048]	Loss: 0.4602
Training Epoch: 40 [18176/50048]	Loss: 0.2704
Training Epoch: 40 [18304/50048]	Loss: 0.4216
Training Epoch: 40 [18432/50048]	Loss: 0.3950
Training Epoch: 40 [18560/50048]	Loss: 0.3737
Training Epoch: 40 [18688/50048]	Loss: 0.2651
Training Epoch: 40 [18816/50048]	Loss: 0.2508
Training Epoch: 40 [18944/50048]	Loss: 0.3975
Training Epoch: 40 [19072/50048]	Loss: 0.3871
Training Epoch: 40 [19200/50048]	Loss: 0.3947
Training Epoch: 40 [19328/50048]	Loss: 0.3761
Training Epoch: 40 [19456/50048]	Loss: 0.4609
Training Epoch: 40 [19584/50048]	Loss: 0.4988
Training Epoch: 40 [19712/50048]	Loss: 0.4613
Training Epoch: 40 [19840/50048]	Loss: 0.3148
Training Epoch: 40 [19968/50048]	Loss: 0.4795
Training Epoch: 40 [20096/50048]	Loss: 0.3702
Training Epoch: 40 [20224/50048]	Loss: 0.5170
Training Epoch: 40 [20352/50048]	Loss: 0.4273
Training Epoch: 40 [20480/50048]	Loss: 0.4201
Training Epoch: 40 [20608/50048]	Loss: 0.4575
Training Epoch: 40 [20736/50048]	Loss: 0.3775
Training Epoch: 40 [20864/50048]	Loss: 0.3068
Training Epoch: 40 [20992/50048]	Loss: 0.4855
Training Epoch: 40 [21120/50048]	Loss: 0.4598
Training Epoch: 40 [21248/50048]	Loss: 0.4057
Training Epoch: 40 [21376/50048]	Loss: 0.5017
Training Epoch: 40 [21504/50048]	Loss: 0.4586
Training Epoch: 40 [21632/50048]	Loss: 0.3929
Training Epoch: 40 [21760/50048]	Loss: 0.3069
Training Epoch: 40 [21888/50048]	Loss: 0.3659
Training Epoch: 40 [22016/50048]	Loss: 0.3608
Training Epoch: 40 [22144/50048]	Loss: 0.3613
Training Epoch: 40 [22272/50048]	Loss: 0.3668
Training Epoch: 40 [22400/50048]	Loss: 0.4384
Training Epoch: 40 [22528/50048]	Loss: 0.4959
Training Epoch: 40 [22656/50048]	Loss: 0.5423
Training Epoch: 40 [22784/50048]	Loss: 0.3287
Training Epoch: 40 [22912/50048]	Loss: 0.3726
Training Epoch: 40 [23040/50048]	Loss: 0.3544
Training Epoch: 40 [23168/50048]	Loss: 0.3593
Training Epoch: 40 [23296/50048]	Loss: 0.3635
Training Epoch: 40 [23424/50048]	Loss: 0.4431
Training Epoch: 40 [23552/50048]	Loss: 0.4412
Training Epoch: 40 [23680/50048]	Loss: 0.4652
Training Epoch: 40 [23808/50048]	Loss: 0.3840
Training Epoch: 40 [23936/50048]	Loss: 0.6396
Training Epoch: 40 [24064/50048]	Loss: 0.3228
Training Epoch: 40 [24192/50048]	Loss: 0.4315
Training Epoch: 40 [24320/50048]	Loss: 0.4227
Training Epoch: 40 [24448/50048]	Loss: 0.3483
Training Epoch: 40 [24576/50048]	Loss: 0.4294
Training Epoch: 40 [24704/50048]	Loss: 0.3289
Training Epoch: 40 [24832/50048]	Loss: 0.3042
Training Epoch: 40 [24960/50048]	Loss: 0.4504
Training Epoch: 40 [25088/50048]	Loss: 0.4395
Training Epoch: 40 [25216/50048]	Loss: 0.5303
Training Epoch: 40 [25344/50048]	Loss: 0.5475
Training Epoch: 40 [25472/50048]	Loss: 0.4041
Training Epoch: 40 [25600/50048]	Loss: 0.5480
Training Epoch: 40 [25728/50048]	Loss: 0.4141
Training Epoch: 40 [25856/50048]	Loss: 0.3570
Training Epoch: 40 [25984/50048]	Loss: 0.4272
Training Epoch: 40 [26112/50048]	Loss: 0.3810
Training Epoch: 40 [26240/50048]	Loss: 0.4246
Training Epoch: 40 [26368/50048]	Loss: 0.4325
Training Epoch: 40 [26496/50048]	Loss: 0.5009
Training Epoch: 40 [26624/50048]	Loss: 0.3669
Training Epoch: 40 [26752/50048]	Loss: 0.4844
Training Epoch: 40 [26880/50048]	Loss: 0.5958
Training Epoch: 40 [27008/50048]	Loss: 0.4770
Training Epoch: 40 [27136/50048]	Loss: 0.3938
Training Epoch: 40 [27264/50048]	Loss: 0.3912
Training Epoch: 40 [27392/50048]	Loss: 0.4504
Training Epoch: 40 [27520/50048]	Loss: 0.3127
Training Epoch: 40 [27648/50048]	Loss: 0.4261
Training Epoch: 40 [27776/50048]	Loss: 0.4624
Training Epoch: 40 [27904/50048]	Loss: 0.2206
Training Epoch: 40 [28032/50048]	Loss: 0.4464
Training Epoch: 40 [28160/50048]	Loss: 0.6025
Training Epoch: 40 [28288/50048]	Loss: 0.3425
Training Epoch: 40 [28416/50048]	Loss: 0.5427
Training Epoch: 40 [28544/50048]	Loss: 0.6084
Training Epoch: 40 [28672/50048]	Loss: 0.4334
Training Epoch: 40 [28800/50048]	Loss: 0.3457
Training Epoch: 40 [28928/50048]	Loss: 0.4739
Training Epoch: 40 [29056/50048]	Loss: 0.3819
Training Epoch: 40 [29184/50048]	Loss: 0.4595
Training Epoch: 40 [29312/50048]	Loss: 0.3956
Training Epoch: 40 [29440/50048]	Loss: 0.5602
Training Epoch: 40 [29568/50048]	Loss: 0.4148
Training Epoch: 40 [29696/50048]	Loss: 0.3594
Training Epoch: 40 [29824/50048]	Loss: 0.4219
Training Epoch: 40 [29952/50048]	Loss: 0.3121
Training Epoch: 40 [30080/50048]	Loss: 0.3389
Training Epoch: 40 [30208/50048]	Loss: 0.4756
Training Epoch: 40 [30336/50048]	Loss: 0.3342
Training Epoch: 40 [30464/50048]	Loss: 0.4714
Training Epoch: 40 [30592/50048]	Loss: 0.4711
Training Epoch: 40 [30720/50048]	Loss: 0.5339
Training Epoch: 40 [30848/50048]	Loss: 0.3948
Training Epoch: 40 [30976/50048]	Loss: 0.3568
Training Epoch: 40 [31104/50048]	Loss: 0.3427
Training Epoch: 40 [31232/50048]	Loss: 0.3458
Training Epoch: 40 [31360/50048]	Loss: 0.6393
Training Epoch: 40 [31488/50048]	Loss: 0.3916
Training Epoch: 40 [31616/50048]	Loss: 0.5790
Training Epoch: 40 [31744/50048]	Loss: 0.4754
Training Epoch: 40 [31872/50048]	Loss: 0.5209
Training Epoch: 40 [32000/50048]	Loss: 0.3858
Training Epoch: 40 [32128/50048]	Loss: 0.6862
Training Epoch: 40 [32256/50048]	Loss: 0.4120
Training Epoch: 40 [32384/50048]	Loss: 0.5554
Training Epoch: 40 [32512/50048]	Loss: 0.3413
Training Epoch: 40 [32640/50048]	Loss: 0.4575
Training Epoch: 40 [32768/50048]	Loss: 0.3859
Training Epoch: 40 [32896/50048]	Loss: 0.4651
Training Epoch: 40 [33024/50048]	Loss: 0.5990
Training Epoch: 40 [33152/50048]	Loss: 0.5159
Training Epoch: 40 [33280/50048]	Loss: 0.5510
Training Epoch: 40 [33408/50048]	Loss: 0.4036
Training Epoch: 40 [33536/50048]	Loss: 0.5503
Training Epoch: 40 [33664/50048]	Loss: 0.5392
Training Epoch: 40 [33792/50048]	Loss: 0.4162
Training Epoch: 40 [33920/50048]	Loss: 0.4388
Training Epoch: 40 [34048/50048]	Loss: 0.4457
Training Epoch: 40 [34176/50048]	Loss: 0.4786
Training Epoch: 40 [34304/50048]	Loss: 0.5212
Training Epoch: 40 [34432/50048]	Loss: 0.4024
Training Epoch: 40 [34560/50048]	Loss: 0.4178
Training Epoch: 40 [34688/50048]	Loss: 0.4284
Training Epoch: 40 [34816/50048]	Loss: 0.3718
Training Epoch: 40 [34944/50048]	Loss: 0.3145
Training Epoch: 40 [35072/50048]	Loss: 0.5751
Training Epoch: 40 [35200/50048]	Loss: 0.3809
Training Epoch: 40 [35328/50048]	Loss: 0.5337
Training Epoch: 40 [35456/50048]	Loss: 0.4190
Training Epoch: 40 [35584/50048]	Loss: 0.4004
Training Epoch: 40 [35712/50048]	Loss: 0.5007
Training Epoch: 40 [35840/50048]	Loss: 0.4264
Training Epoch: 40 [35968/50048]	Loss: 0.4874
Training Epoch: 40 [36096/50048]	Loss: 0.4100
Training Epoch: 40 [36224/50048]	Loss: 0.2320
Training Epoch: 40 [36352/50048]	Loss: 0.6711
Training Epoch: 40 [36480/50048]	Loss: 0.4017
Training Epoch: 40 [36608/50048]	Loss: 0.3715
Training Epoch: 40 [36736/50048]	Loss: 0.3763
Training Epoch: 40 [36864/50048]	Loss: 0.4286
Training Epoch: 40 [36992/50048]	Loss: 0.3496
Training Epoch: 40 [37120/50048]	Loss: 0.4086
Training Epoch: 40 [37248/50048]	Loss: 0.3816
Training Epoch: 40 [37376/50048]	Loss: 0.3675
Training Epoch: 40 [37504/50048]	Loss: 0.4041
Training Epoch: 40 [37632/50048]	Loss: 0.4352
Training Epoch: 40 [37760/50048]	Loss: 0.4038
Training Epoch: 40 [37888/50048]	Loss: 0.4728
Training Epoch: 40 [38016/50048]	Loss: 0.4904
Training Epoch: 40 [38144/50048]	Loss: 0.3869
Training Epoch: 40 [38272/50048]	Loss: 0.5274
Training Epoch: 40 [38400/50048]	Loss: 0.4571
Training Epoch: 40 [38528/50048]	Loss: 0.4240
Training Epoch: 40 [38656/50048]	Loss: 0.5311
Training Epoch: 40 [38784/50048]	Loss: 0.4468
Training Epoch: 40 [38912/50048]	Loss: 0.3605
Training Epoch: 40 [39040/50048]	Loss: 0.3566
Training Epoch: 40 [39168/50048]	Loss: 0.3428
Training Epoch: 40 [39296/50048]	Loss: 0.2811
Training Epoch: 40 [39424/50048]	Loss: 0.5214
Training Epoch: 40 [39552/50048]	Loss: 0.5137
Training Epoch: 40 [39680/50048]	Loss: 0.6044
Training Epoch: 40 [39808/50048]	Loss: 0.5400
Training Epoch: 40 [39936/50048]	Loss: 0.4549
Training Epoch: 40 [40064/50048]	Loss: 0.3432
Training Epoch: 40 [40192/50048]	Loss: 0.5429
Training Epoch: 40 [40320/50048]	Loss: 0.5333
Training Epoch: 40 [40448/50048]	Loss: 0.4784
Training Epoch: 40 [40576/50048]	Loss: 0.3922
Training Epoch: 40 [40704/50048]	Loss: 0.5328
Training Epoch: 40 [40832/50048]	Loss: 0.2526
Training Epoch: 40 [40960/50048]	Loss: 0.5874
Training Epoch: 40 [41088/50048]	Loss: 0.4076
Training Epoch: 40 [41216/50048]	Loss: 0.3361
Training Epoch: 40 [41344/50048]	Loss: 0.4297
Training Epoch: 40 [41472/50048]	Loss: 0.4921
Training Epoch: 40 [41600/50048]	Loss: 0.3979
Training Epoch: 40 [41728/50048]	Loss: 0.3689
Training Epoch: 40 [41856/50048]	Loss: 0.4091
Training Epoch: 40 [41984/50048]	Loss: 0.5570
Training Epoch: 40 [42112/50048]	Loss: 0.4539
Training Epoch: 40 [42240/50048]	Loss: 0.4252
Training Epoch: 40 [42368/50048]	Loss: 0.6097
Training Epoch: 40 [42496/50048]	Loss: 0.5462
Training Epoch: 40 [42624/50048]	Loss: 0.7038
Training Epoch: 40 [42752/50048]	Loss: 0.4859
Training Epoch: 40 [42880/50048]	Loss: 0.3980
Training Epoch: 40 [43008/50048]	Loss: 0.5193
Training Epoch: 40 [43136/50048]	Loss: 0.4257
Training Epoch: 40 [43264/50048]	Loss: 0.4556
Training Epoch: 40 [43392/50048]	Loss: 0.5381
Training Epoch: 40 [43520/50048]	Loss: 0.4185
Training Epoch: 40 [43648/50048]	Loss: 0.5444
Training Epoch: 40 [43776/50048]	Loss: 0.4013
Training Epoch: 40 [43904/50048]	Loss: 0.5220
Training Epoch: 40 [44032/50048]	Loss: 0.4023
Training Epoch: 40 [44160/50048]	Loss: 0.4232
Training Epoch: 40 [44288/50048]	Loss: 0.4639
Training Epoch: 40 [44416/50048]	Loss: 0.4752
Training Epoch: 40 [44544/50048]	Loss: 0.4626
Training Epoch: 40 [44672/50048]	Loss: 0.3376
Training Epoch: 40 [44800/50048]	Loss: 0.5523
Training Epoch: 40 [44928/50048]	Loss: 0.4485
Training Epoch: 40 [45056/50048]	Loss: 0.4269
Training Epoch: 40 [45184/50048]	Loss: 0.5956
Training Epoch: 40 [45312/50048]	Loss: 0.5120
Training Epoch: 40 [45440/50048]	Loss: 0.5234
Training Epoch: 40 [45568/50048]	Loss: 0.4733
Training Epoch: 40 [45696/50048]	Loss: 0.3500
2022-12-06 07:09:28,655 [ZeusDataLoader(train)] train epoch 41 done: time=86.50 energy=10496.78
2022-12-06 07:09:28,656 [ZeusDataLoader(eval)] Epoch 41 begin.
Training Epoch: 40 [45824/50048]	Loss: 0.5003
Training Epoch: 40 [45952/50048]	Loss: 0.4281
Training Epoch: 40 [46080/50048]	Loss: 0.5755
Training Epoch: 40 [46208/50048]	Loss: 0.2996
Training Epoch: 40 [46336/50048]	Loss: 0.3635
Training Epoch: 40 [46464/50048]	Loss: 0.4231
Training Epoch: 40 [46592/50048]	Loss: 0.5982
Training Epoch: 40 [46720/50048]	Loss: 0.4021
Training Epoch: 40 [46848/50048]	Loss: 0.3182
Training Epoch: 40 [46976/50048]	Loss: 0.3948
Training Epoch: 40 [47104/50048]	Loss: 0.4996
Training Epoch: 40 [47232/50048]	Loss: 0.4490
Training Epoch: 40 [47360/50048]	Loss: 0.5200
Training Epoch: 40 [47488/50048]	Loss: 0.4803
Training Epoch: 40 [47616/50048]	Loss: 0.4555
Training Epoch: 40 [47744/50048]	Loss: 0.4831
Training Epoch: 40 [47872/50048]	Loss: 0.4412
Training Epoch: 40 [48000/50048]	Loss: 0.4759
Training Epoch: 40 [48128/50048]	Loss: 0.4125
Training Epoch: 40 [48256/50048]	Loss: 0.4565
Training Epoch: 40 [48384/50048]	Loss: 0.4999
Training Epoch: 40 [48512/50048]	Loss: 0.4010
Training Epoch: 40 [48640/50048]	Loss: 0.3631
Training Epoch: 40 [48768/50048]	Loss: 0.5298
Training Epoch: 40 [48896/50048]	Loss: 0.4230
Training Epoch: 40 [49024/50048]	Loss: 0.6454
Training Epoch: 40 [49152/50048]	Loss: 0.6223
Training Epoch: 40 [49280/50048]	Loss: 0.5027
Training Epoch: 40 [49408/50048]	Loss: 0.3978
Training Epoch: 40 [49536/50048]	Loss: 0.4409
Training Epoch: 40 [49664/50048]	Loss: 0.5835
Training Epoch: 40 [49792/50048]	Loss: 0.3302
Training Epoch: 40 [49920/50048]	Loss: 0.3540
Training Epoch: 40 [50048/50048]	Loss: 0.7258
2022-12-06 12:09:32.365 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:09:32,422 [ZeusDataLoader(eval)] eval epoch 41 done: time=3.76 energy=453.96
2022-12-06 07:09:32,423 [ZeusDataLoader(train)] Up to epoch 41: time=3697.91, energy=448896.86, cost=548015.27
2022-12-06 07:09:32,423 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:09:32,423 [ZeusDataLoader(train)] Expected next epoch: time=3787.71, energy=459694.87, cost=561271.65
2022-12-06 07:09:32,424 [ZeusDataLoader(train)] Epoch 42 begin.
Validation Epoch: 40, Average loss: 0.0135, Accuracy: 0.6222
2022-12-06 07:09:32,624 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:09:32,624 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:09:32.626 [ZeusMonitor] Monitor started.
2022-12-06 12:09:32.626 [ZeusMonitor] Running indefinitely. 2022-12-06 12:09:32.626 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:09:32.626 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e42+gpu0.power.log
Training Epoch: 41 [128/50048]	Loss: 0.3129
Training Epoch: 41 [256/50048]	Loss: 0.1820
Training Epoch: 41 [384/50048]	Loss: 0.3547
Training Epoch: 41 [512/50048]	Loss: 0.3711
Training Epoch: 41 [640/50048]	Loss: 0.3628
Training Epoch: 41 [768/50048]	Loss: 0.2901
Training Epoch: 41 [896/50048]	Loss: 0.3033
Training Epoch: 41 [1024/50048]	Loss: 0.2655
Training Epoch: 41 [1152/50048]	Loss: 0.3950
Training Epoch: 41 [1280/50048]	Loss: 0.3308
Training Epoch: 41 [1408/50048]	Loss: 0.3329
Training Epoch: 41 [1536/50048]	Loss: 0.2604
Training Epoch: 41 [1664/50048]	Loss: 0.4066
Training Epoch: 41 [1792/50048]	Loss: 0.3651
Training Epoch: 41 [1920/50048]	Loss: 0.4139
Training Epoch: 41 [2048/50048]	Loss: 0.2513
Training Epoch: 41 [2176/50048]	Loss: 0.5165
Training Epoch: 41 [2304/50048]	Loss: 0.4412
Training Epoch: 41 [2432/50048]	Loss: 0.4089
Training Epoch: 41 [2560/50048]	Loss: 0.3627
Training Epoch: 41 [2688/50048]	Loss: 0.2763
Training Epoch: 41 [2816/50048]	Loss: 0.3519
Training Epoch: 41 [2944/50048]	Loss: 0.3001
Training Epoch: 41 [3072/50048]	Loss: 0.2419
Training Epoch: 41 [3200/50048]	Loss: 0.3321
Training Epoch: 41 [3328/50048]	Loss: 0.3034
Training Epoch: 41 [3456/50048]	Loss: 0.3413
Training Epoch: 41 [3584/50048]	Loss: 0.3103
Training Epoch: 41 [3712/50048]	Loss: 0.3893
Training Epoch: 41 [3840/50048]	Loss: 0.2348
Training Epoch: 41 [3968/50048]	Loss: 0.3408
Training Epoch: 41 [4096/50048]	Loss: 0.4140
Training Epoch: 41 [4224/50048]	Loss: 0.2739
Training Epoch: 41 [4352/50048]	Loss: 0.3618
Training Epoch: 41 [4480/50048]	Loss: 0.5447
Training Epoch: 41 [4608/50048]	Loss: 0.3189
Training Epoch: 41 [4736/50048]	Loss: 0.4120
Training Epoch: 41 [4864/50048]	Loss: 0.4081
Training Epoch: 41 [4992/50048]	Loss: 0.3347
Training Epoch: 41 [5120/50048]	Loss: 0.3425
Training Epoch: 41 [5248/50048]	Loss: 0.3694
Training Epoch: 41 [5376/50048]	Loss: 0.4603
Training Epoch: 41 [5504/50048]	Loss: 0.4151
Training Epoch: 41 [5632/50048]	Loss: 0.3733
Training Epoch: 41 [5760/50048]	Loss: 0.4740
Training Epoch: 41 [5888/50048]	Loss: 0.3989
Training Epoch: 41 [6016/50048]	Loss: 0.3948
Training Epoch: 41 [6144/50048]	Loss: 0.3278
Training Epoch: 41 [6272/50048]	Loss: 0.3818
Training Epoch: 41 [6400/50048]	Loss: 0.3051
Training Epoch: 41 [6528/50048]	Loss: 0.3698
Training Epoch: 41 [6656/50048]	Loss: 0.2957
Training Epoch: 41 [6784/50048]	Loss: 0.3739
Training Epoch: 41 [6912/50048]	Loss: 0.3949
Training Epoch: 41 [7040/50048]	Loss: 0.2849
Training Epoch: 41 [7168/50048]	Loss: 0.4255
Training Epoch: 41 [7296/50048]	Loss: 0.3844
Training Epoch: 41 [7424/50048]	Loss: 0.2943
Training Epoch: 41 [7552/50048]	Loss: 0.3760
Training Epoch: 41 [7680/50048]	Loss: 0.3652
Training Epoch: 41 [7808/50048]	Loss: 0.3336
Training Epoch: 41 [7936/50048]	Loss: 0.3682
Training Epoch: 41 [8064/50048]	Loss: 0.3349
Training Epoch: 41 [8192/50048]	Loss: 0.4582
Training Epoch: 41 [8320/50048]	Loss: 0.3871
Training Epoch: 41 [8448/50048]	Loss: 0.3082
Training Epoch: 41 [8576/50048]	Loss: 0.3990
Training Epoch: 41 [8704/50048]	Loss: 0.4507
Training Epoch: 41 [8832/50048]	Loss: 0.4114
Training Epoch: 41 [8960/50048]	Loss: 0.3321
Training Epoch: 41 [9088/50048]	Loss: 0.3651
Training Epoch: 41 [9216/50048]	Loss: 0.3838
Training Epoch: 41 [9344/50048]	Loss: 0.3836
Training Epoch: 41 [9472/50048]	Loss: 0.3992
Training Epoch: 41 [9600/50048]	Loss: 0.4238
Training Epoch: 41 [9728/50048]	Loss: 0.5027
Training Epoch: 41 [9856/50048]	Loss: 0.4331
Training Epoch: 41 [9984/50048]	Loss: 0.4795
Training Epoch: 41 [10112/50048]	Loss: 0.3171
Training Epoch: 41 [10240/50048]	Loss: 0.3120
Training Epoch: 41 [10368/50048]	Loss: 0.2489
Training Epoch: 41 [10496/50048]	Loss: 0.4512
Training Epoch: 41 [10624/50048]	Loss: 0.4365
Training Epoch: 41 [10752/50048]	Loss: 0.3602
Training Epoch: 41 [10880/50048]	Loss: 0.3861
Training Epoch: 41 [11008/50048]	Loss: 0.3774
Training Epoch: 41 [11136/50048]	Loss: 0.4662
Training Epoch: 41 [11264/50048]	Loss: 0.4878
Training Epoch: 41 [11392/50048]	Loss: 0.3741
Training Epoch: 41 [11520/50048]	Loss: 0.3889
Training Epoch: 41 [11648/50048]	Loss: 0.4572
Training Epoch: 41 [11776/50048]	Loss: 0.4257
Training Epoch: 41 [11904/50048]	Loss: 0.4294
Training Epoch: 41 [12032/50048]	Loss: 0.3621
Training Epoch: 41 [12160/50048]	Loss: 0.3918
Training Epoch: 41 [12288/50048]	Loss: 0.3281
Training Epoch: 41 [12416/50048]	Loss: 0.4134
Training Epoch: 41 [12544/50048]	Loss: 0.2321
Training Epoch: 41 [12672/50048]	Loss: 0.4763
Training Epoch: 41 [12800/50048]	Loss: 0.5093
Training Epoch: 41 [12928/50048]	Loss: 0.3408
Training Epoch: 41 [13056/50048]	Loss: 0.3189
Training Epoch: 41 [13184/50048]	Loss: 0.4589
Training Epoch: 41 [13312/50048]	Loss: 0.3280
Training Epoch: 41 [13440/50048]	Loss: 0.3456
Training Epoch: 41 [13568/50048]	Loss: 0.5893
Training Epoch: 41 [13696/50048]	Loss: 0.3102
Training Epoch: 41 [13824/50048]	Loss: 0.4372
Training Epoch: 41 [13952/50048]	Loss: 0.4631
Training Epoch: 41 [14080/50048]	Loss: 0.4344
Training Epoch: 41 [14208/50048]	Loss: 0.3688
Training Epoch: 41 [14336/50048]	Loss: 0.3699
Training Epoch: 41 [14464/50048]	Loss: 0.3444
Training Epoch: 41 [14592/50048]	Loss: 0.3779
Training Epoch: 41 [14720/50048]	Loss: 0.3505
Training Epoch: 41 [14848/50048]	Loss: 0.3897
Training Epoch: 41 [14976/50048]	Loss: 0.4412
Training Epoch: 41 [15104/50048]	Loss: 0.3467
Training Epoch: 41 [15232/50048]	Loss: 0.5473
Training Epoch: 41 [15360/50048]	Loss: 0.3351
Training Epoch: 41 [15488/50048]	Loss: 0.3789
Training Epoch: 41 [15616/50048]	Loss: 0.2964
Training Epoch: 41 [15744/50048]	Loss: 0.3449
Training Epoch: 41 [15872/50048]	Loss: 0.4623
Training Epoch: 41 [16000/50048]	Loss: 0.3978
Training Epoch: 41 [16128/50048]	Loss: 0.3946
Training Epoch: 41 [16256/50048]	Loss: 0.3840
Training Epoch: 41 [16384/50048]	Loss: 0.3836
Training Epoch: 41 [16512/50048]	Loss: 0.3854
Training Epoch: 41 [16640/50048]	Loss: 0.3345
Training Epoch: 41 [16768/50048]	Loss: 0.3738
Training Epoch: 41 [16896/50048]	Loss: 0.4153
Training Epoch: 41 [17024/50048]	Loss: 0.5181
Training Epoch: 41 [17152/50048]	Loss: 0.4561
Training Epoch: 41 [17280/50048]	Loss: 0.3226
Training Epoch: 41 [17408/50048]	Loss: 0.3772
Training Epoch: 41 [17536/50048]	Loss: 0.4038
Training Epoch: 41 [17664/50048]	Loss: 0.3923
Training Epoch: 41 [17792/50048]	Loss: 0.4705
Training Epoch: 41 [17920/50048]	Loss: 0.4978
Training Epoch: 41 [18048/50048]	Loss: 0.3838
Training Epoch: 41 [18176/50048]	Loss: 0.3387
Training Epoch: 41 [18304/50048]	Loss: 0.3628
Training Epoch: 41 [18432/50048]	Loss: 0.3685
Training Epoch: 41 [18560/50048]	Loss: 0.3292
Training Epoch: 41 [18688/50048]	Loss: 0.3330
Training Epoch: 41 [18816/50048]	Loss: 0.3106
Training Epoch: 41 [18944/50048]	Loss: 0.5594
Training Epoch: 41 [19072/50048]	Loss: 0.3977
Training Epoch: 41 [19200/50048]	Loss: 0.3939
Training Epoch: 41 [19328/50048]	Loss: 0.3421
Training Epoch: 41 [19456/50048]	Loss: 0.3495
Training Epoch: 41 [19584/50048]	Loss: 0.2705
Training Epoch: 41 [19712/50048]	Loss: 0.3183
Training Epoch: 41 [19840/50048]	Loss: 0.4185
Training Epoch: 41 [19968/50048]	Loss: 0.4253
Training Epoch: 41 [20096/50048]	Loss: 0.2978
Training Epoch: 41 [20224/50048]	Loss: 0.5003
Training Epoch: 41 [20352/50048]	Loss: 0.4949
Training Epoch: 41 [20480/50048]	Loss: 0.3885
Training Epoch: 41 [20608/50048]	Loss: 0.3980
Training Epoch: 41 [20736/50048]	Loss: 0.3456
Training Epoch: 41 [20864/50048]	Loss: 0.3631
Training Epoch: 41 [20992/50048]	Loss: 0.4984
Training Epoch: 41 [21120/50048]	Loss: 0.3415
Training Epoch: 41 [21248/50048]	Loss: 0.3446
Training Epoch: 41 [21376/50048]	Loss: 0.2767
Training Epoch: 41 [21504/50048]	Loss: 0.4194
Training Epoch: 41 [21632/50048]	Loss: 0.4890
Training Epoch: 41 [21760/50048]	Loss: 0.3180
Training Epoch: 41 [21888/50048]	Loss: 0.3896
Training Epoch: 41 [22016/50048]	Loss: 0.3278
Training Epoch: 41 [22144/50048]	Loss: 0.4349
Training Epoch: 41 [22272/50048]	Loss: 0.4072
Training Epoch: 41 [22400/50048]	Loss: 0.3532
Training Epoch: 41 [22528/50048]	Loss: 0.3298
Training Epoch: 41 [22656/50048]	Loss: 0.2944
Training Epoch: 41 [22784/50048]	Loss: 0.3122
Training Epoch: 41 [22912/50048]	Loss: 0.6865
Training Epoch: 41 [23040/50048]	Loss: 0.4564
Training Epoch: 41 [23168/50048]	Loss: 0.2858
Training Epoch: 41 [23296/50048]	Loss: 0.3614
Training Epoch: 41 [23424/50048]	Loss: 0.3487
Training Epoch: 41 [23552/50048]	Loss: 0.2827
Training Epoch: 41 [23680/50048]	Loss: 0.5270
Training Epoch: 41 [23808/50048]	Loss: 0.3874
Training Epoch: 41 [23936/50048]	Loss: 0.5148
Training Epoch: 41 [24064/50048]	Loss: 0.4745
Training Epoch: 41 [24192/50048]	Loss: 0.3617
Training Epoch: 41 [24320/50048]	Loss: 0.4061
Training Epoch: 41 [24448/50048]	Loss: 0.4684
Training Epoch: 41 [24576/50048]	Loss: 0.4348
Training Epoch: 41 [24704/50048]	Loss: 0.3806
Training Epoch: 41 [24832/50048]	Loss: 0.4183
Training Epoch: 41 [24960/50048]	Loss: 0.4356
Training Epoch: 41 [25088/50048]	Loss: 0.3718
Training Epoch: 41 [25216/50048]	Loss: 0.3824
Training Epoch: 41 [25344/50048]	Loss: 0.4428
Training Epoch: 41 [25472/50048]	Loss: 0.3668
Training Epoch: 41 [25600/50048]	Loss: 0.3230
Training Epoch: 41 [25728/50048]	Loss: 0.3816
Training Epoch: 41 [25856/50048]	Loss: 0.3618
Training Epoch: 41 [25984/50048]	Loss: 0.3475
Training Epoch: 41 [26112/50048]	Loss: 0.4872
Training Epoch: 41 [26240/50048]	Loss: 0.4096
Training Epoch: 41 [26368/50048]	Loss: 0.4533
Training Epoch: 41 [26496/50048]	Loss: 0.3935
Training Epoch: 41 [26624/50048]	Loss: 0.5230
Training Epoch: 41 [26752/50048]	Loss: 0.4333
Training Epoch: 41 [26880/50048]	Loss: 0.3236
Training Epoch: 41 [27008/50048]	Loss: 0.4423
Training Epoch: 41 [27136/50048]	Loss: 0.5558
Training Epoch: 41 [27264/50048]	Loss: 0.3342
Training Epoch: 41 [27392/50048]	Loss: 0.3884
Training Epoch: 41 [27520/50048]	Loss: 0.4151
Training Epoch: 41 [27648/50048]	Loss: 0.2680
Training Epoch: 41 [27776/50048]	Loss: 0.4665
Training Epoch: 41 [27904/50048]	Loss: 0.3558
Training Epoch: 41 [28032/50048]	Loss: 0.4269
Training Epoch: 41 [28160/50048]	Loss: 0.4596
Training Epoch: 41 [28288/50048]	Loss: 0.4181
Training Epoch: 41 [28416/50048]	Loss: 0.5082
Training Epoch: 41 [28544/50048]	Loss: 0.4719
Training Epoch: 41 [28672/50048]	Loss: 0.4694
Training Epoch: 41 [28800/50048]	Loss: 0.4407
Training Epoch: 41 [28928/50048]	Loss: 0.4302
Training Epoch: 41 [29056/50048]	Loss: 0.4766
Training Epoch: 41 [29184/50048]	Loss: 0.4648
Training Epoch: 41 [29312/50048]	Loss: 0.2934
Training Epoch: 41 [29440/50048]	Loss: 0.4205
Training Epoch: 41 [29568/50048]	Loss: 0.3609
Training Epoch: 41 [29696/50048]	Loss: 0.4720
Training Epoch: 41 [29824/50048]	Loss: 0.3502
Training Epoch: 41 [29952/50048]	Loss: 0.3497
Training Epoch: 41 [30080/50048]	Loss: 0.3922
Training Epoch: 41 [30208/50048]	Loss: 0.4320
Training Epoch: 41 [30336/50048]	Loss: 0.4662
Training Epoch: 41 [30464/50048]	Loss: 0.6726
Training Epoch: 41 [30592/50048]	Loss: 0.3768
Training Epoch: 41 [30720/50048]	Loss: 0.5403
Training Epoch: 41 [30848/50048]	Loss: 0.4156
Training Epoch: 41 [30976/50048]	Loss: 0.4281
Training Epoch: 41 [31104/50048]	Loss: 0.3608
Training Epoch: 41 [31232/50048]	Loss: 0.2758
Training Epoch: 41 [31360/50048]	Loss: 0.5546
Training Epoch: 41 [31488/50048]	Loss: 0.3819
Training Epoch: 41 [31616/50048]	Loss: 0.5023
Training Epoch: 41 [31744/50048]	Loss: 0.5359
Training Epoch: 41 [31872/50048]	Loss: 0.3650
Training Epoch: 41 [32000/50048]	Loss: 0.3514
Training Epoch: 41 [32128/50048]	Loss: 0.3423
Training Epoch: 41 [32256/50048]	Loss: 0.3352
Training Epoch: 41 [32384/50048]	Loss: 0.3044
Training Epoch: 41 [32512/50048]	Loss: 0.4339
Training Epoch: 41 [32640/50048]	Loss: 0.3012
Training Epoch: 41 [32768/50048]	Loss: 0.4494
Training Epoch: 41 [32896/50048]	Loss: 0.5319
Training Epoch: 41 [33024/50048]	Loss: 0.5147
Training Epoch: 41 [33152/50048]	Loss: 0.3074
Training Epoch: 41 [33280/50048]	Loss: 0.2990
Training Epoch: 41 [33408/50048]	Loss: 0.4119
Training Epoch: 41 [33536/50048]	Loss: 0.4693
Training Epoch: 41 [33664/50048]	Loss: 0.3799
Training Epoch: 41 [33792/50048]	Loss: 0.4673
Training Epoch: 41 [33920/50048]	Loss: 0.3563
Training Epoch: 41 [34048/50048]	Loss: 0.3518
Training Epoch: 41 [34176/50048]	Loss: 0.3706
Training Epoch: 41 [34304/50048]	Loss: 0.4145
Training Epoch: 41 [34432/50048]	Loss: 0.5214
Training Epoch: 41 [34560/50048]	Loss: 0.3947
Training Epoch: 41 [34688/50048]	Loss: 0.3053
Training Epoch: 41 [34816/50048]	Loss: 0.3337
Training Epoch: 41 [34944/50048]	Loss: 0.3871
Training Epoch: 41 [35072/50048]	Loss: 0.4111
Training Epoch: 41 [35200/50048]	Loss: 0.3610
Training Epoch: 41 [35328/50048]	Loss: 0.3869
Training Epoch: 41 [35456/50048]	Loss: 0.3490
Training Epoch: 41 [35584/50048]	Loss: 0.4793
Training Epoch: 41 [35712/50048]	Loss: 0.3930
Training Epoch: 41 [35840/50048]	Loss: 0.4932
Training Epoch: 41 [35968/50048]	Loss: 0.4395
Training Epoch: 41 [36096/50048]	Loss: 0.3523
Training Epoch: 41 [36224/50048]	Loss: 0.3465
Training Epoch: 41 [36352/50048]	Loss: 0.3258
Training Epoch: 41 [36480/50048]	Loss: 0.3945
Training Epoch: 41 [36608/50048]	Loss: 0.3890
Training Epoch: 41 [36736/50048]	Loss: 0.4738
Training Epoch: 41 [36864/50048]	Loss: 0.3590
Training Epoch: 41 [36992/50048]	Loss: 0.4917
Training Epoch: 41 [37120/50048]	Loss: 0.5862
Training Epoch: 41 [37248/50048]	Loss: 0.4164
Training Epoch: 41 [37376/50048]	Loss: 0.3379
Training Epoch: 41 [37504/50048]	Loss: 0.4301
Training Epoch: 41 [37632/50048]	Loss: 0.4543
Training Epoch: 41 [37760/50048]	Loss: 0.4021
Training Epoch: 41 [37888/50048]	Loss: 0.3589
Training Epoch: 41 [38016/50048]	Loss: 0.4174
Training Epoch: 41 [38144/50048]	Loss: 0.4347
Training Epoch: 41 [38272/50048]	Loss: 0.5102
Training Epoch: 41 [38400/50048]	Loss: 0.4579
Training Epoch: 41 [38528/50048]	Loss: 0.4535
Training Epoch: 41 [38656/50048]	Loss: 0.4475
Training Epoch: 41 [38784/50048]	Loss: 0.5795
Training Epoch: 41 [38912/50048]	Loss: 0.5320
Training Epoch: 41 [39040/50048]	Loss: 0.2944
Training Epoch: 41 [39168/50048]	Loss: 0.5622
Training Epoch: 41 [39296/50048]	Loss: 0.4354
Training Epoch: 41 [39424/50048]	Loss: 0.4461
Training Epoch: 41 [39552/50048]	Loss: 0.4612
Training Epoch: 41 [39680/50048]	Loss: 0.3990
Training Epoch: 41 [39808/50048]	Loss: 0.5499
Training Epoch: 41 [39936/50048]	Loss: 0.5452
Training Epoch: 41 [40064/50048]	Loss: 0.4094
Training Epoch: 41 [40192/50048]	Loss: 0.4133
Training Epoch: 41 [40320/50048]	Loss: 0.4941
Training Epoch: 41 [40448/50048]	Loss: 0.4392
Training Epoch: 41 [40576/50048]	Loss: 0.5643
Training Epoch: 41 [40704/50048]	Loss: 0.3588
Training Epoch: 41 [40832/50048]	Loss: 0.6979
Training Epoch: 41 [40960/50048]	Loss: 0.3794
Training Epoch: 41 [41088/50048]	Loss: 0.4721
Training Epoch: 41 [41216/50048]	Loss: 0.5730
Training Epoch: 41 [41344/50048]	Loss: 0.4921
Training Epoch: 41 [41472/50048]	Loss: 0.3767
Training Epoch: 41 [41600/50048]	Loss: 0.3533
Training Epoch: 41 [41728/50048]	Loss: 0.3833
Training Epoch: 41 [41856/50048]	Loss: 0.5014
Training Epoch: 41 [41984/50048]	Loss: 0.3971
Training Epoch: 41 [42112/50048]	Loss: 0.4144
Training Epoch: 41 [42240/50048]	Loss: 0.4623
Training Epoch: 41 [42368/50048]	Loss: 0.6281
Training Epoch: 41 [42496/50048]	Loss: 0.5083
Training Epoch: 41 [42624/50048]	Loss: 0.2924
Training Epoch: 41 [42752/50048]	Loss: 0.4355
Training Epoch: 41 [42880/50048]	Loss: 0.5082
Training Epoch: 41 [43008/50048]	Loss: 0.3223
Training Epoch: 41 [43136/50048]	Loss: 0.4044
Training Epoch: 41 [43264/50048]	Loss: 0.4193
Training Epoch: 41 [43392/50048]	Loss: 0.3984
Training Epoch: 41 [43520/50048]	Loss: 0.4049
Training Epoch: 41 [43648/50048]	Loss: 0.5552
Training Epoch: 41 [43776/50048]	Loss: 0.3216
Training Epoch: 41 [43904/50048]	Loss: 0.4121
Training Epoch: 41 [44032/50048]	Loss: 0.4782
Training Epoch: 41 [44160/50048]	Loss: 0.4080
Training Epoch: 41 [44288/50048]	Loss: 0.6365
Training Epoch: 41 [44416/50048]	Loss: 0.4056
Training Epoch: 41 [44544/50048]	Loss: 0.4116
Training Epoch: 41 [44672/50048]	Loss: 0.4303
Training Epoch: 41 [44800/50048]	Loss: 0.4944
Training Epoch: 41 [44928/50048]	Loss: 0.3621
Training Epoch: 41 [45056/50048]	Loss: 0.5580
Training Epoch: 41 [45184/50048]	Loss: 0.4387
Training Epoch: 41 [45312/50048]	Loss: 0.4551
Training Epoch: 41 [45440/50048]	Loss: 0.5184
Training Epoch: 41 [45568/50048]	Loss: 0.3750
Training Epoch: 41 [45696/50048]	Loss: 0.3830
2022-12-06 07:10:58,763 [ZeusDataLoader(train)] train epoch 42 done: time=86.33 energy=10496.56
2022-12-06 07:10:58,764 [ZeusDataLoader(eval)] Epoch 42 begin.
Training Epoch: 41 [45824/50048]	Loss: 0.4196
Training Epoch: 41 [45952/50048]	Loss: 0.3599
Training Epoch: 41 [46080/50048]	Loss: 0.3274
Training Epoch: 41 [46208/50048]	Loss: 0.4175
Training Epoch: 41 [46336/50048]	Loss: 0.3837
Training Epoch: 41 [46464/50048]	Loss: 0.3921
Training Epoch: 41 [46592/50048]	Loss: 0.4975
Training Epoch: 41 [46720/50048]	Loss: 0.6219
Training Epoch: 41 [46848/50048]	Loss: 0.3517
Training Epoch: 41 [46976/50048]	Loss: 0.4872
Training Epoch: 41 [47104/50048]	Loss: 0.5565
Training Epoch: 41 [47232/50048]	Loss: 0.3876
Training Epoch: 41 [47360/50048]	Loss: 0.4235
Training Epoch: 41 [47488/50048]	Loss: 0.3767
Training Epoch: 41 [47616/50048]	Loss: 0.5830
Training Epoch: 41 [47744/50048]	Loss: 0.4212
Training Epoch: 41 [47872/50048]	Loss: 0.3952
Training Epoch: 41 [48000/50048]	Loss: 0.5349
Training Epoch: 41 [48128/50048]	Loss: 0.4453
Training Epoch: 41 [48256/50048]	Loss: 0.4932
Training Epoch: 41 [48384/50048]	Loss: 0.3529
Training Epoch: 41 [48512/50048]	Loss: 0.6410
Training Epoch: 41 [48640/50048]	Loss: 0.4667
Training Epoch: 41 [48768/50048]	Loss: 0.4777
Training Epoch: 41 [48896/50048]	Loss: 0.5287
Training Epoch: 41 [49024/50048]	Loss: 0.5402
Training Epoch: 41 [49152/50048]	Loss: 0.5449
Training Epoch: 41 [49280/50048]	Loss: 0.5675
Training Epoch: 41 [49408/50048]	Loss: 0.4146
Training Epoch: 41 [49536/50048]	Loss: 0.3739
Training Epoch: 41 [49664/50048]	Loss: 0.5422
Training Epoch: 41 [49792/50048]	Loss: 0.6003
Training Epoch: 41 [49920/50048]	Loss: 0.5563
Training Epoch: 41 [50048/50048]	Loss: 0.3850
2022-12-06 12:11:02.424 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:11:02,437 [ZeusDataLoader(eval)] eval epoch 42 done: time=3.66 energy=440.78
2022-12-06 07:11:02,437 [ZeusDataLoader(train)] Up to epoch 42: time=3787.90, energy=459834.20, cost=561358.28
2022-12-06 07:11:02,437 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:11:02,437 [ZeusDataLoader(train)] Expected next epoch: time=3877.70, energy=470632.22, cost=574614.67
2022-12-06 07:11:02,438 [ZeusDataLoader(train)] Epoch 43 begin.
Validation Epoch: 41, Average loss: 0.0141, Accuracy: 0.6255
2022-12-06 07:11:02,628 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:11:02,629 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:11:02.632 [ZeusMonitor] Monitor started.
2022-12-06 12:11:02.632 [ZeusMonitor] Running indefinitely. 2022-12-06 12:11:02.632 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:11:02.632 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e43+gpu0.power.log
Training Epoch: 42 [128/50048]	Loss: 0.4465
Training Epoch: 42 [256/50048]	Loss: 0.4170
Training Epoch: 42 [384/50048]	Loss: 0.3595
Training Epoch: 42 [512/50048]	Loss: 0.2700
Training Epoch: 42 [640/50048]	Loss: 0.3997
Training Epoch: 42 [768/50048]	Loss: 0.3502
Training Epoch: 42 [896/50048]	Loss: 0.2732
Training Epoch: 42 [1024/50048]	Loss: 0.2625
Training Epoch: 42 [1152/50048]	Loss: 0.3345
Training Epoch: 42 [1280/50048]	Loss: 0.3035
Training Epoch: 42 [1408/50048]	Loss: 0.2234
Training Epoch: 42 [1536/50048]	Loss: 0.4386
Training Epoch: 42 [1664/50048]	Loss: 0.2550
Training Epoch: 42 [1792/50048]	Loss: 0.2949
Training Epoch: 42 [1920/50048]	Loss: 0.3839
Training Epoch: 42 [2048/50048]	Loss: 0.3476
Training Epoch: 42 [2176/50048]	Loss: 0.3500
Training Epoch: 42 [2304/50048]	Loss: 0.2853
Training Epoch: 42 [2432/50048]	Loss: 0.3913
Training Epoch: 42 [2560/50048]	Loss: 0.2683
Training Epoch: 42 [2688/50048]	Loss: 0.3202
Training Epoch: 42 [2816/50048]	Loss: 0.2776
Training Epoch: 42 [2944/50048]	Loss: 0.2998
Training Epoch: 42 [3072/50048]	Loss: 0.3011
Training Epoch: 42 [3200/50048]	Loss: 0.3759
Training Epoch: 42 [3328/50048]	Loss: 0.3123
Training Epoch: 42 [3456/50048]	Loss: 0.5576
Training Epoch: 42 [3584/50048]	Loss: 0.4709
Training Epoch: 42 [3712/50048]	Loss: 0.4320
Training Epoch: 42 [3840/50048]	Loss: 0.3517
Training Epoch: 42 [3968/50048]	Loss: 0.3713
Training Epoch: 42 [4096/50048]	Loss: 0.2901
Training Epoch: 42 [4224/50048]	Loss: 0.4657
Training Epoch: 42 [4352/50048]	Loss: 0.3609
Training Epoch: 42 [4480/50048]	Loss: 0.3553
Training Epoch: 42 [4608/50048]	Loss: 0.3712
Training Epoch: 42 [4736/50048]	Loss: 0.2509
Training Epoch: 42 [4864/50048]	Loss: 0.2883
Training Epoch: 42 [4992/50048]	Loss: 0.4066
Training Epoch: 42 [5120/50048]	Loss: 0.3550
Training Epoch: 42 [5248/50048]	Loss: 0.4046
Training Epoch: 42 [5376/50048]	Loss: 0.3525
Training Epoch: 42 [5504/50048]	Loss: 0.4515
Training Epoch: 42 [5632/50048]	Loss: 0.4946
Training Epoch: 42 [5760/50048]	Loss: 0.4383
Training Epoch: 42 [5888/50048]	Loss: 0.3542
Training Epoch: 42 [6016/50048]	Loss: 0.3702
Training Epoch: 42 [6144/50048]	Loss: 0.2923
Training Epoch: 42 [6272/50048]	Loss: 0.2296
Training Epoch: 42 [6400/50048]	Loss: 0.4023
Training Epoch: 42 [6528/50048]	Loss: 0.2681
Training Epoch: 42 [6656/50048]	Loss: 0.3777
Training Epoch: 42 [6784/50048]	Loss: 0.3800
Training Epoch: 42 [6912/50048]	Loss: 0.2701
Training Epoch: 42 [7040/50048]	Loss: 0.3609
Training Epoch: 42 [7168/50048]	Loss: 0.2589
Training Epoch: 42 [7296/50048]	Loss: 0.3124
Training Epoch: 42 [7424/50048]	Loss: 0.3236
Training Epoch: 42 [7552/50048]	Loss: 0.2855
Training Epoch: 42 [7680/50048]	Loss: 0.3872
Training Epoch: 42 [7808/50048]	Loss: 0.3008
Training Epoch: 42 [7936/50048]	Loss: 0.3038
Training Epoch: 42 [8064/50048]	Loss: 0.2998
Training Epoch: 42 [8192/50048]	Loss: 0.4081
Training Epoch: 42 [8320/50048]	Loss: 0.3745
Training Epoch: 42 [8448/50048]	Loss: 0.3527
Training Epoch: 42 [8576/50048]	Loss: 0.4601
Training Epoch: 42 [8704/50048]	Loss: 0.4949
Training Epoch: 42 [8832/50048]	Loss: 0.4841
Training Epoch: 42 [8960/50048]	Loss: 0.3366
Training Epoch: 42 [9088/50048]	Loss: 0.2478
Training Epoch: 42 [9216/50048]	Loss: 0.3799
Training Epoch: 42 [9344/50048]	Loss: 0.5055
Training Epoch: 42 [9472/50048]	Loss: 0.4945
Training Epoch: 42 [9600/50048]	Loss: 0.3791
Training Epoch: 42 [9728/50048]	Loss: 0.4503
Training Epoch: 42 [9856/50048]	Loss: 0.3721
Training Epoch: 42 [9984/50048]	Loss: 0.4063
Training Epoch: 42 [10112/50048]	Loss: 0.3757
Training Epoch: 42 [10240/50048]	Loss: 0.4170
Training Epoch: 42 [10368/50048]	Loss: 0.3042
Training Epoch: 42 [10496/50048]	Loss: 0.3445
Training Epoch: 42 [10624/50048]	Loss: 0.4504
Training Epoch: 42 [10752/50048]	Loss: 0.3067
Training Epoch: 42 [10880/50048]	Loss: 0.3146
Training Epoch: 42 [11008/50048]	Loss: 0.3874
Training Epoch: 42 [11136/50048]	Loss: 0.3329
Training Epoch: 42 [11264/50048]	Loss: 0.3020
Training Epoch: 42 [11392/50048]	Loss: 0.4240
Training Epoch: 42 [11520/50048]	Loss: 0.2873
Training Epoch: 42 [11648/50048]	Loss: 0.4901
Training Epoch: 42 [11776/50048]	Loss: 0.4102
Training Epoch: 42 [11904/50048]	Loss: 0.4276
Training Epoch: 42 [12032/50048]	Loss: 0.4905
Training Epoch: 42 [12160/50048]	Loss: 0.3579
Training Epoch: 42 [12288/50048]	Loss: 0.3219
Training Epoch: 42 [12416/50048]	Loss: 0.4545
Training Epoch: 42 [12544/50048]	Loss: 0.4786
Training Epoch: 42 [12672/50048]	Loss: 0.3540
Training Epoch: 42 [12800/50048]	Loss: 0.3533
Training Epoch: 42 [12928/50048]	Loss: 0.2793
Training Epoch: 42 [13056/50048]	Loss: 0.3142
Training Epoch: 42 [13184/50048]	Loss: 0.3952
Training Epoch: 42 [13312/50048]	Loss: 0.3153
Training Epoch: 42 [13440/50048]	Loss: 0.2906
Training Epoch: 42 [13568/50048]	Loss: 0.4295
Training Epoch: 42 [13696/50048]	Loss: 0.3289
Training Epoch: 42 [13824/50048]	Loss: 0.5387
Training Epoch: 42 [13952/50048]	Loss: 0.3263
Training Epoch: 42 [14080/50048]	Loss: 0.4187
Training Epoch: 42 [14208/50048]	Loss: 0.4382
Training Epoch: 42 [14336/50048]	Loss: 0.2612
Training Epoch: 42 [14464/50048]	Loss: 0.5026
Training Epoch: 42 [14592/50048]	Loss: 0.4335
Training Epoch: 42 [14720/50048]	Loss: 0.3391
Training Epoch: 42 [14848/50048]	Loss: 0.3045
Training Epoch: 42 [14976/50048]	Loss: 0.3911
Training Epoch: 42 [15104/50048]	Loss: 0.4367
Training Epoch: 42 [15232/50048]	Loss: 0.3524
Training Epoch: 42 [15360/50048]	Loss: 0.4139
Training Epoch: 42 [15488/50048]	Loss: 0.2638
Training Epoch: 42 [15616/50048]	Loss: 0.5133
Training Epoch: 42 [15744/50048]	Loss: 0.2124
Training Epoch: 42 [15872/50048]	Loss: 0.3873
Training Epoch: 42 [16000/50048]	Loss: 0.3657
Training Epoch: 42 [16128/50048]	Loss: 0.2667
Training Epoch: 42 [16256/50048]	Loss: 0.3603
Training Epoch: 42 [16384/50048]	Loss: 0.3534
Training Epoch: 42 [16512/50048]	Loss: 0.4318
Training Epoch: 42 [16640/50048]	Loss: 0.3693
Training Epoch: 42 [16768/50048]	Loss: 0.3572
Training Epoch: 42 [16896/50048]	Loss: 0.4257
Training Epoch: 42 [17024/50048]	Loss: 0.3118
Training Epoch: 42 [17152/50048]	Loss: 0.4186
Training Epoch: 42 [17280/50048]	Loss: 0.3524
Training Epoch: 42 [17408/50048]	Loss: 0.3257
Training Epoch: 42 [17536/50048]	Loss: 0.4922
Training Epoch: 42 [17664/50048]	Loss: 0.3890
Training Epoch: 42 [17792/50048]	Loss: 0.3785
Training Epoch: 42 [17920/50048]	Loss: 0.3955
Training Epoch: 42 [18048/50048]	Loss: 0.3224
Training Epoch: 42 [18176/50048]	Loss: 0.4018
Training Epoch: 42 [18304/50048]	Loss: 0.3290
Training Epoch: 42 [18432/50048]	Loss: 0.3375
Training Epoch: 42 [18560/50048]	Loss: 0.5758
Training Epoch: 42 [18688/50048]	Loss: 0.4340
Training Epoch: 42 [18816/50048]	Loss: 0.3266
Training Epoch: 42 [18944/50048]	Loss: 0.3210
Training Epoch: 42 [19072/50048]	Loss: 0.4150
Training Epoch: 42 [19200/50048]	Loss: 0.4178
Training Epoch: 42 [19328/50048]	Loss: 0.3801
Training Epoch: 42 [19456/50048]	Loss: 0.2875
Training Epoch: 42 [19584/50048]	Loss: 0.3269
Training Epoch: 42 [19712/50048]	Loss: 0.3845
Training Epoch: 42 [19840/50048]	Loss: 0.3490
Training Epoch: 42 [19968/50048]	Loss: 0.3960
Training Epoch: 42 [20096/50048]	Loss: 0.3165
Training Epoch: 42 [20224/50048]	Loss: 0.3654
Training Epoch: 42 [20352/50048]	Loss: 0.3332
Training Epoch: 42 [20480/50048]	Loss: 0.3792
Training Epoch: 42 [20608/50048]	Loss: 0.4427
Training Epoch: 42 [20736/50048]	Loss: 0.2960
Training Epoch: 42 [20864/50048]	Loss: 0.3688
Training Epoch: 42 [20992/50048]	Loss: 0.3351
Training Epoch: 42 [21120/50048]	Loss: 0.4844
Training Epoch: 42 [21248/50048]	Loss: 0.4219
Training Epoch: 42 [21376/50048]	Loss: 0.4057
Training Epoch: 42 [21504/50048]	Loss: 0.4852
Training Epoch: 42 [21632/50048]	Loss: 0.2953
Training Epoch: 42 [21760/50048]	Loss: 0.4168
Training Epoch: 42 [21888/50048]	Loss: 0.3930
Training Epoch: 42 [22016/50048]	Loss: 0.5202
Training Epoch: 42 [22144/50048]	Loss: 0.3407
Training Epoch: 42 [22272/50048]	Loss: 0.3726
Training Epoch: 42 [22400/50048]	Loss: 0.3692
Training Epoch: 42 [22528/50048]	Loss: 0.4086
Training Epoch: 42 [22656/50048]	Loss: 0.4086
Training Epoch: 42 [22784/50048]	Loss: 0.4462
Training Epoch: 42 [22912/50048]	Loss: 0.4138
Training Epoch: 42 [23040/50048]	Loss: 0.3967
Training Epoch: 42 [23168/50048]	Loss: 0.3768
Training Epoch: 42 [23296/50048]	Loss: 0.4004
Training Epoch: 42 [23424/50048]	Loss: 0.4095
Training Epoch: 42 [23552/50048]	Loss: 0.3107
Training Epoch: 42 [23680/50048]	Loss: 0.4820
Training Epoch: 42 [23808/50048]	Loss: 0.2506
Training Epoch: 42 [23936/50048]	Loss: 0.4981
Training Epoch: 42 [24064/50048]	Loss: 0.4058
Training Epoch: 42 [24192/50048]	Loss: 0.4782
Training Epoch: 42 [24320/50048]	Loss: 0.4734
Training Epoch: 42 [24448/50048]	Loss: 0.2917
Training Epoch: 42 [24576/50048]	Loss: 0.4742
Training Epoch: 42 [24704/50048]	Loss: 0.6706
Training Epoch: 42 [24832/50048]	Loss: 0.3818
Training Epoch: 42 [24960/50048]	Loss: 0.5078
Training Epoch: 42 [25088/50048]	Loss: 0.4283
Training Epoch: 42 [25216/50048]	Loss: 0.4357
Training Epoch: 42 [25344/50048]	Loss: 0.3491
Training Epoch: 42 [25472/50048]	Loss: 0.3098
Training Epoch: 42 [25600/50048]	Loss: 0.5189
Training Epoch: 42 [25728/50048]	Loss: 0.3149
Training Epoch: 42 [25856/50048]	Loss: 0.3578
Training Epoch: 42 [25984/50048]	Loss: 0.4650
Training Epoch: 42 [26112/50048]	Loss: 0.4667
Training Epoch: 42 [26240/50048]	Loss: 0.3573
Training Epoch: 42 [26368/50048]	Loss: 0.4057
Training Epoch: 42 [26496/50048]	Loss: 0.3378
Training Epoch: 42 [26624/50048]	Loss: 0.3568
Training Epoch: 42 [26752/50048]	Loss: 0.4569
Training Epoch: 42 [26880/50048]	Loss: 0.2972
Training Epoch: 42 [27008/50048]	Loss: 0.4229
Training Epoch: 42 [27136/50048]	Loss: 0.4242
Training Epoch: 42 [27264/50048]	Loss: 0.3901
Training Epoch: 42 [27392/50048]	Loss: 0.3667
Training Epoch: 42 [27520/50048]	Loss: 0.3413
Training Epoch: 42 [27648/50048]	Loss: 0.4670
Training Epoch: 42 [27776/50048]	Loss: 0.3829
Training Epoch: 42 [27904/50048]	Loss: 0.4491
Training Epoch: 42 [28032/50048]	Loss: 0.3254
Training Epoch: 42 [28160/50048]	Loss: 0.2705
Training Epoch: 42 [28288/50048]	Loss: 0.3981
Training Epoch: 42 [28416/50048]	Loss: 0.4038
Training Epoch: 42 [28544/50048]	Loss: 0.5094
Training Epoch: 42 [28672/50048]	Loss: 0.3663
Training Epoch: 42 [28800/50048]	Loss: 0.4375
Training Epoch: 42 [28928/50048]	Loss: 0.4356
Training Epoch: 42 [29056/50048]	Loss: 0.3912
Training Epoch: 42 [29184/50048]	Loss: 0.4266
Training Epoch: 42 [29312/50048]	Loss: 0.3684
Training Epoch: 42 [29440/50048]	Loss: 0.3906
Training Epoch: 42 [29568/50048]	Loss: 0.5838
Training Epoch: 42 [29696/50048]	Loss: 0.5514
Training Epoch: 42 [29824/50048]	Loss: 0.4655
Training Epoch: 42 [29952/50048]	Loss: 0.3670
Training Epoch: 42 [30080/50048]	Loss: 0.3547
Training Epoch: 42 [30208/50048]	Loss: 0.3842
Training Epoch: 42 [30336/50048]	Loss: 0.5830
Training Epoch: 42 [30464/50048]	Loss: 0.3859
Training Epoch: 42 [30592/50048]	Loss: 0.3511
Training Epoch: 42 [30720/50048]	Loss: 0.4355
Training Epoch: 42 [30848/50048]	Loss: 0.3938
Training Epoch: 42 [30976/50048]	Loss: 0.5791
Training Epoch: 42 [31104/50048]	Loss: 0.2714
Training Epoch: 42 [31232/50048]	Loss: 0.4531
Training Epoch: 42 [31360/50048]	Loss: 0.4563
Training Epoch: 42 [31488/50048]	Loss: 0.4174
Training Epoch: 42 [31616/50048]	Loss: 0.3285
Training Epoch: 42 [31744/50048]	Loss: 0.4523
Training Epoch: 42 [31872/50048]	Loss: 0.3908
Training Epoch: 42 [32000/50048]	Loss: 0.3550
Training Epoch: 42 [32128/50048]	Loss: 0.4642
Training Epoch: 42 [32256/50048]	Loss: 0.4583
Training Epoch: 42 [32384/50048]	Loss: 0.3110
Training Epoch: 42 [32512/50048]	Loss: 0.3915
Training Epoch: 42 [32640/50048]	Loss: 0.5266
Training Epoch: 42 [32768/50048]	Loss: 0.5774
Training Epoch: 42 [32896/50048]	Loss: 0.3639
Training Epoch: 42 [33024/50048]	Loss: 0.3832
Training Epoch: 42 [33152/50048]	Loss: 0.3751
Training Epoch: 42 [33280/50048]	Loss: 0.4127
Training Epoch: 42 [33408/50048]	Loss: 0.3020
Training Epoch: 42 [33536/50048]	Loss: 0.4661
Training Epoch: 42 [33664/50048]	Loss: 0.2549
Training Epoch: 42 [33792/50048]	Loss: 0.2690
Training Epoch: 42 [33920/50048]	Loss: 0.3554
Training Epoch: 42 [34048/50048]	Loss: 0.3678
Training Epoch: 42 [34176/50048]	Loss: 0.2660
Training Epoch: 42 [34304/50048]	Loss: 0.3977
Training Epoch: 42 [34432/50048]	Loss: 0.4945
Training Epoch: 42 [34560/50048]	Loss: 0.3610
Training Epoch: 42 [34688/50048]	Loss: 0.5491
Training Epoch: 42 [34816/50048]	Loss: 0.6218
Training Epoch: 42 [34944/50048]	Loss: 0.4532
Training Epoch: 42 [35072/50048]	Loss: 0.3510
Training Epoch: 42 [35200/50048]	Loss: 0.4801
Training Epoch: 42 [35328/50048]	Loss: 0.3758
Training Epoch: 42 [35456/50048]	Loss: 0.4413
Training Epoch: 42 [35584/50048]	Loss: 0.4067
Training Epoch: 42 [35712/50048]	Loss: 0.3530
Training Epoch: 42 [35840/50048]	Loss: 0.5805
Training Epoch: 42 [35968/50048]	Loss: 0.3915
Training Epoch: 42 [36096/50048]	Loss: 0.3648
Training Epoch: 42 [36224/50048]	Loss: 0.4373
Training Epoch: 42 [36352/50048]	Loss: 0.5635
Training Epoch: 42 [36480/50048]	Loss: 0.4502
Training Epoch: 42 [36608/50048]	Loss: 0.4794
Training Epoch: 42 [36736/50048]	Loss: 0.4926
Training Epoch: 42 [36864/50048]	Loss: 0.3718
Training Epoch: 42 [36992/50048]	Loss: 0.4603
Training Epoch: 42 [37120/50048]	Loss: 0.4273
Training Epoch: 42 [37248/50048]	Loss: 0.4722
Training Epoch: 42 [37376/50048]	Loss: 0.4225
Training Epoch: 42 [37504/50048]	Loss: 0.4565
Training Epoch: 42 [37632/50048]	Loss: 0.4300
Training Epoch: 42 [37760/50048]	Loss: 0.4404
Training Epoch: 42 [37888/50048]	Loss: 0.5568
Training Epoch: 42 [38016/50048]	Loss: 0.3746
Training Epoch: 42 [38144/50048]	Loss: 0.5813
Training Epoch: 42 [38272/50048]	Loss: 0.4627
Training Epoch: 42 [38400/50048]	Loss: 0.3691
Training Epoch: 42 [38528/50048]	Loss: 0.5500
Training Epoch: 42 [38656/50048]	Loss: 0.3766
Training Epoch: 42 [38784/50048]	Loss: 0.4519
Training Epoch: 42 [38912/50048]	Loss: 0.5536
Training Epoch: 42 [39040/50048]	Loss: 0.2984
Training Epoch: 42 [39168/50048]	Loss: 0.4970
Training Epoch: 42 [39296/50048]	Loss: 0.3480
Training Epoch: 42 [39424/50048]	Loss: 0.4776
Training Epoch: 42 [39552/50048]	Loss: 0.5079
Training Epoch: 42 [39680/50048]	Loss: 0.4278
Training Epoch: 42 [39808/50048]	Loss: 0.3676
Training Epoch: 42 [39936/50048]	Loss: 0.3800
Training Epoch: 42 [40064/50048]	Loss: 0.4000
Training Epoch: 42 [40192/50048]	Loss: 0.3614
Training Epoch: 42 [40320/50048]	Loss: 0.4440
Training Epoch: 42 [40448/50048]	Loss: 0.4357
Training Epoch: 42 [40576/50048]	Loss: 0.4881
Training Epoch: 42 [40704/50048]	Loss: 0.4658
Training Epoch: 42 [40832/50048]	Loss: 0.4115
Training Epoch: 42 [40960/50048]	Loss: 0.3864
Training Epoch: 42 [41088/50048]	Loss: 0.4994
Training Epoch: 42 [41216/50048]	Loss: 0.3879
Training Epoch: 42 [41344/50048]	Loss: 0.4872
Training Epoch: 42 [41472/50048]	Loss: 0.5340
Training Epoch: 42 [41600/50048]	Loss: 0.5416
Training Epoch: 42 [41728/50048]	Loss: 0.3359
Training Epoch: 42 [41856/50048]	Loss: 0.2969
Training Epoch: 42 [41984/50048]	Loss: 0.4138
Training Epoch: 42 [42112/50048]	Loss: 0.4076
Training Epoch: 42 [42240/50048]	Loss: 0.4077
Training Epoch: 42 [42368/50048]	Loss: 0.5783
Training Epoch: 42 [42496/50048]	Loss: 0.4649
Training Epoch: 42 [42624/50048]	Loss: 0.4386
Training Epoch: 42 [42752/50048]	Loss: 0.3320
Training Epoch: 42 [42880/50048]	Loss: 0.3671
Training Epoch: 42 [43008/50048]	Loss: 0.4488
Training Epoch: 42 [43136/50048]	Loss: 0.4011
Training Epoch: 42 [43264/50048]	Loss: 0.3849
Training Epoch: 42 [43392/50048]	Loss: 0.4403
Training Epoch: 42 [43520/50048]	Loss: 0.4435
Training Epoch: 42 [43648/50048]	Loss: 0.4395
Training Epoch: 42 [43776/50048]	Loss: 0.4992
Training Epoch: 42 [43904/50048]	Loss: 0.4662
Training Epoch: 42 [44032/50048]	Loss: 0.4135
Training Epoch: 42 [44160/50048]	Loss: 0.4150
Training Epoch: 42 [44288/50048]	Loss: 0.5384
Training Epoch: 42 [44416/50048]	Loss: 0.5210
Training Epoch: 42 [44544/50048]	Loss: 0.3850
Training Epoch: 42 [44672/50048]	Loss: 0.3687
Training Epoch: 42 [44800/50048]	Loss: 0.4952
Training Epoch: 42 [44928/50048]	Loss: 0.5506
Training Epoch: 42 [45056/50048]	Loss: 0.3042
Training Epoch: 42 [45184/50048]	Loss: 0.3116
Training Epoch: 42 [45312/50048]	Loss: 0.4684
Training Epoch: 42 [45440/50048]	Loss: 0.4050
Training Epoch: 42 [45568/50048]	Loss: 0.2588
Training Epoch: 42 [45696/50048]	Loss: 0.4536
2022-12-06 07:12:28,954 [ZeusDataLoader(train)] train epoch 43 done: time=86.51 energy=10492.27
2022-12-06 07:12:28,956 [ZeusDataLoader(eval)] Epoch 43 begin.
Training Epoch: 42 [45824/50048]	Loss: 0.4033
Training Epoch: 42 [45952/50048]	Loss: 0.5531
Training Epoch: 42 [46080/50048]	Loss: 0.5027
Training Epoch: 42 [46208/50048]	Loss: 0.5745
Training Epoch: 42 [46336/50048]	Loss: 0.3831
Training Epoch: 42 [46464/50048]	Loss: 0.3989
Training Epoch: 42 [46592/50048]	Loss: 0.4852
Training Epoch: 42 [46720/50048]	Loss: 0.4477
Training Epoch: 42 [46848/50048]	Loss: 0.3702
Training Epoch: 42 [46976/50048]	Loss: 0.3328
Training Epoch: 42 [47104/50048]	Loss: 0.4883
Training Epoch: 42 [47232/50048]	Loss: 0.4954
Training Epoch: 42 [47360/50048]	Loss: 0.4015
Training Epoch: 42 [47488/50048]	Loss: 0.3444
Training Epoch: 42 [47616/50048]	Loss: 0.6087
Training Epoch: 42 [47744/50048]	Loss: 0.4413
Training Epoch: 42 [47872/50048]	Loss: 0.4847
Training Epoch: 42 [48000/50048]	Loss: 0.4515
Training Epoch: 42 [48128/50048]	Loss: 0.3375
Training Epoch: 42 [48256/50048]	Loss: 0.4390
Training Epoch: 42 [48384/50048]	Loss: 0.5635
Training Epoch: 42 [48512/50048]	Loss: 0.5960
Training Epoch: 42 [48640/50048]	Loss: 0.4712
Training Epoch: 42 [48768/50048]	Loss: 0.4161
Training Epoch: 42 [48896/50048]	Loss: 0.4563
Training Epoch: 42 [49024/50048]	Loss: 0.4841
Training Epoch: 42 [49152/50048]	Loss: 0.5056
Training Epoch: 42 [49280/50048]	Loss: 0.4168
Training Epoch: 42 [49408/50048]	Loss: 0.4289
Training Epoch: 42 [49536/50048]	Loss: 0.3950
Training Epoch: 42 [49664/50048]	Loss: 0.5696
Training Epoch: 42 [49792/50048]	Loss: 0.4997
Training Epoch: 42 [49920/50048]	Loss: 0.3023
Training Epoch: 42 [50048/50048]	Loss: 0.3653
2022-12-06 12:12:32.676 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:12:32,687 [ZeusDataLoader(eval)] eval epoch 43 done: time=3.72 energy=455.57
2022-12-06 07:12:32,687 [ZeusDataLoader(train)] Up to epoch 43: time=3878.13, energy=470782.03, cost=574727.16
2022-12-06 07:12:32,687 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:12:32,687 [ZeusDataLoader(train)] Expected next epoch: time=3967.93, energy=481580.05, cost=587983.54
2022-12-06 07:12:32,688 [ZeusDataLoader(train)] Epoch 44 begin.
Validation Epoch: 42, Average loss: 0.0142, Accuracy: 0.6209
2022-12-06 07:12:32,885 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:12:32,886 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:12:32.888 [ZeusMonitor] Monitor started.
2022-12-06 12:12:32.888 [ZeusMonitor] Running indefinitely. 2022-12-06 12:12:32.888 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:12:32.888 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e44+gpu0.power.log
Training Epoch: 43 [128/50048]	Loss: 0.3217
Training Epoch: 43 [256/50048]	Loss: 0.3748
Training Epoch: 43 [384/50048]	Loss: 0.3773
Training Epoch: 43 [512/50048]	Loss: 0.3465
Training Epoch: 43 [640/50048]	Loss: 0.3827
Training Epoch: 43 [768/50048]	Loss: 0.4884
Training Epoch: 43 [896/50048]	Loss: 0.2924
Training Epoch: 43 [1024/50048]	Loss: 0.2726
Training Epoch: 43 [1152/50048]	Loss: 0.2531
Training Epoch: 43 [1280/50048]	Loss: 0.4352
Training Epoch: 43 [1408/50048]	Loss: 0.3932
Training Epoch: 43 [1536/50048]	Loss: 0.3883
Training Epoch: 43 [1664/50048]	Loss: 0.2519
Training Epoch: 43 [1792/50048]	Loss: 0.3229
Training Epoch: 43 [1920/50048]	Loss: 0.3562
Training Epoch: 43 [2048/50048]	Loss: 0.2944
Training Epoch: 43 [2176/50048]	Loss: 0.3081
Training Epoch: 43 [2304/50048]	Loss: 0.4637
Training Epoch: 43 [2432/50048]	Loss: 0.3358
Training Epoch: 43 [2560/50048]	Loss: 0.3567
Training Epoch: 43 [2688/50048]	Loss: 0.2893
Training Epoch: 43 [2816/50048]	Loss: 0.2793
Training Epoch: 43 [2944/50048]	Loss: 0.3299
Training Epoch: 43 [3072/50048]	Loss: 0.3543
Training Epoch: 43 [3200/50048]	Loss: 0.3822
Training Epoch: 43 [3328/50048]	Loss: 0.3882
Training Epoch: 43 [3456/50048]	Loss: 0.4568
Training Epoch: 43 [3584/50048]	Loss: 0.4447
Training Epoch: 43 [3712/50048]	Loss: 0.4212
Training Epoch: 43 [3840/50048]	Loss: 0.2968
Training Epoch: 43 [3968/50048]	Loss: 0.3747
Training Epoch: 43 [4096/50048]	Loss: 0.2070
Training Epoch: 43 [4224/50048]	Loss: 0.3045
Training Epoch: 43 [4352/50048]	Loss: 0.2081
Training Epoch: 43 [4480/50048]	Loss: 0.2729
Training Epoch: 43 [4608/50048]	Loss: 0.2442
Training Epoch: 43 [4736/50048]	Loss: 0.4572
Training Epoch: 43 [4864/50048]	Loss: 0.3812
Training Epoch: 43 [4992/50048]	Loss: 0.2005
Training Epoch: 43 [5120/50048]	Loss: 0.3342
Training Epoch: 43 [5248/50048]	Loss: 0.3540
Training Epoch: 43 [5376/50048]	Loss: 0.3135
Training Epoch: 43 [5504/50048]	Loss: 0.2935
Training Epoch: 43 [5632/50048]	Loss: 0.3315
Training Epoch: 43 [5760/50048]	Loss: 0.3552
Training Epoch: 43 [5888/50048]	Loss: 0.3634
Training Epoch: 43 [6016/50048]	Loss: 0.3545
Training Epoch: 43 [6144/50048]	Loss: 0.3736
Training Epoch: 43 [6272/50048]	Loss: 0.4246
Training Epoch: 43 [6400/50048]	Loss: 0.4119
Training Epoch: 43 [6528/50048]	Loss: 0.4637
Training Epoch: 43 [6656/50048]	Loss: 0.3341
Training Epoch: 43 [6784/50048]	Loss: 0.4426
Training Epoch: 43 [6912/50048]	Loss: 0.3436
Training Epoch: 43 [7040/50048]	Loss: 0.2702
Training Epoch: 43 [7168/50048]	Loss: 0.3020
Training Epoch: 43 [7296/50048]	Loss: 0.3550
Training Epoch: 43 [7424/50048]	Loss: 0.2379
Training Epoch: 43 [7552/50048]	Loss: 0.3281
Training Epoch: 43 [7680/50048]	Loss: 0.3693
Training Epoch: 43 [7808/50048]	Loss: 0.5379
Training Epoch: 43 [7936/50048]	Loss: 0.3174
Training Epoch: 43 [8064/50048]	Loss: 0.2445
Training Epoch: 43 [8192/50048]	Loss: 0.3309
Training Epoch: 43 [8320/50048]	Loss: 0.3988
Training Epoch: 43 [8448/50048]	Loss: 0.2846
Training Epoch: 43 [8576/50048]	Loss: 0.2222
Training Epoch: 43 [8704/50048]	Loss: 0.4492
Training Epoch: 43 [8832/50048]	Loss: 0.4601
Training Epoch: 43 [8960/50048]	Loss: 0.2962
Training Epoch: 43 [9088/50048]	Loss: 0.4671
Training Epoch: 43 [9216/50048]	Loss: 0.4861
Training Epoch: 43 [9344/50048]	Loss: 0.3668
Training Epoch: 43 [9472/50048]	Loss: 0.2500
Training Epoch: 43 [9600/50048]	Loss: 0.4132
Training Epoch: 43 [9728/50048]	Loss: 0.3449
Training Epoch: 43 [9856/50048]	Loss: 0.3507
Training Epoch: 43 [9984/50048]	Loss: 0.4301
Training Epoch: 43 [10112/50048]	Loss: 0.3572
Training Epoch: 43 [10240/50048]	Loss: 0.3738
Training Epoch: 43 [10368/50048]	Loss: 0.3111
Training Epoch: 43 [10496/50048]	Loss: 0.3214
Training Epoch: 43 [10624/50048]	Loss: 0.3507
Training Epoch: 43 [10752/50048]	Loss: 0.2491
Training Epoch: 43 [10880/50048]	Loss: 0.3257
Training Epoch: 43 [11008/50048]	Loss: 0.2464
Training Epoch: 43 [11136/50048]	Loss: 0.2718
Training Epoch: 43 [11264/50048]	Loss: 0.1411
Training Epoch: 43 [11392/50048]	Loss: 0.3803
Training Epoch: 43 [11520/50048]	Loss: 0.3342
Training Epoch: 43 [11648/50048]	Loss: 0.3049
Training Epoch: 43 [11776/50048]	Loss: 0.4651
Training Epoch: 43 [11904/50048]	Loss: 0.3209
Training Epoch: 43 [12032/50048]	Loss: 0.2206
Training Epoch: 43 [12160/50048]	Loss: 0.3350
Training Epoch: 43 [12288/50048]	Loss: 0.3509
Training Epoch: 43 [12416/50048]	Loss: 0.3843
Training Epoch: 43 [12544/50048]	Loss: 0.4471
Training Epoch: 43 [12672/50048]	Loss: 0.3083
Training Epoch: 43 [12800/50048]	Loss: 0.3954
Training Epoch: 43 [12928/50048]	Loss: 0.3862
Training Epoch: 43 [13056/50048]	Loss: 0.2731
Training Epoch: 43 [13184/50048]	Loss: 0.3321
Training Epoch: 43 [13312/50048]	Loss: 0.3825
Training Epoch: 43 [13440/50048]	Loss: 0.3959
Training Epoch: 43 [13568/50048]	Loss: 0.3455
Training Epoch: 43 [13696/50048]	Loss: 0.5131
Training Epoch: 43 [13824/50048]	Loss: 0.3209
Training Epoch: 43 [13952/50048]	Loss: 0.3639
Training Epoch: 43 [14080/50048]	Loss: 0.3445
Training Epoch: 43 [14208/50048]	Loss: 0.3105
Training Epoch: 43 [14336/50048]	Loss: 0.5330
Training Epoch: 43 [14464/50048]	Loss: 0.3235
Training Epoch: 43 [14592/50048]	Loss: 0.2956
Training Epoch: 43 [14720/50048]	Loss: 0.4116
Training Epoch: 43 [14848/50048]	Loss: 0.3962
Training Epoch: 43 [14976/50048]	Loss: 0.3886
Training Epoch: 43 [15104/50048]	Loss: 0.4603
Training Epoch: 43 [15232/50048]	Loss: 0.3602
Training Epoch: 43 [15360/50048]	Loss: 0.4344
Training Epoch: 43 [15488/50048]	Loss: 0.2765
Training Epoch: 43 [15616/50048]	Loss: 0.4019
Training Epoch: 43 [15744/50048]	Loss: 0.3916
Training Epoch: 43 [15872/50048]	Loss: 0.3508
Training Epoch: 43 [16000/50048]	Loss: 0.4321
Training Epoch: 43 [16128/50048]	Loss: 0.4056
Training Epoch: 43 [16256/50048]	Loss: 0.3813
Training Epoch: 43 [16384/50048]	Loss: 0.3046
Training Epoch: 43 [16512/50048]	Loss: 0.2283
Training Epoch: 43 [16640/50048]	Loss: 0.3540
Training Epoch: 43 [16768/50048]	Loss: 0.3227
Training Epoch: 43 [16896/50048]	Loss: 0.3179
Training Epoch: 43 [17024/50048]	Loss: 0.3627
Training Epoch: 43 [17152/50048]	Loss: 0.3844
Training Epoch: 43 [17280/50048]	Loss: 0.2595
Training Epoch: 43 [17408/50048]	Loss: 0.5488
Training Epoch: 43 [17536/50048]	Loss: 0.3129
Training Epoch: 43 [17664/50048]	Loss: 0.4191
Training Epoch: 43 [17792/50048]	Loss: 0.3123
Training Epoch: 43 [17920/50048]	Loss: 0.3641
Training Epoch: 43 [18048/50048]	Loss: 0.3710
Training Epoch: 43 [18176/50048]	Loss: 0.2516
Training Epoch: 43 [18304/50048]	Loss: 0.4402
Training Epoch: 43 [18432/50048]	Loss: 0.2789
Training Epoch: 43 [18560/50048]	Loss: 0.4372
Training Epoch: 43 [18688/50048]	Loss: 0.5397
Training Epoch: 43 [18816/50048]	Loss: 0.4940
Training Epoch: 43 [18944/50048]	Loss: 0.3879
Training Epoch: 43 [19072/50048]	Loss: 0.4337
Training Epoch: 43 [19200/50048]	Loss: 0.3200
Training Epoch: 43 [19328/50048]	Loss: 0.3260
Training Epoch: 43 [19456/50048]	Loss: 0.3555
Training Epoch: 43 [19584/50048]	Loss: 0.4130
Training Epoch: 43 [19712/50048]	Loss: 0.4472
Training Epoch: 43 [19840/50048]	Loss: 0.3746
Training Epoch: 43 [19968/50048]	Loss: 0.3406
Training Epoch: 43 [20096/50048]	Loss: 0.3395
Training Epoch: 43 [20224/50048]	Loss: 0.3418
Training Epoch: 43 [20352/50048]	Loss: 0.3492
Training Epoch: 43 [20480/50048]	Loss: 0.4026
Training Epoch: 43 [20608/50048]	Loss: 0.3932
Training Epoch: 43 [20736/50048]	Loss: 0.4145
Training Epoch: 43 [20864/50048]	Loss: 0.3627
Training Epoch: 43 [20992/50048]	Loss: 0.5210
Training Epoch: 43 [21120/50048]	Loss: 0.4071
Training Epoch: 43 [21248/50048]	Loss: 0.4322
Training Epoch: 43 [21376/50048]	Loss: 0.4136
Training Epoch: 43 [21504/50048]	Loss: 0.3283
Training Epoch: 43 [21632/50048]	Loss: 0.4982
Training Epoch: 43 [21760/50048]	Loss: 0.3789
Training Epoch: 43 [21888/50048]	Loss: 0.3530
Training Epoch: 43 [22016/50048]	Loss: 0.3679
Training Epoch: 43 [22144/50048]	Loss: 0.4335
Training Epoch: 43 [22272/50048]	Loss: 0.3047
Training Epoch: 43 [22400/50048]	Loss: 0.3799
Training Epoch: 43 [22528/50048]	Loss: 0.2384
Training Epoch: 43 [22656/50048]	Loss: 0.4096
Training Epoch: 43 [22784/50048]	Loss: 0.3198
Training Epoch: 43 [22912/50048]	Loss: 0.4419
Training Epoch: 43 [23040/50048]	Loss: 0.3823
Training Epoch: 43 [23168/50048]	Loss: 0.3142
Training Epoch: 43 [23296/50048]	Loss: 0.4644
Training Epoch: 43 [23424/50048]	Loss: 0.3857
Training Epoch: 43 [23552/50048]	Loss: 0.3297
Training Epoch: 43 [23680/50048]	Loss: 0.4282
Training Epoch: 43 [23808/50048]	Loss: 0.2839
Training Epoch: 43 [23936/50048]	Loss: 0.4534
Training Epoch: 43 [24064/50048]	Loss: 0.3612
Training Epoch: 43 [24192/50048]	Loss: 0.5179
Training Epoch: 43 [24320/50048]	Loss: 0.3481
Training Epoch: 43 [24448/50048]	Loss: 0.3202
Training Epoch: 43 [24576/50048]	Loss: 0.3920
Training Epoch: 43 [24704/50048]	Loss: 0.3908
Training Epoch: 43 [24832/50048]	Loss: 0.3323
Training Epoch: 43 [24960/50048]	Loss: 0.4330
Training Epoch: 43 [25088/50048]	Loss: 0.4215
Training Epoch: 43 [25216/50048]	Loss: 0.4803
Training Epoch: 43 [25344/50048]	Loss: 0.5402
Training Epoch: 43 [25472/50048]	Loss: 0.4570
Training Epoch: 43 [25600/50048]	Loss: 0.4662
Training Epoch: 43 [25728/50048]	Loss: 0.4152
Training Epoch: 43 [25856/50048]	Loss: 0.2956
Training Epoch: 43 [25984/50048]	Loss: 0.4861
Training Epoch: 43 [26112/50048]	Loss: 0.3641
Training Epoch: 43 [26240/50048]	Loss: 0.3541
Training Epoch: 43 [26368/50048]	Loss: 0.3438
Training Epoch: 43 [26496/50048]	Loss: 0.4764
Training Epoch: 43 [26624/50048]	Loss: 0.3672
Training Epoch: 43 [26752/50048]	Loss: 0.4521
Training Epoch: 43 [26880/50048]	Loss: 0.5504
Training Epoch: 43 [27008/50048]	Loss: 0.3896
Training Epoch: 43 [27136/50048]	Loss: 0.4355
Training Epoch: 43 [27264/50048]	Loss: 0.3116
Training Epoch: 43 [27392/50048]	Loss: 0.4123
Training Epoch: 43 [27520/50048]	Loss: 0.3206
Training Epoch: 43 [27648/50048]	Loss: 0.3965
Training Epoch: 43 [27776/50048]	Loss: 0.3983
Training Epoch: 43 [27904/50048]	Loss: 0.4239
Training Epoch: 43 [28032/50048]	Loss: 0.3713
Training Epoch: 43 [28160/50048]	Loss: 0.4009
Training Epoch: 43 [28288/50048]	Loss: 0.3741
Training Epoch: 43 [28416/50048]	Loss: 0.5409
Training Epoch: 43 [28544/50048]	Loss: 0.2991
Training Epoch: 43 [28672/50048]	Loss: 0.3601
Training Epoch: 43 [28800/50048]	Loss: 0.3741
Training Epoch: 43 [28928/50048]	Loss: 0.3621
Training Epoch: 43 [29056/50048]	Loss: 0.4286
Training Epoch: 43 [29184/50048]	Loss: 0.2646
Training Epoch: 43 [29312/50048]	Loss: 0.3156
Training Epoch: 43 [29440/50048]	Loss: 0.3057
Training Epoch: 43 [29568/50048]	Loss: 0.4742
Training Epoch: 43 [29696/50048]	Loss: 0.3636
Training Epoch: 43 [29824/50048]	Loss: 0.3089
Training Epoch: 43 [29952/50048]	Loss: 0.5649
Training Epoch: 43 [30080/50048]	Loss: 0.3043
Training Epoch: 43 [30208/50048]	Loss: 0.4404
Training Epoch: 43 [30336/50048]	Loss: 0.4600
Training Epoch: 43 [30464/50048]	Loss: 0.3078
Training Epoch: 43 [30592/50048]	Loss: 0.3214
Training Epoch: 43 [30720/50048]	Loss: 0.4760
Training Epoch: 43 [30848/50048]	Loss: 0.4073
Training Epoch: 43 [30976/50048]	Loss: 0.3953
Training Epoch: 43 [31104/50048]	Loss: 0.4793
Training Epoch: 43 [31232/50048]	Loss: 0.4392
Training Epoch: 43 [31360/50048]	Loss: 0.2808
Training Epoch: 43 [31488/50048]	Loss: 0.3403
Training Epoch: 43 [31616/50048]	Loss: 0.4252
Training Epoch: 43 [31744/50048]	Loss: 0.3810
Training Epoch: 43 [31872/50048]	Loss: 0.5075
Training Epoch: 43 [32000/50048]	Loss: 0.4039
Training Epoch: 43 [32128/50048]	Loss: 0.3288
Training Epoch: 43 [32256/50048]	Loss: 0.2699
Training Epoch: 43 [32384/50048]	Loss: 0.5378
Training Epoch: 43 [32512/50048]	Loss: 0.5749
Training Epoch: 43 [32640/50048]	Loss: 0.3504
Training Epoch: 43 [32768/50048]	Loss: 0.4287
Training Epoch: 43 [32896/50048]	Loss: 0.4396
Training Epoch: 43 [33024/50048]	Loss: 0.4257
Training Epoch: 43 [33152/50048]	Loss: 0.3148
Training Epoch: 43 [33280/50048]	Loss: 0.4017
Training Epoch: 43 [33408/50048]	Loss: 0.2441
Training Epoch: 43 [33536/50048]	Loss: 0.2253
Training Epoch: 43 [33664/50048]	Loss: 0.3688
Training Epoch: 43 [33792/50048]	Loss: 0.3030
Training Epoch: 43 [33920/50048]	Loss: 0.2409
Training Epoch: 43 [34048/50048]	Loss: 0.4416
Training Epoch: 43 [34176/50048]	Loss: 0.4450
Training Epoch: 43 [34304/50048]	Loss: 0.3598
Training Epoch: 43 [34432/50048]	Loss: 0.3690
Training Epoch: 43 [34560/50048]	Loss: 0.2821
Training Epoch: 43 [34688/50048]	Loss: 0.5432
Training Epoch: 43 [34816/50048]	Loss: 0.3270
Training Epoch: 43 [34944/50048]	Loss: 0.3272
Training Epoch: 43 [35072/50048]	Loss: 0.3398
Training Epoch: 43 [35200/50048]	Loss: 0.2889
Training Epoch: 43 [35328/50048]	Loss: 0.3633
Training Epoch: 43 [35456/50048]	Loss: 0.3955
Training Epoch: 43 [35584/50048]	Loss: 0.3929
Training Epoch: 43 [35712/50048]	Loss: 0.3067
Training Epoch: 43 [35840/50048]	Loss: 0.3147
Training Epoch: 43 [35968/50048]	Loss: 0.2624
Training Epoch: 43 [36096/50048]	Loss: 0.2822
Training Epoch: 43 [36224/50048]	Loss: 0.3951
Training Epoch: 43 [36352/50048]	Loss: 0.4688
Training Epoch: 43 [36480/50048]	Loss: 0.4222
Training Epoch: 43 [36608/50048]	Loss: 0.2540
Training Epoch: 43 [36736/50048]	Loss: 0.4857
Training Epoch: 43 [36864/50048]	Loss: 0.4020
Training Epoch: 43 [36992/50048]	Loss: 0.3927
Training Epoch: 43 [37120/50048]	Loss: 0.5192
Training Epoch: 43 [37248/50048]	Loss: 0.2021
Training Epoch: 43 [37376/50048]	Loss: 0.4381
Training Epoch: 43 [37504/50048]	Loss: 0.3822
Training Epoch: 43 [37632/50048]	Loss: 0.3256
Training Epoch: 43 [37760/50048]	Loss: 0.3205
Training Epoch: 43 [37888/50048]	Loss: 0.3755
Training Epoch: 43 [38016/50048]	Loss: 0.4057
Training Epoch: 43 [38144/50048]	Loss: 0.4978
Training Epoch: 43 [38272/50048]	Loss: 0.3213
Training Epoch: 43 [38400/50048]	Loss: 0.2463
Training Epoch: 43 [38528/50048]	Loss: 0.3884
Training Epoch: 43 [38656/50048]	Loss: 0.2875
Training Epoch: 43 [38784/50048]	Loss: 0.3503
Training Epoch: 43 [38912/50048]	Loss: 0.3701
Training Epoch: 43 [39040/50048]	Loss: 0.5458
Training Epoch: 43 [39168/50048]	Loss: 0.3017
Training Epoch: 43 [39296/50048]	Loss: 0.2906
Training Epoch: 43 [39424/50048]	Loss: 0.2868
Training Epoch: 43 [39552/50048]	Loss: 0.4205
Training Epoch: 43 [39680/50048]	Loss: 0.3319
Training Epoch: 43 [39808/50048]	Loss: 0.2640
Training Epoch: 43 [39936/50048]	Loss: 0.4531
Training Epoch: 43 [40064/50048]	Loss: 0.3005
Training Epoch: 43 [40192/50048]	Loss: 0.3656
Training Epoch: 43 [40320/50048]	Loss: 0.2714
Training Epoch: 43 [40448/50048]	Loss: 0.3915
Training Epoch: 43 [40576/50048]	Loss: 0.3423
Training Epoch: 43 [40704/50048]	Loss: 0.4182
Training Epoch: 43 [40832/50048]	Loss: 0.4510
Training Epoch: 43 [40960/50048]	Loss: 0.3011
Training Epoch: 43 [41088/50048]	Loss: 0.5496
Training Epoch: 43 [41216/50048]	Loss: 0.4828
Training Epoch: 43 [41344/50048]	Loss: 0.3890
Training Epoch: 43 [41472/50048]	Loss: 0.4358
Training Epoch: 43 [41600/50048]	Loss: 0.4308
Training Epoch: 43 [41728/50048]	Loss: 0.3831
Training Epoch: 43 [41856/50048]	Loss: 0.3993
Training Epoch: 43 [41984/50048]	Loss: 0.3442
Training Epoch: 43 [42112/50048]	Loss: 0.3352
Training Epoch: 43 [42240/50048]	Loss: 0.4302
Training Epoch: 43 [42368/50048]	Loss: 0.4767
Training Epoch: 43 [42496/50048]	Loss: 0.4078
Training Epoch: 43 [42624/50048]	Loss: 0.3780
Training Epoch: 43 [42752/50048]	Loss: 0.4215
Training Epoch: 43 [42880/50048]	Loss: 0.4279
Training Epoch: 43 [43008/50048]	Loss: 0.4265
Training Epoch: 43 [43136/50048]	Loss: 0.4498
Training Epoch: 43 [43264/50048]	Loss: 0.4075
Training Epoch: 43 [43392/50048]	Loss: 0.3991
Training Epoch: 43 [43520/50048]	Loss: 0.4714
Training Epoch: 43 [43648/50048]	Loss: 0.2831
Training Epoch: 43 [43776/50048]	Loss: 0.3709
Training Epoch: 43 [43904/50048]	Loss: 0.4711
Training Epoch: 43 [44032/50048]	Loss: 0.2917
Training Epoch: 43 [44160/50048]	Loss: 0.5019
Training Epoch: 43 [44288/50048]	Loss: 0.4797
Training Epoch: 43 [44416/50048]	Loss: 0.3212
Training Epoch: 43 [44544/50048]	Loss: 0.3117
Training Epoch: 43 [44672/50048]	Loss: 0.4541
Training Epoch: 43 [44800/50048]	Loss: 0.4972
Training Epoch: 43 [44928/50048]	Loss: 0.4739
Training Epoch: 43 [45056/50048]	Loss: 0.3438
Training Epoch: 43 [45184/50048]	Loss: 0.3750
Training Epoch: 43 [45312/50048]	Loss: 0.2901
Training Epoch: 43 [45440/50048]	Loss: 0.5492
Training Epoch: 43 [45568/50048]	Loss: 0.3286
Training Epoch: 43 [45696/50048]	Loss: 0.4537
2022-12-06 07:13:59,218 [ZeusDataLoader(train)] train epoch 44 done: time=86.52 energy=10494.56
2022-12-06 07:13:59,219 [ZeusDataLoader(eval)] Epoch 44 begin.
Training Epoch: 43 [45824/50048]	Loss: 0.3538
Training Epoch: 43 [45952/50048]	Loss: 0.3860
Training Epoch: 43 [46080/50048]	Loss: 0.5471
Training Epoch: 43 [46208/50048]	Loss: 0.3201
Training Epoch: 43 [46336/50048]	Loss: 0.4078
Training Epoch: 43 [46464/50048]	Loss: 0.2969
Training Epoch: 43 [46592/50048]	Loss: 0.3775
Training Epoch: 43 [46720/50048]	Loss: 0.3861
Training Epoch: 43 [46848/50048]	Loss: 0.4452
Training Epoch: 43 [46976/50048]	Loss: 0.4989
Training Epoch: 43 [47104/50048]	Loss: 0.4651
Training Epoch: 43 [47232/50048]	Loss: 0.5056
Training Epoch: 43 [47360/50048]	Loss: 0.4162
Training Epoch: 43 [47488/50048]	Loss: 0.3965
Training Epoch: 43 [47616/50048]	Loss: 0.4201
Training Epoch: 43 [47744/50048]	Loss: 0.5513
Training Epoch: 43 [47872/50048]	Loss: 0.4193
Training Epoch: 43 [48000/50048]	Loss: 0.3984
Training Epoch: 43 [48128/50048]	Loss: 0.5397
Training Epoch: 43 [48256/50048]	Loss: 0.4979
Training Epoch: 43 [48384/50048]	Loss: 0.4271
Training Epoch: 43 [48512/50048]	Loss: 0.5799
Training Epoch: 43 [48640/50048]	Loss: 0.3002
Training Epoch: 43 [48768/50048]	Loss: 0.4384
Training Epoch: 43 [48896/50048]	Loss: 0.3334
Training Epoch: 43 [49024/50048]	Loss: 0.4687
Training Epoch: 43 [49152/50048]	Loss: 0.4332
Training Epoch: 43 [49280/50048]	Loss: 0.3771
Training Epoch: 43 [49408/50048]	Loss: 0.3664
Training Epoch: 43 [49536/50048]	Loss: 0.5874
Training Epoch: 43 [49664/50048]	Loss: 0.3835
Training Epoch: 43 [49792/50048]	Loss: 0.3623
Training Epoch: 43 [49920/50048]	Loss: 0.4462
Training Epoch: 43 [50048/50048]	Loss: 0.5104
2022-12-06 12:14:02.945 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:14:02,991 [ZeusDataLoader(eval)] eval epoch 44 done: time=3.76 energy=453.68
2022-12-06 07:14:02,991 [ZeusDataLoader(train)] Up to epoch 44: time=3968.41, energy=481730.27, cost=588100.95
2022-12-06 07:14:02,991 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:14:02,991 [ZeusDataLoader(train)] Expected next epoch: time=4058.21, energy=492528.29, cost=601357.33
2022-12-06 07:14:02,992 [ZeusDataLoader(train)] Epoch 45 begin.
Validation Epoch: 43, Average loss: 0.0145, Accuracy: 0.6196
2022-12-06 07:14:03,187 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:14:03,188 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:14:03.197 [ZeusMonitor] Monitor started.
2022-12-06 12:14:03.197 [ZeusMonitor] Running indefinitely. 2022-12-06 12:14:03.198 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:14:03.198 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e45+gpu0.power.log
Training Epoch: 44 [128/50048]	Loss: 0.4173
Training Epoch: 44 [256/50048]	Loss: 0.2887
Training Epoch: 44 [384/50048]	Loss: 0.3765
Training Epoch: 44 [512/50048]	Loss: 0.2626
Training Epoch: 44 [640/50048]	Loss: 0.3354
Training Epoch: 44 [768/50048]	Loss: 0.3414
Training Epoch: 44 [896/50048]	Loss: 0.2574
Training Epoch: 44 [1024/50048]	Loss: 0.2872
Training Epoch: 44 [1152/50048]	Loss: 0.3834
Training Epoch: 44 [1280/50048]	Loss: 0.4437
Training Epoch: 44 [1408/50048]	Loss: 0.2444
Training Epoch: 44 [1536/50048]	Loss: 0.3920
Training Epoch: 44 [1664/50048]	Loss: 0.2740
Training Epoch: 44 [1792/50048]	Loss: 0.3411
Training Epoch: 44 [1920/50048]	Loss: 0.2343
Training Epoch: 44 [2048/50048]	Loss: 0.3298
Training Epoch: 44 [2176/50048]	Loss: 0.3015
Training Epoch: 44 [2304/50048]	Loss: 0.4578
Training Epoch: 44 [2432/50048]	Loss: 0.2375
Training Epoch: 44 [2560/50048]	Loss: 0.3136
Training Epoch: 44 [2688/50048]	Loss: 0.3017
Training Epoch: 44 [2816/50048]	Loss: 0.3693
Training Epoch: 44 [2944/50048]	Loss: 0.3660
Training Epoch: 44 [3072/50048]	Loss: 0.2491
Training Epoch: 44 [3200/50048]	Loss: 0.2687
Training Epoch: 44 [3328/50048]	Loss: 0.3149
Training Epoch: 44 [3456/50048]	Loss: 0.2837
Training Epoch: 44 [3584/50048]	Loss: 0.2876
Training Epoch: 44 [3712/50048]	Loss: 0.2563
Training Epoch: 44 [3840/50048]	Loss: 0.3152
Training Epoch: 44 [3968/50048]	Loss: 0.3425
Training Epoch: 44 [4096/50048]	Loss: 0.2831
Training Epoch: 44 [4224/50048]	Loss: 0.3169
Training Epoch: 44 [4352/50048]	Loss: 0.2783
Training Epoch: 44 [4480/50048]	Loss: 0.2674
Training Epoch: 44 [4608/50048]	Loss: 0.3094
Training Epoch: 44 [4736/50048]	Loss: 0.3278
Training Epoch: 44 [4864/50048]	Loss: 0.3534
Training Epoch: 44 [4992/50048]	Loss: 0.3205
Training Epoch: 44 [5120/50048]	Loss: 0.2480
Training Epoch: 44 [5248/50048]	Loss: 0.3252
Training Epoch: 44 [5376/50048]	Loss: 0.2647
Training Epoch: 44 [5504/50048]	Loss: 0.3148
Training Epoch: 44 [5632/50048]	Loss: 0.3957
Training Epoch: 44 [5760/50048]	Loss: 0.2846
Training Epoch: 44 [5888/50048]	Loss: 0.2187
Training Epoch: 44 [6016/50048]	Loss: 0.3949
Training Epoch: 44 [6144/50048]	Loss: 0.3077
Training Epoch: 44 [6272/50048]	Loss: 0.3260
Training Epoch: 44 [6400/50048]	Loss: 0.3920
Training Epoch: 44 [6528/50048]	Loss: 0.3254
Training Epoch: 44 [6656/50048]	Loss: 0.2826
Training Epoch: 44 [6784/50048]	Loss: 0.3478
Training Epoch: 44 [6912/50048]	Loss: 0.4119
Training Epoch: 44 [7040/50048]	Loss: 0.2962
Training Epoch: 44 [7168/50048]	Loss: 0.2957
Training Epoch: 44 [7296/50048]	Loss: 0.2883
Training Epoch: 44 [7424/50048]	Loss: 0.3426
Training Epoch: 44 [7552/50048]	Loss: 0.3576
Training Epoch: 44 [7680/50048]	Loss: 0.4065
Training Epoch: 44 [7808/50048]	Loss: 0.2621
Training Epoch: 44 [7936/50048]	Loss: 0.2893
Training Epoch: 44 [8064/50048]	Loss: 0.3459
Training Epoch: 44 [8192/50048]	Loss: 0.2781
Training Epoch: 44 [8320/50048]	Loss: 0.3057
Training Epoch: 44 [8448/50048]	Loss: 0.4685
Training Epoch: 44 [8576/50048]	Loss: 0.2448
Training Epoch: 44 [8704/50048]	Loss: 0.4723
Training Epoch: 44 [8832/50048]	Loss: 0.2821
Training Epoch: 44 [8960/50048]	Loss: 0.3558
Training Epoch: 44 [9088/50048]	Loss: 0.3317
Training Epoch: 44 [9216/50048]	Loss: 0.2827
Training Epoch: 44 [9344/50048]	Loss: 0.2204
Training Epoch: 44 [9472/50048]	Loss: 0.2917
Training Epoch: 44 [9600/50048]	Loss: 0.3334
Training Epoch: 44 [9728/50048]	Loss: 0.3081
Training Epoch: 44 [9856/50048]	Loss: 0.2693
Training Epoch: 44 [9984/50048]	Loss: 0.4150
Training Epoch: 44 [10112/50048]	Loss: 0.3048
Training Epoch: 44 [10240/50048]	Loss: 0.2096
Training Epoch: 44 [10368/50048]	Loss: 0.3394
Training Epoch: 44 [10496/50048]	Loss: 0.3956
Training Epoch: 44 [10624/50048]	Loss: 0.3878
Training Epoch: 44 [10752/50048]	Loss: 0.3609
Training Epoch: 44 [10880/50048]	Loss: 0.3213
Training Epoch: 44 [11008/50048]	Loss: 0.3820
Training Epoch: 44 [11136/50048]	Loss: 0.2917
Training Epoch: 44 [11264/50048]	Loss: 0.3787
Training Epoch: 44 [11392/50048]	Loss: 0.3974
Training Epoch: 44 [11520/50048]	Loss: 0.2425
Training Epoch: 44 [11648/50048]	Loss: 0.2858
Training Epoch: 44 [11776/50048]	Loss: 0.2811
Training Epoch: 44 [11904/50048]	Loss: 0.3219
Training Epoch: 44 [12032/50048]	Loss: 0.3453
Training Epoch: 44 [12160/50048]	Loss: 0.3311
Training Epoch: 44 [12288/50048]	Loss: 0.2894
Training Epoch: 44 [12416/50048]	Loss: 0.4746
Training Epoch: 44 [12544/50048]	Loss: 0.4464
Training Epoch: 44 [12672/50048]	Loss: 0.2957
Training Epoch: 44 [12800/50048]	Loss: 0.3816
Training Epoch: 44 [12928/50048]	Loss: 0.4580
Training Epoch: 44 [13056/50048]	Loss: 0.2486
Training Epoch: 44 [13184/50048]	Loss: 0.3071
Training Epoch: 44 [13312/50048]	Loss: 0.3261
Training Epoch: 44 [13440/50048]	Loss: 0.3934
Training Epoch: 44 [13568/50048]	Loss: 0.3219
Training Epoch: 44 [13696/50048]	Loss: 0.3972
Training Epoch: 44 [13824/50048]	Loss: 0.2527
Training Epoch: 44 [13952/50048]	Loss: 0.3514
Training Epoch: 44 [14080/50048]	Loss: 0.3203
Training Epoch: 44 [14208/50048]	Loss: 0.2831
Training Epoch: 44 [14336/50048]	Loss: 0.3237
Training Epoch: 44 [14464/50048]	Loss: 0.2400
Training Epoch: 44 [14592/50048]	Loss: 0.3328
Training Epoch: 44 [14720/50048]	Loss: 0.3090
Training Epoch: 44 [14848/50048]	Loss: 0.3630
Training Epoch: 44 [14976/50048]	Loss: 0.3975
Training Epoch: 44 [15104/50048]	Loss: 0.3128
Training Epoch: 44 [15232/50048]	Loss: 0.2724
Training Epoch: 44 [15360/50048]	Loss: 0.3075
Training Epoch: 44 [15488/50048]	Loss: 0.3415
Training Epoch: 44 [15616/50048]	Loss: 0.2401
Training Epoch: 44 [15744/50048]	Loss: 0.3387
Training Epoch: 44 [15872/50048]	Loss: 0.2770
Training Epoch: 44 [16000/50048]	Loss: 0.3753
Training Epoch: 44 [16128/50048]	Loss: 0.4639
Training Epoch: 44 [16256/50048]	Loss: 0.3749
Training Epoch: 44 [16384/50048]	Loss: 0.2418
Training Epoch: 44 [16512/50048]	Loss: 0.2700
Training Epoch: 44 [16640/50048]	Loss: 0.3221
Training Epoch: 44 [16768/50048]	Loss: 0.3501
Training Epoch: 44 [16896/50048]	Loss: 0.4489
Training Epoch: 44 [17024/50048]	Loss: 0.4186
Training Epoch: 44 [17152/50048]	Loss: 0.5622
Training Epoch: 44 [17280/50048]	Loss: 0.2855
Training Epoch: 44 [17408/50048]	Loss: 0.4752
Training Epoch: 44 [17536/50048]	Loss: 0.4229
Training Epoch: 44 [17664/50048]	Loss: 0.2737
Training Epoch: 44 [17792/50048]	Loss: 0.3838
Training Epoch: 44 [17920/50048]	Loss: 0.3546
Training Epoch: 44 [18048/50048]	Loss: 0.4515
Training Epoch: 44 [18176/50048]	Loss: 0.3859
Training Epoch: 44 [18304/50048]	Loss: 0.3496
Training Epoch: 44 [18432/50048]	Loss: 0.2590
Training Epoch: 44 [18560/50048]	Loss: 0.3048
Training Epoch: 44 [18688/50048]	Loss: 0.3318
Training Epoch: 44 [18816/50048]	Loss: 0.4511
Training Epoch: 44 [18944/50048]	Loss: 0.3270
Training Epoch: 44 [19072/50048]	Loss: 0.3524
Training Epoch: 44 [19200/50048]	Loss: 0.3430
Training Epoch: 44 [19328/50048]	Loss: 0.3124
Training Epoch: 44 [19456/50048]	Loss: 0.4359
Training Epoch: 44 [19584/50048]	Loss: 0.3842
Training Epoch: 44 [19712/50048]	Loss: 0.2490
Training Epoch: 44 [19840/50048]	Loss: 0.2838
Training Epoch: 44 [19968/50048]	Loss: 0.3353
Training Epoch: 44 [20096/50048]	Loss: 0.2700
Training Epoch: 44 [20224/50048]	Loss: 0.3147
Training Epoch: 44 [20352/50048]	Loss: 0.4157
Training Epoch: 44 [20480/50048]	Loss: 0.3306
Training Epoch: 44 [20608/50048]	Loss: 0.3742
Training Epoch: 44 [20736/50048]	Loss: 0.4931
Training Epoch: 44 [20864/50048]	Loss: 0.3307
Training Epoch: 44 [20992/50048]	Loss: 0.3334
Training Epoch: 44 [21120/50048]	Loss: 0.3865
Training Epoch: 44 [21248/50048]	Loss: 0.4459
Training Epoch: 44 [21376/50048]	Loss: 0.2537
Training Epoch: 44 [21504/50048]	Loss: 0.2957
Training Epoch: 44 [21632/50048]	Loss: 0.3591
Training Epoch: 44 [21760/50048]	Loss: 0.3391
Training Epoch: 44 [21888/50048]	Loss: 0.2917
Training Epoch: 44 [22016/50048]	Loss: 0.4283
Training Epoch: 44 [22144/50048]	Loss: 0.5090
Training Epoch: 44 [22272/50048]	Loss: 0.4376
Training Epoch: 44 [22400/50048]	Loss: 0.3707
Training Epoch: 44 [22528/50048]	Loss: 0.3119
Training Epoch: 44 [22656/50048]	Loss: 0.3348
Training Epoch: 44 [22784/50048]	Loss: 0.2352
Training Epoch: 44 [22912/50048]	Loss: 0.3699
Training Epoch: 44 [23040/50048]	Loss: 0.3409
Training Epoch: 44 [23168/50048]	Loss: 0.2352
Training Epoch: 44 [23296/50048]	Loss: 0.2869
Training Epoch: 44 [23424/50048]	Loss: 0.2866
Training Epoch: 44 [23552/50048]	Loss: 0.5158
Training Epoch: 44 [23680/50048]	Loss: 0.4932
Training Epoch: 44 [23808/50048]	Loss: 0.3927
Training Epoch: 44 [23936/50048]	Loss: 0.3326
Training Epoch: 44 [24064/50048]	Loss: 0.4475
Training Epoch: 44 [24192/50048]	Loss: 0.3391
Training Epoch: 44 [24320/50048]	Loss: 0.3369
Training Epoch: 44 [24448/50048]	Loss: 0.3171
Training Epoch: 44 [24576/50048]	Loss: 0.3353
Training Epoch: 44 [24704/50048]	Loss: 0.3965
Training Epoch: 44 [24832/50048]	Loss: 0.2833
Training Epoch: 44 [24960/50048]	Loss: 0.4422
Training Epoch: 44 [25088/50048]	Loss: 0.2056
Training Epoch: 44 [25216/50048]	Loss: 0.3710
Training Epoch: 44 [25344/50048]	Loss: 0.2925
Training Epoch: 44 [25472/50048]	Loss: 0.3358
Training Epoch: 44 [25600/50048]	Loss: 0.2810
Training Epoch: 44 [25728/50048]	Loss: 0.4131
Training Epoch: 44 [25856/50048]	Loss: 0.4014
Training Epoch: 44 [25984/50048]	Loss: 0.3514
Training Epoch: 44 [26112/50048]	Loss: 0.4620
Training Epoch: 44 [26240/50048]	Loss: 0.3298
Training Epoch: 44 [26368/50048]	Loss: 0.4260
Training Epoch: 44 [26496/50048]	Loss: 0.4381
Training Epoch: 44 [26624/50048]	Loss: 0.3708
Training Epoch: 44 [26752/50048]	Loss: 0.4036
Training Epoch: 44 [26880/50048]	Loss: 0.3561
Training Epoch: 44 [27008/50048]	Loss: 0.4157
Training Epoch: 44 [27136/50048]	Loss: 0.3131
Training Epoch: 44 [27264/50048]	Loss: 0.3889
Training Epoch: 44 [27392/50048]	Loss: 0.3027
Training Epoch: 44 [27520/50048]	Loss: 0.4626
Training Epoch: 44 [27648/50048]	Loss: 0.3734
Training Epoch: 44 [27776/50048]	Loss: 0.3200
Training Epoch: 44 [27904/50048]	Loss: 0.3717
Training Epoch: 44 [28032/50048]	Loss: 0.5334
Training Epoch: 44 [28160/50048]	Loss: 0.2779
Training Epoch: 44 [28288/50048]	Loss: 0.4261
Training Epoch: 44 [28416/50048]	Loss: 0.3508
Training Epoch: 44 [28544/50048]	Loss: 0.3705
Training Epoch: 44 [28672/50048]	Loss: 0.3002
Training Epoch: 44 [28800/50048]	Loss: 0.3769
Training Epoch: 44 [28928/50048]	Loss: 0.4442
Training Epoch: 44 [29056/50048]	Loss: 0.3673
Training Epoch: 44 [29184/50048]	Loss: 0.2266
Training Epoch: 44 [29312/50048]	Loss: 0.3737
Training Epoch: 44 [29440/50048]	Loss: 0.2976
Training Epoch: 44 [29568/50048]	Loss: 0.4305
Training Epoch: 44 [29696/50048]	Loss: 0.3383
Training Epoch: 44 [29824/50048]	Loss: 0.3679
Training Epoch: 44 [29952/50048]	Loss: 0.3466
Training Epoch: 44 [30080/50048]	Loss: 0.3574
Training Epoch: 44 [30208/50048]	Loss: 0.2926
Training Epoch: 44 [30336/50048]	Loss: 0.3435
Training Epoch: 44 [30464/50048]	Loss: 0.5038
Training Epoch: 44 [30592/50048]	Loss: 0.3571
Training Epoch: 44 [30720/50048]	Loss: 0.3810
Training Epoch: 44 [30848/50048]	Loss: 0.4618
Training Epoch: 44 [30976/50048]	Loss: 0.4041
Training Epoch: 44 [31104/50048]	Loss: 0.3044
Training Epoch: 44 [31232/50048]	Loss: 0.4710
Training Epoch: 44 [31360/50048]	Loss: 0.3054
Training Epoch: 44 [31488/50048]	Loss: 0.4289
Training Epoch: 44 [31616/50048]	Loss: 0.3451
Training Epoch: 44 [31744/50048]	Loss: 0.5644
Training Epoch: 44 [31872/50048]	Loss: 0.4292
Training Epoch: 44 [32000/50048]	Loss: 0.4599
Training Epoch: 44 [32128/50048]	Loss: 0.2713
Training Epoch: 44 [32256/50048]	Loss: 0.2729
Training Epoch: 44 [32384/50048]	Loss: 0.2660
Training Epoch: 44 [32512/50048]	Loss: 0.3400
Training Epoch: 44 [32640/50048]	Loss: 0.3233
Training Epoch: 44 [32768/50048]	Loss: 0.3540
Training Epoch: 44 [32896/50048]	Loss: 0.4766
Training Epoch: 44 [33024/50048]	Loss: 0.3544
Training Epoch: 44 [33152/50048]	Loss: 0.3372
Training Epoch: 44 [33280/50048]	Loss: 0.4848
Training Epoch: 44 [33408/50048]	Loss: 0.4796
Training Epoch: 44 [33536/50048]	Loss: 0.3180
Training Epoch: 44 [33664/50048]	Loss: 0.5117
Training Epoch: 44 [33792/50048]	Loss: 0.3919
Training Epoch: 44 [33920/50048]	Loss: 0.3819
Training Epoch: 44 [34048/50048]	Loss: 0.4680
Training Epoch: 44 [34176/50048]	Loss: 0.4770
Training Epoch: 44 [34304/50048]	Loss: 0.5115
Training Epoch: 44 [34432/50048]	Loss: 0.4522
Training Epoch: 44 [34560/50048]	Loss: 0.2977
Training Epoch: 44 [34688/50048]	Loss: 0.3201
Training Epoch: 44 [34816/50048]	Loss: 0.4799
Training Epoch: 44 [34944/50048]	Loss: 0.3837
Training Epoch: 44 [35072/50048]	Loss: 0.3937
Training Epoch: 44 [35200/50048]	Loss: 0.3159
Training Epoch: 44 [35328/50048]	Loss: 0.5089
Training Epoch: 44 [35456/50048]	Loss: 0.4050
Training Epoch: 44 [35584/50048]	Loss: 0.4107
Training Epoch: 44 [35712/50048]	Loss: 0.4357
Training Epoch: 44 [35840/50048]	Loss: 0.3228
Training Epoch: 44 [35968/50048]	Loss: 0.3817
Training Epoch: 44 [36096/50048]	Loss: 0.4577
Training Epoch: 44 [36224/50048]	Loss: 0.3187
Training Epoch: 44 [36352/50048]	Loss: 0.3434
Training Epoch: 44 [36480/50048]	Loss: 0.4224
Training Epoch: 44 [36608/50048]	Loss: 0.4383
Training Epoch: 44 [36736/50048]	Loss: 0.4818
Training Epoch: 44 [36864/50048]	Loss: 0.3992
Training Epoch: 44 [36992/50048]	Loss: 0.3548
Training Epoch: 44 [37120/50048]	Loss: 0.4348
Training Epoch: 44 [37248/50048]	Loss: 0.4282
Training Epoch: 44 [37376/50048]	Loss: 0.4405
Training Epoch: 44 [37504/50048]	Loss: 0.3593
Training Epoch: 44 [37632/50048]	Loss: 0.3712
Training Epoch: 44 [37760/50048]	Loss: 0.3188
Training Epoch: 44 [37888/50048]	Loss: 0.5414
Training Epoch: 44 [38016/50048]	Loss: 0.1921
Training Epoch: 44 [38144/50048]	Loss: 0.3318
Training Epoch: 44 [38272/50048]	Loss: 0.2363
Training Epoch: 44 [38400/50048]	Loss: 0.4282
Training Epoch: 44 [38528/50048]	Loss: 0.3185
Training Epoch: 44 [38656/50048]	Loss: 0.4924
Training Epoch: 44 [38784/50048]	Loss: 0.4351
Training Epoch: 44 [38912/50048]	Loss: 0.3145
Training Epoch: 44 [39040/50048]	Loss: 0.3577
Training Epoch: 44 [39168/50048]	Loss: 0.4776
Training Epoch: 44 [39296/50048]	Loss: 0.2895
Training Epoch: 44 [39424/50048]	Loss: 0.2961
Training Epoch: 44 [39552/50048]	Loss: 0.3158
Training Epoch: 44 [39680/50048]	Loss: 0.3433
Training Epoch: 44 [39808/50048]	Loss: 0.3357
Training Epoch: 44 [39936/50048]	Loss: 0.5438
Training Epoch: 44 [40064/50048]	Loss: 0.3739
Training Epoch: 44 [40192/50048]	Loss: 0.3489
Training Epoch: 44 [40320/50048]	Loss: 0.4159
Training Epoch: 44 [40448/50048]	Loss: 0.4292
Training Epoch: 44 [40576/50048]	Loss: 0.5482
Training Epoch: 44 [40704/50048]	Loss: 0.3717
Training Epoch: 44 [40832/50048]	Loss: 0.3797
Training Epoch: 44 [40960/50048]	Loss: 0.4012
Training Epoch: 44 [41088/50048]	Loss: 0.2947
Training Epoch: 44 [41216/50048]	Loss: 0.4032
Training Epoch: 44 [41344/50048]	Loss: 0.5370
Training Epoch: 44 [41472/50048]	Loss: 0.4212
Training Epoch: 44 [41600/50048]	Loss: 0.2537
Training Epoch: 44 [41728/50048]	Loss: 0.3440
Training Epoch: 44 [41856/50048]	Loss: 0.3389
Training Epoch: 44 [41984/50048]	Loss: 0.4916
Training Epoch: 44 [42112/50048]	Loss: 0.4792
Training Epoch: 44 [42240/50048]	Loss: 0.3108
Training Epoch: 44 [42368/50048]	Loss: 0.2128
Training Epoch: 44 [42496/50048]	Loss: 0.3743
Training Epoch: 44 [42624/50048]	Loss: 0.4963
Training Epoch: 44 [42752/50048]	Loss: 0.2821
Training Epoch: 44 [42880/50048]	Loss: 0.3048
Training Epoch: 44 [43008/50048]	Loss: 0.4029
Training Epoch: 44 [43136/50048]	Loss: 0.4193
Training Epoch: 44 [43264/50048]	Loss: 0.3792
Training Epoch: 44 [43392/50048]	Loss: 0.3950
Training Epoch: 44 [43520/50048]	Loss: 0.3787
Training Epoch: 44 [43648/50048]	Loss: 0.4025
Training Epoch: 44 [43776/50048]	Loss: 0.3962
Training Epoch: 44 [43904/50048]	Loss: 0.4789
Training Epoch: 44 [44032/50048]	Loss: 0.3932
Training Epoch: 44 [44160/50048]	Loss: 0.2797
Training Epoch: 44 [44288/50048]	Loss: 0.4642
Training Epoch: 44 [44416/50048]	Loss: 0.4179
Training Epoch: 44 [44544/50048]	Loss: 0.4506
Training Epoch: 44 [44672/50048]	Loss: 0.3997
Training Epoch: 44 [44800/50048]	Loss: 0.3049
Training Epoch: 44 [44928/50048]	Loss: 0.4161
Training Epoch: 44 [45056/50048]	Loss: 0.3452
Training Epoch: 44 [45184/50048]	Loss: 0.4123
Training Epoch: 44 [45312/50048]	Loss: 0.5563
Training Epoch: 44 [45440/50048]	Loss: 0.4080
Training Epoch: 44 [45568/50048]	Loss: 0.3830
Training Epoch: 44 [45696/50048]	Loss: 0.3312
2022-12-06 07:15:29,883 [ZeusDataLoader(train)] train epoch 45 done: time=86.88 energy=10517.86
2022-12-06 07:15:29,884 [ZeusDataLoader(eval)] Epoch 45 begin.
Training Epoch: 44 [45824/50048]	Loss: 0.2792
Training Epoch: 44 [45952/50048]	Loss: 0.3854
Training Epoch: 44 [46080/50048]	Loss: 0.4393
Training Epoch: 44 [46208/50048]	Loss: 0.1866
Training Epoch: 44 [46336/50048]	Loss: 0.5465
Training Epoch: 44 [46464/50048]	Loss: 0.3520
Training Epoch: 44 [46592/50048]	Loss: 0.2939
Training Epoch: 44 [46720/50048]	Loss: 0.4627
Training Epoch: 44 [46848/50048]	Loss: 0.4201
Training Epoch: 44 [46976/50048]	Loss: 0.4461
Training Epoch: 44 [47104/50048]	Loss: 0.3968
Training Epoch: 44 [47232/50048]	Loss: 0.4198
Training Epoch: 44 [47360/50048]	Loss: 0.4706
Training Epoch: 44 [47488/50048]	Loss: 0.3027
Training Epoch: 44 [47616/50048]	Loss: 0.3953
Training Epoch: 44 [47744/50048]	Loss: 0.3850
Training Epoch: 44 [47872/50048]	Loss: 0.3918
Training Epoch: 44 [48000/50048]	Loss: 0.4392
Training Epoch: 44 [48128/50048]	Loss: 0.3650
Training Epoch: 44 [48256/50048]	Loss: 0.3539
Training Epoch: 44 [48384/50048]	Loss: 0.4426
Training Epoch: 44 [48512/50048]	Loss: 0.3471
Training Epoch: 44 [48640/50048]	Loss: 0.3738
Training Epoch: 44 [48768/50048]	Loss: 0.4165
Training Epoch: 44 [48896/50048]	Loss: 0.4448
Training Epoch: 44 [49024/50048]	Loss: 0.4763
Training Epoch: 44 [49152/50048]	Loss: 0.5498
Training Epoch: 44 [49280/50048]	Loss: 0.3565
Training Epoch: 44 [49408/50048]	Loss: 0.2931
Training Epoch: 44 [49536/50048]	Loss: 0.4740
Training Epoch: 44 [49664/50048]	Loss: 0.5199
Training Epoch: 44 [49792/50048]	Loss: 0.2914
Training Epoch: 44 [49920/50048]	Loss: 0.3701
Training Epoch: 44 [50048/50048]	Loss: 0.2535
2022-12-06 12:15:33.597 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:15:33,611 [ZeusDataLoader(eval)] eval epoch 45 done: time=3.72 energy=453.77
2022-12-06 07:15:33,611 [ZeusDataLoader(train)] Up to epoch 45: time=4059.01, energy=492701.90, cost=601514.08
2022-12-06 07:15:33,612 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:15:33,612 [ZeusDataLoader(train)] Expected next epoch: time=4148.81, energy=503499.92, cost=614770.46
2022-12-06 07:15:33,613 [ZeusDataLoader(train)] Epoch 46 begin.
Validation Epoch: 44, Average loss: 0.0141, Accuracy: 0.6252
2022-12-06 07:15:33,820 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:15:33,821 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:15:33.835 [ZeusMonitor] Monitor started.
2022-12-06 12:15:33.835 [ZeusMonitor] Running indefinitely. 2022-12-06 12:15:33.835 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:15:33.835 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e46+gpu0.power.log
Training Epoch: 45 [128/50048]	Loss: 0.3579
Training Epoch: 45 [256/50048]	Loss: 0.3273
Training Epoch: 45 [384/50048]	Loss: 0.3896
Training Epoch: 45 [512/50048]	Loss: 0.4050
Training Epoch: 45 [640/50048]	Loss: 0.2506
Training Epoch: 45 [768/50048]	Loss: 0.3310
Training Epoch: 45 [896/50048]	Loss: 0.2420
Training Epoch: 45 [1024/50048]	Loss: 0.3276
Training Epoch: 45 [1152/50048]	Loss: 0.3062
Training Epoch: 45 [1280/50048]	Loss: 0.4262
Training Epoch: 45 [1408/50048]	Loss: 0.3271
Training Epoch: 45 [1536/50048]	Loss: 0.3792
Training Epoch: 45 [1664/50048]	Loss: 0.3478
Training Epoch: 45 [1792/50048]	Loss: 0.2406
Training Epoch: 45 [1920/50048]	Loss: 0.2542
Training Epoch: 45 [2048/50048]	Loss: 0.2920
Training Epoch: 45 [2176/50048]	Loss: 0.1996
Training Epoch: 45 [2304/50048]	Loss: 0.3247
Training Epoch: 45 [2432/50048]	Loss: 0.4237
Training Epoch: 45 [2560/50048]	Loss: 0.2501
Training Epoch: 45 [2688/50048]	Loss: 0.2998
Training Epoch: 45 [2816/50048]	Loss: 0.3616
Training Epoch: 45 [2944/50048]	Loss: 0.2735
Training Epoch: 45 [3072/50048]	Loss: 0.2402
Training Epoch: 45 [3200/50048]	Loss: 0.3473
Training Epoch: 45 [3328/50048]	Loss: 0.3287
Training Epoch: 45 [3456/50048]	Loss: 0.3652
Training Epoch: 45 [3584/50048]	Loss: 0.2890
Training Epoch: 45 [3712/50048]	Loss: 0.2397
Training Epoch: 45 [3840/50048]	Loss: 0.3732
Training Epoch: 45 [3968/50048]	Loss: 0.3157
Training Epoch: 45 [4096/50048]	Loss: 0.3017
Training Epoch: 45 [4224/50048]	Loss: 0.2274
Training Epoch: 45 [4352/50048]	Loss: 0.3779
Training Epoch: 45 [4480/50048]	Loss: 0.4286
Training Epoch: 45 [4608/50048]	Loss: 0.2179
Training Epoch: 45 [4736/50048]	Loss: 0.2850
Training Epoch: 45 [4864/50048]	Loss: 0.2882
Training Epoch: 45 [4992/50048]	Loss: 0.4104
Training Epoch: 45 [5120/50048]	Loss: 0.2553
Training Epoch: 45 [5248/50048]	Loss: 0.3659
Training Epoch: 45 [5376/50048]	Loss: 0.3586
Training Epoch: 45 [5504/50048]	Loss: 0.3662
Training Epoch: 45 [5632/50048]	Loss: 0.2872
Training Epoch: 45 [5760/50048]	Loss: 0.3182
Training Epoch: 45 [5888/50048]	Loss: 0.4073
Training Epoch: 45 [6016/50048]	Loss: 0.2898
Training Epoch: 45 [6144/50048]	Loss: 0.3535
Training Epoch: 45 [6272/50048]	Loss: 0.2272
Training Epoch: 45 [6400/50048]	Loss: 0.2296
Training Epoch: 45 [6528/50048]	Loss: 0.3458
Training Epoch: 45 [6656/50048]	Loss: 0.2150
Training Epoch: 45 [6784/50048]	Loss: 0.2808
Training Epoch: 45 [6912/50048]	Loss: 0.3009
Training Epoch: 45 [7040/50048]	Loss: 0.2746
Training Epoch: 45 [7168/50048]	Loss: 0.3234
Training Epoch: 45 [7296/50048]	Loss: 0.5368
Training Epoch: 45 [7424/50048]	Loss: 0.3129
Training Epoch: 45 [7552/50048]	Loss: 0.3624
Training Epoch: 45 [7680/50048]	Loss: 0.4103
Training Epoch: 45 [7808/50048]	Loss: 0.3332
Training Epoch: 45 [7936/50048]	Loss: 0.5369
Training Epoch: 45 [8064/50048]	Loss: 0.3770
Training Epoch: 45 [8192/50048]	Loss: 0.3950
Training Epoch: 45 [8320/50048]	Loss: 0.2877
Training Epoch: 45 [8448/50048]	Loss: 0.2679
Training Epoch: 45 [8576/50048]	Loss: 0.2516
Training Epoch: 45 [8704/50048]	Loss: 0.2972
Training Epoch: 45 [8832/50048]	Loss: 0.2457
Training Epoch: 45 [8960/50048]	Loss: 0.2461
Training Epoch: 45 [9088/50048]	Loss: 0.3080
Training Epoch: 45 [9216/50048]	Loss: 0.3374
Training Epoch: 45 [9344/50048]	Loss: 0.3904
Training Epoch: 45 [9472/50048]	Loss: 0.3573
Training Epoch: 45 [9600/50048]	Loss: 0.3919
Training Epoch: 45 [9728/50048]	Loss: 0.3110
Training Epoch: 45 [9856/50048]	Loss: 0.4092
Training Epoch: 45 [9984/50048]	Loss: 0.2065
Training Epoch: 45 [10112/50048]	Loss: 0.3716
Training Epoch: 45 [10240/50048]	Loss: 0.4081
Training Epoch: 45 [10368/50048]	Loss: 0.4956
Training Epoch: 45 [10496/50048]	Loss: 0.3359
Training Epoch: 45 [10624/50048]	Loss: 0.3286
Training Epoch: 45 [10752/50048]	Loss: 0.3043
Training Epoch: 45 [10880/50048]	Loss: 0.3534
Training Epoch: 45 [11008/50048]	Loss: 0.3434
Training Epoch: 45 [11136/50048]	Loss: 0.4542
Training Epoch: 45 [11264/50048]	Loss: 0.3760
Training Epoch: 45 [11392/50048]	Loss: 0.4417
Training Epoch: 45 [11520/50048]	Loss: 0.3252
Training Epoch: 45 [11648/50048]	Loss: 0.2889
Training Epoch: 45 [11776/50048]	Loss: 0.4175
Training Epoch: 45 [11904/50048]	Loss: 0.2830
Training Epoch: 45 [12032/50048]	Loss: 0.3179
Training Epoch: 45 [12160/50048]	Loss: 0.3016
Training Epoch: 45 [12288/50048]	Loss: 0.3229
Training Epoch: 45 [12416/50048]	Loss: 0.4076
Training Epoch: 45 [12544/50048]	Loss: 0.4006
Training Epoch: 45 [12672/50048]	Loss: 0.3075
Training Epoch: 45 [12800/50048]	Loss: 0.3974
Training Epoch: 45 [12928/50048]	Loss: 0.2818
Training Epoch: 45 [13056/50048]	Loss: 0.2851
Training Epoch: 45 [13184/50048]	Loss: 0.3114
Training Epoch: 45 [13312/50048]	Loss: 0.3215
Training Epoch: 45 [13440/50048]	Loss: 0.2772
Training Epoch: 45 [13568/50048]	Loss: 0.3064
Training Epoch: 45 [13696/50048]	Loss: 0.2586
Training Epoch: 45 [13824/50048]	Loss: 0.2350
Training Epoch: 45 [13952/50048]	Loss: 0.3636
Training Epoch: 45 [14080/50048]	Loss: 0.3523
Training Epoch: 45 [14208/50048]	Loss: 0.2728
Training Epoch: 45 [14336/50048]	Loss: 0.3730
Training Epoch: 45 [14464/50048]	Loss: 0.2519
Training Epoch: 45 [14592/50048]	Loss: 0.3859
Training Epoch: 45 [14720/50048]	Loss: 0.2510
Training Epoch: 45 [14848/50048]	Loss: 0.3485
Training Epoch: 45 [14976/50048]	Loss: 0.2626
Training Epoch: 45 [15104/50048]	Loss: 0.3586
Training Epoch: 45 [15232/50048]	Loss: 0.3707
Training Epoch: 45 [15360/50048]	Loss: 0.4018
Training Epoch: 45 [15488/50048]	Loss: 0.4093
Training Epoch: 45 [15616/50048]	Loss: 0.2215
Training Epoch: 45 [15744/50048]	Loss: 0.3933
Training Epoch: 45 [15872/50048]	Loss: 0.4565
Training Epoch: 45 [16000/50048]	Loss: 0.2833
Training Epoch: 45 [16128/50048]	Loss: 0.2970
Training Epoch: 45 [16256/50048]	Loss: 0.2799
Training Epoch: 45 [16384/50048]	Loss: 0.3093
Training Epoch: 45 [16512/50048]	Loss: 0.2784
Training Epoch: 45 [16640/50048]	Loss: 0.3491
Training Epoch: 45 [16768/50048]	Loss: 0.3392
Training Epoch: 45 [16896/50048]	Loss: 0.2335
Training Epoch: 45 [17024/50048]	Loss: 0.2783
Training Epoch: 45 [17152/50048]	Loss: 0.4137
Training Epoch: 45 [17280/50048]	Loss: 0.3869
Training Epoch: 45 [17408/50048]	Loss: 0.3942
Training Epoch: 45 [17536/50048]	Loss: 0.3592
Training Epoch: 45 [17664/50048]	Loss: 0.4248
Training Epoch: 45 [17792/50048]	Loss: 0.2282
Training Epoch: 45 [17920/50048]	Loss: 0.3074
Training Epoch: 45 [18048/50048]	Loss: 0.4242
Training Epoch: 45 [18176/50048]	Loss: 0.3306
Training Epoch: 45 [18304/50048]	Loss: 0.2679
Training Epoch: 45 [18432/50048]	Loss: 0.3243
Training Epoch: 45 [18560/50048]	Loss: 0.3597
Training Epoch: 45 [18688/50048]	Loss: 0.4258
Training Epoch: 45 [18816/50048]	Loss: 0.3336
Training Epoch: 45 [18944/50048]	Loss: 0.3581
Training Epoch: 45 [19072/50048]	Loss: 0.3567
Training Epoch: 45 [19200/50048]	Loss: 0.4257
Training Epoch: 45 [19328/50048]	Loss: 0.3514
Training Epoch: 45 [19456/50048]	Loss: 0.3956
Training Epoch: 45 [19584/50048]	Loss: 0.2751
Training Epoch: 45 [19712/50048]	Loss: 0.3857
Training Epoch: 45 [19840/50048]	Loss: 0.3761
Training Epoch: 45 [19968/50048]	Loss: 0.3083
Training Epoch: 45 [20096/50048]	Loss: 0.1926
Training Epoch: 45 [20224/50048]	Loss: 0.3247
Training Epoch: 45 [20352/50048]	Loss: 0.3696
Training Epoch: 45 [20480/50048]	Loss: 0.3489
Training Epoch: 45 [20608/50048]	Loss: 0.2479
Training Epoch: 45 [20736/50048]	Loss: 0.4446
Training Epoch: 45 [20864/50048]	Loss: 0.3683
Training Epoch: 45 [20992/50048]	Loss: 0.2390
Training Epoch: 45 [21120/50048]	Loss: 0.3108
Training Epoch: 45 [21248/50048]	Loss: 0.3249
Training Epoch: 45 [21376/50048]	Loss: 0.2627
Training Epoch: 45 [21504/50048]	Loss: 0.5039
Training Epoch: 45 [21632/50048]	Loss: 0.3607
Training Epoch: 45 [21760/50048]	Loss: 0.4210
Training Epoch: 45 [21888/50048]	Loss: 0.4390
Training Epoch: 45 [22016/50048]	Loss: 0.4537
Training Epoch: 45 [22144/50048]	Loss: 0.4721
Training Epoch: 45 [22272/50048]	Loss: 0.3046
Training Epoch: 45 [22400/50048]	Loss: 0.2928
Training Epoch: 45 [22528/50048]	Loss: 0.3579
Training Epoch: 45 [22656/50048]	Loss: 0.2895
Training Epoch: 45 [22784/50048]	Loss: 0.2843
Training Epoch: 45 [22912/50048]	Loss: 0.3192
Training Epoch: 45 [23040/50048]	Loss: 0.2843
Training Epoch: 45 [23168/50048]	Loss: 0.4002
Training Epoch: 45 [23296/50048]	Loss: 0.3586
Training Epoch: 45 [23424/50048]	Loss: 0.3351
Training Epoch: 45 [23552/50048]	Loss: 0.3962
Training Epoch: 45 [23680/50048]	Loss: 0.3668
Training Epoch: 45 [23808/50048]	Loss: 0.3076
Training Epoch: 45 [23936/50048]	Loss: 0.2900
Training Epoch: 45 [24064/50048]	Loss: 0.3100
Training Epoch: 45 [24192/50048]	Loss: 0.3702
Training Epoch: 45 [24320/50048]	Loss: 0.3325
Training Epoch: 45 [24448/50048]	Loss: 0.4246
Training Epoch: 45 [24576/50048]	Loss: 0.3865
Training Epoch: 45 [24704/50048]	Loss: 0.4342
Training Epoch: 45 [24832/50048]	Loss: 0.2523
Training Epoch: 45 [24960/50048]	Loss: 0.2908
Training Epoch: 45 [25088/50048]	Loss: 0.3545
Training Epoch: 45 [25216/50048]	Loss: 0.3396
Training Epoch: 45 [25344/50048]	Loss: 0.4180
Training Epoch: 45 [25472/50048]	Loss: 0.2338
Training Epoch: 45 [25600/50048]	Loss: 0.2375
Training Epoch: 45 [25728/50048]	Loss: 0.4005
Training Epoch: 45 [25856/50048]	Loss: 0.3958
Training Epoch: 45 [25984/50048]	Loss: 0.2741
Training Epoch: 45 [26112/50048]	Loss: 0.3759
Training Epoch: 45 [26240/50048]	Loss: 0.3846
Training Epoch: 45 [26368/50048]	Loss: 0.4463
Training Epoch: 45 [26496/50048]	Loss: 0.3584
Training Epoch: 45 [26624/50048]	Loss: 0.2940
Training Epoch: 45 [26752/50048]	Loss: 0.2801
Training Epoch: 45 [26880/50048]	Loss: 0.3883
Training Epoch: 45 [27008/50048]	Loss: 0.2923
Training Epoch: 45 [27136/50048]	Loss: 0.2231
Training Epoch: 45 [27264/50048]	Loss: 0.4363
Training Epoch: 45 [27392/50048]	Loss: 0.3828
Training Epoch: 45 [27520/50048]	Loss: 0.3295
Training Epoch: 45 [27648/50048]	Loss: 0.3175
Training Epoch: 45 [27776/50048]	Loss: 0.4383
Training Epoch: 45 [27904/50048]	Loss: 0.4527
Training Epoch: 45 [28032/50048]	Loss: 0.4957
Training Epoch: 45 [28160/50048]	Loss: 0.3349
Training Epoch: 45 [28288/50048]	Loss: 0.3602
Training Epoch: 45 [28416/50048]	Loss: 0.3806
Training Epoch: 45 [28544/50048]	Loss: 0.2349
Training Epoch: 45 [28672/50048]	Loss: 0.3082
Training Epoch: 45 [28800/50048]	Loss: 0.2617
Training Epoch: 45 [28928/50048]	Loss: 0.2816
Training Epoch: 45 [29056/50048]	Loss: 0.3101
Training Epoch: 45 [29184/50048]	Loss: 0.3433
Training Epoch: 45 [29312/50048]	Loss: 0.4547
Training Epoch: 45 [29440/50048]	Loss: 0.4206
Training Epoch: 45 [29568/50048]	Loss: 0.2648
Training Epoch: 45 [29696/50048]	Loss: 0.2977
Training Epoch: 45 [29824/50048]	Loss: 0.3815
Training Epoch: 45 [29952/50048]	Loss: 0.3650
Training Epoch: 45 [30080/50048]	Loss: 0.3218
Training Epoch: 45 [30208/50048]	Loss: 0.4302
Training Epoch: 45 [30336/50048]	Loss: 0.3342
Training Epoch: 45 [30464/50048]	Loss: 0.3699
Training Epoch: 45 [30592/50048]	Loss: 0.2824
Training Epoch: 45 [30720/50048]	Loss: 0.3024
Training Epoch: 45 [30848/50048]	Loss: 0.3583
Training Epoch: 45 [30976/50048]	Loss: 0.3172
Training Epoch: 45 [31104/50048]	Loss: 0.3545
Training Epoch: 45 [31232/50048]	Loss: 0.3862
Training Epoch: 45 [31360/50048]	Loss: 0.4351
Training Epoch: 45 [31488/50048]	Loss: 0.3276
Training Epoch: 45 [31616/50048]	Loss: 0.3498
Training Epoch: 45 [31744/50048]	Loss: 0.3700
Training Epoch: 45 [31872/50048]	Loss: 0.3285
Training Epoch: 45 [32000/50048]	Loss: 0.2749
Training Epoch: 45 [32128/50048]	Loss: 0.2666
Training Epoch: 45 [32256/50048]	Loss: 0.2723
Training Epoch: 45 [32384/50048]	Loss: 0.2536
Training Epoch: 45 [32512/50048]	Loss: 0.3302
Training Epoch: 45 [32640/50048]	Loss: 0.3181
Training Epoch: 45 [32768/50048]	Loss: 0.4974
Training Epoch: 45 [32896/50048]	Loss: 0.2964
Training Epoch: 45 [33024/50048]	Loss: 0.3043
Training Epoch: 45 [33152/50048]	Loss: 0.3275
Training Epoch: 45 [33280/50048]	Loss: 0.4634
Training Epoch: 45 [33408/50048]	Loss: 0.3953
Training Epoch: 45 [33536/50048]	Loss: 0.3466
Training Epoch: 45 [33664/50048]	Loss: 0.3906
Training Epoch: 45 [33792/50048]	Loss: 0.4289
Training Epoch: 45 [33920/50048]	Loss: 0.3265
Training Epoch: 45 [34048/50048]	Loss: 0.3219
Training Epoch: 45 [34176/50048]	Loss: 0.3178
Training Epoch: 45 [34304/50048]	Loss: 0.2701
Training Epoch: 45 [34432/50048]	Loss: 0.4239
Training Epoch: 45 [34560/50048]	Loss: 0.3397
Training Epoch: 45 [34688/50048]	Loss: 0.3131
Training Epoch: 45 [34816/50048]	Loss: 0.2394
Training Epoch: 45 [34944/50048]	Loss: 0.3831
Training Epoch: 45 [35072/50048]	Loss: 0.3499
Training Epoch: 45 [35200/50048]	Loss: 0.3536
Training Epoch: 45 [35328/50048]	Loss: 0.3555
Training Epoch: 45 [35456/50048]	Loss: 0.3399
Training Epoch: 45 [35584/50048]	Loss: 0.4732
Training Epoch: 45 [35712/50048]	Loss: 0.4029
Training Epoch: 45 [35840/50048]	Loss: 0.3332
Training Epoch: 45 [35968/50048]	Loss: 0.2888
Training Epoch: 45 [36096/50048]	Loss: 0.3695
Training Epoch: 45 [36224/50048]	Loss: 0.3030
Training Epoch: 45 [36352/50048]	Loss: 0.2782
Training Epoch: 45 [36480/50048]	Loss: 0.4207
Training Epoch: 45 [36608/50048]	Loss: 0.3102
Training Epoch: 45 [36736/50048]	Loss: 0.3362
Training Epoch: 45 [36864/50048]	Loss: 0.3561
Training Epoch: 45 [36992/50048]	Loss: 0.3918
Training Epoch: 45 [37120/50048]	Loss: 0.3251
Training Epoch: 45 [37248/50048]	Loss: 0.4116
Training Epoch: 45 [37376/50048]	Loss: 0.4191
Training Epoch: 45 [37504/50048]	Loss: 0.4045
Training Epoch: 45 [37632/50048]	Loss: 0.3602
Training Epoch: 45 [37760/50048]	Loss: 0.3991
Training Epoch: 45 [37888/50048]	Loss: 0.3849
Training Epoch: 45 [38016/50048]	Loss: 0.4020
Training Epoch: 45 [38144/50048]	Loss: 0.3380
Training Epoch: 45 [38272/50048]	Loss: 0.3318
Training Epoch: 45 [38400/50048]	Loss: 0.2865
Training Epoch: 45 [38528/50048]	Loss: 0.2655
Training Epoch: 45 [38656/50048]	Loss: 0.3987
Training Epoch: 45 [38784/50048]	Loss: 0.2571
Training Epoch: 45 [38912/50048]	Loss: 0.5285
Training Epoch: 45 [39040/50048]	Loss: 0.3898
Training Epoch: 45 [39168/50048]	Loss: 0.3208
Training Epoch: 45 [39296/50048]	Loss: 0.6031
Training Epoch: 45 [39424/50048]	Loss: 0.3164
Training Epoch: 45 [39552/50048]	Loss: 0.3921
Training Epoch: 45 [39680/50048]	Loss: 0.4451
Training Epoch: 45 [39808/50048]	Loss: 0.5051
Training Epoch: 45 [39936/50048]	Loss: 0.3769
Training Epoch: 45 [40064/50048]	Loss: 0.3359
Training Epoch: 45 [40192/50048]	Loss: 0.4480
Training Epoch: 45 [40320/50048]	Loss: 0.3564
Training Epoch: 45 [40448/50048]	Loss: 0.3100
Training Epoch: 45 [40576/50048]	Loss: 0.3592
Training Epoch: 45 [40704/50048]	Loss: 0.3394
Training Epoch: 45 [40832/50048]	Loss: 0.3206
Training Epoch: 45 [40960/50048]	Loss: 0.4189
Training Epoch: 45 [41088/50048]	Loss: 0.3317
Training Epoch: 45 [41216/50048]	Loss: 0.4983
Training Epoch: 45 [41344/50048]	Loss: 0.3616
Training Epoch: 45 [41472/50048]	Loss: 0.5009
Training Epoch: 45 [41600/50048]	Loss: 0.4300
Training Epoch: 45 [41728/50048]	Loss: 0.2653
Training Epoch: 45 [41856/50048]	Loss: 0.4559
Training Epoch: 45 [41984/50048]	Loss: 0.3363
Training Epoch: 45 [42112/50048]	Loss: 0.3510
Training Epoch: 45 [42240/50048]	Loss: 0.3402
Training Epoch: 45 [42368/50048]	Loss: 0.4344
Training Epoch: 45 [42496/50048]	Loss: 0.4753
Training Epoch: 45 [42624/50048]	Loss: 0.3646
Training Epoch: 45 [42752/50048]	Loss: 0.3493
Training Epoch: 45 [42880/50048]	Loss: 0.4630
Training Epoch: 45 [43008/50048]	Loss: 0.3672
Training Epoch: 45 [43136/50048]	Loss: 0.4027
Training Epoch: 45 [43264/50048]	Loss: 0.3624
Training Epoch: 45 [43392/50048]	Loss: 0.3772
Training Epoch: 45 [43520/50048]	Loss: 0.4740
Training Epoch: 45 [43648/50048]	Loss: 0.4461
Training Epoch: 45 [43776/50048]	Loss: 0.2912
Training Epoch: 45 [43904/50048]	Loss: 0.3556
Training Epoch: 45 [44032/50048]	Loss: 0.3143
Training Epoch: 45 [44160/50048]	Loss: 0.3930
Training Epoch: 45 [44288/50048]	Loss: 0.5546
Training Epoch: 45 [44416/50048]	Loss: 0.3849
Training Epoch: 45 [44544/50048]	Loss: 0.3085
Training Epoch: 45 [44672/50048]	Loss: 0.4908
Training Epoch: 45 [44800/50048]	Loss: 0.3776
Training Epoch: 45 [44928/50048]	Loss: 0.4227
Training Epoch: 45 [45056/50048]	Loss: 0.3844
Training Epoch: 45 [45184/50048]	Loss: 0.2821
Training Epoch: 45 [45312/50048]	Loss: 0.3941
Training Epoch: 45 [45440/50048]	Loss: 0.4415
Training Epoch: 45 [45568/50048]	Loss: 0.4313
Training Epoch: 45 [45696/50048]	Loss: 0.3048
2022-12-06 07:16:59,985 [ZeusDataLoader(train)] train epoch 46 done: time=86.36 energy=10491.37
2022-12-06 07:16:59,986 [ZeusDataLoader(eval)] Epoch 46 begin.
Training Epoch: 45 [45824/50048]	Loss: 0.2541
Training Epoch: 45 [45952/50048]	Loss: 0.3753
Training Epoch: 45 [46080/50048]	Loss: 0.5137
Training Epoch: 45 [46208/50048]	Loss: 0.3602
Training Epoch: 45 [46336/50048]	Loss: 0.2989
Training Epoch: 45 [46464/50048]	Loss: 0.4680
Training Epoch: 45 [46592/50048]	Loss: 0.3197
Training Epoch: 45 [46720/50048]	Loss: 0.3794
Training Epoch: 45 [46848/50048]	Loss: 0.4257
Training Epoch: 45 [46976/50048]	Loss: 0.2887
Training Epoch: 45 [47104/50048]	Loss: 0.5082
Training Epoch: 45 [47232/50048]	Loss: 0.2987
Training Epoch: 45 [47360/50048]	Loss: 0.2709
Training Epoch: 45 [47488/50048]	Loss: 0.4206
Training Epoch: 45 [47616/50048]	Loss: 0.3211
Training Epoch: 45 [47744/50048]	Loss: 0.3901
Training Epoch: 45 [47872/50048]	Loss: 0.3419
Training Epoch: 45 [48000/50048]	Loss: 0.3614
Training Epoch: 45 [48128/50048]	Loss: 0.3520
Training Epoch: 45 [48256/50048]	Loss: 0.4524
Training Epoch: 45 [48384/50048]	Loss: 0.3729
Training Epoch: 45 [48512/50048]	Loss: 0.2883
Training Epoch: 45 [48640/50048]	Loss: 0.5376
Training Epoch: 45 [48768/50048]	Loss: 0.5587
Training Epoch: 45 [48896/50048]	Loss: 0.3150
Training Epoch: 45 [49024/50048]	Loss: 0.4146
Training Epoch: 45 [49152/50048]	Loss: 0.4211
Training Epoch: 45 [49280/50048]	Loss: 0.5061
Training Epoch: 45 [49408/50048]	Loss: 0.3106
Training Epoch: 45 [49536/50048]	Loss: 0.4385
Training Epoch: 45 [49664/50048]	Loss: 0.4781
Training Epoch: 45 [49792/50048]	Loss: 0.3298
Training Epoch: 45 [49920/50048]	Loss: 0.4271
Training Epoch: 45 [50048/50048]	Loss: 0.3848
2022-12-06 12:17:03.696 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:17:03,752 [ZeusDataLoader(eval)] eval epoch 46 done: time=3.76 energy=452.94
2022-12-06 07:17:03,752 [ZeusDataLoader(train)] Up to epoch 46: time=4149.13, energy=503646.22, cost=614871.64
2022-12-06 07:17:03,752 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:17:03,752 [ZeusDataLoader(train)] Expected next epoch: time=4238.92, energy=514444.23, cost=628128.03
2022-12-06 07:17:03,753 [ZeusDataLoader(train)] Epoch 47 begin.
Validation Epoch: 45, Average loss: 0.0143, Accuracy: 0.6241
2022-12-06 07:17:03,942 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:17:03,943 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:17:03.945 [ZeusMonitor] Monitor started.
2022-12-06 12:17:03.945 [ZeusMonitor] Running indefinitely. 2022-12-06 12:17:03.945 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:17:03.945 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e47+gpu0.power.log
Training Epoch: 46 [128/50048]	Loss: 0.2308
Training Epoch: 46 [256/50048]	Loss: 0.2530
Training Epoch: 46 [384/50048]	Loss: 0.3359
Training Epoch: 46 [512/50048]	Loss: 0.2510
Training Epoch: 46 [640/50048]	Loss: 0.2457
Training Epoch: 46 [768/50048]	Loss: 0.3340
Training Epoch: 46 [896/50048]	Loss: 0.4118
Training Epoch: 46 [1024/50048]	Loss: 0.3638
Training Epoch: 46 [1152/50048]	Loss: 0.2463
Training Epoch: 46 [1280/50048]	Loss: 0.3949
Training Epoch: 46 [1408/50048]	Loss: 0.2741
Training Epoch: 46 [1536/50048]	Loss: 0.3039
Training Epoch: 46 [1664/50048]	Loss: 0.2802
Training Epoch: 46 [1792/50048]	Loss: 0.3360
Training Epoch: 46 [1920/50048]	Loss: 0.2781
Training Epoch: 46 [2048/50048]	Loss: 0.2669
Training Epoch: 46 [2176/50048]	Loss: 0.3926
Training Epoch: 46 [2304/50048]	Loss: 0.2574
Training Epoch: 46 [2432/50048]	Loss: 0.3256
Training Epoch: 46 [2560/50048]	Loss: 0.2347
Training Epoch: 46 [2688/50048]	Loss: 0.2167
Training Epoch: 46 [2816/50048]	Loss: 0.1945
Training Epoch: 46 [2944/50048]	Loss: 0.2477
Training Epoch: 46 [3072/50048]	Loss: 0.3083
Training Epoch: 46 [3200/50048]	Loss: 0.2443
Training Epoch: 46 [3328/50048]	Loss: 0.2142
Training Epoch: 46 [3456/50048]	Loss: 0.3665
Training Epoch: 46 [3584/50048]	Loss: 0.3548
Training Epoch: 46 [3712/50048]	Loss: 0.3528
Training Epoch: 46 [3840/50048]	Loss: 0.4369
Training Epoch: 46 [3968/50048]	Loss: 0.2486
Training Epoch: 46 [4096/50048]	Loss: 0.2526
Training Epoch: 46 [4224/50048]	Loss: 0.3328
Training Epoch: 46 [4352/50048]	Loss: 0.2711
Training Epoch: 46 [4480/50048]	Loss: 0.3983
Training Epoch: 46 [4608/50048]	Loss: 0.2528
Training Epoch: 46 [4736/50048]	Loss: 0.2518
Training Epoch: 46 [4864/50048]	Loss: 0.3678
Training Epoch: 46 [4992/50048]	Loss: 0.2487
Training Epoch: 46 [5120/50048]	Loss: 0.2666
Training Epoch: 46 [5248/50048]	Loss: 0.4166
Training Epoch: 46 [5376/50048]	Loss: 0.2407
Training Epoch: 46 [5504/50048]	Loss: 0.3258
Training Epoch: 46 [5632/50048]	Loss: 0.2622
Training Epoch: 46 [5760/50048]	Loss: 0.4645
Training Epoch: 46 [5888/50048]	Loss: 0.2812
Training Epoch: 46 [6016/50048]	Loss: 0.2219
Training Epoch: 46 [6144/50048]	Loss: 0.4240
Training Epoch: 46 [6272/50048]	Loss: 0.3668
Training Epoch: 46 [6400/50048]	Loss: 0.3697
Training Epoch: 46 [6528/50048]	Loss: 0.2571
Training Epoch: 46 [6656/50048]	Loss: 0.2398
Training Epoch: 46 [6784/50048]	Loss: 0.3335
Training Epoch: 46 [6912/50048]	Loss: 0.3206
Training Epoch: 46 [7040/50048]	Loss: 0.2415
Training Epoch: 46 [7168/50048]	Loss: 0.2320
Training Epoch: 46 [7296/50048]	Loss: 0.2336
Training Epoch: 46 [7424/50048]	Loss: 0.2320
Training Epoch: 46 [7552/50048]	Loss: 0.2900
Training Epoch: 46 [7680/50048]	Loss: 0.2861
Training Epoch: 46 [7808/50048]	Loss: 0.2197
Training Epoch: 46 [7936/50048]	Loss: 0.2613
Training Epoch: 46 [8064/50048]	Loss: 0.3195
Training Epoch: 46 [8192/50048]	Loss: 0.2806
Training Epoch: 46 [8320/50048]	Loss: 0.3251
Training Epoch: 46 [8448/50048]	Loss: 0.2154
Training Epoch: 46 [8576/50048]	Loss: 0.3947
Training Epoch: 46 [8704/50048]	Loss: 0.3372
Training Epoch: 46 [8832/50048]	Loss: 0.2898
Training Epoch: 46 [8960/50048]	Loss: 0.2481
Training Epoch: 46 [9088/50048]	Loss: 0.1951
Training Epoch: 46 [9216/50048]	Loss: 0.2081
Training Epoch: 46 [9344/50048]	Loss: 0.3326
Training Epoch: 46 [9472/50048]	Loss: 0.2156
Training Epoch: 46 [9600/50048]	Loss: 0.2718
Training Epoch: 46 [9728/50048]	Loss: 0.3492
Training Epoch: 46 [9856/50048]	Loss: 0.3302
Training Epoch: 46 [9984/50048]	Loss: 0.4096
Training Epoch: 46 [10112/50048]	Loss: 0.3026
Training Epoch: 46 [10240/50048]	Loss: 0.3083
Training Epoch: 46 [10368/50048]	Loss: 0.2498
Training Epoch: 46 [10496/50048]	Loss: 0.3768
Training Epoch: 46 [10624/50048]	Loss: 0.3160
Training Epoch: 46 [10752/50048]	Loss: 0.4630
Training Epoch: 46 [10880/50048]	Loss: 0.2753
Training Epoch: 46 [11008/50048]	Loss: 0.2687
Training Epoch: 46 [11136/50048]	Loss: 0.3883
Training Epoch: 46 [11264/50048]	Loss: 0.3327
Training Epoch: 46 [11392/50048]	Loss: 0.3631
Training Epoch: 46 [11520/50048]	Loss: 0.4504
Training Epoch: 46 [11648/50048]	Loss: 0.2513
Training Epoch: 46 [11776/50048]	Loss: 0.2113
Training Epoch: 46 [11904/50048]	Loss: 0.3251
Training Epoch: 46 [12032/50048]	Loss: 0.4130
Training Epoch: 46 [12160/50048]	Loss: 0.3247
Training Epoch: 46 [12288/50048]	Loss: 0.2821
Training Epoch: 46 [12416/50048]	Loss: 0.1997
Training Epoch: 46 [12544/50048]	Loss: 0.2093
Training Epoch: 46 [12672/50048]	Loss: 0.3471
Training Epoch: 46 [12800/50048]	Loss: 0.3054
Training Epoch: 46 [12928/50048]	Loss: 0.2790
Training Epoch: 46 [13056/50048]	Loss: 0.3378
Training Epoch: 46 [13184/50048]	Loss: 0.3230
Training Epoch: 46 [13312/50048]	Loss: 0.2753
Training Epoch: 46 [13440/50048]	Loss: 0.3065
Training Epoch: 46 [13568/50048]	Loss: 0.2923
Training Epoch: 46 [13696/50048]	Loss: 0.2291
Training Epoch: 46 [13824/50048]	Loss: 0.2235
Training Epoch: 46 [13952/50048]	Loss: 0.3046
Training Epoch: 46 [14080/50048]	Loss: 0.2548
Training Epoch: 46 [14208/50048]	Loss: 0.4272
Training Epoch: 46 [14336/50048]	Loss: 0.2944
Training Epoch: 46 [14464/50048]	Loss: 0.3275
Training Epoch: 46 [14592/50048]	Loss: 0.2770
Training Epoch: 46 [14720/50048]	Loss: 0.3455
Training Epoch: 46 [14848/50048]	Loss: 0.2870
Training Epoch: 46 [14976/50048]	Loss: 0.2462
Training Epoch: 46 [15104/50048]	Loss: 0.3044
Training Epoch: 46 [15232/50048]	Loss: 0.2744
Training Epoch: 46 [15360/50048]	Loss: 0.2842
Training Epoch: 46 [15488/50048]	Loss: 0.3017
Training Epoch: 46 [15616/50048]	Loss: 0.3513
Training Epoch: 46 [15744/50048]	Loss: 0.2713
Training Epoch: 46 [15872/50048]	Loss: 0.3774
Training Epoch: 46 [16000/50048]	Loss: 0.2217
Training Epoch: 46 [16128/50048]	Loss: 0.2437
Training Epoch: 46 [16256/50048]	Loss: 0.3612
Training Epoch: 46 [16384/50048]	Loss: 0.3166
Training Epoch: 46 [16512/50048]	Loss: 0.2573
Training Epoch: 46 [16640/50048]	Loss: 0.3940
Training Epoch: 46 [16768/50048]	Loss: 0.2705
Training Epoch: 46 [16896/50048]	Loss: 0.2295
Training Epoch: 46 [17024/50048]	Loss: 0.2309
Training Epoch: 46 [17152/50048]	Loss: 0.3742
Training Epoch: 46 [17280/50048]	Loss: 0.3701
Training Epoch: 46 [17408/50048]	Loss: 0.3654
Training Epoch: 46 [17536/50048]	Loss: 0.2983
Training Epoch: 46 [17664/50048]	Loss: 0.2133
Training Epoch: 46 [17792/50048]	Loss: 0.2470
Training Epoch: 46 [17920/50048]	Loss: 0.2675
Training Epoch: 46 [18048/50048]	Loss: 0.4364
Training Epoch: 46 [18176/50048]	Loss: 0.3067
Training Epoch: 46 [18304/50048]	Loss: 0.2116
Training Epoch: 46 [18432/50048]	Loss: 0.4071
Training Epoch: 46 [18560/50048]	Loss: 0.3211
Training Epoch: 46 [18688/50048]	Loss: 0.4400
Training Epoch: 46 [18816/50048]	Loss: 0.2998
Training Epoch: 46 [18944/50048]	Loss: 0.3989
Training Epoch: 46 [19072/50048]	Loss: 0.2780
Training Epoch: 46 [19200/50048]	Loss: 0.3354
Training Epoch: 46 [19328/50048]	Loss: 0.3298
Training Epoch: 46 [19456/50048]	Loss: 0.4478
Training Epoch: 46 [19584/50048]	Loss: 0.3027
Training Epoch: 46 [19712/50048]	Loss: 0.2310
Training Epoch: 46 [19840/50048]	Loss: 0.2931
Training Epoch: 46 [19968/50048]	Loss: 0.2865
Training Epoch: 46 [20096/50048]	Loss: 0.3554
Training Epoch: 46 [20224/50048]	Loss: 0.4331
Training Epoch: 46 [20352/50048]	Loss: 0.2904
Training Epoch: 46 [20480/50048]	Loss: 0.3192
Training Epoch: 46 [20608/50048]	Loss: 0.4619
Training Epoch: 46 [20736/50048]	Loss: 0.3117
Training Epoch: 46 [20864/50048]	Loss: 0.4250
Training Epoch: 46 [20992/50048]	Loss: 0.2210
Training Epoch: 46 [21120/50048]	Loss: 0.2965
Training Epoch: 46 [21248/50048]	Loss: 0.2828
Training Epoch: 46 [21376/50048]	Loss: 0.2966
Training Epoch: 46 [21504/50048]	Loss: 0.4090
Training Epoch: 46 [21632/50048]	Loss: 0.2348
Training Epoch: 46 [21760/50048]	Loss: 0.3259
Training Epoch: 46 [21888/50048]	Loss: 0.2013
Training Epoch: 46 [22016/50048]	Loss: 0.3957
Training Epoch: 46 [22144/50048]	Loss: 0.3518
Training Epoch: 46 [22272/50048]	Loss: 0.3659
Training Epoch: 46 [22400/50048]	Loss: 0.2862
Training Epoch: 46 [22528/50048]	Loss: 0.3986
Training Epoch: 46 [22656/50048]	Loss: 0.5242
Training Epoch: 46 [22784/50048]	Loss: 0.2815
Training Epoch: 46 [22912/50048]	Loss: 0.4176
Training Epoch: 46 [23040/50048]	Loss: 0.4187
Training Epoch: 46 [23168/50048]	Loss: 0.2749
Training Epoch: 46 [23296/50048]	Loss: 0.2696
Training Epoch: 46 [23424/50048]	Loss: 0.4220
Training Epoch: 46 [23552/50048]	Loss: 0.2542
Training Epoch: 46 [23680/50048]	Loss: 0.2272
Training Epoch: 46 [23808/50048]	Loss: 0.3509
Training Epoch: 46 [23936/50048]	Loss: 0.3236
Training Epoch: 46 [24064/50048]	Loss: 0.3375
Training Epoch: 46 [24192/50048]	Loss: 0.4056
Training Epoch: 46 [24320/50048]	Loss: 0.4054
Training Epoch: 46 [24448/50048]	Loss: 0.2903
Training Epoch: 46 [24576/50048]	Loss: 0.3071
Training Epoch: 46 [24704/50048]	Loss: 0.5139
Training Epoch: 46 [24832/50048]	Loss: 0.3201
Training Epoch: 46 [24960/50048]	Loss: 0.4142
Training Epoch: 46 [25088/50048]	Loss: 0.3177
Training Epoch: 46 [25216/50048]	Loss: 0.2899
Training Epoch: 46 [25344/50048]	Loss: 0.2741
Training Epoch: 46 [25472/50048]	Loss: 0.2621
Training Epoch: 46 [25600/50048]	Loss: 0.2905
Training Epoch: 46 [25728/50048]	Loss: 0.2929
Training Epoch: 46 [25856/50048]	Loss: 0.2791
Training Epoch: 46 [25984/50048]	Loss: 0.3043
Training Epoch: 46 [26112/50048]	Loss: 0.3736
Training Epoch: 46 [26240/50048]	Loss: 0.2676
Training Epoch: 46 [26368/50048]	Loss: 0.3290
Training Epoch: 46 [26496/50048]	Loss: 0.3861
Training Epoch: 46 [26624/50048]	Loss: 0.3917
Training Epoch: 46 [26752/50048]	Loss: 0.4117
Training Epoch: 46 [26880/50048]	Loss: 0.3889
Training Epoch: 46 [27008/50048]	Loss: 0.3782
Training Epoch: 46 [27136/50048]	Loss: 0.3091
Training Epoch: 46 [27264/50048]	Loss: 0.4545
Training Epoch: 46 [27392/50048]	Loss: 0.3036
Training Epoch: 46 [27520/50048]	Loss: 0.3012
Training Epoch: 46 [27648/50048]	Loss: 0.3563
Training Epoch: 46 [27776/50048]	Loss: 0.2614
Training Epoch: 46 [27904/50048]	Loss: 0.3046
Training Epoch: 46 [28032/50048]	Loss: 0.5545
Training Epoch: 46 [28160/50048]	Loss: 0.3381
Training Epoch: 46 [28288/50048]	Loss: 0.3241
Training Epoch: 46 [28416/50048]	Loss: 0.3120
Training Epoch: 46 [28544/50048]	Loss: 0.2300
Training Epoch: 46 [28672/50048]	Loss: 0.2558
Training Epoch: 46 [28800/50048]	Loss: 0.3860
Training Epoch: 46 [28928/50048]	Loss: 0.3122
Training Epoch: 46 [29056/50048]	Loss: 0.3558
Training Epoch: 46 [29184/50048]	Loss: 0.3209
Training Epoch: 46 [29312/50048]	Loss: 0.3668
Training Epoch: 46 [29440/50048]	Loss: 0.3072
Training Epoch: 46 [29568/50048]	Loss: 0.2985
Training Epoch: 46 [29696/50048]	Loss: 0.4602
Training Epoch: 46 [29824/50048]	Loss: 0.3895
Training Epoch: 46 [29952/50048]	Loss: 0.4229
Training Epoch: 46 [30080/50048]	Loss: 0.3234
Training Epoch: 46 [30208/50048]	Loss: 0.4526
Training Epoch: 46 [30336/50048]	Loss: 0.3242
Training Epoch: 46 [30464/50048]	Loss: 0.3560
Training Epoch: 46 [30592/50048]	Loss: 0.5004
Training Epoch: 46 [30720/50048]	Loss: 0.3338
Training Epoch: 46 [30848/50048]	Loss: 0.4797
Training Epoch: 46 [30976/50048]	Loss: 0.3276
Training Epoch: 46 [31104/50048]	Loss: 0.2508
Training Epoch: 46 [31232/50048]	Loss: 0.2561
Training Epoch: 46 [31360/50048]	Loss: 0.3237
Training Epoch: 46 [31488/50048]	Loss: 0.4008
Training Epoch: 46 [31616/50048]	Loss: 0.3503
Training Epoch: 46 [31744/50048]	Loss: 0.2497
Training Epoch: 46 [31872/50048]	Loss: 0.4068
Training Epoch: 46 [32000/50048]	Loss: 0.2886
Training Epoch: 46 [32128/50048]	Loss: 0.2640
Training Epoch: 46 [32256/50048]	Loss: 0.3130
Training Epoch: 46 [32384/50048]	Loss: 0.2829
Training Epoch: 46 [32512/50048]	Loss: 0.3652
Training Epoch: 46 [32640/50048]	Loss: 0.4018
Training Epoch: 46 [32768/50048]	Loss: 0.4102
Training Epoch: 46 [32896/50048]	Loss: 0.2695
Training Epoch: 46 [33024/50048]	Loss: 0.3547
Training Epoch: 46 [33152/50048]	Loss: 0.3409
Training Epoch: 46 [33280/50048]	Loss: 0.2879
Training Epoch: 46 [33408/50048]	Loss: 0.5423
Training Epoch: 46 [33536/50048]	Loss: 0.4466
Training Epoch: 46 [33664/50048]	Loss: 0.4871
Training Epoch: 46 [33792/50048]	Loss: 0.3148
Training Epoch: 46 [33920/50048]	Loss: 0.3576
Training Epoch: 46 [34048/50048]	Loss: 0.4012
Training Epoch: 46 [34176/50048]	Loss: 0.4378
Training Epoch: 46 [34304/50048]	Loss: 0.4096
Training Epoch: 46 [34432/50048]	Loss: 0.2790
Training Epoch: 46 [34560/50048]	Loss: 0.3901
Training Epoch: 46 [34688/50048]	Loss: 0.3383
Training Epoch: 46 [34816/50048]	Loss: 0.3810
Training Epoch: 46 [34944/50048]	Loss: 0.3016
Training Epoch: 46 [35072/50048]	Loss: 0.4354
Training Epoch: 46 [35200/50048]	Loss: 0.4241
Training Epoch: 46 [35328/50048]	Loss: 0.2890
Training Epoch: 46 [35456/50048]	Loss: 0.4027
Training Epoch: 46 [35584/50048]	Loss: 0.4012
Training Epoch: 46 [35712/50048]	Loss: 0.3412
Training Epoch: 46 [35840/50048]	Loss: 0.3459
Training Epoch: 46 [35968/50048]	Loss: 0.3520
Training Epoch: 46 [36096/50048]	Loss: 0.2716
Training Epoch: 46 [36224/50048]	Loss: 0.3589
Training Epoch: 46 [36352/50048]	Loss: 0.4238
Training Epoch: 46 [36480/50048]	Loss: 0.4085
Training Epoch: 46 [36608/50048]	Loss: 0.2750
Training Epoch: 46 [36736/50048]	Loss: 0.2570
Training Epoch: 46 [36864/50048]	Loss: 0.4469
Training Epoch: 46 [36992/50048]	Loss: 0.3613
Training Epoch: 46 [37120/50048]	Loss: 0.2719
Training Epoch: 46 [37248/50048]	Loss: 0.3492
Training Epoch: 46 [37376/50048]	Loss: 0.3976
Training Epoch: 46 [37504/50048]	Loss: 0.3342
Training Epoch: 46 [37632/50048]	Loss: 0.2268
Training Epoch: 46 [37760/50048]	Loss: 0.3113
Training Epoch: 46 [37888/50048]	Loss: 0.3499
Training Epoch: 46 [38016/50048]	Loss: 0.4715
Training Epoch: 46 [38144/50048]	Loss: 0.5157
Training Epoch: 46 [38272/50048]	Loss: 0.4057
Training Epoch: 46 [38400/50048]	Loss: 0.3372
Training Epoch: 46 [38528/50048]	Loss: 0.3470
Training Epoch: 46 [38656/50048]	Loss: 0.3191
Training Epoch: 46 [38784/50048]	Loss: 0.4283
Training Epoch: 46 [38912/50048]	Loss: 0.3875
Training Epoch: 46 [39040/50048]	Loss: 0.4010
Training Epoch: 46 [39168/50048]	Loss: 0.3608
Training Epoch: 46 [39296/50048]	Loss: 0.3978
Training Epoch: 46 [39424/50048]	Loss: 0.4456
Training Epoch: 46 [39552/50048]	Loss: 0.2879
Training Epoch: 46 [39680/50048]	Loss: 0.3453
Training Epoch: 46 [39808/50048]	Loss: 0.4287
Training Epoch: 46 [39936/50048]	Loss: 0.2531
Training Epoch: 46 [40064/50048]	Loss: 0.4725
Training Epoch: 46 [40192/50048]	Loss: 0.3060
Training Epoch: 46 [40320/50048]	Loss: 0.5075
Training Epoch: 46 [40448/50048]	Loss: 0.2770
Training Epoch: 46 [40576/50048]	Loss: 0.3762
Training Epoch: 46 [40704/50048]	Loss: 0.3125
Training Epoch: 46 [40832/50048]	Loss: 0.4997
Training Epoch: 46 [40960/50048]	Loss: 0.2374
Training Epoch: 46 [41088/50048]	Loss: 0.4389
Training Epoch: 46 [41216/50048]	Loss: 0.3551
Training Epoch: 46 [41344/50048]	Loss: 0.4118
Training Epoch: 46 [41472/50048]	Loss: 0.3033
Training Epoch: 46 [41600/50048]	Loss: 0.2779
Training Epoch: 46 [41728/50048]	Loss: 0.4524
Training Epoch: 46 [41856/50048]	Loss: 0.4184
Training Epoch: 46 [41984/50048]	Loss: 0.2914
Training Epoch: 46 [42112/50048]	Loss: 0.3191
Training Epoch: 46 [42240/50048]	Loss: 0.3234
Training Epoch: 46 [42368/50048]	Loss: 0.4514
Training Epoch: 46 [42496/50048]	Loss: 0.3141
Training Epoch: 46 [42624/50048]	Loss: 0.3119
Training Epoch: 46 [42752/50048]	Loss: 0.3679
Training Epoch: 46 [42880/50048]	Loss: 0.3628
Training Epoch: 46 [43008/50048]	Loss: 0.2996
Training Epoch: 46 [43136/50048]	Loss: 0.3179
Training Epoch: 46 [43264/50048]	Loss: 0.3923
Training Epoch: 46 [43392/50048]	Loss: 0.3055
Training Epoch: 46 [43520/50048]	Loss: 0.3092
Training Epoch: 46 [43648/50048]	Loss: 0.4405
Training Epoch: 46 [43776/50048]	Loss: 0.3682
Training Epoch: 46 [43904/50048]	Loss: 0.3829
Training Epoch: 46 [44032/50048]	Loss: 0.3103
Training Epoch: 46 [44160/50048]	Loss: 0.3225
Training Epoch: 46 [44288/50048]	Loss: 0.2998
Training Epoch: 46 [44416/50048]	Loss: 0.5081
Training Epoch: 46 [44544/50048]	Loss: 0.3451
Training Epoch: 46 [44672/50048]	Loss: 0.4506
Training Epoch: 46 [44800/50048]	Loss: 0.3282
Training Epoch: 46 [44928/50048]	Loss: 0.3285
Training Epoch: 46 [45056/50048]	Loss: 0.4992
Training Epoch: 46 [45184/50048]	Loss: 0.4029
Training Epoch: 46 [45312/50048]	Loss: 0.4851
Training Epoch: 46 [45440/50048]	Loss: 0.4450
Training Epoch: 46 [45568/50048]	Loss: 0.3954
Training Epoch: 46 [45696/50048]	Loss: 0.3650
2022-12-06 07:18:30,253 [ZeusDataLoader(train)] train epoch 47 done: time=86.49 energy=10499.97
2022-12-06 07:18:30,254 [ZeusDataLoader(eval)] Epoch 47 begin.
Training Epoch: 46 [45824/50048]	Loss: 0.2803
Training Epoch: 46 [45952/50048]	Loss: 0.5637
Training Epoch: 46 [46080/50048]	Loss: 0.3833
Training Epoch: 46 [46208/50048]	Loss: 0.4106
Training Epoch: 46 [46336/50048]	Loss: 0.3570
Training Epoch: 46 [46464/50048]	Loss: 0.3120
Training Epoch: 46 [46592/50048]	Loss: 0.3281
Training Epoch: 46 [46720/50048]	Loss: 0.3356
Training Epoch: 46 [46848/50048]	Loss: 0.3630
Training Epoch: 46 [46976/50048]	Loss: 0.3370
Training Epoch: 46 [47104/50048]	Loss: 0.4639
Training Epoch: 46 [47232/50048]	Loss: 0.2911
Training Epoch: 46 [47360/50048]	Loss: 0.4003
Training Epoch: 46 [47488/50048]	Loss: 0.3502
Training Epoch: 46 [47616/50048]	Loss: 0.4696
Training Epoch: 46 [47744/50048]	Loss: 0.4440
Training Epoch: 46 [47872/50048]	Loss: 0.2050
Training Epoch: 46 [48000/50048]	Loss: 0.5236
Training Epoch: 46 [48128/50048]	Loss: 0.4576
Training Epoch: 46 [48256/50048]	Loss: 0.4601
Training Epoch: 46 [48384/50048]	Loss: 0.4077
Training Epoch: 46 [48512/50048]	Loss: 0.4103
Training Epoch: 46 [48640/50048]	Loss: 0.3627
Training Epoch: 46 [48768/50048]	Loss: 0.5237
Training Epoch: 46 [48896/50048]	Loss: 0.2420
Training Epoch: 46 [49024/50048]	Loss: 0.3663
Training Epoch: 46 [49152/50048]	Loss: 0.4630
Training Epoch: 46 [49280/50048]	Loss: 0.3952
Training Epoch: 46 [49408/50048]	Loss: 0.4514
Training Epoch: 46 [49536/50048]	Loss: 0.4782
Training Epoch: 46 [49664/50048]	Loss: 0.4265
Training Epoch: 46 [49792/50048]	Loss: 0.2887
Training Epoch: 46 [49920/50048]	Loss: 0.3689
Training Epoch: 46 [50048/50048]	Loss: 0.3999
2022-12-06 12:18:33.949 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:18:33,965 [ZeusDataLoader(eval)] eval epoch 47 done: time=3.70 energy=452.77
2022-12-06 07:18:33,965 [ZeusDataLoader(train)] Up to epoch 47: time=4239.32, energy=514598.96, cost=628239.70
2022-12-06 07:18:33,965 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:18:33,965 [ZeusDataLoader(train)] Expected next epoch: time=4329.12, energy=525396.98, cost=641496.09
2022-12-06 07:18:33,966 [ZeusDataLoader(train)] Epoch 48 begin.
Validation Epoch: 46, Average loss: 0.0147, Accuracy: 0.6237
2022-12-06 07:18:34,158 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:18:34,159 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:18:34.169 [ZeusMonitor] Monitor started.
2022-12-06 12:18:34.169 [ZeusMonitor] Running indefinitely. 2022-12-06 12:18:34.169 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:18:34.169 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e48+gpu0.power.log
Training Epoch: 47 [128/50048]	Loss: 0.2811
Training Epoch: 47 [256/50048]	Loss: 0.2540
Training Epoch: 47 [384/50048]	Loss: 0.1875
Training Epoch: 47 [512/50048]	Loss: 0.3980
Training Epoch: 47 [640/50048]	Loss: 0.3080
Training Epoch: 47 [768/50048]	Loss: 0.2132
Training Epoch: 47 [896/50048]	Loss: 0.3036
Training Epoch: 47 [1024/50048]	Loss: 0.2548
Training Epoch: 47 [1152/50048]	Loss: 0.2852
Training Epoch: 47 [1280/50048]	Loss: 0.2828
Training Epoch: 47 [1408/50048]	Loss: 0.2019
Training Epoch: 47 [1536/50048]	Loss: 0.2756
Training Epoch: 47 [1664/50048]	Loss: 0.2879
Training Epoch: 47 [1792/50048]	Loss: 0.2231
Training Epoch: 47 [1920/50048]	Loss: 0.2326
Training Epoch: 47 [2048/50048]	Loss: 0.2454
Training Epoch: 47 [2176/50048]	Loss: 0.2901
Training Epoch: 47 [2304/50048]	Loss: 0.3551
Training Epoch: 47 [2432/50048]	Loss: 0.2691
Training Epoch: 47 [2560/50048]	Loss: 0.4919
Training Epoch: 47 [2688/50048]	Loss: 0.3158
Training Epoch: 47 [2816/50048]	Loss: 0.3511
Training Epoch: 47 [2944/50048]	Loss: 0.2722
Training Epoch: 47 [3072/50048]	Loss: 0.3441
Training Epoch: 47 [3200/50048]	Loss: 0.2585
Training Epoch: 47 [3328/50048]	Loss: 0.2395
Training Epoch: 47 [3456/50048]	Loss: 0.2658
Training Epoch: 47 [3584/50048]	Loss: 0.3260
Training Epoch: 47 [3712/50048]	Loss: 0.2895
Training Epoch: 47 [3840/50048]	Loss: 0.3595
Training Epoch: 47 [3968/50048]	Loss: 0.2064
Training Epoch: 47 [4096/50048]	Loss: 0.2898
Training Epoch: 47 [4224/50048]	Loss: 0.2908
Training Epoch: 47 [4352/50048]	Loss: 0.2333
Training Epoch: 47 [4480/50048]	Loss: 0.2224
Training Epoch: 47 [4608/50048]	Loss: 0.2022
Training Epoch: 47 [4736/50048]	Loss: 0.2530
Training Epoch: 47 [4864/50048]	Loss: 0.3371
Training Epoch: 47 [4992/50048]	Loss: 0.3297
Training Epoch: 47 [5120/50048]	Loss: 0.3700
Training Epoch: 47 [5248/50048]	Loss: 0.2825
Training Epoch: 47 [5376/50048]	Loss: 0.3308
Training Epoch: 47 [5504/50048]	Loss: 0.4156
Training Epoch: 47 [5632/50048]	Loss: 0.3423
Training Epoch: 47 [5760/50048]	Loss: 0.2265
Training Epoch: 47 [5888/50048]	Loss: 0.3188
Training Epoch: 47 [6016/50048]	Loss: 0.3036
Training Epoch: 47 [6144/50048]	Loss: 0.2499
Training Epoch: 47 [6272/50048]	Loss: 0.4127
Training Epoch: 47 [6400/50048]	Loss: 0.1775
Training Epoch: 47 [6528/50048]	Loss: 0.2794
Training Epoch: 47 [6656/50048]	Loss: 0.2602
Training Epoch: 47 [6784/50048]	Loss: 0.3823
Training Epoch: 47 [6912/50048]	Loss: 0.2165
Training Epoch: 47 [7040/50048]	Loss: 0.2670
Training Epoch: 47 [7168/50048]	Loss: 0.2732
Training Epoch: 47 [7296/50048]	Loss: 0.2895
Training Epoch: 47 [7424/50048]	Loss: 0.2740
Training Epoch: 47 [7552/50048]	Loss: 0.2272
Training Epoch: 47 [7680/50048]	Loss: 0.4295
Training Epoch: 47 [7808/50048]	Loss: 0.3161
Training Epoch: 47 [7936/50048]	Loss: 0.2938
Training Epoch: 47 [8064/50048]	Loss: 0.3523
Training Epoch: 47 [8192/50048]	Loss: 0.3076
Training Epoch: 47 [8320/50048]	Loss: 0.2355
Training Epoch: 47 [8448/50048]	Loss: 0.2567
Training Epoch: 47 [8576/50048]	Loss: 0.2870
Training Epoch: 47 [8704/50048]	Loss: 0.4004
Training Epoch: 47 [8832/50048]	Loss: 0.2414
Training Epoch: 47 [8960/50048]	Loss: 0.2811
Training Epoch: 47 [9088/50048]	Loss: 0.1924
Training Epoch: 47 [9216/50048]	Loss: 0.3187
Training Epoch: 47 [9344/50048]	Loss: 0.3201
Training Epoch: 47 [9472/50048]	Loss: 0.2378
Training Epoch: 47 [9600/50048]	Loss: 0.2817
Training Epoch: 47 [9728/50048]	Loss: 0.2775
Training Epoch: 47 [9856/50048]	Loss: 0.3372
Training Epoch: 47 [9984/50048]	Loss: 0.2125
Training Epoch: 47 [10112/50048]	Loss: 0.2353
Training Epoch: 47 [10240/50048]	Loss: 0.2109
Training Epoch: 47 [10368/50048]	Loss: 0.3768
Training Epoch: 47 [10496/50048]	Loss: 0.2629
Training Epoch: 47 [10624/50048]	Loss: 0.2794
Training Epoch: 47 [10752/50048]	Loss: 0.3298
Training Epoch: 47 [10880/50048]	Loss: 0.2984
Training Epoch: 47 [11008/50048]	Loss: 0.3997
Training Epoch: 47 [11136/50048]	Loss: 0.4044
Training Epoch: 47 [11264/50048]	Loss: 0.3335
Training Epoch: 47 [11392/50048]	Loss: 0.4058
Training Epoch: 47 [11520/50048]	Loss: 0.2378
Training Epoch: 47 [11648/50048]	Loss: 0.3533
Training Epoch: 47 [11776/50048]	Loss: 0.3326
Training Epoch: 47 [11904/50048]	Loss: 0.2799
Training Epoch: 47 [12032/50048]	Loss: 0.3566
Training Epoch: 47 [12160/50048]	Loss: 0.3735
Training Epoch: 47 [12288/50048]	Loss: 0.3164
Training Epoch: 47 [12416/50048]	Loss: 0.2734
Training Epoch: 47 [12544/50048]	Loss: 0.1960
Training Epoch: 47 [12672/50048]	Loss: 0.3251
Training Epoch: 47 [12800/50048]	Loss: 0.2598
Training Epoch: 47 [12928/50048]	Loss: 0.2339
Training Epoch: 47 [13056/50048]	Loss: 0.3235
Training Epoch: 47 [13184/50048]	Loss: 0.3455
Training Epoch: 47 [13312/50048]	Loss: 0.1917
Training Epoch: 47 [13440/50048]	Loss: 0.2931
Training Epoch: 47 [13568/50048]	Loss: 0.2188
Training Epoch: 47 [13696/50048]	Loss: 0.4357
Training Epoch: 47 [13824/50048]	Loss: 0.3426
Training Epoch: 47 [13952/50048]	Loss: 0.2439
Training Epoch: 47 [14080/50048]	Loss: 0.3263
Training Epoch: 47 [14208/50048]	Loss: 0.3917
Training Epoch: 47 [14336/50048]	Loss: 0.3500
Training Epoch: 47 [14464/50048]	Loss: 0.4305
Training Epoch: 47 [14592/50048]	Loss: 0.3227
Training Epoch: 47 [14720/50048]	Loss: 0.3710
Training Epoch: 47 [14848/50048]	Loss: 0.2794
Training Epoch: 47 [14976/50048]	Loss: 0.3510
Training Epoch: 47 [15104/50048]	Loss: 0.2989
Training Epoch: 47 [15232/50048]	Loss: 0.2979
Training Epoch: 47 [15360/50048]	Loss: 0.3630
Training Epoch: 47 [15488/50048]	Loss: 0.3132
Training Epoch: 47 [15616/50048]	Loss: 0.4552
Training Epoch: 47 [15744/50048]	Loss: 0.3963
Training Epoch: 47 [15872/50048]	Loss: 0.3337
Training Epoch: 47 [16000/50048]	Loss: 0.2327
Training Epoch: 47 [16128/50048]	Loss: 0.2278
Training Epoch: 47 [16256/50048]	Loss: 0.2984
Training Epoch: 47 [16384/50048]	Loss: 0.2887
Training Epoch: 47 [16512/50048]	Loss: 0.2769
Training Epoch: 47 [16640/50048]	Loss: 0.3090
Training Epoch: 47 [16768/50048]	Loss: 0.3397
Training Epoch: 47 [16896/50048]	Loss: 0.3077
Training Epoch: 47 [17024/50048]	Loss: 0.2957
Training Epoch: 47 [17152/50048]	Loss: 0.3157
Training Epoch: 47 [17280/50048]	Loss: 0.2262
Training Epoch: 47 [17408/50048]	Loss: 0.2808
Training Epoch: 47 [17536/50048]	Loss: 0.4215
Training Epoch: 47 [17664/50048]	Loss: 0.2698
Training Epoch: 47 [17792/50048]	Loss: 0.2692
Training Epoch: 47 [17920/50048]	Loss: 0.3813
Training Epoch: 47 [18048/50048]	Loss: 0.2677
Training Epoch: 47 [18176/50048]	Loss: 0.3259
Training Epoch: 47 [18304/50048]	Loss: 0.2808
Training Epoch: 47 [18432/50048]	Loss: 0.2795
Training Epoch: 47 [18560/50048]	Loss: 0.2841
Training Epoch: 47 [18688/50048]	Loss: 0.4069
Training Epoch: 47 [18816/50048]	Loss: 0.3201
Training Epoch: 47 [18944/50048]	Loss: 0.2864
Training Epoch: 47 [19072/50048]	Loss: 0.3082
Training Epoch: 47 [19200/50048]	Loss: 0.3168
Training Epoch: 47 [19328/50048]	Loss: 0.3464
Training Epoch: 47 [19456/50048]	Loss: 0.3852
Training Epoch: 47 [19584/50048]	Loss: 0.4033
Training Epoch: 47 [19712/50048]	Loss: 0.2972
Training Epoch: 47 [19840/50048]	Loss: 0.3057
Training Epoch: 47 [19968/50048]	Loss: 0.2941
Training Epoch: 47 [20096/50048]	Loss: 0.3765
Training Epoch: 47 [20224/50048]	Loss: 0.3633
Training Epoch: 47 [20352/50048]	Loss: 0.4379
Training Epoch: 47 [20480/50048]	Loss: 0.3641
Training Epoch: 47 [20608/50048]	Loss: 0.3723
Training Epoch: 47 [20736/50048]	Loss: 0.2794
Training Epoch: 47 [20864/50048]	Loss: 0.3185
Training Epoch: 47 [20992/50048]	Loss: 0.1820
Training Epoch: 47 [21120/50048]	Loss: 0.3214
Training Epoch: 47 [21248/50048]	Loss: 0.3857
Training Epoch: 47 [21376/50048]	Loss: 0.2579
Training Epoch: 47 [21504/50048]	Loss: 0.3144
Training Epoch: 47 [21632/50048]	Loss: 0.2727
Training Epoch: 47 [21760/50048]	Loss: 0.3669
Training Epoch: 47 [21888/50048]	Loss: 0.3149
Training Epoch: 47 [22016/50048]	Loss: 0.2867
Training Epoch: 47 [22144/50048]	Loss: 0.3737
Training Epoch: 47 [22272/50048]	Loss: 0.2881
Training Epoch: 47 [22400/50048]	Loss: 0.2435
Training Epoch: 47 [22528/50048]	Loss: 0.3804
Training Epoch: 47 [22656/50048]	Loss: 0.3290
Training Epoch: 47 [22784/50048]	Loss: 0.2585
Training Epoch: 47 [22912/50048]	Loss: 0.2282
Training Epoch: 47 [23040/50048]	Loss: 0.2412
Training Epoch: 47 [23168/50048]	Loss: 0.2956
Training Epoch: 47 [23296/50048]	Loss: 0.3836
Training Epoch: 47 [23424/50048]	Loss: 0.4123
Training Epoch: 47 [23552/50048]	Loss: 0.3287
Training Epoch: 47 [23680/50048]	Loss: 0.1947
Training Epoch: 47 [23808/50048]	Loss: 0.3385
Training Epoch: 47 [23936/50048]	Loss: 0.3035
Training Epoch: 47 [24064/50048]	Loss: 0.3931
Training Epoch: 47 [24192/50048]	Loss: 0.4426
Training Epoch: 47 [24320/50048]	Loss: 0.2607
Training Epoch: 47 [24448/50048]	Loss: 0.2476
Training Epoch: 47 [24576/50048]	Loss: 0.2634
Training Epoch: 47 [24704/50048]	Loss: 0.3227
Training Epoch: 47 [24832/50048]	Loss: 0.3022
Training Epoch: 47 [24960/50048]	Loss: 0.3102
Training Epoch: 47 [25088/50048]	Loss: 0.4024
Training Epoch: 47 [25216/50048]	Loss: 0.2479
Training Epoch: 47 [25344/50048]	Loss: 0.3105
Training Epoch: 47 [25472/50048]	Loss: 0.3554
Training Epoch: 47 [25600/50048]	Loss: 0.4655
Training Epoch: 47 [25728/50048]	Loss: 0.2575
Training Epoch: 47 [25856/50048]	Loss: 0.4455
Training Epoch: 47 [25984/50048]	Loss: 0.4917
Training Epoch: 47 [26112/50048]	Loss: 0.2661
Training Epoch: 47 [26240/50048]	Loss: 0.3202
Training Epoch: 47 [26368/50048]	Loss: 0.2923
Training Epoch: 47 [26496/50048]	Loss: 0.2727
Training Epoch: 47 [26624/50048]	Loss: 0.2604
Training Epoch: 47 [26752/50048]	Loss: 0.3780
Training Epoch: 47 [26880/50048]	Loss: 0.2869
Training Epoch: 47 [27008/50048]	Loss: 0.3557
Training Epoch: 47 [27136/50048]	Loss: 0.4231
Training Epoch: 47 [27264/50048]	Loss: 0.2362
Training Epoch: 47 [27392/50048]	Loss: 0.2123
Training Epoch: 47 [27520/50048]	Loss: 0.2233
Training Epoch: 47 [27648/50048]	Loss: 0.2989
Training Epoch: 47 [27776/50048]	Loss: 0.2827
Training Epoch: 47 [27904/50048]	Loss: 0.3866
Training Epoch: 47 [28032/50048]	Loss: 0.3046
Training Epoch: 47 [28160/50048]	Loss: 0.2920
Training Epoch: 47 [28288/50048]	Loss: 0.3324
Training Epoch: 47 [28416/50048]	Loss: 0.3679
Training Epoch: 47 [28544/50048]	Loss: 0.2678
Training Epoch: 47 [28672/50048]	Loss: 0.2410
Training Epoch: 47 [28800/50048]	Loss: 0.2210
Training Epoch: 47 [28928/50048]	Loss: 0.3179
Training Epoch: 47 [29056/50048]	Loss: 0.5149
Training Epoch: 47 [29184/50048]	Loss: 0.3999
Training Epoch: 47 [29312/50048]	Loss: 0.3006
Training Epoch: 47 [29440/50048]	Loss: 0.4542
Training Epoch: 47 [29568/50048]	Loss: 0.2700
Training Epoch: 47 [29696/50048]	Loss: 0.2886
Training Epoch: 47 [29824/50048]	Loss: 0.4337
Training Epoch: 47 [29952/50048]	Loss: 0.3831
Training Epoch: 47 [30080/50048]	Loss: 0.2781
Training Epoch: 47 [30208/50048]	Loss: 0.2708
Training Epoch: 47 [30336/50048]	Loss: 0.3077
Training Epoch: 47 [30464/50048]	Loss: 0.3210
Training Epoch: 47 [30592/50048]	Loss: 0.3885
Training Epoch: 47 [30720/50048]	Loss: 0.2613
Training Epoch: 47 [30848/50048]	Loss: 0.2055
Training Epoch: 47 [30976/50048]	Loss: 0.3697
Training Epoch: 47 [31104/50048]	Loss: 0.3871
Training Epoch: 47 [31232/50048]	Loss: 0.3545
Training Epoch: 47 [31360/50048]	Loss: 0.2720
Training Epoch: 47 [31488/50048]	Loss: 0.3456
Training Epoch: 47 [31616/50048]	Loss: 0.3928
Training Epoch: 47 [31744/50048]	Loss: 0.3473
Training Epoch: 47 [31872/50048]	Loss: 0.4269
Training Epoch: 47 [32000/50048]	Loss: 0.4716
Training Epoch: 47 [32128/50048]	Loss: 0.3902
Training Epoch: 47 [32256/50048]	Loss: 0.3096
Training Epoch: 47 [32384/50048]	Loss: 0.2357
Training Epoch: 47 [32512/50048]	Loss: 0.3132
Training Epoch: 47 [32640/50048]	Loss: 0.3347
Training Epoch: 47 [32768/50048]	Loss: 0.3532
Training Epoch: 47 [32896/50048]	Loss: 0.3691
Training Epoch: 47 [33024/50048]	Loss: 0.3404
Training Epoch: 47 [33152/50048]	Loss: 0.3762
Training Epoch: 47 [33280/50048]	Loss: 0.2874
Training Epoch: 47 [33408/50048]	Loss: 0.3930
Training Epoch: 47 [33536/50048]	Loss: 0.2392
Training Epoch: 47 [33664/50048]	Loss: 0.3069
Training Epoch: 47 [33792/50048]	Loss: 0.3822
Training Epoch: 47 [33920/50048]	Loss: 0.4313
Training Epoch: 47 [34048/50048]	Loss: 0.3414
Training Epoch: 47 [34176/50048]	Loss: 0.3769
Training Epoch: 47 [34304/50048]	Loss: 0.3110
Training Epoch: 47 [34432/50048]	Loss: 0.3461
Training Epoch: 47 [34560/50048]	Loss: 0.2703
Training Epoch: 47 [34688/50048]	Loss: 0.3426
Training Epoch: 47 [34816/50048]	Loss: 0.3187
Training Epoch: 47 [34944/50048]	Loss: 0.4069
Training Epoch: 47 [35072/50048]	Loss: 0.2690
Training Epoch: 47 [35200/50048]	Loss: 0.2858
Training Epoch: 47 [35328/50048]	Loss: 0.3191
Training Epoch: 47 [35456/50048]	Loss: 0.3485
Training Epoch: 47 [35584/50048]	Loss: 0.2843
Training Epoch: 47 [35712/50048]	Loss: 0.3134
Training Epoch: 47 [35840/50048]	Loss: 0.3411
Training Epoch: 47 [35968/50048]	Loss: 0.3204
Training Epoch: 47 [36096/50048]	Loss: 0.3079
Training Epoch: 47 [36224/50048]	Loss: 0.3557
Training Epoch: 47 [36352/50048]	Loss: 0.2934
Training Epoch: 47 [36480/50048]	Loss: 0.3676
Training Epoch: 47 [36608/50048]	Loss: 0.2818
Training Epoch: 47 [36736/50048]	Loss: 0.3810
Training Epoch: 47 [36864/50048]	Loss: 0.4541
Training Epoch: 47 [36992/50048]	Loss: 0.4306
Training Epoch: 47 [37120/50048]	Loss: 0.3447
Training Epoch: 47 [37248/50048]	Loss: 0.3475
Training Epoch: 47 [37376/50048]	Loss: 0.3352
Training Epoch: 47 [37504/50048]	Loss: 0.3612
Training Epoch: 47 [37632/50048]	Loss: 0.4455
Training Epoch: 47 [37760/50048]	Loss: 0.3705
Training Epoch: 47 [37888/50048]	Loss: 0.2564
Training Epoch: 47 [38016/50048]	Loss: 0.5172
Training Epoch: 47 [38144/50048]	Loss: 0.3941
Training Epoch: 47 [38272/50048]	Loss: 0.2465
Training Epoch: 47 [38400/50048]	Loss: 0.4260
Training Epoch: 47 [38528/50048]	Loss: 0.3670
Training Epoch: 47 [38656/50048]	Loss: 0.3585
Training Epoch: 47 [38784/50048]	Loss: 0.2175
Training Epoch: 47 [38912/50048]	Loss: 0.4420
Training Epoch: 47 [39040/50048]	Loss: 0.4491
Training Epoch: 47 [39168/50048]	Loss: 0.4452
Training Epoch: 47 [39296/50048]	Loss: 0.3162
Training Epoch: 47 [39424/50048]	Loss: 0.2740
Training Epoch: 47 [39552/50048]	Loss: 0.3412
Training Epoch: 47 [39680/50048]	Loss: 0.4486
Training Epoch: 47 [39808/50048]	Loss: 0.2933
Training Epoch: 47 [39936/50048]	Loss: 0.4246
Training Epoch: 47 [40064/50048]	Loss: 0.3658
Training Epoch: 47 [40192/50048]	Loss: 0.2804
Training Epoch: 47 [40320/50048]	Loss: 0.3399
Training Epoch: 47 [40448/50048]	Loss: 0.2895
Training Epoch: 47 [40576/50048]	Loss: 0.2974
Training Epoch: 47 [40704/50048]	Loss: 0.3218
Training Epoch: 47 [40832/50048]	Loss: 0.2438
Training Epoch: 47 [40960/50048]	Loss: 0.2053
Training Epoch: 47 [41088/50048]	Loss: 0.2794
Training Epoch: 47 [41216/50048]	Loss: 0.3738
Training Epoch: 47 [41344/50048]	Loss: 0.3132
Training Epoch: 47 [41472/50048]	Loss: 0.4394
Training Epoch: 47 [41600/50048]	Loss: 0.2606
Training Epoch: 47 [41728/50048]	Loss: 0.4728
Training Epoch: 47 [41856/50048]	Loss: 0.3058
Training Epoch: 47 [41984/50048]	Loss: 0.3897
Training Epoch: 47 [42112/50048]	Loss: 0.3173
Training Epoch: 47 [42240/50048]	Loss: 0.2411
Training Epoch: 47 [42368/50048]	Loss: 0.4481
Training Epoch: 47 [42496/50048]	Loss: 0.3840
Training Epoch: 47 [42624/50048]	Loss: 0.4644
Training Epoch: 47 [42752/50048]	Loss: 0.3231
Training Epoch: 47 [42880/50048]	Loss: 0.3475
Training Epoch: 47 [43008/50048]	Loss: 0.3458
Training Epoch: 47 [43136/50048]	Loss: 0.4494
Training Epoch: 47 [43264/50048]	Loss: 0.3493
Training Epoch: 47 [43392/50048]	Loss: 0.5216
Training Epoch: 47 [43520/50048]	Loss: 0.2534
Training Epoch: 47 [43648/50048]	Loss: 0.4262
Training Epoch: 47 [43776/50048]	Loss: 0.3943
Training Epoch: 47 [43904/50048]	Loss: 0.2967
Training Epoch: 47 [44032/50048]	Loss: 0.3306
Training Epoch: 47 [44160/50048]	Loss: 0.4674
Training Epoch: 47 [44288/50048]	Loss: 0.3783
Training Epoch: 47 [44416/50048]	Loss: 0.3138
Training Epoch: 47 [44544/50048]	Loss: 0.3479
Training Epoch: 47 [44672/50048]	Loss: 0.3319
Training Epoch: 47 [44800/50048]	Loss: 0.3581
Training Epoch: 47 [44928/50048]	Loss: 0.2998
Training Epoch: 47 [45056/50048]	Loss: 0.2719
Training Epoch: 47 [45184/50048]	Loss: 0.4300
Training Epoch: 47 [45312/50048]	Loss: 0.4528
Training Epoch: 47 [45440/50048]	Loss: 0.2668
Training Epoch: 47 [45568/50048]	Loss: 0.3354
Training Epoch: 47 [45696/50048]	Loss: 0.3448
2022-12-06 07:20:00,495 [ZeusDataLoader(train)] train epoch 48 done: time=86.52 energy=10499.66
2022-12-06 07:20:00,496 [ZeusDataLoader(eval)] Epoch 48 begin.
Training Epoch: 47 [45824/50048]	Loss: 0.3710
Training Epoch: 47 [45952/50048]	Loss: 0.2168
Training Epoch: 47 [46080/50048]	Loss: 0.4769
Training Epoch: 47 [46208/50048]	Loss: 0.3692
Training Epoch: 47 [46336/50048]	Loss: 0.3573
Training Epoch: 47 [46464/50048]	Loss: 0.2587
Training Epoch: 47 [46592/50048]	Loss: 0.3305
Training Epoch: 47 [46720/50048]	Loss: 0.3726
Training Epoch: 47 [46848/50048]	Loss: 0.3243
Training Epoch: 47 [46976/50048]	Loss: 0.3697
Training Epoch: 47 [47104/50048]	Loss: 0.3684
Training Epoch: 47 [47232/50048]	Loss: 0.3987
Training Epoch: 47 [47360/50048]	Loss: 0.3636
Training Epoch: 47 [47488/50048]	Loss: 0.3218
Training Epoch: 47 [47616/50048]	Loss: 0.3672
Training Epoch: 47 [47744/50048]	Loss: 0.3030
Training Epoch: 47 [47872/50048]	Loss: 0.4455
Training Epoch: 47 [48000/50048]	Loss: 0.3445
Training Epoch: 47 [48128/50048]	Loss: 0.3534
Training Epoch: 47 [48256/50048]	Loss: 0.3336
Training Epoch: 47 [48384/50048]	Loss: 0.4114
Training Epoch: 47 [48512/50048]	Loss: 0.3856
Training Epoch: 47 [48640/50048]	Loss: 0.4163
Training Epoch: 47 [48768/50048]	Loss: 0.5810
Training Epoch: 47 [48896/50048]	Loss: 0.2234
Training Epoch: 47 [49024/50048]	Loss: 0.3522
Training Epoch: 47 [49152/50048]	Loss: 0.4365
Training Epoch: 47 [49280/50048]	Loss: 0.3361
Training Epoch: 47 [49408/50048]	Loss: 0.4494
Training Epoch: 47 [49536/50048]	Loss: 0.3682
Training Epoch: 47 [49664/50048]	Loss: 0.2457
Training Epoch: 47 [49792/50048]	Loss: 0.3242
Training Epoch: 47 [49920/50048]	Loss: 0.3843
Training Epoch: 47 [50048/50048]	Loss: 0.4524
2022-12-06 12:20:04.223 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:20:04,233 [ZeusDataLoader(eval)] eval epoch 48 done: time=3.73 energy=452.69
2022-12-06 07:20:04,233 [ZeusDataLoader(train)] Up to epoch 48: time=4329.56, energy=525551.30, cost=641612.37
2022-12-06 07:20:04,233 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:20:04,233 [ZeusDataLoader(train)] Expected next epoch: time=4419.36, energy=536349.32, cost=654868.75
2022-12-06 07:20:04,234 [ZeusDataLoader(train)] Epoch 49 begin.
Validation Epoch: 47, Average loss: 0.0149, Accuracy: 0.6294
2022-12-06 07:20:04,421 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:20:04,422 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:20:04.424 [ZeusMonitor] Monitor started.
2022-12-06 12:20:04.424 [ZeusMonitor] Running indefinitely. 2022-12-06 12:20:04.424 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:20:04.424 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e49+gpu0.power.log
Training Epoch: 48 [128/50048]	Loss: 0.3287
Training Epoch: 48 [256/50048]	Loss: 0.2847
Training Epoch: 48 [384/50048]	Loss: 0.2524
Training Epoch: 48 [512/50048]	Loss: 0.2777
Training Epoch: 48 [640/50048]	Loss: 0.2042
Training Epoch: 48 [768/50048]	Loss: 0.3006
Training Epoch: 48 [896/50048]	Loss: 0.2597
Training Epoch: 48 [1024/50048]	Loss: 0.1999
Training Epoch: 48 [1152/50048]	Loss: 0.2872
Training Epoch: 48 [1280/50048]	Loss: 0.2618
Training Epoch: 48 [1408/50048]	Loss: 0.2228
Training Epoch: 48 [1536/50048]	Loss: 0.2971
Training Epoch: 48 [1664/50048]	Loss: 0.1856
Training Epoch: 48 [1792/50048]	Loss: 0.3240
Training Epoch: 48 [1920/50048]	Loss: 0.3165
Training Epoch: 48 [2048/50048]	Loss: 0.2426
Training Epoch: 48 [2176/50048]	Loss: 0.2926
Training Epoch: 48 [2304/50048]	Loss: 0.3079
Training Epoch: 48 [2432/50048]	Loss: 0.2559
Training Epoch: 48 [2560/50048]	Loss: 0.2317
Training Epoch: 48 [2688/50048]	Loss: 0.1899
Training Epoch: 48 [2816/50048]	Loss: 0.2713
Training Epoch: 48 [2944/50048]	Loss: 0.2557
Training Epoch: 48 [3072/50048]	Loss: 0.2798
Training Epoch: 48 [3200/50048]	Loss: 0.2667
Training Epoch: 48 [3328/50048]	Loss: 0.2336
Training Epoch: 48 [3456/50048]	Loss: 0.2864
Training Epoch: 48 [3584/50048]	Loss: 0.3057
Training Epoch: 48 [3712/50048]	Loss: 0.2087
Training Epoch: 48 [3840/50048]	Loss: 0.2509
Training Epoch: 48 [3968/50048]	Loss: 0.2167
Training Epoch: 48 [4096/50048]	Loss: 0.3354
Training Epoch: 48 [4224/50048]	Loss: 0.2720
Training Epoch: 48 [4352/50048]	Loss: 0.3028
Training Epoch: 48 [4480/50048]	Loss: 0.2981
Training Epoch: 48 [4608/50048]	Loss: 0.3286
Training Epoch: 48 [4736/50048]	Loss: 0.1793
Training Epoch: 48 [4864/50048]	Loss: 0.1973
Training Epoch: 48 [4992/50048]	Loss: 0.3172
Training Epoch: 48 [5120/50048]	Loss: 0.2592
Training Epoch: 48 [5248/50048]	Loss: 0.2250
Training Epoch: 48 [5376/50048]	Loss: 0.2158
Training Epoch: 48 [5504/50048]	Loss: 0.1903
Training Epoch: 48 [5632/50048]	Loss: 0.2691
Training Epoch: 48 [5760/50048]	Loss: 0.2906
Training Epoch: 48 [5888/50048]	Loss: 0.1549
Training Epoch: 48 [6016/50048]	Loss: 0.2611
Training Epoch: 48 [6144/50048]	Loss: 0.2935
Training Epoch: 48 [6272/50048]	Loss: 0.2766
Training Epoch: 48 [6400/50048]	Loss: 0.1871
Training Epoch: 48 [6528/50048]	Loss: 0.3338
Training Epoch: 48 [6656/50048]	Loss: 0.1817
Training Epoch: 48 [6784/50048]	Loss: 0.2592
Training Epoch: 48 [6912/50048]	Loss: 0.2751
Training Epoch: 48 [7040/50048]	Loss: 0.2421
Training Epoch: 48 [7168/50048]	Loss: 0.2970
Training Epoch: 48 [7296/50048]	Loss: 0.2716
Training Epoch: 48 [7424/50048]	Loss: 0.3466
Training Epoch: 48 [7552/50048]	Loss: 0.2386
Training Epoch: 48 [7680/50048]	Loss: 0.3867
Training Epoch: 48 [7808/50048]	Loss: 0.2973
Training Epoch: 48 [7936/50048]	Loss: 0.2540
Training Epoch: 48 [8064/50048]	Loss: 0.2475
Training Epoch: 48 [8192/50048]	Loss: 0.2889
Training Epoch: 48 [8320/50048]	Loss: 0.1806
Training Epoch: 48 [8448/50048]	Loss: 0.3340
Training Epoch: 48 [8576/50048]	Loss: 0.2383
Training Epoch: 48 [8704/50048]	Loss: 0.2149
Training Epoch: 48 [8832/50048]	Loss: 0.2582
Training Epoch: 48 [8960/50048]	Loss: 0.1972
Training Epoch: 48 [9088/50048]	Loss: 0.2940
Training Epoch: 48 [9216/50048]	Loss: 0.2671
Training Epoch: 48 [9344/50048]	Loss: 0.2344
Training Epoch: 48 [9472/50048]	Loss: 0.2262
Training Epoch: 48 [9600/50048]	Loss: 0.2585
Training Epoch: 48 [9728/50048]	Loss: 0.2259
Training Epoch: 48 [9856/50048]	Loss: 0.3741
Training Epoch: 48 [9984/50048]	Loss: 0.3201
Training Epoch: 48 [10112/50048]	Loss: 0.3260
Training Epoch: 48 [10240/50048]	Loss: 0.3035
Training Epoch: 48 [10368/50048]	Loss: 0.3582
Training Epoch: 48 [10496/50048]	Loss: 0.2495
Training Epoch: 48 [10624/50048]	Loss: 0.2482
Training Epoch: 48 [10752/50048]	Loss: 0.3366
Training Epoch: 48 [10880/50048]	Loss: 0.2717
Training Epoch: 48 [11008/50048]	Loss: 0.2685
Training Epoch: 48 [11136/50048]	Loss: 0.2752
Training Epoch: 48 [11264/50048]	Loss: 0.2182
Training Epoch: 48 [11392/50048]	Loss: 0.3883
Training Epoch: 48 [11520/50048]	Loss: 0.1883
Training Epoch: 48 [11648/50048]	Loss: 0.2414
Training Epoch: 48 [11776/50048]	Loss: 0.3768
Training Epoch: 48 [11904/50048]	Loss: 0.4408
Training Epoch: 48 [12032/50048]	Loss: 0.3425
Training Epoch: 48 [12160/50048]	Loss: 0.2590
Training Epoch: 48 [12288/50048]	Loss: 0.2869
Training Epoch: 48 [12416/50048]	Loss: 0.3106
Training Epoch: 48 [12544/50048]	Loss: 0.3241
Training Epoch: 48 [12672/50048]	Loss: 0.2407
Training Epoch: 48 [12800/50048]	Loss: 0.2161
Training Epoch: 48 [12928/50048]	Loss: 0.2979
Training Epoch: 48 [13056/50048]	Loss: 0.3239
Training Epoch: 48 [13184/50048]	Loss: 0.3587
Training Epoch: 48 [13312/50048]	Loss: 0.3078
Training Epoch: 48 [13440/50048]	Loss: 0.2589
Training Epoch: 48 [13568/50048]	Loss: 0.2681
Training Epoch: 48 [13696/50048]	Loss: 0.3671
Training Epoch: 48 [13824/50048]	Loss: 0.2892
Training Epoch: 48 [13952/50048]	Loss: 0.4106
Training Epoch: 48 [14080/50048]	Loss: 0.3877
Training Epoch: 48 [14208/50048]	Loss: 0.2482
Training Epoch: 48 [14336/50048]	Loss: 0.3060
Training Epoch: 48 [14464/50048]	Loss: 0.3921
Training Epoch: 48 [14592/50048]	Loss: 0.2747
Training Epoch: 48 [14720/50048]	Loss: 0.3979
Training Epoch: 48 [14848/50048]	Loss: 0.5304
Training Epoch: 48 [14976/50048]	Loss: 0.3855
Training Epoch: 48 [15104/50048]	Loss: 0.1630
Training Epoch: 48 [15232/50048]	Loss: 0.3474
Training Epoch: 48 [15360/50048]	Loss: 0.3289
Training Epoch: 48 [15488/50048]	Loss: 0.2407
Training Epoch: 48 [15616/50048]	Loss: 0.2836
Training Epoch: 48 [15744/50048]	Loss: 0.2538
Training Epoch: 48 [15872/50048]	Loss: 0.3004
Training Epoch: 48 [16000/50048]	Loss: 0.2870
Training Epoch: 48 [16128/50048]	Loss: 0.3559
Training Epoch: 48 [16256/50048]	Loss: 0.1714
Training Epoch: 48 [16384/50048]	Loss: 0.3441
Training Epoch: 48 [16512/50048]	Loss: 0.2265
Training Epoch: 48 [16640/50048]	Loss: 0.3791
Training Epoch: 48 [16768/50048]	Loss: 0.2820
Training Epoch: 48 [16896/50048]	Loss: 0.2909
Training Epoch: 48 [17024/50048]	Loss: 0.3202
Training Epoch: 48 [17152/50048]	Loss: 0.2314
Training Epoch: 48 [17280/50048]	Loss: 0.2744
Training Epoch: 48 [17408/50048]	Loss: 0.3951
Training Epoch: 48 [17536/50048]	Loss: 0.2939
Training Epoch: 48 [17664/50048]	Loss: 0.2864
Training Epoch: 48 [17792/50048]	Loss: 0.2738
Training Epoch: 48 [17920/50048]	Loss: 0.2798
Training Epoch: 48 [18048/50048]	Loss: 0.2968
Training Epoch: 48 [18176/50048]	Loss: 0.2975
Training Epoch: 48 [18304/50048]	Loss: 0.3075
Training Epoch: 48 [18432/50048]	Loss: 0.2923
Training Epoch: 48 [18560/50048]	Loss: 0.2899
Training Epoch: 48 [18688/50048]	Loss: 0.2653
Training Epoch: 48 [18816/50048]	Loss: 0.2775
Training Epoch: 48 [18944/50048]	Loss: 0.3257
Training Epoch: 48 [19072/50048]	Loss: 0.2748
Training Epoch: 48 [19200/50048]	Loss: 0.3152
Training Epoch: 48 [19328/50048]	Loss: 0.2852
Training Epoch: 48 [19456/50048]	Loss: 0.3262
Training Epoch: 48 [19584/50048]	Loss: 0.2759
Training Epoch: 48 [19712/50048]	Loss: 0.3162
Training Epoch: 48 [19840/50048]	Loss: 0.3433
Training Epoch: 48 [19968/50048]	Loss: 0.3898
Training Epoch: 48 [20096/50048]	Loss: 0.2909
Training Epoch: 48 [20224/50048]	Loss: 0.4418
Training Epoch: 48 [20352/50048]	Loss: 0.2409
Training Epoch: 48 [20480/50048]	Loss: 0.3956
Training Epoch: 48 [20608/50048]	Loss: 0.1939
Training Epoch: 48 [20736/50048]	Loss: 0.3059
Training Epoch: 48 [20864/50048]	Loss: 0.3991
Training Epoch: 48 [20992/50048]	Loss: 0.3374
Training Epoch: 48 [21120/50048]	Loss: 0.3386
Training Epoch: 48 [21248/50048]	Loss: 0.3041
Training Epoch: 48 [21376/50048]	Loss: 0.2645
Training Epoch: 48 [21504/50048]	Loss: 0.3128
Training Epoch: 48 [21632/50048]	Loss: 0.2725
Training Epoch: 48 [21760/50048]	Loss: 0.2982
Training Epoch: 48 [21888/50048]	Loss: 0.3364
Training Epoch: 48 [22016/50048]	Loss: 0.4087
Training Epoch: 48 [22144/50048]	Loss: 0.2486
Training Epoch: 48 [22272/50048]	Loss: 0.2437
Training Epoch: 48 [22400/50048]	Loss: 0.3685
Training Epoch: 48 [22528/50048]	Loss: 0.2114
Training Epoch: 48 [22656/50048]	Loss: 0.2787
Training Epoch: 48 [22784/50048]	Loss: 0.3450
Training Epoch: 48 [22912/50048]	Loss: 0.2868
Training Epoch: 48 [23040/50048]	Loss: 0.3011
Training Epoch: 48 [23168/50048]	Loss: 0.3257
Training Epoch: 48 [23296/50048]	Loss: 0.2465
Training Epoch: 48 [23424/50048]	Loss: 0.2912
Training Epoch: 48 [23552/50048]	Loss: 0.3609
Training Epoch: 48 [23680/50048]	Loss: 0.2924
Training Epoch: 48 [23808/50048]	Loss: 0.2269
Training Epoch: 48 [23936/50048]	Loss: 0.3478
Training Epoch: 48 [24064/50048]	Loss: 0.3379
Training Epoch: 48 [24192/50048]	Loss: 0.2925
Training Epoch: 48 [24320/50048]	Loss: 0.3140
Training Epoch: 48 [24448/50048]	Loss: 0.4195
Training Epoch: 48 [24576/50048]	Loss: 0.2441
Training Epoch: 48 [24704/50048]	Loss: 0.2863
Training Epoch: 48 [24832/50048]	Loss: 0.3083
Training Epoch: 48 [24960/50048]	Loss: 0.2584
Training Epoch: 48 [25088/50048]	Loss: 0.4087
Training Epoch: 48 [25216/50048]	Loss: 0.3005
Training Epoch: 48 [25344/50048]	Loss: 0.4015
Training Epoch: 48 [25472/50048]	Loss: 0.3958
Training Epoch: 48 [25600/50048]	Loss: 0.2211
Training Epoch: 48 [25728/50048]	Loss: 0.2877
Training Epoch: 48 [25856/50048]	Loss: 0.3307
Training Epoch: 48 [25984/50048]	Loss: 0.4117
Training Epoch: 48 [26112/50048]	Loss: 0.3168
Training Epoch: 48 [26240/50048]	Loss: 0.3137
Training Epoch: 48 [26368/50048]	Loss: 0.2728
Training Epoch: 48 [26496/50048]	Loss: 0.3037
Training Epoch: 48 [26624/50048]	Loss: 0.2581
Training Epoch: 48 [26752/50048]	Loss: 0.2835
Training Epoch: 48 [26880/50048]	Loss: 0.2814
Training Epoch: 48 [27008/50048]	Loss: 0.4708
Training Epoch: 48 [27136/50048]	Loss: 0.3698
Training Epoch: 48 [27264/50048]	Loss: 0.3185
Training Epoch: 48 [27392/50048]	Loss: 0.2955
Training Epoch: 48 [27520/50048]	Loss: 0.3088
Training Epoch: 48 [27648/50048]	Loss: 0.2508
Training Epoch: 48 [27776/50048]	Loss: 0.4287
Training Epoch: 48 [27904/50048]	Loss: 0.2696
Training Epoch: 48 [28032/50048]	Loss: 0.3795
Training Epoch: 48 [28160/50048]	Loss: 0.3707
Training Epoch: 48 [28288/50048]	Loss: 0.2676
Training Epoch: 48 [28416/50048]	Loss: 0.2137
Training Epoch: 48 [28544/50048]	Loss: 0.1399
Training Epoch: 48 [28672/50048]	Loss: 0.3263
Training Epoch: 48 [28800/50048]	Loss: 0.2825
Training Epoch: 48 [28928/50048]	Loss: 0.3040
Training Epoch: 48 [29056/50048]	Loss: 0.3943
Training Epoch: 48 [29184/50048]	Loss: 0.3271
Training Epoch: 48 [29312/50048]	Loss: 0.1918
Training Epoch: 48 [29440/50048]	Loss: 0.3053
Training Epoch: 48 [29568/50048]	Loss: 0.2923
Training Epoch: 48 [29696/50048]	Loss: 0.1738
Training Epoch: 48 [29824/50048]	Loss: 0.2154
Training Epoch: 48 [29952/50048]	Loss: 0.2747
Training Epoch: 48 [30080/50048]	Loss: 0.2616
Training Epoch: 48 [30208/50048]	Loss: 0.2444
Training Epoch: 48 [30336/50048]	Loss: 0.4867
Training Epoch: 48 [30464/50048]	Loss: 0.3244
Training Epoch: 48 [30592/50048]	Loss: 0.3581
Training Epoch: 48 [30720/50048]	Loss: 0.3263
Training Epoch: 48 [30848/50048]	Loss: 0.2815
Training Epoch: 48 [30976/50048]	Loss: 0.2445
Training Epoch: 48 [31104/50048]	Loss: 0.3413
Training Epoch: 48 [31232/50048]	Loss: 0.2337
Training Epoch: 48 [31360/50048]	Loss: 0.4329
Training Epoch: 48 [31488/50048]	Loss: 0.3508
Training Epoch: 48 [31616/50048]	Loss: 0.2449
Training Epoch: 48 [31744/50048]	Loss: 0.2911
Training Epoch: 48 [31872/50048]	Loss: 0.3275
Training Epoch: 48 [32000/50048]	Loss: 0.3052
Training Epoch: 48 [32128/50048]	Loss: 0.2913
Training Epoch: 48 [32256/50048]	Loss: 0.3540
Training Epoch: 48 [32384/50048]	Loss: 0.2992
Training Epoch: 48 [32512/50048]	Loss: 0.3400
Training Epoch: 48 [32640/50048]	Loss: 0.2663
Training Epoch: 48 [32768/50048]	Loss: 0.2769
Training Epoch: 48 [32896/50048]	Loss: 0.2749
Training Epoch: 48 [33024/50048]	Loss: 0.2538
Training Epoch: 48 [33152/50048]	Loss: 0.3084
Training Epoch: 48 [33280/50048]	Loss: 0.4178
Training Epoch: 48 [33408/50048]	Loss: 0.3842
Training Epoch: 48 [33536/50048]	Loss: 0.2849
Training Epoch: 48 [33664/50048]	Loss: 0.3787
Training Epoch: 48 [33792/50048]	Loss: 0.3064
Training Epoch: 48 [33920/50048]	Loss: 0.3010
Training Epoch: 48 [34048/50048]	Loss: 0.2810
Training Epoch: 48 [34176/50048]	Loss: 0.3239
Training Epoch: 48 [34304/50048]	Loss: 0.2696
Training Epoch: 48 [34432/50048]	Loss: 0.3548
Training Epoch: 48 [34560/50048]	Loss: 0.3714
Training Epoch: 48 [34688/50048]	Loss: 0.4214
Training Epoch: 48 [34816/50048]	Loss: 0.2659
Training Epoch: 48 [34944/50048]	Loss: 0.3498
Training Epoch: 48 [35072/50048]	Loss: 0.2146
Training Epoch: 48 [35200/50048]	Loss: 0.3529
Training Epoch: 48 [35328/50048]	Loss: 0.2792
Training Epoch: 48 [35456/50048]	Loss: 0.3476
Training Epoch: 48 [35584/50048]	Loss: 0.2573
Training Epoch: 48 [35712/50048]	Loss: 0.3057
Training Epoch: 48 [35840/50048]	Loss: 0.2582
Training Epoch: 48 [35968/50048]	Loss: 0.2797
Training Epoch: 48 [36096/50048]	Loss: 0.4084
Training Epoch: 48 [36224/50048]	Loss: 0.6107
Training Epoch: 48 [36352/50048]	Loss: 0.2701
Training Epoch: 48 [36480/50048]	Loss: 0.3202
Training Epoch: 48 [36608/50048]	Loss: 0.2697
Training Epoch: 48 [36736/50048]	Loss: 0.4364
Training Epoch: 48 [36864/50048]	Loss: 0.3448
Training Epoch: 48 [36992/50048]	Loss: 0.3172
Training Epoch: 48 [37120/50048]	Loss: 0.2556
Training Epoch: 48 [37248/50048]	Loss: 0.4042
Training Epoch: 48 [37376/50048]	Loss: 0.2745
Training Epoch: 48 [37504/50048]	Loss: 0.3145
Training Epoch: 48 [37632/50048]	Loss: 0.4770
Training Epoch: 48 [37760/50048]	Loss: 0.2883
Training Epoch: 48 [37888/50048]	Loss: 0.2993
Training Epoch: 48 [38016/50048]	Loss: 0.3107
Training Epoch: 48 [38144/50048]	Loss: 0.2489
Training Epoch: 48 [38272/50048]	Loss: 0.2343
Training Epoch: 48 [38400/50048]	Loss: 0.4125
Training Epoch: 48 [38528/50048]	Loss: 0.3712
Training Epoch: 48 [38656/50048]	Loss: 0.4171
Training Epoch: 48 [38784/50048]	Loss: 0.4109
Training Epoch: 48 [38912/50048]	Loss: 0.3554
Training Epoch: 48 [39040/50048]	Loss: 0.1737
Training Epoch: 48 [39168/50048]	Loss: 0.2923
Training Epoch: 48 [39296/50048]	Loss: 0.2738
Training Epoch: 48 [39424/50048]	Loss: 0.2950
Training Epoch: 48 [39552/50048]	Loss: 0.2444
Training Epoch: 48 [39680/50048]	Loss: 0.3087
Training Epoch: 48 [39808/50048]	Loss: 0.3809
Training Epoch: 48 [39936/50048]	Loss: 0.2639
Training Epoch: 48 [40064/50048]	Loss: 0.3351
Training Epoch: 48 [40192/50048]	Loss: 0.3288
Training Epoch: 48 [40320/50048]	Loss: 0.3259
Training Epoch: 48 [40448/50048]	Loss: 0.3058
Training Epoch: 48 [40576/50048]	Loss: 0.3123
Training Epoch: 48 [40704/50048]	Loss: 0.4369
Training Epoch: 48 [40832/50048]	Loss: 0.3760
Training Epoch: 48 [40960/50048]	Loss: 0.3346
Training Epoch: 48 [41088/50048]	Loss: 0.2769
Training Epoch: 48 [41216/50048]	Loss: 0.2486
Training Epoch: 48 [41344/50048]	Loss: 0.2849
Training Epoch: 48 [41472/50048]	Loss: 0.3514
Training Epoch: 48 [41600/50048]	Loss: 0.2114
Training Epoch: 48 [41728/50048]	Loss: 0.3616
Training Epoch: 48 [41856/50048]	Loss: 0.2842
Training Epoch: 48 [41984/50048]	Loss: 0.2902
Training Epoch: 48 [42112/50048]	Loss: 0.4413
Training Epoch: 48 [42240/50048]	Loss: 0.2163
Training Epoch: 48 [42368/50048]	Loss: 0.4852
Training Epoch: 48 [42496/50048]	Loss: 0.3429
Training Epoch: 48 [42624/50048]	Loss: 0.3122
Training Epoch: 48 [42752/50048]	Loss: 0.3656
Training Epoch: 48 [42880/50048]	Loss: 0.2959
Training Epoch: 48 [43008/50048]	Loss: 0.3466
Training Epoch: 48 [43136/50048]	Loss: 0.2405
Training Epoch: 48 [43264/50048]	Loss: 0.2648
Training Epoch: 48 [43392/50048]	Loss: 0.3800
Training Epoch: 48 [43520/50048]	Loss: 0.2816
Training Epoch: 48 [43648/50048]	Loss: 0.4188
Training Epoch: 48 [43776/50048]	Loss: 0.3171
Training Epoch: 48 [43904/50048]	Loss: 0.4143
Training Epoch: 48 [44032/50048]	Loss: 0.4076
Training Epoch: 48 [44160/50048]	Loss: 0.2703
Training Epoch: 48 [44288/50048]	Loss: 0.3580
Training Epoch: 48 [44416/50048]	Loss: 0.4348
Training Epoch: 48 [44544/50048]	Loss: 0.2411
Training Epoch: 48 [44672/50048]	Loss: 0.3019
Training Epoch: 48 [44800/50048]	Loss: 0.3822
Training Epoch: 48 [44928/50048]	Loss: 0.4830
Training Epoch: 48 [45056/50048]	Loss: 0.2419
Training Epoch: 48 [45184/50048]	Loss: 0.3776
Training Epoch: 48 [45312/50048]	Loss: 0.4962
Training Epoch: 48 [45440/50048]	Loss: 0.3947
Training Epoch: 48 [45568/50048]	Loss: 0.3342
Training Epoch: 48 [45696/50048]	Loss: 0.3508
2022-12-06 07:21:30,785 [ZeusDataLoader(train)] train epoch 49 done: time=86.54 energy=10497.48
2022-12-06 07:21:30,787 [ZeusDataLoader(eval)] Epoch 49 begin.
Training Epoch: 48 [45824/50048]	Loss: 0.4062
Training Epoch: 48 [45952/50048]	Loss: 0.2940
Training Epoch: 48 [46080/50048]	Loss: 0.4522
Training Epoch: 48 [46208/50048]	Loss: 0.3418
Training Epoch: 48 [46336/50048]	Loss: 0.3475
Training Epoch: 48 [46464/50048]	Loss: 0.3354
Training Epoch: 48 [46592/50048]	Loss: 0.2802
Training Epoch: 48 [46720/50048]	Loss: 0.3658
Training Epoch: 48 [46848/50048]	Loss: 0.2294
Training Epoch: 48 [46976/50048]	Loss: 0.2878
Training Epoch: 48 [47104/50048]	Loss: 0.2978
Training Epoch: 48 [47232/50048]	Loss: 0.3442
Training Epoch: 48 [47360/50048]	Loss: 0.2722
Training Epoch: 48 [47488/50048]	Loss: 0.4049
Training Epoch: 48 [47616/50048]	Loss: 0.3595
Training Epoch: 48 [47744/50048]	Loss: 0.3663
Training Epoch: 48 [47872/50048]	Loss: 0.1884
Training Epoch: 48 [48000/50048]	Loss: 0.3359
Training Epoch: 48 [48128/50048]	Loss: 0.4237
Training Epoch: 48 [48256/50048]	Loss: 0.4409
Training Epoch: 48 [48384/50048]	Loss: 0.3347
Training Epoch: 48 [48512/50048]	Loss: 0.2644
Training Epoch: 48 [48640/50048]	Loss: 0.4180
Training Epoch: 48 [48768/50048]	Loss: 0.3662
Training Epoch: 48 [48896/50048]	Loss: 0.2872
Training Epoch: 48 [49024/50048]	Loss: 0.2610
Training Epoch: 48 [49152/50048]	Loss: 0.2729
Training Epoch: 48 [49280/50048]	Loss: 0.3870
Training Epoch: 48 [49408/50048]	Loss: 0.3012
Training Epoch: 48 [49536/50048]	Loss: 0.3796
Training Epoch: 48 [49664/50048]	Loss: 0.3452
Training Epoch: 48 [49792/50048]	Loss: 0.3914
Training Epoch: 48 [49920/50048]	Loss: 0.3160
Training Epoch: 48 [50048/50048]	Loss: 0.2163
2022-12-06 12:21:34.525 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:21:34,560 [ZeusDataLoader(eval)] eval epoch 49 done: time=3.76 energy=453.86
2022-12-06 07:21:34,560 [ZeusDataLoader(train)] Up to epoch 49: time=4419.87, energy=536502.64, cost=654989.71
2022-12-06 07:21:34,560 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:21:34,560 [ZeusDataLoader(train)] Expected next epoch: time=4509.67, energy=547300.66, cost=668246.09
2022-12-06 07:21:34,561 [ZeusDataLoader(train)] Epoch 50 begin.
Validation Epoch: 48, Average loss: 0.0148, Accuracy: 0.6266
2022-12-06 07:21:34,747 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:21:34,747 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:21:34.749 [ZeusMonitor] Monitor started.
2022-12-06 12:21:34.749 [ZeusMonitor] Running indefinitely. 2022-12-06 12:21:34.749 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:21:34.749 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e50+gpu0.power.log
Training Epoch: 49 [128/50048]	Loss: 0.1735
Training Epoch: 49 [256/50048]	Loss: 0.2445
Training Epoch: 49 [384/50048]	Loss: 0.2377
Training Epoch: 49 [512/50048]	Loss: 0.2699
Training Epoch: 49 [640/50048]	Loss: 0.3447
Training Epoch: 49 [768/50048]	Loss: 0.2141
Training Epoch: 49 [896/50048]	Loss: 0.2520
Training Epoch: 49 [1024/50048]	Loss: 0.2141
Training Epoch: 49 [1152/50048]	Loss: 0.2883
Training Epoch: 49 [1280/50048]	Loss: 0.2950
Training Epoch: 49 [1408/50048]	Loss: 0.2949
Training Epoch: 49 [1536/50048]	Loss: 0.2638
Training Epoch: 49 [1664/50048]	Loss: 0.3081
Training Epoch: 49 [1792/50048]	Loss: 0.2737
Training Epoch: 49 [1920/50048]	Loss: 0.1700
Training Epoch: 49 [2048/50048]	Loss: 0.2134
Training Epoch: 49 [2176/50048]	Loss: 0.1953
Training Epoch: 49 [2304/50048]	Loss: 0.3116
Training Epoch: 49 [2432/50048]	Loss: 0.2410
Training Epoch: 49 [2560/50048]	Loss: 0.2358
Training Epoch: 49 [2688/50048]	Loss: 0.2862
Training Epoch: 49 [2816/50048]	Loss: 0.2164
Training Epoch: 49 [2944/50048]	Loss: 0.2276
Training Epoch: 49 [3072/50048]	Loss: 0.2425
Training Epoch: 49 [3200/50048]	Loss: 0.3125
Training Epoch: 49 [3328/50048]	Loss: 0.2109
Training Epoch: 49 [3456/50048]	Loss: 0.3321
Training Epoch: 49 [3584/50048]	Loss: 0.2718
Training Epoch: 49 [3712/50048]	Loss: 0.3115
Training Epoch: 49 [3840/50048]	Loss: 0.2863
Training Epoch: 49 [3968/50048]	Loss: 0.2983
Training Epoch: 49 [4096/50048]	Loss: 0.1884
Training Epoch: 49 [4224/50048]	Loss: 0.1944
Training Epoch: 49 [4352/50048]	Loss: 0.1967
Training Epoch: 49 [4480/50048]	Loss: 0.2165
Training Epoch: 49 [4608/50048]	Loss: 0.2638
Training Epoch: 49 [4736/50048]	Loss: 0.2743
Training Epoch: 49 [4864/50048]	Loss: 0.3160
Training Epoch: 49 [4992/50048]	Loss: 0.4212
Training Epoch: 49 [5120/50048]	Loss: 0.2265
Training Epoch: 49 [5248/50048]	Loss: 0.2091
Training Epoch: 49 [5376/50048]	Loss: 0.2480
Training Epoch: 49 [5504/50048]	Loss: 0.2618
Training Epoch: 49 [5632/50048]	Loss: 0.2761
Training Epoch: 49 [5760/50048]	Loss: 0.3260
Training Epoch: 49 [5888/50048]	Loss: 0.2666
Training Epoch: 49 [6016/50048]	Loss: 0.2481
Training Epoch: 49 [6144/50048]	Loss: 0.2139
Training Epoch: 49 [6272/50048]	Loss: 0.2139
Training Epoch: 49 [6400/50048]	Loss: 0.2160
Training Epoch: 49 [6528/50048]	Loss: 0.2338
Training Epoch: 49 [6656/50048]	Loss: 0.2439
Training Epoch: 49 [6784/50048]	Loss: 0.3393
Training Epoch: 49 [6912/50048]	Loss: 0.2721
Training Epoch: 49 [7040/50048]	Loss: 0.2562
Training Epoch: 49 [7168/50048]	Loss: 0.2558
Training Epoch: 49 [7296/50048]	Loss: 0.3196
Training Epoch: 49 [7424/50048]	Loss: 0.2633
Training Epoch: 49 [7552/50048]	Loss: 0.2135
Training Epoch: 49 [7680/50048]	Loss: 0.2356
Training Epoch: 49 [7808/50048]	Loss: 0.3689
Training Epoch: 49 [7936/50048]	Loss: 0.3168
Training Epoch: 49 [8064/50048]	Loss: 0.2666
Training Epoch: 49 [8192/50048]	Loss: 0.2512
Training Epoch: 49 [8320/50048]	Loss: 0.2501
Training Epoch: 49 [8448/50048]	Loss: 0.2613
Training Epoch: 49 [8576/50048]	Loss: 0.2489
Training Epoch: 49 [8704/50048]	Loss: 0.3127
Training Epoch: 49 [8832/50048]	Loss: 0.2026
Training Epoch: 49 [8960/50048]	Loss: 0.2384
Training Epoch: 49 [9088/50048]	Loss: 0.3425
Training Epoch: 49 [9216/50048]	Loss: 0.2140
Training Epoch: 49 [9344/50048]	Loss: 0.1741
Training Epoch: 49 [9472/50048]	Loss: 0.3218
Training Epoch: 49 [9600/50048]	Loss: 0.3893
Training Epoch: 49 [9728/50048]	Loss: 0.2713
Training Epoch: 49 [9856/50048]	Loss: 0.2168
Training Epoch: 49 [9984/50048]	Loss: 0.1909
Training Epoch: 49 [10112/50048]	Loss: 0.1676
Training Epoch: 49 [10240/50048]	Loss: 0.2223
Training Epoch: 49 [10368/50048]	Loss: 0.3150
Training Epoch: 49 [10496/50048]	Loss: 0.3891
Training Epoch: 49 [10624/50048]	Loss: 0.3682
Training Epoch: 49 [10752/50048]	Loss: 0.3024
Training Epoch: 49 [10880/50048]	Loss: 0.2707
Training Epoch: 49 [11008/50048]	Loss: 0.3300
Training Epoch: 49 [11136/50048]	Loss: 0.2377
Training Epoch: 49 [11264/50048]	Loss: 0.2435
Training Epoch: 49 [11392/50048]	Loss: 0.1629
Training Epoch: 49 [11520/50048]	Loss: 0.2837
Training Epoch: 49 [11648/50048]	Loss: 0.3133
Training Epoch: 49 [11776/50048]	Loss: 0.2007
Training Epoch: 49 [11904/50048]	Loss: 0.3353
Training Epoch: 49 [12032/50048]	Loss: 0.2970
Training Epoch: 49 [12160/50048]	Loss: 0.1923
Training Epoch: 49 [12288/50048]	Loss: 0.1135
Training Epoch: 49 [12416/50048]	Loss: 0.2685
Training Epoch: 49 [12544/50048]	Loss: 0.4045
Training Epoch: 49 [12672/50048]	Loss: 0.2962
Training Epoch: 49 [12800/50048]	Loss: 0.3446
Training Epoch: 49 [12928/50048]	Loss: 0.2093
Training Epoch: 49 [13056/50048]	Loss: 0.2045
Training Epoch: 49 [13184/50048]	Loss: 0.2804
Training Epoch: 49 [13312/50048]	Loss: 0.3996
Training Epoch: 49 [13440/50048]	Loss: 0.2387
Training Epoch: 49 [13568/50048]	Loss: 0.2781
Training Epoch: 49 [13696/50048]	Loss: 0.2572
Training Epoch: 49 [13824/50048]	Loss: 0.3210
Training Epoch: 49 [13952/50048]	Loss: 0.1606
Training Epoch: 49 [14080/50048]	Loss: 0.2249
Training Epoch: 49 [14208/50048]	Loss: 0.2685
Training Epoch: 49 [14336/50048]	Loss: 0.2435
Training Epoch: 49 [14464/50048]	Loss: 0.2457
Training Epoch: 49 [14592/50048]	Loss: 0.2743
Training Epoch: 49 [14720/50048]	Loss: 0.3019
Training Epoch: 49 [14848/50048]	Loss: 0.3032
Training Epoch: 49 [14976/50048]	Loss: 0.2676
Training Epoch: 49 [15104/50048]	Loss: 0.2293
Training Epoch: 49 [15232/50048]	Loss: 0.3051
Training Epoch: 49 [15360/50048]	Loss: 0.3303
Training Epoch: 49 [15488/50048]	Loss: 0.3797
Training Epoch: 49 [15616/50048]	Loss: 0.2945
Training Epoch: 49 [15744/50048]	Loss: 0.3397
Training Epoch: 49 [15872/50048]	Loss: 0.2916
Training Epoch: 49 [16000/50048]	Loss: 0.3166
Training Epoch: 49 [16128/50048]	Loss: 0.2767
Training Epoch: 49 [16256/50048]	Loss: 0.2980
Training Epoch: 49 [16384/50048]	Loss: 0.2198
Training Epoch: 49 [16512/50048]	Loss: 0.2700
Training Epoch: 49 [16640/50048]	Loss: 0.2290
Training Epoch: 49 [16768/50048]	Loss: 0.2862
Training Epoch: 49 [16896/50048]	Loss: 0.2828
Training Epoch: 49 [17024/50048]	Loss: 0.2584
Training Epoch: 49 [17152/50048]	Loss: 0.2779
Training Epoch: 49 [17280/50048]	Loss: 0.2701
Training Epoch: 49 [17408/50048]	Loss: 0.3005
Training Epoch: 49 [17536/50048]	Loss: 0.3255
Training Epoch: 49 [17664/50048]	Loss: 0.1961
Training Epoch: 49 [17792/50048]	Loss: 0.2554
Training Epoch: 49 [17920/50048]	Loss: 0.2695
Training Epoch: 49 [18048/50048]	Loss: 0.3034
Training Epoch: 49 [18176/50048]	Loss: 0.3290
Training Epoch: 49 [18304/50048]	Loss: 0.2519
Training Epoch: 49 [18432/50048]	Loss: 0.2741
Training Epoch: 49 [18560/50048]	Loss: 0.2380
Training Epoch: 49 [18688/50048]	Loss: 0.1862
Training Epoch: 49 [18816/50048]	Loss: 0.1857
Training Epoch: 49 [18944/50048]	Loss: 0.3583
Training Epoch: 49 [19072/50048]	Loss: 0.2244
Training Epoch: 49 [19200/50048]	Loss: 0.2425
Training Epoch: 49 [19328/50048]	Loss: 0.3163
Training Epoch: 49 [19456/50048]	Loss: 0.2857
Training Epoch: 49 [19584/50048]	Loss: 0.2251
Training Epoch: 49 [19712/50048]	Loss: 0.3055
Training Epoch: 49 [19840/50048]	Loss: 0.3366
Training Epoch: 49 [19968/50048]	Loss: 0.2484
Training Epoch: 49 [20096/50048]	Loss: 0.2093
Training Epoch: 49 [20224/50048]	Loss: 0.2813
Training Epoch: 49 [20352/50048]	Loss: 0.3050
Training Epoch: 49 [20480/50048]	Loss: 0.2369
Training Epoch: 49 [20608/50048]	Loss: 0.3622
Training Epoch: 49 [20736/50048]	Loss: 0.3672
Training Epoch: 49 [20864/50048]	Loss: 0.3189
Training Epoch: 49 [20992/50048]	Loss: 0.2463
Training Epoch: 49 [21120/50048]	Loss: 0.2933
Training Epoch: 49 [21248/50048]	Loss: 0.2679
Training Epoch: 49 [21376/50048]	Loss: 0.3103
Training Epoch: 49 [21504/50048]	Loss: 0.2635
Training Epoch: 49 [21632/50048]	Loss: 0.3044
Training Epoch: 49 [21760/50048]	Loss: 0.3439
Training Epoch: 49 [21888/50048]	Loss: 0.2842
Training Epoch: 49 [22016/50048]	Loss: 0.3407
Training Epoch: 49 [22144/50048]	Loss: 0.2497
Training Epoch: 49 [22272/50048]	Loss: 0.2815
Training Epoch: 49 [22400/50048]	Loss: 0.3557
Training Epoch: 49 [22528/50048]	Loss: 0.2180
Training Epoch: 49 [22656/50048]	Loss: 0.4041
Training Epoch: 49 [22784/50048]	Loss: 0.2119
Training Epoch: 49 [22912/50048]	Loss: 0.2735
Training Epoch: 49 [23040/50048]	Loss: 0.4232
Training Epoch: 49 [23168/50048]	Loss: 0.2627
Training Epoch: 49 [23296/50048]	Loss: 0.2953
Training Epoch: 49 [23424/50048]	Loss: 0.2373
Training Epoch: 49 [23552/50048]	Loss: 0.3873
Training Epoch: 49 [23680/50048]	Loss: 0.2292
Training Epoch: 49 [23808/50048]	Loss: 0.3186
Training Epoch: 49 [23936/50048]	Loss: 0.3205
Training Epoch: 49 [24064/50048]	Loss: 0.2828
Training Epoch: 49 [24192/50048]	Loss: 0.3940
Training Epoch: 49 [24320/50048]	Loss: 0.1863
Training Epoch: 49 [24448/50048]	Loss: 0.1616
Training Epoch: 49 [24576/50048]	Loss: 0.2650
Training Epoch: 49 [24704/50048]	Loss: 0.3613
Training Epoch: 49 [24832/50048]	Loss: 0.3454
Training Epoch: 49 [24960/50048]	Loss: 0.3404
Training Epoch: 49 [25088/50048]	Loss: 0.2502
Training Epoch: 49 [25216/50048]	Loss: 0.2877
Training Epoch: 49 [25344/50048]	Loss: 0.3708
Training Epoch: 49 [25472/50048]	Loss: 0.3147
Training Epoch: 49 [25600/50048]	Loss: 0.2526
Training Epoch: 49 [25728/50048]	Loss: 0.2236
Training Epoch: 49 [25856/50048]	Loss: 0.1754
Training Epoch: 49 [25984/50048]	Loss: 0.2746
Training Epoch: 49 [26112/50048]	Loss: 0.4312
Training Epoch: 49 [26240/50048]	Loss: 0.2167
Training Epoch: 49 [26368/50048]	Loss: 0.2651
Training Epoch: 49 [26496/50048]	Loss: 0.3150
Training Epoch: 49 [26624/50048]	Loss: 0.2758
Training Epoch: 49 [26752/50048]	Loss: 0.2860
Training Epoch: 49 [26880/50048]	Loss: 0.3689
Training Epoch: 49 [27008/50048]	Loss: 0.3560
Training Epoch: 49 [27136/50048]	Loss: 0.3537
Training Epoch: 49 [27264/50048]	Loss: 0.2984
Training Epoch: 49 [27392/50048]	Loss: 0.3315
Training Epoch: 49 [27520/50048]	Loss: 0.3325
Training Epoch: 49 [27648/50048]	Loss: 0.2973
Training Epoch: 49 [27776/50048]	Loss: 0.2722
Training Epoch: 49 [27904/50048]	Loss: 0.3257
Training Epoch: 49 [28032/50048]	Loss: 0.2060
Training Epoch: 49 [28160/50048]	Loss: 0.2885
Training Epoch: 49 [28288/50048]	Loss: 0.2871
Training Epoch: 49 [28416/50048]	Loss: 0.3212
Training Epoch: 49 [28544/50048]	Loss: 0.2597
Training Epoch: 49 [28672/50048]	Loss: 0.2580
Training Epoch: 49 [28800/50048]	Loss: 0.2686
Training Epoch: 49 [28928/50048]	Loss: 0.3102
Training Epoch: 49 [29056/50048]	Loss: 0.2390
Training Epoch: 49 [29184/50048]	Loss: 0.2345
Training Epoch: 49 [29312/50048]	Loss: 0.3325
Training Epoch: 49 [29440/50048]	Loss: 0.2404
Training Epoch: 49 [29568/50048]	Loss: 0.3603
Training Epoch: 49 [29696/50048]	Loss: 0.3347
Training Epoch: 49 [29824/50048]	Loss: 0.4498
Training Epoch: 49 [29952/50048]	Loss: 0.3078
Training Epoch: 49 [30080/50048]	Loss: 0.3043
Training Epoch: 49 [30208/50048]	Loss: 0.3120
Training Epoch: 49 [30336/50048]	Loss: 0.3254
Training Epoch: 49 [30464/50048]	Loss: 0.3898
Training Epoch: 49 [30592/50048]	Loss: 0.2660
Training Epoch: 49 [30720/50048]	Loss: 0.3447
Training Epoch: 49 [30848/50048]	Loss: 0.3101
Training Epoch: 49 [30976/50048]	Loss: 0.3262
Training Epoch: 49 [31104/50048]	Loss: 0.2272
Training Epoch: 49 [31232/50048]	Loss: 0.4211
Training Epoch: 49 [31360/50048]	Loss: 0.2647
Training Epoch: 49 [31488/50048]	Loss: 0.2552
Training Epoch: 49 [31616/50048]	Loss: 0.2476
Training Epoch: 49 [31744/50048]	Loss: 0.4128
Training Epoch: 49 [31872/50048]	Loss: 0.2501
Training Epoch: 49 [32000/50048]	Loss: 0.2788
Training Epoch: 49 [32128/50048]	Loss: 0.2890
Training Epoch: 49 [32256/50048]	Loss: 0.3254
Training Epoch: 49 [32384/50048]	Loss: 0.4421
Training Epoch: 49 [32512/50048]	Loss: 0.3945
Training Epoch: 49 [32640/50048]	Loss: 0.2524
Training Epoch: 49 [32768/50048]	Loss: 0.2915
Training Epoch: 49 [32896/50048]	Loss: 0.3110
Training Epoch: 49 [33024/50048]	Loss: 0.3007
Training Epoch: 49 [33152/50048]	Loss: 0.2724
Training Epoch: 49 [33280/50048]	Loss: 0.2373
Training Epoch: 49 [33408/50048]	Loss: 0.2478
Training Epoch: 49 [33536/50048]	Loss: 0.3287
Training Epoch: 49 [33664/50048]	Loss: 0.3040
Training Epoch: 49 [33792/50048]	Loss: 0.3507
Training Epoch: 49 [33920/50048]	Loss: 0.2324
Training Epoch: 49 [34048/50048]	Loss: 0.3215
Training Epoch: 49 [34176/50048]	Loss: 0.3754
Training Epoch: 49 [34304/50048]	Loss: 0.2463
Training Epoch: 49 [34432/50048]	Loss: 0.3371
Training Epoch: 49 [34560/50048]	Loss: 0.2794
Training Epoch: 49 [34688/50048]	Loss: 0.2657
Training Epoch: 49 [34816/50048]	Loss: 0.2916
Training Epoch: 49 [34944/50048]	Loss: 0.2579
Training Epoch: 49 [35072/50048]	Loss: 0.2569
Training Epoch: 49 [35200/50048]	Loss: 0.3191
Training Epoch: 49 [35328/50048]	Loss: 0.2496
Training Epoch: 49 [35456/50048]	Loss: 0.1932
Training Epoch: 49 [35584/50048]	Loss: 0.2849
Training Epoch: 49 [35712/50048]	Loss: 0.2932
Training Epoch: 49 [35840/50048]	Loss: 0.3205
Training Epoch: 49 [35968/50048]	Loss: 0.2824
Training Epoch: 49 [36096/50048]	Loss: 0.3032
Training Epoch: 49 [36224/50048]	Loss: 0.3844
Training Epoch: 49 [36352/50048]	Loss: 0.3249
Training Epoch: 49 [36480/50048]	Loss: 0.2087
Training Epoch: 49 [36608/50048]	Loss: 0.3168
Training Epoch: 49 [36736/50048]	Loss: 0.3490
Training Epoch: 49 [36864/50048]	Loss: 0.3659
Training Epoch: 49 [36992/50048]	Loss: 0.3017
Training Epoch: 49 [37120/50048]	Loss: 0.2535
Training Epoch: 49 [37248/50048]	Loss: 0.3548
Training Epoch: 49 [37376/50048]	Loss: 0.4384
Training Epoch: 49 [37504/50048]	Loss: 0.3637
Training Epoch: 49 [37632/50048]	Loss: 0.2207
Training Epoch: 49 [37760/50048]	Loss: 0.3083
Training Epoch: 49 [37888/50048]	Loss: 0.2312
Training Epoch: 49 [38016/50048]	Loss: 0.2630
Training Epoch: 49 [38144/50048]	Loss: 0.4129
Training Epoch: 49 [38272/50048]	Loss: 0.3544
Training Epoch: 49 [38400/50048]	Loss: 0.2817
Training Epoch: 49 [38528/50048]	Loss: 0.3361
Training Epoch: 49 [38656/50048]	Loss: 0.2933
Training Epoch: 49 [38784/50048]	Loss: 0.2616
Training Epoch: 49 [38912/50048]	Loss: 0.3679
Training Epoch: 49 [39040/50048]	Loss: 0.3005
Training Epoch: 49 [39168/50048]	Loss: 0.3477
Training Epoch: 49 [39296/50048]	Loss: 0.2786
Training Epoch: 49 [39424/50048]	Loss: 0.3754
Training Epoch: 49 [39552/50048]	Loss: 0.2505
Training Epoch: 49 [39680/50048]	Loss: 0.3190
Training Epoch: 49 [39808/50048]	Loss: 0.3272
Training Epoch: 49 [39936/50048]	Loss: 0.2505
Training Epoch: 49 [40064/50048]	Loss: 0.2519
Training Epoch: 49 [40192/50048]	Loss: 0.3006
Training Epoch: 49 [40320/50048]	Loss: 0.3198
Training Epoch: 49 [40448/50048]	Loss: 0.4404
Training Epoch: 49 [40576/50048]	Loss: 0.2489
Training Epoch: 49 [40704/50048]	Loss: 0.3025
Training Epoch: 49 [40832/50048]	Loss: 0.1772
Training Epoch: 49 [40960/50048]	Loss: 0.3860
Training Epoch: 49 [41088/50048]	Loss: 0.3162
Training Epoch: 49 [41216/50048]	Loss: 0.3016
Training Epoch: 49 [41344/50048]	Loss: 0.2365
Training Epoch: 49 [41472/50048]	Loss: 0.3313
Training Epoch: 49 [41600/50048]	Loss: 0.2284
Training Epoch: 49 [41728/50048]	Loss: 0.2126
Training Epoch: 49 [41856/50048]	Loss: 0.2348
Training Epoch: 49 [41984/50048]	Loss: 0.4975
Training Epoch: 49 [42112/50048]	Loss: 0.2790
Training Epoch: 49 [42240/50048]	Loss: 0.2305
Training Epoch: 49 [42368/50048]	Loss: 0.3556
Training Epoch: 49 [42496/50048]	Loss: 0.3520
Training Epoch: 49 [42624/50048]	Loss: 0.3633
Training Epoch: 49 [42752/50048]	Loss: 0.3469
Training Epoch: 49 [42880/50048]	Loss: 0.2993
Training Epoch: 49 [43008/50048]	Loss: 0.3508
Training Epoch: 49 [43136/50048]	Loss: 0.3456
Training Epoch: 49 [43264/50048]	Loss: 0.3906
Training Epoch: 49 [43392/50048]	Loss: 0.3119
Training Epoch: 49 [43520/50048]	Loss: 0.2534
Training Epoch: 49 [43648/50048]	Loss: 0.3977
Training Epoch: 49 [43776/50048]	Loss: 0.3315
Training Epoch: 49 [43904/50048]	Loss: 0.4202
Training Epoch: 49 [44032/50048]	Loss: 0.2369
Training Epoch: 49 [44160/50048]	Loss: 0.3500
Training Epoch: 49 [44288/50048]	Loss: 0.2448
Training Epoch: 49 [44416/50048]	Loss: 0.3632
Training Epoch: 49 [44544/50048]	Loss: 0.2997
Training Epoch: 49 [44672/50048]	Loss: 0.3512
Training Epoch: 49 [44800/50048]	Loss: 0.2481
Training Epoch: 49 [44928/50048]	Loss: 0.3288
Training Epoch: 49 [45056/50048]	Loss: 0.2232
Training Epoch: 49 [45184/50048]	Loss: 0.4042
Training Epoch: 49 [45312/50048]	Loss: 0.3503
Training Epoch: 49 [45440/50048]	Loss: 0.3895
Training Epoch: 49 [45568/50048]	Loss: 0.2363
Training Epoch: 49 [45696/50048]	Loss: 0.2926
2022-12-06 07:23:01,065 [ZeusDataLoader(train)] train epoch 50 done: time=86.49 energy=10494.66
2022-12-06 07:23:01,066 [ZeusDataLoader(eval)] Epoch 50 begin.
Training Epoch: 49 [45824/50048]	Loss: 0.2484
Training Epoch: 49 [45952/50048]	Loss: 0.3080
Training Epoch: 49 [46080/50048]	Loss: 0.3949
Training Epoch: 49 [46208/50048]	Loss: 0.4725
Training Epoch: 49 [46336/50048]	Loss: 0.5121
Training Epoch: 49 [46464/50048]	Loss: 0.3792
Training Epoch: 49 [46592/50048]	Loss: 0.3459
Training Epoch: 49 [46720/50048]	Loss: 0.2468
Training Epoch: 49 [46848/50048]	Loss: 0.3220
Training Epoch: 49 [46976/50048]	Loss: 0.2873
Training Epoch: 49 [47104/50048]	Loss: 0.4301
Training Epoch: 49 [47232/50048]	Loss: 0.3386
Training Epoch: 49 [47360/50048]	Loss: 0.4444
Training Epoch: 49 [47488/50048]	Loss: 0.3433
Training Epoch: 49 [47616/50048]	Loss: 0.3075
Training Epoch: 49 [47744/50048]	Loss: 0.2054
Training Epoch: 49 [47872/50048]	Loss: 0.4537
Training Epoch: 49 [48000/50048]	Loss: 0.2527
Training Epoch: 49 [48128/50048]	Loss: 0.2149
Training Epoch: 49 [48256/50048]	Loss: 0.2507
Training Epoch: 49 [48384/50048]	Loss: 0.3171
Training Epoch: 49 [48512/50048]	Loss: 0.3959
Training Epoch: 49 [48640/50048]	Loss: 0.2312
Training Epoch: 49 [48768/50048]	Loss: 0.3446
Training Epoch: 49 [48896/50048]	Loss: 0.3665
Training Epoch: 49 [49024/50048]	Loss: 0.4003
Training Epoch: 49 [49152/50048]	Loss: 0.2743
Training Epoch: 49 [49280/50048]	Loss: 0.3353
Training Epoch: 49 [49408/50048]	Loss: 0.3427
Training Epoch: 49 [49536/50048]	Loss: 0.4171
Training Epoch: 49 [49664/50048]	Loss: 0.2962
Training Epoch: 49 [49792/50048]	Loss: 0.4460
Training Epoch: 49 [49920/50048]	Loss: 0.2520
Training Epoch: 49 [50048/50048]	Loss: 0.4043
2022-12-06 12:23:04.717 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:23:04,731 [ZeusDataLoader(eval)] eval epoch 50 done: time=3.66 energy=440.97
2022-12-06 07:23:04,731 [ZeusDataLoader(train)] Up to epoch 50: time=4510.02, energy=547438.27, cost=668345.53
2022-12-06 07:23:04,731 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:23:04,731 [ZeusDataLoader(train)] Expected next epoch: time=4599.81, energy=558236.28, cost=681601.92
2022-12-06 07:23:04,732 [ZeusDataLoader(train)] Epoch 51 begin.
Validation Epoch: 49, Average loss: 0.0150, Accuracy: 0.6299
2022-12-06 07:23:04,878 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:23:04,879 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:23:04.881 [ZeusMonitor] Monitor started.
2022-12-06 12:23:04.881 [ZeusMonitor] Running indefinitely. 2022-12-06 12:23:04.881 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:23:04.881 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e51+gpu0.power.log
Training Epoch: 50 [128/50048]	Loss: 0.2288
Training Epoch: 50 [256/50048]	Loss: 0.1576
Training Epoch: 50 [384/50048]	Loss: 0.2306
Training Epoch: 50 [512/50048]	Loss: 0.1807
Training Epoch: 50 [640/50048]	Loss: 0.2654
Training Epoch: 50 [768/50048]	Loss: 0.2718
Training Epoch: 50 [896/50048]	Loss: 0.2818
Training Epoch: 50 [1024/50048]	Loss: 0.3840
Training Epoch: 50 [1152/50048]	Loss: 0.1799
Training Epoch: 50 [1280/50048]	Loss: 0.2887
Training Epoch: 50 [1408/50048]	Loss: 0.1547
Training Epoch: 50 [1536/50048]	Loss: 0.2323
Training Epoch: 50 [1664/50048]	Loss: 0.2166
Training Epoch: 50 [1792/50048]	Loss: 0.2331
Training Epoch: 50 [1920/50048]	Loss: 0.2423
Training Epoch: 50 [2048/50048]	Loss: 0.1783
Training Epoch: 50 [2176/50048]	Loss: 0.2676
Training Epoch: 50 [2304/50048]	Loss: 0.2437
Training Epoch: 50 [2432/50048]	Loss: 0.2348
Training Epoch: 50 [2560/50048]	Loss: 0.2687
Training Epoch: 50 [2688/50048]	Loss: 0.2116
Training Epoch: 50 [2816/50048]	Loss: 0.1698
Training Epoch: 50 [2944/50048]	Loss: 0.3934
Training Epoch: 50 [3072/50048]	Loss: 0.2638
Training Epoch: 50 [3200/50048]	Loss: 0.3236
Training Epoch: 50 [3328/50048]	Loss: 0.2137
Training Epoch: 50 [3456/50048]	Loss: 0.1731
Training Epoch: 50 [3584/50048]	Loss: 0.3009
Training Epoch: 50 [3712/50048]	Loss: 0.2243
Training Epoch: 50 [3840/50048]	Loss: 0.2782
Training Epoch: 50 [3968/50048]	Loss: 0.2085
Training Epoch: 50 [4096/50048]	Loss: 0.2387
Training Epoch: 50 [4224/50048]	Loss: 0.3354
Training Epoch: 50 [4352/50048]	Loss: 0.3526
Training Epoch: 50 [4480/50048]	Loss: 0.3080
Training Epoch: 50 [4608/50048]	Loss: 0.3140
Training Epoch: 50 [4736/50048]	Loss: 0.1823
Training Epoch: 50 [4864/50048]	Loss: 0.2595
Training Epoch: 50 [4992/50048]	Loss: 0.2752
Training Epoch: 50 [5120/50048]	Loss: 0.2041
Training Epoch: 50 [5248/50048]	Loss: 0.2908
Training Epoch: 50 [5376/50048]	Loss: 0.2220
Training Epoch: 50 [5504/50048]	Loss: 0.2068
Training Epoch: 50 [5632/50048]	Loss: 0.1815
Training Epoch: 50 [5760/50048]	Loss: 0.2685
Training Epoch: 50 [5888/50048]	Loss: 0.2702
Training Epoch: 50 [6016/50048]	Loss: 0.2176
Training Epoch: 50 [6144/50048]	Loss: 0.2343
Training Epoch: 50 [6272/50048]	Loss: 0.2099
Training Epoch: 50 [6400/50048]	Loss: 0.2829
Training Epoch: 50 [6528/50048]	Loss: 0.2578
Training Epoch: 50 [6656/50048]	Loss: 0.2108
Training Epoch: 50 [6784/50048]	Loss: 0.2332
Training Epoch: 50 [6912/50048]	Loss: 0.2983
Training Epoch: 50 [7040/50048]	Loss: 0.2130
Training Epoch: 50 [7168/50048]	Loss: 0.2885
Training Epoch: 50 [7296/50048]	Loss: 0.2244
Training Epoch: 50 [7424/50048]	Loss: 0.2355
Training Epoch: 50 [7552/50048]	Loss: 0.2165
Training Epoch: 50 [7680/50048]	Loss: 0.2615
Training Epoch: 50 [7808/50048]	Loss: 0.2913
Training Epoch: 50 [7936/50048]	Loss: 0.3866
Training Epoch: 50 [8064/50048]	Loss: 0.2505
Training Epoch: 50 [8192/50048]	Loss: 0.2395
Training Epoch: 50 [8320/50048]	Loss: 0.3399
Training Epoch: 50 [8448/50048]	Loss: 0.2360
Training Epoch: 50 [8576/50048]	Loss: 0.3478
Training Epoch: 50 [8704/50048]	Loss: 0.2495
Training Epoch: 50 [8832/50048]	Loss: 0.2646
Training Epoch: 50 [8960/50048]	Loss: 0.2500
Training Epoch: 50 [9088/50048]	Loss: 0.2953
Training Epoch: 50 [9216/50048]	Loss: 0.2490
Training Epoch: 50 [9344/50048]	Loss: 0.2716
Training Epoch: 50 [9472/50048]	Loss: 0.2619
Training Epoch: 50 [9600/50048]	Loss: 0.2328
Training Epoch: 50 [9728/50048]	Loss: 0.2185
Training Epoch: 50 [9856/50048]	Loss: 0.2505
Training Epoch: 50 [9984/50048]	Loss: 0.2031
Training Epoch: 50 [10112/50048]	Loss: 0.3172
Training Epoch: 50 [10240/50048]	Loss: 0.2460
Training Epoch: 50 [10368/50048]	Loss: 0.3554
Training Epoch: 50 [10496/50048]	Loss: 0.3819
Training Epoch: 50 [10624/50048]	Loss: 0.3038
Training Epoch: 50 [10752/50048]	Loss: 0.2953
Training Epoch: 50 [10880/50048]	Loss: 0.1823
Training Epoch: 50 [11008/50048]	Loss: 0.1875
Training Epoch: 50 [11136/50048]	Loss: 0.3408
Training Epoch: 50 [11264/50048]	Loss: 0.2892
Training Epoch: 50 [11392/50048]	Loss: 0.1653
Training Epoch: 50 [11520/50048]	Loss: 0.2559
Training Epoch: 50 [11648/50048]	Loss: 0.2645
Training Epoch: 50 [11776/50048]	Loss: 0.3593
Training Epoch: 50 [11904/50048]	Loss: 0.2305
Training Epoch: 50 [12032/50048]	Loss: 0.3194
Training Epoch: 50 [12160/50048]	Loss: 0.3371
Training Epoch: 50 [12288/50048]	Loss: 0.3477
Training Epoch: 50 [12416/50048]	Loss: 0.3301
Training Epoch: 50 [12544/50048]	Loss: 0.2742
Training Epoch: 50 [12672/50048]	Loss: 0.3042
Training Epoch: 50 [12800/50048]	Loss: 0.2496
Training Epoch: 50 [12928/50048]	Loss: 0.2679
Training Epoch: 50 [13056/50048]	Loss: 0.3046
Training Epoch: 50 [13184/50048]	Loss: 0.2653
Training Epoch: 50 [13312/50048]	Loss: 0.2704
Training Epoch: 50 [13440/50048]	Loss: 0.2880
Training Epoch: 50 [13568/50048]	Loss: 0.1780
Training Epoch: 50 [13696/50048]	Loss: 0.2938
Training Epoch: 50 [13824/50048]	Loss: 0.2829
Training Epoch: 50 [13952/50048]	Loss: 0.2935
Training Epoch: 50 [14080/50048]	Loss: 0.3191
Training Epoch: 50 [14208/50048]	Loss: 0.1411
Training Epoch: 50 [14336/50048]	Loss: 0.2981
Training Epoch: 50 [14464/50048]	Loss: 0.2656
Training Epoch: 50 [14592/50048]	Loss: 0.2561
Training Epoch: 50 [14720/50048]	Loss: 0.3070
Training Epoch: 50 [14848/50048]	Loss: 0.3754
Training Epoch: 50 [14976/50048]	Loss: 0.2454
Training Epoch: 50 [15104/50048]	Loss: 0.3387
Training Epoch: 50 [15232/50048]	Loss: 0.3153
Training Epoch: 50 [15360/50048]	Loss: 0.2289
Training Epoch: 50 [15488/50048]	Loss: 0.2636
Training Epoch: 50 [15616/50048]	Loss: 0.1726
Training Epoch: 50 [15744/50048]	Loss: 0.3282
Training Epoch: 50 [15872/50048]	Loss: 0.4809
Training Epoch: 50 [16000/50048]	Loss: 0.2353
Training Epoch: 50 [16128/50048]	Loss: 0.1825
Training Epoch: 50 [16256/50048]	Loss: 0.3243
Training Epoch: 50 [16384/50048]	Loss: 0.1774
Training Epoch: 50 [16512/50048]	Loss: 0.2373
Training Epoch: 50 [16640/50048]	Loss: 0.1976
Training Epoch: 50 [16768/50048]	Loss: 0.3908
Training Epoch: 50 [16896/50048]	Loss: 0.2729
Training Epoch: 50 [17024/50048]	Loss: 0.2484
Training Epoch: 50 [17152/50048]	Loss: 0.2295
Training Epoch: 50 [17280/50048]	Loss: 0.2982
Training Epoch: 50 [17408/50048]	Loss: 0.2899
Training Epoch: 50 [17536/50048]	Loss: 0.2336
Training Epoch: 50 [17664/50048]	Loss: 0.3560
Training Epoch: 50 [17792/50048]	Loss: 0.3814
Training Epoch: 50 [17920/50048]	Loss: 0.3913
Training Epoch: 50 [18048/50048]	Loss: 0.2524
Training Epoch: 50 [18176/50048]	Loss: 0.2022
Training Epoch: 50 [18304/50048]	Loss: 0.2934
Training Epoch: 50 [18432/50048]	Loss: 0.2590
Training Epoch: 50 [18560/50048]	Loss: 0.2621
Training Epoch: 50 [18688/50048]	Loss: 0.2620
Training Epoch: 50 [18816/50048]	Loss: 0.3972
Training Epoch: 50 [18944/50048]	Loss: 0.2403
Training Epoch: 50 [19072/50048]	Loss: 0.3060
Training Epoch: 50 [19200/50048]	Loss: 0.1632
Training Epoch: 50 [19328/50048]	Loss: 0.3056
Training Epoch: 50 [19456/50048]	Loss: 0.2335
Training Epoch: 50 [19584/50048]	Loss: 0.2410
Training Epoch: 50 [19712/50048]	Loss: 0.2055
Training Epoch: 50 [19840/50048]	Loss: 0.2266
Training Epoch: 50 [19968/50048]	Loss: 0.2776
Training Epoch: 50 [20096/50048]	Loss: 0.3519
Training Epoch: 50 [20224/50048]	Loss: 0.2989
Training Epoch: 50 [20352/50048]	Loss: 0.2427
Training Epoch: 50 [20480/50048]	Loss: 0.2804
Training Epoch: 50 [20608/50048]	Loss: 0.2570
Training Epoch: 50 [20736/50048]	Loss: 0.2075
Training Epoch: 50 [20864/50048]	Loss: 0.3726
Training Epoch: 50 [20992/50048]	Loss: 0.2974
Training Epoch: 50 [21120/50048]	Loss: 0.3018
Training Epoch: 50 [21248/50048]	Loss: 0.3626
Training Epoch: 50 [21376/50048]	Loss: 0.2815
Training Epoch: 50 [21504/50048]	Loss: 0.3028
Training Epoch: 50 [21632/50048]	Loss: 0.2208
Training Epoch: 50 [21760/50048]	Loss: 0.3936
Training Epoch: 50 [21888/50048]	Loss: 0.2420
Training Epoch: 50 [22016/50048]	Loss: 0.2750
Training Epoch: 50 [22144/50048]	Loss: 0.3390
Training Epoch: 50 [22272/50048]	Loss: 0.2791
Training Epoch: 50 [22400/50048]	Loss: 0.2195
Training Epoch: 50 [22528/50048]	Loss: 0.2819
Training Epoch: 50 [22656/50048]	Loss: 0.2481
Training Epoch: 50 [22784/50048]	Loss: 0.2588
Training Epoch: 50 [22912/50048]	Loss: 0.2503
Training Epoch: 50 [23040/50048]	Loss: 0.2952
Training Epoch: 50 [23168/50048]	Loss: 0.1940
Training Epoch: 50 [23296/50048]	Loss: 0.3462
Training Epoch: 50 [23424/50048]	Loss: 0.2621
Training Epoch: 50 [23552/50048]	Loss: 0.2153
Training Epoch: 50 [23680/50048]	Loss: 0.2282
Training Epoch: 50 [23808/50048]	Loss: 0.3240
Training Epoch: 50 [23936/50048]	Loss: 0.2362
Training Epoch: 50 [24064/50048]	Loss: 0.2963
Training Epoch: 50 [24192/50048]	Loss: 0.1447
Training Epoch: 50 [24320/50048]	Loss: 0.2388
Training Epoch: 50 [24448/50048]	Loss: 0.4234
Training Epoch: 50 [24576/50048]	Loss: 0.2078
Training Epoch: 50 [24704/50048]	Loss: 0.3015
Training Epoch: 50 [24832/50048]	Loss: 0.3175
Training Epoch: 50 [24960/50048]	Loss: 0.2907
Training Epoch: 50 [25088/50048]	Loss: 0.1770
Training Epoch: 50 [25216/50048]	Loss: 0.2051
Training Epoch: 50 [25344/50048]	Loss: 0.3649
Training Epoch: 50 [25472/50048]	Loss: 0.2937
Training Epoch: 50 [25600/50048]	Loss: 0.3507
Training Epoch: 50 [25728/50048]	Loss: 0.3327
Training Epoch: 50 [25856/50048]	Loss: 0.2548
Training Epoch: 50 [25984/50048]	Loss: 0.2797
Training Epoch: 50 [26112/50048]	Loss: 0.3649
Training Epoch: 50 [26240/50048]	Loss: 0.2759
Training Epoch: 50 [26368/50048]	Loss: 0.2080
Training Epoch: 50 [26496/50048]	Loss: 0.3287
Training Epoch: 50 [26624/50048]	Loss: 0.2347
Training Epoch: 50 [26752/50048]	Loss: 0.3594
Training Epoch: 50 [26880/50048]	Loss: 0.2725
Training Epoch: 50 [27008/50048]	Loss: 0.2960
Training Epoch: 50 [27136/50048]	Loss: 0.2933
Training Epoch: 50 [27264/50048]	Loss: 0.2415
Training Epoch: 50 [27392/50048]	Loss: 0.3180
Training Epoch: 50 [27520/50048]	Loss: 0.3235
Training Epoch: 50 [27648/50048]	Loss: 0.3672
Training Epoch: 50 [27776/50048]	Loss: 0.2973
Training Epoch: 50 [27904/50048]	Loss: 0.3565
Training Epoch: 50 [28032/50048]	Loss: 0.2013
Training Epoch: 50 [28160/50048]	Loss: 0.2975
Training Epoch: 50 [28288/50048]	Loss: 0.2074
Training Epoch: 50 [28416/50048]	Loss: 0.2756
Training Epoch: 50 [28544/50048]	Loss: 0.1658
Training Epoch: 50 [28672/50048]	Loss: 0.3373
Training Epoch: 50 [28800/50048]	Loss: 0.3575
Training Epoch: 50 [28928/50048]	Loss: 0.2214
Training Epoch: 50 [29056/50048]	Loss: 0.2907
Training Epoch: 50 [29184/50048]	Loss: 0.2265
Training Epoch: 50 [29312/50048]	Loss: 0.1890
Training Epoch: 50 [29440/50048]	Loss: 0.3544
Training Epoch: 50 [29568/50048]	Loss: 0.2953
Training Epoch: 50 [29696/50048]	Loss: 0.3225
Training Epoch: 50 [29824/50048]	Loss: 0.2802
Training Epoch: 50 [29952/50048]	Loss: 0.3716
Training Epoch: 50 [30080/50048]	Loss: 0.3666
Training Epoch: 50 [30208/50048]	Loss: 0.2683
Training Epoch: 50 [30336/50048]	Loss: 0.2905
Training Epoch: 50 [30464/50048]	Loss: 0.3812
Training Epoch: 50 [30592/50048]	Loss: 0.2569
Training Epoch: 50 [30720/50048]	Loss: 0.2797
Training Epoch: 50 [30848/50048]	Loss: 0.2622
Training Epoch: 50 [30976/50048]	Loss: 0.2751
Training Epoch: 50 [31104/50048]	Loss: 0.2766
Training Epoch: 50 [31232/50048]	Loss: 0.2890
Training Epoch: 50 [31360/50048]	Loss: 0.3147
Training Epoch: 50 [31488/50048]	Loss: 0.2714
Training Epoch: 50 [31616/50048]	Loss: 0.2725
Training Epoch: 50 [31744/50048]	Loss: 0.3604
Training Epoch: 50 [31872/50048]	Loss: 0.2566
Training Epoch: 50 [32000/50048]	Loss: 0.2104
Training Epoch: 50 [32128/50048]	Loss: 0.3670
Training Epoch: 50 [32256/50048]	Loss: 0.2299
Training Epoch: 50 [32384/50048]	Loss: 0.3193
Training Epoch: 50 [32512/50048]	Loss: 0.2192
Training Epoch: 50 [32640/50048]	Loss: 0.2660
Training Epoch: 50 [32768/50048]	Loss: 0.3346
Training Epoch: 50 [32896/50048]	Loss: 0.3793
Training Epoch: 50 [33024/50048]	Loss: 0.3273
Training Epoch: 50 [33152/50048]	Loss: 0.2354
Training Epoch: 50 [33280/50048]	Loss: 0.1945
Training Epoch: 50 [33408/50048]	Loss: 0.3130
Training Epoch: 50 [33536/50048]	Loss: 0.2904
Training Epoch: 50 [33664/50048]	Loss: 0.3583
Training Epoch: 50 [33792/50048]	Loss: 0.2487
Training Epoch: 50 [33920/50048]	Loss: 0.1970
Training Epoch: 50 [34048/50048]	Loss: 0.2216
Training Epoch: 50 [34176/50048]	Loss: 0.2746
Training Epoch: 50 [34304/50048]	Loss: 0.2876
Training Epoch: 50 [34432/50048]	Loss: 0.3901
Training Epoch: 50 [34560/50048]	Loss: 0.3640
Training Epoch: 50 [34688/50048]	Loss: 0.2613
Training Epoch: 50 [34816/50048]	Loss: 0.3423
Training Epoch: 50 [34944/50048]	Loss: 0.2334
Training Epoch: 50 [35072/50048]	Loss: 0.2444
Training Epoch: 50 [35200/50048]	Loss: 0.2864
Training Epoch: 50 [35328/50048]	Loss: 0.2172
Training Epoch: 50 [35456/50048]	Loss: 0.2302
Training Epoch: 50 [35584/50048]	Loss: 0.3361
Training Epoch: 50 [35712/50048]	Loss: 0.2695
Training Epoch: 50 [35840/50048]	Loss: 0.2413
Training Epoch: 50 [35968/50048]	Loss: 0.3389
Training Epoch: 50 [36096/50048]	Loss: 0.2144
Training Epoch: 50 [36224/50048]	Loss: 0.3583
Training Epoch: 50 [36352/50048]	Loss: 0.3370
Training Epoch: 50 [36480/50048]	Loss: 0.2420
Training Epoch: 50 [36608/50048]	Loss: 0.2283
Training Epoch: 50 [36736/50048]	Loss: 0.3034
Training Epoch: 50 [36864/50048]	Loss: 0.3634
Training Epoch: 50 [36992/50048]	Loss: 0.2614
Training Epoch: 50 [37120/50048]	Loss: 0.3372
Training Epoch: 50 [37248/50048]	Loss: 0.2953
Training Epoch: 50 [37376/50048]	Loss: 0.3667
Training Epoch: 50 [37504/50048]	Loss: 0.2931
Training Epoch: 50 [37632/50048]	Loss: 0.2418
Training Epoch: 50 [37760/50048]	Loss: 0.3379
Training Epoch: 50 [37888/50048]	Loss: 0.2761
Training Epoch: 50 [38016/50048]	Loss: 0.2586
Training Epoch: 50 [38144/50048]	Loss: 0.3128
Training Epoch: 50 [38272/50048]	Loss: 0.2563
Training Epoch: 50 [38400/50048]	Loss: 0.1916
Training Epoch: 50 [38528/50048]	Loss: 0.3884
Training Epoch: 50 [38656/50048]	Loss: 0.3142
Training Epoch: 50 [38784/50048]	Loss: 0.2060
Training Epoch: 50 [38912/50048]	Loss: 0.2987
Training Epoch: 50 [39040/50048]	Loss: 0.2721
Training Epoch: 50 [39168/50048]	Loss: 0.3061
Training Epoch: 50 [39296/50048]	Loss: 0.1644
Training Epoch: 50 [39424/50048]	Loss: 0.2937
Training Epoch: 50 [39552/50048]	Loss: 0.3101
Training Epoch: 50 [39680/50048]	Loss: 0.3086
Training Epoch: 50 [39808/50048]	Loss: 0.3590
Training Epoch: 50 [39936/50048]	Loss: 0.4604
Training Epoch: 50 [40064/50048]	Loss: 0.3450
Training Epoch: 50 [40192/50048]	Loss: 0.2118
Training Epoch: 50 [40320/50048]	Loss: 0.2386
Training Epoch: 50 [40448/50048]	Loss: 0.3221
Training Epoch: 50 [40576/50048]	Loss: 0.2994
Training Epoch: 50 [40704/50048]	Loss: 0.2394
Training Epoch: 50 [40832/50048]	Loss: 0.2513
Training Epoch: 50 [40960/50048]	Loss: 0.3040
Training Epoch: 50 [41088/50048]	Loss: 0.3757
Training Epoch: 50 [41216/50048]	Loss: 0.2042
Training Epoch: 50 [41344/50048]	Loss: 0.2614
Training Epoch: 50 [41472/50048]	Loss: 0.1747
Training Epoch: 50 [41600/50048]	Loss: 0.3004
Training Epoch: 50 [41728/50048]	Loss: 0.1733
Training Epoch: 50 [41856/50048]	Loss: 0.2530
Training Epoch: 50 [41984/50048]	Loss: 0.2813
Training Epoch: 50 [42112/50048]	Loss: 0.3474
Training Epoch: 50 [42240/50048]	Loss: 0.2455
Training Epoch: 50 [42368/50048]	Loss: 0.3330
Training Epoch: 50 [42496/50048]	Loss: 0.3237
Training Epoch: 50 [42624/50048]	Loss: 0.3199
Training Epoch: 50 [42752/50048]	Loss: 0.3487
Training Epoch: 50 [42880/50048]	Loss: 0.2289
Training Epoch: 50 [43008/50048]	Loss: 0.2015
Training Epoch: 50 [43136/50048]	Loss: 0.3029
Training Epoch: 50 [43264/50048]	Loss: 0.2053
Training Epoch: 50 [43392/50048]	Loss: 0.3363
Training Epoch: 50 [43520/50048]	Loss: 0.3544
Training Epoch: 50 [43648/50048]	Loss: 0.3732
Training Epoch: 50 [43776/50048]	Loss: 0.2570
Training Epoch: 50 [43904/50048]	Loss: 0.2111
Training Epoch: 50 [44032/50048]	Loss: 0.4519
Training Epoch: 50 [44160/50048]	Loss: 0.3553
Training Epoch: 50 [44288/50048]	Loss: 0.2979
Training Epoch: 50 [44416/50048]	Loss: 0.3568
Training Epoch: 50 [44544/50048]	Loss: 0.2308
Training Epoch: 50 [44672/50048]	Loss: 0.2987
Training Epoch: 50 [44800/50048]	Loss: 0.3052
Training Epoch: 50 [44928/50048]	Loss: 0.2810
Training Epoch: 50 [45056/50048]	Loss: 0.3736
Training Epoch: 50 [45184/50048]	Loss: 0.2805
Training Epoch: 50 [45312/50048]	Loss: 0.4276
Training Epoch: 50 [45440/50048]	Loss: 0.2282
Training Epoch: 50 [45568/50048]	Loss: 0.3309
Training Epoch: 50 [45696/50048]	Loss: 0.4681
2022-12-06 07:24:31,169 [ZeusDataLoader(train)] train epoch 51 done: time=86.43 energy=10502.95
2022-12-06 07:24:31,171 [ZeusDataLoader(eval)] Epoch 51 begin.
Training Epoch: 50 [45824/50048]	Loss: 0.2657
Training Epoch: 50 [45952/50048]	Loss: 0.3392
Training Epoch: 50 [46080/50048]	Loss: 0.4336
Training Epoch: 50 [46208/50048]	Loss: 0.3049
Training Epoch: 50 [46336/50048]	Loss: 0.3216
Training Epoch: 50 [46464/50048]	Loss: 0.3987
Training Epoch: 50 [46592/50048]	Loss: 0.3845
Training Epoch: 50 [46720/50048]	Loss: 0.3530
Training Epoch: 50 [46848/50048]	Loss: 0.3860
Training Epoch: 50 [46976/50048]	Loss: 0.3329
Training Epoch: 50 [47104/50048]	Loss: 0.2822
Training Epoch: 50 [47232/50048]	Loss: 0.2490
Training Epoch: 50 [47360/50048]	Loss: 0.2843
Training Epoch: 50 [47488/50048]	Loss: 0.3613
Training Epoch: 50 [47616/50048]	Loss: 0.2586
Training Epoch: 50 [47744/50048]	Loss: 0.2554
Training Epoch: 50 [47872/50048]	Loss: 0.2339
Training Epoch: 50 [48000/50048]	Loss: 0.3761
Training Epoch: 50 [48128/50048]	Loss: 0.2870
Training Epoch: 50 [48256/50048]	Loss: 0.4173
Training Epoch: 50 [48384/50048]	Loss: 0.2793
Training Epoch: 50 [48512/50048]	Loss: 0.3042
Training Epoch: 50 [48640/50048]	Loss: 0.2721
Training Epoch: 50 [48768/50048]	Loss: 0.2890
Training Epoch: 50 [48896/50048]	Loss: 0.2494
Training Epoch: 50 [49024/50048]	Loss: 0.1979
Training Epoch: 50 [49152/50048]	Loss: 0.3281
Training Epoch: 50 [49280/50048]	Loss: 0.2558
Training Epoch: 50 [49408/50048]	Loss: 0.2467
Training Epoch: 50 [49536/50048]	Loss: 0.2431
Training Epoch: 50 [49664/50048]	Loss: 0.3319
Training Epoch: 50 [49792/50048]	Loss: 0.2938
Training Epoch: 50 [49920/50048]	Loss: 0.3013
Training Epoch: 50 [50048/50048]	Loss: 0.4624
2022-12-06 12:24:34.871 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:24:34,911 [ZeusDataLoader(eval)] eval epoch 51 done: time=3.73 energy=451.82
2022-12-06 07:24:34,911 [ZeusDataLoader(train)] Up to epoch 51: time=4600.17, energy=558393.04, cost=681711.67
2022-12-06 07:24:34,911 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:24:34,911 [ZeusDataLoader(train)] Expected next epoch: time=4689.97, energy=569191.05, cost=694968.06
2022-12-06 07:24:34,912 [ZeusDataLoader(train)] Epoch 52 begin.
Validation Epoch: 50, Average loss: 0.0146, Accuracy: 0.6311
2022-12-06 07:24:35,093 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:24:35,094 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:24:35.097 [ZeusMonitor] Monitor started.
2022-12-06 12:24:35.097 [ZeusMonitor] Running indefinitely. 2022-12-06 12:24:35.097 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:24:35.097 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e52+gpu0.power.log
Training Epoch: 51 [128/50048]	Loss: 0.1808
Training Epoch: 51 [256/50048]	Loss: 0.2389
Training Epoch: 51 [384/50048]	Loss: 0.1309
Training Epoch: 51 [512/50048]	Loss: 0.2182
Training Epoch: 51 [640/50048]	Loss: 0.2166
Training Epoch: 51 [768/50048]	Loss: 0.3475
Training Epoch: 51 [896/50048]	Loss: 0.2494
Training Epoch: 51 [1024/50048]	Loss: 0.2026
Training Epoch: 51 [1152/50048]	Loss: 0.2498
Training Epoch: 51 [1280/50048]	Loss: 0.2426
Training Epoch: 51 [1408/50048]	Loss: 0.2629
Training Epoch: 51 [1536/50048]	Loss: 0.2098
Training Epoch: 51 [1664/50048]	Loss: 0.2940
Training Epoch: 51 [1792/50048]	Loss: 0.2204
Training Epoch: 51 [1920/50048]	Loss: 0.2564
Training Epoch: 51 [2048/50048]	Loss: 0.1729
Training Epoch: 51 [2176/50048]	Loss: 0.2605
Training Epoch: 51 [2304/50048]	Loss: 0.2014
Training Epoch: 51 [2432/50048]	Loss: 0.1568
Training Epoch: 51 [2560/50048]	Loss: 0.2064
Training Epoch: 51 [2688/50048]	Loss: 0.3084
Training Epoch: 51 [2816/50048]	Loss: 0.3170
Training Epoch: 51 [2944/50048]	Loss: 0.2492
Training Epoch: 51 [3072/50048]	Loss: 0.2517
Training Epoch: 51 [3200/50048]	Loss: 0.2141
Training Epoch: 51 [3328/50048]	Loss: 0.2225
Training Epoch: 51 [3456/50048]	Loss: 0.3221
Training Epoch: 51 [3584/50048]	Loss: 0.2154
Training Epoch: 51 [3712/50048]	Loss: 0.2916
Training Epoch: 51 [3840/50048]	Loss: 0.2537
Training Epoch: 51 [3968/50048]	Loss: 0.2165
Training Epoch: 51 [4096/50048]	Loss: 0.3674
Training Epoch: 51 [4224/50048]	Loss: 0.1343
Training Epoch: 51 [4352/50048]	Loss: 0.2484
Training Epoch: 51 [4480/50048]	Loss: 0.1950
Training Epoch: 51 [4608/50048]	Loss: 0.2374
Training Epoch: 51 [4736/50048]	Loss: 0.2759
Training Epoch: 51 [4864/50048]	Loss: 0.1591
Training Epoch: 51 [4992/50048]	Loss: 0.2239
Training Epoch: 51 [5120/50048]	Loss: 0.1747
Training Epoch: 51 [5248/50048]	Loss: 0.3269
Training Epoch: 51 [5376/50048]	Loss: 0.2168
Training Epoch: 51 [5504/50048]	Loss: 0.2608
Training Epoch: 51 [5632/50048]	Loss: 0.2465
Training Epoch: 51 [5760/50048]	Loss: 0.2886
Training Epoch: 51 [5888/50048]	Loss: 0.1915
Training Epoch: 51 [6016/50048]	Loss: 0.1889
Training Epoch: 51 [6144/50048]	Loss: 0.2335
Training Epoch: 51 [6272/50048]	Loss: 0.1287
Training Epoch: 51 [6400/50048]	Loss: 0.2177
Training Epoch: 51 [6528/50048]	Loss: 0.2154
Training Epoch: 51 [6656/50048]	Loss: 0.4364
Training Epoch: 51 [6784/50048]	Loss: 0.2270
Training Epoch: 51 [6912/50048]	Loss: 0.2286
Training Epoch: 51 [7040/50048]	Loss: 0.1238
Training Epoch: 51 [7168/50048]	Loss: 0.2668
Training Epoch: 51 [7296/50048]	Loss: 0.1706
Training Epoch: 51 [7424/50048]	Loss: 0.2399
Training Epoch: 51 [7552/50048]	Loss: 0.2591
Training Epoch: 51 [7680/50048]	Loss: 0.2555
Training Epoch: 51 [7808/50048]	Loss: 0.2194
Training Epoch: 51 [7936/50048]	Loss: 0.3011
Training Epoch: 51 [8064/50048]	Loss: 0.1663
Training Epoch: 51 [8192/50048]	Loss: 0.2434
Training Epoch: 51 [8320/50048]	Loss: 0.3442
Training Epoch: 51 [8448/50048]	Loss: 0.2004
Training Epoch: 51 [8576/50048]	Loss: 0.2973
Training Epoch: 51 [8704/50048]	Loss: 0.2130
Training Epoch: 51 [8832/50048]	Loss: 0.1897
Training Epoch: 51 [8960/50048]	Loss: 0.2135
Training Epoch: 51 [9088/50048]	Loss: 0.2859
Training Epoch: 51 [9216/50048]	Loss: 0.2018
Training Epoch: 51 [9344/50048]	Loss: 0.2444
Training Epoch: 51 [9472/50048]	Loss: 0.1937
Training Epoch: 51 [9600/50048]	Loss: 0.2926
Training Epoch: 51 [9728/50048]	Loss: 0.2061
Training Epoch: 51 [9856/50048]	Loss: 0.2207
Training Epoch: 51 [9984/50048]	Loss: 0.2268
Training Epoch: 51 [10112/50048]	Loss: 0.3208
Training Epoch: 51 [10240/50048]	Loss: 0.1962
Training Epoch: 51 [10368/50048]	Loss: 0.1537
Training Epoch: 51 [10496/50048]	Loss: 0.2739
Training Epoch: 51 [10624/50048]	Loss: 0.1875
Training Epoch: 51 [10752/50048]	Loss: 0.2854
Training Epoch: 51 [10880/50048]	Loss: 0.4074
Training Epoch: 51 [11008/50048]	Loss: 0.4191
Training Epoch: 51 [11136/50048]	Loss: 0.1790
Training Epoch: 51 [11264/50048]	Loss: 0.3421
Training Epoch: 51 [11392/50048]	Loss: 0.3115
Training Epoch: 51 [11520/50048]	Loss: 0.2882
Training Epoch: 51 [11648/50048]	Loss: 0.3473
Training Epoch: 51 [11776/50048]	Loss: 0.1866
Training Epoch: 51 [11904/50048]	Loss: 0.1158
Training Epoch: 51 [12032/50048]	Loss: 0.2077
Training Epoch: 51 [12160/50048]	Loss: 0.2776
Training Epoch: 51 [12288/50048]	Loss: 0.2346
Training Epoch: 51 [12416/50048]	Loss: 0.2037
Training Epoch: 51 [12544/50048]	Loss: 0.2839
Training Epoch: 51 [12672/50048]	Loss: 0.3085
Training Epoch: 51 [12800/50048]	Loss: 0.2678
Training Epoch: 51 [12928/50048]	Loss: 0.3174
Training Epoch: 51 [13056/50048]	Loss: 0.2051
Training Epoch: 51 [13184/50048]	Loss: 0.1833
Training Epoch: 51 [13312/50048]	Loss: 0.2610
Training Epoch: 51 [13440/50048]	Loss: 0.1353
Training Epoch: 51 [13568/50048]	Loss: 0.2146
Training Epoch: 51 [13696/50048]	Loss: 0.1546
Training Epoch: 51 [13824/50048]	Loss: 0.1870
Training Epoch: 51 [13952/50048]	Loss: 0.2934
Training Epoch: 51 [14080/50048]	Loss: 0.2580
Training Epoch: 51 [14208/50048]	Loss: 0.2480
Training Epoch: 51 [14336/50048]	Loss: 0.2065
Training Epoch: 51 [14464/50048]	Loss: 0.2726
Training Epoch: 51 [14592/50048]	Loss: 0.2207
Training Epoch: 51 [14720/50048]	Loss: 0.2214
Training Epoch: 51 [14848/50048]	Loss: 0.2854
Training Epoch: 51 [14976/50048]	Loss: 0.3266
Training Epoch: 51 [15104/50048]	Loss: 0.3071
Training Epoch: 51 [15232/50048]	Loss: 0.3469
Training Epoch: 51 [15360/50048]	Loss: 0.2471
Training Epoch: 51 [15488/50048]	Loss: 0.2009
Training Epoch: 51 [15616/50048]	Loss: 0.3042
Training Epoch: 51 [15744/50048]	Loss: 0.1247
Training Epoch: 51 [15872/50048]	Loss: 0.2364
Training Epoch: 51 [16000/50048]	Loss: 0.2734
Training Epoch: 51 [16128/50048]	Loss: 0.2371
Training Epoch: 51 [16256/50048]	Loss: 0.3446
Training Epoch: 51 [16384/50048]	Loss: 0.2474
Training Epoch: 51 [16512/50048]	Loss: 0.3018
Training Epoch: 51 [16640/50048]	Loss: 0.2447
Training Epoch: 51 [16768/50048]	Loss: 0.2060
Training Epoch: 51 [16896/50048]	Loss: 0.4469
Training Epoch: 51 [17024/50048]	Loss: 0.2692
Training Epoch: 51 [17152/50048]	Loss: 0.3321
Training Epoch: 51 [17280/50048]	Loss: 0.2273
Training Epoch: 51 [17408/50048]	Loss: 0.2752
Training Epoch: 51 [17536/50048]	Loss: 0.3413
Training Epoch: 51 [17664/50048]	Loss: 0.2904
Training Epoch: 51 [17792/50048]	Loss: 0.1980
Training Epoch: 51 [17920/50048]	Loss: 0.3459
Training Epoch: 51 [18048/50048]	Loss: 0.4661
Training Epoch: 51 [18176/50048]	Loss: 0.1822
Training Epoch: 51 [18304/50048]	Loss: 0.2801
Training Epoch: 51 [18432/50048]	Loss: 0.3681
Training Epoch: 51 [18560/50048]	Loss: 0.3306
Training Epoch: 51 [18688/50048]	Loss: 0.3580
Training Epoch: 51 [18816/50048]	Loss: 0.1976
Training Epoch: 51 [18944/50048]	Loss: 0.2596
Training Epoch: 51 [19072/50048]	Loss: 0.2300
Training Epoch: 51 [19200/50048]	Loss: 0.2859
Training Epoch: 51 [19328/50048]	Loss: 0.3433
Training Epoch: 51 [19456/50048]	Loss: 0.2837
Training Epoch: 51 [19584/50048]	Loss: 0.1522
Training Epoch: 51 [19712/50048]	Loss: 0.2907
Training Epoch: 51 [19840/50048]	Loss: 0.3079
Training Epoch: 51 [19968/50048]	Loss: 0.2720
Training Epoch: 51 [20096/50048]	Loss: 0.1764
Training Epoch: 51 [20224/50048]	Loss: 0.2427
Training Epoch: 51 [20352/50048]	Loss: 0.2220
Training Epoch: 51 [20480/50048]	Loss: 0.2556
Training Epoch: 51 [20608/50048]	Loss: 0.2817
Training Epoch: 51 [20736/50048]	Loss: 0.2021
Training Epoch: 51 [20864/50048]	Loss: 0.1920
Training Epoch: 51 [20992/50048]	Loss: 0.2914
Training Epoch: 51 [21120/50048]	Loss: 0.4005
Training Epoch: 51 [21248/50048]	Loss: 0.2594
Training Epoch: 51 [21376/50048]	Loss: 0.2895
Training Epoch: 51 [21504/50048]	Loss: 0.2979
Training Epoch: 51 [21632/50048]	Loss: 0.3059
Training Epoch: 51 [21760/50048]	Loss: 0.2501
Training Epoch: 51 [21888/50048]	Loss: 0.2031
Training Epoch: 51 [22016/50048]	Loss: 0.1904
Training Epoch: 51 [22144/50048]	Loss: 0.2373
Training Epoch: 51 [22272/50048]	Loss: 0.2488
Training Epoch: 51 [22400/50048]	Loss: 0.2177
Training Epoch: 51 [22528/50048]	Loss: 0.2102
Training Epoch: 51 [22656/50048]	Loss: 0.2246
Training Epoch: 51 [22784/50048]	Loss: 0.3435
Training Epoch: 51 [22912/50048]	Loss: 0.2780
Training Epoch: 51 [23040/50048]	Loss: 0.2945
Training Epoch: 51 [23168/50048]	Loss: 0.2209
Training Epoch: 51 [23296/50048]	Loss: 0.3730
Training Epoch: 51 [23424/50048]	Loss: 0.2638
Training Epoch: 51 [23552/50048]	Loss: 0.4066
Training Epoch: 51 [23680/50048]	Loss: 0.2278
Training Epoch: 51 [23808/50048]	Loss: 0.3219
Training Epoch: 51 [23936/50048]	Loss: 0.1813
Training Epoch: 51 [24064/50048]	Loss: 0.3572
Training Epoch: 51 [24192/50048]	Loss: 0.2412
Training Epoch: 51 [24320/50048]	Loss: 0.2520
Training Epoch: 51 [24448/50048]	Loss: 0.1390
Training Epoch: 51 [24576/50048]	Loss: 0.2024
Training Epoch: 51 [24704/50048]	Loss: 0.2808
Training Epoch: 51 [24832/50048]	Loss: 0.2150
Training Epoch: 51 [24960/50048]	Loss: 0.2506
Training Epoch: 51 [25088/50048]	Loss: 0.2761
Training Epoch: 51 [25216/50048]	Loss: 0.3058
Training Epoch: 51 [25344/50048]	Loss: 0.2583
Training Epoch: 51 [25472/50048]	Loss: 0.2712
Training Epoch: 51 [25600/50048]	Loss: 0.2473
Training Epoch: 51 [25728/50048]	Loss: 0.2794
Training Epoch: 51 [25856/50048]	Loss: 0.2642
Training Epoch: 51 [25984/50048]	Loss: 0.2427
Training Epoch: 51 [26112/50048]	Loss: 0.3331
Training Epoch: 51 [26240/50048]	Loss: 0.2382
Training Epoch: 51 [26368/50048]	Loss: 0.3125
Training Epoch: 51 [26496/50048]	Loss: 0.2265
Training Epoch: 51 [26624/50048]	Loss: 0.3095
Training Epoch: 51 [26752/50048]	Loss: 0.3225
Training Epoch: 51 [26880/50048]	Loss: 0.2708
Training Epoch: 51 [27008/50048]	Loss: 0.3112
Training Epoch: 51 [27136/50048]	Loss: 0.2947
Training Epoch: 51 [27264/50048]	Loss: 0.3207
Training Epoch: 51 [27392/50048]	Loss: 0.2862
Training Epoch: 51 [27520/50048]	Loss: 0.3063
Training Epoch: 51 [27648/50048]	Loss: 0.3170
Training Epoch: 51 [27776/50048]	Loss: 0.2899
Training Epoch: 51 [27904/50048]	Loss: 0.3885
Training Epoch: 51 [28032/50048]	Loss: 0.3048
Training Epoch: 51 [28160/50048]	Loss: 0.2319
Training Epoch: 51 [28288/50048]	Loss: 0.2550
Training Epoch: 51 [28416/50048]	Loss: 0.2865
Training Epoch: 51 [28544/50048]	Loss: 0.3585
Training Epoch: 51 [28672/50048]	Loss: 0.1569
Training Epoch: 51 [28800/50048]	Loss: 0.2561
Training Epoch: 51 [28928/50048]	Loss: 0.1536
Training Epoch: 51 [29056/50048]	Loss: 0.3557
Training Epoch: 51 [29184/50048]	Loss: 0.2339
Training Epoch: 51 [29312/50048]	Loss: 0.2517
Training Epoch: 51 [29440/50048]	Loss: 0.2473
Training Epoch: 51 [29568/50048]	Loss: 0.2097
Training Epoch: 51 [29696/50048]	Loss: 0.2199
Training Epoch: 51 [29824/50048]	Loss: 0.2004
Training Epoch: 51 [29952/50048]	Loss: 0.3296
Training Epoch: 51 [30080/50048]	Loss: 0.2858
Training Epoch: 51 [30208/50048]	Loss: 0.1855
Training Epoch: 51 [30336/50048]	Loss: 0.3358
Training Epoch: 51 [30464/50048]	Loss: 0.2239
Training Epoch: 51 [30592/50048]	Loss: 0.2882
Training Epoch: 51 [30720/50048]	Loss: 0.2695
Training Epoch: 51 [30848/50048]	Loss: 0.2733
Training Epoch: 51 [30976/50048]	Loss: 0.3399
Training Epoch: 51 [31104/50048]	Loss: 0.2995
Training Epoch: 51 [31232/50048]	Loss: 0.2260
Training Epoch: 51 [31360/50048]	Loss: 0.3890
Training Epoch: 51 [31488/50048]	Loss: 0.3563
Training Epoch: 51 [31616/50048]	Loss: 0.4475
Training Epoch: 51 [31744/50048]	Loss: 0.2167
Training Epoch: 51 [31872/50048]	Loss: 0.3872
Training Epoch: 51 [32000/50048]	Loss: 0.2376
Training Epoch: 51 [32128/50048]	Loss: 0.3012
Training Epoch: 51 [32256/50048]	Loss: 0.1434
Training Epoch: 51 [32384/50048]	Loss: 0.2090
Training Epoch: 51 [32512/50048]	Loss: 0.2559
Training Epoch: 51 [32640/50048]	Loss: 0.2949
Training Epoch: 51 [32768/50048]	Loss: 0.2199
Training Epoch: 51 [32896/50048]	Loss: 0.3054
Training Epoch: 51 [33024/50048]	Loss: 0.2806
Training Epoch: 51 [33152/50048]	Loss: 0.3366
Training Epoch: 51 [33280/50048]	Loss: 0.3369
Training Epoch: 51 [33408/50048]	Loss: 0.3157
Training Epoch: 51 [33536/50048]	Loss: 0.3205
Training Epoch: 51 [33664/50048]	Loss: 0.3794
Training Epoch: 51 [33792/50048]	Loss: 0.2407
Training Epoch: 51 [33920/50048]	Loss: 0.2677
Training Epoch: 51 [34048/50048]	Loss: 0.3284
Training Epoch: 51 [34176/50048]	Loss: 0.2612
Training Epoch: 51 [34304/50048]	Loss: 0.1999
Training Epoch: 51 [34432/50048]	Loss: 0.2374
Training Epoch: 51 [34560/50048]	Loss: 0.3356
Training Epoch: 51 [34688/50048]	Loss: 0.3261
Training Epoch: 51 [34816/50048]	Loss: 0.2036
Training Epoch: 51 [34944/50048]	Loss: 0.2687
Training Epoch: 51 [35072/50048]	Loss: 0.1978
Training Epoch: 51 [35200/50048]	Loss: 0.1816
Training Epoch: 51 [35328/50048]	Loss: 0.1088
Training Epoch: 51 [35456/50048]	Loss: 0.2365
Training Epoch: 51 [35584/50048]	Loss: 0.3076
Training Epoch: 51 [35712/50048]	Loss: 0.3265
Training Epoch: 51 [35840/50048]	Loss: 0.2390
Training Epoch: 51 [35968/50048]	Loss: 0.4226
Training Epoch: 51 [36096/50048]	Loss: 0.2777
Training Epoch: 51 [36224/50048]	Loss: 0.3350
Training Epoch: 51 [36352/50048]	Loss: 0.2566
Training Epoch: 51 [36480/50048]	Loss: 0.2208
Training Epoch: 51 [36608/50048]	Loss: 0.2893
Training Epoch: 51 [36736/50048]	Loss: 0.2251
Training Epoch: 51 [36864/50048]	Loss: 0.2959
Training Epoch: 51 [36992/50048]	Loss: 0.2926
Training Epoch: 51 [37120/50048]	Loss: 0.3343
Training Epoch: 51 [37248/50048]	Loss: 0.3937
Training Epoch: 51 [37376/50048]	Loss: 0.2574
Training Epoch: 51 [37504/50048]	Loss: 0.1997
Training Epoch: 51 [37632/50048]	Loss: 0.3064
Training Epoch: 51 [37760/50048]	Loss: 0.3344
Training Epoch: 51 [37888/50048]	Loss: 0.2138
Training Epoch: 51 [38016/50048]	Loss: 0.3150
Training Epoch: 51 [38144/50048]	Loss: 0.2464
Training Epoch: 51 [38272/50048]	Loss: 0.2800
Training Epoch: 51 [38400/50048]	Loss: 0.1348
Training Epoch: 51 [38528/50048]	Loss: 0.3330
Training Epoch: 51 [38656/50048]	Loss: 0.2176
Training Epoch: 51 [38784/50048]	Loss: 0.2610
Training Epoch: 51 [38912/50048]	Loss: 0.3353
Training Epoch: 51 [39040/50048]	Loss: 0.3157
Training Epoch: 51 [39168/50048]	Loss: 0.2599
Training Epoch: 51 [39296/50048]	Loss: 0.3064
Training Epoch: 51 [39424/50048]	Loss: 0.3331
Training Epoch: 51 [39552/50048]	Loss: 0.3443
Training Epoch: 51 [39680/50048]	Loss: 0.3003
Training Epoch: 51 [39808/50048]	Loss: 0.3339
Training Epoch: 51 [39936/50048]	Loss: 0.2944
Training Epoch: 51 [40064/50048]	Loss: 0.2324
Training Epoch: 51 [40192/50048]	Loss: 0.2695
Training Epoch: 51 [40320/50048]	Loss: 0.3242
Training Epoch: 51 [40448/50048]	Loss: 0.2877
Training Epoch: 51 [40576/50048]	Loss: 0.2145
Training Epoch: 51 [40704/50048]	Loss: 0.3390
Training Epoch: 51 [40832/50048]	Loss: 0.2159
Training Epoch: 51 [40960/50048]	Loss: 0.2590
Training Epoch: 51 [41088/50048]	Loss: 0.3269
Training Epoch: 51 [41216/50048]	Loss: 0.3539
Training Epoch: 51 [41344/50048]	Loss: 0.3966
Training Epoch: 51 [41472/50048]	Loss: 0.3168
Training Epoch: 51 [41600/50048]	Loss: 0.2694
Training Epoch: 51 [41728/50048]	Loss: 0.2708
Training Epoch: 51 [41856/50048]	Loss: 0.2779
Training Epoch: 51 [41984/50048]	Loss: 0.2140
Training Epoch: 51 [42112/50048]	Loss: 0.2520
Training Epoch: 51 [42240/50048]	Loss: 0.3840
Training Epoch: 51 [42368/50048]	Loss: 0.1911
Training Epoch: 51 [42496/50048]	Loss: 0.2004
Training Epoch: 51 [42624/50048]	Loss: 0.2202
Training Epoch: 51 [42752/50048]	Loss: 0.2940
Training Epoch: 51 [42880/50048]	Loss: 0.1871
Training Epoch: 51 [43008/50048]	Loss: 0.2745
Training Epoch: 51 [43136/50048]	Loss: 0.2404
Training Epoch: 51 [43264/50048]	Loss: 0.2750
Training Epoch: 51 [43392/50048]	Loss: 0.4241
Training Epoch: 51 [43520/50048]	Loss: 0.2581
Training Epoch: 51 [43648/50048]	Loss: 0.2210
Training Epoch: 51 [43776/50048]	Loss: 0.2687
Training Epoch: 51 [43904/50048]	Loss: 0.2372
Training Epoch: 51 [44032/50048]	Loss: 0.2835
Training Epoch: 51 [44160/50048]	Loss: 0.3243
Training Epoch: 51 [44288/50048]	Loss: 0.3044
Training Epoch: 51 [44416/50048]	Loss: 0.2954
Training Epoch: 51 [44544/50048]	Loss: 0.2917
Training Epoch: 51 [44672/50048]	Loss: 0.3556
Training Epoch: 51 [44800/50048]	Loss: 0.3564
Training Epoch: 51 [44928/50048]	Loss: 0.3192
Training Epoch: 51 [45056/50048]	Loss: 0.3017
Training Epoch: 51 [45184/50048]	Loss: 0.2410
Training Epoch: 51 [45312/50048]	Loss: 0.2854
Training Epoch: 51 [45440/50048]	Loss: 0.2324
Training Epoch: 51 [45568/50048]	Loss: 0.3031
Training Epoch: 51 [45696/50048]	Loss: 0.3760
2022-12-06 07:26:01,428 [ZeusDataLoader(train)] train epoch 52 done: time=86.51 energy=10517.20
2022-12-06 07:26:01,429 [ZeusDataLoader(eval)] Epoch 52 begin.
Training Epoch: 51 [45824/50048]	Loss: 0.3194
Training Epoch: 51 [45952/50048]	Loss: 0.3030
Training Epoch: 51 [46080/50048]	Loss: 0.2944
Training Epoch: 51 [46208/50048]	Loss: 0.3579
Training Epoch: 51 [46336/50048]	Loss: 0.3420
Training Epoch: 51 [46464/50048]	Loss: 0.3084
Training Epoch: 51 [46592/50048]	Loss: 0.3755
Training Epoch: 51 [46720/50048]	Loss: 0.2876
Training Epoch: 51 [46848/50048]	Loss: 0.2783
Training Epoch: 51 [46976/50048]	Loss: 0.2875
Training Epoch: 51 [47104/50048]	Loss: 0.3547
Training Epoch: 51 [47232/50048]	Loss: 0.2720
Training Epoch: 51 [47360/50048]	Loss: 0.3738
Training Epoch: 51 [47488/50048]	Loss: 0.3789
Training Epoch: 51 [47616/50048]	Loss: 0.2414
Training Epoch: 51 [47744/50048]	Loss: 0.2694
Training Epoch: 51 [47872/50048]	Loss: 0.2945
Training Epoch: 51 [48000/50048]	Loss: 0.2464
Training Epoch: 51 [48128/50048]	Loss: 0.2460
Training Epoch: 51 [48256/50048]	Loss: 0.4084
Training Epoch: 51 [48384/50048]	Loss: 0.2984
Training Epoch: 51 [48512/50048]	Loss: 0.3567
Training Epoch: 51 [48640/50048]	Loss: 0.2359
Training Epoch: 51 [48768/50048]	Loss: 0.2846
Training Epoch: 51 [48896/50048]	Loss: 0.2502
Training Epoch: 51 [49024/50048]	Loss: 0.3265
Training Epoch: 51 [49152/50048]	Loss: 0.2898
Training Epoch: 51 [49280/50048]	Loss: 0.4166
Training Epoch: 51 [49408/50048]	Loss: 0.3037
Training Epoch: 51 [49536/50048]	Loss: 0.3431
Training Epoch: 51 [49664/50048]	Loss: 0.2436
Training Epoch: 51 [49792/50048]	Loss: 0.1891
Training Epoch: 51 [49920/50048]	Loss: 0.3873
Training Epoch: 51 [50048/50048]	Loss: 0.2529
2022-12-06 12:26:05.073 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:26:05,083 [ZeusDataLoader(eval)] eval epoch 52 done: time=3.64 energy=442.60
2022-12-06 07:26:05,083 [ZeusDataLoader(train)] Up to epoch 52: time=4690.32, energy=569352.83, cost=695079.66
2022-12-06 07:26:05,083 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:26:05,083 [ZeusDataLoader(train)] Expected next epoch: time=4780.12, energy=580150.84, cost=708336.04
2022-12-06 07:26:05,084 [ZeusDataLoader(train)] Epoch 53 begin.
Validation Epoch: 51, Average loss: 0.0151, Accuracy: 0.6294
2022-12-06 07:26:05,278 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:26:05,279 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:26:05.280 [ZeusMonitor] Monitor started.
2022-12-06 12:26:05.280 [ZeusMonitor] Running indefinitely. 2022-12-06 12:26:05.280 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:26:05.280 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e53+gpu0.power.log
Training Epoch: 52 [128/50048]	Loss: 0.2983
Training Epoch: 52 [256/50048]	Loss: 0.1593
Training Epoch: 52 [384/50048]	Loss: 0.2558
Training Epoch: 52 [512/50048]	Loss: 0.1465
Training Epoch: 52 [640/50048]	Loss: 0.2744
Training Epoch: 52 [768/50048]	Loss: 0.2445
Training Epoch: 52 [896/50048]	Loss: 0.2333
Training Epoch: 52 [1024/50048]	Loss: 0.2819
Training Epoch: 52 [1152/50048]	Loss: 0.2643
Training Epoch: 52 [1280/50048]	Loss: 0.2745
Training Epoch: 52 [1408/50048]	Loss: 0.2491
Training Epoch: 52 [1536/50048]	Loss: 0.2083
Training Epoch: 52 [1664/50048]	Loss: 0.1896
Training Epoch: 52 [1792/50048]	Loss: 0.1854
Training Epoch: 52 [1920/50048]	Loss: 0.2975
Training Epoch: 52 [2048/50048]	Loss: 0.1899
Training Epoch: 52 [2176/50048]	Loss: 0.2811
Training Epoch: 52 [2304/50048]	Loss: 0.1312
Training Epoch: 52 [2432/50048]	Loss: 0.1963
Training Epoch: 52 [2560/50048]	Loss: 0.2307
Training Epoch: 52 [2688/50048]	Loss: 0.3238
Training Epoch: 52 [2816/50048]	Loss: 0.2530
Training Epoch: 52 [2944/50048]	Loss: 0.2423
Training Epoch: 52 [3072/50048]	Loss: 0.1673
Training Epoch: 52 [3200/50048]	Loss: 0.1997
Training Epoch: 52 [3328/50048]	Loss: 0.1438
Training Epoch: 52 [3456/50048]	Loss: 0.2772
Training Epoch: 52 [3584/50048]	Loss: 0.2668
Training Epoch: 52 [3712/50048]	Loss: 0.1955
Training Epoch: 52 [3840/50048]	Loss: 0.2103
Training Epoch: 52 [3968/50048]	Loss: 0.3255
Training Epoch: 52 [4096/50048]	Loss: 0.2482
Training Epoch: 52 [4224/50048]	Loss: 0.3561
Training Epoch: 52 [4352/50048]	Loss: 0.3068
Training Epoch: 52 [4480/50048]	Loss: 0.2124
Training Epoch: 52 [4608/50048]	Loss: 0.2496
Training Epoch: 52 [4736/50048]	Loss: 0.1916
Training Epoch: 52 [4864/50048]	Loss: 0.1867
Training Epoch: 52 [4992/50048]	Loss: 0.1927
Training Epoch: 52 [5120/50048]	Loss: 0.3309
Training Epoch: 52 [5248/50048]	Loss: 0.2406
Training Epoch: 52 [5376/50048]	Loss: 0.2609
Training Epoch: 52 [5504/50048]	Loss: 0.1554
Training Epoch: 52 [5632/50048]	Loss: 0.2175
Training Epoch: 52 [5760/50048]	Loss: 0.2138
Training Epoch: 52 [5888/50048]	Loss: 0.2465
Training Epoch: 52 [6016/50048]	Loss: 0.2085
Training Epoch: 52 [6144/50048]	Loss: 0.2341
Training Epoch: 52 [6272/50048]	Loss: 0.2106
Training Epoch: 52 [6400/50048]	Loss: 0.2261
Training Epoch: 52 [6528/50048]	Loss: 0.2288
Training Epoch: 52 [6656/50048]	Loss: 0.1938
Training Epoch: 52 [6784/50048]	Loss: 0.2363
Training Epoch: 52 [6912/50048]	Loss: 0.1772
Training Epoch: 52 [7040/50048]	Loss: 0.2055
Training Epoch: 52 [7168/50048]	Loss: 0.1909
Training Epoch: 52 [7296/50048]	Loss: 0.2120
Training Epoch: 52 [7424/50048]	Loss: 0.2359
Training Epoch: 52 [7552/50048]	Loss: 0.2585
Training Epoch: 52 [7680/50048]	Loss: 0.1754
Training Epoch: 52 [7808/50048]	Loss: 0.2236
Training Epoch: 52 [7936/50048]	Loss: 0.2033
Training Epoch: 52 [8064/50048]	Loss: 0.3193
Training Epoch: 52 [8192/50048]	Loss: 0.2693
Training Epoch: 52 [8320/50048]	Loss: 0.3695
Training Epoch: 52 [8448/50048]	Loss: 0.2055
Training Epoch: 52 [8576/50048]	Loss: 0.2855
Training Epoch: 52 [8704/50048]	Loss: 0.1412
Training Epoch: 52 [8832/50048]	Loss: 0.2746
Training Epoch: 52 [8960/50048]	Loss: 0.2794
Training Epoch: 52 [9088/50048]	Loss: 0.2003
Training Epoch: 52 [9216/50048]	Loss: 0.3010
Training Epoch: 52 [9344/50048]	Loss: 0.2791
Training Epoch: 52 [9472/50048]	Loss: 0.1752
Training Epoch: 52 [9600/50048]	Loss: 0.2276
Training Epoch: 52 [9728/50048]	Loss: 0.2989
Training Epoch: 52 [9856/50048]	Loss: 0.2784
Training Epoch: 52 [9984/50048]	Loss: 0.2757
Training Epoch: 52 [10112/50048]	Loss: 0.3199
Training Epoch: 52 [10240/50048]	Loss: 0.3106
Training Epoch: 52 [10368/50048]	Loss: 0.1846
Training Epoch: 52 [10496/50048]	Loss: 0.2635
Training Epoch: 52 [10624/50048]	Loss: 0.1747
Training Epoch: 52 [10752/50048]	Loss: 0.1975
Training Epoch: 52 [10880/50048]	Loss: 0.2270
Training Epoch: 52 [11008/50048]	Loss: 0.2275
Training Epoch: 52 [11136/50048]	Loss: 0.2295
Training Epoch: 52 [11264/50048]	Loss: 0.3263
Training Epoch: 52 [11392/50048]	Loss: 0.1818
Training Epoch: 52 [11520/50048]	Loss: 0.1955
Training Epoch: 52 [11648/50048]	Loss: 0.2339
Training Epoch: 52 [11776/50048]	Loss: 0.2688
Training Epoch: 52 [11904/50048]	Loss: 0.3435
Training Epoch: 52 [12032/50048]	Loss: 0.2719
Training Epoch: 52 [12160/50048]	Loss: 0.2855
Training Epoch: 52 [12288/50048]	Loss: 0.2286
Training Epoch: 52 [12416/50048]	Loss: 0.2197
Training Epoch: 52 [12544/50048]	Loss: 0.1963
Training Epoch: 52 [12672/50048]	Loss: 0.2032
Training Epoch: 52 [12800/50048]	Loss: 0.2717
Training Epoch: 52 [12928/50048]	Loss: 0.2297
Training Epoch: 52 [13056/50048]	Loss: 0.2481
Training Epoch: 52 [13184/50048]	Loss: 0.1930
Training Epoch: 52 [13312/50048]	Loss: 0.2448
Training Epoch: 52 [13440/50048]	Loss: 0.2690
Training Epoch: 52 [13568/50048]	Loss: 0.2951
Training Epoch: 52 [13696/50048]	Loss: 0.3291
Training Epoch: 52 [13824/50048]	Loss: 0.1915
Training Epoch: 52 [13952/50048]	Loss: 0.2551
Training Epoch: 52 [14080/50048]	Loss: 0.2248
Training Epoch: 52 [14208/50048]	Loss: 0.2360
Training Epoch: 52 [14336/50048]	Loss: 0.2222
Training Epoch: 52 [14464/50048]	Loss: 0.1616
Training Epoch: 52 [14592/50048]	Loss: 0.3211
Training Epoch: 52 [14720/50048]	Loss: 0.2519
Training Epoch: 52 [14848/50048]	Loss: 0.3030
Training Epoch: 52 [14976/50048]	Loss: 0.2252
Training Epoch: 52 [15104/50048]	Loss: 0.2402
Training Epoch: 52 [15232/50048]	Loss: 0.3064
Training Epoch: 52 [15360/50048]	Loss: 0.2359
Training Epoch: 52 [15488/50048]	Loss: 0.1924
Training Epoch: 52 [15616/50048]	Loss: 0.2362
Training Epoch: 52 [15744/50048]	Loss: 0.2229
Training Epoch: 52 [15872/50048]	Loss: 0.3133
Training Epoch: 52 [16000/50048]	Loss: 0.3132
Training Epoch: 52 [16128/50048]	Loss: 0.2502
Training Epoch: 52 [16256/50048]	Loss: 0.2306
Training Epoch: 52 [16384/50048]	Loss: 0.2624
Training Epoch: 52 [16512/50048]	Loss: 0.2846
Training Epoch: 52 [16640/50048]	Loss: 0.1940
Training Epoch: 52 [16768/50048]	Loss: 0.1838
Training Epoch: 52 [16896/50048]	Loss: 0.4181
Training Epoch: 52 [17024/50048]	Loss: 0.2616
Training Epoch: 52 [17152/50048]	Loss: 0.2394
Training Epoch: 52 [17280/50048]	Loss: 0.2655
Training Epoch: 52 [17408/50048]	Loss: 0.2666
Training Epoch: 52 [17536/50048]	Loss: 0.2694
Training Epoch: 52 [17664/50048]	Loss: 0.3590
Training Epoch: 52 [17792/50048]	Loss: 0.2676
Training Epoch: 52 [17920/50048]	Loss: 0.2930
Training Epoch: 52 [18048/50048]	Loss: 0.2323
Training Epoch: 52 [18176/50048]	Loss: 0.2216
Training Epoch: 52 [18304/50048]	Loss: 0.1685
Training Epoch: 52 [18432/50048]	Loss: 0.3417
Training Epoch: 52 [18560/50048]	Loss: 0.1768
Training Epoch: 52 [18688/50048]	Loss: 0.3462
Training Epoch: 52 [18816/50048]	Loss: 0.1624
Training Epoch: 52 [18944/50048]	Loss: 0.2444
Training Epoch: 52 [19072/50048]	Loss: 0.2377
Training Epoch: 52 [19200/50048]	Loss: 0.2864
Training Epoch: 52 [19328/50048]	Loss: 0.1943
Training Epoch: 52 [19456/50048]	Loss: 0.2954
Training Epoch: 52 [19584/50048]	Loss: 0.1925
Training Epoch: 52 [19712/50048]	Loss: 0.3511
Training Epoch: 52 [19840/50048]	Loss: 0.2405
Training Epoch: 52 [19968/50048]	Loss: 0.1921
Training Epoch: 52 [20096/50048]	Loss: 0.2842
Training Epoch: 52 [20224/50048]	Loss: 0.3360
Training Epoch: 52 [20352/50048]	Loss: 0.3005
Training Epoch: 52 [20480/50048]	Loss: 0.2571
Training Epoch: 52 [20608/50048]	Loss: 0.3266
Training Epoch: 52 [20736/50048]	Loss: 0.1319
Training Epoch: 52 [20864/50048]	Loss: 0.2738
Training Epoch: 52 [20992/50048]	Loss: 0.3730
Training Epoch: 52 [21120/50048]	Loss: 0.3620
Training Epoch: 52 [21248/50048]	Loss: 0.3068
Training Epoch: 52 [21376/50048]	Loss: 0.2163
Training Epoch: 52 [21504/50048]	Loss: 0.2792
Training Epoch: 52 [21632/50048]	Loss: 0.2906
Training Epoch: 52 [21760/50048]	Loss: 0.3396
Training Epoch: 52 [21888/50048]	Loss: 0.2928
Training Epoch: 52 [22016/50048]	Loss: 0.1702
Training Epoch: 52 [22144/50048]	Loss: 0.2339
Training Epoch: 52 [22272/50048]	Loss: 0.2303
Training Epoch: 52 [22400/50048]	Loss: 0.2615
Training Epoch: 52 [22528/50048]	Loss: 0.3350
Training Epoch: 52 [22656/50048]	Loss: 0.2765
Training Epoch: 52 [22784/50048]	Loss: 0.2558
Training Epoch: 52 [22912/50048]	Loss: 0.2214
Training Epoch: 52 [23040/50048]	Loss: 0.2508
Training Epoch: 52 [23168/50048]	Loss: 0.2221
Training Epoch: 52 [23296/50048]	Loss: 0.0975
Training Epoch: 52 [23424/50048]	Loss: 0.2674
Training Epoch: 52 [23552/50048]	Loss: 0.2980
Training Epoch: 52 [23680/50048]	Loss: 0.2294
Training Epoch: 52 [23808/50048]	Loss: 0.2330
Training Epoch: 52 [23936/50048]	Loss: 0.3851
Training Epoch: 52 [24064/50048]	Loss: 0.3148
Training Epoch: 52 [24192/50048]	Loss: 0.1322
Training Epoch: 52 [24320/50048]	Loss: 0.3170
Training Epoch: 52 [24448/50048]	Loss: 0.2278
Training Epoch: 52 [24576/50048]	Loss: 0.3688
Training Epoch: 52 [24704/50048]	Loss: 0.2920
Training Epoch: 52 [24832/50048]	Loss: 0.2508
Training Epoch: 52 [24960/50048]	Loss: 0.3276
Training Epoch: 52 [25088/50048]	Loss: 0.2075
Training Epoch: 52 [25216/50048]	Loss: 0.2253
Training Epoch: 52 [25344/50048]	Loss: 0.2262
Training Epoch: 52 [25472/50048]	Loss: 0.3288
Training Epoch: 52 [25600/50048]	Loss: 0.3326
Training Epoch: 52 [25728/50048]	Loss: 0.3134
Training Epoch: 52 [25856/50048]	Loss: 0.2420
Training Epoch: 52 [25984/50048]	Loss: 0.2277
Training Epoch: 52 [26112/50048]	Loss: 0.2149
Training Epoch: 52 [26240/50048]	Loss: 0.4200
Training Epoch: 52 [26368/50048]	Loss: 0.3502
Training Epoch: 52 [26496/50048]	Loss: 0.2754
Training Epoch: 52 [26624/50048]	Loss: 0.2876
Training Epoch: 52 [26752/50048]	Loss: 0.1941
Training Epoch: 52 [26880/50048]	Loss: 0.3019
Training Epoch: 52 [27008/50048]	Loss: 0.3094
Training Epoch: 52 [27136/50048]	Loss: 0.3072
Training Epoch: 52 [27264/50048]	Loss: 0.3283
Training Epoch: 52 [27392/50048]	Loss: 0.2613
Training Epoch: 52 [27520/50048]	Loss: 0.2718
Training Epoch: 52 [27648/50048]	Loss: 0.2558
Training Epoch: 52 [27776/50048]	Loss: 0.2570
Training Epoch: 52 [27904/50048]	Loss: 0.1944
Training Epoch: 52 [28032/50048]	Loss: 0.3470
Training Epoch: 52 [28160/50048]	Loss: 0.1665
Training Epoch: 52 [28288/50048]	Loss: 0.2925
Training Epoch: 52 [28416/50048]	Loss: 0.3736
Training Epoch: 52 [28544/50048]	Loss: 0.2476
Training Epoch: 52 [28672/50048]	Loss: 0.2924
Training Epoch: 52 [28800/50048]	Loss: 0.1959
Training Epoch: 52 [28928/50048]	Loss: 0.1917
Training Epoch: 52 [29056/50048]	Loss: 0.1779
Training Epoch: 52 [29184/50048]	Loss: 0.1839
Training Epoch: 52 [29312/50048]	Loss: 0.2921
Training Epoch: 52 [29440/50048]	Loss: 0.2763
Training Epoch: 52 [29568/50048]	Loss: 0.3378
Training Epoch: 52 [29696/50048]	Loss: 0.1737
Training Epoch: 52 [29824/50048]	Loss: 0.2379
Training Epoch: 52 [29952/50048]	Loss: 0.2350
Training Epoch: 52 [30080/50048]	Loss: 0.1989
Training Epoch: 52 [30208/50048]	Loss: 0.1995
Training Epoch: 52 [30336/50048]	Loss: 0.2182
Training Epoch: 52 [30464/50048]	Loss: 0.2229
Training Epoch: 52 [30592/50048]	Loss: 0.2817
Training Epoch: 52 [30720/50048]	Loss: 0.1922
Training Epoch: 52 [30848/50048]	Loss: 0.2736
Training Epoch: 52 [30976/50048]	Loss: 0.2626
Training Epoch: 52 [31104/50048]	Loss: 0.2903
Training Epoch: 52 [31232/50048]	Loss: 0.2354
Training Epoch: 52 [31360/50048]	Loss: 0.1638
Training Epoch: 52 [31488/50048]	Loss: 0.1986
Training Epoch: 52 [31616/50048]	Loss: 0.2513
Training Epoch: 52 [31744/50048]	Loss: 0.3745
Training Epoch: 52 [31872/50048]	Loss: 0.2345
Training Epoch: 52 [32000/50048]	Loss: 0.3312
Training Epoch: 52 [32128/50048]	Loss: 0.2842
Training Epoch: 52 [32256/50048]	Loss: 0.2430
Training Epoch: 52 [32384/50048]	Loss: 0.2414
Training Epoch: 52 [32512/50048]	Loss: 0.1423
Training Epoch: 52 [32640/50048]	Loss: 0.3635
Training Epoch: 52 [32768/50048]	Loss: 0.2765
Training Epoch: 52 [32896/50048]	Loss: 0.2400
Training Epoch: 52 [33024/50048]	Loss: 0.4403
Training Epoch: 52 [33152/50048]	Loss: 0.1777
Training Epoch: 52 [33280/50048]	Loss: 0.2458
Training Epoch: 52 [33408/50048]	Loss: 0.2541
Training Epoch: 52 [33536/50048]	Loss: 0.3058
Training Epoch: 52 [33664/50048]	Loss: 0.2848
Training Epoch: 52 [33792/50048]	Loss: 0.1473
Training Epoch: 52 [33920/50048]	Loss: 0.2631
Training Epoch: 52 [34048/50048]	Loss: 0.2415
Training Epoch: 52 [34176/50048]	Loss: 0.1914
Training Epoch: 52 [34304/50048]	Loss: 0.2649
Training Epoch: 52 [34432/50048]	Loss: 0.2359
Training Epoch: 52 [34560/50048]	Loss: 0.4146
Training Epoch: 52 [34688/50048]	Loss: 0.2216
Training Epoch: 52 [34816/50048]	Loss: 0.1579
Training Epoch: 52 [34944/50048]	Loss: 0.3628
Training Epoch: 52 [35072/50048]	Loss: 0.2906
Training Epoch: 52 [35200/50048]	Loss: 0.2392
Training Epoch: 52 [35328/50048]	Loss: 0.2778
Training Epoch: 52 [35456/50048]	Loss: 0.2904
Training Epoch: 52 [35584/50048]	Loss: 0.2732
Training Epoch: 52 [35712/50048]	Loss: 0.1952
Training Epoch: 52 [35840/50048]	Loss: 0.2843
Training Epoch: 52 [35968/50048]	Loss: 0.3613
Training Epoch: 52 [36096/50048]	Loss: 0.1826
Training Epoch: 52 [36224/50048]	Loss: 0.3900
Training Epoch: 52 [36352/50048]	Loss: 0.2246
Training Epoch: 52 [36480/50048]	Loss: 0.2630
Training Epoch: 52 [36608/50048]	Loss: 0.2411
Training Epoch: 52 [36736/50048]	Loss: 0.2752
Training Epoch: 52 [36864/50048]	Loss: 0.2898
Training Epoch: 52 [36992/50048]	Loss: 0.2325
Training Epoch: 52 [37120/50048]	Loss: 0.2588
Training Epoch: 52 [37248/50048]	Loss: 0.1550
Training Epoch: 52 [37376/50048]	Loss: 0.3202
Training Epoch: 52 [37504/50048]	Loss: 0.2335
Training Epoch: 52 [37632/50048]	Loss: 0.2281
Training Epoch: 52 [37760/50048]	Loss: 0.2530
Training Epoch: 52 [37888/50048]	Loss: 0.2062
Training Epoch: 52 [38016/50048]	Loss: 0.2568
Training Epoch: 52 [38144/50048]	Loss: 0.3913
Training Epoch: 52 [38272/50048]	Loss: 0.3102
Training Epoch: 52 [38400/50048]	Loss: 0.2535
Training Epoch: 52 [38528/50048]	Loss: 0.2465
Training Epoch: 52 [38656/50048]	Loss: 0.3312
Training Epoch: 52 [38784/50048]	Loss: 0.2829
Training Epoch: 52 [38912/50048]	Loss: 0.2636
Training Epoch: 52 [39040/50048]	Loss: 0.2625
Training Epoch: 52 [39168/50048]	Loss: 0.1711
Training Epoch: 52 [39296/50048]	Loss: 0.4075
Training Epoch: 52 [39424/50048]	Loss: 0.2643
Training Epoch: 52 [39552/50048]	Loss: 0.3003
Training Epoch: 52 [39680/50048]	Loss: 0.2048
Training Epoch: 52 [39808/50048]	Loss: 0.3361
Training Epoch: 52 [39936/50048]	Loss: 0.3123
Training Epoch: 52 [40064/50048]	Loss: 0.2754
Training Epoch: 52 [40192/50048]	Loss: 0.2684
Training Epoch: 52 [40320/50048]	Loss: 0.1858
Training Epoch: 52 [40448/50048]	Loss: 0.2617
Training Epoch: 52 [40576/50048]	Loss: 0.2185
Training Epoch: 52 [40704/50048]	Loss: 0.1894
Training Epoch: 52 [40832/50048]	Loss: 0.4186
Training Epoch: 52 [40960/50048]	Loss: 0.2323
Training Epoch: 52 [41088/50048]	Loss: 0.1937
Training Epoch: 52 [41216/50048]	Loss: 0.3076
Training Epoch: 52 [41344/50048]	Loss: 0.2065
Training Epoch: 52 [41472/50048]	Loss: 0.2347
Training Epoch: 52 [41600/50048]	Loss: 0.1753
Training Epoch: 52 [41728/50048]	Loss: 0.2668
Training Epoch: 52 [41856/50048]	Loss: 0.2460
Training Epoch: 52 [41984/50048]	Loss: 0.2938
Training Epoch: 52 [42112/50048]	Loss: 0.2669
Training Epoch: 52 [42240/50048]	Loss: 0.3284
Training Epoch: 52 [42368/50048]	Loss: 0.2636
Training Epoch: 52 [42496/50048]	Loss: 0.2456
Training Epoch: 52 [42624/50048]	Loss: 0.2448
Training Epoch: 52 [42752/50048]	Loss: 0.3004
Training Epoch: 52 [42880/50048]	Loss: 0.2199
Training Epoch: 52 [43008/50048]	Loss: 0.1940
Training Epoch: 52 [43136/50048]	Loss: 0.2161
Training Epoch: 52 [43264/50048]	Loss: 0.2555
Training Epoch: 52 [43392/50048]	Loss: 0.4424
Training Epoch: 52 [43520/50048]	Loss: 0.2680
Training Epoch: 52 [43648/50048]	Loss: 0.1963
Training Epoch: 52 [43776/50048]	Loss: 0.4007
Training Epoch: 52 [43904/50048]	Loss: 0.3004
Training Epoch: 52 [44032/50048]	Loss: 0.2967
Training Epoch: 52 [44160/50048]	Loss: 0.3574
Training Epoch: 52 [44288/50048]	Loss: 0.2970
Training Epoch: 52 [44416/50048]	Loss: 0.2706
Training Epoch: 52 [44544/50048]	Loss: 0.2420
Training Epoch: 52 [44672/50048]	Loss: 0.2410
Training Epoch: 52 [44800/50048]	Loss: 0.2813
Training Epoch: 52 [44928/50048]	Loss: 0.3185
Training Epoch: 52 [45056/50048]	Loss: 0.2423
Training Epoch: 52 [45184/50048]	Loss: 0.2782
Training Epoch: 52 [45312/50048]	Loss: 0.4313
Training Epoch: 52 [45440/50048]	Loss: 0.2222
Training Epoch: 52 [45568/50048]	Loss: 0.3260
Training Epoch: 52 [45696/50048]	Loss: 0.3099
2022-12-06 07:27:31,519 [ZeusDataLoader(train)] train epoch 53 done: time=86.42 energy=10507.92
2022-12-06 07:27:31,521 [ZeusDataLoader(eval)] Epoch 53 begin.
Training Epoch: 52 [45824/50048]	Loss: 0.2547
Training Epoch: 52 [45952/50048]	Loss: 0.2214
Training Epoch: 52 [46080/50048]	Loss: 0.3744
Training Epoch: 52 [46208/50048]	Loss: 0.3585
Training Epoch: 52 [46336/50048]	Loss: 0.1712
Training Epoch: 52 [46464/50048]	Loss: 0.3101
Training Epoch: 52 [46592/50048]	Loss: 0.3733
Training Epoch: 52 [46720/50048]	Loss: 0.2690
Training Epoch: 52 [46848/50048]	Loss: 0.3217
Training Epoch: 52 [46976/50048]	Loss: 0.3204
Training Epoch: 52 [47104/50048]	Loss: 0.3532
Training Epoch: 52 [47232/50048]	Loss: 0.2306
Training Epoch: 52 [47360/50048]	Loss: 0.2386
Training Epoch: 52 [47488/50048]	Loss: 0.2308
Training Epoch: 52 [47616/50048]	Loss: 0.2873
Training Epoch: 52 [47744/50048]	Loss: 0.2762
Training Epoch: 52 [47872/50048]	Loss: 0.2780
Training Epoch: 52 [48000/50048]	Loss: 0.3004
Training Epoch: 52 [48128/50048]	Loss: 0.3368
Training Epoch: 52 [48256/50048]	Loss: 0.3265
Training Epoch: 52 [48384/50048]	Loss: 0.2458
Training Epoch: 52 [48512/50048]	Loss: 0.2621
Training Epoch: 52 [48640/50048]	Loss: 0.2710
Training Epoch: 52 [48768/50048]	Loss: 0.4066
Training Epoch: 52 [48896/50048]	Loss: 0.3242
Training Epoch: 52 [49024/50048]	Loss: 0.4217
Training Epoch: 52 [49152/50048]	Loss: 0.3025
Training Epoch: 52 [49280/50048]	Loss: 0.2440
Training Epoch: 52 [49408/50048]	Loss: 0.2457
Training Epoch: 52 [49536/50048]	Loss: 0.3612
Training Epoch: 52 [49664/50048]	Loss: 0.2884
Training Epoch: 52 [49792/50048]	Loss: 0.2348
Training Epoch: 52 [49920/50048]	Loss: 0.1646
Training Epoch: 52 [50048/50048]	Loss: 0.4223
2022-12-06 12:27:35.286 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:27:35,314 [ZeusDataLoader(eval)] eval epoch 53 done: time=3.78 energy=453.65
2022-12-06 07:27:35,314 [ZeusDataLoader(train)] Up to epoch 53: time=4780.53, energy=580314.39, cost=708453.70
2022-12-06 07:27:35,314 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:27:35,314 [ZeusDataLoader(train)] Expected next epoch: time=4870.33, energy=591112.41, cost=721710.08
2022-12-06 07:27:35,315 [ZeusDataLoader(train)] Epoch 54 begin.
Validation Epoch: 52, Average loss: 0.0153, Accuracy: 0.6363
2022-12-06 07:27:35,463 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:27:35,464 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:27:35.467 [ZeusMonitor] Monitor started.
2022-12-06 12:27:35.467 [ZeusMonitor] Running indefinitely. 2022-12-06 12:27:35.467 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:27:35.467 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e54+gpu0.power.log
Training Epoch: 53 [128/50048]	Loss: 0.1660
Training Epoch: 53 [256/50048]	Loss: 0.2263
Training Epoch: 53 [384/50048]	Loss: 0.2456
Training Epoch: 53 [512/50048]	Loss: 0.2487
Training Epoch: 53 [640/50048]	Loss: 0.1203
Training Epoch: 53 [768/50048]	Loss: 0.2157
Training Epoch: 53 [896/50048]	Loss: 0.2686
Training Epoch: 53 [1024/50048]	Loss: 0.2385
Training Epoch: 53 [1152/50048]	Loss: 0.2112
Training Epoch: 53 [1280/50048]	Loss: 0.2737
Training Epoch: 53 [1408/50048]	Loss: 0.1863
Training Epoch: 53 [1536/50048]	Loss: 0.1689
Training Epoch: 53 [1664/50048]	Loss: 0.2641
Training Epoch: 53 [1792/50048]	Loss: 0.2014
Training Epoch: 53 [1920/50048]	Loss: 0.2402
Training Epoch: 53 [2048/50048]	Loss: 0.1977
Training Epoch: 53 [2176/50048]	Loss: 0.1233
Training Epoch: 53 [2304/50048]	Loss: 0.2204
Training Epoch: 53 [2432/50048]	Loss: 0.2072
Training Epoch: 53 [2560/50048]	Loss: 0.2865
Training Epoch: 53 [2688/50048]	Loss: 0.2209
Training Epoch: 53 [2816/50048]	Loss: 0.2944
Training Epoch: 53 [2944/50048]	Loss: 0.3419
Training Epoch: 53 [3072/50048]	Loss: 0.2930
Training Epoch: 53 [3200/50048]	Loss: 0.2715
Training Epoch: 53 [3328/50048]	Loss: 0.1878
Training Epoch: 53 [3456/50048]	Loss: 0.2288
Training Epoch: 53 [3584/50048]	Loss: 0.2156
Training Epoch: 53 [3712/50048]	Loss: 0.1928
Training Epoch: 53 [3840/50048]	Loss: 0.2463
Training Epoch: 53 [3968/50048]	Loss: 0.2004
Training Epoch: 53 [4096/50048]	Loss: 0.2836
Training Epoch: 53 [4224/50048]	Loss: 0.1560
Training Epoch: 53 [4352/50048]	Loss: 0.1464
Training Epoch: 53 [4480/50048]	Loss: 0.2052
Training Epoch: 53 [4608/50048]	Loss: 0.2551
Training Epoch: 53 [4736/50048]	Loss: 0.1684
Training Epoch: 53 [4864/50048]	Loss: 0.2934
Training Epoch: 53 [4992/50048]	Loss: 0.2013
Training Epoch: 53 [5120/50048]	Loss: 0.1939
Training Epoch: 53 [5248/50048]	Loss: 0.1866
Training Epoch: 53 [5376/50048]	Loss: 0.1562
Training Epoch: 53 [5504/50048]	Loss: 0.1824
Training Epoch: 53 [5632/50048]	Loss: 0.2415
Training Epoch: 53 [5760/50048]	Loss: 0.2826
Training Epoch: 53 [5888/50048]	Loss: 0.2651
Training Epoch: 53 [6016/50048]	Loss: 0.2116
Training Epoch: 53 [6144/50048]	Loss: 0.3377
Training Epoch: 53 [6272/50048]	Loss: 0.2026
Training Epoch: 53 [6400/50048]	Loss: 0.3261
Training Epoch: 53 [6528/50048]	Loss: 0.3116
Training Epoch: 53 [6656/50048]	Loss: 0.1947
Training Epoch: 53 [6784/50048]	Loss: 0.2571
Training Epoch: 53 [6912/50048]	Loss: 0.1906
Training Epoch: 53 [7040/50048]	Loss: 0.1229
Training Epoch: 53 [7168/50048]	Loss: 0.2657
Training Epoch: 53 [7296/50048]	Loss: 0.2624
Training Epoch: 53 [7424/50048]	Loss: 0.1549
Training Epoch: 53 [7552/50048]	Loss: 0.1315
Training Epoch: 53 [7680/50048]	Loss: 0.1903
Training Epoch: 53 [7808/50048]	Loss: 0.2346
Training Epoch: 53 [7936/50048]	Loss: 0.1376
Training Epoch: 53 [8064/50048]	Loss: 0.2626
Training Epoch: 53 [8192/50048]	Loss: 0.2902
Training Epoch: 53 [8320/50048]	Loss: 0.2757
Training Epoch: 53 [8448/50048]	Loss: 0.1504
Training Epoch: 53 [8576/50048]	Loss: 0.1615
Training Epoch: 53 [8704/50048]	Loss: 0.1364
Training Epoch: 53 [8832/50048]	Loss: 0.4537
Training Epoch: 53 [8960/50048]	Loss: 0.2177
Training Epoch: 53 [9088/50048]	Loss: 0.1562
Training Epoch: 53 [9216/50048]	Loss: 0.1290
Training Epoch: 53 [9344/50048]	Loss: 0.2902
Training Epoch: 53 [9472/50048]	Loss: 0.2189
Training Epoch: 53 [9600/50048]	Loss: 0.3100
Training Epoch: 53 [9728/50048]	Loss: 0.2668
Training Epoch: 53 [9856/50048]	Loss: 0.1659
Training Epoch: 53 [9984/50048]	Loss: 0.1829
Training Epoch: 53 [10112/50048]	Loss: 0.2251
Training Epoch: 53 [10240/50048]	Loss: 0.1381
Training Epoch: 53 [10368/50048]	Loss: 0.2525
Training Epoch: 53 [10496/50048]	Loss: 0.2108
Training Epoch: 53 [10624/50048]	Loss: 0.1661
Training Epoch: 53 [10752/50048]	Loss: 0.1883
Training Epoch: 53 [10880/50048]	Loss: 0.1184
Training Epoch: 53 [11008/50048]	Loss: 0.3206
Training Epoch: 53 [11136/50048]	Loss: 0.1919
Training Epoch: 53 [11264/50048]	Loss: 0.1385
Training Epoch: 53 [11392/50048]	Loss: 0.2037
Training Epoch: 53 [11520/50048]	Loss: 0.2524
Training Epoch: 53 [11648/50048]	Loss: 0.1734
Training Epoch: 53 [11776/50048]	Loss: 0.2415
Training Epoch: 53 [11904/50048]	Loss: 0.2178
Training Epoch: 53 [12032/50048]	Loss: 0.2744
Training Epoch: 53 [12160/50048]	Loss: 0.2258
Training Epoch: 53 [12288/50048]	Loss: 0.2067
Training Epoch: 53 [12416/50048]	Loss: 0.2032
Training Epoch: 53 [12544/50048]	Loss: 0.2859
Training Epoch: 53 [12672/50048]	Loss: 0.2818
Training Epoch: 53 [12800/50048]	Loss: 0.1664
Training Epoch: 53 [12928/50048]	Loss: 0.3242
Training Epoch: 53 [13056/50048]	Loss: 0.2042
Training Epoch: 53 [13184/50048]	Loss: 0.1586
Training Epoch: 53 [13312/50048]	Loss: 0.2520
Training Epoch: 53 [13440/50048]	Loss: 0.2058
Training Epoch: 53 [13568/50048]	Loss: 0.2788
Training Epoch: 53 [13696/50048]	Loss: 0.2369
Training Epoch: 53 [13824/50048]	Loss: 0.2381
Training Epoch: 53 [13952/50048]	Loss: 0.2522
Training Epoch: 53 [14080/50048]	Loss: 0.3416
Training Epoch: 53 [14208/50048]	Loss: 0.2567
Training Epoch: 53 [14336/50048]	Loss: 0.2404
Training Epoch: 53 [14464/50048]	Loss: 0.1810
Training Epoch: 53 [14592/50048]	Loss: 0.2310
Training Epoch: 53 [14720/50048]	Loss: 0.2925
Training Epoch: 53 [14848/50048]	Loss: 0.2691
Training Epoch: 53 [14976/50048]	Loss: 0.2314
Training Epoch: 53 [15104/50048]	Loss: 0.3748
Training Epoch: 53 [15232/50048]	Loss: 0.1855
Training Epoch: 53 [15360/50048]	Loss: 0.1837
Training Epoch: 53 [15488/50048]	Loss: 0.3017
Training Epoch: 53 [15616/50048]	Loss: 0.2769
Training Epoch: 53 [15744/50048]	Loss: 0.2866
Training Epoch: 53 [15872/50048]	Loss: 0.3282
Training Epoch: 53 [16000/50048]	Loss: 0.2530
Training Epoch: 53 [16128/50048]	Loss: 0.2951
Training Epoch: 53 [16256/50048]	Loss: 0.2530
Training Epoch: 53 [16384/50048]	Loss: 0.2250
Training Epoch: 53 [16512/50048]	Loss: 0.2932
Training Epoch: 53 [16640/50048]	Loss: 0.2911
Training Epoch: 53 [16768/50048]	Loss: 0.2802
Training Epoch: 53 [16896/50048]	Loss: 0.2288
Training Epoch: 53 [17024/50048]	Loss: 0.2253
Training Epoch: 53 [17152/50048]	Loss: 0.2320
Training Epoch: 53 [17280/50048]	Loss: 0.2043
Training Epoch: 53 [17408/50048]	Loss: 0.2293
Training Epoch: 53 [17536/50048]	Loss: 0.3163
Training Epoch: 53 [17664/50048]	Loss: 0.2553
Training Epoch: 53 [17792/50048]	Loss: 0.1105
Training Epoch: 53 [17920/50048]	Loss: 0.2185
Training Epoch: 53 [18048/50048]	Loss: 0.1954
Training Epoch: 53 [18176/50048]	Loss: 0.1699
Training Epoch: 53 [18304/50048]	Loss: 0.2365
Training Epoch: 53 [18432/50048]	Loss: 0.0906
Training Epoch: 53 [18560/50048]	Loss: 0.2814
Training Epoch: 53 [18688/50048]	Loss: 0.2140
Training Epoch: 53 [18816/50048]	Loss: 0.2495
Training Epoch: 53 [18944/50048]	Loss: 0.2559
Training Epoch: 53 [19072/50048]	Loss: 0.1875
Training Epoch: 53 [19200/50048]	Loss: 0.1873
Training Epoch: 53 [19328/50048]	Loss: 0.1890
Training Epoch: 53 [19456/50048]	Loss: 0.2643
Training Epoch: 53 [19584/50048]	Loss: 0.2317
Training Epoch: 53 [19712/50048]	Loss: 0.1792
Training Epoch: 53 [19840/50048]	Loss: 0.2960
Training Epoch: 53 [19968/50048]	Loss: 0.2396
Training Epoch: 53 [20096/50048]	Loss: 0.2773
Training Epoch: 53 [20224/50048]	Loss: 0.2573
Training Epoch: 53 [20352/50048]	Loss: 0.2831
Training Epoch: 53 [20480/50048]	Loss: 0.2138
Training Epoch: 53 [20608/50048]	Loss: 0.2195
Training Epoch: 53 [20736/50048]	Loss: 0.1538
Training Epoch: 53 [20864/50048]	Loss: 0.1533
Training Epoch: 53 [20992/50048]	Loss: 0.1909
Training Epoch: 53 [21120/50048]	Loss: 0.3508
Training Epoch: 53 [21248/50048]	Loss: 0.1541
Training Epoch: 53 [21376/50048]	Loss: 0.2438
Training Epoch: 53 [21504/50048]	Loss: 0.2800
Training Epoch: 53 [21632/50048]	Loss: 0.2489
Training Epoch: 53 [21760/50048]	Loss: 0.2930
Training Epoch: 53 [21888/50048]	Loss: 0.3094
Training Epoch: 53 [22016/50048]	Loss: 0.1945
Training Epoch: 53 [22144/50048]	Loss: 0.1395
Training Epoch: 53 [22272/50048]	Loss: 0.1508
Training Epoch: 53 [22400/50048]	Loss: 0.1712
Training Epoch: 53 [22528/50048]	Loss: 0.2050
Training Epoch: 53 [22656/50048]	Loss: 0.2804
Training Epoch: 53 [22784/50048]	Loss: 0.1185
Training Epoch: 53 [22912/50048]	Loss: 0.2830
Training Epoch: 53 [23040/50048]	Loss: 0.1403
Training Epoch: 53 [23168/50048]	Loss: 0.2017
Training Epoch: 53 [23296/50048]	Loss: 0.2592
Training Epoch: 53 [23424/50048]	Loss: 0.2280
Training Epoch: 53 [23552/50048]	Loss: 0.1430
Training Epoch: 53 [23680/50048]	Loss: 0.2391
Training Epoch: 53 [23808/50048]	Loss: 0.3398
Training Epoch: 53 [23936/50048]	Loss: 0.2789
Training Epoch: 53 [24064/50048]	Loss: 0.3762
Training Epoch: 53 [24192/50048]	Loss: 0.3244
Training Epoch: 53 [24320/50048]	Loss: 0.2365
Training Epoch: 53 [24448/50048]	Loss: 0.2719
Training Epoch: 53 [24576/50048]	Loss: 0.2432
Training Epoch: 53 [24704/50048]	Loss: 0.2127
Training Epoch: 53 [24832/50048]	Loss: 0.2431
Training Epoch: 53 [24960/50048]	Loss: 0.2857
Training Epoch: 53 [25088/50048]	Loss: 0.1338
Training Epoch: 53 [25216/50048]	Loss: 0.3754
Training Epoch: 53 [25344/50048]	Loss: 0.1426
Training Epoch: 53 [25472/50048]	Loss: 0.2074
Training Epoch: 53 [25600/50048]	Loss: 0.1239
Training Epoch: 53 [25728/50048]	Loss: 0.1507
Training Epoch: 53 [25856/50048]	Loss: 0.1758
Training Epoch: 53 [25984/50048]	Loss: 0.2659
Training Epoch: 53 [26112/50048]	Loss: 0.2537
Training Epoch: 53 [26240/50048]	Loss: 0.1584
Training Epoch: 53 [26368/50048]	Loss: 0.1983
Training Epoch: 53 [26496/50048]	Loss: 0.2023
Training Epoch: 53 [26624/50048]	Loss: 0.2447
Training Epoch: 53 [26752/50048]	Loss: 0.1863
Training Epoch: 53 [26880/50048]	Loss: 0.2415
Training Epoch: 53 [27008/50048]	Loss: 0.1731
Training Epoch: 53 [27136/50048]	Loss: 0.2302
Training Epoch: 53 [27264/50048]	Loss: 0.4070
Training Epoch: 53 [27392/50048]	Loss: 0.2278
Training Epoch: 53 [27520/50048]	Loss: 0.3003
Training Epoch: 53 [27648/50048]	Loss: 0.2678
Training Epoch: 53 [27776/50048]	Loss: 0.2256
Training Epoch: 53 [27904/50048]	Loss: 0.2871
Training Epoch: 53 [28032/50048]	Loss: 0.1707
Training Epoch: 53 [28160/50048]	Loss: 0.1783
Training Epoch: 53 [28288/50048]	Loss: 0.1897
Training Epoch: 53 [28416/50048]	Loss: 0.1968
Training Epoch: 53 [28544/50048]	Loss: 0.2554
Training Epoch: 53 [28672/50048]	Loss: 0.3244
Training Epoch: 53 [28800/50048]	Loss: 0.2386
Training Epoch: 53 [28928/50048]	Loss: 0.2467
Training Epoch: 53 [29056/50048]	Loss: 0.1872
Training Epoch: 53 [29184/50048]	Loss: 0.2912
Training Epoch: 53 [29312/50048]	Loss: 0.1636
Training Epoch: 53 [29440/50048]	Loss: 0.1882
Training Epoch: 53 [29568/50048]	Loss: 0.3153
Training Epoch: 53 [29696/50048]	Loss: 0.2893
Training Epoch: 53 [29824/50048]	Loss: 0.3238
Training Epoch: 53 [29952/50048]	Loss: 0.2386
Training Epoch: 53 [30080/50048]	Loss: 0.2603
Training Epoch: 53 [30208/50048]	Loss: 0.2941
Training Epoch: 53 [30336/50048]	Loss: 0.3898
Training Epoch: 53 [30464/50048]	Loss: 0.2106
Training Epoch: 53 [30592/50048]	Loss: 0.2167
Training Epoch: 53 [30720/50048]	Loss: 0.2275
Training Epoch: 53 [30848/50048]	Loss: 0.2474
Training Epoch: 53 [30976/50048]	Loss: 0.3422
Training Epoch: 53 [31104/50048]	Loss: 0.2156
Training Epoch: 53 [31232/50048]	Loss: 0.3076
Training Epoch: 53 [31360/50048]	Loss: 0.2536
Training Epoch: 53 [31488/50048]	Loss: 0.2203
Training Epoch: 53 [31616/50048]	Loss: 0.2643
Training Epoch: 53 [31744/50048]	Loss: 0.3482
Training Epoch: 53 [31872/50048]	Loss: 0.1988
Training Epoch: 53 [32000/50048]	Loss: 0.2770
Training Epoch: 53 [32128/50048]	Loss: 0.1654
Training Epoch: 53 [32256/50048]	Loss: 0.3504
Training Epoch: 53 [32384/50048]	Loss: 0.1869
Training Epoch: 53 [32512/50048]	Loss: 0.2798
Training Epoch: 53 [32640/50048]	Loss: 0.3757
Training Epoch: 53 [32768/50048]	Loss: 0.3153
Training Epoch: 53 [32896/50048]	Loss: 0.2542
Training Epoch: 53 [33024/50048]	Loss: 0.2886
Training Epoch: 53 [33152/50048]	Loss: 0.2744
Training Epoch: 53 [33280/50048]	Loss: 0.2085
Training Epoch: 53 [33408/50048]	Loss: 0.1743
Training Epoch: 53 [33536/50048]	Loss: 0.1951
Training Epoch: 53 [33664/50048]	Loss: 0.2427
Training Epoch: 53 [33792/50048]	Loss: 0.2847
Training Epoch: 53 [33920/50048]	Loss: 0.2554
Training Epoch: 53 [34048/50048]	Loss: 0.2937
Training Epoch: 53 [34176/50048]	Loss: 0.2121
Training Epoch: 53 [34304/50048]	Loss: 0.2407
Training Epoch: 53 [34432/50048]	Loss: 0.3041
Training Epoch: 53 [34560/50048]	Loss: 0.2361
Training Epoch: 53 [34688/50048]	Loss: 0.3033
Training Epoch: 53 [34816/50048]	Loss: 0.2114
Training Epoch: 53 [34944/50048]	Loss: 0.2692
Training Epoch: 53 [35072/50048]	Loss: 0.2645
Training Epoch: 53 [35200/50048]	Loss: 0.4403
Training Epoch: 53 [35328/50048]	Loss: 0.3298
Training Epoch: 53 [35456/50048]	Loss: 0.2281
Training Epoch: 53 [35584/50048]	Loss: 0.1942
Training Epoch: 53 [35712/50048]	Loss: 0.2334
Training Epoch: 53 [35840/50048]	Loss: 0.3167
Training Epoch: 53 [35968/50048]	Loss: 0.3697
Training Epoch: 53 [36096/50048]	Loss: 0.2924
Training Epoch: 53 [36224/50048]	Loss: 0.3524
Training Epoch: 53 [36352/50048]	Loss: 0.2474
Training Epoch: 53 [36480/50048]	Loss: 0.2481
Training Epoch: 53 [36608/50048]	Loss: 0.1983
Training Epoch: 53 [36736/50048]	Loss: 0.2812
Training Epoch: 53 [36864/50048]	Loss: 0.1983
Training Epoch: 53 [36992/50048]	Loss: 0.3082
Training Epoch: 53 [37120/50048]	Loss: 0.2359
Training Epoch: 53 [37248/50048]	Loss: 0.2826
Training Epoch: 53 [37376/50048]	Loss: 0.3014
Training Epoch: 53 [37504/50048]	Loss: 0.2767
Training Epoch: 53 [37632/50048]	Loss: 0.2618
Training Epoch: 53 [37760/50048]	Loss: 0.3279
Training Epoch: 53 [37888/50048]	Loss: 0.3311
Training Epoch: 53 [38016/50048]	Loss: 0.3793
Training Epoch: 53 [38144/50048]	Loss: 0.3852
Training Epoch: 53 [38272/50048]	Loss: 0.2012
Training Epoch: 53 [38400/50048]	Loss: 0.2015
Training Epoch: 53 [38528/50048]	Loss: 0.3145
Training Epoch: 53 [38656/50048]	Loss: 0.3384
Training Epoch: 53 [38784/50048]	Loss: 0.3941
Training Epoch: 53 [38912/50048]	Loss: 0.2349
Training Epoch: 53 [39040/50048]	Loss: 0.2083
Training Epoch: 53 [39168/50048]	Loss: 0.4119
Training Epoch: 53 [39296/50048]	Loss: 0.2786
Training Epoch: 53 [39424/50048]	Loss: 0.2183
Training Epoch: 53 [39552/50048]	Loss: 0.3671
Training Epoch: 53 [39680/50048]	Loss: 0.2370
Training Epoch: 53 [39808/50048]	Loss: 0.2282
Training Epoch: 53 [39936/50048]	Loss: 0.3123
Training Epoch: 53 [40064/50048]	Loss: 0.2776
Training Epoch: 53 [40192/50048]	Loss: 0.2103
Training Epoch: 53 [40320/50048]	Loss: 0.2795
Training Epoch: 53 [40448/50048]	Loss: 0.1471
Training Epoch: 53 [40576/50048]	Loss: 0.3702
Training Epoch: 53 [40704/50048]	Loss: 0.2582
Training Epoch: 53 [40832/50048]	Loss: 0.2150
Training Epoch: 53 [40960/50048]	Loss: 0.2647
Training Epoch: 53 [41088/50048]	Loss: 0.2975
Training Epoch: 53 [41216/50048]	Loss: 0.2498
Training Epoch: 53 [41344/50048]	Loss: 0.2752
Training Epoch: 53 [41472/50048]	Loss: 0.4543
Training Epoch: 53 [41600/50048]	Loss: 0.2542
Training Epoch: 53 [41728/50048]	Loss: 0.3381
Training Epoch: 53 [41856/50048]	Loss: 0.2309
Training Epoch: 53 [41984/50048]	Loss: 0.2449
Training Epoch: 53 [42112/50048]	Loss: 0.2910
Training Epoch: 53 [42240/50048]	Loss: 0.1725
Training Epoch: 53 [42368/50048]	Loss: 0.2646
Training Epoch: 53 [42496/50048]	Loss: 0.2469
Training Epoch: 53 [42624/50048]	Loss: 0.3640
Training Epoch: 53 [42752/50048]	Loss: 0.2985
Training Epoch: 53 [42880/50048]	Loss: 0.2504
Training Epoch: 53 [43008/50048]	Loss: 0.2319
Training Epoch: 53 [43136/50048]	Loss: 0.2189
Training Epoch: 53 [43264/50048]	Loss: 0.2625
Training Epoch: 53 [43392/50048]	Loss: 0.4664
Training Epoch: 53 [43520/50048]	Loss: 0.3387
Training Epoch: 53 [43648/50048]	Loss: 0.2843
Training Epoch: 53 [43776/50048]	Loss: 0.3010
Training Epoch: 53 [43904/50048]	Loss: 0.2706
Training Epoch: 53 [44032/50048]	Loss: 0.1532
Training Epoch: 53 [44160/50048]	Loss: 0.2700
Training Epoch: 53 [44288/50048]	Loss: 0.2483
Training Epoch: 53 [44416/50048]	Loss: 0.3025
Training Epoch: 53 [44544/50048]	Loss: 0.3820
Training Epoch: 53 [44672/50048]	Loss: 0.2696
Training Epoch: 53 [44800/50048]	Loss: 0.3256
Training Epoch: 53 [44928/50048]	Loss: 0.2381
Training Epoch: 53 [45056/50048]	Loss: 0.3253
Training Epoch: 53 [45184/50048]	Loss: 0.2708
Training Epoch: 53 [45312/50048]	Loss: 0.3048
Training Epoch: 53 [45440/50048]	Loss: 0.2662
Training Epoch: 53 [45568/50048]	Loss: 0.3390
Training Epoch: 53 [45696/50048]	Loss: 0.2993
2022-12-06 07:29:01,723 [ZeusDataLoader(train)] train epoch 54 done: time=86.40 energy=10487.50
2022-12-06 07:29:01,724 [ZeusDataLoader(eval)] Epoch 54 begin.
Training Epoch: 53 [45824/50048]	Loss: 0.2755
Training Epoch: 53 [45952/50048]	Loss: 0.1811
Training Epoch: 53 [46080/50048]	Loss: 0.2926
Training Epoch: 53 [46208/50048]	Loss: 0.2768
Training Epoch: 53 [46336/50048]	Loss: 0.3962
Training Epoch: 53 [46464/50048]	Loss: 0.2036
Training Epoch: 53 [46592/50048]	Loss: 0.2737
Training Epoch: 53 [46720/50048]	Loss: 0.1476
Training Epoch: 53 [46848/50048]	Loss: 0.2810
Training Epoch: 53 [46976/50048]	Loss: 0.2635
Training Epoch: 53 [47104/50048]	Loss: 0.2811
Training Epoch: 53 [47232/50048]	Loss: 0.2406
Training Epoch: 53 [47360/50048]	Loss: 0.2563
Training Epoch: 53 [47488/50048]	Loss: 0.2830
Training Epoch: 53 [47616/50048]	Loss: 0.3600
Training Epoch: 53 [47744/50048]	Loss: 0.1929
Training Epoch: 53 [47872/50048]	Loss: 0.2680
Training Epoch: 53 [48000/50048]	Loss: 0.2512
Training Epoch: 53 [48128/50048]	Loss: 0.2807
Training Epoch: 53 [48256/50048]	Loss: 0.2039
Training Epoch: 53 [48384/50048]	Loss: 0.2435
Training Epoch: 53 [48512/50048]	Loss: 0.3327
Training Epoch: 53 [48640/50048]	Loss: 0.2043
Training Epoch: 53 [48768/50048]	Loss: 0.2475
Training Epoch: 53 [48896/50048]	Loss: 0.3084
Training Epoch: 53 [49024/50048]	Loss: 0.3856
Training Epoch: 53 [49152/50048]	Loss: 0.2185
Training Epoch: 53 [49280/50048]	Loss: 0.2297
Training Epoch: 53 [49408/50048]	Loss: 0.3371
Training Epoch: 53 [49536/50048]	Loss: 0.2760
Training Epoch: 53 [49664/50048]	Loss: 0.2797
Training Epoch: 53 [49792/50048]	Loss: 0.3115
Training Epoch: 53 [49920/50048]	Loss: 0.3715
Training Epoch: 53 [50048/50048]	Loss: 0.3513
2022-12-06 12:29:05.401 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:29:05,431 [ZeusDataLoader(eval)] eval epoch 54 done: time=3.70 energy=454.25
2022-12-06 07:29:05,432 [ZeusDataLoader(train)] Up to epoch 54: time=4870.63, energy=591256.15, cost=721807.90
2022-12-06 07:29:05,432 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:29:05,432 [ZeusDataLoader(train)] Expected next epoch: time=4960.43, energy=602054.16, cost=735064.28
2022-12-06 07:29:05,433 [ZeusDataLoader(train)] Epoch 55 begin.
Validation Epoch: 53, Average loss: 0.0157, Accuracy: 0.6238
2022-12-06 07:29:05,623 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:29:05,623 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:29:05.625 [ZeusMonitor] Monitor started.
2022-12-06 12:29:05.625 [ZeusMonitor] Running indefinitely. 2022-12-06 12:29:05.625 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:29:05.626 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e55+gpu0.power.log
Training Epoch: 54 [128/50048]	Loss: 0.2600
Training Epoch: 54 [256/50048]	Loss: 0.2304
Training Epoch: 54 [384/50048]	Loss: 0.2801
Training Epoch: 54 [512/50048]	Loss: 0.1597
Training Epoch: 54 [640/50048]	Loss: 0.2649
Training Epoch: 54 [768/50048]	Loss: 0.3184
Training Epoch: 54 [896/50048]	Loss: 0.2947
Training Epoch: 54 [1024/50048]	Loss: 0.2185
Training Epoch: 54 [1152/50048]	Loss: 0.3080
Training Epoch: 54 [1280/50048]	Loss: 0.2807
Training Epoch: 54 [1408/50048]	Loss: 0.1973
Training Epoch: 54 [1536/50048]	Loss: 0.1914
Training Epoch: 54 [1664/50048]	Loss: 0.1671
Training Epoch: 54 [1792/50048]	Loss: 0.1804
Training Epoch: 54 [1920/50048]	Loss: 0.1866
Training Epoch: 54 [2048/50048]	Loss: 0.3400
Training Epoch: 54 [2176/50048]	Loss: 0.3474
Training Epoch: 54 [2304/50048]	Loss: 0.1868
Training Epoch: 54 [2432/50048]	Loss: 0.1688
Training Epoch: 54 [2560/50048]	Loss: 0.2699
Training Epoch: 54 [2688/50048]	Loss: 0.2270
Training Epoch: 54 [2816/50048]	Loss: 0.1730
Training Epoch: 54 [2944/50048]	Loss: 0.2383
Training Epoch: 54 [3072/50048]	Loss: 0.3098
Training Epoch: 54 [3200/50048]	Loss: 0.2041
Training Epoch: 54 [3328/50048]	Loss: 0.1865
Training Epoch: 54 [3456/50048]	Loss: 0.2902
Training Epoch: 54 [3584/50048]	Loss: 0.1486
Training Epoch: 54 [3712/50048]	Loss: 0.2404
Training Epoch: 54 [3840/50048]	Loss: 0.3066
Training Epoch: 54 [3968/50048]	Loss: 0.2842
Training Epoch: 54 [4096/50048]	Loss: 0.1179
Training Epoch: 54 [4224/50048]	Loss: 0.2552
Training Epoch: 54 [4352/50048]	Loss: 0.2776
Training Epoch: 54 [4480/50048]	Loss: 0.2886
Training Epoch: 54 [4608/50048]	Loss: 0.3104
Training Epoch: 54 [4736/50048]	Loss: 0.1832
Training Epoch: 54 [4864/50048]	Loss: 0.2303
Training Epoch: 54 [4992/50048]	Loss: 0.2047
Training Epoch: 54 [5120/50048]	Loss: 0.2387
Training Epoch: 54 [5248/50048]	Loss: 0.2340
Training Epoch: 54 [5376/50048]	Loss: 0.1689
Training Epoch: 54 [5504/50048]	Loss: 0.3084
Training Epoch: 54 [5632/50048]	Loss: 0.1993
Training Epoch: 54 [5760/50048]	Loss: 0.1370
Training Epoch: 54 [5888/50048]	Loss: 0.1873
Training Epoch: 54 [6016/50048]	Loss: 0.2642
Training Epoch: 54 [6144/50048]	Loss: 0.2337
Training Epoch: 54 [6272/50048]	Loss: 0.2391
Training Epoch: 54 [6400/50048]	Loss: 0.1819
Training Epoch: 54 [6528/50048]	Loss: 0.2421
Training Epoch: 54 [6656/50048]	Loss: 0.2790
Training Epoch: 54 [6784/50048]	Loss: 0.1596
Training Epoch: 54 [6912/50048]	Loss: 0.1774
Training Epoch: 54 [7040/50048]	Loss: 0.2321
Training Epoch: 54 [7168/50048]	Loss: 0.2164
Training Epoch: 54 [7296/50048]	Loss: 0.2802
Training Epoch: 54 [7424/50048]	Loss: 0.2054
Training Epoch: 54 [7552/50048]	Loss: 0.3061
Training Epoch: 54 [7680/50048]	Loss: 0.2166
Training Epoch: 54 [7808/50048]	Loss: 0.2713
Training Epoch: 54 [7936/50048]	Loss: 0.2274
Training Epoch: 54 [8064/50048]	Loss: 0.2769
Training Epoch: 54 [8192/50048]	Loss: 0.2145
Training Epoch: 54 [8320/50048]	Loss: 0.2691
Training Epoch: 54 [8448/50048]	Loss: 0.2380
Training Epoch: 54 [8576/50048]	Loss: 0.1806
Training Epoch: 54 [8704/50048]	Loss: 0.2440
Training Epoch: 54 [8832/50048]	Loss: 0.2644
Training Epoch: 54 [8960/50048]	Loss: 0.1978
Training Epoch: 54 [9088/50048]	Loss: 0.2653
Training Epoch: 54 [9216/50048]	Loss: 0.1575
Training Epoch: 54 [9344/50048]	Loss: 0.2710
Training Epoch: 54 [9472/50048]	Loss: 0.2160
Training Epoch: 54 [9600/50048]	Loss: 0.2441
Training Epoch: 54 [9728/50048]	Loss: 0.1632
Training Epoch: 54 [9856/50048]	Loss: 0.1683
Training Epoch: 54 [9984/50048]	Loss: 0.2392
Training Epoch: 54 [10112/50048]	Loss: 0.3479
Training Epoch: 54 [10240/50048]	Loss: 0.1035
Training Epoch: 54 [10368/50048]	Loss: 0.2146
Training Epoch: 54 [10496/50048]	Loss: 0.2529
Training Epoch: 54 [10624/50048]	Loss: 0.1905
Training Epoch: 54 [10752/50048]	Loss: 0.1775
Training Epoch: 54 [10880/50048]	Loss: 0.1694
Training Epoch: 54 [11008/50048]	Loss: 0.2766
Training Epoch: 54 [11136/50048]	Loss: 0.1497
Training Epoch: 54 [11264/50048]	Loss: 0.2099
Training Epoch: 54 [11392/50048]	Loss: 0.1853
Training Epoch: 54 [11520/50048]	Loss: 0.2057
Training Epoch: 54 [11648/50048]	Loss: 0.2216
Training Epoch: 54 [11776/50048]	Loss: 0.1835
Training Epoch: 54 [11904/50048]	Loss: 0.2493
Training Epoch: 54 [12032/50048]	Loss: 0.2291
Training Epoch: 54 [12160/50048]	Loss: 0.1594
Training Epoch: 54 [12288/50048]	Loss: 0.2368
Training Epoch: 54 [12416/50048]	Loss: 0.1934
Training Epoch: 54 [12544/50048]	Loss: 0.1148
Training Epoch: 54 [12672/50048]	Loss: 0.2327
Training Epoch: 54 [12800/50048]	Loss: 0.1544
Training Epoch: 54 [12928/50048]	Loss: 0.2604
Training Epoch: 54 [13056/50048]	Loss: 0.2591
Training Epoch: 54 [13184/50048]	Loss: 0.1890
Training Epoch: 54 [13312/50048]	Loss: 0.2834
Training Epoch: 54 [13440/50048]	Loss: 0.4023
Training Epoch: 54 [13568/50048]	Loss: 0.1661
Training Epoch: 54 [13696/50048]	Loss: 0.2472
Training Epoch: 54 [13824/50048]	Loss: 0.1530
Training Epoch: 54 [13952/50048]	Loss: 0.1543
Training Epoch: 54 [14080/50048]	Loss: 0.3341
Training Epoch: 54 [14208/50048]	Loss: 0.2385
Training Epoch: 54 [14336/50048]	Loss: 0.2705
Training Epoch: 54 [14464/50048]	Loss: 0.1831
Training Epoch: 54 [14592/50048]	Loss: 0.1852
Training Epoch: 54 [14720/50048]	Loss: 0.3274
Training Epoch: 54 [14848/50048]	Loss: 0.2605
Training Epoch: 54 [14976/50048]	Loss: 0.1867
Training Epoch: 54 [15104/50048]	Loss: 0.3239
Training Epoch: 54 [15232/50048]	Loss: 0.2065
Training Epoch: 54 [15360/50048]	Loss: 0.3265
Training Epoch: 54 [15488/50048]	Loss: 0.1472
Training Epoch: 54 [15616/50048]	Loss: 0.2534
Training Epoch: 54 [15744/50048]	Loss: 0.2944
Training Epoch: 54 [15872/50048]	Loss: 0.2031
Training Epoch: 54 [16000/50048]	Loss: 0.2714
Training Epoch: 54 [16128/50048]	Loss: 0.2425
Training Epoch: 54 [16256/50048]	Loss: 0.1874
Training Epoch: 54 [16384/50048]	Loss: 0.2410
Training Epoch: 54 [16512/50048]	Loss: 0.2782
Training Epoch: 54 [16640/50048]	Loss: 0.1791
Training Epoch: 54 [16768/50048]	Loss: 0.4029
Training Epoch: 54 [16896/50048]	Loss: 0.2279
Training Epoch: 54 [17024/50048]	Loss: 0.1539
Training Epoch: 54 [17152/50048]	Loss: 0.1730
Training Epoch: 54 [17280/50048]	Loss: 0.2010
Training Epoch: 54 [17408/50048]	Loss: 0.1602
Training Epoch: 54 [17536/50048]	Loss: 0.1176
Training Epoch: 54 [17664/50048]	Loss: 0.2412
Training Epoch: 54 [17792/50048]	Loss: 0.2566
Training Epoch: 54 [17920/50048]	Loss: 0.2189
Training Epoch: 54 [18048/50048]	Loss: 0.1987
Training Epoch: 54 [18176/50048]	Loss: 0.2970
Training Epoch: 54 [18304/50048]	Loss: 0.2519
Training Epoch: 54 [18432/50048]	Loss: 0.3336
Training Epoch: 54 [18560/50048]	Loss: 0.2244
Training Epoch: 54 [18688/50048]	Loss: 0.3607
Training Epoch: 54 [18816/50048]	Loss: 0.2038
Training Epoch: 54 [18944/50048]	Loss: 0.2619
Training Epoch: 54 [19072/50048]	Loss: 0.2298
Training Epoch: 54 [19200/50048]	Loss: 0.1995
Training Epoch: 54 [19328/50048]	Loss: 0.3183
Training Epoch: 54 [19456/50048]	Loss: 0.3136
Training Epoch: 54 [19584/50048]	Loss: 0.1844
Training Epoch: 54 [19712/50048]	Loss: 0.2301
Training Epoch: 54 [19840/50048]	Loss: 0.2786
Training Epoch: 54 [19968/50048]	Loss: 0.2676
Training Epoch: 54 [20096/50048]	Loss: 0.2948
Training Epoch: 54 [20224/50048]	Loss: 0.1179
Training Epoch: 54 [20352/50048]	Loss: 0.1962
Training Epoch: 54 [20480/50048]	Loss: 0.2968
Training Epoch: 54 [20608/50048]	Loss: 0.2184
Training Epoch: 54 [20736/50048]	Loss: 0.1915
Training Epoch: 54 [20864/50048]	Loss: 0.2092
Training Epoch: 54 [20992/50048]	Loss: 0.1709
Training Epoch: 54 [21120/50048]	Loss: 0.2418
Training Epoch: 54 [21248/50048]	Loss: 0.2260
Training Epoch: 54 [21376/50048]	Loss: 0.2835
Training Epoch: 54 [21504/50048]	Loss: 0.1591
Training Epoch: 54 [21632/50048]	Loss: 0.3010
Training Epoch: 54 [21760/50048]	Loss: 0.2020
Training Epoch: 54 [21888/50048]	Loss: 0.1662
Training Epoch: 54 [22016/50048]	Loss: 0.1898
Training Epoch: 54 [22144/50048]	Loss: 0.2650
Training Epoch: 54 [22272/50048]	Loss: 0.2984
Training Epoch: 54 [22400/50048]	Loss: 0.2397
Training Epoch: 54 [22528/50048]	Loss: 0.2012
Training Epoch: 54 [22656/50048]	Loss: 0.3045
Training Epoch: 54 [22784/50048]	Loss: 0.3015
Training Epoch: 54 [22912/50048]	Loss: 0.1411
Training Epoch: 54 [23040/50048]	Loss: 0.1924
Training Epoch: 54 [23168/50048]	Loss: 0.2701
Training Epoch: 54 [23296/50048]	Loss: 0.2080
Training Epoch: 54 [23424/50048]	Loss: 0.1979
Training Epoch: 54 [23552/50048]	Loss: 0.2174
Training Epoch: 54 [23680/50048]	Loss: 0.1929
Training Epoch: 54 [23808/50048]	Loss: 0.3141
Training Epoch: 54 [23936/50048]	Loss: 0.3873
Training Epoch: 54 [24064/50048]	Loss: 0.2499
Training Epoch: 54 [24192/50048]	Loss: 0.1692
Training Epoch: 54 [24320/50048]	Loss: 0.2428
Training Epoch: 54 [24448/50048]	Loss: 0.1845
Training Epoch: 54 [24576/50048]	Loss: 0.2859
Training Epoch: 54 [24704/50048]	Loss: 0.2837
Training Epoch: 54 [24832/50048]	Loss: 0.1778
Training Epoch: 54 [24960/50048]	Loss: 0.2868
Training Epoch: 54 [25088/50048]	Loss: 0.3299
Training Epoch: 54 [25216/50048]	Loss: 0.2742
Training Epoch: 54 [25344/50048]	Loss: 0.2797
Training Epoch: 54 [25472/50048]	Loss: 0.2191
Training Epoch: 54 [25600/50048]	Loss: 0.2742
Training Epoch: 54 [25728/50048]	Loss: 0.1842
Training Epoch: 54 [25856/50048]	Loss: 0.2491
Training Epoch: 54 [25984/50048]	Loss: 0.2098
Training Epoch: 54 [26112/50048]	Loss: 0.2202
Training Epoch: 54 [26240/50048]	Loss: 0.3287
Training Epoch: 54 [26368/50048]	Loss: 0.3496
Training Epoch: 54 [26496/50048]	Loss: 0.2510
Training Epoch: 54 [26624/50048]	Loss: 0.2481
Training Epoch: 54 [26752/50048]	Loss: 0.1761
Training Epoch: 54 [26880/50048]	Loss: 0.2389
Training Epoch: 54 [27008/50048]	Loss: 0.3312
Training Epoch: 54 [27136/50048]	Loss: 0.3800
Training Epoch: 54 [27264/50048]	Loss: 0.2388
Training Epoch: 54 [27392/50048]	Loss: 0.2710
Training Epoch: 54 [27520/50048]	Loss: 0.2047
Training Epoch: 54 [27648/50048]	Loss: 0.2746
Training Epoch: 54 [27776/50048]	Loss: 0.2535
Training Epoch: 54 [27904/50048]	Loss: 0.2589
Training Epoch: 54 [28032/50048]	Loss: 0.3023
Training Epoch: 54 [28160/50048]	Loss: 0.1955
Training Epoch: 54 [28288/50048]	Loss: 0.2986
Training Epoch: 54 [28416/50048]	Loss: 0.2021
Training Epoch: 54 [28544/50048]	Loss: 0.2568
Training Epoch: 54 [28672/50048]	Loss: 0.2310
Training Epoch: 54 [28800/50048]	Loss: 0.2508
Training Epoch: 54 [28928/50048]	Loss: 0.2119
Training Epoch: 54 [29056/50048]	Loss: 0.2574
Training Epoch: 54 [29184/50048]	Loss: 0.2245
Training Epoch: 54 [29312/50048]	Loss: 0.2313
Training Epoch: 54 [29440/50048]	Loss: 0.2712
Training Epoch: 54 [29568/50048]	Loss: 0.1589
Training Epoch: 54 [29696/50048]	Loss: 0.1412
Training Epoch: 54 [29824/50048]	Loss: 0.2732
Training Epoch: 54 [29952/50048]	Loss: 0.2825
Training Epoch: 54 [30080/50048]	Loss: 0.2764
Training Epoch: 54 [30208/50048]	Loss: 0.2252
Training Epoch: 54 [30336/50048]	Loss: 0.2474
Training Epoch: 54 [30464/50048]	Loss: 0.2672
Training Epoch: 54 [30592/50048]	Loss: 0.1416
Training Epoch: 54 [30720/50048]	Loss: 0.1954
Training Epoch: 54 [30848/50048]	Loss: 0.2938
Training Epoch: 54 [30976/50048]	Loss: 0.2343
Training Epoch: 54 [31104/50048]	Loss: 0.2296
Training Epoch: 54 [31232/50048]	Loss: 0.2367
Training Epoch: 54 [31360/50048]	Loss: 0.2768
Training Epoch: 54 [31488/50048]	Loss: 0.2441
Training Epoch: 54 [31616/50048]	Loss: 0.1308
Training Epoch: 54 [31744/50048]	Loss: 0.1229
Training Epoch: 54 [31872/50048]	Loss: 0.1898
Training Epoch: 54 [32000/50048]	Loss: 0.4098
Training Epoch: 54 [32128/50048]	Loss: 0.2209
Training Epoch: 54 [32256/50048]	Loss: 0.1960
Training Epoch: 54 [32384/50048]	Loss: 0.2318
Training Epoch: 54 [32512/50048]	Loss: 0.2678
Training Epoch: 54 [32640/50048]	Loss: 0.1985
Training Epoch: 54 [32768/50048]	Loss: 0.3608
Training Epoch: 54 [32896/50048]	Loss: 0.3393
Training Epoch: 54 [33024/50048]	Loss: 0.2721
Training Epoch: 54 [33152/50048]	Loss: 0.3556
Training Epoch: 54 [33280/50048]	Loss: 0.2769
Training Epoch: 54 [33408/50048]	Loss: 0.3813
Training Epoch: 54 [33536/50048]	Loss: 0.2363
Training Epoch: 54 [33664/50048]	Loss: 0.2216
Training Epoch: 54 [33792/50048]	Loss: 0.3028
Training Epoch: 54 [33920/50048]	Loss: 0.3208
Training Epoch: 54 [34048/50048]	Loss: 0.2431
Training Epoch: 54 [34176/50048]	Loss: 0.2729
Training Epoch: 54 [34304/50048]	Loss: 0.2468
Training Epoch: 54 [34432/50048]	Loss: 0.3031
Training Epoch: 54 [34560/50048]	Loss: 0.2810
Training Epoch: 54 [34688/50048]	Loss: 0.2947
Training Epoch: 54 [34816/50048]	Loss: 0.2693
Training Epoch: 54 [34944/50048]	Loss: 0.3990
Training Epoch: 54 [35072/50048]	Loss: 0.3219
Training Epoch: 54 [35200/50048]	Loss: 0.2777
Training Epoch: 54 [35328/50048]	Loss: 0.1995
Training Epoch: 54 [35456/50048]	Loss: 0.2374
Training Epoch: 54 [35584/50048]	Loss: 0.3777
Training Epoch: 54 [35712/50048]	Loss: 0.4209
Training Epoch: 54 [35840/50048]	Loss: 0.3976
Training Epoch: 54 [35968/50048]	Loss: 0.1923
Training Epoch: 54 [36096/50048]	Loss: 0.3613
Training Epoch: 54 [36224/50048]	Loss: 0.1832
Training Epoch: 54 [36352/50048]	Loss: 0.2195
Training Epoch: 54 [36480/50048]	Loss: 0.2577
Training Epoch: 54 [36608/50048]	Loss: 0.2743
Training Epoch: 54 [36736/50048]	Loss: 0.3527
Training Epoch: 54 [36864/50048]	Loss: 0.2163
Training Epoch: 54 [36992/50048]	Loss: 0.1190
Training Epoch: 54 [37120/50048]	Loss: 0.1330
Training Epoch: 54 [37248/50048]	Loss: 0.2535
Training Epoch: 54 [37376/50048]	Loss: 0.3465
Training Epoch: 54 [37504/50048]	Loss: 0.2506
Training Epoch: 54 [37632/50048]	Loss: 0.2632
Training Epoch: 54 [37760/50048]	Loss: 0.2659
Training Epoch: 54 [37888/50048]	Loss: 0.2591
Training Epoch: 54 [38016/50048]	Loss: 0.1490
Training Epoch: 54 [38144/50048]	Loss: 0.3997
Training Epoch: 54 [38272/50048]	Loss: 0.3106
Training Epoch: 54 [38400/50048]	Loss: 0.2505
Training Epoch: 54 [38528/50048]	Loss: 0.2736
Training Epoch: 54 [38656/50048]	Loss: 0.2171
Training Epoch: 54 [38784/50048]	Loss: 0.3866
Training Epoch: 54 [38912/50048]	Loss: 0.2760
Training Epoch: 54 [39040/50048]	Loss: 0.1771
Training Epoch: 54 [39168/50048]	Loss: 0.2464
Training Epoch: 54 [39296/50048]	Loss: 0.2588
Training Epoch: 54 [39424/50048]	Loss: 0.2964
Training Epoch: 54 [39552/50048]	Loss: 0.2441
Training Epoch: 54 [39680/50048]	Loss: 0.4938
Training Epoch: 54 [39808/50048]	Loss: 0.2807
Training Epoch: 54 [39936/50048]	Loss: 0.2677
Training Epoch: 54 [40064/50048]	Loss: 0.2765
Training Epoch: 54 [40192/50048]	Loss: 0.2571
Training Epoch: 54 [40320/50048]	Loss: 0.3370
Training Epoch: 54 [40448/50048]	Loss: 0.3636
Training Epoch: 54 [40576/50048]	Loss: 0.2510
Training Epoch: 54 [40704/50048]	Loss: 0.3057
Training Epoch: 54 [40832/50048]	Loss: 0.3438
Training Epoch: 54 [40960/50048]	Loss: 0.3037
Training Epoch: 54 [41088/50048]	Loss: 0.2071
Training Epoch: 54 [41216/50048]	Loss: 0.2775
Training Epoch: 54 [41344/50048]	Loss: 0.2668
Training Epoch: 54 [41472/50048]	Loss: 0.3620
Training Epoch: 54 [41600/50048]	Loss: 0.2801
Training Epoch: 54 [41728/50048]	Loss: 0.3335
Training Epoch: 54 [41856/50048]	Loss: 0.2629
Training Epoch: 54 [41984/50048]	Loss: 0.3935
Training Epoch: 54 [42112/50048]	Loss: 0.3519
Training Epoch: 54 [42240/50048]	Loss: 0.3556
Training Epoch: 54 [42368/50048]	Loss: 0.2081
Training Epoch: 54 [42496/50048]	Loss: 0.3363
Training Epoch: 54 [42624/50048]	Loss: 0.2478
Training Epoch: 54 [42752/50048]	Loss: 0.3081
Training Epoch: 54 [42880/50048]	Loss: 0.3261
Training Epoch: 54 [43008/50048]	Loss: 0.2714
Training Epoch: 54 [43136/50048]	Loss: 0.3427
Training Epoch: 54 [43264/50048]	Loss: 0.2856
Training Epoch: 54 [43392/50048]	Loss: 0.3483
Training Epoch: 54 [43520/50048]	Loss: 0.2650
Training Epoch: 54 [43648/50048]	Loss: 0.2370
Training Epoch: 54 [43776/50048]	Loss: 0.2781
Training Epoch: 54 [43904/50048]	Loss: 0.3682
Training Epoch: 54 [44032/50048]	Loss: 0.4631
Training Epoch: 54 [44160/50048]	Loss: 0.3722
Training Epoch: 54 [44288/50048]	Loss: 0.2853
Training Epoch: 54 [44416/50048]	Loss: 0.2972
Training Epoch: 54 [44544/50048]	Loss: 0.3228
Training Epoch: 54 [44672/50048]	Loss: 0.2712
Training Epoch: 54 [44800/50048]	Loss: 0.1626
Training Epoch: 54 [44928/50048]	Loss: 0.3055
Training Epoch: 54 [45056/50048]	Loss: 0.2624
Training Epoch: 54 [45184/50048]	Loss: 0.1962
Training Epoch: 54 [45312/50048]	Loss: 0.3006
Training Epoch: 54 [45440/50048]	Loss: 0.3005
Training Epoch: 54 [45568/50048]	Loss: 0.2090
Training Epoch: 54 [45696/50048]	Loss: 0.2432
2022-12-06 07:30:31,933 [ZeusDataLoader(train)] train epoch 55 done: time=86.49 energy=10502.94
2022-12-06 07:30:31,934 [ZeusDataLoader(eval)] Epoch 55 begin.
Training Epoch: 54 [45824/50048]	Loss: 0.1967
Training Epoch: 54 [45952/50048]	Loss: 0.1782
Training Epoch: 54 [46080/50048]	Loss: 0.2506
Training Epoch: 54 [46208/50048]	Loss: 0.1851
Training Epoch: 54 [46336/50048]	Loss: 0.1811
Training Epoch: 54 [46464/50048]	Loss: 0.2143
Training Epoch: 54 [46592/50048]	Loss: 0.3390
Training Epoch: 54 [46720/50048]	Loss: 0.2213
Training Epoch: 54 [46848/50048]	Loss: 0.1457
Training Epoch: 54 [46976/50048]	Loss: 0.3291
Training Epoch: 54 [47104/50048]	Loss: 0.2488
Training Epoch: 54 [47232/50048]	Loss: 0.2528
Training Epoch: 54 [47360/50048]	Loss: 0.2811
Training Epoch: 54 [47488/50048]	Loss: 0.1442
Training Epoch: 54 [47616/50048]	Loss: 0.2877
Training Epoch: 54 [47744/50048]	Loss: 0.3232
Training Epoch: 54 [47872/50048]	Loss: 0.1645
Training Epoch: 54 [48000/50048]	Loss: 0.2740
Training Epoch: 54 [48128/50048]	Loss: 0.3615
Training Epoch: 54 [48256/50048]	Loss: 0.3225
Training Epoch: 54 [48384/50048]	Loss: 0.2300
Training Epoch: 54 [48512/50048]	Loss: 0.2684
Training Epoch: 54 [48640/50048]	Loss: 0.3147
Training Epoch: 54 [48768/50048]	Loss: 0.1657
Training Epoch: 54 [48896/50048]	Loss: 0.3972
Training Epoch: 54 [49024/50048]	Loss: 0.2430
Training Epoch: 54 [49152/50048]	Loss: 0.3029
Training Epoch: 54 [49280/50048]	Loss: 0.3559
Training Epoch: 54 [49408/50048]	Loss: 0.1598
Training Epoch: 54 [49536/50048]	Loss: 0.3857
Training Epoch: 54 [49664/50048]	Loss: 0.3682
Training Epoch: 54 [49792/50048]	Loss: 0.1917
Training Epoch: 54 [49920/50048]	Loss: 0.2956
Training Epoch: 54 [50048/50048]	Loss: 0.3047
2022-12-06 12:30:35.608 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:30:35,647 [ZeusDataLoader(eval)] eval epoch 55 done: time=3.70 energy=453.83
2022-12-06 07:30:35,647 [ZeusDataLoader(train)] Up to epoch 55: time=4960.82, energy=602212.92, cost=735178.24
2022-12-06 07:30:35,647 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:30:35,648 [ZeusDataLoader(train)] Expected next epoch: time=5050.62, energy=613010.93, cost=748434.62
2022-12-06 07:30:35,648 [ZeusDataLoader(train)] Epoch 56 begin.
Validation Epoch: 54, Average loss: 0.0149, Accuracy: 0.6352
2022-12-06 07:30:35,839 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:30:35,840 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:30:35.842 [ZeusMonitor] Monitor started.
2022-12-06 12:30:35.842 [ZeusMonitor] Running indefinitely. 2022-12-06 12:30:35.842 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:30:35.842 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e56+gpu0.power.log
Training Epoch: 55 [128/50048]	Loss: 0.2507
Training Epoch: 55 [256/50048]	Loss: 0.3207
Training Epoch: 55 [384/50048]	Loss: 0.2581
Training Epoch: 55 [512/50048]	Loss: 0.2201
Training Epoch: 55 [640/50048]	Loss: 0.1848
Training Epoch: 55 [768/50048]	Loss: 0.2403
Training Epoch: 55 [896/50048]	Loss: 0.2297
Training Epoch: 55 [1024/50048]	Loss: 0.1711
Training Epoch: 55 [1152/50048]	Loss: 0.1342
Training Epoch: 55 [1280/50048]	Loss: 0.1472
Training Epoch: 55 [1408/50048]	Loss: 0.1902
Training Epoch: 55 [1536/50048]	Loss: 0.1647
Training Epoch: 55 [1664/50048]	Loss: 0.1679
Training Epoch: 55 [1792/50048]	Loss: 0.1472
Training Epoch: 55 [1920/50048]	Loss: 0.1872
Training Epoch: 55 [2048/50048]	Loss: 0.1748
Training Epoch: 55 [2176/50048]	Loss: 0.2051
Training Epoch: 55 [2304/50048]	Loss: 0.1552
Training Epoch: 55 [2432/50048]	Loss: 0.2178
Training Epoch: 55 [2560/50048]	Loss: 0.1255
Training Epoch: 55 [2688/50048]	Loss: 0.1919
Training Epoch: 55 [2816/50048]	Loss: 0.1855
Training Epoch: 55 [2944/50048]	Loss: 0.1813
Training Epoch: 55 [3072/50048]	Loss: 0.2601
Training Epoch: 55 [3200/50048]	Loss: 0.2482
Training Epoch: 55 [3328/50048]	Loss: 0.2094
Training Epoch: 55 [3456/50048]	Loss: 0.2079
Training Epoch: 55 [3584/50048]	Loss: 0.2221
Training Epoch: 55 [3712/50048]	Loss: 0.1903
Training Epoch: 55 [3840/50048]	Loss: 0.1761
Training Epoch: 55 [3968/50048]	Loss: 0.2074
Training Epoch: 55 [4096/50048]	Loss: 0.2029
Training Epoch: 55 [4224/50048]	Loss: 0.1532
Training Epoch: 55 [4352/50048]	Loss: 0.1884
Training Epoch: 55 [4480/50048]	Loss: 0.1972
Training Epoch: 55 [4608/50048]	Loss: 0.3130
Training Epoch: 55 [4736/50048]	Loss: 0.1873
Training Epoch: 55 [4864/50048]	Loss: 0.2543
Training Epoch: 55 [4992/50048]	Loss: 0.1443
Training Epoch: 55 [5120/50048]	Loss: 0.1567
Training Epoch: 55 [5248/50048]	Loss: 0.2687
Training Epoch: 55 [5376/50048]	Loss: 0.1667
Training Epoch: 55 [5504/50048]	Loss: 0.1989
Training Epoch: 55 [5632/50048]	Loss: 0.2677
Training Epoch: 55 [5760/50048]	Loss: 0.2382
Training Epoch: 55 [5888/50048]	Loss: 0.1708
Training Epoch: 55 [6016/50048]	Loss: 0.2210
Training Epoch: 55 [6144/50048]	Loss: 0.1944
Training Epoch: 55 [6272/50048]	Loss: 0.1517
Training Epoch: 55 [6400/50048]	Loss: 0.2153
Training Epoch: 55 [6528/50048]	Loss: 0.1629
Training Epoch: 55 [6656/50048]	Loss: 0.2084
Training Epoch: 55 [6784/50048]	Loss: 0.2077
Training Epoch: 55 [6912/50048]	Loss: 0.2324
Training Epoch: 55 [7040/50048]	Loss: 0.3271
Training Epoch: 55 [7168/50048]	Loss: 0.1312
Training Epoch: 55 [7296/50048]	Loss: 0.2412
Training Epoch: 55 [7424/50048]	Loss: 0.1938
Training Epoch: 55 [7552/50048]	Loss: 0.2936
Training Epoch: 55 [7680/50048]	Loss: 0.2137
Training Epoch: 55 [7808/50048]	Loss: 0.2401
Training Epoch: 55 [7936/50048]	Loss: 0.1719
Training Epoch: 55 [8064/50048]	Loss: 0.1875
Training Epoch: 55 [8192/50048]	Loss: 0.2184
Training Epoch: 55 [8320/50048]	Loss: 0.1576
Training Epoch: 55 [8448/50048]	Loss: 0.2458
Training Epoch: 55 [8576/50048]	Loss: 0.2907
Training Epoch: 55 [8704/50048]	Loss: 0.3162
Training Epoch: 55 [8832/50048]	Loss: 0.1826
Training Epoch: 55 [8960/50048]	Loss: 0.2848
Training Epoch: 55 [9088/50048]	Loss: 0.1673
Training Epoch: 55 [9216/50048]	Loss: 0.1250
Training Epoch: 55 [9344/50048]	Loss: 0.2005
Training Epoch: 55 [9472/50048]	Loss: 0.2040
Training Epoch: 55 [9600/50048]	Loss: 0.2524
Training Epoch: 55 [9728/50048]	Loss: 0.1947
Training Epoch: 55 [9856/50048]	Loss: 0.1836
Training Epoch: 55 [9984/50048]	Loss: 0.1872
Training Epoch: 55 [10112/50048]	Loss: 0.0811
Training Epoch: 55 [10240/50048]	Loss: 0.2644
Training Epoch: 55 [10368/50048]	Loss: 0.1711
Training Epoch: 55 [10496/50048]	Loss: 0.1887
Training Epoch: 55 [10624/50048]	Loss: 0.3390
Training Epoch: 55 [10752/50048]	Loss: 0.2608
Training Epoch: 55 [10880/50048]	Loss: 0.2461
Training Epoch: 55 [11008/50048]	Loss: 0.1898
Training Epoch: 55 [11136/50048]	Loss: 0.1841
Training Epoch: 55 [11264/50048]	Loss: 0.1465
Training Epoch: 55 [11392/50048]	Loss: 0.1257
Training Epoch: 55 [11520/50048]	Loss: 0.2815
Training Epoch: 55 [11648/50048]	Loss: 0.2790
Training Epoch: 55 [11776/50048]	Loss: 0.1735
Training Epoch: 55 [11904/50048]	Loss: 0.1729
Training Epoch: 55 [12032/50048]	Loss: 0.1671
Training Epoch: 55 [12160/50048]	Loss: 0.1956
Training Epoch: 55 [12288/50048]	Loss: 0.1771
Training Epoch: 55 [12416/50048]	Loss: 0.1652
Training Epoch: 55 [12544/50048]	Loss: 0.1567
Training Epoch: 55 [12672/50048]	Loss: 0.2259
Training Epoch: 55 [12800/50048]	Loss: 0.1792
Training Epoch: 55 [12928/50048]	Loss: 0.2638
Training Epoch: 55 [13056/50048]	Loss: 0.1405
Training Epoch: 55 [13184/50048]	Loss: 0.1622
Training Epoch: 55 [13312/50048]	Loss: 0.2136
Training Epoch: 55 [13440/50048]	Loss: 0.2771
Training Epoch: 55 [13568/50048]	Loss: 0.2630
Training Epoch: 55 [13696/50048]	Loss: 0.1806
Training Epoch: 55 [13824/50048]	Loss: 0.1764
Training Epoch: 55 [13952/50048]	Loss: 0.2333
Training Epoch: 55 [14080/50048]	Loss: 0.2116
Training Epoch: 55 [14208/50048]	Loss: 0.1709
Training Epoch: 55 [14336/50048]	Loss: 0.2240
Training Epoch: 55 [14464/50048]	Loss: 0.2855
Training Epoch: 55 [14592/50048]	Loss: 0.2777
Training Epoch: 55 [14720/50048]	Loss: 0.1577
Training Epoch: 55 [14848/50048]	Loss: 0.2213
Training Epoch: 55 [14976/50048]	Loss: 0.2189
Training Epoch: 55 [15104/50048]	Loss: 0.1896
Training Epoch: 55 [15232/50048]	Loss: 0.1206
Training Epoch: 55 [15360/50048]	Loss: 0.1964
Training Epoch: 55 [15488/50048]	Loss: 0.1899
Training Epoch: 55 [15616/50048]	Loss: 0.1547
Training Epoch: 55 [15744/50048]	Loss: 0.2837
Training Epoch: 55 [15872/50048]	Loss: 0.2436
Training Epoch: 55 [16000/50048]	Loss: 0.2690
Training Epoch: 55 [16128/50048]	Loss: 0.1396
Training Epoch: 55 [16256/50048]	Loss: 0.2353
Training Epoch: 55 [16384/50048]	Loss: 0.1888
Training Epoch: 55 [16512/50048]	Loss: 0.1785
Training Epoch: 55 [16640/50048]	Loss: 0.2578
Training Epoch: 55 [16768/50048]	Loss: 0.3058
Training Epoch: 55 [16896/50048]	Loss: 0.2163
Training Epoch: 55 [17024/50048]	Loss: 0.1239
Training Epoch: 55 [17152/50048]	Loss: 0.2185
Training Epoch: 55 [17280/50048]	Loss: 0.2635
Training Epoch: 55 [17408/50048]	Loss: 0.2027
Training Epoch: 55 [17536/50048]	Loss: 0.3088
Training Epoch: 55 [17664/50048]	Loss: 0.3381
Training Epoch: 55 [17792/50048]	Loss: 0.1832
Training Epoch: 55 [17920/50048]	Loss: 0.2186
Training Epoch: 55 [18048/50048]	Loss: 0.1701
Training Epoch: 55 [18176/50048]	Loss: 0.1504
Training Epoch: 55 [18304/50048]	Loss: 0.1752
Training Epoch: 55 [18432/50048]	Loss: 0.2252
Training Epoch: 55 [18560/50048]	Loss: 0.2965
Training Epoch: 55 [18688/50048]	Loss: 0.1096
Training Epoch: 55 [18816/50048]	Loss: 0.1250
Training Epoch: 55 [18944/50048]	Loss: 0.3715
Training Epoch: 55 [19072/50048]	Loss: 0.1964
Training Epoch: 55 [19200/50048]	Loss: 0.2350
Training Epoch: 55 [19328/50048]	Loss: 0.2783
Training Epoch: 55 [19456/50048]	Loss: 0.2488
Training Epoch: 55 [19584/50048]	Loss: 0.2891
Training Epoch: 55 [19712/50048]	Loss: 0.3167
Training Epoch: 55 [19840/50048]	Loss: 0.3717
Training Epoch: 55 [19968/50048]	Loss: 0.1429
Training Epoch: 55 [20096/50048]	Loss: 0.2042
Training Epoch: 55 [20224/50048]	Loss: 0.1924
Training Epoch: 55 [20352/50048]	Loss: 0.2501
Training Epoch: 55 [20480/50048]	Loss: 0.2724
Training Epoch: 55 [20608/50048]	Loss: 0.2740
Training Epoch: 55 [20736/50048]	Loss: 0.3022
Training Epoch: 55 [20864/50048]	Loss: 0.1928
Training Epoch: 55 [20992/50048]	Loss: 0.1771
Training Epoch: 55 [21120/50048]	Loss: 0.2266
Training Epoch: 55 [21248/50048]	Loss: 0.3802
Training Epoch: 55 [21376/50048]	Loss: 0.3150
Training Epoch: 55 [21504/50048]	Loss: 0.2616
Training Epoch: 55 [21632/50048]	Loss: 0.3353
Training Epoch: 55 [21760/50048]	Loss: 0.1997
Training Epoch: 55 [21888/50048]	Loss: 0.2169
Training Epoch: 55 [22016/50048]	Loss: 0.2709
Training Epoch: 55 [22144/50048]	Loss: 0.1591
Training Epoch: 55 [22272/50048]	Loss: 0.1767
Training Epoch: 55 [22400/50048]	Loss: 0.1851
Training Epoch: 55 [22528/50048]	Loss: 0.2526
Training Epoch: 55 [22656/50048]	Loss: 0.3527
Training Epoch: 55 [22784/50048]	Loss: 0.2744
Training Epoch: 55 [22912/50048]	Loss: 0.2049
Training Epoch: 55 [23040/50048]	Loss: 0.3091
Training Epoch: 55 [23168/50048]	Loss: 0.1751
Training Epoch: 55 [23296/50048]	Loss: 0.2278
Training Epoch: 55 [23424/50048]	Loss: 0.2341
Training Epoch: 55 [23552/50048]	Loss: 0.1959
Training Epoch: 55 [23680/50048]	Loss: 0.1154
Training Epoch: 55 [23808/50048]	Loss: 0.2343
Training Epoch: 55 [23936/50048]	Loss: 0.3490
Training Epoch: 55 [24064/50048]	Loss: 0.3183
Training Epoch: 55 [24192/50048]	Loss: 0.0984
Training Epoch: 55 [24320/50048]	Loss: 0.2391
Training Epoch: 55 [24448/50048]	Loss: 0.1970
Training Epoch: 55 [24576/50048]	Loss: 0.2476
Training Epoch: 55 [24704/50048]	Loss: 0.1769
Training Epoch: 55 [24832/50048]	Loss: 0.1147
Training Epoch: 55 [24960/50048]	Loss: 0.2672
Training Epoch: 55 [25088/50048]	Loss: 0.2442
Training Epoch: 55 [25216/50048]	Loss: 0.2179
Training Epoch: 55 [25344/50048]	Loss: 0.2123
Training Epoch: 55 [25472/50048]	Loss: 0.1231
Training Epoch: 55 [25600/50048]	Loss: 0.2713
Training Epoch: 55 [25728/50048]	Loss: 0.3147
Training Epoch: 55 [25856/50048]	Loss: 0.1915
Training Epoch: 55 [25984/50048]	Loss: 0.1960
Training Epoch: 55 [26112/50048]	Loss: 0.2950
Training Epoch: 55 [26240/50048]	Loss: 0.2451
Training Epoch: 55 [26368/50048]	Loss: 0.1832
Training Epoch: 55 [26496/50048]	Loss: 0.2745
Training Epoch: 55 [26624/50048]	Loss: 0.3234
Training Epoch: 55 [26752/50048]	Loss: 0.1412
Training Epoch: 55 [26880/50048]	Loss: 0.1833
Training Epoch: 55 [27008/50048]	Loss: 0.2464
Training Epoch: 55 [27136/50048]	Loss: 0.2234
Training Epoch: 55 [27264/50048]	Loss: 0.2261
Training Epoch: 55 [27392/50048]	Loss: 0.2004
Training Epoch: 55 [27520/50048]	Loss: 0.2890
Training Epoch: 55 [27648/50048]	Loss: 0.2415
Training Epoch: 55 [27776/50048]	Loss: 0.1095
Training Epoch: 55 [27904/50048]	Loss: 0.2817
Training Epoch: 55 [28032/50048]	Loss: 0.1824
Training Epoch: 55 [28160/50048]	Loss: 0.2895
Training Epoch: 55 [28288/50048]	Loss: 0.2408
Training Epoch: 55 [28416/50048]	Loss: 0.2765
Training Epoch: 55 [28544/50048]	Loss: 0.2412
Training Epoch: 55 [28672/50048]	Loss: 0.2324
Training Epoch: 55 [28800/50048]	Loss: 0.3141
Training Epoch: 55 [28928/50048]	Loss: 0.1582
Training Epoch: 55 [29056/50048]	Loss: 0.1593
Training Epoch: 55 [29184/50048]	Loss: 0.1960
Training Epoch: 55 [29312/50048]	Loss: 0.4246
Training Epoch: 55 [29440/50048]	Loss: 0.3017
Training Epoch: 55 [29568/50048]	Loss: 0.2525
Training Epoch: 55 [29696/50048]	Loss: 0.2875
Training Epoch: 55 [29824/50048]	Loss: 0.2163
Training Epoch: 55 [29952/50048]	Loss: 0.2129
Training Epoch: 55 [30080/50048]	Loss: 0.1723
Training Epoch: 55 [30208/50048]	Loss: 0.2025
Training Epoch: 55 [30336/50048]	Loss: 0.2977
Training Epoch: 55 [30464/50048]	Loss: 0.2803
Training Epoch: 55 [30592/50048]	Loss: 0.2634
Training Epoch: 55 [30720/50048]	Loss: 0.2209
Training Epoch: 55 [30848/50048]	Loss: 0.2611
Training Epoch: 55 [30976/50048]	Loss: 0.3258
Training Epoch: 55 [31104/50048]	Loss: 0.3048
Training Epoch: 55 [31232/50048]	Loss: 0.3390
Training Epoch: 55 [31360/50048]	Loss: 0.2696
Training Epoch: 55 [31488/50048]	Loss: 0.2468
Training Epoch: 55 [31616/50048]	Loss: 0.2587
Training Epoch: 55 [31744/50048]	Loss: 0.2365
Training Epoch: 55 [31872/50048]	Loss: 0.3369
Training Epoch: 55 [32000/50048]	Loss: 0.3422
Training Epoch: 55 [32128/50048]	Loss: 0.2099
Training Epoch: 55 [32256/50048]	Loss: 0.2018
Training Epoch: 55 [32384/50048]	Loss: 0.2805
Training Epoch: 55 [32512/50048]	Loss: 0.2096
Training Epoch: 55 [32640/50048]	Loss: 0.2096
Training Epoch: 55 [32768/50048]	Loss: 0.2427
Training Epoch: 55 [32896/50048]	Loss: 0.4869
Training Epoch: 55 [33024/50048]	Loss: 0.2255
Training Epoch: 55 [33152/50048]	Loss: 0.1906
Training Epoch: 55 [33280/50048]	Loss: 0.1728
Training Epoch: 55 [33408/50048]	Loss: 0.2406
Training Epoch: 55 [33536/50048]	Loss: 0.2198
Training Epoch: 55 [33664/50048]	Loss: 0.2535
Training Epoch: 55 [33792/50048]	Loss: 0.2905
Training Epoch: 55 [33920/50048]	Loss: 0.2895
Training Epoch: 55 [34048/50048]	Loss: 0.2402
Training Epoch: 55 [34176/50048]	Loss: 0.2573
Training Epoch: 55 [34304/50048]	Loss: 0.1639
Training Epoch: 55 [34432/50048]	Loss: 0.3288
Training Epoch: 55 [34560/50048]	Loss: 0.2531
Training Epoch: 55 [34688/50048]	Loss: 0.2079
Training Epoch: 55 [34816/50048]	Loss: 0.1924
Training Epoch: 55 [34944/50048]	Loss: 0.2144
Training Epoch: 55 [35072/50048]	Loss: 0.3281
Training Epoch: 55 [35200/50048]	Loss: 0.2090
Training Epoch: 55 [35328/50048]	Loss: 0.3588
Training Epoch: 55 [35456/50048]	Loss: 0.2933
Training Epoch: 55 [35584/50048]	Loss: 0.1869
Training Epoch: 55 [35712/50048]	Loss: 0.2660
Training Epoch: 55 [35840/50048]	Loss: 0.2544
Training Epoch: 55 [35968/50048]	Loss: 0.1906
Training Epoch: 55 [36096/50048]	Loss: 0.2186
Training Epoch: 55 [36224/50048]	Loss: 0.3884
Training Epoch: 55 [36352/50048]	Loss: 0.2611
Training Epoch: 55 [36480/50048]	Loss: 0.3605
Training Epoch: 55 [36608/50048]	Loss: 0.2576
Training Epoch: 55 [36736/50048]	Loss: 0.2072
Training Epoch: 55 [36864/50048]	Loss: 0.3448
Training Epoch: 55 [36992/50048]	Loss: 0.2527
Training Epoch: 55 [37120/50048]	Loss: 0.2929
Training Epoch: 55 [37248/50048]	Loss: 0.1945
Training Epoch: 55 [37376/50048]	Loss: 0.2719
Training Epoch: 55 [37504/50048]	Loss: 0.1976
Training Epoch: 55 [37632/50048]	Loss: 0.2774
Training Epoch: 55 [37760/50048]	Loss: 0.2772
Training Epoch: 55 [37888/50048]	Loss: 0.2966
Training Epoch: 55 [38016/50048]	Loss: 0.2846
Training Epoch: 55 [38144/50048]	Loss: 0.2657
Training Epoch: 55 [38272/50048]	Loss: 0.2603
Training Epoch: 55 [38400/50048]	Loss: 0.2904
Training Epoch: 55 [38528/50048]	Loss: 0.1711
Training Epoch: 55 [38656/50048]	Loss: 0.2851
Training Epoch: 55 [38784/50048]	Loss: 0.1994
Training Epoch: 55 [38912/50048]	Loss: 0.3361
Training Epoch: 55 [39040/50048]	Loss: 0.2806
Training Epoch: 55 [39168/50048]	Loss: 0.2044
Training Epoch: 55 [39296/50048]	Loss: 0.1970
Training Epoch: 55 [39424/50048]	Loss: 0.3528
Training Epoch: 55 [39552/50048]	Loss: 0.2710
Training Epoch: 55 [39680/50048]	Loss: 0.2332
Training Epoch: 55 [39808/50048]	Loss: 0.2572
Training Epoch: 55 [39936/50048]	Loss: 0.1446
Training Epoch: 55 [40064/50048]	Loss: 0.1409
Training Epoch: 55 [40192/50048]	Loss: 0.3113
Training Epoch: 55 [40320/50048]	Loss: 0.3103
Training Epoch: 55 [40448/50048]	Loss: 0.2148
Training Epoch: 55 [40576/50048]	Loss: 0.2566
Training Epoch: 55 [40704/50048]	Loss: 0.2469
Training Epoch: 55 [40832/50048]	Loss: 0.2069
Training Epoch: 55 [40960/50048]	Loss: 0.2544
Training Epoch: 55 [41088/50048]	Loss: 0.2488
Training Epoch: 55 [41216/50048]	Loss: 0.1838
Training Epoch: 55 [41344/50048]	Loss: 0.1877
Training Epoch: 55 [41472/50048]	Loss: 0.3968
Training Epoch: 55 [41600/50048]	Loss: 0.2056
Training Epoch: 55 [41728/50048]	Loss: 0.2970
Training Epoch: 55 [41856/50048]	Loss: 0.2901
Training Epoch: 55 [41984/50048]	Loss: 0.1936
Training Epoch: 55 [42112/50048]	Loss: 0.4251
Training Epoch: 55 [42240/50048]	Loss: 0.2606
Training Epoch: 55 [42368/50048]	Loss: 0.2637
Training Epoch: 55 [42496/50048]	Loss: 0.3341
Training Epoch: 55 [42624/50048]	Loss: 0.2901
Training Epoch: 55 [42752/50048]	Loss: 0.2473
Training Epoch: 55 [42880/50048]	Loss: 0.2560
Training Epoch: 55 [43008/50048]	Loss: 0.3789
Training Epoch: 55 [43136/50048]	Loss: 0.2471
Training Epoch: 55 [43264/50048]	Loss: 0.3156
Training Epoch: 55 [43392/50048]	Loss: 0.2771
Training Epoch: 55 [43520/50048]	Loss: 0.3268
Training Epoch: 55 [43648/50048]	Loss: 0.2168
Training Epoch: 55 [43776/50048]	Loss: 0.1992
Training Epoch: 55 [43904/50048]	Loss: 0.2853
Training Epoch: 55 [44032/50048]	Loss: 0.1360
Training Epoch: 55 [44160/50048]	Loss: 0.2512
Training Epoch: 55 [44288/50048]	Loss: 0.2830
Training Epoch: 55 [44416/50048]	Loss: 0.2003
Training Epoch: 55 [44544/50048]	Loss: 0.2470
Training Epoch: 55 [44672/50048]	Loss: 0.3490
Training Epoch: 55 [44800/50048]	Loss: 0.1749
Training Epoch: 55 [44928/50048]	Loss: 0.3377
Training Epoch: 55 [45056/50048]	Loss: 0.3537
Training Epoch: 55 [45184/50048]	Loss: 0.1677
Training Epoch: 55 [45312/50048]	Loss: 0.2214
Training Epoch: 55 [45440/50048]	Loss: 0.2491
Training Epoch: 55 [45568/50048]	Loss: 0.2908
Training Epoch: 55 [45696/50048]	Loss: 0.1437
2022-12-06 07:32:02,329 [ZeusDataLoader(train)] train epoch 56 done: time=86.67 energy=10508.76
2022-12-06 07:32:02,330 [ZeusDataLoader(eval)] Epoch 56 begin.
Training Epoch: 55 [45824/50048]	Loss: 0.2165
Training Epoch: 55 [45952/50048]	Loss: 0.2207
Training Epoch: 55 [46080/50048]	Loss: 0.1955
Training Epoch: 55 [46208/50048]	Loss: 0.3019
Training Epoch: 55 [46336/50048]	Loss: 0.2517
Training Epoch: 55 [46464/50048]	Loss: 0.2149
Training Epoch: 55 [46592/50048]	Loss: 0.3496
Training Epoch: 55 [46720/50048]	Loss: 0.2070
Training Epoch: 55 [46848/50048]	Loss: 0.3252
Training Epoch: 55 [46976/50048]	Loss: 0.2892
Training Epoch: 55 [47104/50048]	Loss: 0.2393
Training Epoch: 55 [47232/50048]	Loss: 0.3048
Training Epoch: 55 [47360/50048]	Loss: 0.2698
Training Epoch: 55 [47488/50048]	Loss: 0.2648
Training Epoch: 55 [47616/50048]	Loss: 0.3179
Training Epoch: 55 [47744/50048]	Loss: 0.2208
Training Epoch: 55 [47872/50048]	Loss: 0.2963
Training Epoch: 55 [48000/50048]	Loss: 0.2751
Training Epoch: 55 [48128/50048]	Loss: 0.2511
Training Epoch: 55 [48256/50048]	Loss: 0.2769
Training Epoch: 55 [48384/50048]	Loss: 0.2447
Training Epoch: 55 [48512/50048]	Loss: 0.2591
Training Epoch: 55 [48640/50048]	Loss: 0.1703
Training Epoch: 55 [48768/50048]	Loss: 0.4142
Training Epoch: 55 [48896/50048]	Loss: 0.3603
Training Epoch: 55 [49024/50048]	Loss: 0.1624
Training Epoch: 55 [49152/50048]	Loss: 0.2342
Training Epoch: 55 [49280/50048]	Loss: 0.2338
Training Epoch: 55 [49408/50048]	Loss: 0.3710
Training Epoch: 55 [49536/50048]	Loss: 0.1945
Training Epoch: 55 [49664/50048]	Loss: 0.2289
Training Epoch: 55 [49792/50048]	Loss: 0.2497
Training Epoch: 55 [49920/50048]	Loss: 0.2596
Training Epoch: 55 [50048/50048]	Loss: 0.2925
2022-12-06 12:32:05.985 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:32:06,008 [ZeusDataLoader(eval)] eval epoch 56 done: time=3.67 energy=441.30
2022-12-06 07:32:06,008 [ZeusDataLoader(train)] Up to epoch 56: time=5051.16, energy=613162.98, cost=748557.84
2022-12-06 07:32:06,008 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:32:06,008 [ZeusDataLoader(train)] Expected next epoch: time=5140.96, energy=623960.99, cost=761814.23
2022-12-06 07:32:06,009 [ZeusDataLoader(train)] Epoch 57 begin.
Validation Epoch: 55, Average loss: 0.0152, Accuracy: 0.6403
2022-12-06 07:32:06,189 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:32:06,190 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:32:06.195 [ZeusMonitor] Monitor started.
2022-12-06 12:32:06.195 [ZeusMonitor] Running indefinitely. 2022-12-06 12:32:06.195 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:32:06.195 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e57+gpu0.power.log
Training Epoch: 56 [128/50048]	Loss: 0.1739
Training Epoch: 56 [256/50048]	Loss: 0.1884
Training Epoch: 56 [384/50048]	Loss: 0.1123
Training Epoch: 56 [512/50048]	Loss: 0.1106
Training Epoch: 56 [640/50048]	Loss: 0.2094
Training Epoch: 56 [768/50048]	Loss: 0.1667
Training Epoch: 56 [896/50048]	Loss: 0.1343
Training Epoch: 56 [1024/50048]	Loss: 0.1236
Training Epoch: 56 [1152/50048]	Loss: 0.1901
Training Epoch: 56 [1280/50048]	Loss: 0.2153
Training Epoch: 56 [1408/50048]	Loss: 0.1942
Training Epoch: 56 [1536/50048]	Loss: 0.3237
Training Epoch: 56 [1664/50048]	Loss: 0.2234
Training Epoch: 56 [1792/50048]	Loss: 0.2284
Training Epoch: 56 [1920/50048]	Loss: 0.1876
Training Epoch: 56 [2048/50048]	Loss: 0.2502
Training Epoch: 56 [2176/50048]	Loss: 0.0924
Training Epoch: 56 [2304/50048]	Loss: 0.2181
Training Epoch: 56 [2432/50048]	Loss: 0.2026
Training Epoch: 56 [2560/50048]	Loss: 0.2552
Training Epoch: 56 [2688/50048]	Loss: 0.2184
Training Epoch: 56 [2816/50048]	Loss: 0.2639
Training Epoch: 56 [2944/50048]	Loss: 0.1458
Training Epoch: 56 [3072/50048]	Loss: 0.1003
Training Epoch: 56 [3200/50048]	Loss: 0.1446
Training Epoch: 56 [3328/50048]	Loss: 0.2143
Training Epoch: 56 [3456/50048]	Loss: 0.1690
Training Epoch: 56 [3584/50048]	Loss: 0.3422
Training Epoch: 56 [3712/50048]	Loss: 0.1638
Training Epoch: 56 [3840/50048]	Loss: 0.2178
Training Epoch: 56 [3968/50048]	Loss: 0.2078
Training Epoch: 56 [4096/50048]	Loss: 0.1215
Training Epoch: 56 [4224/50048]	Loss: 0.1315
Training Epoch: 56 [4352/50048]	Loss: 0.0919
Training Epoch: 56 [4480/50048]	Loss: 0.1279
Training Epoch: 56 [4608/50048]	Loss: 0.1750
Training Epoch: 56 [4736/50048]	Loss: 0.1873
Training Epoch: 56 [4864/50048]	Loss: 0.1950
Training Epoch: 56 [4992/50048]	Loss: 0.2128
Training Epoch: 56 [5120/50048]	Loss: 0.2597
Training Epoch: 56 [5248/50048]	Loss: 0.1726
Training Epoch: 56 [5376/50048]	Loss: 0.1564
Training Epoch: 56 [5504/50048]	Loss: 0.1470
Training Epoch: 56 [5632/50048]	Loss: 0.1821
Training Epoch: 56 [5760/50048]	Loss: 0.1946
Training Epoch: 56 [5888/50048]	Loss: 0.1604
Training Epoch: 56 [6016/50048]	Loss: 0.1830
Training Epoch: 56 [6144/50048]	Loss: 0.1651
Training Epoch: 56 [6272/50048]	Loss: 0.1131
Training Epoch: 56 [6400/50048]	Loss: 0.2090
Training Epoch: 56 [6528/50048]	Loss: 0.2079
Training Epoch: 56 [6656/50048]	Loss: 0.2519
Training Epoch: 56 [6784/50048]	Loss: 0.1532
Training Epoch: 56 [6912/50048]	Loss: 0.1426
Training Epoch: 56 [7040/50048]	Loss: 0.1838
Training Epoch: 56 [7168/50048]	Loss: 0.2726
Training Epoch: 56 [7296/50048]	Loss: 0.1848
Training Epoch: 56 [7424/50048]	Loss: 0.1841
Training Epoch: 56 [7552/50048]	Loss: 0.1928
Training Epoch: 56 [7680/50048]	Loss: 0.1586
Training Epoch: 56 [7808/50048]	Loss: 0.1634
Training Epoch: 56 [7936/50048]	Loss: 0.2777
Training Epoch: 56 [8064/50048]	Loss: 0.1171
Training Epoch: 56 [8192/50048]	Loss: 0.1488
Training Epoch: 56 [8320/50048]	Loss: 0.2604
Training Epoch: 56 [8448/50048]	Loss: 0.2489
Training Epoch: 56 [8576/50048]	Loss: 0.2164
Training Epoch: 56 [8704/50048]	Loss: 0.1352
Training Epoch: 56 [8832/50048]	Loss: 0.1868
Training Epoch: 56 [8960/50048]	Loss: 0.1762
Training Epoch: 56 [9088/50048]	Loss: 0.1625
Training Epoch: 56 [9216/50048]	Loss: 0.2148
Training Epoch: 56 [9344/50048]	Loss: 0.1331
Training Epoch: 56 [9472/50048]	Loss: 0.1719
Training Epoch: 56 [9600/50048]	Loss: 0.2020
Training Epoch: 56 [9728/50048]	Loss: 0.1262
Training Epoch: 56 [9856/50048]	Loss: 0.2121
Training Epoch: 56 [9984/50048]	Loss: 0.2457
Training Epoch: 56 [10112/50048]	Loss: 0.2301
Training Epoch: 56 [10240/50048]	Loss: 0.2260
Training Epoch: 56 [10368/50048]	Loss: 0.2181
Training Epoch: 56 [10496/50048]	Loss: 0.1724
Training Epoch: 56 [10624/50048]	Loss: 0.1315
Training Epoch: 56 [10752/50048]	Loss: 0.2201
Training Epoch: 56 [10880/50048]	Loss: 0.2474
Training Epoch: 56 [11008/50048]	Loss: 0.1676
Training Epoch: 56 [11136/50048]	Loss: 0.2526
Training Epoch: 56 [11264/50048]	Loss: 0.2587
Training Epoch: 56 [11392/50048]	Loss: 0.3299
Training Epoch: 56 [11520/50048]	Loss: 0.1493
Training Epoch: 56 [11648/50048]	Loss: 0.1943
Training Epoch: 56 [11776/50048]	Loss: 0.3113
Training Epoch: 56 [11904/50048]	Loss: 0.2169
Training Epoch: 56 [12032/50048]	Loss: 0.2355
Training Epoch: 56 [12160/50048]	Loss: 0.2228
Training Epoch: 56 [12288/50048]	Loss: 0.1401
Training Epoch: 56 [12416/50048]	Loss: 0.1816
Training Epoch: 56 [12544/50048]	Loss: 0.2290
Training Epoch: 56 [12672/50048]	Loss: 0.2170
Training Epoch: 56 [12800/50048]	Loss: 0.2210
Training Epoch: 56 [12928/50048]	Loss: 0.2590
Training Epoch: 56 [13056/50048]	Loss: 0.2240
Training Epoch: 56 [13184/50048]	Loss: 0.2684
Training Epoch: 56 [13312/50048]	Loss: 0.3347
Training Epoch: 56 [13440/50048]	Loss: 0.1064
Training Epoch: 56 [13568/50048]	Loss: 0.1346
Training Epoch: 56 [13696/50048]	Loss: 0.1187
Training Epoch: 56 [13824/50048]	Loss: 0.2179
Training Epoch: 56 [13952/50048]	Loss: 0.1557
Training Epoch: 56 [14080/50048]	Loss: 0.2145
Training Epoch: 56 [14208/50048]	Loss: 0.1646
Training Epoch: 56 [14336/50048]	Loss: 0.1768
Training Epoch: 56 [14464/50048]	Loss: 0.1973
Training Epoch: 56 [14592/50048]	Loss: 0.2815
Training Epoch: 56 [14720/50048]	Loss: 0.2331
Training Epoch: 56 [14848/50048]	Loss: 0.2481
Training Epoch: 56 [14976/50048]	Loss: 0.2815
Training Epoch: 56 [15104/50048]	Loss: 0.3350
Training Epoch: 56 [15232/50048]	Loss: 0.2039
Training Epoch: 56 [15360/50048]	Loss: 0.2708
Training Epoch: 56 [15488/50048]	Loss: 0.1813
Training Epoch: 56 [15616/50048]	Loss: 0.3102
Training Epoch: 56 [15744/50048]	Loss: 0.2457
Training Epoch: 56 [15872/50048]	Loss: 0.3057
Training Epoch: 56 [16000/50048]	Loss: 0.2133
Training Epoch: 56 [16128/50048]	Loss: 0.2191
Training Epoch: 56 [16256/50048]	Loss: 0.1833
Training Epoch: 56 [16384/50048]	Loss: 0.1662
Training Epoch: 56 [16512/50048]	Loss: 0.1976
Training Epoch: 56 [16640/50048]	Loss: 0.1420
Training Epoch: 56 [16768/50048]	Loss: 0.2019
Training Epoch: 56 [16896/50048]	Loss: 0.1694
Training Epoch: 56 [17024/50048]	Loss: 0.1893
Training Epoch: 56 [17152/50048]	Loss: 0.1884
Training Epoch: 56 [17280/50048]	Loss: 0.2004
Training Epoch: 56 [17408/50048]	Loss: 0.2413
Training Epoch: 56 [17536/50048]	Loss: 0.2482
Training Epoch: 56 [17664/50048]	Loss: 0.1904
Training Epoch: 56 [17792/50048]	Loss: 0.2368
Training Epoch: 56 [17920/50048]	Loss: 0.2276
Training Epoch: 56 [18048/50048]	Loss: 0.2132
Training Epoch: 56 [18176/50048]	Loss: 0.3320
Training Epoch: 56 [18304/50048]	Loss: 0.2262
Training Epoch: 56 [18432/50048]	Loss: 0.2089
Training Epoch: 56 [18560/50048]	Loss: 0.1658
Training Epoch: 56 [18688/50048]	Loss: 0.2097
Training Epoch: 56 [18816/50048]	Loss: 0.1794
Training Epoch: 56 [18944/50048]	Loss: 0.2589
Training Epoch: 56 [19072/50048]	Loss: 0.2109
Training Epoch: 56 [19200/50048]	Loss: 0.1810
Training Epoch: 56 [19328/50048]	Loss: 0.2130
Training Epoch: 56 [19456/50048]	Loss: 0.2748
Training Epoch: 56 [19584/50048]	Loss: 0.2325
Training Epoch: 56 [19712/50048]	Loss: 0.2272
Training Epoch: 56 [19840/50048]	Loss: 0.1738
Training Epoch: 56 [19968/50048]	Loss: 0.1472
Training Epoch: 56 [20096/50048]	Loss: 0.2964
Training Epoch: 56 [20224/50048]	Loss: 0.2658
Training Epoch: 56 [20352/50048]	Loss: 0.2649
Training Epoch: 56 [20480/50048]	Loss: 0.2096
Training Epoch: 56 [20608/50048]	Loss: 0.1431
Training Epoch: 56 [20736/50048]	Loss: 0.2953
Training Epoch: 56 [20864/50048]	Loss: 0.1962
Training Epoch: 56 [20992/50048]	Loss: 0.2630
Training Epoch: 56 [21120/50048]	Loss: 0.2493
Training Epoch: 56 [21248/50048]	Loss: 0.1806
Training Epoch: 56 [21376/50048]	Loss: 0.1962
Training Epoch: 56 [21504/50048]	Loss: 0.2058
Training Epoch: 56 [21632/50048]	Loss: 0.1461
Training Epoch: 56 [21760/50048]	Loss: 0.2183
Training Epoch: 56 [21888/50048]	Loss: 0.1992
Training Epoch: 56 [22016/50048]	Loss: 0.2456
Training Epoch: 56 [22144/50048]	Loss: 0.1817
Training Epoch: 56 [22272/50048]	Loss: 0.2112
Training Epoch: 56 [22400/50048]	Loss: 0.2981
Training Epoch: 56 [22528/50048]	Loss: 0.2934
Training Epoch: 56 [22656/50048]	Loss: 0.2283
Training Epoch: 56 [22784/50048]	Loss: 0.3529
Training Epoch: 56 [22912/50048]	Loss: 0.2328
Training Epoch: 56 [23040/50048]	Loss: 0.2036
Training Epoch: 56 [23168/50048]	Loss: 0.1514
Training Epoch: 56 [23296/50048]	Loss: 0.3954
Training Epoch: 56 [23424/50048]	Loss: 0.2031
Training Epoch: 56 [23552/50048]	Loss: 0.1451
Training Epoch: 56 [23680/50048]	Loss: 0.3445
Training Epoch: 56 [23808/50048]	Loss: 0.2193
Training Epoch: 56 [23936/50048]	Loss: 0.2884
Training Epoch: 56 [24064/50048]	Loss: 0.1912
Training Epoch: 56 [24192/50048]	Loss: 0.2417
Training Epoch: 56 [24320/50048]	Loss: 0.2473
Training Epoch: 56 [24448/50048]	Loss: 0.3179
Training Epoch: 56 [24576/50048]	Loss: 0.2529
Training Epoch: 56 [24704/50048]	Loss: 0.1691
Training Epoch: 56 [24832/50048]	Loss: 0.1408
Training Epoch: 56 [24960/50048]	Loss: 0.1986
Training Epoch: 56 [25088/50048]	Loss: 0.2981
Training Epoch: 56 [25216/50048]	Loss: 0.1898
Training Epoch: 56 [25344/50048]	Loss: 0.2917
Training Epoch: 56 [25472/50048]	Loss: 0.2804
Training Epoch: 56 [25600/50048]	Loss: 0.2558
Training Epoch: 56 [25728/50048]	Loss: 0.2681
Training Epoch: 56 [25856/50048]	Loss: 0.2550
Training Epoch: 56 [25984/50048]	Loss: 0.2306
Training Epoch: 56 [26112/50048]	Loss: 0.1986
Training Epoch: 56 [26240/50048]	Loss: 0.2285
Training Epoch: 56 [26368/50048]	Loss: 0.1519
Training Epoch: 56 [26496/50048]	Loss: 0.1943
Training Epoch: 56 [26624/50048]	Loss: 0.2384
Training Epoch: 56 [26752/50048]	Loss: 0.2231
Training Epoch: 56 [26880/50048]	Loss: 0.2061
Training Epoch: 56 [27008/50048]	Loss: 0.2231
Training Epoch: 56 [27136/50048]	Loss: 0.1763
Training Epoch: 56 [27264/50048]	Loss: 0.2596
Training Epoch: 56 [27392/50048]	Loss: 0.2261
Training Epoch: 56 [27520/50048]	Loss: 0.1241
Training Epoch: 56 [27648/50048]	Loss: 0.2063
Training Epoch: 56 [27776/50048]	Loss: 0.2650
Training Epoch: 56 [27904/50048]	Loss: 0.2251
Training Epoch: 56 [28032/50048]	Loss: 0.3359
Training Epoch: 56 [28160/50048]	Loss: 0.1779
Training Epoch: 56 [28288/50048]	Loss: 0.1955
Training Epoch: 56 [28416/50048]	Loss: 0.2693
Training Epoch: 56 [28544/50048]	Loss: 0.2027
Training Epoch: 56 [28672/50048]	Loss: 0.2803
Training Epoch: 56 [28800/50048]	Loss: 0.2215
Training Epoch: 56 [28928/50048]	Loss: 0.2229
Training Epoch: 56 [29056/50048]	Loss: 0.1968
Training Epoch: 56 [29184/50048]	Loss: 0.2469
Training Epoch: 56 [29312/50048]	Loss: 0.1205
Training Epoch: 56 [29440/50048]	Loss: 0.2689
Training Epoch: 56 [29568/50048]	Loss: 0.1533
Training Epoch: 56 [29696/50048]	Loss: 0.2451
Training Epoch: 56 [29824/50048]	Loss: 0.2705
Training Epoch: 56 [29952/50048]	Loss: 0.2868
Training Epoch: 56 [30080/50048]	Loss: 0.3602
Training Epoch: 56 [30208/50048]	Loss: 0.3722
Training Epoch: 56 [30336/50048]	Loss: 0.1835
Training Epoch: 56 [30464/50048]	Loss: 0.3267
Training Epoch: 56 [30592/50048]	Loss: 0.2314
Training Epoch: 56 [30720/50048]	Loss: 0.1686
Training Epoch: 56 [30848/50048]	Loss: 0.2247
Training Epoch: 56 [30976/50048]	Loss: 0.1963
Training Epoch: 56 [31104/50048]	Loss: 0.1967
Training Epoch: 56 [31232/50048]	Loss: 0.2688
Training Epoch: 56 [31360/50048]	Loss: 0.1135
Training Epoch: 56 [31488/50048]	Loss: 0.3020
Training Epoch: 56 [31616/50048]	Loss: 0.2497
Training Epoch: 56 [31744/50048]	Loss: 0.2756
Training Epoch: 56 [31872/50048]	Loss: 0.1522
Training Epoch: 56 [32000/50048]	Loss: 0.1358
Training Epoch: 56 [32128/50048]	Loss: 0.1815
Training Epoch: 56 [32256/50048]	Loss: 0.1768
Training Epoch: 56 [32384/50048]	Loss: 0.2996
Training Epoch: 56 [32512/50048]	Loss: 0.1622
Training Epoch: 56 [32640/50048]	Loss: 0.3030
Training Epoch: 56 [32768/50048]	Loss: 0.3316
Training Epoch: 56 [32896/50048]	Loss: 0.1583
Training Epoch: 56 [33024/50048]	Loss: 0.1673
Training Epoch: 56 [33152/50048]	Loss: 0.1731
Training Epoch: 56 [33280/50048]	Loss: 0.2161
Training Epoch: 56 [33408/50048]	Loss: 0.2523
Training Epoch: 56 [33536/50048]	Loss: 0.1552
Training Epoch: 56 [33664/50048]	Loss: 0.1652
Training Epoch: 56 [33792/50048]	Loss: 0.2505
Training Epoch: 56 [33920/50048]	Loss: 0.2005
Training Epoch: 56 [34048/50048]	Loss: 0.2471
Training Epoch: 56 [34176/50048]	Loss: 0.1598
Training Epoch: 56 [34304/50048]	Loss: 0.1989
Training Epoch: 56 [34432/50048]	Loss: 0.2612
Training Epoch: 56 [34560/50048]	Loss: 0.1912
Training Epoch: 56 [34688/50048]	Loss: 0.1384
Training Epoch: 56 [34816/50048]	Loss: 0.2503
Training Epoch: 56 [34944/50048]	Loss: 0.4055
Training Epoch: 56 [35072/50048]	Loss: 0.2246
Training Epoch: 56 [35200/50048]	Loss: 0.2811
Training Epoch: 56 [35328/50048]	Loss: 0.3333
Training Epoch: 56 [35456/50048]	Loss: 0.2139
Training Epoch: 56 [35584/50048]	Loss: 0.2405
Training Epoch: 56 [35712/50048]	Loss: 0.2563
Training Epoch: 56 [35840/50048]	Loss: 0.2299
Training Epoch: 56 [35968/50048]	Loss: 0.2752
Training Epoch: 56 [36096/50048]	Loss: 0.2603
Training Epoch: 56 [36224/50048]	Loss: 0.3304
Training Epoch: 56 [36352/50048]	Loss: 0.1915
Training Epoch: 56 [36480/50048]	Loss: 0.1853
Training Epoch: 56 [36608/50048]	Loss: 0.2700
Training Epoch: 56 [36736/50048]	Loss: 0.2662
Training Epoch: 56 [36864/50048]	Loss: 0.1443
Training Epoch: 56 [36992/50048]	Loss: 0.2135
Training Epoch: 56 [37120/50048]	Loss: 0.2623
Training Epoch: 56 [37248/50048]	Loss: 0.1132
Training Epoch: 56 [37376/50048]	Loss: 0.1763
Training Epoch: 56 [37504/50048]	Loss: 0.2191
Training Epoch: 56 [37632/50048]	Loss: 0.2150
Training Epoch: 56 [37760/50048]	Loss: 0.1572
Training Epoch: 56 [37888/50048]	Loss: 0.2746
Training Epoch: 56 [38016/50048]	Loss: 0.2372
Training Epoch: 56 [38144/50048]	Loss: 0.3682
Training Epoch: 56 [38272/50048]	Loss: 0.3416
Training Epoch: 56 [38400/50048]	Loss: 0.1954
Training Epoch: 56 [38528/50048]	Loss: 0.2282
Training Epoch: 56 [38656/50048]	Loss: 0.2962
Training Epoch: 56 [38784/50048]	Loss: 0.2819
Training Epoch: 56 [38912/50048]	Loss: 0.4103
Training Epoch: 56 [39040/50048]	Loss: 0.2847
Training Epoch: 56 [39168/50048]	Loss: 0.2396
Training Epoch: 56 [39296/50048]	Loss: 0.2561
Training Epoch: 56 [39424/50048]	Loss: 0.1809
Training Epoch: 56 [39552/50048]	Loss: 0.2144
Training Epoch: 56 [39680/50048]	Loss: 0.2403
Training Epoch: 56 [39808/50048]	Loss: 0.1780
Training Epoch: 56 [39936/50048]	Loss: 0.2070
Training Epoch: 56 [40064/50048]	Loss: 0.1735
Training Epoch: 56 [40192/50048]	Loss: 0.2590
Training Epoch: 56 [40320/50048]	Loss: 0.2184
Training Epoch: 56 [40448/50048]	Loss: 0.1665
Training Epoch: 56 [40576/50048]	Loss: 0.2933
Training Epoch: 56 [40704/50048]	Loss: 0.2031
Training Epoch: 56 [40832/50048]	Loss: 0.2459
Training Epoch: 56 [40960/50048]	Loss: 0.2444
Training Epoch: 56 [41088/50048]	Loss: 0.2211
Training Epoch: 56 [41216/50048]	Loss: 0.2812
Training Epoch: 56 [41344/50048]	Loss: 0.1963
Training Epoch: 56 [41472/50048]	Loss: 0.3168
Training Epoch: 56 [41600/50048]	Loss: 0.3172
Training Epoch: 56 [41728/50048]	Loss: 0.1979
Training Epoch: 56 [41856/50048]	Loss: 0.3213
Training Epoch: 56 [41984/50048]	Loss: 0.1912
Training Epoch: 56 [42112/50048]	Loss: 0.3075
Training Epoch: 56 [42240/50048]	Loss: 0.2908
Training Epoch: 56 [42368/50048]	Loss: 0.2379
Training Epoch: 56 [42496/50048]	Loss: 0.2545
Training Epoch: 56 [42624/50048]	Loss: 0.1936
Training Epoch: 56 [42752/50048]	Loss: 0.1672
Training Epoch: 56 [42880/50048]	Loss: 0.2532
Training Epoch: 56 [43008/50048]	Loss: 0.3094
Training Epoch: 56 [43136/50048]	Loss: 0.1662
Training Epoch: 56 [43264/50048]	Loss: 0.3056
Training Epoch: 56 [43392/50048]	Loss: 0.2663
Training Epoch: 56 [43520/50048]	Loss: 0.2415
Training Epoch: 56 [43648/50048]	Loss: 0.2782
Training Epoch: 56 [43776/50048]	Loss: 0.3925
Training Epoch: 56 [43904/50048]	Loss: 0.2552
Training Epoch: 56 [44032/50048]	Loss: 0.2755
Training Epoch: 56 [44160/50048]	Loss: 0.2650
Training Epoch: 56 [44288/50048]	Loss: 0.2770
Training Epoch: 56 [44416/50048]	Loss: 0.2402
Training Epoch: 56 [44544/50048]	Loss: 0.2638
Training Epoch: 56 [44672/50048]	Loss: 0.2213
Training Epoch: 56 [44800/50048]	Loss: 0.2966
Training Epoch: 56 [44928/50048]	Loss: 0.1760
Training Epoch: 56 [45056/50048]	Loss: 0.2782
Training Epoch: 56 [45184/50048]	Loss: 0.2302
Training Epoch: 56 [45312/50048]	Loss: 0.2543
Training Epoch: 56 [45440/50048]	Loss: 0.2382
Training Epoch: 56 [45568/50048]	Loss: 0.1719
Training Epoch: 56 [45696/50048]	Loss: 0.2720
2022-12-06 07:33:32,432 [ZeusDataLoader(train)] train epoch 57 done: time=86.41 energy=10489.48
2022-12-06 07:33:32,433 [ZeusDataLoader(eval)] Epoch 57 begin.
Training Epoch: 56 [45824/50048]	Loss: 0.2941
Training Epoch: 56 [45952/50048]	Loss: 0.1972
Training Epoch: 56 [46080/50048]	Loss: 0.2153
Training Epoch: 56 [46208/50048]	Loss: 0.2312
Training Epoch: 56 [46336/50048]	Loss: 0.2471
Training Epoch: 56 [46464/50048]	Loss: 0.2641
Training Epoch: 56 [46592/50048]	Loss: 0.3703
Training Epoch: 56 [46720/50048]	Loss: 0.1494
Training Epoch: 56 [46848/50048]	Loss: 0.2765
Training Epoch: 56 [46976/50048]	Loss: 0.3653
Training Epoch: 56 [47104/50048]	Loss: 0.2571
Training Epoch: 56 [47232/50048]	Loss: 0.3090
Training Epoch: 56 [47360/50048]	Loss: 0.2562
Training Epoch: 56 [47488/50048]	Loss: 0.2588
Training Epoch: 56 [47616/50048]	Loss: 0.2658
Training Epoch: 56 [47744/50048]	Loss: 0.2161
Training Epoch: 56 [47872/50048]	Loss: 0.2714
Training Epoch: 56 [48000/50048]	Loss: 0.2824
Training Epoch: 56 [48128/50048]	Loss: 0.1523
Training Epoch: 56 [48256/50048]	Loss: 0.2313
Training Epoch: 56 [48384/50048]	Loss: 0.2408
Training Epoch: 56 [48512/50048]	Loss: 0.1403
Training Epoch: 56 [48640/50048]	Loss: 0.3335
Training Epoch: 56 [48768/50048]	Loss: 0.2547
Training Epoch: 56 [48896/50048]	Loss: 0.2110
Training Epoch: 56 [49024/50048]	Loss: 0.2684
Training Epoch: 56 [49152/50048]	Loss: 0.1994
Training Epoch: 56 [49280/50048]	Loss: 0.2179
Training Epoch: 56 [49408/50048]	Loss: 0.1992
Training Epoch: 56 [49536/50048]	Loss: 0.2966
Training Epoch: 56 [49664/50048]	Loss: 0.3161
Training Epoch: 56 [49792/50048]	Loss: 0.1203
Training Epoch: 56 [49920/50048]	Loss: 0.2151
Training Epoch: 56 [50048/50048]	Loss: 0.2971
2022-12-06 12:33:36.115 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:33:36,151 [ZeusDataLoader(eval)] eval epoch 57 done: time=3.71 energy=452.63
2022-12-06 07:33:36,151 [ZeusDataLoader(train)] Up to epoch 57: time=5141.28, energy=624105.09, cost=761914.46
2022-12-06 07:33:36,152 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:33:36,152 [ZeusDataLoader(train)] Expected next epoch: time=5231.08, energy=634903.10, cost=775170.84
2022-12-06 07:33:36,153 [ZeusDataLoader(train)] Epoch 58 begin.
Validation Epoch: 56, Average loss: 0.0154, Accuracy: 0.6337
2022-12-06 07:33:36,333 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:33:36,334 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:33:36.336 [ZeusMonitor] Monitor started.
2022-12-06 12:33:36.336 [ZeusMonitor] Running indefinitely. 2022-12-06 12:33:36.336 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:33:36.336 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e58+gpu0.power.log
Training Epoch: 57 [128/50048]	Loss: 0.2221
Training Epoch: 57 [256/50048]	Loss: 0.1119
Training Epoch: 57 [384/50048]	Loss: 0.1236
Training Epoch: 57 [512/50048]	Loss: 0.1310
Training Epoch: 57 [640/50048]	Loss: 0.2350
Training Epoch: 57 [768/50048]	Loss: 0.1672
Training Epoch: 57 [896/50048]	Loss: 0.1693
Training Epoch: 57 [1024/50048]	Loss: 0.1764
Training Epoch: 57 [1152/50048]	Loss: 0.2214
Training Epoch: 57 [1280/50048]	Loss: 0.1733
Training Epoch: 57 [1408/50048]	Loss: 0.2634
Training Epoch: 57 [1536/50048]	Loss: 0.1917
Training Epoch: 57 [1664/50048]	Loss: 0.2357
Training Epoch: 57 [1792/50048]	Loss: 0.1226
Training Epoch: 57 [1920/50048]	Loss: 0.2220
Training Epoch: 57 [2048/50048]	Loss: 0.1637
Training Epoch: 57 [2176/50048]	Loss: 0.2141
Training Epoch: 57 [2304/50048]	Loss: 0.1884
Training Epoch: 57 [2432/50048]	Loss: 0.3189
Training Epoch: 57 [2560/50048]	Loss: 0.2070
Training Epoch: 57 [2688/50048]	Loss: 0.1784
Training Epoch: 57 [2816/50048]	Loss: 0.1696
Training Epoch: 57 [2944/50048]	Loss: 0.1146
Training Epoch: 57 [3072/50048]	Loss: 0.2467
Training Epoch: 57 [3200/50048]	Loss: 0.2208
Training Epoch: 57 [3328/50048]	Loss: 0.1651
Training Epoch: 57 [3456/50048]	Loss: 0.2083
Training Epoch: 57 [3584/50048]	Loss: 0.1556
Training Epoch: 57 [3712/50048]	Loss: 0.1996
Training Epoch: 57 [3840/50048]	Loss: 0.1391
Training Epoch: 57 [3968/50048]	Loss: 0.1547
Training Epoch: 57 [4096/50048]	Loss: 0.2308
Training Epoch: 57 [4224/50048]	Loss: 0.2118
Training Epoch: 57 [4352/50048]	Loss: 0.1147
Training Epoch: 57 [4480/50048]	Loss: 0.1633
Training Epoch: 57 [4608/50048]	Loss: 0.2147
Training Epoch: 57 [4736/50048]	Loss: 0.2905
Training Epoch: 57 [4864/50048]	Loss: 0.2747
Training Epoch: 57 [4992/50048]	Loss: 0.1488
Training Epoch: 57 [5120/50048]	Loss: 0.1804
Training Epoch: 57 [5248/50048]	Loss: 0.1724
Training Epoch: 57 [5376/50048]	Loss: 0.1839
Training Epoch: 57 [5504/50048]	Loss: 0.1794
Training Epoch: 57 [5632/50048]	Loss: 0.1386
Training Epoch: 57 [5760/50048]	Loss: 0.1483
Training Epoch: 57 [5888/50048]	Loss: 0.1170
Training Epoch: 57 [6016/50048]	Loss: 0.2186
Training Epoch: 57 [6144/50048]	Loss: 0.2092
Training Epoch: 57 [6272/50048]	Loss: 0.2066
Training Epoch: 57 [6400/50048]	Loss: 0.1726
Training Epoch: 57 [6528/50048]	Loss: 0.2806
Training Epoch: 57 [6656/50048]	Loss: 0.3099
Training Epoch: 57 [6784/50048]	Loss: 0.2134
Training Epoch: 57 [6912/50048]	Loss: 0.1796
Training Epoch: 57 [7040/50048]	Loss: 0.2103
Training Epoch: 57 [7168/50048]	Loss: 0.1985
Training Epoch: 57 [7296/50048]	Loss: 0.2020
Training Epoch: 57 [7424/50048]	Loss: 0.1071
Training Epoch: 57 [7552/50048]	Loss: 0.2412
Training Epoch: 57 [7680/50048]	Loss: 0.2022
Training Epoch: 57 [7808/50048]	Loss: 0.1960
Training Epoch: 57 [7936/50048]	Loss: 0.0845
Training Epoch: 57 [8064/50048]	Loss: 0.1616
Training Epoch: 57 [8192/50048]	Loss: 0.2425
Training Epoch: 57 [8320/50048]	Loss: 0.1562
Training Epoch: 57 [8448/50048]	Loss: 0.1732
Training Epoch: 57 [8576/50048]	Loss: 0.2195
Training Epoch: 57 [8704/50048]	Loss: 0.1512
Training Epoch: 57 [8832/50048]	Loss: 0.1533
Training Epoch: 57 [8960/50048]	Loss: 0.1742
Training Epoch: 57 [9088/50048]	Loss: 0.1862
Training Epoch: 57 [9216/50048]	Loss: 0.1541
Training Epoch: 57 [9344/50048]	Loss: 0.1611
Training Epoch: 57 [9472/50048]	Loss: 0.1344
Training Epoch: 57 [9600/50048]	Loss: 0.2074
Training Epoch: 57 [9728/50048]	Loss: 0.2031
Training Epoch: 57 [9856/50048]	Loss: 0.1922
Training Epoch: 57 [9984/50048]	Loss: 0.2262
Training Epoch: 57 [10112/50048]	Loss: 0.2769
Training Epoch: 57 [10240/50048]	Loss: 0.1482
Training Epoch: 57 [10368/50048]	Loss: 0.1612
Training Epoch: 57 [10496/50048]	Loss: 0.1971
Training Epoch: 57 [10624/50048]	Loss: 0.1696
Training Epoch: 57 [10752/50048]	Loss: 0.1936
Training Epoch: 57 [10880/50048]	Loss: 0.2109
Training Epoch: 57 [11008/50048]	Loss: 0.1327
Training Epoch: 57 [11136/50048]	Loss: 0.2619
Training Epoch: 57 [11264/50048]	Loss: 0.3040
Training Epoch: 57 [11392/50048]	Loss: 0.1393
Training Epoch: 57 [11520/50048]	Loss: 0.1205
Training Epoch: 57 [11648/50048]	Loss: 0.1856
Training Epoch: 57 [11776/50048]	Loss: 0.1237
Training Epoch: 57 [11904/50048]	Loss: 0.1682
Training Epoch: 57 [12032/50048]	Loss: 0.1947
Training Epoch: 57 [12160/50048]	Loss: 0.2574
Training Epoch: 57 [12288/50048]	Loss: 0.1285
Training Epoch: 57 [12416/50048]	Loss: 0.1210
Training Epoch: 57 [12544/50048]	Loss: 0.1461
Training Epoch: 57 [12672/50048]	Loss: 0.1019
Training Epoch: 57 [12800/50048]	Loss: 0.1915
Training Epoch: 57 [12928/50048]	Loss: 0.2135
Training Epoch: 57 [13056/50048]	Loss: 0.1902
Training Epoch: 57 [13184/50048]	Loss: 0.1335
Training Epoch: 57 [13312/50048]	Loss: 0.1797
Training Epoch: 57 [13440/50048]	Loss: 0.1568
Training Epoch: 57 [13568/50048]	Loss: 0.2356
Training Epoch: 57 [13696/50048]	Loss: 0.1526
Training Epoch: 57 [13824/50048]	Loss: 0.2488
Training Epoch: 57 [13952/50048]	Loss: 0.1806
Training Epoch: 57 [14080/50048]	Loss: 0.1964
Training Epoch: 57 [14208/50048]	Loss: 0.1838
Training Epoch: 57 [14336/50048]	Loss: 0.2207
Training Epoch: 57 [14464/50048]	Loss: 0.2063
Training Epoch: 57 [14592/50048]	Loss: 0.2039
Training Epoch: 57 [14720/50048]	Loss: 0.2413
Training Epoch: 57 [14848/50048]	Loss: 0.1511
Training Epoch: 57 [14976/50048]	Loss: 0.2269
Training Epoch: 57 [15104/50048]	Loss: 0.1599
Training Epoch: 57 [15232/50048]	Loss: 0.2436
Training Epoch: 57 [15360/50048]	Loss: 0.1861
Training Epoch: 57 [15488/50048]	Loss: 0.2582
Training Epoch: 57 [15616/50048]	Loss: 0.1933
Training Epoch: 57 [15744/50048]	Loss: 0.2378
Training Epoch: 57 [15872/50048]	Loss: 0.1789
Training Epoch: 57 [16000/50048]	Loss: 0.2331
Training Epoch: 57 [16128/50048]	Loss: 0.1952
Training Epoch: 57 [16256/50048]	Loss: 0.2472
Training Epoch: 57 [16384/50048]	Loss: 0.0850
Training Epoch: 57 [16512/50048]	Loss: 0.2478
Training Epoch: 57 [16640/50048]	Loss: 0.1336
Training Epoch: 57 [16768/50048]	Loss: 0.1727
Training Epoch: 57 [16896/50048]	Loss: 0.2573
Training Epoch: 57 [17024/50048]	Loss: 0.1814
Training Epoch: 57 [17152/50048]	Loss: 0.1604
Training Epoch: 57 [17280/50048]	Loss: 0.3959
Training Epoch: 57 [17408/50048]	Loss: 0.1831
Training Epoch: 57 [17536/50048]	Loss: 0.1881
Training Epoch: 57 [17664/50048]	Loss: 0.2059
Training Epoch: 57 [17792/50048]	Loss: 0.2617
Training Epoch: 57 [17920/50048]	Loss: 0.1573
Training Epoch: 57 [18048/50048]	Loss: 0.3351
Training Epoch: 57 [18176/50048]	Loss: 0.1614
Training Epoch: 57 [18304/50048]	Loss: 0.1451
Training Epoch: 57 [18432/50048]	Loss: 0.1565
Training Epoch: 57 [18560/50048]	Loss: 0.1916
Training Epoch: 57 [18688/50048]	Loss: 0.1633
Training Epoch: 57 [18816/50048]	Loss: 0.2596
Training Epoch: 57 [18944/50048]	Loss: 0.2750
Training Epoch: 57 [19072/50048]	Loss: 0.2079
Training Epoch: 57 [19200/50048]	Loss: 0.2052
Training Epoch: 57 [19328/50048]	Loss: 0.2578
Training Epoch: 57 [19456/50048]	Loss: 0.1113
Training Epoch: 57 [19584/50048]	Loss: 0.2578
Training Epoch: 57 [19712/50048]	Loss: 0.2337
Training Epoch: 57 [19840/50048]	Loss: 0.1067
Training Epoch: 57 [19968/50048]	Loss: 0.1669
Training Epoch: 57 [20096/50048]	Loss: 0.1994
Training Epoch: 57 [20224/50048]	Loss: 0.2599
Training Epoch: 57 [20352/50048]	Loss: 0.1936
Training Epoch: 57 [20480/50048]	Loss: 0.2551
Training Epoch: 57 [20608/50048]	Loss: 0.2184
Training Epoch: 57 [20736/50048]	Loss: 0.1960
Training Epoch: 57 [20864/50048]	Loss: 0.2714
Training Epoch: 57 [20992/50048]	Loss: 0.2383
Training Epoch: 57 [21120/50048]	Loss: 0.1858
Training Epoch: 57 [21248/50048]	Loss: 0.3280
Training Epoch: 57 [21376/50048]	Loss: 0.2913
Training Epoch: 57 [21504/50048]	Loss: 0.2295
Training Epoch: 57 [21632/50048]	Loss: 0.3486
Training Epoch: 57 [21760/50048]	Loss: 0.1458
Training Epoch: 57 [21888/50048]	Loss: 0.1916
Training Epoch: 57 [22016/50048]	Loss: 0.1188
Training Epoch: 57 [22144/50048]	Loss: 0.2036
Training Epoch: 57 [22272/50048]	Loss: 0.1820
Training Epoch: 57 [22400/50048]	Loss: 0.1831
Training Epoch: 57 [22528/50048]	Loss: 0.1822
Training Epoch: 57 [22656/50048]	Loss: 0.2162
Training Epoch: 57 [22784/50048]	Loss: 0.1255
Training Epoch: 57 [22912/50048]	Loss: 0.2329
Training Epoch: 57 [23040/50048]	Loss: 0.2133
Training Epoch: 57 [23168/50048]	Loss: 0.2084
Training Epoch: 57 [23296/50048]	Loss: 0.2030
Training Epoch: 57 [23424/50048]	Loss: 0.2353
Training Epoch: 57 [23552/50048]	Loss: 0.2178
Training Epoch: 57 [23680/50048]	Loss: 0.2030
Training Epoch: 57 [23808/50048]	Loss: 0.2670
Training Epoch: 57 [23936/50048]	Loss: 0.2017
Training Epoch: 57 [24064/50048]	Loss: 0.2618
Training Epoch: 57 [24192/50048]	Loss: 0.3335
Training Epoch: 57 [24320/50048]	Loss: 0.2494
Training Epoch: 57 [24448/50048]	Loss: 0.2112
Training Epoch: 57 [24576/50048]	Loss: 0.1574
Training Epoch: 57 [24704/50048]	Loss: 0.2388
Training Epoch: 57 [24832/50048]	Loss: 0.2610
Training Epoch: 57 [24960/50048]	Loss: 0.1222
Training Epoch: 57 [25088/50048]	Loss: 0.2169
Training Epoch: 57 [25216/50048]	Loss: 0.1857
Training Epoch: 57 [25344/50048]	Loss: 0.1445
Training Epoch: 57 [25472/50048]	Loss: 0.2635
Training Epoch: 57 [25600/50048]	Loss: 0.1460
Training Epoch: 57 [25728/50048]	Loss: 0.2340
Training Epoch: 57 [25856/50048]	Loss: 0.1989
Training Epoch: 57 [25984/50048]	Loss: 0.1984
Training Epoch: 57 [26112/50048]	Loss: 0.1030
Training Epoch: 57 [26240/50048]	Loss: 0.2140
Training Epoch: 57 [26368/50048]	Loss: 0.1281
Training Epoch: 57 [26496/50048]	Loss: 0.2675
Training Epoch: 57 [26624/50048]	Loss: 0.1933
Training Epoch: 57 [26752/50048]	Loss: 0.2704
Training Epoch: 57 [26880/50048]	Loss: 0.2125
Training Epoch: 57 [27008/50048]	Loss: 0.3034
Training Epoch: 57 [27136/50048]	Loss: 0.1670
Training Epoch: 57 [27264/50048]	Loss: 0.1961
Training Epoch: 57 [27392/50048]	Loss: 0.1458
Training Epoch: 57 [27520/50048]	Loss: 0.2321
Training Epoch: 57 [27648/50048]	Loss: 0.2077
Training Epoch: 57 [27776/50048]	Loss: 0.3031
Training Epoch: 57 [27904/50048]	Loss: 0.2636
Training Epoch: 57 [28032/50048]	Loss: 0.1777
Training Epoch: 57 [28160/50048]	Loss: 0.2803
Training Epoch: 57 [28288/50048]	Loss: 0.2275
Training Epoch: 57 [28416/50048]	Loss: 0.2409
Training Epoch: 57 [28544/50048]	Loss: 0.2607
Training Epoch: 57 [28672/50048]	Loss: 0.1493
Training Epoch: 57 [28800/50048]	Loss: 0.2512
Training Epoch: 57 [28928/50048]	Loss: 0.2820
Training Epoch: 57 [29056/50048]	Loss: 0.2719
Training Epoch: 57 [29184/50048]	Loss: 0.1654
Training Epoch: 57 [29312/50048]	Loss: 0.1816
Training Epoch: 57 [29440/50048]	Loss: 0.2548
Training Epoch: 57 [29568/50048]	Loss: 0.2293
Training Epoch: 57 [29696/50048]	Loss: 0.2435
Training Epoch: 57 [29824/50048]	Loss: 0.3016
Training Epoch: 57 [29952/50048]	Loss: 0.1885
Training Epoch: 57 [30080/50048]	Loss: 0.3502
Training Epoch: 57 [30208/50048]	Loss: 0.3928
Training Epoch: 57 [30336/50048]	Loss: 0.1400
Training Epoch: 57 [30464/50048]	Loss: 0.1947
Training Epoch: 57 [30592/50048]	Loss: 0.2627
Training Epoch: 57 [30720/50048]	Loss: 0.3834
Training Epoch: 57 [30848/50048]	Loss: 0.2166
Training Epoch: 57 [30976/50048]	Loss: 0.1436
Training Epoch: 57 [31104/50048]	Loss: 0.1664
Training Epoch: 57 [31232/50048]	Loss: 0.1651
Training Epoch: 57 [31360/50048]	Loss: 0.2297
Training Epoch: 57 [31488/50048]	Loss: 0.3070
Training Epoch: 57 [31616/50048]	Loss: 0.2067
Training Epoch: 57 [31744/50048]	Loss: 0.2450
Training Epoch: 57 [31872/50048]	Loss: 0.3315
Training Epoch: 57 [32000/50048]	Loss: 0.3951
Training Epoch: 57 [32128/50048]	Loss: 0.1906
Training Epoch: 57 [32256/50048]	Loss: 0.3096
Training Epoch: 57 [32384/50048]	Loss: 0.1845
Training Epoch: 57 [32512/50048]	Loss: 0.1751
Training Epoch: 57 [32640/50048]	Loss: 0.3076
Training Epoch: 57 [32768/50048]	Loss: 0.2041
Training Epoch: 57 [32896/50048]	Loss: 0.2560
Training Epoch: 57 [33024/50048]	Loss: 0.2582
Training Epoch: 57 [33152/50048]	Loss: 0.2333
Training Epoch: 57 [33280/50048]	Loss: 0.2339
Training Epoch: 57 [33408/50048]	Loss: 0.1587
Training Epoch: 57 [33536/50048]	Loss: 0.3362
Training Epoch: 57 [33664/50048]	Loss: 0.2241
Training Epoch: 57 [33792/50048]	Loss: 0.1899
Training Epoch: 57 [33920/50048]	Loss: 0.3510
Training Epoch: 57 [34048/50048]	Loss: 0.2880
Training Epoch: 57 [34176/50048]	Loss: 0.1951
Training Epoch: 57 [34304/50048]	Loss: 0.3189
Training Epoch: 57 [34432/50048]	Loss: 0.2634
Training Epoch: 57 [34560/50048]	Loss: 0.2213
Training Epoch: 57 [34688/50048]	Loss: 0.2250
Training Epoch: 57 [34816/50048]	Loss: 0.1730
Training Epoch: 57 [34944/50048]	Loss: 0.1600
Training Epoch: 57 [35072/50048]	Loss: 0.2096
Training Epoch: 57 [35200/50048]	Loss: 0.2046
Training Epoch: 57 [35328/50048]	Loss: 0.1740
Training Epoch: 57 [35456/50048]	Loss: 0.2951
Training Epoch: 57 [35584/50048]	Loss: 0.1583
Training Epoch: 57 [35712/50048]	Loss: 0.2152
Training Epoch: 57 [35840/50048]	Loss: 0.2491
Training Epoch: 57 [35968/50048]	Loss: 0.1651
Training Epoch: 57 [36096/50048]	Loss: 0.1689
Training Epoch: 57 [36224/50048]	Loss: 0.2158
Training Epoch: 57 [36352/50048]	Loss: 0.1317
Training Epoch: 57 [36480/50048]	Loss: 0.1679
Training Epoch: 57 [36608/50048]	Loss: 0.2559
Training Epoch: 57 [36736/50048]	Loss: 0.2811
Training Epoch: 57 [36864/50048]	Loss: 0.2353
Training Epoch: 57 [36992/50048]	Loss: 0.2119
Training Epoch: 57 [37120/50048]	Loss: 0.2246
Training Epoch: 57 [37248/50048]	Loss: 0.2206
Training Epoch: 57 [37376/50048]	Loss: 0.2315
Training Epoch: 57 [37504/50048]	Loss: 0.1295
Training Epoch: 57 [37632/50048]	Loss: 0.2228
Training Epoch: 57 [37760/50048]	Loss: 0.2192
Training Epoch: 57 [37888/50048]	Loss: 0.2059
Training Epoch: 57 [38016/50048]	Loss: 0.2515
Training Epoch: 57 [38144/50048]	Loss: 0.1999
Training Epoch: 57 [38272/50048]	Loss: 0.2694
Training Epoch: 57 [38400/50048]	Loss: 0.3768
Training Epoch: 57 [38528/50048]	Loss: 0.3278
Training Epoch: 57 [38656/50048]	Loss: 0.2499
Training Epoch: 57 [38784/50048]	Loss: 0.2309
Training Epoch: 57 [38912/50048]	Loss: 0.3474
Training Epoch: 57 [39040/50048]	Loss: 0.2879
Training Epoch: 57 [39168/50048]	Loss: 0.2874
Training Epoch: 57 [39296/50048]	Loss: 0.2844
Training Epoch: 57 [39424/50048]	Loss: 0.1706
Training Epoch: 57 [39552/50048]	Loss: 0.2302
Training Epoch: 57 [39680/50048]	Loss: 0.1308
Training Epoch: 57 [39808/50048]	Loss: 0.2811
Training Epoch: 57 [39936/50048]	Loss: 0.3031
Training Epoch: 57 [40064/50048]	Loss: 0.3057
Training Epoch: 57 [40192/50048]	Loss: 0.2796
Training Epoch: 57 [40320/50048]	Loss: 0.1843
Training Epoch: 57 [40448/50048]	Loss: 0.2814
Training Epoch: 57 [40576/50048]	Loss: 0.2209
Training Epoch: 57 [40704/50048]	Loss: 0.2919
Training Epoch: 57 [40832/50048]	Loss: 0.2648
Training Epoch: 57 [40960/50048]	Loss: 0.2886
Training Epoch: 57 [41088/50048]	Loss: 0.1719
Training Epoch: 57 [41216/50048]	Loss: 0.2935
Training Epoch: 57 [41344/50048]	Loss: 0.2919
Training Epoch: 57 [41472/50048]	Loss: 0.2637
Training Epoch: 57 [41600/50048]	Loss: 0.3123
Training Epoch: 57 [41728/50048]	Loss: 0.1927
Training Epoch: 57 [41856/50048]	Loss: 0.2977
Training Epoch: 57 [41984/50048]	Loss: 0.2347
Training Epoch: 57 [42112/50048]	Loss: 0.2874
Training Epoch: 57 [42240/50048]	Loss: 0.1949
Training Epoch: 57 [42368/50048]	Loss: 0.3044
Training Epoch: 57 [42496/50048]	Loss: 0.1959
Training Epoch: 57 [42624/50048]	Loss: 0.2195
Training Epoch: 57 [42752/50048]	Loss: 0.1791
Training Epoch: 57 [42880/50048]	Loss: 0.3535
Training Epoch: 57 [43008/50048]	Loss: 0.2336
Training Epoch: 57 [43136/50048]	Loss: 0.2291
Training Epoch: 57 [43264/50048]	Loss: 0.2734
Training Epoch: 57 [43392/50048]	Loss: 0.2729
Training Epoch: 57 [43520/50048]	Loss: 0.2898
Training Epoch: 57 [43648/50048]	Loss: 0.2934
Training Epoch: 57 [43776/50048]	Loss: 0.2755
Training Epoch: 57 [43904/50048]	Loss: 0.2828
Training Epoch: 57 [44032/50048]	Loss: 0.1967
Training Epoch: 57 [44160/50048]	Loss: 0.2897
Training Epoch: 57 [44288/50048]	Loss: 0.2694
Training Epoch: 57 [44416/50048]	Loss: 0.2758
Training Epoch: 57 [44544/50048]	Loss: 0.2493
Training Epoch: 57 [44672/50048]	Loss: 0.2152
Training Epoch: 57 [44800/50048]	Loss: 0.2412
Training Epoch: 57 [44928/50048]	Loss: 0.1828
Training Epoch: 57 [45056/50048]	Loss: 0.2334
Training Epoch: 57 [45184/50048]	Loss: 0.2962
Training Epoch: 57 [45312/50048]	Loss: 0.2186
Training Epoch: 57 [45440/50048]	Loss: 0.2247
Training Epoch: 57 [45568/50048]	Loss: 0.2141
Training Epoch: 57 [45696/50048]	Loss: 0.3165
2022-12-06 07:35:02,658 [ZeusDataLoader(train)] train epoch 58 done: time=86.49 energy=10500.43
2022-12-06 07:35:02,659 [ZeusDataLoader(eval)] Epoch 58 begin.
Training Epoch: 57 [45824/50048]	Loss: 0.2527
Training Epoch: 57 [45952/50048]	Loss: 0.3069
Training Epoch: 57 [46080/50048]	Loss: 0.2768
Training Epoch: 57 [46208/50048]	Loss: 0.2750
Training Epoch: 57 [46336/50048]	Loss: 0.2857
Training Epoch: 57 [46464/50048]	Loss: 0.2200
Training Epoch: 57 [46592/50048]	Loss: 0.2066
Training Epoch: 57 [46720/50048]	Loss: 0.1842
Training Epoch: 57 [46848/50048]	Loss: 0.3472
Training Epoch: 57 [46976/50048]	Loss: 0.1678
Training Epoch: 57 [47104/50048]	Loss: 0.2414
Training Epoch: 57 [47232/50048]	Loss: 0.2055
Training Epoch: 57 [47360/50048]	Loss: 0.2354
Training Epoch: 57 [47488/50048]	Loss: 0.2274
Training Epoch: 57 [47616/50048]	Loss: 0.2926
Training Epoch: 57 [47744/50048]	Loss: 0.3248
Training Epoch: 57 [47872/50048]	Loss: 0.1856
Training Epoch: 57 [48000/50048]	Loss: 0.2113
Training Epoch: 57 [48128/50048]	Loss: 0.3002
Training Epoch: 57 [48256/50048]	Loss: 0.1757
Training Epoch: 57 [48384/50048]	Loss: 0.2743
Training Epoch: 57 [48512/50048]	Loss: 0.2097
Training Epoch: 57 [48640/50048]	Loss: 0.2485
Training Epoch: 57 [48768/50048]	Loss: 0.2287
Training Epoch: 57 [48896/50048]	Loss: 0.2093
Training Epoch: 57 [49024/50048]	Loss: 0.2835
Training Epoch: 57 [49152/50048]	Loss: 0.1979
Training Epoch: 57 [49280/50048]	Loss: 0.2957
Training Epoch: 57 [49408/50048]	Loss: 0.2914
Training Epoch: 57 [49536/50048]	Loss: 0.2847
Training Epoch: 57 [49664/50048]	Loss: 0.3188
Training Epoch: 57 [49792/50048]	Loss: 0.2086
Training Epoch: 57 [49920/50048]	Loss: 0.2840
Training Epoch: 57 [50048/50048]	Loss: 0.2576
2022-12-06 12:35:06.420 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:35:06,445 [ZeusDataLoader(eval)] eval epoch 58 done: time=3.78 energy=451.80
2022-12-06 07:35:06,445 [ZeusDataLoader(train)] Up to epoch 58: time=5231.55, energy=635057.31, cost=775289.33
2022-12-06 07:35:06,445 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:35:06,445 [ZeusDataLoader(train)] Expected next epoch: time=5321.35, energy=645855.33, cost=788545.72
2022-12-06 07:35:06,446 [ZeusDataLoader(train)] Epoch 59 begin.
Validation Epoch: 57, Average loss: 0.0158, Accuracy: 0.6308
2022-12-06 07:35:06,634 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:35:06,634 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:35:06.636 [ZeusMonitor] Monitor started.
2022-12-06 12:35:06.636 [ZeusMonitor] Running indefinitely. 2022-12-06 12:35:06.636 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:35:06.636 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e59+gpu0.power.log
Training Epoch: 58 [128/50048]	Loss: 0.1777
Training Epoch: 58 [256/50048]	Loss: 0.1046
Training Epoch: 58 [384/50048]	Loss: 0.1767
Training Epoch: 58 [512/50048]	Loss: 0.1971
Training Epoch: 58 [640/50048]	Loss: 0.2454
Training Epoch: 58 [768/50048]	Loss: 0.2864
Training Epoch: 58 [896/50048]	Loss: 0.1246
Training Epoch: 58 [1024/50048]	Loss: 0.2306
Training Epoch: 58 [1152/50048]	Loss: 0.1336
Training Epoch: 58 [1280/50048]	Loss: 0.2583
Training Epoch: 58 [1408/50048]	Loss: 0.1190
Training Epoch: 58 [1536/50048]	Loss: 0.2256
Training Epoch: 58 [1664/50048]	Loss: 0.1549
Training Epoch: 58 [1792/50048]	Loss: 0.1341
Training Epoch: 58 [1920/50048]	Loss: 0.1606
Training Epoch: 58 [2048/50048]	Loss: 0.2048
Training Epoch: 58 [2176/50048]	Loss: 0.1911
Training Epoch: 58 [2304/50048]	Loss: 0.1705
Training Epoch: 58 [2432/50048]	Loss: 0.1910
Training Epoch: 58 [2560/50048]	Loss: 0.2212
Training Epoch: 58 [2688/50048]	Loss: 0.2368
Training Epoch: 58 [2816/50048]	Loss: 0.2212
Training Epoch: 58 [2944/50048]	Loss: 0.2219
Training Epoch: 58 [3072/50048]	Loss: 0.1278
Training Epoch: 58 [3200/50048]	Loss: 0.1808
Training Epoch: 58 [3328/50048]	Loss: 0.1772
Training Epoch: 58 [3456/50048]	Loss: 0.1467
Training Epoch: 58 [3584/50048]	Loss: 0.1868
Training Epoch: 58 [3712/50048]	Loss: 0.1577
Training Epoch: 58 [3840/50048]	Loss: 0.1356
Training Epoch: 58 [3968/50048]	Loss: 0.1648
Training Epoch: 58 [4096/50048]	Loss: 0.1450
Training Epoch: 58 [4224/50048]	Loss: 0.1702
Training Epoch: 58 [4352/50048]	Loss: 0.2026
Training Epoch: 58 [4480/50048]	Loss: 0.2081
Training Epoch: 58 [4608/50048]	Loss: 0.1357
Training Epoch: 58 [4736/50048]	Loss: 0.1396
Training Epoch: 58 [4864/50048]	Loss: 0.1550
Training Epoch: 58 [4992/50048]	Loss: 0.1364
Training Epoch: 58 [5120/50048]	Loss: 0.1451
Training Epoch: 58 [5248/50048]	Loss: 0.1219
Training Epoch: 58 [5376/50048]	Loss: 0.1755
Training Epoch: 58 [5504/50048]	Loss: 0.1449
Training Epoch: 58 [5632/50048]	Loss: 0.2141
Training Epoch: 58 [5760/50048]	Loss: 0.2067
Training Epoch: 58 [5888/50048]	Loss: 0.1622
Training Epoch: 58 [6016/50048]	Loss: 0.2702
Training Epoch: 58 [6144/50048]	Loss: 0.2044
Training Epoch: 58 [6272/50048]	Loss: 0.3161
Training Epoch: 58 [6400/50048]	Loss: 0.1238
Training Epoch: 58 [6528/50048]	Loss: 0.1510
Training Epoch: 58 [6656/50048]	Loss: 0.1965
Training Epoch: 58 [6784/50048]	Loss: 0.1072
Training Epoch: 58 [6912/50048]	Loss: 0.0888
Training Epoch: 58 [7040/50048]	Loss: 0.1889
Training Epoch: 58 [7168/50048]	Loss: 0.1542
Training Epoch: 58 [7296/50048]	Loss: 0.2576
Training Epoch: 58 [7424/50048]	Loss: 0.2345
Training Epoch: 58 [7552/50048]	Loss: 0.2373
Training Epoch: 58 [7680/50048]	Loss: 0.1968
Training Epoch: 58 [7808/50048]	Loss: 0.1205
Training Epoch: 58 [7936/50048]	Loss: 0.3048
Training Epoch: 58 [8064/50048]	Loss: 0.2808
Training Epoch: 58 [8192/50048]	Loss: 0.2038
Training Epoch: 58 [8320/50048]	Loss: 0.2259
Training Epoch: 58 [8448/50048]	Loss: 0.1559
Training Epoch: 58 [8576/50048]	Loss: 0.2323
Training Epoch: 58 [8704/50048]	Loss: 0.1219
Training Epoch: 58 [8832/50048]	Loss: 0.2117
Training Epoch: 58 [8960/50048]	Loss: 0.2578
Training Epoch: 58 [9088/50048]	Loss: 0.1511
Training Epoch: 58 [9216/50048]	Loss: 0.2065
Training Epoch: 58 [9344/50048]	Loss: 0.1581
Training Epoch: 58 [9472/50048]	Loss: 0.1705
Training Epoch: 58 [9600/50048]	Loss: 0.1508
Training Epoch: 58 [9728/50048]	Loss: 0.2011
Training Epoch: 58 [9856/50048]	Loss: 0.1816
Training Epoch: 58 [9984/50048]	Loss: 0.1611
Training Epoch: 58 [10112/50048]	Loss: 0.1212
Training Epoch: 58 [10240/50048]	Loss: 0.2261
Training Epoch: 58 [10368/50048]	Loss: 0.2410
Training Epoch: 58 [10496/50048]	Loss: 0.2392
Training Epoch: 58 [10624/50048]	Loss: 0.1433
Training Epoch: 58 [10752/50048]	Loss: 0.2304
Training Epoch: 58 [10880/50048]	Loss: 0.2276
Training Epoch: 58 [11008/50048]	Loss: 0.1383
Training Epoch: 58 [11136/50048]	Loss: 0.1587
Training Epoch: 58 [11264/50048]	Loss: 0.1022
Training Epoch: 58 [11392/50048]	Loss: 0.2129
Training Epoch: 58 [11520/50048]	Loss: 0.2731
Training Epoch: 58 [11648/50048]	Loss: 0.1975
Training Epoch: 58 [11776/50048]	Loss: 0.3187
Training Epoch: 58 [11904/50048]	Loss: 0.1524
Training Epoch: 58 [12032/50048]	Loss: 0.2643
Training Epoch: 58 [12160/50048]	Loss: 0.2408
Training Epoch: 58 [12288/50048]	Loss: 0.2684
Training Epoch: 58 [12416/50048]	Loss: 0.1535
Training Epoch: 58 [12544/50048]	Loss: 0.1913
Training Epoch: 58 [12672/50048]	Loss: 0.1692
Training Epoch: 58 [12800/50048]	Loss: 0.1243
Training Epoch: 58 [12928/50048]	Loss: 0.2232
Training Epoch: 58 [13056/50048]	Loss: 0.1029
Training Epoch: 58 [13184/50048]	Loss: 0.2001
Training Epoch: 58 [13312/50048]	Loss: 0.1769
Training Epoch: 58 [13440/50048]	Loss: 0.1205
Training Epoch: 58 [13568/50048]	Loss: 0.1687
Training Epoch: 58 [13696/50048]	Loss: 0.1220
Training Epoch: 58 [13824/50048]	Loss: 0.2171
Training Epoch: 58 [13952/50048]	Loss: 0.1470
Training Epoch: 58 [14080/50048]	Loss: 0.1106
Training Epoch: 58 [14208/50048]	Loss: 0.1788
Training Epoch: 58 [14336/50048]	Loss: 0.1249
Training Epoch: 58 [14464/50048]	Loss: 0.1144
Training Epoch: 58 [14592/50048]	Loss: 0.1600
Training Epoch: 58 [14720/50048]	Loss: 0.4188
Training Epoch: 58 [14848/50048]	Loss: 0.2242
Training Epoch: 58 [14976/50048]	Loss: 0.1518
Training Epoch: 58 [15104/50048]	Loss: 0.2842
Training Epoch: 58 [15232/50048]	Loss: 0.1797
Training Epoch: 58 [15360/50048]	Loss: 0.3206
Training Epoch: 58 [15488/50048]	Loss: 0.3135
Training Epoch: 58 [15616/50048]	Loss: 0.1925
Training Epoch: 58 [15744/50048]	Loss: 0.2067
Training Epoch: 58 [15872/50048]	Loss: 0.2580
Training Epoch: 58 [16000/50048]	Loss: 0.1531
Training Epoch: 58 [16128/50048]	Loss: 0.2422
Training Epoch: 58 [16256/50048]	Loss: 0.1863
Training Epoch: 58 [16384/50048]	Loss: 0.2463
Training Epoch: 58 [16512/50048]	Loss: 0.1626
Training Epoch: 58 [16640/50048]	Loss: 0.2230
Training Epoch: 58 [16768/50048]	Loss: 0.2333
Training Epoch: 58 [16896/50048]	Loss: 0.2242
Training Epoch: 58 [17024/50048]	Loss: 0.2570
Training Epoch: 58 [17152/50048]	Loss: 0.2142
Training Epoch: 58 [17280/50048]	Loss: 0.1803
Training Epoch: 58 [17408/50048]	Loss: 0.1915
Training Epoch: 58 [17536/50048]	Loss: 0.2021
Training Epoch: 58 [17664/50048]	Loss: 0.2233
Training Epoch: 58 [17792/50048]	Loss: 0.2514
Training Epoch: 58 [17920/50048]	Loss: 0.2220
Training Epoch: 58 [18048/50048]	Loss: 0.1155
Training Epoch: 58 [18176/50048]	Loss: 0.3249
Training Epoch: 58 [18304/50048]	Loss: 0.2365
Training Epoch: 58 [18432/50048]	Loss: 0.2316
Training Epoch: 58 [18560/50048]	Loss: 0.2279
Training Epoch: 58 [18688/50048]	Loss: 0.1533
Training Epoch: 58 [18816/50048]	Loss: 0.1413
Training Epoch: 58 [18944/50048]	Loss: 0.2696
Training Epoch: 58 [19072/50048]	Loss: 0.2201
Training Epoch: 58 [19200/50048]	Loss: 0.2474
Training Epoch: 58 [19328/50048]	Loss: 0.2409
Training Epoch: 58 [19456/50048]	Loss: 0.1742
Training Epoch: 58 [19584/50048]	Loss: 0.2064
Training Epoch: 58 [19712/50048]	Loss: 0.2084
Training Epoch: 58 [19840/50048]	Loss: 0.0901
Training Epoch: 58 [19968/50048]	Loss: 0.1610
Training Epoch: 58 [20096/50048]	Loss: 0.2447
Training Epoch: 58 [20224/50048]	Loss: 0.2755
Training Epoch: 58 [20352/50048]	Loss: 0.2452
Training Epoch: 58 [20480/50048]	Loss: 0.0771
Training Epoch: 58 [20608/50048]	Loss: 0.2359
Training Epoch: 58 [20736/50048]	Loss: 0.2015
Training Epoch: 58 [20864/50048]	Loss: 0.3002
Training Epoch: 58 [20992/50048]	Loss: 0.2977
Training Epoch: 58 [21120/50048]	Loss: 0.2858
Training Epoch: 58 [21248/50048]	Loss: 0.1628
Training Epoch: 58 [21376/50048]	Loss: 0.3229
Training Epoch: 58 [21504/50048]	Loss: 0.2082
Training Epoch: 58 [21632/50048]	Loss: 0.1436
Training Epoch: 58 [21760/50048]	Loss: 0.2292
Training Epoch: 58 [21888/50048]	Loss: 0.1273
Training Epoch: 58 [22016/50048]	Loss: 0.1796
Training Epoch: 58 [22144/50048]	Loss: 0.2533
Training Epoch: 58 [22272/50048]	Loss: 0.1635
Training Epoch: 58 [22400/50048]	Loss: 0.1056
Training Epoch: 58 [22528/50048]	Loss: 0.1888
Training Epoch: 58 [22656/50048]	Loss: 0.1911
Training Epoch: 58 [22784/50048]	Loss: 0.1515
Training Epoch: 58 [22912/50048]	Loss: 0.2558
Training Epoch: 58 [23040/50048]	Loss: 0.1524
Training Epoch: 58 [23168/50048]	Loss: 0.1783
Training Epoch: 58 [23296/50048]	Loss: 0.2471
Training Epoch: 58 [23424/50048]	Loss: 0.2216
Training Epoch: 58 [23552/50048]	Loss: 0.2358
Training Epoch: 58 [23680/50048]	Loss: 0.1812
Training Epoch: 58 [23808/50048]	Loss: 0.1958
Training Epoch: 58 [23936/50048]	Loss: 0.2526
Training Epoch: 58 [24064/50048]	Loss: 0.2001
Training Epoch: 58 [24192/50048]	Loss: 0.3438
Training Epoch: 58 [24320/50048]	Loss: 0.2208
Training Epoch: 58 [24448/50048]	Loss: 0.1726
Training Epoch: 58 [24576/50048]	Loss: 0.1888
Training Epoch: 58 [24704/50048]	Loss: 0.1877
Training Epoch: 58 [24832/50048]	Loss: 0.3040
Training Epoch: 58 [24960/50048]	Loss: 0.1841
Training Epoch: 58 [25088/50048]	Loss: 0.1849
Training Epoch: 58 [25216/50048]	Loss: 0.2854
Training Epoch: 58 [25344/50048]	Loss: 0.1504
Training Epoch: 58 [25472/50048]	Loss: 0.1950
Training Epoch: 58 [25600/50048]	Loss: 0.2888
Training Epoch: 58 [25728/50048]	Loss: 0.2817
Training Epoch: 58 [25856/50048]	Loss: 0.2048
Training Epoch: 58 [25984/50048]	Loss: 0.2382
Training Epoch: 58 [26112/50048]	Loss: 0.1889
Training Epoch: 58 [26240/50048]	Loss: 0.2797
Training Epoch: 58 [26368/50048]	Loss: 0.2048
Training Epoch: 58 [26496/50048]	Loss: 0.1827
Training Epoch: 58 [26624/50048]	Loss: 0.1813
Training Epoch: 58 [26752/50048]	Loss: 0.2232
Training Epoch: 58 [26880/50048]	Loss: 0.3155
Training Epoch: 58 [27008/50048]	Loss: 0.3331
Training Epoch: 58 [27136/50048]	Loss: 0.2328
Training Epoch: 58 [27264/50048]	Loss: 0.3085
Training Epoch: 58 [27392/50048]	Loss: 0.2503
Training Epoch: 58 [27520/50048]	Loss: 0.1679
Training Epoch: 58 [27648/50048]	Loss: 0.1946
Training Epoch: 58 [27776/50048]	Loss: 0.2438
Training Epoch: 58 [27904/50048]	Loss: 0.3168
Training Epoch: 58 [28032/50048]	Loss: 0.2352
Training Epoch: 58 [28160/50048]	Loss: 0.3626
Training Epoch: 58 [28288/50048]	Loss: 0.2592
Training Epoch: 58 [28416/50048]	Loss: 0.2491
Training Epoch: 58 [28544/50048]	Loss: 0.3880
Training Epoch: 58 [28672/50048]	Loss: 0.2471
Training Epoch: 58 [28800/50048]	Loss: 0.2933
Training Epoch: 58 [28928/50048]	Loss: 0.2400
Training Epoch: 58 [29056/50048]	Loss: 0.2097
Training Epoch: 58 [29184/50048]	Loss: 0.3289
Training Epoch: 58 [29312/50048]	Loss: 0.1946
Training Epoch: 58 [29440/50048]	Loss: 0.2613
Training Epoch: 58 [29568/50048]	Loss: 0.1722
Training Epoch: 58 [29696/50048]	Loss: 0.2673
Training Epoch: 58 [29824/50048]	Loss: 0.2043
Training Epoch: 58 [29952/50048]	Loss: 0.2015
Training Epoch: 58 [30080/50048]	Loss: 0.1689
Training Epoch: 58 [30208/50048]	Loss: 0.1624
Training Epoch: 58 [30336/50048]	Loss: 0.1642
Training Epoch: 58 [30464/50048]	Loss: 0.2360
Training Epoch: 58 [30592/50048]	Loss: 0.3657
Training Epoch: 58 [30720/50048]	Loss: 0.2976
Training Epoch: 58 [30848/50048]	Loss: 0.2521
Training Epoch: 58 [30976/50048]	Loss: 0.1890
Training Epoch: 58 [31104/50048]	Loss: 0.2176
Training Epoch: 58 [31232/50048]	Loss: 0.4099
Training Epoch: 58 [31360/50048]	Loss: 0.2386
Training Epoch: 58 [31488/50048]	Loss: 0.1343
Training Epoch: 58 [31616/50048]	Loss: 0.3038
Training Epoch: 58 [31744/50048]	Loss: 0.1344
Training Epoch: 58 [31872/50048]	Loss: 0.1818
Training Epoch: 58 [32000/50048]	Loss: 0.1644
Training Epoch: 58 [32128/50048]	Loss: 0.3203
Training Epoch: 58 [32256/50048]	Loss: 0.2522
Training Epoch: 58 [32384/50048]	Loss: 0.2551
Training Epoch: 58 [32512/50048]	Loss: 0.1692
Training Epoch: 58 [32640/50048]	Loss: 0.1629
Training Epoch: 58 [32768/50048]	Loss: 0.2041
Training Epoch: 58 [32896/50048]	Loss: 0.1895
Training Epoch: 58 [33024/50048]	Loss: 0.2129
Training Epoch: 58 [33152/50048]	Loss: 0.2187
Training Epoch: 58 [33280/50048]	Loss: 0.1435
Training Epoch: 58 [33408/50048]	Loss: 0.3028
Training Epoch: 58 [33536/50048]	Loss: 0.1336
Training Epoch: 58 [33664/50048]	Loss: 0.2247
Training Epoch: 58 [33792/50048]	Loss: 0.1956
Training Epoch: 58 [33920/50048]	Loss: 0.1960
Training Epoch: 58 [34048/50048]	Loss: 0.1946
Training Epoch: 58 [34176/50048]	Loss: 0.1920
Training Epoch: 58 [34304/50048]	Loss: 0.3391
Training Epoch: 58 [34432/50048]	Loss: 0.1645
Training Epoch: 58 [34560/50048]	Loss: 0.2946
Training Epoch: 58 [34688/50048]	Loss: 0.1550
Training Epoch: 58 [34816/50048]	Loss: 0.2215
Training Epoch: 58 [34944/50048]	Loss: 0.1611
Training Epoch: 58 [35072/50048]	Loss: 0.2193
Training Epoch: 58 [35200/50048]	Loss: 0.2570
Training Epoch: 58 [35328/50048]	Loss: 0.2345
Training Epoch: 58 [35456/50048]	Loss: 0.1197
Training Epoch: 58 [35584/50048]	Loss: 0.1402
Training Epoch: 58 [35712/50048]	Loss: 0.2192
Training Epoch: 58 [35840/50048]	Loss: 0.3007
Training Epoch: 58 [35968/50048]	Loss: 0.2202
Training Epoch: 58 [36096/50048]	Loss: 0.1685
Training Epoch: 58 [36224/50048]	Loss: 0.2436
Training Epoch: 58 [36352/50048]	Loss: 0.1233
Training Epoch: 58 [36480/50048]	Loss: 0.0991
Training Epoch: 58 [36608/50048]	Loss: 0.2257
Training Epoch: 58 [36736/50048]	Loss: 0.2419
Training Epoch: 58 [36864/50048]	Loss: 0.1418
Training Epoch: 58 [36992/50048]	Loss: 0.2033
Training Epoch: 58 [37120/50048]	Loss: 0.1555
Training Epoch: 58 [37248/50048]	Loss: 0.2455
Training Epoch: 58 [37376/50048]	Loss: 0.2938
Training Epoch: 58 [37504/50048]	Loss: 0.2533
Training Epoch: 58 [37632/50048]	Loss: 0.1550
Training Epoch: 58 [37760/50048]	Loss: 0.1521
Training Epoch: 58 [37888/50048]	Loss: 0.2136
Training Epoch: 58 [38016/50048]	Loss: 0.1500
Training Epoch: 58 [38144/50048]	Loss: 0.2780
Training Epoch: 58 [38272/50048]	Loss: 0.3374
Training Epoch: 58 [38400/50048]	Loss: 0.1342
Training Epoch: 58 [38528/50048]	Loss: 0.2310
Training Epoch: 58 [38656/50048]	Loss: 0.1978
Training Epoch: 58 [38784/50048]	Loss: 0.2462
Training Epoch: 58 [38912/50048]	Loss: 0.3072
Training Epoch: 58 [39040/50048]	Loss: 0.2866
Training Epoch: 58 [39168/50048]	Loss: 0.2079
Training Epoch: 58 [39296/50048]	Loss: 0.2465
Training Epoch: 58 [39424/50048]	Loss: 0.1271
Training Epoch: 58 [39552/50048]	Loss: 0.2372
Training Epoch: 58 [39680/50048]	Loss: 0.1562
Training Epoch: 58 [39808/50048]	Loss: 0.1686
Training Epoch: 58 [39936/50048]	Loss: 0.3212
Training Epoch: 58 [40064/50048]	Loss: 0.2093
Training Epoch: 58 [40192/50048]	Loss: 0.1959
Training Epoch: 58 [40320/50048]	Loss: 0.2685
Training Epoch: 58 [40448/50048]	Loss: 0.3578
Training Epoch: 58 [40576/50048]	Loss: 0.1695
Training Epoch: 58 [40704/50048]	Loss: 0.1895
Training Epoch: 58 [40832/50048]	Loss: 0.1543
Training Epoch: 58 [40960/50048]	Loss: 0.2244
Training Epoch: 58 [41088/50048]	Loss: 0.1946
Training Epoch: 58 [41216/50048]	Loss: 0.1306
Training Epoch: 58 [41344/50048]	Loss: 0.2667
Training Epoch: 58 [41472/50048]	Loss: 0.2406
Training Epoch: 58 [41600/50048]	Loss: 0.2159
Training Epoch: 58 [41728/50048]	Loss: 0.3028
Training Epoch: 58 [41856/50048]	Loss: 0.2749
Training Epoch: 58 [41984/50048]	Loss: 0.2767
Training Epoch: 58 [42112/50048]	Loss: 0.2088
Training Epoch: 58 [42240/50048]	Loss: 0.2733
Training Epoch: 58 [42368/50048]	Loss: 0.2346
Training Epoch: 58 [42496/50048]	Loss: 0.3282
Training Epoch: 58 [42624/50048]	Loss: 0.2272
Training Epoch: 58 [42752/50048]	Loss: 0.2665
Training Epoch: 58 [42880/50048]	Loss: 0.2003
Training Epoch: 58 [43008/50048]	Loss: 0.2541
Training Epoch: 58 [43136/50048]	Loss: 0.1988
Training Epoch: 58 [43264/50048]	Loss: 0.1562
Training Epoch: 58 [43392/50048]	Loss: 0.1794
Training Epoch: 58 [43520/50048]	Loss: 0.2069
Training Epoch: 58 [43648/50048]	Loss: 0.2938
Training Epoch: 58 [43776/50048]	Loss: 0.1887
Training Epoch: 58 [43904/50048]	Loss: 0.1344
Training Epoch: 58 [44032/50048]	Loss: 0.1848
Training Epoch: 58 [44160/50048]	Loss: 0.1584
Training Epoch: 58 [44288/50048]	Loss: 0.2610
Training Epoch: 58 [44416/50048]	Loss: 0.1861
Training Epoch: 58 [44544/50048]	Loss: 0.2960
Training Epoch: 58 [44672/50048]	Loss: 0.2237
Training Epoch: 58 [44800/50048]	Loss: 0.2396
Training Epoch: 58 [44928/50048]	Loss: 0.3250
Training Epoch: 58 [45056/50048]	Loss: 0.2906
Training Epoch: 58 [45184/50048]	Loss: 0.3964
Training Epoch: 58 [45312/50048]	Loss: 0.2566
Training Epoch: 58 [45440/50048]	Loss: 0.2975
Training Epoch: 58 [45568/50048]	Loss: 0.2426
Training Epoch: 58 [45696/50048]	Loss: 0.2450
2022-12-06 07:36:33,009 [ZeusDataLoader(train)] train epoch 59 done: time=86.55 energy=10507.99
2022-12-06 07:36:33,010 [ZeusDataLoader(eval)] Epoch 59 begin.
Training Epoch: 58 [45824/50048]	Loss: 0.2505
Training Epoch: 58 [45952/50048]	Loss: 0.1658
Training Epoch: 58 [46080/50048]	Loss: 0.2099
Training Epoch: 58 [46208/50048]	Loss: 0.1682
Training Epoch: 58 [46336/50048]	Loss: 0.1451
Training Epoch: 58 [46464/50048]	Loss: 0.1739
Training Epoch: 58 [46592/50048]	Loss: 0.2536
Training Epoch: 58 [46720/50048]	Loss: 0.1506
Training Epoch: 58 [46848/50048]	Loss: 0.2267
Training Epoch: 58 [46976/50048]	Loss: 0.1977
Training Epoch: 58 [47104/50048]	Loss: 0.1527
Training Epoch: 58 [47232/50048]	Loss: 0.2065
Training Epoch: 58 [47360/50048]	Loss: 0.2055
Training Epoch: 58 [47488/50048]	Loss: 0.2107
Training Epoch: 58 [47616/50048]	Loss: 0.1872
Training Epoch: 58 [47744/50048]	Loss: 0.2443
Training Epoch: 58 [47872/50048]	Loss: 0.2781
Training Epoch: 58 [48000/50048]	Loss: 0.2317
Training Epoch: 58 [48128/50048]	Loss: 0.2208
Training Epoch: 58 [48256/50048]	Loss: 0.2291
Training Epoch: 58 [48384/50048]	Loss: 0.2675
Training Epoch: 58 [48512/50048]	Loss: 0.2497
Training Epoch: 58 [48640/50048]	Loss: 0.3110
Training Epoch: 58 [48768/50048]	Loss: 0.4796
Training Epoch: 58 [48896/50048]	Loss: 0.2266
Training Epoch: 58 [49024/50048]	Loss: 0.2576
Training Epoch: 58 [49152/50048]	Loss: 0.2454
Training Epoch: 58 [49280/50048]	Loss: 0.2144
Training Epoch: 58 [49408/50048]	Loss: 0.4775
Training Epoch: 58 [49536/50048]	Loss: 0.1410
Training Epoch: 58 [49664/50048]	Loss: 0.2350
Training Epoch: 58 [49792/50048]	Loss: 0.1654
Training Epoch: 58 [49920/50048]	Loss: 0.2685
Training Epoch: 58 [50048/50048]	Loss: 0.2646
2022-12-06 12:36:36.650 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:36:36,663 [ZeusDataLoader(eval)] eval epoch 59 done: time=3.64 energy=441.35
2022-12-06 07:36:36,663 [ZeusDataLoader(train)] Up to epoch 59: time=5321.75, energy=646006.66, cost=788656.15
2022-12-06 07:36:36,663 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:36:36,663 [ZeusDataLoader(train)] Expected next epoch: time=5411.55, energy=656804.68, cost=801912.54
2022-12-06 07:36:36,664 [ZeusDataLoader(train)] Epoch 60 begin.
Validation Epoch: 58, Average loss: 0.0163, Accuracy: 0.6273
2022-12-06 07:36:36,854 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:36:36,855 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:36:36.857 [ZeusMonitor] Monitor started.
2022-12-06 12:36:36.857 [ZeusMonitor] Running indefinitely. 2022-12-06 12:36:36.857 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:36:36.857 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e60+gpu0.power.log
Training Epoch: 59 [128/50048]	Loss: 0.1383
Training Epoch: 59 [256/50048]	Loss: 0.2322
Training Epoch: 59 [384/50048]	Loss: 0.1535
Training Epoch: 59 [512/50048]	Loss: 0.1744
Training Epoch: 59 [640/50048]	Loss: 0.1500
Training Epoch: 59 [768/50048]	Loss: 0.1890
Training Epoch: 59 [896/50048]	Loss: 0.2182
Training Epoch: 59 [1024/50048]	Loss: 0.1412
Training Epoch: 59 [1152/50048]	Loss: 0.2353
Training Epoch: 59 [1280/50048]	Loss: 0.2300
Training Epoch: 59 [1408/50048]	Loss: 0.0996
Training Epoch: 59 [1536/50048]	Loss: 0.1883
Training Epoch: 59 [1664/50048]	Loss: 0.1870
Training Epoch: 59 [1792/50048]	Loss: 0.1955
Training Epoch: 59 [1920/50048]	Loss: 0.1864
Training Epoch: 59 [2048/50048]	Loss: 0.0997
Training Epoch: 59 [2176/50048]	Loss: 0.2103
Training Epoch: 59 [2304/50048]	Loss: 0.2359
Training Epoch: 59 [2432/50048]	Loss: 0.3376
Training Epoch: 59 [2560/50048]	Loss: 0.1781
Training Epoch: 59 [2688/50048]	Loss: 0.1235
Training Epoch: 59 [2816/50048]	Loss: 0.2449
Training Epoch: 59 [2944/50048]	Loss: 0.2406
Training Epoch: 59 [3072/50048]	Loss: 0.2151
Training Epoch: 59 [3200/50048]	Loss: 0.1432
Training Epoch: 59 [3328/50048]	Loss: 0.1613
Training Epoch: 59 [3456/50048]	Loss: 0.1864
Training Epoch: 59 [3584/50048]	Loss: 0.2138
Training Epoch: 59 [3712/50048]	Loss: 0.2218
Training Epoch: 59 [3840/50048]	Loss: 0.1688
Training Epoch: 59 [3968/50048]	Loss: 0.1775
Training Epoch: 59 [4096/50048]	Loss: 0.1437
Training Epoch: 59 [4224/50048]	Loss: 0.1492
Training Epoch: 59 [4352/50048]	Loss: 0.3223
Training Epoch: 59 [4480/50048]	Loss: 0.2161
Training Epoch: 59 [4608/50048]	Loss: 0.1225
Training Epoch: 59 [4736/50048]	Loss: 0.2905
Training Epoch: 59 [4864/50048]	Loss: 0.1334
Training Epoch: 59 [4992/50048]	Loss: 0.2229
Training Epoch: 59 [5120/50048]	Loss: 0.1803
Training Epoch: 59 [5248/50048]	Loss: 0.2496
Training Epoch: 59 [5376/50048]	Loss: 0.2265
Training Epoch: 59 [5504/50048]	Loss: 0.2285
Training Epoch: 59 [5632/50048]	Loss: 0.2919
Training Epoch: 59 [5760/50048]	Loss: 0.2582
Training Epoch: 59 [5888/50048]	Loss: 0.2272
Training Epoch: 59 [6016/50048]	Loss: 0.1641
Training Epoch: 59 [6144/50048]	Loss: 0.1185
Training Epoch: 59 [6272/50048]	Loss: 0.2449
Training Epoch: 59 [6400/50048]	Loss: 0.1540
Training Epoch: 59 [6528/50048]	Loss: 0.2617
Training Epoch: 59 [6656/50048]	Loss: 0.1842
Training Epoch: 59 [6784/50048]	Loss: 0.2286
Training Epoch: 59 [6912/50048]	Loss: 0.1246
Training Epoch: 59 [7040/50048]	Loss: 0.1470
Training Epoch: 59 [7168/50048]	Loss: 0.2036
Training Epoch: 59 [7296/50048]	Loss: 0.1640
Training Epoch: 59 [7424/50048]	Loss: 0.1988
Training Epoch: 59 [7552/50048]	Loss: 0.2584
Training Epoch: 59 [7680/50048]	Loss: 0.1020
Training Epoch: 59 [7808/50048]	Loss: 0.2000
Training Epoch: 59 [7936/50048]	Loss: 0.1713
Training Epoch: 59 [8064/50048]	Loss: 0.1858
Training Epoch: 59 [8192/50048]	Loss: 0.2040
Training Epoch: 59 [8320/50048]	Loss: 0.1674
Training Epoch: 59 [8448/50048]	Loss: 0.2228
Training Epoch: 59 [8576/50048]	Loss: 0.2766
Training Epoch: 59 [8704/50048]	Loss: 0.1130
Training Epoch: 59 [8832/50048]	Loss: 0.1189
Training Epoch: 59 [8960/50048]	Loss: 0.1351
Training Epoch: 59 [9088/50048]	Loss: 0.2320
Training Epoch: 59 [9216/50048]	Loss: 0.2075
Training Epoch: 59 [9344/50048]	Loss: 0.1692
Training Epoch: 59 [9472/50048]	Loss: 0.2118
Training Epoch: 59 [9600/50048]	Loss: 0.1016
Training Epoch: 59 [9728/50048]	Loss: 0.2236
Training Epoch: 59 [9856/50048]	Loss: 0.1089
Training Epoch: 59 [9984/50048]	Loss: 0.1839
Training Epoch: 59 [10112/50048]	Loss: 0.2516
Training Epoch: 59 [10240/50048]	Loss: 0.2761
Training Epoch: 59 [10368/50048]	Loss: 0.2447
Training Epoch: 59 [10496/50048]	Loss: 0.1493
Training Epoch: 59 [10624/50048]	Loss: 0.1809
Training Epoch: 59 [10752/50048]	Loss: 0.1883
Training Epoch: 59 [10880/50048]	Loss: 0.2316
Training Epoch: 59 [11008/50048]	Loss: 0.1653
Training Epoch: 59 [11136/50048]	Loss: 0.2142
Training Epoch: 59 [11264/50048]	Loss: 0.1984
Training Epoch: 59 [11392/50048]	Loss: 0.1863
Training Epoch: 59 [11520/50048]	Loss: 0.2568
Training Epoch: 59 [11648/50048]	Loss: 0.2339
Training Epoch: 59 [11776/50048]	Loss: 0.1658
Training Epoch: 59 [11904/50048]	Loss: 0.2203
Training Epoch: 59 [12032/50048]	Loss: 0.2184
Training Epoch: 59 [12160/50048]	Loss: 0.1377
Training Epoch: 59 [12288/50048]	Loss: 0.1305
Training Epoch: 59 [12416/50048]	Loss: 0.1717
Training Epoch: 59 [12544/50048]	Loss: 0.1746
Training Epoch: 59 [12672/50048]	Loss: 0.1235
Training Epoch: 59 [12800/50048]	Loss: 0.2291
Training Epoch: 59 [12928/50048]	Loss: 0.2762
Training Epoch: 59 [13056/50048]	Loss: 0.2193
Training Epoch: 59 [13184/50048]	Loss: 0.1753
Training Epoch: 59 [13312/50048]	Loss: 0.1630
Training Epoch: 59 [13440/50048]	Loss: 0.1671
Training Epoch: 59 [13568/50048]	Loss: 0.0798
Training Epoch: 59 [13696/50048]	Loss: 0.1424
Training Epoch: 59 [13824/50048]	Loss: 0.1480
Training Epoch: 59 [13952/50048]	Loss: 0.3332
Training Epoch: 59 [14080/50048]	Loss: 0.1691
Training Epoch: 59 [14208/50048]	Loss: 0.1406
Training Epoch: 59 [14336/50048]	Loss: 0.1670
Training Epoch: 59 [14464/50048]	Loss: 0.3037
Training Epoch: 59 [14592/50048]	Loss: 0.1387
Training Epoch: 59 [14720/50048]	Loss: 0.1643
Training Epoch: 59 [14848/50048]	Loss: 0.0988
Training Epoch: 59 [14976/50048]	Loss: 0.2049
Training Epoch: 59 [15104/50048]	Loss: 0.1407
Training Epoch: 59 [15232/50048]	Loss: 0.2011
Training Epoch: 59 [15360/50048]	Loss: 0.1381
Training Epoch: 59 [15488/50048]	Loss: 0.1378
Training Epoch: 59 [15616/50048]	Loss: 0.1453
Training Epoch: 59 [15744/50048]	Loss: 0.1550
Training Epoch: 59 [15872/50048]	Loss: 0.1944
Training Epoch: 59 [16000/50048]	Loss: 0.1264
Training Epoch: 59 [16128/50048]	Loss: 0.2535
Training Epoch: 59 [16256/50048]	Loss: 0.1827
Training Epoch: 59 [16384/50048]	Loss: 0.1705
Training Epoch: 59 [16512/50048]	Loss: 0.2570
Training Epoch: 59 [16640/50048]	Loss: 0.2324
Training Epoch: 59 [16768/50048]	Loss: 0.1951
Training Epoch: 59 [16896/50048]	Loss: 0.1807
Training Epoch: 59 [17024/50048]	Loss: 0.1891
Training Epoch: 59 [17152/50048]	Loss: 0.2585
Training Epoch: 59 [17280/50048]	Loss: 0.1436
Training Epoch: 59 [17408/50048]	Loss: 0.1987
Training Epoch: 59 [17536/50048]	Loss: 0.0814
Training Epoch: 59 [17664/50048]	Loss: 0.2208
Training Epoch: 59 [17792/50048]	Loss: 0.1241
Training Epoch: 59 [17920/50048]	Loss: 0.1875
Training Epoch: 59 [18048/50048]	Loss: 0.1953
Training Epoch: 59 [18176/50048]	Loss: 0.1802
Training Epoch: 59 [18304/50048]	Loss: 0.2081
Training Epoch: 59 [18432/50048]	Loss: 0.2395
Training Epoch: 59 [18560/50048]	Loss: 0.1911
Training Epoch: 59 [18688/50048]	Loss: 0.2112
Training Epoch: 59 [18816/50048]	Loss: 0.1300
Training Epoch: 59 [18944/50048]	Loss: 0.2132
Training Epoch: 59 [19072/50048]	Loss: 0.2075
Training Epoch: 59 [19200/50048]	Loss: 0.1636
Training Epoch: 59 [19328/50048]	Loss: 0.2015
Training Epoch: 59 [19456/50048]	Loss: 0.2061
Training Epoch: 59 [19584/50048]	Loss: 0.3834
Training Epoch: 59 [19712/50048]	Loss: 0.2181
Training Epoch: 59 [19840/50048]	Loss: 0.1823
Training Epoch: 59 [19968/50048]	Loss: 0.1829
Training Epoch: 59 [20096/50048]	Loss: 0.1501
Training Epoch: 59 [20224/50048]	Loss: 0.2818
Training Epoch: 59 [20352/50048]	Loss: 0.2594
Training Epoch: 59 [20480/50048]	Loss: 0.1797
Training Epoch: 59 [20608/50048]	Loss: 0.1811
Training Epoch: 59 [20736/50048]	Loss: 0.1357
Training Epoch: 59 [20864/50048]	Loss: 0.3030
Training Epoch: 59 [20992/50048]	Loss: 0.2141
Training Epoch: 59 [21120/50048]	Loss: 0.2871
Training Epoch: 59 [21248/50048]	Loss: 0.1231
Training Epoch: 59 [21376/50048]	Loss: 0.1937
Training Epoch: 59 [21504/50048]	Loss: 0.1885
Training Epoch: 59 [21632/50048]	Loss: 0.1657
Training Epoch: 59 [21760/50048]	Loss: 0.1812
Training Epoch: 59 [21888/50048]	Loss: 0.1555
Training Epoch: 59 [22016/50048]	Loss: 0.2618
Training Epoch: 59 [22144/50048]	Loss: 0.0812
Training Epoch: 59 [22272/50048]	Loss: 0.1915
Training Epoch: 59 [22400/50048]	Loss: 0.1520
Training Epoch: 59 [22528/50048]	Loss: 0.2341
Training Epoch: 59 [22656/50048]	Loss: 0.2155
Training Epoch: 59 [22784/50048]	Loss: 0.1603
Training Epoch: 59 [22912/50048]	Loss: 0.2283
Training Epoch: 59 [23040/50048]	Loss: 0.2048
Training Epoch: 59 [23168/50048]	Loss: 0.2535
Training Epoch: 59 [23296/50048]	Loss: 0.1538
Training Epoch: 59 [23424/50048]	Loss: 0.2111
Training Epoch: 59 [23552/50048]	Loss: 0.2409
Training Epoch: 59 [23680/50048]	Loss: 0.2072
Training Epoch: 59 [23808/50048]	Loss: 0.2305
Training Epoch: 59 [23936/50048]	Loss: 0.2936
Training Epoch: 59 [24064/50048]	Loss: 0.2192
Training Epoch: 59 [24192/50048]	Loss: 0.2292
Training Epoch: 59 [24320/50048]	Loss: 0.2338
Training Epoch: 59 [24448/50048]	Loss: 0.1416
Training Epoch: 59 [24576/50048]	Loss: 0.2981
Training Epoch: 59 [24704/50048]	Loss: 0.3553
Training Epoch: 59 [24832/50048]	Loss: 0.2953
Training Epoch: 59 [24960/50048]	Loss: 0.2246
Training Epoch: 59 [25088/50048]	Loss: 0.2804
Training Epoch: 59 [25216/50048]	Loss: 0.2685
Training Epoch: 59 [25344/50048]	Loss: 0.1393
Training Epoch: 59 [25472/50048]	Loss: 0.2297
Training Epoch: 59 [25600/50048]	Loss: 0.1202
Training Epoch: 59 [25728/50048]	Loss: 0.2443
Training Epoch: 59 [25856/50048]	Loss: 0.2357
Training Epoch: 59 [25984/50048]	Loss: 0.2508
Training Epoch: 59 [26112/50048]	Loss: 0.2251
Training Epoch: 59 [26240/50048]	Loss: 0.2094
Training Epoch: 59 [26368/50048]	Loss: 0.1446
Training Epoch: 59 [26496/50048]	Loss: 0.1566
Training Epoch: 59 [26624/50048]	Loss: 0.2305
Training Epoch: 59 [26752/50048]	Loss: 0.1657
Training Epoch: 59 [26880/50048]	Loss: 0.2523
Training Epoch: 59 [27008/50048]	Loss: 0.1433
Training Epoch: 59 [27136/50048]	Loss: 0.2092
Training Epoch: 59 [27264/50048]	Loss: 0.1603
Training Epoch: 59 [27392/50048]	Loss: 0.3861
Training Epoch: 59 [27520/50048]	Loss: 0.2558
Training Epoch: 59 [27648/50048]	Loss: 0.3031
Training Epoch: 59 [27776/50048]	Loss: 0.2146
Training Epoch: 59 [27904/50048]	Loss: 0.1489
Training Epoch: 59 [28032/50048]	Loss: 0.2290
Training Epoch: 59 [28160/50048]	Loss: 0.2032
Training Epoch: 59 [28288/50048]	Loss: 0.1132
Training Epoch: 59 [28416/50048]	Loss: 0.2235
Training Epoch: 59 [28544/50048]	Loss: 0.1815
Training Epoch: 59 [28672/50048]	Loss: 0.2711
Training Epoch: 59 [28800/50048]	Loss: 0.2357
Training Epoch: 59 [28928/50048]	Loss: 0.2291
Training Epoch: 59 [29056/50048]	Loss: 0.1398
Training Epoch: 59 [29184/50048]	Loss: 0.2100
Training Epoch: 59 [29312/50048]	Loss: 0.1702
Training Epoch: 59 [29440/50048]	Loss: 0.1216
Training Epoch: 59 [29568/50048]	Loss: 0.1728
Training Epoch: 59 [29696/50048]	Loss: 0.1548
Training Epoch: 59 [29824/50048]	Loss: 0.2341
Training Epoch: 59 [29952/50048]	Loss: 0.2337
Training Epoch: 59 [30080/50048]	Loss: 0.1978
Training Epoch: 59 [30208/50048]	Loss: 0.3409
Training Epoch: 59 [30336/50048]	Loss: 0.2526
Training Epoch: 59 [30464/50048]	Loss: 0.1565
Training Epoch: 59 [30592/50048]	Loss: 0.1862
Training Epoch: 59 [30720/50048]	Loss: 0.2779
Training Epoch: 59 [30848/50048]	Loss: 0.2331
Training Epoch: 59 [30976/50048]	Loss: 0.2867
Training Epoch: 59 [31104/50048]	Loss: 0.2142
Training Epoch: 59 [31232/50048]	Loss: 0.1611
Training Epoch: 59 [31360/50048]	Loss: 0.1659
Training Epoch: 59 [31488/50048]	Loss: 0.1130
Training Epoch: 59 [31616/50048]	Loss: 0.1794
Training Epoch: 59 [31744/50048]	Loss: 0.2208
Training Epoch: 59 [31872/50048]	Loss: 0.1892
Training Epoch: 59 [32000/50048]	Loss: 0.1684
Training Epoch: 59 [32128/50048]	Loss: 0.2092
Training Epoch: 59 [32256/50048]	Loss: 0.2072
Training Epoch: 59 [32384/50048]	Loss: 0.1818
Training Epoch: 59 [32512/50048]	Loss: 0.1869
Training Epoch: 59 [32640/50048]	Loss: 0.2043
Training Epoch: 59 [32768/50048]	Loss: 0.1467
Training Epoch: 59 [32896/50048]	Loss: 0.2114
Training Epoch: 59 [33024/50048]	Loss: 0.2058
Training Epoch: 59 [33152/50048]	Loss: 0.1800
Training Epoch: 59 [33280/50048]	Loss: 0.1961
Training Epoch: 59 [33408/50048]	Loss: 0.2431
Training Epoch: 59 [33536/50048]	Loss: 0.3511
Training Epoch: 59 [33664/50048]	Loss: 0.1778
Training Epoch: 59 [33792/50048]	Loss: 0.1148
Training Epoch: 59 [33920/50048]	Loss: 0.1368
Training Epoch: 59 [34048/50048]	Loss: 0.2061
Training Epoch: 59 [34176/50048]	Loss: 0.2417
Training Epoch: 59 [34304/50048]	Loss: 0.1770
Training Epoch: 59 [34432/50048]	Loss: 0.2298
Training Epoch: 59 [34560/50048]	Loss: 0.2104
Training Epoch: 59 [34688/50048]	Loss: 0.2063
Training Epoch: 59 [34816/50048]	Loss: 0.2166
Training Epoch: 59 [34944/50048]	Loss: 0.1399
Training Epoch: 59 [35072/50048]	Loss: 0.2106
Training Epoch: 59 [35200/50048]	Loss: 0.2000
Training Epoch: 59 [35328/50048]	Loss: 0.2291
Training Epoch: 59 [35456/50048]	Loss: 0.2242
Training Epoch: 59 [35584/50048]	Loss: 0.2649
Training Epoch: 59 [35712/50048]	Loss: 0.2278
Training Epoch: 59 [35840/50048]	Loss: 0.2271
Training Epoch: 59 [35968/50048]	Loss: 0.2614
Training Epoch: 59 [36096/50048]	Loss: 0.1796
Training Epoch: 59 [36224/50048]	Loss: 0.2154
Training Epoch: 59 [36352/50048]	Loss: 0.0946
Training Epoch: 59 [36480/50048]	Loss: 0.2259
Training Epoch: 59 [36608/50048]	Loss: 0.1985
Training Epoch: 59 [36736/50048]	Loss: 0.2095
Training Epoch: 59 [36864/50048]	Loss: 0.3030
Training Epoch: 59 [36992/50048]	Loss: 0.1711
Training Epoch: 59 [37120/50048]	Loss: 0.1449
Training Epoch: 59 [37248/50048]	Loss: 0.2198
Training Epoch: 59 [37376/50048]	Loss: 0.3552
Training Epoch: 59 [37504/50048]	Loss: 0.2218
Training Epoch: 59 [37632/50048]	Loss: 0.1685
Training Epoch: 59 [37760/50048]	Loss: 0.1495
Training Epoch: 59 [37888/50048]	Loss: 0.3159
Training Epoch: 59 [38016/50048]	Loss: 0.2019
Training Epoch: 59 [38144/50048]	Loss: 0.1753
Training Epoch: 59 [38272/50048]	Loss: 0.2232
Training Epoch: 59 [38400/50048]	Loss: 0.2363
Training Epoch: 59 [38528/50048]	Loss: 0.1881
Training Epoch: 59 [38656/50048]	Loss: 0.1807
Training Epoch: 59 [38784/50048]	Loss: 0.2433
Training Epoch: 59 [38912/50048]	Loss: 0.2172
Training Epoch: 59 [39040/50048]	Loss: 0.2223
Training Epoch: 59 [39168/50048]	Loss: 0.1867
Training Epoch: 59 [39296/50048]	Loss: 0.2192
Training Epoch: 59 [39424/50048]	Loss: 0.1830
Training Epoch: 59 [39552/50048]	Loss: 0.2391
Training Epoch: 59 [39680/50048]	Loss: 0.4033
Training Epoch: 59 [39808/50048]	Loss: 0.2076
Training Epoch: 59 [39936/50048]	Loss: 0.1778
Training Epoch: 59 [40064/50048]	Loss: 0.2323
Training Epoch: 59 [40192/50048]	Loss: 0.1625
Training Epoch: 59 [40320/50048]	Loss: 0.2669
Training Epoch: 59 [40448/50048]	Loss: 0.1806
Training Epoch: 59 [40576/50048]	Loss: 0.1166
Training Epoch: 59 [40704/50048]	Loss: 0.1655
Training Epoch: 59 [40832/50048]	Loss: 0.2430
Training Epoch: 59 [40960/50048]	Loss: 0.2515
Training Epoch: 59 [41088/50048]	Loss: 0.2223
Training Epoch: 59 [41216/50048]	Loss: 0.1357
Training Epoch: 59 [41344/50048]	Loss: 0.2000
Training Epoch: 59 [41472/50048]	Loss: 0.1998
Training Epoch: 59 [41600/50048]	Loss: 0.2060
Training Epoch: 59 [41728/50048]	Loss: 0.2537
Training Epoch: 59 [41856/50048]	Loss: 0.2178
Training Epoch: 59 [41984/50048]	Loss: 0.2525
Training Epoch: 59 [42112/50048]	Loss: 0.2793
Training Epoch: 59 [42240/50048]	Loss: 0.2083
Training Epoch: 59 [42368/50048]	Loss: 0.2635
Training Epoch: 59 [42496/50048]	Loss: 0.1522
Training Epoch: 59 [42624/50048]	Loss: 0.2346
Training Epoch: 59 [42752/50048]	Loss: 0.2066
Training Epoch: 59 [42880/50048]	Loss: 0.3037
Training Epoch: 59 [43008/50048]	Loss: 0.1460
Training Epoch: 59 [43136/50048]	Loss: 0.1306
Training Epoch: 59 [43264/50048]	Loss: 0.1606
Training Epoch: 59 [43392/50048]	Loss: 0.2103
Training Epoch: 59 [43520/50048]	Loss: 0.1312
Training Epoch: 59 [43648/50048]	Loss: 0.3104
Training Epoch: 59 [43776/50048]	Loss: 0.2027
Training Epoch: 59 [43904/50048]	Loss: 0.2349
Training Epoch: 59 [44032/50048]	Loss: 0.3154
Training Epoch: 59 [44160/50048]	Loss: 0.1370
Training Epoch: 59 [44288/50048]	Loss: 0.2125
Training Epoch: 59 [44416/50048]	Loss: 0.2135
Training Epoch: 59 [44544/50048]	Loss: 0.1615
Training Epoch: 59 [44672/50048]	Loss: 0.1790
Training Epoch: 59 [44800/50048]	Loss: 0.2664
Training Epoch: 59 [44928/50048]	Loss: 0.1935
Training Epoch: 59 [45056/50048]	Loss: 0.1596
Training Epoch: 59 [45184/50048]	Loss: 0.2060
Training Epoch: 59 [45312/50048]	Loss: 0.2168
Training Epoch: 59 [45440/50048]	Loss: 0.1785
Training Epoch: 59 [45568/50048]	Loss: 0.2553
Training Epoch: 59 [45696/50048]	Loss: 0.2674
2022-12-06 07:38:03,271 [ZeusDataLoader(train)] train epoch 60 done: time=86.60 energy=10508.15
2022-12-06 07:38:03,272 [ZeusDataLoader(eval)] Epoch 60 begin.
Training Epoch: 59 [45824/50048]	Loss: 0.2649
Training Epoch: 59 [45952/50048]	Loss: 0.1615
Training Epoch: 59 [46080/50048]	Loss: 0.2635
Training Epoch: 59 [46208/50048]	Loss: 0.1896
Training Epoch: 59 [46336/50048]	Loss: 0.2163
Training Epoch: 59 [46464/50048]	Loss: 0.3708
Training Epoch: 59 [46592/50048]	Loss: 0.2466
Training Epoch: 59 [46720/50048]	Loss: 0.2620
Training Epoch: 59 [46848/50048]	Loss: 0.2153
Training Epoch: 59 [46976/50048]	Loss: 0.3040
Training Epoch: 59 [47104/50048]	Loss: 0.2479
Training Epoch: 59 [47232/50048]	Loss: 0.2051
Training Epoch: 59 [47360/50048]	Loss: 0.2595
Training Epoch: 59 [47488/50048]	Loss: 0.1837
Training Epoch: 59 [47616/50048]	Loss: 0.2520
Training Epoch: 59 [47744/50048]	Loss: 0.2169
Training Epoch: 59 [47872/50048]	Loss: 0.1816
Training Epoch: 59 [48000/50048]	Loss: 0.3141
Training Epoch: 59 [48128/50048]	Loss: 0.3014
Training Epoch: 59 [48256/50048]	Loss: 0.1695
Training Epoch: 59 [48384/50048]	Loss: 0.2410
Training Epoch: 59 [48512/50048]	Loss: 0.2144
Training Epoch: 59 [48640/50048]	Loss: 0.1850
Training Epoch: 59 [48768/50048]	Loss: 0.1777
Training Epoch: 59 [48896/50048]	Loss: 0.2763
Training Epoch: 59 [49024/50048]	Loss: 0.1767
Training Epoch: 59 [49152/50048]	Loss: 0.1835
Training Epoch: 59 [49280/50048]	Loss: 0.2218
Training Epoch: 59 [49408/50048]	Loss: 0.2076
Training Epoch: 59 [49536/50048]	Loss: 0.2508
Training Epoch: 59 [49664/50048]	Loss: 0.3494
Training Epoch: 59 [49792/50048]	Loss: 0.2525
Training Epoch: 59 [49920/50048]	Loss: 0.2356
Training Epoch: 59 [50048/50048]	Loss: 0.3377
2022-12-06 12:38:06.927 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:38:06,942 [ZeusDataLoader(eval)] eval epoch 60 done: time=3.66 energy=440.09
2022-12-06 07:38:06,942 [ZeusDataLoader(train)] Up to epoch 60: time=5412.00, energy=656954.90, cost=802027.79
2022-12-06 07:38:06,942 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:38:06,943 [ZeusDataLoader(train)] Expected next epoch: time=5501.80, energy=667752.91, cost=815284.18
2022-12-06 07:38:06,944 [ZeusDataLoader(train)] Epoch 61 begin.
Validation Epoch: 59, Average loss: 0.0159, Accuracy: 0.6342
2022-12-06 07:38:07,130 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:38:07,131 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:38:07.145 [ZeusMonitor] Monitor started.
2022-12-06 12:38:07.145 [ZeusMonitor] Running indefinitely. 2022-12-06 12:38:07.145 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:38:07.145 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e61+gpu0.power.log
Training Epoch: 60 [128/50048]	Loss: 0.2046
Training Epoch: 60 [256/50048]	Loss: 0.2314
Training Epoch: 60 [384/50048]	Loss: 0.1688
Training Epoch: 60 [512/50048]	Loss: 0.1944
Training Epoch: 60 [640/50048]	Loss: 0.1210
Training Epoch: 60 [768/50048]	Loss: 0.2745
Training Epoch: 60 [896/50048]	Loss: 0.0810
Training Epoch: 60 [1024/50048]	Loss: 0.2009
Training Epoch: 60 [1152/50048]	Loss: 0.0881
Training Epoch: 60 [1280/50048]	Loss: 0.2036
Training Epoch: 60 [1408/50048]	Loss: 0.1626
Training Epoch: 60 [1536/50048]	Loss: 0.1160
Training Epoch: 60 [1664/50048]	Loss: 0.1019
Training Epoch: 60 [1792/50048]	Loss: 0.1769
Training Epoch: 60 [1920/50048]	Loss: 0.1603
Training Epoch: 60 [2048/50048]	Loss: 0.1394
Training Epoch: 60 [2176/50048]	Loss: 0.1844
Training Epoch: 60 [2304/50048]	Loss: 0.1778
Training Epoch: 60 [2432/50048]	Loss: 0.1215
Training Epoch: 60 [2560/50048]	Loss: 0.1932
Training Epoch: 60 [2688/50048]	Loss: 0.2251
Training Epoch: 60 [2816/50048]	Loss: 0.1798
Training Epoch: 60 [2944/50048]	Loss: 0.1688
Training Epoch: 60 [3072/50048]	Loss: 0.1556
Training Epoch: 60 [3200/50048]	Loss: 0.2115
Training Epoch: 60 [3328/50048]	Loss: 0.1219
Training Epoch: 60 [3456/50048]	Loss: 0.1483
Training Epoch: 60 [3584/50048]	Loss: 0.1368
Training Epoch: 60 [3712/50048]	Loss: 0.1167
Training Epoch: 60 [3840/50048]	Loss: 0.1180
Training Epoch: 60 [3968/50048]	Loss: 0.1370
Training Epoch: 60 [4096/50048]	Loss: 0.1900
Training Epoch: 60 [4224/50048]	Loss: 0.1935
Training Epoch: 60 [4352/50048]	Loss: 0.1476
Training Epoch: 60 [4480/50048]	Loss: 0.1339
Training Epoch: 60 [4608/50048]	Loss: 0.2171
Training Epoch: 60 [4736/50048]	Loss: 0.2144
Training Epoch: 60 [4864/50048]	Loss: 0.1732
Training Epoch: 60 [4992/50048]	Loss: 0.2070
Training Epoch: 60 [5120/50048]	Loss: 0.1061
Training Epoch: 60 [5248/50048]	Loss: 0.1631
Training Epoch: 60 [5376/50048]	Loss: 0.2455
Training Epoch: 60 [5504/50048]	Loss: 0.1338
Training Epoch: 60 [5632/50048]	Loss: 0.1631
Training Epoch: 60 [5760/50048]	Loss: 0.1689
Training Epoch: 60 [5888/50048]	Loss: 0.1590
Training Epoch: 60 [6016/50048]	Loss: 0.2365
Training Epoch: 60 [6144/50048]	Loss: 0.1803
Training Epoch: 60 [6272/50048]	Loss: 0.2163
Training Epoch: 60 [6400/50048]	Loss: 0.1339
Training Epoch: 60 [6528/50048]	Loss: 0.0985
Training Epoch: 60 [6656/50048]	Loss: 0.1260
Training Epoch: 60 [6784/50048]	Loss: 0.1394
Training Epoch: 60 [6912/50048]	Loss: 0.2741
Training Epoch: 60 [7040/50048]	Loss: 0.3548
Training Epoch: 60 [7168/50048]	Loss: 0.1523
Training Epoch: 60 [7296/50048]	Loss: 0.1605
Training Epoch: 60 [7424/50048]	Loss: 0.2108
Training Epoch: 60 [7552/50048]	Loss: 0.2610
Training Epoch: 60 [7680/50048]	Loss: 0.1328
Training Epoch: 60 [7808/50048]	Loss: 0.1635
Training Epoch: 60 [7936/50048]	Loss: 0.1407
Training Epoch: 60 [8064/50048]	Loss: 0.2154
Training Epoch: 60 [8192/50048]	Loss: 0.1285
Training Epoch: 60 [8320/50048]	Loss: 0.0634
Training Epoch: 60 [8448/50048]	Loss: 0.1698
Training Epoch: 60 [8576/50048]	Loss: 0.2275
Training Epoch: 60 [8704/50048]	Loss: 0.2358
Training Epoch: 60 [8832/50048]	Loss: 0.1171
Training Epoch: 60 [8960/50048]	Loss: 0.2024
Training Epoch: 60 [9088/50048]	Loss: 0.1376
Training Epoch: 60 [9216/50048]	Loss: 0.1704
Training Epoch: 60 [9344/50048]	Loss: 0.1396
Training Epoch: 60 [9472/50048]	Loss: 0.1411
Training Epoch: 60 [9600/50048]	Loss: 0.1596
Training Epoch: 60 [9728/50048]	Loss: 0.1840
Training Epoch: 60 [9856/50048]	Loss: 0.1734
Training Epoch: 60 [9984/50048]	Loss: 0.1250
Training Epoch: 60 [10112/50048]	Loss: 0.1521
Training Epoch: 60 [10240/50048]	Loss: 0.2189
Training Epoch: 60 [10368/50048]	Loss: 0.2199
Training Epoch: 60 [10496/50048]	Loss: 0.1265
Training Epoch: 60 [10624/50048]	Loss: 0.2150
Training Epoch: 60 [10752/50048]	Loss: 0.1242
Training Epoch: 60 [10880/50048]	Loss: 0.1303
Training Epoch: 60 [11008/50048]	Loss: 0.1396
Training Epoch: 60 [11136/50048]	Loss: 0.2163
Training Epoch: 60 [11264/50048]	Loss: 0.1499
Training Epoch: 60 [11392/50048]	Loss: 0.1725
Training Epoch: 60 [11520/50048]	Loss: 0.2763
Training Epoch: 60 [11648/50048]	Loss: 0.1256
Training Epoch: 60 [11776/50048]	Loss: 0.1582
Training Epoch: 60 [11904/50048]	Loss: 0.3566
Training Epoch: 60 [12032/50048]	Loss: 0.2255
Training Epoch: 60 [12160/50048]	Loss: 0.1667
Training Epoch: 60 [12288/50048]	Loss: 0.1865
Training Epoch: 60 [12416/50048]	Loss: 0.1411
Training Epoch: 60 [12544/50048]	Loss: 0.2246
Training Epoch: 60 [12672/50048]	Loss: 0.1616
Training Epoch: 60 [12800/50048]	Loss: 0.2344
Training Epoch: 60 [12928/50048]	Loss: 0.1954
Training Epoch: 60 [13056/50048]	Loss: 0.2557
Training Epoch: 60 [13184/50048]	Loss: 0.2416
Training Epoch: 60 [13312/50048]	Loss: 0.1696
Training Epoch: 60 [13440/50048]	Loss: 0.2919
Training Epoch: 60 [13568/50048]	Loss: 0.2433
Training Epoch: 60 [13696/50048]	Loss: 0.1626
Training Epoch: 60 [13824/50048]	Loss: 0.1650
Training Epoch: 60 [13952/50048]	Loss: 0.2552
Training Epoch: 60 [14080/50048]	Loss: 0.1852
Training Epoch: 60 [14208/50048]	Loss: 0.2147
Training Epoch: 60 [14336/50048]	Loss: 0.1560
Training Epoch: 60 [14464/50048]	Loss: 0.1482
Training Epoch: 60 [14592/50048]	Loss: 0.1013
Training Epoch: 60 [14720/50048]	Loss: 0.1780
Training Epoch: 60 [14848/50048]	Loss: 0.2470
Training Epoch: 60 [14976/50048]	Loss: 0.2395
Training Epoch: 60 [15104/50048]	Loss: 0.1694
Training Epoch: 60 [15232/50048]	Loss: 0.1941
Training Epoch: 60 [15360/50048]	Loss: 0.1715
Training Epoch: 60 [15488/50048]	Loss: 0.2001
Training Epoch: 60 [15616/50048]	Loss: 0.3320
Training Epoch: 60 [15744/50048]	Loss: 0.2364
Training Epoch: 60 [15872/50048]	Loss: 0.1509
Training Epoch: 60 [16000/50048]	Loss: 0.1931
Training Epoch: 60 [16128/50048]	Loss: 0.1273
Training Epoch: 60 [16256/50048]	Loss: 0.2770
Training Epoch: 60 [16384/50048]	Loss: 0.2055
Training Epoch: 60 [16512/50048]	Loss: 0.1934
Training Epoch: 60 [16640/50048]	Loss: 0.1876
Training Epoch: 60 [16768/50048]	Loss: 0.1874
Training Epoch: 60 [16896/50048]	Loss: 0.2205
Training Epoch: 60 [17024/50048]	Loss: 0.1639
Training Epoch: 60 [17152/50048]	Loss: 0.1704
Training Epoch: 60 [17280/50048]	Loss: 0.2517
Training Epoch: 60 [17408/50048]	Loss: 0.2065
Training Epoch: 60 [17536/50048]	Loss: 0.1675
Training Epoch: 60 [17664/50048]	Loss: 0.1797
Training Epoch: 60 [17792/50048]	Loss: 0.1822
Training Epoch: 60 [17920/50048]	Loss: 0.1894
Training Epoch: 60 [18048/50048]	Loss: 0.1231
Training Epoch: 60 [18176/50048]	Loss: 0.1812
Training Epoch: 60 [18304/50048]	Loss: 0.2352
Training Epoch: 60 [18432/50048]	Loss: 0.1274
Training Epoch: 60 [18560/50048]	Loss: 0.1400
Training Epoch: 60 [18688/50048]	Loss: 0.1719
Training Epoch: 60 [18816/50048]	Loss: 0.1944
Training Epoch: 60 [18944/50048]	Loss: 0.1796
Training Epoch: 60 [19072/50048]	Loss: 0.2557
Training Epoch: 60 [19200/50048]	Loss: 0.2169
Training Epoch: 60 [19328/50048]	Loss: 0.2904
Training Epoch: 60 [19456/50048]	Loss: 0.1200
Training Epoch: 60 [19584/50048]	Loss: 0.1434
Training Epoch: 60 [19712/50048]	Loss: 0.0940
Training Epoch: 60 [19840/50048]	Loss: 0.1557
Training Epoch: 60 [19968/50048]	Loss: 0.1942
Training Epoch: 60 [20096/50048]	Loss: 0.1600
Training Epoch: 60 [20224/50048]	Loss: 0.1758
Training Epoch: 60 [20352/50048]	Loss: 0.1794
Training Epoch: 60 [20480/50048]	Loss: 0.2162
Training Epoch: 60 [20608/50048]	Loss: 0.1820
Training Epoch: 60 [20736/50048]	Loss: 0.1075
Training Epoch: 60 [20864/50048]	Loss: 0.2539
Training Epoch: 60 [20992/50048]	Loss: 0.2672
Training Epoch: 60 [21120/50048]	Loss: 0.2341
Training Epoch: 60 [21248/50048]	Loss: 0.1463
Training Epoch: 60 [21376/50048]	Loss: 0.2108
Training Epoch: 60 [21504/50048]	Loss: 0.1804
Training Epoch: 60 [21632/50048]	Loss: 0.2320
Training Epoch: 60 [21760/50048]	Loss: 0.2053
Training Epoch: 60 [21888/50048]	Loss: 0.2606
Training Epoch: 60 [22016/50048]	Loss: 0.2332
Training Epoch: 60 [22144/50048]	Loss: 0.1679
Training Epoch: 60 [22272/50048]	Loss: 0.1012
Training Epoch: 60 [22400/50048]	Loss: 0.1750
Training Epoch: 60 [22528/50048]	Loss: 0.1524
Training Epoch: 60 [22656/50048]	Loss: 0.1665
Training Epoch: 60 [22784/50048]	Loss: 0.1227
Training Epoch: 60 [22912/50048]	Loss: 0.2443
Training Epoch: 60 [23040/50048]	Loss: 0.1548
Training Epoch: 60 [23168/50048]	Loss: 0.1366
Training Epoch: 60 [23296/50048]	Loss: 0.2355
Training Epoch: 60 [23424/50048]	Loss: 0.1748
Training Epoch: 60 [23552/50048]	Loss: 0.1719
Training Epoch: 60 [23680/50048]	Loss: 0.3865
Training Epoch: 60 [23808/50048]	Loss: 0.1551
Training Epoch: 60 [23936/50048]	Loss: 0.1839
Training Epoch: 60 [24064/50048]	Loss: 0.1940
Training Epoch: 60 [24192/50048]	Loss: 0.1889
Training Epoch: 60 [24320/50048]	Loss: 0.1496
Training Epoch: 60 [24448/50048]	Loss: 0.1733
Training Epoch: 60 [24576/50048]	Loss: 0.2058
Training Epoch: 60 [24704/50048]	Loss: 0.1858
Training Epoch: 60 [24832/50048]	Loss: 0.2902
Training Epoch: 60 [24960/50048]	Loss: 0.2081
Training Epoch: 60 [25088/50048]	Loss: 0.1886
Training Epoch: 60 [25216/50048]	Loss: 0.1902
Training Epoch: 60 [25344/50048]	Loss: 0.2084
Training Epoch: 60 [25472/50048]	Loss: 0.2385
Training Epoch: 60 [25600/50048]	Loss: 0.2462
Training Epoch: 60 [25728/50048]	Loss: 0.1183
Training Epoch: 60 [25856/50048]	Loss: 0.1888
Training Epoch: 60 [25984/50048]	Loss: 0.1216
Training Epoch: 60 [26112/50048]	Loss: 0.2161
Training Epoch: 60 [26240/50048]	Loss: 0.0898
Training Epoch: 60 [26368/50048]	Loss: 0.2328
Training Epoch: 60 [26496/50048]	Loss: 0.2118
Training Epoch: 60 [26624/50048]	Loss: 0.2776
Training Epoch: 60 [26752/50048]	Loss: 0.2297
Training Epoch: 60 [26880/50048]	Loss: 0.1375
Training Epoch: 60 [27008/50048]	Loss: 0.1831
Training Epoch: 60 [27136/50048]	Loss: 0.2024
Training Epoch: 60 [27264/50048]	Loss: 0.2265
Training Epoch: 60 [27392/50048]	Loss: 0.1591
Training Epoch: 60 [27520/50048]	Loss: 0.1720
Training Epoch: 60 [27648/50048]	Loss: 0.3020
Training Epoch: 60 [27776/50048]	Loss: 0.1687
Training Epoch: 60 [27904/50048]	Loss: 0.1845
Training Epoch: 60 [28032/50048]	Loss: 0.1826
Training Epoch: 60 [28160/50048]	Loss: 0.1689
Training Epoch: 60 [28288/50048]	Loss: 0.1415
Training Epoch: 60 [28416/50048]	Loss: 0.1693
Training Epoch: 60 [28544/50048]	Loss: 0.1886
Training Epoch: 60 [28672/50048]	Loss: 0.1713
Training Epoch: 60 [28800/50048]	Loss: 0.2535
Training Epoch: 60 [28928/50048]	Loss: 0.2378
Training Epoch: 60 [29056/50048]	Loss: 0.2013
Training Epoch: 60 [29184/50048]	Loss: 0.2865
Training Epoch: 60 [29312/50048]	Loss: 0.1414
Training Epoch: 60 [29440/50048]	Loss: 0.1970
Training Epoch: 60 [29568/50048]	Loss: 0.2492
Training Epoch: 60 [29696/50048]	Loss: 0.2141
Training Epoch: 60 [29824/50048]	Loss: 0.1854
Training Epoch: 60 [29952/50048]	Loss: 0.2712
Training Epoch: 60 [30080/50048]	Loss: 0.1494
Training Epoch: 60 [30208/50048]	Loss: 0.2189
Training Epoch: 60 [30336/50048]	Loss: 0.1812
Training Epoch: 60 [30464/50048]	Loss: 0.2418
Training Epoch: 60 [30592/50048]	Loss: 0.2917
Training Epoch: 60 [30720/50048]	Loss: 0.1899
Training Epoch: 60 [30848/50048]	Loss: 0.2737
Training Epoch: 60 [30976/50048]	Loss: 0.1403
Training Epoch: 60 [31104/50048]	Loss: 0.2207
Training Epoch: 60 [31232/50048]	Loss: 0.2415
Training Epoch: 60 [31360/50048]	Loss: 0.1543
Training Epoch: 60 [31488/50048]	Loss: 0.2727
Training Epoch: 60 [31616/50048]	Loss: 0.2789
Training Epoch: 60 [31744/50048]	Loss: 0.1785
Training Epoch: 60 [31872/50048]	Loss: 0.1759
Training Epoch: 60 [32000/50048]	Loss: 0.2501
Training Epoch: 60 [32128/50048]	Loss: 0.2181
Training Epoch: 60 [32256/50048]	Loss: 0.2594
Training Epoch: 60 [32384/50048]	Loss: 0.1573
Training Epoch: 60 [32512/50048]	Loss: 0.1958
Training Epoch: 60 [32640/50048]	Loss: 0.2690
Training Epoch: 60 [32768/50048]	Loss: 0.2547
Training Epoch: 60 [32896/50048]	Loss: 0.2256
Training Epoch: 60 [33024/50048]	Loss: 0.1824
Training Epoch: 60 [33152/50048]	Loss: 0.2086
Training Epoch: 60 [33280/50048]	Loss: 0.1985
Training Epoch: 60 [33408/50048]	Loss: 0.2084
Training Epoch: 60 [33536/50048]	Loss: 0.2111
Training Epoch: 60 [33664/50048]	Loss: 0.1110
Training Epoch: 60 [33792/50048]	Loss: 0.1414
Training Epoch: 60 [33920/50048]	Loss: 0.1548
Training Epoch: 60 [34048/50048]	Loss: 0.2678
Training Epoch: 60 [34176/50048]	Loss: 0.2565
Training Epoch: 60 [34304/50048]	Loss: 0.1465
Training Epoch: 60 [34432/50048]	Loss: 0.1846
Training Epoch: 60 [34560/50048]	Loss: 0.3004
Training Epoch: 60 [34688/50048]	Loss: 0.1599
Training Epoch: 60 [34816/50048]	Loss: 0.1364
Training Epoch: 60 [34944/50048]	Loss: 0.2830
Training Epoch: 60 [35072/50048]	Loss: 0.2244
Training Epoch: 60 [35200/50048]	Loss: 0.1605
Training Epoch: 60 [35328/50048]	Loss: 0.2240
Training Epoch: 60 [35456/50048]	Loss: 0.1699
Training Epoch: 60 [35584/50048]	Loss: 0.1872
Training Epoch: 60 [35712/50048]	Loss: 0.1975
Training Epoch: 60 [35840/50048]	Loss: 0.2468
Training Epoch: 60 [35968/50048]	Loss: 0.1749
Training Epoch: 60 [36096/50048]	Loss: 0.2012
Training Epoch: 60 [36224/50048]	Loss: 0.2139
Training Epoch: 60 [36352/50048]	Loss: 0.1991
Training Epoch: 60 [36480/50048]	Loss: 0.1830
Training Epoch: 60 [36608/50048]	Loss: 0.2989
Training Epoch: 60 [36736/50048]	Loss: 0.1232
Training Epoch: 60 [36864/50048]	Loss: 0.1738
Training Epoch: 60 [36992/50048]	Loss: 0.1466
Training Epoch: 60 [37120/50048]	Loss: 0.1604
Training Epoch: 60 [37248/50048]	Loss: 0.1985
Training Epoch: 60 [37376/50048]	Loss: 0.2918
Training Epoch: 60 [37504/50048]	Loss: 0.2510
Training Epoch: 60 [37632/50048]	Loss: 0.1981
Training Epoch: 60 [37760/50048]	Loss: 0.2669
Training Epoch: 60 [37888/50048]	Loss: 0.2362
Training Epoch: 60 [38016/50048]	Loss: 0.1739
Training Epoch: 60 [38144/50048]	Loss: 0.2338
Training Epoch: 60 [38272/50048]	Loss: 0.2593
Training Epoch: 60 [38400/50048]	Loss: 0.2245
Training Epoch: 60 [38528/50048]	Loss: 0.2098
Training Epoch: 60 [38656/50048]	Loss: 0.1783
Training Epoch: 60 [38784/50048]	Loss: 0.1468
Training Epoch: 60 [38912/50048]	Loss: 0.2133
Training Epoch: 60 [39040/50048]	Loss: 0.2540
Training Epoch: 60 [39168/50048]	Loss: 0.1701
Training Epoch: 60 [39296/50048]	Loss: 0.2261
Training Epoch: 60 [39424/50048]	Loss: 0.1650
Training Epoch: 60 [39552/50048]	Loss: 0.2290
Training Epoch: 60 [39680/50048]	Loss: 0.1734
Training Epoch: 60 [39808/50048]	Loss: 0.2134
Training Epoch: 60 [39936/50048]	Loss: 0.1673
Training Epoch: 60 [40064/50048]	Loss: 0.2072
Training Epoch: 60 [40192/50048]	Loss: 0.1783
Training Epoch: 60 [40320/50048]	Loss: 0.2188
Training Epoch: 60 [40448/50048]	Loss: 0.2653
Training Epoch: 60 [40576/50048]	Loss: 0.2532
Training Epoch: 60 [40704/50048]	Loss: 0.1332
Training Epoch: 60 [40832/50048]	Loss: 0.2537
Training Epoch: 60 [40960/50048]	Loss: 0.1948
Training Epoch: 60 [41088/50048]	Loss: 0.1631
Training Epoch: 60 [41216/50048]	Loss: 0.2739
Training Epoch: 60 [41344/50048]	Loss: 0.2085
Training Epoch: 60 [41472/50048]	Loss: 0.2137
Training Epoch: 60 [41600/50048]	Loss: 0.2675
Training Epoch: 60 [41728/50048]	Loss: 0.1130
Training Epoch: 60 [41856/50048]	Loss: 0.1256
Training Epoch: 60 [41984/50048]	Loss: 0.3073
Training Epoch: 60 [42112/50048]	Loss: 0.1509
Training Epoch: 60 [42240/50048]	Loss: 0.1342
Training Epoch: 60 [42368/50048]	Loss: 0.2278
Training Epoch: 60 [42496/50048]	Loss: 0.1946
Training Epoch: 60 [42624/50048]	Loss: 0.2557
Training Epoch: 60 [42752/50048]	Loss: 0.2409
Training Epoch: 60 [42880/50048]	Loss: 0.2166
Training Epoch: 60 [43008/50048]	Loss: 0.1637
Training Epoch: 60 [43136/50048]	Loss: 0.2390
Training Epoch: 60 [43264/50048]	Loss: 0.2360
Training Epoch: 60 [43392/50048]	Loss: 0.2136
Training Epoch: 60 [43520/50048]	Loss: 0.2209
Training Epoch: 60 [43648/50048]	Loss: 0.2431
Training Epoch: 60 [43776/50048]	Loss: 0.1942
Training Epoch: 60 [43904/50048]	Loss: 0.3291
Training Epoch: 60 [44032/50048]	Loss: 0.1588
Training Epoch: 60 [44160/50048]	Loss: 0.2376
Training Epoch: 60 [44288/50048]	Loss: 0.2486
Training Epoch: 60 [44416/50048]	Loss: 0.0963
Training Epoch: 60 [44544/50048]	Loss: 0.1307
Training Epoch: 60 [44672/50048]	Loss: 0.1683
Training Epoch: 60 [44800/50048]	Loss: 0.2601
Training Epoch: 60 [44928/50048]	Loss: 0.1560
Training Epoch: 60 [45056/50048]	Loss: 0.2063
Training Epoch: 60 [45184/50048]	Loss: 0.1716
Training Epoch: 60 [45312/50048]	Loss: 0.2011
Training Epoch: 60 [45440/50048]	Loss: 0.2238
Training Epoch: 60 [45568/50048]	Loss: 0.1950
Training Epoch: 60 [45696/50048]	Loss: 0.1432
2022-12-06 07:39:33,441 [ZeusDataLoader(train)] train epoch 61 done: time=86.49 energy=10499.04
2022-12-06 07:39:33,443 [ZeusDataLoader(eval)] Epoch 61 begin.
Training Epoch: 60 [45824/50048]	Loss: 0.2232
Training Epoch: 60 [45952/50048]	Loss: 0.1782
Training Epoch: 60 [46080/50048]	Loss: 0.2003
Training Epoch: 60 [46208/50048]	Loss: 0.2457
Training Epoch: 60 [46336/50048]	Loss: 0.2647
Training Epoch: 60 [46464/50048]	Loss: 0.3842
Training Epoch: 60 [46592/50048]	Loss: 0.4617
Training Epoch: 60 [46720/50048]	Loss: 0.1967
Training Epoch: 60 [46848/50048]	Loss: 0.2071
Training Epoch: 60 [46976/50048]	Loss: 0.2614
Training Epoch: 60 [47104/50048]	Loss: 0.2072
Training Epoch: 60 [47232/50048]	Loss: 0.2153
Training Epoch: 60 [47360/50048]	Loss: 0.3258
Training Epoch: 60 [47488/50048]	Loss: 0.1790
Training Epoch: 60 [47616/50048]	Loss: 0.2736
Training Epoch: 60 [47744/50048]	Loss: 0.1725
Training Epoch: 60 [47872/50048]	Loss: 0.1830
Training Epoch: 60 [48000/50048]	Loss: 0.2624
Training Epoch: 60 [48128/50048]	Loss: 0.2479
Training Epoch: 60 [48256/50048]	Loss: 0.2350
Training Epoch: 60 [48384/50048]	Loss: 0.1732
Training Epoch: 60 [48512/50048]	Loss: 0.1926
Training Epoch: 60 [48640/50048]	Loss: 0.2746
Training Epoch: 60 [48768/50048]	Loss: 0.3660
Training Epoch: 60 [48896/50048]	Loss: 0.2533
Training Epoch: 60 [49024/50048]	Loss: 0.1355
Training Epoch: 60 [49152/50048]	Loss: 0.2386
Training Epoch: 60 [49280/50048]	Loss: 0.3377
Training Epoch: 60 [49408/50048]	Loss: 0.1546
Training Epoch: 60 [49536/50048]	Loss: 0.2517
Training Epoch: 60 [49664/50048]	Loss: 0.1897
Training Epoch: 60 [49792/50048]	Loss: 0.2188
Training Epoch: 60 [49920/50048]	Loss: 0.2142
Training Epoch: 60 [50048/50048]	Loss: 0.1132
2022-12-06 12:39:37.178 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:39:37,209 [ZeusDataLoader(eval)] eval epoch 61 done: time=3.76 energy=455.23
2022-12-06 07:39:37,210 [ZeusDataLoader(train)] Up to epoch 61: time=5502.25, energy=667909.17, cost=815401.39
2022-12-06 07:39:37,210 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:39:37,210 [ZeusDataLoader(train)] Expected next epoch: time=5592.05, energy=678707.19, cost=828657.78
2022-12-06 07:39:37,211 [ZeusDataLoader(train)] Epoch 62 begin.
Validation Epoch: 60, Average loss: 0.0163, Accuracy: 0.6376
2022-12-06 07:39:37,393 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:39:37,394 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:39:37.396 [ZeusMonitor] Monitor started.
2022-12-06 12:39:37.396 [ZeusMonitor] Running indefinitely. 2022-12-06 12:39:37.396 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:39:37.396 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e62+gpu0.power.log
Training Epoch: 61 [128/50048]	Loss: 0.1813
Training Epoch: 61 [256/50048]	Loss: 0.1388
Training Epoch: 61 [384/50048]	Loss: 0.1351
Training Epoch: 61 [512/50048]	Loss: 0.1602
Training Epoch: 61 [640/50048]	Loss: 0.2025
Training Epoch: 61 [768/50048]	Loss: 0.2122
Training Epoch: 61 [896/50048]	Loss: 0.2714
Training Epoch: 61 [1024/50048]	Loss: 0.2058
Training Epoch: 61 [1152/50048]	Loss: 0.1970
Training Epoch: 61 [1280/50048]	Loss: 0.1923
Training Epoch: 61 [1408/50048]	Loss: 0.2061
Training Epoch: 61 [1536/50048]	Loss: 0.1940
Training Epoch: 61 [1664/50048]	Loss: 0.1423
Training Epoch: 61 [1792/50048]	Loss: 0.1809
Training Epoch: 61 [1920/50048]	Loss: 0.1820
Training Epoch: 61 [2048/50048]	Loss: 0.1238
Training Epoch: 61 [2176/50048]	Loss: 0.1899
Training Epoch: 61 [2304/50048]	Loss: 0.2811
Training Epoch: 61 [2432/50048]	Loss: 0.2395
Training Epoch: 61 [2560/50048]	Loss: 0.1241
Training Epoch: 61 [2688/50048]	Loss: 0.1672
Training Epoch: 61 [2816/50048]	Loss: 0.1586
Training Epoch: 61 [2944/50048]	Loss: 0.1428
Training Epoch: 61 [3072/50048]	Loss: 0.1833
Training Epoch: 61 [3200/50048]	Loss: 0.2360
Training Epoch: 61 [3328/50048]	Loss: 0.2565
Training Epoch: 61 [3456/50048]	Loss: 0.2159
Training Epoch: 61 [3584/50048]	Loss: 0.1897
Training Epoch: 61 [3712/50048]	Loss: 0.1433
Training Epoch: 61 [3840/50048]	Loss: 0.2222
Training Epoch: 61 [3968/50048]	Loss: 0.1664
Training Epoch: 61 [4096/50048]	Loss: 0.1423
Training Epoch: 61 [4224/50048]	Loss: 0.1032
Training Epoch: 61 [4352/50048]	Loss: 0.2477
Training Epoch: 61 [4480/50048]	Loss: 0.2294
Training Epoch: 61 [4608/50048]	Loss: 0.1336
Training Epoch: 61 [4736/50048]	Loss: 0.1892
Training Epoch: 61 [4864/50048]	Loss: 0.1226
Training Epoch: 61 [4992/50048]	Loss: 0.1128
Training Epoch: 61 [5120/50048]	Loss: 0.1445
Training Epoch: 61 [5248/50048]	Loss: 0.1206
Training Epoch: 61 [5376/50048]	Loss: 0.1605
Training Epoch: 61 [5504/50048]	Loss: 0.2116
Training Epoch: 61 [5632/50048]	Loss: 0.1605
Training Epoch: 61 [5760/50048]	Loss: 0.1587
Training Epoch: 61 [5888/50048]	Loss: 0.2271
Training Epoch: 61 [6016/50048]	Loss: 0.2072
Training Epoch: 61 [6144/50048]	Loss: 0.1865
Training Epoch: 61 [6272/50048]	Loss: 0.2577
Training Epoch: 61 [6400/50048]	Loss: 0.1864
Training Epoch: 61 [6528/50048]	Loss: 0.1365
Training Epoch: 61 [6656/50048]	Loss: 0.2441
Training Epoch: 61 [6784/50048]	Loss: 0.1418
Training Epoch: 61 [6912/50048]	Loss: 0.0936
Training Epoch: 61 [7040/50048]	Loss: 0.0923
Training Epoch: 61 [7168/50048]	Loss: 0.1460
Training Epoch: 61 [7296/50048]	Loss: 0.1947
Training Epoch: 61 [7424/50048]	Loss: 0.1990
Training Epoch: 61 [7552/50048]	Loss: 0.1905
Training Epoch: 61 [7680/50048]	Loss: 0.1347
Training Epoch: 61 [7808/50048]	Loss: 0.1403
Training Epoch: 61 [7936/50048]	Loss: 0.1119
Training Epoch: 61 [8064/50048]	Loss: 0.1930
Training Epoch: 61 [8192/50048]	Loss: 0.1658
Training Epoch: 61 [8320/50048]	Loss: 0.1881
Training Epoch: 61 [8448/50048]	Loss: 0.2059
Training Epoch: 61 [8576/50048]	Loss: 0.1605
Training Epoch: 61 [8704/50048]	Loss: 0.1779
Training Epoch: 61 [8832/50048]	Loss: 0.1304
Training Epoch: 61 [8960/50048]	Loss: 0.1558
Training Epoch: 61 [9088/50048]	Loss: 0.2285
Training Epoch: 61 [9216/50048]	Loss: 0.2114
Training Epoch: 61 [9344/50048]	Loss: 0.2659
Training Epoch: 61 [9472/50048]	Loss: 0.1602
Training Epoch: 61 [9600/50048]	Loss: 0.1025
Training Epoch: 61 [9728/50048]	Loss: 0.2114
Training Epoch: 61 [9856/50048]	Loss: 0.1720
Training Epoch: 61 [9984/50048]	Loss: 0.1752
Training Epoch: 61 [10112/50048]	Loss: 0.2281
Training Epoch: 61 [10240/50048]	Loss: 0.1651
Training Epoch: 61 [10368/50048]	Loss: 0.1582
Training Epoch: 61 [10496/50048]	Loss: 0.1725
Training Epoch: 61 [10624/50048]	Loss: 0.3209
Training Epoch: 61 [10752/50048]	Loss: 0.1141
Training Epoch: 61 [10880/50048]	Loss: 0.1969
Training Epoch: 61 [11008/50048]	Loss: 0.1105
Training Epoch: 61 [11136/50048]	Loss: 0.2292
Training Epoch: 61 [11264/50048]	Loss: 0.1680
Training Epoch: 61 [11392/50048]	Loss: 0.1257
Training Epoch: 61 [11520/50048]	Loss: 0.1937
Training Epoch: 61 [11648/50048]	Loss: 0.1833
Training Epoch: 61 [11776/50048]	Loss: 0.1754
Training Epoch: 61 [11904/50048]	Loss: 0.2432
Training Epoch: 61 [12032/50048]	Loss: 0.2645
Training Epoch: 61 [12160/50048]	Loss: 0.1496
Training Epoch: 61 [12288/50048]	Loss: 0.1865
Training Epoch: 61 [12416/50048]	Loss: 0.1550
Training Epoch: 61 [12544/50048]	Loss: 0.1974
Training Epoch: 61 [12672/50048]	Loss: 0.1132
Training Epoch: 61 [12800/50048]	Loss: 0.1299
Training Epoch: 61 [12928/50048]	Loss: 0.1783
Training Epoch: 61 [13056/50048]	Loss: 0.1233
Training Epoch: 61 [13184/50048]	Loss: 0.2562
Training Epoch: 61 [13312/50048]	Loss: 0.2203
Training Epoch: 61 [13440/50048]	Loss: 0.1466
Training Epoch: 61 [13568/50048]	Loss: 0.1200
Training Epoch: 61 [13696/50048]	Loss: 0.2972
Training Epoch: 61 [13824/50048]	Loss: 0.1452
Training Epoch: 61 [13952/50048]	Loss: 0.2034
Training Epoch: 61 [14080/50048]	Loss: 0.1869
Training Epoch: 61 [14208/50048]	Loss: 0.1229
Training Epoch: 61 [14336/50048]	Loss: 0.2423
Training Epoch: 61 [14464/50048]	Loss: 0.1134
Training Epoch: 61 [14592/50048]	Loss: 0.2293
Training Epoch: 61 [14720/50048]	Loss: 0.2382
Training Epoch: 61 [14848/50048]	Loss: 0.2669
Training Epoch: 61 [14976/50048]	Loss: 0.1578
Training Epoch: 61 [15104/50048]	Loss: 0.1709
Training Epoch: 61 [15232/50048]	Loss: 0.0999
Training Epoch: 61 [15360/50048]	Loss: 0.1747
Training Epoch: 61 [15488/50048]	Loss: 0.2171
Training Epoch: 61 [15616/50048]	Loss: 0.3478
Training Epoch: 61 [15744/50048]	Loss: 0.1643
Training Epoch: 61 [15872/50048]	Loss: 0.1486
Training Epoch: 61 [16000/50048]	Loss: 0.1686
Training Epoch: 61 [16128/50048]	Loss: 0.1984
Training Epoch: 61 [16256/50048]	Loss: 0.3306
Training Epoch: 61 [16384/50048]	Loss: 0.1727
Training Epoch: 61 [16512/50048]	Loss: 0.1391
Training Epoch: 61 [16640/50048]	Loss: 0.1311
Training Epoch: 61 [16768/50048]	Loss: 0.1613
Training Epoch: 61 [16896/50048]	Loss: 0.1876
Training Epoch: 61 [17024/50048]	Loss: 0.2234
Training Epoch: 61 [17152/50048]	Loss: 0.2959
Training Epoch: 61 [17280/50048]	Loss: 0.1300
Training Epoch: 61 [17408/50048]	Loss: 0.2185
Training Epoch: 61 [17536/50048]	Loss: 0.1721
Training Epoch: 61 [17664/50048]	Loss: 0.0902
Training Epoch: 61 [17792/50048]	Loss: 0.1714
Training Epoch: 61 [17920/50048]	Loss: 0.1381
Training Epoch: 61 [18048/50048]	Loss: 0.1803
Training Epoch: 61 [18176/50048]	Loss: 0.1242
Training Epoch: 61 [18304/50048]	Loss: 0.0910
Training Epoch: 61 [18432/50048]	Loss: 0.1003
Training Epoch: 61 [18560/50048]	Loss: 0.1450
Training Epoch: 61 [18688/50048]	Loss: 0.1582
Training Epoch: 61 [18816/50048]	Loss: 0.1680
Training Epoch: 61 [18944/50048]	Loss: 0.1215
Training Epoch: 61 [19072/50048]	Loss: 0.1635
Training Epoch: 61 [19200/50048]	Loss: 0.1400
Training Epoch: 61 [19328/50048]	Loss: 0.1460
Training Epoch: 61 [19456/50048]	Loss: 0.1518
Training Epoch: 61 [19584/50048]	Loss: 0.1127
Training Epoch: 61 [19712/50048]	Loss: 0.2571
Training Epoch: 61 [19840/50048]	Loss: 0.3346
Training Epoch: 61 [19968/50048]	Loss: 0.1678
Training Epoch: 61 [20096/50048]	Loss: 0.1878
Training Epoch: 61 [20224/50048]	Loss: 0.1443
Training Epoch: 61 [20352/50048]	Loss: 0.1389
Training Epoch: 61 [20480/50048]	Loss: 0.1624
Training Epoch: 61 [20608/50048]	Loss: 0.2117
Training Epoch: 61 [20736/50048]	Loss: 0.1454
Training Epoch: 61 [20864/50048]	Loss: 0.2106
Training Epoch: 61 [20992/50048]	Loss: 0.1426
Training Epoch: 61 [21120/50048]	Loss: 0.1670
Training Epoch: 61 [21248/50048]	Loss: 0.2431
Training Epoch: 61 [21376/50048]	Loss: 0.2940
Training Epoch: 61 [21504/50048]	Loss: 0.2279
Training Epoch: 61 [21632/50048]	Loss: 0.1329
Training Epoch: 61 [21760/50048]	Loss: 0.1338
Training Epoch: 61 [21888/50048]	Loss: 0.2628
Training Epoch: 61 [22016/50048]	Loss: 0.1901
Training Epoch: 61 [22144/50048]	Loss: 0.1942
Training Epoch: 61 [22272/50048]	Loss: 0.1342
Training Epoch: 61 [22400/50048]	Loss: 0.1808
Training Epoch: 61 [22528/50048]	Loss: 0.1850
Training Epoch: 61 [22656/50048]	Loss: 0.2334
Training Epoch: 61 [22784/50048]	Loss: 0.1866
Training Epoch: 61 [22912/50048]	Loss: 0.2056
Training Epoch: 61 [23040/50048]	Loss: 0.0909
Training Epoch: 61 [23168/50048]	Loss: 0.3000
Training Epoch: 61 [23296/50048]	Loss: 0.1619
Training Epoch: 61 [23424/50048]	Loss: 0.1887
Training Epoch: 61 [23552/50048]	Loss: 0.1063
Training Epoch: 61 [23680/50048]	Loss: 0.1745
Training Epoch: 61 [23808/50048]	Loss: 0.1064
Training Epoch: 61 [23936/50048]	Loss: 0.1574
Training Epoch: 61 [24064/50048]	Loss: 0.1687
Training Epoch: 61 [24192/50048]	Loss: 0.2019
Training Epoch: 61 [24320/50048]	Loss: 0.1390
Training Epoch: 61 [24448/50048]	Loss: 0.1694
Training Epoch: 61 [24576/50048]	Loss: 0.1932
Training Epoch: 61 [24704/50048]	Loss: 0.1868
Training Epoch: 61 [24832/50048]	Loss: 0.1298
Training Epoch: 61 [24960/50048]	Loss: 0.2321
Training Epoch: 61 [25088/50048]	Loss: 0.2102
Training Epoch: 61 [25216/50048]	Loss: 0.2224
Training Epoch: 61 [25344/50048]	Loss: 0.1599
Training Epoch: 61 [25472/50048]	Loss: 0.1877
Training Epoch: 61 [25600/50048]	Loss: 0.2271
Training Epoch: 61 [25728/50048]	Loss: 0.1259
Training Epoch: 61 [25856/50048]	Loss: 0.2119
Training Epoch: 61 [25984/50048]	Loss: 0.3019
Training Epoch: 61 [26112/50048]	Loss: 0.2822
Training Epoch: 61 [26240/50048]	Loss: 0.2183
Training Epoch: 61 [26368/50048]	Loss: 0.2783
Training Epoch: 61 [26496/50048]	Loss: 0.1354
Training Epoch: 61 [26624/50048]	Loss: 0.2173
Training Epoch: 61 [26752/50048]	Loss: 0.1994
Training Epoch: 61 [26880/50048]	Loss: 0.2743
Training Epoch: 61 [27008/50048]	Loss: 0.1757
Training Epoch: 61 [27136/50048]	Loss: 0.3832
Training Epoch: 61 [27264/50048]	Loss: 0.2924
Training Epoch: 61 [27392/50048]	Loss: 0.1346
Training Epoch: 61 [27520/50048]	Loss: 0.2081
Training Epoch: 61 [27648/50048]	Loss: 0.2296
Training Epoch: 61 [27776/50048]	Loss: 0.2539
Training Epoch: 61 [27904/50048]	Loss: 0.1874
Training Epoch: 61 [28032/50048]	Loss: 0.2127
Training Epoch: 61 [28160/50048]	Loss: 0.1553
Training Epoch: 61 [28288/50048]	Loss: 0.1656
Training Epoch: 61 [28416/50048]	Loss: 0.1376
Training Epoch: 61 [28544/50048]	Loss: 0.2657
Training Epoch: 61 [28672/50048]	Loss: 0.1332
Training Epoch: 61 [28800/50048]	Loss: 0.1773
Training Epoch: 61 [28928/50048]	Loss: 0.0804
Training Epoch: 61 [29056/50048]	Loss: 0.2036
Training Epoch: 61 [29184/50048]	Loss: 0.1950
Training Epoch: 61 [29312/50048]	Loss: 0.1646
Training Epoch: 61 [29440/50048]	Loss: 0.2043
Training Epoch: 61 [29568/50048]	Loss: 0.1591
Training Epoch: 61 [29696/50048]	Loss: 0.1303
Training Epoch: 61 [29824/50048]	Loss: 0.2565
Training Epoch: 61 [29952/50048]	Loss: 0.1098
Training Epoch: 61 [30080/50048]	Loss: 0.2173
Training Epoch: 61 [30208/50048]	Loss: 0.1019
Training Epoch: 61 [30336/50048]	Loss: 0.2659
Training Epoch: 61 [30464/50048]	Loss: 0.1482
Training Epoch: 61 [30592/50048]	Loss: 0.1341
Training Epoch: 61 [30720/50048]	Loss: 0.2259
Training Epoch: 61 [30848/50048]	Loss: 0.2341
Training Epoch: 61 [30976/50048]	Loss: 0.2530
Training Epoch: 61 [31104/50048]	Loss: 0.1590
Training Epoch: 61 [31232/50048]	Loss: 0.2259
Training Epoch: 61 [31360/50048]	Loss: 0.1249
Training Epoch: 61 [31488/50048]	Loss: 0.1400
Training Epoch: 61 [31616/50048]	Loss: 0.1422
Training Epoch: 61 [31744/50048]	Loss: 0.2940
Training Epoch: 61 [31872/50048]	Loss: 0.2070
Training Epoch: 61 [32000/50048]	Loss: 0.1713
Training Epoch: 61 [32128/50048]	Loss: 0.1486
Training Epoch: 61 [32256/50048]	Loss: 0.3187
Training Epoch: 61 [32384/50048]	Loss: 0.2236
Training Epoch: 61 [32512/50048]	Loss: 0.2461
Training Epoch: 61 [32640/50048]	Loss: 0.1695
Training Epoch: 61 [32768/50048]	Loss: 0.1474
Training Epoch: 61 [32896/50048]	Loss: 0.1828
Training Epoch: 61 [33024/50048]	Loss: 0.3296
Training Epoch: 61 [33152/50048]	Loss: 0.3203
Training Epoch: 61 [33280/50048]	Loss: 0.1772
Training Epoch: 61 [33408/50048]	Loss: 0.2074
Training Epoch: 61 [33536/50048]	Loss: 0.2189
Training Epoch: 61 [33664/50048]	Loss: 0.2320
Training Epoch: 61 [33792/50048]	Loss: 0.1884
Training Epoch: 61 [33920/50048]	Loss: 0.2352
Training Epoch: 61 [34048/50048]	Loss: 0.1404
Training Epoch: 61 [34176/50048]	Loss: 0.2036
Training Epoch: 61 [34304/50048]	Loss: 0.2058
Training Epoch: 61 [34432/50048]	Loss: 0.2157
Training Epoch: 61 [34560/50048]	Loss: 0.2358
Training Epoch: 61 [34688/50048]	Loss: 0.1198
Training Epoch: 61 [34816/50048]	Loss: 0.2834
Training Epoch: 61 [34944/50048]	Loss: 0.2121
Training Epoch: 61 [35072/50048]	Loss: 0.3662
Training Epoch: 61 [35200/50048]	Loss: 0.2389
Training Epoch: 61 [35328/50048]	Loss: 0.1463
Training Epoch: 61 [35456/50048]	Loss: 0.1585
Training Epoch: 61 [35584/50048]	Loss: 0.2257
Training Epoch: 61 [35712/50048]	Loss: 0.2993
Training Epoch: 61 [35840/50048]	Loss: 0.2234
Training Epoch: 61 [35968/50048]	Loss: 0.1909
Training Epoch: 61 [36096/50048]	Loss: 0.1460
Training Epoch: 61 [36224/50048]	Loss: 0.2731
Training Epoch: 61 [36352/50048]	Loss: 0.1920
Training Epoch: 61 [36480/50048]	Loss: 0.1558
Training Epoch: 61 [36608/50048]	Loss: 0.2352
Training Epoch: 61 [36736/50048]	Loss: 0.1977
Training Epoch: 61 [36864/50048]	Loss: 0.2692
Training Epoch: 61 [36992/50048]	Loss: 0.1184
Training Epoch: 61 [37120/50048]	Loss: 0.1441
Training Epoch: 61 [37248/50048]	Loss: 0.2329
Training Epoch: 61 [37376/50048]	Loss: 0.1746
Training Epoch: 61 [37504/50048]	Loss: 0.1204
Training Epoch: 61 [37632/50048]	Loss: 0.1465
Training Epoch: 61 [37760/50048]	Loss: 0.2221
Training Epoch: 61 [37888/50048]	Loss: 0.1266
Training Epoch: 61 [38016/50048]	Loss: 0.2171
Training Epoch: 61 [38144/50048]	Loss: 0.1394
Training Epoch: 61 [38272/50048]	Loss: 0.1614
Training Epoch: 61 [38400/50048]	Loss: 0.2155
Training Epoch: 61 [38528/50048]	Loss: 0.2252
Training Epoch: 61 [38656/50048]	Loss: 0.1663
Training Epoch: 61 [38784/50048]	Loss: 0.2170
Training Epoch: 61 [38912/50048]	Loss: 0.3127
Training Epoch: 61 [39040/50048]	Loss: 0.1313
Training Epoch: 61 [39168/50048]	Loss: 0.1415
Training Epoch: 61 [39296/50048]	Loss: 0.2111
Training Epoch: 61 [39424/50048]	Loss: 0.2555
Training Epoch: 61 [39552/50048]	Loss: 0.1769
Training Epoch: 61 [39680/50048]	Loss: 0.1952
Training Epoch: 61 [39808/50048]	Loss: 0.1745
Training Epoch: 61 [39936/50048]	Loss: 0.1914
Training Epoch: 61 [40064/50048]	Loss: 0.3058
Training Epoch: 61 [40192/50048]	Loss: 0.1034
Training Epoch: 61 [40320/50048]	Loss: 0.2125
Training Epoch: 61 [40448/50048]	Loss: 0.3559
Training Epoch: 61 [40576/50048]	Loss: 0.1894
Training Epoch: 61 [40704/50048]	Loss: 0.2186
Training Epoch: 61 [40832/50048]	Loss: 0.2111
Training Epoch: 61 [40960/50048]	Loss: 0.2242
Training Epoch: 61 [41088/50048]	Loss: 0.1757
Training Epoch: 61 [41216/50048]	Loss: 0.2322
Training Epoch: 61 [41344/50048]	Loss: 0.2989
Training Epoch: 61 [41472/50048]	Loss: 0.2672
Training Epoch: 61 [41600/50048]	Loss: 0.1972
Training Epoch: 61 [41728/50048]	Loss: 0.2085
Training Epoch: 61 [41856/50048]	Loss: 0.3048
Training Epoch: 61 [41984/50048]	Loss: 0.2348
Training Epoch: 61 [42112/50048]	Loss: 0.1664
Training Epoch: 61 [42240/50048]	Loss: 0.1552
Training Epoch: 61 [42368/50048]	Loss: 0.1807
Training Epoch: 61 [42496/50048]	Loss: 0.1937
Training Epoch: 61 [42624/50048]	Loss: 0.1657
Training Epoch: 61 [42752/50048]	Loss: 0.1895
Training Epoch: 61 [42880/50048]	Loss: 0.1894
Training Epoch: 61 [43008/50048]	Loss: 0.2594
Training Epoch: 61 [43136/50048]	Loss: 0.2815
Training Epoch: 61 [43264/50048]	Loss: 0.0896
Training Epoch: 61 [43392/50048]	Loss: 0.1578
Training Epoch: 61 [43520/50048]	Loss: 0.2139
Training Epoch: 61 [43648/50048]	Loss: 0.1259
Training Epoch: 61 [43776/50048]	Loss: 0.3202
Training Epoch: 61 [43904/50048]	Loss: 0.2110
Training Epoch: 61 [44032/50048]	Loss: 0.1201
Training Epoch: 61 [44160/50048]	Loss: 0.2493
Training Epoch: 61 [44288/50048]	Loss: 0.2519
Training Epoch: 61 [44416/50048]	Loss: 0.1805
Training Epoch: 61 [44544/50048]	Loss: 0.1792
Training Epoch: 61 [44672/50048]	Loss: 0.2126
Training Epoch: 61 [44800/50048]	Loss: 0.1647
Training Epoch: 61 [44928/50048]	Loss: 0.1519
Training Epoch: 61 [45056/50048]	Loss: 0.1711
Training Epoch: 61 [45184/50048]	Loss: 0.1961
Training Epoch: 61 [45312/50048]	Loss: 0.2618
Training Epoch: 61 [45440/50048]	Loss: 0.2058
Training Epoch: 61 [45568/50048]	Loss: 0.3414
Training Epoch: 61 [45696/50048]	Loss: 0.2034
2022-12-06 07:41:03,633 [ZeusDataLoader(train)] train epoch 62 done: time=86.41 energy=10490.95
2022-12-06 07:41:03,635 [ZeusDataLoader(eval)] Epoch 62 begin.
Training Epoch: 61 [45824/50048]	Loss: 0.3043
Training Epoch: 61 [45952/50048]	Loss: 0.1831
Training Epoch: 61 [46080/50048]	Loss: 0.2181
Training Epoch: 61 [46208/50048]	Loss: 0.2303
Training Epoch: 61 [46336/50048]	Loss: 0.1269
Training Epoch: 61 [46464/50048]	Loss: 0.3561
Training Epoch: 61 [46592/50048]	Loss: 0.2017
Training Epoch: 61 [46720/50048]	Loss: 0.2447
Training Epoch: 61 [46848/50048]	Loss: 0.2254
Training Epoch: 61 [46976/50048]	Loss: 0.2293
Training Epoch: 61 [47104/50048]	Loss: 0.1945
Training Epoch: 61 [47232/50048]	Loss: 0.2368
Training Epoch: 61 [47360/50048]	Loss: 0.2013
Training Epoch: 61 [47488/50048]	Loss: 0.1585
Training Epoch: 61 [47616/50048]	Loss: 0.2418
Training Epoch: 61 [47744/50048]	Loss: 0.2040
Training Epoch: 61 [47872/50048]	Loss: 0.2253
Training Epoch: 61 [48000/50048]	Loss: 0.2356
Training Epoch: 61 [48128/50048]	Loss: 0.2283
Training Epoch: 61 [48256/50048]	Loss: 0.2608
Training Epoch: 61 [48384/50048]	Loss: 0.2586
Training Epoch: 61 [48512/50048]	Loss: 0.2939
Training Epoch: 61 [48640/50048]	Loss: 0.2297
Training Epoch: 61 [48768/50048]	Loss: 0.1907
Training Epoch: 61 [48896/50048]	Loss: 0.2697
Training Epoch: 61 [49024/50048]	Loss: 0.3215
Training Epoch: 61 [49152/50048]	Loss: 0.1905
Training Epoch: 61 [49280/50048]	Loss: 0.1913
Training Epoch: 61 [49408/50048]	Loss: 0.2267
Training Epoch: 61 [49536/50048]	Loss: 0.2288
Training Epoch: 61 [49664/50048]	Loss: 0.1504
Training Epoch: 61 [49792/50048]	Loss: 0.1791
Training Epoch: 61 [49920/50048]	Loss: 0.1438
Training Epoch: 61 [50048/50048]	Loss: 0.2513
2022-12-06 12:41:07.288 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:41:07,298 [ZeusDataLoader(eval)] eval epoch 62 done: time=3.65 energy=441.78
2022-12-06 07:41:07,298 [ZeusDataLoader(train)] Up to epoch 62: time=5592.32, energy=678841.90, cost=828748.56
2022-12-06 07:41:07,298 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:41:07,298 [ZeusDataLoader(train)] Expected next epoch: time=5682.11, energy=689639.92, cost=842004.95
2022-12-06 07:41:07,299 [ZeusDataLoader(train)] Epoch 63 begin.
Validation Epoch: 61, Average loss: 0.0163, Accuracy: 0.6357
2022-12-06 07:41:07,486 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:41:07,487 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:41:07.489 [ZeusMonitor] Monitor started.
2022-12-06 12:41:07.489 [ZeusMonitor] Running indefinitely. 2022-12-06 12:41:07.489 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:41:07.489 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e63+gpu0.power.log
Training Epoch: 62 [128/50048]	Loss: 0.1871
Training Epoch: 62 [256/50048]	Loss: 0.1064
Training Epoch: 62 [384/50048]	Loss: 0.2173
Training Epoch: 62 [512/50048]	Loss: 0.1595
Training Epoch: 62 [640/50048]	Loss: 0.1499
Training Epoch: 62 [768/50048]	Loss: 0.1696
Training Epoch: 62 [896/50048]	Loss: 0.1164
Training Epoch: 62 [1024/50048]	Loss: 0.2032
Training Epoch: 62 [1152/50048]	Loss: 0.2327
Training Epoch: 62 [1280/50048]	Loss: 0.1021
Training Epoch: 62 [1408/50048]	Loss: 0.1676
Training Epoch: 62 [1536/50048]	Loss: 0.1696
Training Epoch: 62 [1664/50048]	Loss: 0.1751
Training Epoch: 62 [1792/50048]	Loss: 0.1158
Training Epoch: 62 [1920/50048]	Loss: 0.1634
Training Epoch: 62 [2048/50048]	Loss: 0.1650
Training Epoch: 62 [2176/50048]	Loss: 0.1511
Training Epoch: 62 [2304/50048]	Loss: 0.1488
Training Epoch: 62 [2432/50048]	Loss: 0.1587
Training Epoch: 62 [2560/50048]	Loss: 0.1670
Training Epoch: 62 [2688/50048]	Loss: 0.1695
Training Epoch: 62 [2816/50048]	Loss: 0.1571
Training Epoch: 62 [2944/50048]	Loss: 0.2787
Training Epoch: 62 [3072/50048]	Loss: 0.1472
Training Epoch: 62 [3200/50048]	Loss: 0.1622
Training Epoch: 62 [3328/50048]	Loss: 0.1212
Training Epoch: 62 [3456/50048]	Loss: 0.1313
Training Epoch: 62 [3584/50048]	Loss: 0.1297
Training Epoch: 62 [3712/50048]	Loss: 0.1168
Training Epoch: 62 [3840/50048]	Loss: 0.1238
Training Epoch: 62 [3968/50048]	Loss: 0.1175
Training Epoch: 62 [4096/50048]	Loss: 0.1577
Training Epoch: 62 [4224/50048]	Loss: 0.2042
Training Epoch: 62 [4352/50048]	Loss: 0.1193
Training Epoch: 62 [4480/50048]	Loss: 0.2158
Training Epoch: 62 [4608/50048]	Loss: 0.1679
Training Epoch: 62 [4736/50048]	Loss: 0.2136
Training Epoch: 62 [4864/50048]	Loss: 0.1386
Training Epoch: 62 [4992/50048]	Loss: 0.1753
Training Epoch: 62 [5120/50048]	Loss: 0.1848
Training Epoch: 62 [5248/50048]	Loss: 0.1433
Training Epoch: 62 [5376/50048]	Loss: 0.2184
Training Epoch: 62 [5504/50048]	Loss: 0.2053
Training Epoch: 62 [5632/50048]	Loss: 0.1672
Training Epoch: 62 [5760/50048]	Loss: 0.3202
Training Epoch: 62 [5888/50048]	Loss: 0.2151
Training Epoch: 62 [6016/50048]	Loss: 0.1542
Training Epoch: 62 [6144/50048]	Loss: 0.1620
Training Epoch: 62 [6272/50048]	Loss: 0.1753
Training Epoch: 62 [6400/50048]	Loss: 0.2115
Training Epoch: 62 [6528/50048]	Loss: 0.2534
Training Epoch: 62 [6656/50048]	Loss: 0.2571
Training Epoch: 62 [6784/50048]	Loss: 0.1734
Training Epoch: 62 [6912/50048]	Loss: 0.1419
Training Epoch: 62 [7040/50048]	Loss: 0.1122
Training Epoch: 62 [7168/50048]	Loss: 0.1569
Training Epoch: 62 [7296/50048]	Loss: 0.2205
Training Epoch: 62 [7424/50048]	Loss: 0.1059
Training Epoch: 62 [7552/50048]	Loss: 0.1992
Training Epoch: 62 [7680/50048]	Loss: 0.1112
Training Epoch: 62 [7808/50048]	Loss: 0.1766
Training Epoch: 62 [7936/50048]	Loss: 0.1786
Training Epoch: 62 [8064/50048]	Loss: 0.1398
Training Epoch: 62 [8192/50048]	Loss: 0.1989
Training Epoch: 62 [8320/50048]	Loss: 0.1363
Training Epoch: 62 [8448/50048]	Loss: 0.1506
Training Epoch: 62 [8576/50048]	Loss: 0.1775
Training Epoch: 62 [8704/50048]	Loss: 0.1569
Training Epoch: 62 [8832/50048]	Loss: 0.1551
Training Epoch: 62 [8960/50048]	Loss: 0.3039
Training Epoch: 62 [9088/50048]	Loss: 0.1045
Training Epoch: 62 [9216/50048]	Loss: 0.2997
Training Epoch: 62 [9344/50048]	Loss: 0.1703
Training Epoch: 62 [9472/50048]	Loss: 0.0940
Training Epoch: 62 [9600/50048]	Loss: 0.0870
Training Epoch: 62 [9728/50048]	Loss: 0.1973
Training Epoch: 62 [9856/50048]	Loss: 0.1757
Training Epoch: 62 [9984/50048]	Loss: 0.1967
Training Epoch: 62 [10112/50048]	Loss: 0.3053
Training Epoch: 62 [10240/50048]	Loss: 0.1173
Training Epoch: 62 [10368/50048]	Loss: 0.1211
Training Epoch: 62 [10496/50048]	Loss: 0.1583
Training Epoch: 62 [10624/50048]	Loss: 0.2094
Training Epoch: 62 [10752/50048]	Loss: 0.1780
Training Epoch: 62 [10880/50048]	Loss: 0.1705
Training Epoch: 62 [11008/50048]	Loss: 0.3111
Training Epoch: 62 [11136/50048]	Loss: 0.2677
Training Epoch: 62 [11264/50048]	Loss: 0.1554
Training Epoch: 62 [11392/50048]	Loss: 0.2265
Training Epoch: 62 [11520/50048]	Loss: 0.1604
Training Epoch: 62 [11648/50048]	Loss: 0.2787
Training Epoch: 62 [11776/50048]	Loss: 0.1505
Training Epoch: 62 [11904/50048]	Loss: 0.1102
Training Epoch: 62 [12032/50048]	Loss: 0.1865
Training Epoch: 62 [12160/50048]	Loss: 0.2107
Training Epoch: 62 [12288/50048]	Loss: 0.1824
Training Epoch: 62 [12416/50048]	Loss: 0.2367
Training Epoch: 62 [12544/50048]	Loss: 0.1237
Training Epoch: 62 [12672/50048]	Loss: 0.1880
Training Epoch: 62 [12800/50048]	Loss: 0.1180
Training Epoch: 62 [12928/50048]	Loss: 0.2100
Training Epoch: 62 [13056/50048]	Loss: 0.0869
Training Epoch: 62 [13184/50048]	Loss: 0.2743
Training Epoch: 62 [13312/50048]	Loss: 0.1472
Training Epoch: 62 [13440/50048]	Loss: 0.1533
Training Epoch: 62 [13568/50048]	Loss: 0.1824
Training Epoch: 62 [13696/50048]	Loss: 0.1682
Training Epoch: 62 [13824/50048]	Loss: 0.2269
Training Epoch: 62 [13952/50048]	Loss: 0.2668
Training Epoch: 62 [14080/50048]	Loss: 0.1818
Training Epoch: 62 [14208/50048]	Loss: 0.1826
Training Epoch: 62 [14336/50048]	Loss: 0.2324
Training Epoch: 62 [14464/50048]	Loss: 0.2027
Training Epoch: 62 [14592/50048]	Loss: 0.2101
Training Epoch: 62 [14720/50048]	Loss: 0.2792
Training Epoch: 62 [14848/50048]	Loss: 0.1655
Training Epoch: 62 [14976/50048]	Loss: 0.1605
Training Epoch: 62 [15104/50048]	Loss: 0.1157
Training Epoch: 62 [15232/50048]	Loss: 0.2279
Training Epoch: 62 [15360/50048]	Loss: 0.0969
Training Epoch: 62 [15488/50048]	Loss: 0.1266
Training Epoch: 62 [15616/50048]	Loss: 0.1705
Training Epoch: 62 [15744/50048]	Loss: 0.1288
Training Epoch: 62 [15872/50048]	Loss: 0.1350
Training Epoch: 62 [16000/50048]	Loss: 0.1601
Training Epoch: 62 [16128/50048]	Loss: 0.1661
Training Epoch: 62 [16256/50048]	Loss: 0.1731
Training Epoch: 62 [16384/50048]	Loss: 0.3265
Training Epoch: 62 [16512/50048]	Loss: 0.2016
Training Epoch: 62 [16640/50048]	Loss: 0.1305
Training Epoch: 62 [16768/50048]	Loss: 0.2019
Training Epoch: 62 [16896/50048]	Loss: 0.2497
Training Epoch: 62 [17024/50048]	Loss: 0.1742
Training Epoch: 62 [17152/50048]	Loss: 0.1573
Training Epoch: 62 [17280/50048]	Loss: 0.2859
Training Epoch: 62 [17408/50048]	Loss: 0.1077
Training Epoch: 62 [17536/50048]	Loss: 0.1418
Training Epoch: 62 [17664/50048]	Loss: 0.0768
Training Epoch: 62 [17792/50048]	Loss: 0.1727
Training Epoch: 62 [17920/50048]	Loss: 0.1501
Training Epoch: 62 [18048/50048]	Loss: 0.2459
Training Epoch: 62 [18176/50048]	Loss: 0.1228
Training Epoch: 62 [18304/50048]	Loss: 0.1377
Training Epoch: 62 [18432/50048]	Loss: 0.1430
Training Epoch: 62 [18560/50048]	Loss: 0.1946
Training Epoch: 62 [18688/50048]	Loss: 0.2064
Training Epoch: 62 [18816/50048]	Loss: 0.1537
Training Epoch: 62 [18944/50048]	Loss: 0.2171
Training Epoch: 62 [19072/50048]	Loss: 0.2148
Training Epoch: 62 [19200/50048]	Loss: 0.1671
Training Epoch: 62 [19328/50048]	Loss: 0.1020
Training Epoch: 62 [19456/50048]	Loss: 0.2292
Training Epoch: 62 [19584/50048]	Loss: 0.1692
Training Epoch: 62 [19712/50048]	Loss: 0.1978
Training Epoch: 62 [19840/50048]	Loss: 0.2644
Training Epoch: 62 [19968/50048]	Loss: 0.2404
Training Epoch: 62 [20096/50048]	Loss: 0.1514
Training Epoch: 62 [20224/50048]	Loss: 0.2400
Training Epoch: 62 [20352/50048]	Loss: 0.1151
Training Epoch: 62 [20480/50048]	Loss: 0.1645
Training Epoch: 62 [20608/50048]	Loss: 0.1918
Training Epoch: 62 [20736/50048]	Loss: 0.2079
Training Epoch: 62 [20864/50048]	Loss: 0.1407
Training Epoch: 62 [20992/50048]	Loss: 0.0989
Training Epoch: 62 [21120/50048]	Loss: 0.1322
Training Epoch: 62 [21248/50048]	Loss: 0.1353
Training Epoch: 62 [21376/50048]	Loss: 0.1140
Training Epoch: 62 [21504/50048]	Loss: 0.1877
Training Epoch: 62 [21632/50048]	Loss: 0.1288
Training Epoch: 62 [21760/50048]	Loss: 0.1529
Training Epoch: 62 [21888/50048]	Loss: 0.0709
Training Epoch: 62 [22016/50048]	Loss: 0.2582
Training Epoch: 62 [22144/50048]	Loss: 0.2049
Training Epoch: 62 [22272/50048]	Loss: 0.2082
Training Epoch: 62 [22400/50048]	Loss: 0.2242
Training Epoch: 62 [22528/50048]	Loss: 0.1398
Training Epoch: 62 [22656/50048]	Loss: 0.1344
Training Epoch: 62 [22784/50048]	Loss: 0.1691
Training Epoch: 62 [22912/50048]	Loss: 0.2172
Training Epoch: 62 [23040/50048]	Loss: 0.1978
Training Epoch: 62 [23168/50048]	Loss: 0.0872
Training Epoch: 62 [23296/50048]	Loss: 0.0990
Training Epoch: 62 [23424/50048]	Loss: 0.2108
Training Epoch: 62 [23552/50048]	Loss: 0.1397
Training Epoch: 62 [23680/50048]	Loss: 0.1873
Training Epoch: 62 [23808/50048]	Loss: 0.2314
Training Epoch: 62 [23936/50048]	Loss: 0.1129
Training Epoch: 62 [24064/50048]	Loss: 0.2295
Training Epoch: 62 [24192/50048]	Loss: 0.2497
Training Epoch: 62 [24320/50048]	Loss: 0.0951
Training Epoch: 62 [24448/50048]	Loss: 0.1805
Training Epoch: 62 [24576/50048]	Loss: 0.1383
Training Epoch: 62 [24704/50048]	Loss: 0.1894
Training Epoch: 62 [24832/50048]	Loss: 0.1964
Training Epoch: 62 [24960/50048]	Loss: 0.1229
Training Epoch: 62 [25088/50048]	Loss: 0.2523
Training Epoch: 62 [25216/50048]	Loss: 0.1557
Training Epoch: 62 [25344/50048]	Loss: 0.2477
Training Epoch: 62 [25472/50048]	Loss: 0.1574
Training Epoch: 62 [25600/50048]	Loss: 0.3305
Training Epoch: 62 [25728/50048]	Loss: 0.2347
Training Epoch: 62 [25856/50048]	Loss: 0.1318
Training Epoch: 62 [25984/50048]	Loss: 0.1851
Training Epoch: 62 [26112/50048]	Loss: 0.2128
Training Epoch: 62 [26240/50048]	Loss: 0.1522
Training Epoch: 62 [26368/50048]	Loss: 0.2244
Training Epoch: 62 [26496/50048]	Loss: 0.0958
Training Epoch: 62 [26624/50048]	Loss: 0.1791
Training Epoch: 62 [26752/50048]	Loss: 0.1976
Training Epoch: 62 [26880/50048]	Loss: 0.1667
Training Epoch: 62 [27008/50048]	Loss: 0.2382
Training Epoch: 62 [27136/50048]	Loss: 0.1366
Training Epoch: 62 [27264/50048]	Loss: 0.1459
Training Epoch: 62 [27392/50048]	Loss: 0.2423
Training Epoch: 62 [27520/50048]	Loss: 0.2433
Training Epoch: 62 [27648/50048]	Loss: 0.2541
Training Epoch: 62 [27776/50048]	Loss: 0.1398
Training Epoch: 62 [27904/50048]	Loss: 0.2554
Training Epoch: 62 [28032/50048]	Loss: 0.2142
Training Epoch: 62 [28160/50048]	Loss: 0.1462
Training Epoch: 62 [28288/50048]	Loss: 0.1778
Training Epoch: 62 [28416/50048]	Loss: 0.0724
Training Epoch: 62 [28544/50048]	Loss: 0.1172
Training Epoch: 62 [28672/50048]	Loss: 0.1687
Training Epoch: 62 [28800/50048]	Loss: 0.1419
Training Epoch: 62 [28928/50048]	Loss: 0.1923
Training Epoch: 62 [29056/50048]	Loss: 0.1901
Training Epoch: 62 [29184/50048]	Loss: 0.2656
Training Epoch: 62 [29312/50048]	Loss: 0.1573
Training Epoch: 62 [29440/50048]	Loss: 0.3369
Training Epoch: 62 [29568/50048]	Loss: 0.2492
Training Epoch: 62 [29696/50048]	Loss: 0.1315
Training Epoch: 62 [29824/50048]	Loss: 0.1578
Training Epoch: 62 [29952/50048]	Loss: 0.0905
Training Epoch: 62 [30080/50048]	Loss: 0.2976
Training Epoch: 62 [30208/50048]	Loss: 0.1745
Training Epoch: 62 [30336/50048]	Loss: 0.2546
Training Epoch: 62 [30464/50048]	Loss: 0.1775
Training Epoch: 62 [30592/50048]	Loss: 0.1419
Training Epoch: 62 [30720/50048]	Loss: 0.0845
Training Epoch: 62 [30848/50048]	Loss: 0.1996
Training Epoch: 62 [30976/50048]	Loss: 0.2063
Training Epoch: 62 [31104/50048]	Loss: 0.2027
Training Epoch: 62 [31232/50048]	Loss: 0.2685
Training Epoch: 62 [31360/50048]	Loss: 0.3004
Training Epoch: 62 [31488/50048]	Loss: 0.2043
Training Epoch: 62 [31616/50048]	Loss: 0.2504
Training Epoch: 62 [31744/50048]	Loss: 0.2631
Training Epoch: 62 [31872/50048]	Loss: 0.3672
Training Epoch: 62 [32000/50048]	Loss: 0.2068
Training Epoch: 62 [32128/50048]	Loss: 0.2977
Training Epoch: 62 [32256/50048]	Loss: 0.2026
Training Epoch: 62 [32384/50048]	Loss: 0.2358
Training Epoch: 62 [32512/50048]	Loss: 0.1448
Training Epoch: 62 [32640/50048]	Loss: 0.2435
Training Epoch: 62 [32768/50048]	Loss: 0.1677
Training Epoch: 62 [32896/50048]	Loss: 0.1746
Training Epoch: 62 [33024/50048]	Loss: 0.1924
Training Epoch: 62 [33152/50048]	Loss: 0.2378
Training Epoch: 62 [33280/50048]	Loss: 0.2655
Training Epoch: 62 [33408/50048]	Loss: 0.2249
Training Epoch: 62 [33536/50048]	Loss: 0.1828
Training Epoch: 62 [33664/50048]	Loss: 0.1901
Training Epoch: 62 [33792/50048]	Loss: 0.1335
Training Epoch: 62 [33920/50048]	Loss: 0.1513
Training Epoch: 62 [34048/50048]	Loss: 0.1328
Training Epoch: 62 [34176/50048]	Loss: 0.1907
Training Epoch: 62 [34304/50048]	Loss: 0.1117
Training Epoch: 62 [34432/50048]	Loss: 0.2198
Training Epoch: 62 [34560/50048]	Loss: 0.2678
Training Epoch: 62 [34688/50048]	Loss: 0.1796
Training Epoch: 62 [34816/50048]	Loss: 0.2333
Training Epoch: 62 [34944/50048]	Loss: 0.2116
Training Epoch: 62 [35072/50048]	Loss: 0.2385
Training Epoch: 62 [35200/50048]	Loss: 0.2360
Training Epoch: 62 [35328/50048]	Loss: 0.1638
Training Epoch: 62 [35456/50048]	Loss: 0.1619
Training Epoch: 62 [35584/50048]	Loss: 0.1808
Training Epoch: 62 [35712/50048]	Loss: 0.1951
Training Epoch: 62 [35840/50048]	Loss: 0.1085
Training Epoch: 62 [35968/50048]	Loss: 0.2346
Training Epoch: 62 [36096/50048]	Loss: 0.1372
Training Epoch: 62 [36224/50048]	Loss: 0.1892
Training Epoch: 62 [36352/50048]	Loss: 0.2479
Training Epoch: 62 [36480/50048]	Loss: 0.1638
Training Epoch: 62 [36608/50048]	Loss: 0.1979
Training Epoch: 62 [36736/50048]	Loss: 0.1783
Training Epoch: 62 [36864/50048]	Loss: 0.1799
Training Epoch: 62 [36992/50048]	Loss: 0.2657
Training Epoch: 62 [37120/50048]	Loss: 0.1300
Training Epoch: 62 [37248/50048]	Loss: 0.2161
Training Epoch: 62 [37376/50048]	Loss: 0.1865
Training Epoch: 62 [37504/50048]	Loss: 0.2267
Training Epoch: 62 [37632/50048]	Loss: 0.1367
Training Epoch: 62 [37760/50048]	Loss: 0.1395
Training Epoch: 62 [37888/50048]	Loss: 0.2265
Training Epoch: 62 [38016/50048]	Loss: 0.1393
Training Epoch: 62 [38144/50048]	Loss: 0.2833
Training Epoch: 62 [38272/50048]	Loss: 0.1622
Training Epoch: 62 [38400/50048]	Loss: 0.2063
Training Epoch: 62 [38528/50048]	Loss: 0.1114
Training Epoch: 62 [38656/50048]	Loss: 0.2001
Training Epoch: 62 [38784/50048]	Loss: 0.1237
Training Epoch: 62 [38912/50048]	Loss: 0.2443
Training Epoch: 62 [39040/50048]	Loss: 0.2220
Training Epoch: 62 [39168/50048]	Loss: 0.1702
Training Epoch: 62 [39296/50048]	Loss: 0.2058
Training Epoch: 62 [39424/50048]	Loss: 0.0960
Training Epoch: 62 [39552/50048]	Loss: 0.1918
Training Epoch: 62 [39680/50048]	Loss: 0.2359
Training Epoch: 62 [39808/50048]	Loss: 0.2401
Training Epoch: 62 [39936/50048]	Loss: 0.3061
Training Epoch: 62 [40064/50048]	Loss: 0.2104
Training Epoch: 62 [40192/50048]	Loss: 0.1033
Training Epoch: 62 [40320/50048]	Loss: 0.1680
Training Epoch: 62 [40448/50048]	Loss: 0.2712
Training Epoch: 62 [40576/50048]	Loss: 0.2006
Training Epoch: 62 [40704/50048]	Loss: 0.2244
Training Epoch: 62 [40832/50048]	Loss: 0.2359
Training Epoch: 62 [40960/50048]	Loss: 0.1227
Training Epoch: 62 [41088/50048]	Loss: 0.1814
Training Epoch: 62 [41216/50048]	Loss: 0.2731
Training Epoch: 62 [41344/50048]	Loss: 0.1182
Training Epoch: 62 [41472/50048]	Loss: 0.1912
Training Epoch: 62 [41600/50048]	Loss: 0.1335
Training Epoch: 62 [41728/50048]	Loss: 0.1515
Training Epoch: 62 [41856/50048]	Loss: 0.1657
Training Epoch: 62 [41984/50048]	Loss: 0.1422
Training Epoch: 62 [42112/50048]	Loss: 0.2410
Training Epoch: 62 [42240/50048]	Loss: 0.1933
Training Epoch: 62 [42368/50048]	Loss: 0.1919
Training Epoch: 62 [42496/50048]	Loss: 0.3279
Training Epoch: 62 [42624/50048]	Loss: 0.1934
Training Epoch: 62 [42752/50048]	Loss: 0.2834
Training Epoch: 62 [42880/50048]	Loss: 0.2630
Training Epoch: 62 [43008/50048]	Loss: 0.1852
Training Epoch: 62 [43136/50048]	Loss: 0.2145
Training Epoch: 62 [43264/50048]	Loss: 0.1584
Training Epoch: 62 [43392/50048]	Loss: 0.0750
Training Epoch: 62 [43520/50048]	Loss: 0.2503
Training Epoch: 62 [43648/50048]	Loss: 0.1484
Training Epoch: 62 [43776/50048]	Loss: 0.1615
Training Epoch: 62 [43904/50048]	Loss: 0.2675
Training Epoch: 62 [44032/50048]	Loss: 0.1796
Training Epoch: 62 [44160/50048]	Loss: 0.2318
Training Epoch: 62 [44288/50048]	Loss: 0.2061
Training Epoch: 62 [44416/50048]	Loss: 0.1346
Training Epoch: 62 [44544/50048]	Loss: 0.1278
Training Epoch: 62 [44672/50048]	Loss: 0.1231
Training Epoch: 62 [44800/50048]	Loss: 0.2899
Training Epoch: 62 [44928/50048]	Loss: 0.1117
Training Epoch: 62 [45056/50048]	Loss: 0.1031
Training Epoch: 62 [45184/50048]	Loss: 0.1761
Training Epoch: 62 [45312/50048]	Loss: 0.2427
Training Epoch: 62 [45440/50048]	Loss: 0.2034
Training Epoch: 62 [45568/50048]	Loss: 0.2096
Training Epoch: 62 [45696/50048]	Loss: 0.2593
2022-12-06 07:42:33,753 [ZeusDataLoader(train)] train epoch 63 done: time=86.44 energy=10500.09
2022-12-06 07:42:33,755 [ZeusDataLoader(eval)] Epoch 63 begin.
Training Epoch: 62 [45824/50048]	Loss: 0.2049
Training Epoch: 62 [45952/50048]	Loss: 0.2630
Training Epoch: 62 [46080/50048]	Loss: 0.2785
Training Epoch: 62 [46208/50048]	Loss: 0.2507
Training Epoch: 62 [46336/50048]	Loss: 0.1968
Training Epoch: 62 [46464/50048]	Loss: 0.1737
Training Epoch: 62 [46592/50048]	Loss: 0.2089
Training Epoch: 62 [46720/50048]	Loss: 0.2068
Training Epoch: 62 [46848/50048]	Loss: 0.2921
Training Epoch: 62 [46976/50048]	Loss: 0.2371
Training Epoch: 62 [47104/50048]	Loss: 0.1512
Training Epoch: 62 [47232/50048]	Loss: 0.0946
Training Epoch: 62 [47360/50048]	Loss: 0.2395
Training Epoch: 62 [47488/50048]	Loss: 0.3329
Training Epoch: 62 [47616/50048]	Loss: 0.1851
Training Epoch: 62 [47744/50048]	Loss: 0.1763
Training Epoch: 62 [47872/50048]	Loss: 0.3065
Training Epoch: 62 [48000/50048]	Loss: 0.3482
Training Epoch: 62 [48128/50048]	Loss: 0.1938
Training Epoch: 62 [48256/50048]	Loss: 0.2246
Training Epoch: 62 [48384/50048]	Loss: 0.2268
Training Epoch: 62 [48512/50048]	Loss: 0.2546
Training Epoch: 62 [48640/50048]	Loss: 0.1646
Training Epoch: 62 [48768/50048]	Loss: 0.2437
Training Epoch: 62 [48896/50048]	Loss: 0.1727
Training Epoch: 62 [49024/50048]	Loss: 0.1880
Training Epoch: 62 [49152/50048]	Loss: 0.1942
Training Epoch: 62 [49280/50048]	Loss: 0.3539
Training Epoch: 62 [49408/50048]	Loss: 0.2162
Training Epoch: 62 [49536/50048]	Loss: 0.1286
Training Epoch: 62 [49664/50048]	Loss: 0.2408
Training Epoch: 62 [49792/50048]	Loss: 0.2382
Training Epoch: 62 [49920/50048]	Loss: 0.2094
Training Epoch: 62 [50048/50048]	Loss: 0.1169
2022-12-06 12:42:37.436 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:42:37,471 [ZeusDataLoader(eval)] eval epoch 63 done: time=3.71 energy=454.69
2022-12-06 07:42:37,471 [ZeusDataLoader(train)] Up to epoch 63: time=5682.47, energy=689796.68, cost=842114.16
2022-12-06 07:42:37,471 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:42:37,471 [ZeusDataLoader(train)] Expected next epoch: time=5772.27, energy=700594.69, cost=855370.55
2022-12-06 07:42:37,472 [ZeusDataLoader(train)] Epoch 64 begin.
Validation Epoch: 62, Average loss: 0.0164, Accuracy: 0.6290
2022-12-06 07:42:37,660 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:42:37,661 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:42:37.663 [ZeusMonitor] Monitor started.
2022-12-06 12:42:37.663 [ZeusMonitor] Running indefinitely. 2022-12-06 12:42:37.663 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:42:37.663 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e64+gpu0.power.log
Training Epoch: 63 [128/50048]	Loss: 0.2241
Training Epoch: 63 [256/50048]	Loss: 0.1693
Training Epoch: 63 [384/50048]	Loss: 0.1585
Training Epoch: 63 [512/50048]	Loss: 0.1929
Training Epoch: 63 [640/50048]	Loss: 0.1444
Training Epoch: 63 [768/50048]	Loss: 0.2215
Training Epoch: 63 [896/50048]	Loss: 0.1688
Training Epoch: 63 [1024/50048]	Loss: 0.1225
Training Epoch: 63 [1152/50048]	Loss: 0.2324
Training Epoch: 63 [1280/50048]	Loss: 0.1370
Training Epoch: 63 [1408/50048]	Loss: 0.2204
Training Epoch: 63 [1536/50048]	Loss: 0.1320
Training Epoch: 63 [1664/50048]	Loss: 0.1468
Training Epoch: 63 [1792/50048]	Loss: 0.1042
Training Epoch: 63 [1920/50048]	Loss: 0.1772
Training Epoch: 63 [2048/50048]	Loss: 0.2354
Training Epoch: 63 [2176/50048]	Loss: 0.1307
Training Epoch: 63 [2304/50048]	Loss: 0.1245
Training Epoch: 63 [2432/50048]	Loss: 0.1148
Training Epoch: 63 [2560/50048]	Loss: 0.1904
Training Epoch: 63 [2688/50048]	Loss: 0.2019
Training Epoch: 63 [2816/50048]	Loss: 0.1883
Training Epoch: 63 [2944/50048]	Loss: 0.0955
Training Epoch: 63 [3072/50048]	Loss: 0.2060
Training Epoch: 63 [3200/50048]	Loss: 0.1210
Training Epoch: 63 [3328/50048]	Loss: 0.1854
Training Epoch: 63 [3456/50048]	Loss: 0.1910
Training Epoch: 63 [3584/50048]	Loss: 0.1740
Training Epoch: 63 [3712/50048]	Loss: 0.1826
Training Epoch: 63 [3840/50048]	Loss: 0.1451
Training Epoch: 63 [3968/50048]	Loss: 0.1244
Training Epoch: 63 [4096/50048]	Loss: 0.1802
Training Epoch: 63 [4224/50048]	Loss: 0.2248
Training Epoch: 63 [4352/50048]	Loss: 0.1598
Training Epoch: 63 [4480/50048]	Loss: 0.2441
Training Epoch: 63 [4608/50048]	Loss: 0.1607
Training Epoch: 63 [4736/50048]	Loss: 0.1338
Training Epoch: 63 [4864/50048]	Loss: 0.1557
Training Epoch: 63 [4992/50048]	Loss: 0.0891
Training Epoch: 63 [5120/50048]	Loss: 0.2373
Training Epoch: 63 [5248/50048]	Loss: 0.1192
Training Epoch: 63 [5376/50048]	Loss: 0.2443
Training Epoch: 63 [5504/50048]	Loss: 0.1506
Training Epoch: 63 [5632/50048]	Loss: 0.1744
Training Epoch: 63 [5760/50048]	Loss: 0.0703
Training Epoch: 63 [5888/50048]	Loss: 0.0837
Training Epoch: 63 [6016/50048]	Loss: 0.1683
Training Epoch: 63 [6144/50048]	Loss: 0.2098
Training Epoch: 63 [6272/50048]	Loss: 0.1665
Training Epoch: 63 [6400/50048]	Loss: 0.1632
Training Epoch: 63 [6528/50048]	Loss: 0.1706
Training Epoch: 63 [6656/50048]	Loss: 0.1486
Training Epoch: 63 [6784/50048]	Loss: 0.1040
Training Epoch: 63 [6912/50048]	Loss: 0.2523
Training Epoch: 63 [7040/50048]	Loss: 0.1444
Training Epoch: 63 [7168/50048]	Loss: 0.1584
Training Epoch: 63 [7296/50048]	Loss: 0.1149
Training Epoch: 63 [7424/50048]	Loss: 0.1806
Training Epoch: 63 [7552/50048]	Loss: 0.1499
Training Epoch: 63 [7680/50048]	Loss: 0.1948
Training Epoch: 63 [7808/50048]	Loss: 0.2496
Training Epoch: 63 [7936/50048]	Loss: 0.1316
Training Epoch: 63 [8064/50048]	Loss: 0.1375
Training Epoch: 63 [8192/50048]	Loss: 0.1337
Training Epoch: 63 [8320/50048]	Loss: 0.1634
Training Epoch: 63 [8448/50048]	Loss: 0.1234
Training Epoch: 63 [8576/50048]	Loss: 0.2535
Training Epoch: 63 [8704/50048]	Loss: 0.1370
Training Epoch: 63 [8832/50048]	Loss: 0.1430
Training Epoch: 63 [8960/50048]	Loss: 0.1257
Training Epoch: 63 [9088/50048]	Loss: 0.1076
Training Epoch: 63 [9216/50048]	Loss: 0.0566
Training Epoch: 63 [9344/50048]	Loss: 0.1053
Training Epoch: 63 [9472/50048]	Loss: 0.1623
Training Epoch: 63 [9600/50048]	Loss: 0.1786
Training Epoch: 63 [9728/50048]	Loss: 0.1146
Training Epoch: 63 [9856/50048]	Loss: 0.1620
Training Epoch: 63 [9984/50048]	Loss: 0.1450
Training Epoch: 63 [10112/50048]	Loss: 0.1545
Training Epoch: 63 [10240/50048]	Loss: 0.1114
Training Epoch: 63 [10368/50048]	Loss: 0.1548
Training Epoch: 63 [10496/50048]	Loss: 0.1581
Training Epoch: 63 [10624/50048]	Loss: 0.1690
Training Epoch: 63 [10752/50048]	Loss: 0.2072
Training Epoch: 63 [10880/50048]	Loss: 0.1204
Training Epoch: 63 [11008/50048]	Loss: 0.1638
Training Epoch: 63 [11136/50048]	Loss: 0.1373
Training Epoch: 63 [11264/50048]	Loss: 0.1791
Training Epoch: 63 [11392/50048]	Loss: 0.1822
Training Epoch: 63 [11520/50048]	Loss: 0.1589
Training Epoch: 63 [11648/50048]	Loss: 0.1176
Training Epoch: 63 [11776/50048]	Loss: 0.1437
Training Epoch: 63 [11904/50048]	Loss: 0.1607
Training Epoch: 63 [12032/50048]	Loss: 0.1869
Training Epoch: 63 [12160/50048]	Loss: 0.0985
Training Epoch: 63 [12288/50048]	Loss: 0.1500
Training Epoch: 63 [12416/50048]	Loss: 0.1418
Training Epoch: 63 [12544/50048]	Loss: 0.2015
Training Epoch: 63 [12672/50048]	Loss: 0.1071
Training Epoch: 63 [12800/50048]	Loss: 0.1340
Training Epoch: 63 [12928/50048]	Loss: 0.1875
Training Epoch: 63 [13056/50048]	Loss: 0.1627
Training Epoch: 63 [13184/50048]	Loss: 0.1391
Training Epoch: 63 [13312/50048]	Loss: 0.1092
Training Epoch: 63 [13440/50048]	Loss: 0.1239
Training Epoch: 63 [13568/50048]	Loss: 0.1266
Training Epoch: 63 [13696/50048]	Loss: 0.1729
Training Epoch: 63 [13824/50048]	Loss: 0.2433
Training Epoch: 63 [13952/50048]	Loss: 0.1496
Training Epoch: 63 [14080/50048]	Loss: 0.1424
Training Epoch: 63 [14208/50048]	Loss: 0.1854
Training Epoch: 63 [14336/50048]	Loss: 0.1247
Training Epoch: 63 [14464/50048]	Loss: 0.1825
Training Epoch: 63 [14592/50048]	Loss: 0.1634
Training Epoch: 63 [14720/50048]	Loss: 0.1334
Training Epoch: 63 [14848/50048]	Loss: 0.1405
Training Epoch: 63 [14976/50048]	Loss: 0.2238
Training Epoch: 63 [15104/50048]	Loss: 0.1721
Training Epoch: 63 [15232/50048]	Loss: 0.2342
Training Epoch: 63 [15360/50048]	Loss: 0.3074
Training Epoch: 63 [15488/50048]	Loss: 0.1134
Training Epoch: 63 [15616/50048]	Loss: 0.1082
Training Epoch: 63 [15744/50048]	Loss: 0.1140
Training Epoch: 63 [15872/50048]	Loss: 0.1014
Training Epoch: 63 [16000/50048]	Loss: 0.1513
Training Epoch: 63 [16128/50048]	Loss: 0.1994
Training Epoch: 63 [16256/50048]	Loss: 0.1793
Training Epoch: 63 [16384/50048]	Loss: 0.0929
Training Epoch: 63 [16512/50048]	Loss: 0.1045
Training Epoch: 63 [16640/50048]	Loss: 0.1631
Training Epoch: 63 [16768/50048]	Loss: 0.1811
Training Epoch: 63 [16896/50048]	Loss: 0.0849
Training Epoch: 63 [17024/50048]	Loss: 0.2185
Training Epoch: 63 [17152/50048]	Loss: 0.1180
Training Epoch: 63 [17280/50048]	Loss: 0.0969
Training Epoch: 63 [17408/50048]	Loss: 0.1078
Training Epoch: 63 [17536/50048]	Loss: 0.1281
Training Epoch: 63 [17664/50048]	Loss: 0.1205
Training Epoch: 63 [17792/50048]	Loss: 0.2083
Training Epoch: 63 [17920/50048]	Loss: 0.2068
Training Epoch: 63 [18048/50048]	Loss: 0.2949
Training Epoch: 63 [18176/50048]	Loss: 0.2895
Training Epoch: 63 [18304/50048]	Loss: 0.1691
Training Epoch: 63 [18432/50048]	Loss: 0.1695
Training Epoch: 63 [18560/50048]	Loss: 0.1692
Training Epoch: 63 [18688/50048]	Loss: 0.1238
Training Epoch: 63 [18816/50048]	Loss: 0.1523
Training Epoch: 63 [18944/50048]	Loss: 0.1588
Training Epoch: 63 [19072/50048]	Loss: 0.1858
Training Epoch: 63 [19200/50048]	Loss: 0.1345
Training Epoch: 63 [19328/50048]	Loss: 0.1255
Training Epoch: 63 [19456/50048]	Loss: 0.2516
Training Epoch: 63 [19584/50048]	Loss: 0.1169
Training Epoch: 63 [19712/50048]	Loss: 0.0966
Training Epoch: 63 [19840/50048]	Loss: 0.1295
Training Epoch: 63 [19968/50048]	Loss: 0.1538
Training Epoch: 63 [20096/50048]	Loss: 0.2288
Training Epoch: 63 [20224/50048]	Loss: 0.1865
Training Epoch: 63 [20352/50048]	Loss: 0.1872
Training Epoch: 63 [20480/50048]	Loss: 0.1730
Training Epoch: 63 [20608/50048]	Loss: 0.1530
Training Epoch: 63 [20736/50048]	Loss: 0.1530
Training Epoch: 63 [20864/50048]	Loss: 0.1522
Training Epoch: 63 [20992/50048]	Loss: 0.1523
Training Epoch: 63 [21120/50048]	Loss: 0.1040
Training Epoch: 63 [21248/50048]	Loss: 0.1711
Training Epoch: 63 [21376/50048]	Loss: 0.2361
Training Epoch: 63 [21504/50048]	Loss: 0.1406
Training Epoch: 63 [21632/50048]	Loss: 0.1544
Training Epoch: 63 [21760/50048]	Loss: 0.2260
Training Epoch: 63 [21888/50048]	Loss: 0.1384
Training Epoch: 63 [22016/50048]	Loss: 0.2008
Training Epoch: 63 [22144/50048]	Loss: 0.1269
Training Epoch: 63 [22272/50048]	Loss: 0.2191
Training Epoch: 63 [22400/50048]	Loss: 0.2432
Training Epoch: 63 [22528/50048]	Loss: 0.1020
Training Epoch: 63 [22656/50048]	Loss: 0.1371
Training Epoch: 63 [22784/50048]	Loss: 0.1759
Training Epoch: 63 [22912/50048]	Loss: 0.1234
Training Epoch: 63 [23040/50048]	Loss: 0.1464
Training Epoch: 63 [23168/50048]	Loss: 0.1623
Training Epoch: 63 [23296/50048]	Loss: 0.1388
Training Epoch: 63 [23424/50048]	Loss: 0.1836
Training Epoch: 63 [23552/50048]	Loss: 0.2205
Training Epoch: 63 [23680/50048]	Loss: 0.1409
Training Epoch: 63 [23808/50048]	Loss: 0.1666
Training Epoch: 63 [23936/50048]	Loss: 0.1601
Training Epoch: 63 [24064/50048]	Loss: 0.1603
Training Epoch: 63 [24192/50048]	Loss: 0.2231
Training Epoch: 63 [24320/50048]	Loss: 0.2246
Training Epoch: 63 [24448/50048]	Loss: 0.1975
Training Epoch: 63 [24576/50048]	Loss: 0.2477
Training Epoch: 63 [24704/50048]	Loss: 0.1408
Training Epoch: 63 [24832/50048]	Loss: 0.1539
Training Epoch: 63 [24960/50048]	Loss: 0.1941
Training Epoch: 63 [25088/50048]	Loss: 0.1673
Training Epoch: 63 [25216/50048]	Loss: 0.1839
Training Epoch: 63 [25344/50048]	Loss: 0.2936
Training Epoch: 63 [25472/50048]	Loss: 0.1875
Training Epoch: 63 [25600/50048]	Loss: 0.1956
Training Epoch: 63 [25728/50048]	Loss: 0.1312
Training Epoch: 63 [25856/50048]	Loss: 0.2729
Training Epoch: 63 [25984/50048]	Loss: 0.2766
Training Epoch: 63 [26112/50048]	Loss: 0.1515
Training Epoch: 63 [26240/50048]	Loss: 0.1389
Training Epoch: 63 [26368/50048]	Loss: 0.1591
Training Epoch: 63 [26496/50048]	Loss: 0.0936
Training Epoch: 63 [26624/50048]	Loss: 0.2532
Training Epoch: 63 [26752/50048]	Loss: 0.1698
Training Epoch: 63 [26880/50048]	Loss: 0.1245
Training Epoch: 63 [27008/50048]	Loss: 0.1673
Training Epoch: 63 [27136/50048]	Loss: 0.2132
Training Epoch: 63 [27264/50048]	Loss: 0.1181
Training Epoch: 63 [27392/50048]	Loss: 0.2530
Training Epoch: 63 [27520/50048]	Loss: 0.1134
Training Epoch: 63 [27648/50048]	Loss: 0.2321
Training Epoch: 63 [27776/50048]	Loss: 0.1549
Training Epoch: 63 [27904/50048]	Loss: 0.2433
Training Epoch: 63 [28032/50048]	Loss: 0.1010
Training Epoch: 63 [28160/50048]	Loss: 0.1049
Training Epoch: 63 [28288/50048]	Loss: 0.1322
Training Epoch: 63 [28416/50048]	Loss: 0.1791
Training Epoch: 63 [28544/50048]	Loss: 0.1227
Training Epoch: 63 [28672/50048]	Loss: 0.1390
Training Epoch: 63 [28800/50048]	Loss: 0.2469
Training Epoch: 63 [28928/50048]	Loss: 0.1809
Training Epoch: 63 [29056/50048]	Loss: 0.2980
Training Epoch: 63 [29184/50048]	Loss: 0.1638
Training Epoch: 63 [29312/50048]	Loss: 0.1881
Training Epoch: 63 [29440/50048]	Loss: 0.1757
Training Epoch: 63 [29568/50048]	Loss: 0.1959
Training Epoch: 63 [29696/50048]	Loss: 0.2028
Training Epoch: 63 [29824/50048]	Loss: 0.1388
Training Epoch: 63 [29952/50048]	Loss: 0.1441
Training Epoch: 63 [30080/50048]	Loss: 0.1438
Training Epoch: 63 [30208/50048]	Loss: 0.1196
Training Epoch: 63 [30336/50048]	Loss: 0.1254
Training Epoch: 63 [30464/50048]	Loss: 0.2526
Training Epoch: 63 [30592/50048]	Loss: 0.0848
Training Epoch: 63 [30720/50048]	Loss: 0.2383
Training Epoch: 63 [30848/50048]	Loss: 0.2584
Training Epoch: 63 [30976/50048]	Loss: 0.1663
Training Epoch: 63 [31104/50048]	Loss: 0.1431
Training Epoch: 63 [31232/50048]	Loss: 0.0906
Training Epoch: 63 [31360/50048]	Loss: 0.1871
Training Epoch: 63 [31488/50048]	Loss: 0.1596
Training Epoch: 63 [31616/50048]	Loss: 0.2421
Training Epoch: 63 [31744/50048]	Loss: 0.2362
Training Epoch: 63 [31872/50048]	Loss: 0.1665
Training Epoch: 63 [32000/50048]	Loss: 0.2052
Training Epoch: 63 [32128/50048]	Loss: 0.1261
Training Epoch: 63 [32256/50048]	Loss: 0.2023
Training Epoch: 63 [32384/50048]	Loss: 0.2754
Training Epoch: 63 [32512/50048]	Loss: 0.2460
Training Epoch: 63 [32640/50048]	Loss: 0.1660
Training Epoch: 63 [32768/50048]	Loss: 0.1969
Training Epoch: 63 [32896/50048]	Loss: 0.2237
Training Epoch: 63 [33024/50048]	Loss: 0.1816
Training Epoch: 63 [33152/50048]	Loss: 0.1901
Training Epoch: 63 [33280/50048]	Loss: 0.1294
Training Epoch: 63 [33408/50048]	Loss: 0.1055
Training Epoch: 63 [33536/50048]	Loss: 0.1690
Training Epoch: 63 [33664/50048]	Loss: 0.2445
Training Epoch: 63 [33792/50048]	Loss: 0.2166
Training Epoch: 63 [33920/50048]	Loss: 0.1709
Training Epoch: 63 [34048/50048]	Loss: 0.2983
Training Epoch: 63 [34176/50048]	Loss: 0.1566
Training Epoch: 63 [34304/50048]	Loss: 0.2275
Training Epoch: 63 [34432/50048]	Loss: 0.2396
Training Epoch: 63 [34560/50048]	Loss: 0.1405
Training Epoch: 63 [34688/50048]	Loss: 0.2234
Training Epoch: 63 [34816/50048]	Loss: 0.1737
Training Epoch: 63 [34944/50048]	Loss: 0.2450
Training Epoch: 63 [35072/50048]	Loss: 0.1822
Training Epoch: 63 [35200/50048]	Loss: 0.1493
Training Epoch: 63 [35328/50048]	Loss: 0.2658
Training Epoch: 63 [35456/50048]	Loss: 0.2352
Training Epoch: 63 [35584/50048]	Loss: 0.2109
Training Epoch: 63 [35712/50048]	Loss: 0.1824
Training Epoch: 63 [35840/50048]	Loss: 0.1997
Training Epoch: 63 [35968/50048]	Loss: 0.1724
Training Epoch: 63 [36096/50048]	Loss: 0.2383
Training Epoch: 63 [36224/50048]	Loss: 0.1602
Training Epoch: 63 [36352/50048]	Loss: 0.2663
Training Epoch: 63 [36480/50048]	Loss: 0.1479
Training Epoch: 63 [36608/50048]	Loss: 0.1608
Training Epoch: 63 [36736/50048]	Loss: 0.2132
Training Epoch: 63 [36864/50048]	Loss: 0.2021
Training Epoch: 63 [36992/50048]	Loss: 0.1946
Training Epoch: 63 [37120/50048]	Loss: 0.1823
Training Epoch: 63 [37248/50048]	Loss: 0.1868
Training Epoch: 63 [37376/50048]	Loss: 0.2516
Training Epoch: 63 [37504/50048]	Loss: 0.1734
Training Epoch: 63 [37632/50048]	Loss: 0.2515
Training Epoch: 63 [37760/50048]	Loss: 0.1747
Training Epoch: 63 [37888/50048]	Loss: 0.2053
Training Epoch: 63 [38016/50048]	Loss: 0.1956
Training Epoch: 63 [38144/50048]	Loss: 0.2156
Training Epoch: 63 [38272/50048]	Loss: 0.2275
Training Epoch: 63 [38400/50048]	Loss: 0.3393
Training Epoch: 63 [38528/50048]	Loss: 0.2786
Training Epoch: 63 [38656/50048]	Loss: 0.1058
Training Epoch: 63 [38784/50048]	Loss: 0.1491
Training Epoch: 63 [38912/50048]	Loss: 0.2088
Training Epoch: 63 [39040/50048]	Loss: 0.1517
Training Epoch: 63 [39168/50048]	Loss: 0.2675
Training Epoch: 63 [39296/50048]	Loss: 0.2420
Training Epoch: 63 [39424/50048]	Loss: 0.1606
Training Epoch: 63 [39552/50048]	Loss: 0.2256
Training Epoch: 63 [39680/50048]	Loss: 0.2292
Training Epoch: 63 [39808/50048]	Loss: 0.1142
Training Epoch: 63 [39936/50048]	Loss: 0.2125
Training Epoch: 63 [40064/50048]	Loss: 0.2014
Training Epoch: 63 [40192/50048]	Loss: 0.2427
Training Epoch: 63 [40320/50048]	Loss: 0.1648
Training Epoch: 63 [40448/50048]	Loss: 0.1392
Training Epoch: 63 [40576/50048]	Loss: 0.1839
Training Epoch: 63 [40704/50048]	Loss: 0.1932
Training Epoch: 63 [40832/50048]	Loss: 0.1944
Training Epoch: 63 [40960/50048]	Loss: 0.2078
Training Epoch: 63 [41088/50048]	Loss: 0.1662
Training Epoch: 63 [41216/50048]	Loss: 0.2566
Training Epoch: 63 [41344/50048]	Loss: 0.1118
Training Epoch: 63 [41472/50048]	Loss: 0.2006
Training Epoch: 63 [41600/50048]	Loss: 0.1506
Training Epoch: 63 [41728/50048]	Loss: 0.1765
Training Epoch: 63 [41856/50048]	Loss: 0.2330
Training Epoch: 63 [41984/50048]	Loss: 0.2110
Training Epoch: 63 [42112/50048]	Loss: 0.1944
Training Epoch: 63 [42240/50048]	Loss: 0.1661
Training Epoch: 63 [42368/50048]	Loss: 0.2295
Training Epoch: 63 [42496/50048]	Loss: 0.1211
Training Epoch: 63 [42624/50048]	Loss: 0.2203
Training Epoch: 63 [42752/50048]	Loss: 0.2148
Training Epoch: 63 [42880/50048]	Loss: 0.1688
Training Epoch: 63 [43008/50048]	Loss: 0.1842
Training Epoch: 63 [43136/50048]	Loss: 0.2856
Training Epoch: 63 [43264/50048]	Loss: 0.2077
Training Epoch: 63 [43392/50048]	Loss: 0.1451
Training Epoch: 63 [43520/50048]	Loss: 0.1193
Training Epoch: 63 [43648/50048]	Loss: 0.2048
Training Epoch: 63 [43776/50048]	Loss: 0.2434
Training Epoch: 63 [43904/50048]	Loss: 0.1251
Training Epoch: 63 [44032/50048]	Loss: 0.1576
Training Epoch: 63 [44160/50048]	Loss: 0.2048
Training Epoch: 63 [44288/50048]	Loss: 0.1750
Training Epoch: 63 [44416/50048]	Loss: 0.3138
Training Epoch: 63 [44544/50048]	Loss: 0.2118
Training Epoch: 63 [44672/50048]	Loss: 0.1843
Training Epoch: 63 [44800/50048]	Loss: 0.2343
Training Epoch: 63 [44928/50048]	Loss: 0.1710
Training Epoch: 63 [45056/50048]	Loss: 0.1990
Training Epoch: 63 [45184/50048]	Loss: 0.1326
Training Epoch: 63 [45312/50048]	Loss: 0.2047
Training Epoch: 63 [45440/50048]	Loss: 0.1463
Training Epoch: 63 [45568/50048]	Loss: 0.1724
Training Epoch: 63 [45696/50048]	Loss: 0.2308
2022-12-06 07:44:03,980 [ZeusDataLoader(train)] train epoch 64 done: time=86.50 energy=10498.60
2022-12-06 07:44:03,982 [ZeusDataLoader(eval)] Epoch 64 begin.
Training Epoch: 63 [45824/50048]	Loss: 0.1876
Training Epoch: 63 [45952/50048]	Loss: 0.1334
Training Epoch: 63 [46080/50048]	Loss: 0.1513
Training Epoch: 63 [46208/50048]	Loss: 0.1925
Training Epoch: 63 [46336/50048]	Loss: 0.2105
Training Epoch: 63 [46464/50048]	Loss: 0.1710
Training Epoch: 63 [46592/50048]	Loss: 0.1875
Training Epoch: 63 [46720/50048]	Loss: 0.1568
Training Epoch: 63 [46848/50048]	Loss: 0.1184
Training Epoch: 63 [46976/50048]	Loss: 0.2348
Training Epoch: 63 [47104/50048]	Loss: 0.1362
Training Epoch: 63 [47232/50048]	Loss: 0.1819
Training Epoch: 63 [47360/50048]	Loss: 0.1093
Training Epoch: 63 [47488/50048]	Loss: 0.1807
Training Epoch: 63 [47616/50048]	Loss: 0.1435
Training Epoch: 63 [47744/50048]	Loss: 0.1887
Training Epoch: 63 [47872/50048]	Loss: 0.1471
Training Epoch: 63 [48000/50048]	Loss: 0.2021
Training Epoch: 63 [48128/50048]	Loss: 0.1181
Training Epoch: 63 [48256/50048]	Loss: 0.1620
Training Epoch: 63 [48384/50048]	Loss: 0.2406
Training Epoch: 63 [48512/50048]	Loss: 0.2259
Training Epoch: 63 [48640/50048]	Loss: 0.1549
Training Epoch: 63 [48768/50048]	Loss: 0.1844
Training Epoch: 63 [48896/50048]	Loss: 0.2789
Training Epoch: 63 [49024/50048]	Loss: 0.2838
Training Epoch: 63 [49152/50048]	Loss: 0.1947
Training Epoch: 63 [49280/50048]	Loss: 0.2288
Training Epoch: 63 [49408/50048]	Loss: 0.2184
Training Epoch: 63 [49536/50048]	Loss: 0.2542
Training Epoch: 63 [49664/50048]	Loss: 0.1734
Training Epoch: 63 [49792/50048]	Loss: 0.1838
Training Epoch: 63 [49920/50048]	Loss: 0.2476
Training Epoch: 63 [50048/50048]	Loss: 0.1853
2022-12-06 12:44:07.639 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:44:07,661 [ZeusDataLoader(eval)] eval epoch 64 done: time=3.67 energy=439.83
2022-12-06 07:44:07,661 [ZeusDataLoader(train)] Up to epoch 64: time=5772.63, energy=700735.10, cost=855473.10
2022-12-06 07:44:07,662 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:44:07,662 [ZeusDataLoader(train)] Expected next epoch: time=5862.43, energy=711533.12, cost=868729.49
2022-12-06 07:44:07,663 [ZeusDataLoader(train)] Epoch 65 begin.
Validation Epoch: 63, Average loss: 0.0165, Accuracy: 0.6305
2022-12-06 07:44:07,852 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:44:07,853 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:44:07.854 [ZeusMonitor] Monitor started.
2022-12-06 12:44:07.855 [ZeusMonitor] Running indefinitely. 2022-12-06 12:44:07.855 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:44:07.855 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e65+gpu0.power.log
Training Epoch: 64 [128/50048]	Loss: 0.2437
Training Epoch: 64 [256/50048]	Loss: 0.2097
Training Epoch: 64 [384/50048]	Loss: 0.1726
Training Epoch: 64 [512/50048]	Loss: 0.1958
Training Epoch: 64 [640/50048]	Loss: 0.2053
Training Epoch: 64 [768/50048]	Loss: 0.2236
Training Epoch: 64 [896/50048]	Loss: 0.1820
Training Epoch: 64 [1024/50048]	Loss: 0.1184
Training Epoch: 64 [1152/50048]	Loss: 0.1036
Training Epoch: 64 [1280/50048]	Loss: 0.0821
Training Epoch: 64 [1408/50048]	Loss: 0.1370
Training Epoch: 64 [1536/50048]	Loss: 0.2260
Training Epoch: 64 [1664/50048]	Loss: 0.1912
Training Epoch: 64 [1792/50048]	Loss: 0.1496
Training Epoch: 64 [1920/50048]	Loss: 0.1028
Training Epoch: 64 [2048/50048]	Loss: 0.1653
Training Epoch: 64 [2176/50048]	Loss: 0.2005
Training Epoch: 64 [2304/50048]	Loss: 0.1320
Training Epoch: 64 [2432/50048]	Loss: 0.1627
Training Epoch: 64 [2560/50048]	Loss: 0.1467
Training Epoch: 64 [2688/50048]	Loss: 0.1785
Training Epoch: 64 [2816/50048]	Loss: 0.1603
Training Epoch: 64 [2944/50048]	Loss: 0.1363
Training Epoch: 64 [3072/50048]	Loss: 0.1957
Training Epoch: 64 [3200/50048]	Loss: 0.1269
Training Epoch: 64 [3328/50048]	Loss: 0.2308
Training Epoch: 64 [3456/50048]	Loss: 0.1351
Training Epoch: 64 [3584/50048]	Loss: 0.1635
Training Epoch: 64 [3712/50048]	Loss: 0.1808
Training Epoch: 64 [3840/50048]	Loss: 0.1942
Training Epoch: 64 [3968/50048]	Loss: 0.1681
Training Epoch: 64 [4096/50048]	Loss: 0.1649
Training Epoch: 64 [4224/50048]	Loss: 0.1338
Training Epoch: 64 [4352/50048]	Loss: 0.1481
Training Epoch: 64 [4480/50048]	Loss: 0.1636
Training Epoch: 64 [4608/50048]	Loss: 0.1832
Training Epoch: 64 [4736/50048]	Loss: 0.1311
Training Epoch: 64 [4864/50048]	Loss: 0.2325
Training Epoch: 64 [4992/50048]	Loss: 0.2078
Training Epoch: 64 [5120/50048]	Loss: 0.1045
Training Epoch: 64 [5248/50048]	Loss: 0.1627
Training Epoch: 64 [5376/50048]	Loss: 0.1668
Training Epoch: 64 [5504/50048]	Loss: 0.1837
Training Epoch: 64 [5632/50048]	Loss: 0.1130
Training Epoch: 64 [5760/50048]	Loss: 0.2632
Training Epoch: 64 [5888/50048]	Loss: 0.1417
Training Epoch: 64 [6016/50048]	Loss: 0.2015
Training Epoch: 64 [6144/50048]	Loss: 0.1403
Training Epoch: 64 [6272/50048]	Loss: 0.1615
Training Epoch: 64 [6400/50048]	Loss: 0.1400
Training Epoch: 64 [6528/50048]	Loss: 0.2041
Training Epoch: 64 [6656/50048]	Loss: 0.0990
Training Epoch: 64 [6784/50048]	Loss: 0.1267
Training Epoch: 64 [6912/50048]	Loss: 0.1422
Training Epoch: 64 [7040/50048]	Loss: 0.1003
Training Epoch: 64 [7168/50048]	Loss: 0.2273
Training Epoch: 64 [7296/50048]	Loss: 0.1640
Training Epoch: 64 [7424/50048]	Loss: 0.0660
Training Epoch: 64 [7552/50048]	Loss: 0.2073
Training Epoch: 64 [7680/50048]	Loss: 0.2172
Training Epoch: 64 [7808/50048]	Loss: 0.2829
Training Epoch: 64 [7936/50048]	Loss: 0.1589
Training Epoch: 64 [8064/50048]	Loss: 0.1834
Training Epoch: 64 [8192/50048]	Loss: 0.2049
Training Epoch: 64 [8320/50048]	Loss: 0.1532
Training Epoch: 64 [8448/50048]	Loss: 0.1936
Training Epoch: 64 [8576/50048]	Loss: 0.1636
Training Epoch: 64 [8704/50048]	Loss: 0.1468
Training Epoch: 64 [8832/50048]	Loss: 0.1050
Training Epoch: 64 [8960/50048]	Loss: 0.1598
Training Epoch: 64 [9088/50048]	Loss: 0.0691
Training Epoch: 64 [9216/50048]	Loss: 0.1288
Training Epoch: 64 [9344/50048]	Loss: 0.2195
Training Epoch: 64 [9472/50048]	Loss: 0.2030
Training Epoch: 64 [9600/50048]	Loss: 0.2222
Training Epoch: 64 [9728/50048]	Loss: 0.1935
Training Epoch: 64 [9856/50048]	Loss: 0.1933
Training Epoch: 64 [9984/50048]	Loss: 0.1089
Training Epoch: 64 [10112/50048]	Loss: 0.1096
Training Epoch: 64 [10240/50048]	Loss: 0.1690
Training Epoch: 64 [10368/50048]	Loss: 0.1660
Training Epoch: 64 [10496/50048]	Loss: 0.1336
Training Epoch: 64 [10624/50048]	Loss: 0.1220
Training Epoch: 64 [10752/50048]	Loss: 0.1396
Training Epoch: 64 [10880/50048]	Loss: 0.2353
Training Epoch: 64 [11008/50048]	Loss: 0.1465
Training Epoch: 64 [11136/50048]	Loss: 0.1352
Training Epoch: 64 [11264/50048]	Loss: 0.1382
Training Epoch: 64 [11392/50048]	Loss: 0.1117
Training Epoch: 64 [11520/50048]	Loss: 0.0926
Training Epoch: 64 [11648/50048]	Loss: 0.1576
Training Epoch: 64 [11776/50048]	Loss: 0.1524
Training Epoch: 64 [11904/50048]	Loss: 0.1087
Training Epoch: 64 [12032/50048]	Loss: 0.1406
Training Epoch: 64 [12160/50048]	Loss: 0.0721
Training Epoch: 64 [12288/50048]	Loss: 0.1325
Training Epoch: 64 [12416/50048]	Loss: 0.1341
Training Epoch: 64 [12544/50048]	Loss: 0.1810
Training Epoch: 64 [12672/50048]	Loss: 0.1100
Training Epoch: 64 [12800/50048]	Loss: 0.1754
Training Epoch: 64 [12928/50048]	Loss: 0.1977
Training Epoch: 64 [13056/50048]	Loss: 0.2043
Training Epoch: 64 [13184/50048]	Loss: 0.1156
Training Epoch: 64 [13312/50048]	Loss: 0.1950
Training Epoch: 64 [13440/50048]	Loss: 0.1846
Training Epoch: 64 [13568/50048]	Loss: 0.1270
Training Epoch: 64 [13696/50048]	Loss: 0.1728
Training Epoch: 64 [13824/50048]	Loss: 0.0983
Training Epoch: 64 [13952/50048]	Loss: 0.1239
Training Epoch: 64 [14080/50048]	Loss: 0.1353
Training Epoch: 64 [14208/50048]	Loss: 0.1439
Training Epoch: 64 [14336/50048]	Loss: 0.1611
Training Epoch: 64 [14464/50048]	Loss: 0.1721
Training Epoch: 64 [14592/50048]	Loss: 0.1423
Training Epoch: 64 [14720/50048]	Loss: 0.1641
Training Epoch: 64 [14848/50048]	Loss: 0.1402
Training Epoch: 64 [14976/50048]	Loss: 0.2149
Training Epoch: 64 [15104/50048]	Loss: 0.1648
Training Epoch: 64 [15232/50048]	Loss: 0.1301
Training Epoch: 64 [15360/50048]	Loss: 0.2037
Training Epoch: 64 [15488/50048]	Loss: 0.2246
Training Epoch: 64 [15616/50048]	Loss: 0.1135
Training Epoch: 64 [15744/50048]	Loss: 0.1290
Training Epoch: 64 [15872/50048]	Loss: 0.1524
Training Epoch: 64 [16000/50048]	Loss: 0.0964
Training Epoch: 64 [16128/50048]	Loss: 0.1009
Training Epoch: 64 [16256/50048]	Loss: 0.1354
Training Epoch: 64 [16384/50048]	Loss: 0.1174
Training Epoch: 64 [16512/50048]	Loss: 0.1936
Training Epoch: 64 [16640/50048]	Loss: 0.1708
Training Epoch: 64 [16768/50048]	Loss: 0.2263
Training Epoch: 64 [16896/50048]	Loss: 0.1350
Training Epoch: 64 [17024/50048]	Loss: 0.3033
Training Epoch: 64 [17152/50048]	Loss: 0.2348
Training Epoch: 64 [17280/50048]	Loss: 0.1853
Training Epoch: 64 [17408/50048]	Loss: 0.1576
Training Epoch: 64 [17536/50048]	Loss: 0.2189
Training Epoch: 64 [17664/50048]	Loss: 0.2514
Training Epoch: 64 [17792/50048]	Loss: 0.1472
Training Epoch: 64 [17920/50048]	Loss: 0.1260
Training Epoch: 64 [18048/50048]	Loss: 0.2649
Training Epoch: 64 [18176/50048]	Loss: 0.1252
Training Epoch: 64 [18304/50048]	Loss: 0.1108
Training Epoch: 64 [18432/50048]	Loss: 0.2689
Training Epoch: 64 [18560/50048]	Loss: 0.1405
Training Epoch: 64 [18688/50048]	Loss: 0.1998
Training Epoch: 64 [18816/50048]	Loss: 0.2075
Training Epoch: 64 [18944/50048]	Loss: 0.1450
Training Epoch: 64 [19072/50048]	Loss: 0.1594
Training Epoch: 64 [19200/50048]	Loss: 0.1828
Training Epoch: 64 [19328/50048]	Loss: 0.2146
Training Epoch: 64 [19456/50048]	Loss: 0.2275
Training Epoch: 64 [19584/50048]	Loss: 0.1814
Training Epoch: 64 [19712/50048]	Loss: 0.1143
Training Epoch: 64 [19840/50048]	Loss: 0.1816
Training Epoch: 64 [19968/50048]	Loss: 0.1457
Training Epoch: 64 [20096/50048]	Loss: 0.0870
Training Epoch: 64 [20224/50048]	Loss: 0.1089
Training Epoch: 64 [20352/50048]	Loss: 0.1735
Training Epoch: 64 [20480/50048]	Loss: 0.1927
Training Epoch: 64 [20608/50048]	Loss: 0.1113
Training Epoch: 64 [20736/50048]	Loss: 0.1188
Training Epoch: 64 [20864/50048]	Loss: 0.1944
Training Epoch: 64 [20992/50048]	Loss: 0.1960
Training Epoch: 64 [21120/50048]	Loss: 0.2253
Training Epoch: 64 [21248/50048]	Loss: 0.1053
Training Epoch: 64 [21376/50048]	Loss: 0.0672
Training Epoch: 64 [21504/50048]	Loss: 0.2829
Training Epoch: 64 [21632/50048]	Loss: 0.1479
Training Epoch: 64 [21760/50048]	Loss: 0.1482
Training Epoch: 64 [21888/50048]	Loss: 0.0829
Training Epoch: 64 [22016/50048]	Loss: 0.1751
Training Epoch: 64 [22144/50048]	Loss: 0.2806
Training Epoch: 64 [22272/50048]	Loss: 0.2307
Training Epoch: 64 [22400/50048]	Loss: 0.1548
Training Epoch: 64 [22528/50048]	Loss: 0.2596
Training Epoch: 64 [22656/50048]	Loss: 0.1337
Training Epoch: 64 [22784/50048]	Loss: 0.1736
Training Epoch: 64 [22912/50048]	Loss: 0.2238
Training Epoch: 64 [23040/50048]	Loss: 0.1414
Training Epoch: 64 [23168/50048]	Loss: 0.1484
Training Epoch: 64 [23296/50048]	Loss: 0.1039
Training Epoch: 64 [23424/50048]	Loss: 0.2062
Training Epoch: 64 [23552/50048]	Loss: 0.0889
Training Epoch: 64 [23680/50048]	Loss: 0.1496
Training Epoch: 64 [23808/50048]	Loss: 0.1241
Training Epoch: 64 [23936/50048]	Loss: 0.1618
Training Epoch: 64 [24064/50048]	Loss: 0.1245
Training Epoch: 64 [24192/50048]	Loss: 0.2007
Training Epoch: 64 [24320/50048]	Loss: 0.2470
Training Epoch: 64 [24448/50048]	Loss: 0.1645
Training Epoch: 64 [24576/50048]	Loss: 0.2180
Training Epoch: 64 [24704/50048]	Loss: 0.1392
Training Epoch: 64 [24832/50048]	Loss: 0.1643
Training Epoch: 64 [24960/50048]	Loss: 0.0899
Training Epoch: 64 [25088/50048]	Loss: 0.1002
Training Epoch: 64 [25216/50048]	Loss: 0.1321
Training Epoch: 64 [25344/50048]	Loss: 0.2067
Training Epoch: 64 [25472/50048]	Loss: 0.1790
Training Epoch: 64 [25600/50048]	Loss: 0.2098
Training Epoch: 64 [25728/50048]	Loss: 0.2040
Training Epoch: 64 [25856/50048]	Loss: 0.1270
Training Epoch: 64 [25984/50048]	Loss: 0.1578
Training Epoch: 64 [26112/50048]	Loss: 0.2280
Training Epoch: 64 [26240/50048]	Loss: 0.1919
Training Epoch: 64 [26368/50048]	Loss: 0.1375
Training Epoch: 64 [26496/50048]	Loss: 0.2043
Training Epoch: 64 [26624/50048]	Loss: 0.1422
Training Epoch: 64 [26752/50048]	Loss: 0.2424
Training Epoch: 64 [26880/50048]	Loss: 0.1682
Training Epoch: 64 [27008/50048]	Loss: 0.2640
Training Epoch: 64 [27136/50048]	Loss: 0.1916
Training Epoch: 64 [27264/50048]	Loss: 0.2011
Training Epoch: 64 [27392/50048]	Loss: 0.1400
Training Epoch: 64 [27520/50048]	Loss: 0.2262
Training Epoch: 64 [27648/50048]	Loss: 0.1041
Training Epoch: 64 [27776/50048]	Loss: 0.2783
Training Epoch: 64 [27904/50048]	Loss: 0.2711
Training Epoch: 64 [28032/50048]	Loss: 0.1403
Training Epoch: 64 [28160/50048]	Loss: 0.2426
Training Epoch: 64 [28288/50048]	Loss: 0.1330
Training Epoch: 64 [28416/50048]	Loss: 0.2627
Training Epoch: 64 [28544/50048]	Loss: 0.1392
Training Epoch: 64 [28672/50048]	Loss: 0.1753
Training Epoch: 64 [28800/50048]	Loss: 0.1775
Training Epoch: 64 [28928/50048]	Loss: 0.1795
Training Epoch: 64 [29056/50048]	Loss: 0.2134
Training Epoch: 64 [29184/50048]	Loss: 0.1786
Training Epoch: 64 [29312/50048]	Loss: 0.2686
Training Epoch: 64 [29440/50048]	Loss: 0.2417
Training Epoch: 64 [29568/50048]	Loss: 0.1753
Training Epoch: 64 [29696/50048]	Loss: 0.1654
Training Epoch: 64 [29824/50048]	Loss: 0.3358
Training Epoch: 64 [29952/50048]	Loss: 0.1661
Training Epoch: 64 [30080/50048]	Loss: 0.1508
Training Epoch: 64 [30208/50048]	Loss: 0.1414
Training Epoch: 64 [30336/50048]	Loss: 0.2928
Training Epoch: 64 [30464/50048]	Loss: 0.2482
Training Epoch: 64 [30592/50048]	Loss: 0.1111
Training Epoch: 64 [30720/50048]	Loss: 0.2284
Training Epoch: 64 [30848/50048]	Loss: 0.1849
Training Epoch: 64 [30976/50048]	Loss: 0.1163
Training Epoch: 64 [31104/50048]	Loss: 0.2377
Training Epoch: 64 [31232/50048]	Loss: 0.1846
Training Epoch: 64 [31360/50048]	Loss: 0.1770
Training Epoch: 64 [31488/50048]	Loss: 0.1943
Training Epoch: 64 [31616/50048]	Loss: 0.1985
Training Epoch: 64 [31744/50048]	Loss: 0.2017
Training Epoch: 64 [31872/50048]	Loss: 0.1796
Training Epoch: 64 [32000/50048]	Loss: 0.1193
Training Epoch: 64 [32128/50048]	Loss: 0.2284
Training Epoch: 64 [32256/50048]	Loss: 0.1577
Training Epoch: 64 [32384/50048]	Loss: 0.2032
Training Epoch: 64 [32512/50048]	Loss: 0.1268
Training Epoch: 64 [32640/50048]	Loss: 0.1834
Training Epoch: 64 [32768/50048]	Loss: 0.1280
Training Epoch: 64 [32896/50048]	Loss: 0.1309
Training Epoch: 64 [33024/50048]	Loss: 0.1922
Training Epoch: 64 [33152/50048]	Loss: 0.0802
Training Epoch: 64 [33280/50048]	Loss: 0.1753
Training Epoch: 64 [33408/50048]	Loss: 0.1974
Training Epoch: 64 [33536/50048]	Loss: 0.1565
Training Epoch: 64 [33664/50048]	Loss: 0.1409
Training Epoch: 64 [33792/50048]	Loss: 0.2571
Training Epoch: 64 [33920/50048]	Loss: 0.2951
Training Epoch: 64 [34048/50048]	Loss: 0.1879
Training Epoch: 64 [34176/50048]	Loss: 0.1604
Training Epoch: 64 [34304/50048]	Loss: 0.1510
Training Epoch: 64 [34432/50048]	Loss: 0.1074
Training Epoch: 64 [34560/50048]	Loss: 0.2452
Training Epoch: 64 [34688/50048]	Loss: 0.2441
Training Epoch: 64 [34816/50048]	Loss: 0.1196
Training Epoch: 64 [34944/50048]	Loss: 0.1481
Training Epoch: 64 [35072/50048]	Loss: 0.1673
Training Epoch: 64 [35200/50048]	Loss: 0.1021
Training Epoch: 64 [35328/50048]	Loss: 0.1550
Training Epoch: 64 [35456/50048]	Loss: 0.2263
Training Epoch: 64 [35584/50048]	Loss: 0.1756
Training Epoch: 64 [35712/50048]	Loss: 0.1630
Training Epoch: 64 [35840/50048]	Loss: 0.2024
Training Epoch: 64 [35968/50048]	Loss: 0.1871
Training Epoch: 64 [36096/50048]	Loss: 0.2140
Training Epoch: 64 [36224/50048]	Loss: 0.1742
Training Epoch: 64 [36352/50048]	Loss: 0.2149
Training Epoch: 64 [36480/50048]	Loss: 0.2039
Training Epoch: 64 [36608/50048]	Loss: 0.2780
Training Epoch: 64 [36736/50048]	Loss: 0.1316
Training Epoch: 64 [36864/50048]	Loss: 0.1716
Training Epoch: 64 [36992/50048]	Loss: 0.1696
Training Epoch: 64 [37120/50048]	Loss: 0.2190
Training Epoch: 64 [37248/50048]	Loss: 0.2608
Training Epoch: 64 [37376/50048]	Loss: 0.2033
Training Epoch: 64 [37504/50048]	Loss: 0.1697
Training Epoch: 64 [37632/50048]	Loss: 0.1319
Training Epoch: 64 [37760/50048]	Loss: 0.1866
Training Epoch: 64 [37888/50048]	Loss: 0.2795
Training Epoch: 64 [38016/50048]	Loss: 0.2519
Training Epoch: 64 [38144/50048]	Loss: 0.2042
Training Epoch: 64 [38272/50048]	Loss: 0.1672
Training Epoch: 64 [38400/50048]	Loss: 0.1648
Training Epoch: 64 [38528/50048]	Loss: 0.1632
Training Epoch: 64 [38656/50048]	Loss: 0.1520
Training Epoch: 64 [38784/50048]	Loss: 0.1875
Training Epoch: 64 [38912/50048]	Loss: 0.2951
Training Epoch: 64 [39040/50048]	Loss: 0.1814
Training Epoch: 64 [39168/50048]	Loss: 0.2064
Training Epoch: 64 [39296/50048]	Loss: 0.1405
Training Epoch: 64 [39424/50048]	Loss: 0.1549
Training Epoch: 64 [39552/50048]	Loss: 0.1555
Training Epoch: 64 [39680/50048]	Loss: 0.1664
Training Epoch: 64 [39808/50048]	Loss: 0.2598
Training Epoch: 64 [39936/50048]	Loss: 0.1673
Training Epoch: 64 [40064/50048]	Loss: 0.1352
Training Epoch: 64 [40192/50048]	Loss: 0.1876
Training Epoch: 64 [40320/50048]	Loss: 0.1364
Training Epoch: 64 [40448/50048]	Loss: 0.0990
Training Epoch: 64 [40576/50048]	Loss: 0.2001
Training Epoch: 64 [40704/50048]	Loss: 0.2145
Training Epoch: 64 [40832/50048]	Loss: 0.3055
Training Epoch: 64 [40960/50048]	Loss: 0.1647
Training Epoch: 64 [41088/50048]	Loss: 0.1483
Training Epoch: 64 [41216/50048]	Loss: 0.1801
Training Epoch: 64 [41344/50048]	Loss: 0.3549
Training Epoch: 64 [41472/50048]	Loss: 0.2799
Training Epoch: 64 [41600/50048]	Loss: 0.1324
Training Epoch: 64 [41728/50048]	Loss: 0.1988
Training Epoch: 64 [41856/50048]	Loss: 0.1760
Training Epoch: 64 [41984/50048]	Loss: 0.1434
Training Epoch: 64 [42112/50048]	Loss: 0.2352
Training Epoch: 64 [42240/50048]	Loss: 0.2187
Training Epoch: 64 [42368/50048]	Loss: 0.1397
Training Epoch: 64 [42496/50048]	Loss: 0.2969
Training Epoch: 64 [42624/50048]	Loss: 0.1497
Training Epoch: 64 [42752/50048]	Loss: 0.2539
Training Epoch: 64 [42880/50048]	Loss: 0.2392
Training Epoch: 64 [43008/50048]	Loss: 0.1743
Training Epoch: 64 [43136/50048]	Loss: 0.1638
Training Epoch: 64 [43264/50048]	Loss: 0.0875
Training Epoch: 64 [43392/50048]	Loss: 0.2687
Training Epoch: 64 [43520/50048]	Loss: 0.1455
Training Epoch: 64 [43648/50048]	Loss: 0.1992
Training Epoch: 64 [43776/50048]	Loss: 0.1605
Training Epoch: 64 [43904/50048]	Loss: 0.2931
Training Epoch: 64 [44032/50048]	Loss: 0.3678
Training Epoch: 64 [44160/50048]	Loss: 0.2540
Training Epoch: 64 [44288/50048]	Loss: 0.1770
Training Epoch: 64 [44416/50048]	Loss: 0.1615
Training Epoch: 64 [44544/50048]	Loss: 0.2254
Training Epoch: 64 [44672/50048]	Loss: 0.2354
Training Epoch: 64 [44800/50048]	Loss: 0.1949
Training Epoch: 64 [44928/50048]	Loss: 0.2542
Training Epoch: 64 [45056/50048]	Loss: 0.2131
Training Epoch: 64 [45184/50048]	Loss: 0.0930
Training Epoch: 64 [45312/50048]	Loss: 0.1045
Training Epoch: 64 [45440/50048]	Loss: 0.1579
Training Epoch: 64 [45568/50048]	Loss: 0.1143
Training Epoch: 64 [45696/50048]	Loss: 0.2050
2022-12-06 07:45:34,141 [ZeusDataLoader(train)] train epoch 65 done: time=86.47 energy=10491.29
2022-12-06 07:45:34,142 [ZeusDataLoader(eval)] Epoch 65 begin.
Training Epoch: 64 [45824/50048]	Loss: 0.2232
Training Epoch: 64 [45952/50048]	Loss: 0.1776
Training Epoch: 64 [46080/50048]	Loss: 0.1961
Training Epoch: 64 [46208/50048]	Loss: 0.1432
Training Epoch: 64 [46336/50048]	Loss: 0.1852
Training Epoch: 64 [46464/50048]	Loss: 0.2532
Training Epoch: 64 [46592/50048]	Loss: 0.0904
Training Epoch: 64 [46720/50048]	Loss: 0.1526
Training Epoch: 64 [46848/50048]	Loss: 0.1469
Training Epoch: 64 [46976/50048]	Loss: 0.1404
Training Epoch: 64 [47104/50048]	Loss: 0.1576
Training Epoch: 64 [47232/50048]	Loss: 0.1883
Training Epoch: 64 [47360/50048]	Loss: 0.1595
Training Epoch: 64 [47488/50048]	Loss: 0.1693
Training Epoch: 64 [47616/50048]	Loss: 0.1180
Training Epoch: 64 [47744/50048]	Loss: 0.1616
Training Epoch: 64 [47872/50048]	Loss: 0.1388
Training Epoch: 64 [48000/50048]	Loss: 0.1759
Training Epoch: 64 [48128/50048]	Loss: 0.2076
Training Epoch: 64 [48256/50048]	Loss: 0.0960
Training Epoch: 64 [48384/50048]	Loss: 0.2097
Training Epoch: 64 [48512/50048]	Loss: 0.2050
Training Epoch: 64 [48640/50048]	Loss: 0.2434
Training Epoch: 64 [48768/50048]	Loss: 0.2127
Training Epoch: 64 [48896/50048]	Loss: 0.2665
Training Epoch: 64 [49024/50048]	Loss: 0.1370
Training Epoch: 64 [49152/50048]	Loss: 0.1546
Training Epoch: 64 [49280/50048]	Loss: 0.2077
Training Epoch: 64 [49408/50048]	Loss: 0.2152
Training Epoch: 64 [49536/50048]	Loss: 0.1929
Training Epoch: 64 [49664/50048]	Loss: 0.1516
Training Epoch: 64 [49792/50048]	Loss: 0.1008
Training Epoch: 64 [49920/50048]	Loss: 0.2117
Training Epoch: 64 [50048/50048]	Loss: 0.2258
2022-12-06 12:45:37.854 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:45:37,904 [ZeusDataLoader(eval)] eval epoch 65 done: time=3.75 energy=454.27
2022-12-06 07:45:37,904 [ZeusDataLoader(train)] Up to epoch 65: time=5862.86, energy=711680.67, cost=868840.20
2022-12-06 07:45:37,904 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:45:37,904 [ZeusDataLoader(train)] Expected next epoch: time=5952.65, energy=722478.68, cost=882096.59
2022-12-06 07:45:37,905 [ZeusDataLoader(train)] Epoch 66 begin.
Validation Epoch: 64, Average loss: 0.0165, Accuracy: 0.6319
2022-12-06 07:45:38,050 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:45:38,051 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:45:38.054 [ZeusMonitor] Monitor started.
2022-12-06 12:45:38.054 [ZeusMonitor] Running indefinitely. 2022-12-06 12:45:38.054 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:45:38.054 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e66+gpu0.power.log
Training Epoch: 65 [128/50048]	Loss: 0.1156
Training Epoch: 65 [256/50048]	Loss: 0.2862
Training Epoch: 65 [384/50048]	Loss: 0.1031
Training Epoch: 65 [512/50048]	Loss: 0.1984
Training Epoch: 65 [640/50048]	Loss: 0.1474
Training Epoch: 65 [768/50048]	Loss: 0.1081
Training Epoch: 65 [896/50048]	Loss: 0.2025
Training Epoch: 65 [1024/50048]	Loss: 0.0546
Training Epoch: 65 [1152/50048]	Loss: 0.1477
Training Epoch: 65 [1280/50048]	Loss: 0.1442
Training Epoch: 65 [1408/50048]	Loss: 0.1119
Training Epoch: 65 [1536/50048]	Loss: 0.1646
Training Epoch: 65 [1664/50048]	Loss: 0.1302
Training Epoch: 65 [1792/50048]	Loss: 0.1145
Training Epoch: 65 [1920/50048]	Loss: 0.0836
Training Epoch: 65 [2048/50048]	Loss: 0.1663
Training Epoch: 65 [2176/50048]	Loss: 0.1619
Training Epoch: 65 [2304/50048]	Loss: 0.1716
Training Epoch: 65 [2432/50048]	Loss: 0.0893
Training Epoch: 65 [2560/50048]	Loss: 0.1165
Training Epoch: 65 [2688/50048]	Loss: 0.2297
Training Epoch: 65 [2816/50048]	Loss: 0.1624
Training Epoch: 65 [2944/50048]	Loss: 0.1518
Training Epoch: 65 [3072/50048]	Loss: 0.1204
Training Epoch: 65 [3200/50048]	Loss: 0.1697
Training Epoch: 65 [3328/50048]	Loss: 0.0687
Training Epoch: 65 [3456/50048]	Loss: 0.1197
Training Epoch: 65 [3584/50048]	Loss: 0.1729
Training Epoch: 65 [3712/50048]	Loss: 0.2417
Training Epoch: 65 [3840/50048]	Loss: 0.2461
Training Epoch: 65 [3968/50048]	Loss: 0.1235
Training Epoch: 65 [4096/50048]	Loss: 0.2198
Training Epoch: 65 [4224/50048]	Loss: 0.1194
Training Epoch: 65 [4352/50048]	Loss: 0.1774
Training Epoch: 65 [4480/50048]	Loss: 0.1175
Training Epoch: 65 [4608/50048]	Loss: 0.1759
Training Epoch: 65 [4736/50048]	Loss: 0.1258
Training Epoch: 65 [4864/50048]	Loss: 0.0809
Training Epoch: 65 [4992/50048]	Loss: 0.1425
Training Epoch: 65 [5120/50048]	Loss: 0.2147
Training Epoch: 65 [5248/50048]	Loss: 0.1642
Training Epoch: 65 [5376/50048]	Loss: 0.1277
Training Epoch: 65 [5504/50048]	Loss: 0.1329
Training Epoch: 65 [5632/50048]	Loss: 0.1703
Training Epoch: 65 [5760/50048]	Loss: 0.1351
Training Epoch: 65 [5888/50048]	Loss: 0.1162
Training Epoch: 65 [6016/50048]	Loss: 0.1756
Training Epoch: 65 [6144/50048]	Loss: 0.1838
Training Epoch: 65 [6272/50048]	Loss: 0.1553
Training Epoch: 65 [6400/50048]	Loss: 0.1735
Training Epoch: 65 [6528/50048]	Loss: 0.1688
Training Epoch: 65 [6656/50048]	Loss: 0.0583
Training Epoch: 65 [6784/50048]	Loss: 0.1596
Training Epoch: 65 [6912/50048]	Loss: 0.1575
Training Epoch: 65 [7040/50048]	Loss: 0.1699
Training Epoch: 65 [7168/50048]	Loss: 0.1290
Training Epoch: 65 [7296/50048]	Loss: 0.1401
Training Epoch: 65 [7424/50048]	Loss: 0.0825
Training Epoch: 65 [7552/50048]	Loss: 0.1555
Training Epoch: 65 [7680/50048]	Loss: 0.1557
Training Epoch: 65 [7808/50048]	Loss: 0.1278
Training Epoch: 65 [7936/50048]	Loss: 0.1564
Training Epoch: 65 [8064/50048]	Loss: 0.1128
Training Epoch: 65 [8192/50048]	Loss: 0.1795
Training Epoch: 65 [8320/50048]	Loss: 0.1146
Training Epoch: 65 [8448/50048]	Loss: 0.0879
Training Epoch: 65 [8576/50048]	Loss: 0.1570
Training Epoch: 65 [8704/50048]	Loss: 0.1541
Training Epoch: 65 [8832/50048]	Loss: 0.1033
Training Epoch: 65 [8960/50048]	Loss: 0.1404
Training Epoch: 65 [9088/50048]	Loss: 0.1338
Training Epoch: 65 [9216/50048]	Loss: 0.1860
Training Epoch: 65 [9344/50048]	Loss: 0.1426
Training Epoch: 65 [9472/50048]	Loss: 0.1770
Training Epoch: 65 [9600/50048]	Loss: 0.1414
Training Epoch: 65 [9728/50048]	Loss: 0.1573
Training Epoch: 65 [9856/50048]	Loss: 0.1985
Training Epoch: 65 [9984/50048]	Loss: 0.1816
Training Epoch: 65 [10112/50048]	Loss: 0.1804
Training Epoch: 65 [10240/50048]	Loss: 0.1329
Training Epoch: 65 [10368/50048]	Loss: 0.1279
Training Epoch: 65 [10496/50048]	Loss: 0.1431
Training Epoch: 65 [10624/50048]	Loss: 0.0948
Training Epoch: 65 [10752/50048]	Loss: 0.2612
Training Epoch: 65 [10880/50048]	Loss: 0.2039
Training Epoch: 65 [11008/50048]	Loss: 0.1340
Training Epoch: 65 [11136/50048]	Loss: 0.1460
Training Epoch: 65 [11264/50048]	Loss: 0.1716
Training Epoch: 65 [11392/50048]	Loss: 0.1061
Training Epoch: 65 [11520/50048]	Loss: 0.1450
Training Epoch: 65 [11648/50048]	Loss: 0.1107
Training Epoch: 65 [11776/50048]	Loss: 0.1212
Training Epoch: 65 [11904/50048]	Loss: 0.1214
Training Epoch: 65 [12032/50048]	Loss: 0.1459
Training Epoch: 65 [12160/50048]	Loss: 0.1614
Training Epoch: 65 [12288/50048]	Loss: 0.2054
Training Epoch: 65 [12416/50048]	Loss: 0.1737
Training Epoch: 65 [12544/50048]	Loss: 0.1669
Training Epoch: 65 [12672/50048]	Loss: 0.1791
Training Epoch: 65 [12800/50048]	Loss: 0.2118
Training Epoch: 65 [12928/50048]	Loss: 0.1496
Training Epoch: 65 [13056/50048]	Loss: 0.0830
Training Epoch: 65 [13184/50048]	Loss: 0.1479
Training Epoch: 65 [13312/50048]	Loss: 0.1652
Training Epoch: 65 [13440/50048]	Loss: 0.1282
Training Epoch: 65 [13568/50048]	Loss: 0.1027
Training Epoch: 65 [13696/50048]	Loss: 0.0998
Training Epoch: 65 [13824/50048]	Loss: 0.1427
Training Epoch: 65 [13952/50048]	Loss: 0.1608
Training Epoch: 65 [14080/50048]	Loss: 0.1421
Training Epoch: 65 [14208/50048]	Loss: 0.1041
Training Epoch: 65 [14336/50048]	Loss: 0.1769
Training Epoch: 65 [14464/50048]	Loss: 0.1297
Training Epoch: 65 [14592/50048]	Loss: 0.1336
Training Epoch: 65 [14720/50048]	Loss: 0.1606
Training Epoch: 65 [14848/50048]	Loss: 0.2367
Training Epoch: 65 [14976/50048]	Loss: 0.1167
Training Epoch: 65 [15104/50048]	Loss: 0.1477
Training Epoch: 65 [15232/50048]	Loss: 0.1694
Training Epoch: 65 [15360/50048]	Loss: 0.2123
Training Epoch: 65 [15488/50048]	Loss: 0.1470
Training Epoch: 65 [15616/50048]	Loss: 0.1840
Training Epoch: 65 [15744/50048]	Loss: 0.1540
Training Epoch: 65 [15872/50048]	Loss: 0.2230
Training Epoch: 65 [16000/50048]	Loss: 0.1531
Training Epoch: 65 [16128/50048]	Loss: 0.2082
Training Epoch: 65 [16256/50048]	Loss: 0.1374
Training Epoch: 65 [16384/50048]	Loss: 0.1761
Training Epoch: 65 [16512/50048]	Loss: 0.1252
Training Epoch: 65 [16640/50048]	Loss: 0.1353
Training Epoch: 65 [16768/50048]	Loss: 0.1093
Training Epoch: 65 [16896/50048]	Loss: 0.1193
Training Epoch: 65 [17024/50048]	Loss: 0.1224
Training Epoch: 65 [17152/50048]	Loss: 0.1623
Training Epoch: 65 [17280/50048]	Loss: 0.1809
Training Epoch: 65 [17408/50048]	Loss: 0.2319
Training Epoch: 65 [17536/50048]	Loss: 0.1102
Training Epoch: 65 [17664/50048]	Loss: 0.2357
Training Epoch: 65 [17792/50048]	Loss: 0.1697
Training Epoch: 65 [17920/50048]	Loss: 0.1307
Training Epoch: 65 [18048/50048]	Loss: 0.1879
Training Epoch: 65 [18176/50048]	Loss: 0.1119
Training Epoch: 65 [18304/50048]	Loss: 0.1121
Training Epoch: 65 [18432/50048]	Loss: 0.1737
Training Epoch: 65 [18560/50048]	Loss: 0.2555
Training Epoch: 65 [18688/50048]	Loss: 0.2052
Training Epoch: 65 [18816/50048]	Loss: 0.1955
Training Epoch: 65 [18944/50048]	Loss: 0.1427
Training Epoch: 65 [19072/50048]	Loss: 0.1429
Training Epoch: 65 [19200/50048]	Loss: 0.1718
Training Epoch: 65 [19328/50048]	Loss: 0.1715
Training Epoch: 65 [19456/50048]	Loss: 0.1586
Training Epoch: 65 [19584/50048]	Loss: 0.1976
Training Epoch: 65 [19712/50048]	Loss: 0.1573
Training Epoch: 65 [19840/50048]	Loss: 0.2590
Training Epoch: 65 [19968/50048]	Loss: 0.1696
Training Epoch: 65 [20096/50048]	Loss: 0.1812
Training Epoch: 65 [20224/50048]	Loss: 0.1450
Training Epoch: 65 [20352/50048]	Loss: 0.1196
Training Epoch: 65 [20480/50048]	Loss: 0.2536
Training Epoch: 65 [20608/50048]	Loss: 0.1444
Training Epoch: 65 [20736/50048]	Loss: 0.1525
Training Epoch: 65 [20864/50048]	Loss: 0.0965
Training Epoch: 65 [20992/50048]	Loss: 0.2637
Training Epoch: 65 [21120/50048]	Loss: 0.2390
Training Epoch: 65 [21248/50048]	Loss: 0.1519
Training Epoch: 65 [21376/50048]	Loss: 0.1636
Training Epoch: 65 [21504/50048]	Loss: 0.1937
Training Epoch: 65 [21632/50048]	Loss: 0.2155
Training Epoch: 65 [21760/50048]	Loss: 0.1621
Training Epoch: 65 [21888/50048]	Loss: 0.2365
Training Epoch: 65 [22016/50048]	Loss: 0.2706
Training Epoch: 65 [22144/50048]	Loss: 0.1316
Training Epoch: 65 [22272/50048]	Loss: 0.1691
Training Epoch: 65 [22400/50048]	Loss: 0.2469
Training Epoch: 65 [22528/50048]	Loss: 0.1272
Training Epoch: 65 [22656/50048]	Loss: 0.1767
Training Epoch: 65 [22784/50048]	Loss: 0.0916
Training Epoch: 65 [22912/50048]	Loss: 0.2122
Training Epoch: 65 [23040/50048]	Loss: 0.2232
Training Epoch: 65 [23168/50048]	Loss: 0.2345
Training Epoch: 65 [23296/50048]	Loss: 0.2246
Training Epoch: 65 [23424/50048]	Loss: 0.1606
Training Epoch: 65 [23552/50048]	Loss: 0.2360
Training Epoch: 65 [23680/50048]	Loss: 0.1976
Training Epoch: 65 [23808/50048]	Loss: 0.2092
Training Epoch: 65 [23936/50048]	Loss: 0.1295
Training Epoch: 65 [24064/50048]	Loss: 0.1810
Training Epoch: 65 [24192/50048]	Loss: 0.1322
Training Epoch: 65 [24320/50048]	Loss: 0.1750
Training Epoch: 65 [24448/50048]	Loss: 0.1082
Training Epoch: 65 [24576/50048]	Loss: 0.2237
Training Epoch: 65 [24704/50048]	Loss: 0.1108
Training Epoch: 65 [24832/50048]	Loss: 0.1594
Training Epoch: 65 [24960/50048]	Loss: 0.1076
Training Epoch: 65 [25088/50048]	Loss: 0.2074
Training Epoch: 65 [25216/50048]	Loss: 0.2174
Training Epoch: 65 [25344/50048]	Loss: 0.1724
Training Epoch: 65 [25472/50048]	Loss: 0.1381
Training Epoch: 65 [25600/50048]	Loss: 0.1969
Training Epoch: 65 [25728/50048]	Loss: 0.1900
Training Epoch: 65 [25856/50048]	Loss: 0.1556
Training Epoch: 65 [25984/50048]	Loss: 0.1113
Training Epoch: 65 [26112/50048]	Loss: 0.1820
Training Epoch: 65 [26240/50048]	Loss: 0.1685
Training Epoch: 65 [26368/50048]	Loss: 0.1754
Training Epoch: 65 [26496/50048]	Loss: 0.1773
Training Epoch: 65 [26624/50048]	Loss: 0.1795
Training Epoch: 65 [26752/50048]	Loss: 0.2097
Training Epoch: 65 [26880/50048]	Loss: 0.2365
Training Epoch: 65 [27008/50048]	Loss: 0.1411
Training Epoch: 65 [27136/50048]	Loss: 0.1436
Training Epoch: 65 [27264/50048]	Loss: 0.1598
Training Epoch: 65 [27392/50048]	Loss: 0.1126
Training Epoch: 65 [27520/50048]	Loss: 0.1167
Training Epoch: 65 [27648/50048]	Loss: 0.1952
Training Epoch: 65 [27776/50048]	Loss: 0.1373
Training Epoch: 65 [27904/50048]	Loss: 0.1969
Training Epoch: 65 [28032/50048]	Loss: 0.1363
Training Epoch: 65 [28160/50048]	Loss: 0.1784
Training Epoch: 65 [28288/50048]	Loss: 0.1058
Training Epoch: 65 [28416/50048]	Loss: 0.2150
Training Epoch: 65 [28544/50048]	Loss: 0.2050
Training Epoch: 65 [28672/50048]	Loss: 0.1855
Training Epoch: 65 [28800/50048]	Loss: 0.1469
Training Epoch: 65 [28928/50048]	Loss: 0.2138
Training Epoch: 65 [29056/50048]	Loss: 0.2096
Training Epoch: 65 [29184/50048]	Loss: 0.2615
Training Epoch: 65 [29312/50048]	Loss: 0.1441
Training Epoch: 65 [29440/50048]	Loss: 0.1050
Training Epoch: 65 [29568/50048]	Loss: 0.1032
Training Epoch: 65 [29696/50048]	Loss: 0.1139
Training Epoch: 65 [29824/50048]	Loss: 0.1719
Training Epoch: 65 [29952/50048]	Loss: 0.1971
Training Epoch: 65 [30080/50048]	Loss: 0.1613
Training Epoch: 65 [30208/50048]	Loss: 0.1975
Training Epoch: 65 [30336/50048]	Loss: 0.1494
Training Epoch: 65 [30464/50048]	Loss: 0.1507
Training Epoch: 65 [30592/50048]	Loss: 0.1061
Training Epoch: 65 [30720/50048]	Loss: 0.1896
Training Epoch: 65 [30848/50048]	Loss: 0.1730
Training Epoch: 65 [30976/50048]	Loss: 0.1594
Training Epoch: 65 [31104/50048]	Loss: 0.1744
Training Epoch: 65 [31232/50048]	Loss: 0.2956
Training Epoch: 65 [31360/50048]	Loss: 0.1405
Training Epoch: 65 [31488/50048]	Loss: 0.2567
Training Epoch: 65 [31616/50048]	Loss: 0.2764
Training Epoch: 65 [31744/50048]	Loss: 0.1122
Training Epoch: 65 [31872/50048]	Loss: 0.1114
Training Epoch: 65 [32000/50048]	Loss: 0.1514
Training Epoch: 65 [32128/50048]	Loss: 0.1323
Training Epoch: 65 [32256/50048]	Loss: 0.2025
Training Epoch: 65 [32384/50048]	Loss: 0.2324
Training Epoch: 65 [32512/50048]	Loss: 0.2494
Training Epoch: 65 [32640/50048]	Loss: 0.1259
Training Epoch: 65 [32768/50048]	Loss: 0.1802
Training Epoch: 65 [32896/50048]	Loss: 0.1618
Training Epoch: 65 [33024/50048]	Loss: 0.1512
Training Epoch: 65 [33152/50048]	Loss: 0.1051
Training Epoch: 65 [33280/50048]	Loss: 0.2353
Training Epoch: 65 [33408/50048]	Loss: 0.1128
Training Epoch: 65 [33536/50048]	Loss: 0.2689
Training Epoch: 65 [33664/50048]	Loss: 0.2800
Training Epoch: 65 [33792/50048]	Loss: 0.1619
Training Epoch: 65 [33920/50048]	Loss: 0.1872
Training Epoch: 65 [34048/50048]	Loss: 0.2693
Training Epoch: 65 [34176/50048]	Loss: 0.1236
Training Epoch: 65 [34304/50048]	Loss: 0.1093
Training Epoch: 65 [34432/50048]	Loss: 0.1360
Training Epoch: 65 [34560/50048]	Loss: 0.1998
Training Epoch: 65 [34688/50048]	Loss: 0.1227
Training Epoch: 65 [34816/50048]	Loss: 0.2265
Training Epoch: 65 [34944/50048]	Loss: 0.1434
Training Epoch: 65 [35072/50048]	Loss: 0.2445
Training Epoch: 65 [35200/50048]	Loss: 0.1556
Training Epoch: 65 [35328/50048]	Loss: 0.1361
Training Epoch: 65 [35456/50048]	Loss: 0.1929
Training Epoch: 65 [35584/50048]	Loss: 0.1791
Training Epoch: 65 [35712/50048]	Loss: 0.1603
Training Epoch: 65 [35840/50048]	Loss: 0.1729
Training Epoch: 65 [35968/50048]	Loss: 0.1909
Training Epoch: 65 [36096/50048]	Loss: 0.1869
Training Epoch: 65 [36224/50048]	Loss: 0.1921
Training Epoch: 65 [36352/50048]	Loss: 0.1316
Training Epoch: 65 [36480/50048]	Loss: 0.1395
Training Epoch: 65 [36608/50048]	Loss: 0.1705
Training Epoch: 65 [36736/50048]	Loss: 0.1502
Training Epoch: 65 [36864/50048]	Loss: 0.2631
Training Epoch: 65 [36992/50048]	Loss: 0.1964
Training Epoch: 65 [37120/50048]	Loss: 0.1201
Training Epoch: 65 [37248/50048]	Loss: 0.1713
Training Epoch: 65 [37376/50048]	Loss: 0.2098
Training Epoch: 65 [37504/50048]	Loss: 0.1758
Training Epoch: 65 [37632/50048]	Loss: 0.2618
Training Epoch: 65 [37760/50048]	Loss: 0.2317
Training Epoch: 65 [37888/50048]	Loss: 0.1722
Training Epoch: 65 [38016/50048]	Loss: 0.1993
Training Epoch: 65 [38144/50048]	Loss: 0.1731
Training Epoch: 65 [38272/50048]	Loss: 0.2054
Training Epoch: 65 [38400/50048]	Loss: 0.1942
Training Epoch: 65 [38528/50048]	Loss: 0.1930
Training Epoch: 65 [38656/50048]	Loss: 0.2782
Training Epoch: 65 [38784/50048]	Loss: 0.1825
Training Epoch: 65 [38912/50048]	Loss: 0.2679
Training Epoch: 65 [39040/50048]	Loss: 0.1056
Training Epoch: 65 [39168/50048]	Loss: 0.1887
Training Epoch: 65 [39296/50048]	Loss: 0.2157
Training Epoch: 65 [39424/50048]	Loss: 0.1729
Training Epoch: 65 [39552/50048]	Loss: 0.1855
Training Epoch: 65 [39680/50048]	Loss: 0.1473
Training Epoch: 65 [39808/50048]	Loss: 0.2957
Training Epoch: 65 [39936/50048]	Loss: 0.1400
Training Epoch: 65 [40064/50048]	Loss: 0.2293
Training Epoch: 65 [40192/50048]	Loss: 0.1394
Training Epoch: 65 [40320/50048]	Loss: 0.1551
Training Epoch: 65 [40448/50048]	Loss: 0.2123
Training Epoch: 65 [40576/50048]	Loss: 0.2736
Training Epoch: 65 [40704/50048]	Loss: 0.2430
Training Epoch: 65 [40832/50048]	Loss: 0.1580
Training Epoch: 65 [40960/50048]	Loss: 0.1693
Training Epoch: 65 [41088/50048]	Loss: 0.1608
Training Epoch: 65 [41216/50048]	Loss: 0.1079
Training Epoch: 65 [41344/50048]	Loss: 0.1004
Training Epoch: 65 [41472/50048]	Loss: 0.1509
Training Epoch: 65 [41600/50048]	Loss: 0.1812
Training Epoch: 65 [41728/50048]	Loss: 0.1999
Training Epoch: 65 [41856/50048]	Loss: 0.1549
Training Epoch: 65 [41984/50048]	Loss: 0.2252
Training Epoch: 65 [42112/50048]	Loss: 0.1564
Training Epoch: 65 [42240/50048]	Loss: 0.2238
Training Epoch: 65 [42368/50048]	Loss: 0.2013
Training Epoch: 65 [42496/50048]	Loss: 0.1232
Training Epoch: 65 [42624/50048]	Loss: 0.1080
Training Epoch: 65 [42752/50048]	Loss: 0.1994
Training Epoch: 65 [42880/50048]	Loss: 0.2243
Training Epoch: 65 [43008/50048]	Loss: 0.2351
Training Epoch: 65 [43136/50048]	Loss: 0.0976
Training Epoch: 65 [43264/50048]	Loss: 0.1464
Training Epoch: 65 [43392/50048]	Loss: 0.2529
Training Epoch: 65 [43520/50048]	Loss: 0.2157
Training Epoch: 65 [43648/50048]	Loss: 0.1598
Training Epoch: 65 [43776/50048]	Loss: 0.1729
Training Epoch: 65 [43904/50048]	Loss: 0.1930
Training Epoch: 65 [44032/50048]	Loss: 0.2115
Training Epoch: 65 [44160/50048]	Loss: 0.2381
Training Epoch: 65 [44288/50048]	Loss: 0.2696
Training Epoch: 65 [44416/50048]	Loss: 0.1618
Training Epoch: 65 [44544/50048]	Loss: 0.1989
Training Epoch: 65 [44672/50048]	Loss: 0.1655
Training Epoch: 65 [44800/50048]	Loss: 0.0808
Training Epoch: 65 [44928/50048]	Loss: 0.1261
Training Epoch: 65 [45056/50048]	Loss: 0.1436
Training Epoch: 65 [45184/50048]	Loss: 0.1749
Training Epoch: 65 [45312/50048]	Loss: 0.1249
Training Epoch: 65 [45440/50048]	Loss: 0.2758
Training Epoch: 65 [45568/50048]	Loss: 0.1368
Training Epoch: 65 [45696/50048]	Loss: 0.2411
2022-12-06 07:47:04,359 [ZeusDataLoader(train)] train epoch 66 done: time=86.44 energy=10500.96
2022-12-06 07:47:04,361 [ZeusDataLoader(eval)] Epoch 66 begin.
Training Epoch: 65 [45824/50048]	Loss: 0.2196
Training Epoch: 65 [45952/50048]	Loss: 0.2451
Training Epoch: 65 [46080/50048]	Loss: 0.1450
Training Epoch: 65 [46208/50048]	Loss: 0.2440
Training Epoch: 65 [46336/50048]	Loss: 0.1619
Training Epoch: 65 [46464/50048]	Loss: 0.1770
Training Epoch: 65 [46592/50048]	Loss: 0.1147
Training Epoch: 65 [46720/50048]	Loss: 0.1187
Training Epoch: 65 [46848/50048]	Loss: 0.1542
Training Epoch: 65 [46976/50048]	Loss: 0.2388
Training Epoch: 65 [47104/50048]	Loss: 0.2725
Training Epoch: 65 [47232/50048]	Loss: 0.1476
Training Epoch: 65 [47360/50048]	Loss: 0.1552
Training Epoch: 65 [47488/50048]	Loss: 0.1737
Training Epoch: 65 [47616/50048]	Loss: 0.2123
Training Epoch: 65 [47744/50048]	Loss: 0.2325
Training Epoch: 65 [47872/50048]	Loss: 0.3407
Training Epoch: 65 [48000/50048]	Loss: 0.2516
Training Epoch: 65 [48128/50048]	Loss: 0.1636
Training Epoch: 65 [48256/50048]	Loss: 0.1750
Training Epoch: 65 [48384/50048]	Loss: 0.2306
Training Epoch: 65 [48512/50048]	Loss: 0.2212
Training Epoch: 65 [48640/50048]	Loss: 0.1761
Training Epoch: 65 [48768/50048]	Loss: 0.2710
Training Epoch: 65 [48896/50048]	Loss: 0.1979
Training Epoch: 65 [49024/50048]	Loss: 0.2616
Training Epoch: 65 [49152/50048]	Loss: 0.1684
Training Epoch: 65 [49280/50048]	Loss: 0.2345
Training Epoch: 65 [49408/50048]	Loss: 0.1163
Training Epoch: 65 [49536/50048]	Loss: 0.1555
Training Epoch: 65 [49664/50048]	Loss: 0.1058
Training Epoch: 65 [49792/50048]	Loss: 0.1862
Training Epoch: 65 [49920/50048]	Loss: 0.1716
Training Epoch: 65 [50048/50048]	Loss: 0.2306
2022-12-06 12:47:08.013 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:47:08,039 [ZeusDataLoader(eval)] eval epoch 66 done: time=3.67 energy=442.73
2022-12-06 07:47:08,039 [ZeusDataLoader(train)] Up to epoch 66: time=5952.97, energy=722624.35, cost=882196.95
2022-12-06 07:47:08,039 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:47:08,039 [ZeusDataLoader(train)] Expected next epoch: time=6042.77, energy=733422.37, cost=895453.33
2022-12-06 07:47:08,040 [ZeusDataLoader(train)] Epoch 67 begin.
Validation Epoch: 65, Average loss: 0.0165, Accuracy: 0.6385
2022-12-06 07:47:08,229 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:47:08,230 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:47:08.233 [ZeusMonitor] Monitor started.
2022-12-06 12:47:08.233 [ZeusMonitor] Running indefinitely. 2022-12-06 12:47:08.233 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:47:08.233 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e67+gpu0.power.log
Training Epoch: 66 [128/50048]	Loss: 0.1093
Training Epoch: 66 [256/50048]	Loss: 0.1383
Training Epoch: 66 [384/50048]	Loss: 0.1123
Training Epoch: 66 [512/50048]	Loss: 0.1856
Training Epoch: 66 [640/50048]	Loss: 0.1432
Training Epoch: 66 [768/50048]	Loss: 0.1300
Training Epoch: 66 [896/50048]	Loss: 0.1069
Training Epoch: 66 [1024/50048]	Loss: 0.1074
Training Epoch: 66 [1152/50048]	Loss: 0.0841
Training Epoch: 66 [1280/50048]	Loss: 0.1259
Training Epoch: 66 [1408/50048]	Loss: 0.1132
Training Epoch: 66 [1536/50048]	Loss: 0.0933
Training Epoch: 66 [1664/50048]	Loss: 0.0477
Training Epoch: 66 [1792/50048]	Loss: 0.1131
Training Epoch: 66 [1920/50048]	Loss: 0.1044
Training Epoch: 66 [2048/50048]	Loss: 0.0901
Training Epoch: 66 [2176/50048]	Loss: 0.1187
Training Epoch: 66 [2304/50048]	Loss: 0.1017
Training Epoch: 66 [2432/50048]	Loss: 0.1116
Training Epoch: 66 [2560/50048]	Loss: 0.2215
Training Epoch: 66 [2688/50048]	Loss: 0.1335
Training Epoch: 66 [2816/50048]	Loss: 0.1424
Training Epoch: 66 [2944/50048]	Loss: 0.1454
Training Epoch: 66 [3072/50048]	Loss: 0.1076
Training Epoch: 66 [3200/50048]	Loss: 0.1075
Training Epoch: 66 [3328/50048]	Loss: 0.1081
Training Epoch: 66 [3456/50048]	Loss: 0.1620
Training Epoch: 66 [3584/50048]	Loss: 0.1531
Training Epoch: 66 [3712/50048]	Loss: 0.0950
Training Epoch: 66 [3840/50048]	Loss: 0.1181
Training Epoch: 66 [3968/50048]	Loss: 0.1335
Training Epoch: 66 [4096/50048]	Loss: 0.1358
Training Epoch: 66 [4224/50048]	Loss: 0.1494
Training Epoch: 66 [4352/50048]	Loss: 0.1307
Training Epoch: 66 [4480/50048]	Loss: 0.1824
Training Epoch: 66 [4608/50048]	Loss: 0.1232
Training Epoch: 66 [4736/50048]	Loss: 0.1902
Training Epoch: 66 [4864/50048]	Loss: 0.1394
Training Epoch: 66 [4992/50048]	Loss: 0.1893
Training Epoch: 66 [5120/50048]	Loss: 0.1413
Training Epoch: 66 [5248/50048]	Loss: 0.1136
Training Epoch: 66 [5376/50048]	Loss: 0.0971
Training Epoch: 66 [5504/50048]	Loss: 0.0995
Training Epoch: 66 [5632/50048]	Loss: 0.1104
Training Epoch: 66 [5760/50048]	Loss: 0.1022
Training Epoch: 66 [5888/50048]	Loss: 0.1399
Training Epoch: 66 [6016/50048]	Loss: 0.1489
Training Epoch: 66 [6144/50048]	Loss: 0.0747
Training Epoch: 66 [6272/50048]	Loss: 0.0989
Training Epoch: 66 [6400/50048]	Loss: 0.1236
Training Epoch: 66 [6528/50048]	Loss: 0.1402
Training Epoch: 66 [6656/50048]	Loss: 0.0894
Training Epoch: 66 [6784/50048]	Loss: 0.1389
Training Epoch: 66 [6912/50048]	Loss: 0.1098
Training Epoch: 66 [7040/50048]	Loss: 0.1335
Training Epoch: 66 [7168/50048]	Loss: 0.1877
Training Epoch: 66 [7296/50048]	Loss: 0.1613
Training Epoch: 66 [7424/50048]	Loss: 0.1301
Training Epoch: 66 [7552/50048]	Loss: 0.1904
Training Epoch: 66 [7680/50048]	Loss: 0.1111
Training Epoch: 66 [7808/50048]	Loss: 0.1490
Training Epoch: 66 [7936/50048]	Loss: 0.1721
Training Epoch: 66 [8064/50048]	Loss: 0.1819
Training Epoch: 66 [8192/50048]	Loss: 0.1068
Training Epoch: 66 [8320/50048]	Loss: 0.0596
Training Epoch: 66 [8448/50048]	Loss: 0.1809
Training Epoch: 66 [8576/50048]	Loss: 0.1130
Training Epoch: 66 [8704/50048]	Loss: 0.1804
Training Epoch: 66 [8832/50048]	Loss: 0.0869
Training Epoch: 66 [8960/50048]	Loss: 0.1186
Training Epoch: 66 [9088/50048]	Loss: 0.1585
Training Epoch: 66 [9216/50048]	Loss: 0.1498
Training Epoch: 66 [9344/50048]	Loss: 0.0802
Training Epoch: 66 [9472/50048]	Loss: 0.2118
Training Epoch: 66 [9600/50048]	Loss: 0.2602
Training Epoch: 66 [9728/50048]	Loss: 0.1559
Training Epoch: 66 [9856/50048]	Loss: 0.2107
Training Epoch: 66 [9984/50048]	Loss: 0.1086
Training Epoch: 66 [10112/50048]	Loss: 0.1369
Training Epoch: 66 [10240/50048]	Loss: 0.0482
Training Epoch: 66 [10368/50048]	Loss: 0.2274
Training Epoch: 66 [10496/50048]	Loss: 0.1049
Training Epoch: 66 [10624/50048]	Loss: 0.0995
Training Epoch: 66 [10752/50048]	Loss: 0.0902
Training Epoch: 66 [10880/50048]	Loss: 0.1505
Training Epoch: 66 [11008/50048]	Loss: 0.1166
Training Epoch: 66 [11136/50048]	Loss: 0.0757
Training Epoch: 66 [11264/50048]	Loss: 0.1144
Training Epoch: 66 [11392/50048]	Loss: 0.1796
Training Epoch: 66 [11520/50048]	Loss: 0.1655
Training Epoch: 66 [11648/50048]	Loss: 0.0673
Training Epoch: 66 [11776/50048]	Loss: 0.1516
Training Epoch: 66 [11904/50048]	Loss: 0.1753
Training Epoch: 66 [12032/50048]	Loss: 0.1452
Training Epoch: 66 [12160/50048]	Loss: 0.1615
Training Epoch: 66 [12288/50048]	Loss: 0.1305
Training Epoch: 66 [12416/50048]	Loss: 0.1154
Training Epoch: 66 [12544/50048]	Loss: 0.1309
Training Epoch: 66 [12672/50048]	Loss: 0.1508
Training Epoch: 66 [12800/50048]	Loss: 0.1396
Training Epoch: 66 [12928/50048]	Loss: 0.1167
Training Epoch: 66 [13056/50048]	Loss: 0.0992
Training Epoch: 66 [13184/50048]	Loss: 0.0668
Training Epoch: 66 [13312/50048]	Loss: 0.1214
Training Epoch: 66 [13440/50048]	Loss: 0.1511
Training Epoch: 66 [13568/50048]	Loss: 0.1331
Training Epoch: 66 [13696/50048]	Loss: 0.1269
Training Epoch: 66 [13824/50048]	Loss: 0.1884
Training Epoch: 66 [13952/50048]	Loss: 0.2224
Training Epoch: 66 [14080/50048]	Loss: 0.2332
Training Epoch: 66 [14208/50048]	Loss: 0.1425
Training Epoch: 66 [14336/50048]	Loss: 0.1134
Training Epoch: 66 [14464/50048]	Loss: 0.1594
Training Epoch: 66 [14592/50048]	Loss: 0.2092
Training Epoch: 66 [14720/50048]	Loss: 0.1229
Training Epoch: 66 [14848/50048]	Loss: 0.1432
Training Epoch: 66 [14976/50048]	Loss: 0.1751
Training Epoch: 66 [15104/50048]	Loss: 0.0746
Training Epoch: 66 [15232/50048]	Loss: 0.1603
Training Epoch: 66 [15360/50048]	Loss: 0.1461
Training Epoch: 66 [15488/50048]	Loss: 0.0853
Training Epoch: 66 [15616/50048]	Loss: 0.1157
Training Epoch: 66 [15744/50048]	Loss: 0.2027
Training Epoch: 66 [15872/50048]	Loss: 0.1119
Training Epoch: 66 [16000/50048]	Loss: 0.1342
Training Epoch: 66 [16128/50048]	Loss: 0.1934
Training Epoch: 66 [16256/50048]	Loss: 0.0934
Training Epoch: 66 [16384/50048]	Loss: 0.1183
Training Epoch: 66 [16512/50048]	Loss: 0.2497
Training Epoch: 66 [16640/50048]	Loss: 0.0737
Training Epoch: 66 [16768/50048]	Loss: 0.2007
Training Epoch: 66 [16896/50048]	Loss: 0.1747
Training Epoch: 66 [17024/50048]	Loss: 0.2366
Training Epoch: 66 [17152/50048]	Loss: 0.2117
Training Epoch: 66 [17280/50048]	Loss: 0.1567
Training Epoch: 66 [17408/50048]	Loss: 0.2323
Training Epoch: 66 [17536/50048]	Loss: 0.2573
Training Epoch: 66 [17664/50048]	Loss: 0.1810
Training Epoch: 66 [17792/50048]	Loss: 0.1787
Training Epoch: 66 [17920/50048]	Loss: 0.1312
Training Epoch: 66 [18048/50048]	Loss: 0.1001
Training Epoch: 66 [18176/50048]	Loss: 0.2300
Training Epoch: 66 [18304/50048]	Loss: 0.1027
Training Epoch: 66 [18432/50048]	Loss: 0.1252
Training Epoch: 66 [18560/50048]	Loss: 0.1717
Training Epoch: 66 [18688/50048]	Loss: 0.1860
Training Epoch: 66 [18816/50048]	Loss: 0.0882
Training Epoch: 66 [18944/50048]	Loss: 0.1488
Training Epoch: 66 [19072/50048]	Loss: 0.1723
Training Epoch: 66 [19200/50048]	Loss: 0.2659
Training Epoch: 66 [19328/50048]	Loss: 0.0868
Training Epoch: 66 [19456/50048]	Loss: 0.1812
Training Epoch: 66 [19584/50048]	Loss: 0.2634
Training Epoch: 66 [19712/50048]	Loss: 0.1631
Training Epoch: 66 [19840/50048]	Loss: 0.2433
Training Epoch: 66 [19968/50048]	Loss: 0.0990
Training Epoch: 66 [20096/50048]	Loss: 0.1593
Training Epoch: 66 [20224/50048]	Loss: 0.1404
Training Epoch: 66 [20352/50048]	Loss: 0.1709
Training Epoch: 66 [20480/50048]	Loss: 0.1043
Training Epoch: 66 [20608/50048]	Loss: 0.2034
Training Epoch: 66 [20736/50048]	Loss: 0.1848
Training Epoch: 66 [20864/50048]	Loss: 0.1855
Training Epoch: 66 [20992/50048]	Loss: 0.1744
Training Epoch: 66 [21120/50048]	Loss: 0.0829
Training Epoch: 66 [21248/50048]	Loss: 0.1197
Training Epoch: 66 [21376/50048]	Loss: 0.0884
Training Epoch: 66 [21504/50048]	Loss: 0.1168
Training Epoch: 66 [21632/50048]	Loss: 0.1502
Training Epoch: 66 [21760/50048]	Loss: 0.1268
Training Epoch: 66 [21888/50048]	Loss: 0.1652
Training Epoch: 66 [22016/50048]	Loss: 0.1692
Training Epoch: 66 [22144/50048]	Loss: 0.1566
Training Epoch: 66 [22272/50048]	Loss: 0.1507
Training Epoch: 66 [22400/50048]	Loss: 0.1330
Training Epoch: 66 [22528/50048]	Loss: 0.1103
Training Epoch: 66 [22656/50048]	Loss: 0.1951
Training Epoch: 66 [22784/50048]	Loss: 0.2189
Training Epoch: 66 [22912/50048]	Loss: 0.1316
Training Epoch: 66 [23040/50048]	Loss: 0.2291
Training Epoch: 66 [23168/50048]	Loss: 0.1936
Training Epoch: 66 [23296/50048]	Loss: 0.2144
Training Epoch: 66 [23424/50048]	Loss: 0.2314
Training Epoch: 66 [23552/50048]	Loss: 0.2144
Training Epoch: 66 [23680/50048]	Loss: 0.2025
Training Epoch: 66 [23808/50048]	Loss: 0.2169
Training Epoch: 66 [23936/50048]	Loss: 0.0743
Training Epoch: 66 [24064/50048]	Loss: 0.2104
Training Epoch: 66 [24192/50048]	Loss: 0.2103
Training Epoch: 66 [24320/50048]	Loss: 0.2730
Training Epoch: 66 [24448/50048]	Loss: 0.1810
Training Epoch: 66 [24576/50048]	Loss: 0.0938
Training Epoch: 66 [24704/50048]	Loss: 0.1245
Training Epoch: 66 [24832/50048]	Loss: 0.1252
Training Epoch: 66 [24960/50048]	Loss: 0.1457
Training Epoch: 66 [25088/50048]	Loss: 0.1377
Training Epoch: 66 [25216/50048]	Loss: 0.1614
Training Epoch: 66 [25344/50048]	Loss: 0.1467
Training Epoch: 66 [25472/50048]	Loss: 0.1123
Training Epoch: 66 [25600/50048]	Loss: 0.1597
Training Epoch: 66 [25728/50048]	Loss: 0.1469
Training Epoch: 66 [25856/50048]	Loss: 0.1640
Training Epoch: 66 [25984/50048]	Loss: 0.2058
Training Epoch: 66 [26112/50048]	Loss: 0.2683
Training Epoch: 66 [26240/50048]	Loss: 0.1698
Training Epoch: 66 [26368/50048]	Loss: 0.1136
Training Epoch: 66 [26496/50048]	Loss: 0.2029
Training Epoch: 66 [26624/50048]	Loss: 0.1856
Training Epoch: 66 [26752/50048]	Loss: 0.1697
Training Epoch: 66 [26880/50048]	Loss: 0.1866
Training Epoch: 66 [27008/50048]	Loss: 0.1576
Training Epoch: 66 [27136/50048]	Loss: 0.2443
Training Epoch: 66 [27264/50048]	Loss: 0.1213
Training Epoch: 66 [27392/50048]	Loss: 0.1033
Training Epoch: 66 [27520/50048]	Loss: 0.1781
Training Epoch: 66 [27648/50048]	Loss: 0.1387
Training Epoch: 66 [27776/50048]	Loss: 0.1946
Training Epoch: 66 [27904/50048]	Loss: 0.1583
Training Epoch: 66 [28032/50048]	Loss: 0.1350
Training Epoch: 66 [28160/50048]	Loss: 0.2084
Training Epoch: 66 [28288/50048]	Loss: 0.1729
Training Epoch: 66 [28416/50048]	Loss: 0.1430
Training Epoch: 66 [28544/50048]	Loss: 0.2445
Training Epoch: 66 [28672/50048]	Loss: 0.1669
Training Epoch: 66 [28800/50048]	Loss: 0.1128
Training Epoch: 66 [28928/50048]	Loss: 0.1993
Training Epoch: 66 [29056/50048]	Loss: 0.1402
Training Epoch: 66 [29184/50048]	Loss: 0.1226
Training Epoch: 66 [29312/50048]	Loss: 0.1499
Training Epoch: 66 [29440/50048]	Loss: 0.1916
Training Epoch: 66 [29568/50048]	Loss: 0.1968
Training Epoch: 66 [29696/50048]	Loss: 0.2064
Training Epoch: 66 [29824/50048]	Loss: 0.1802
Training Epoch: 66 [29952/50048]	Loss: 0.1548
Training Epoch: 66 [30080/50048]	Loss: 0.1467
Training Epoch: 66 [30208/50048]	Loss: 0.1866
Training Epoch: 66 [30336/50048]	Loss: 0.1613
Training Epoch: 66 [30464/50048]	Loss: 0.1378
Training Epoch: 66 [30592/50048]	Loss: 0.0967
Training Epoch: 66 [30720/50048]	Loss: 0.1942
Training Epoch: 66 [30848/50048]	Loss: 0.2256
Training Epoch: 66 [30976/50048]	Loss: 0.1354
Training Epoch: 66 [31104/50048]	Loss: 0.1448
Training Epoch: 66 [31232/50048]	Loss: 0.1190
Training Epoch: 66 [31360/50048]	Loss: 0.1666
Training Epoch: 66 [31488/50048]	Loss: 0.2259
Training Epoch: 66 [31616/50048]	Loss: 0.2257
Training Epoch: 66 [31744/50048]	Loss: 0.1550
Training Epoch: 66 [31872/50048]	Loss: 0.1039
Training Epoch: 66 [32000/50048]	Loss: 0.2329
Training Epoch: 66 [32128/50048]	Loss: 0.2509
Training Epoch: 66 [32256/50048]	Loss: 0.2067
Training Epoch: 66 [32384/50048]	Loss: 0.1792
Training Epoch: 66 [32512/50048]	Loss: 0.1265
Training Epoch: 66 [32640/50048]	Loss: 0.1934
Training Epoch: 66 [32768/50048]	Loss: 0.2050
Training Epoch: 66 [32896/50048]	Loss: 0.1201
Training Epoch: 66 [33024/50048]	Loss: 0.0910
Training Epoch: 66 [33152/50048]	Loss: 0.0712
Training Epoch: 66 [33280/50048]	Loss: 0.3106
Training Epoch: 66 [33408/50048]	Loss: 0.2191
Training Epoch: 66 [33536/50048]	Loss: 0.1477
Training Epoch: 66 [33664/50048]	Loss: 0.1976
Training Epoch: 66 [33792/50048]	Loss: 0.1954
Training Epoch: 66 [33920/50048]	Loss: 0.1691
Training Epoch: 66 [34048/50048]	Loss: 0.1228
Training Epoch: 66 [34176/50048]	Loss: 0.1969
Training Epoch: 66 [34304/50048]	Loss: 0.2282
Training Epoch: 66 [34432/50048]	Loss: 0.1158
Training Epoch: 66 [34560/50048]	Loss: 0.1277
Training Epoch: 66 [34688/50048]	Loss: 0.3195
Training Epoch: 66 [34816/50048]	Loss: 0.1918
Training Epoch: 66 [34944/50048]	Loss: 0.2165
Training Epoch: 66 [35072/50048]	Loss: 0.2014
Training Epoch: 66 [35200/50048]	Loss: 0.1240
Training Epoch: 66 [35328/50048]	Loss: 0.2012
Training Epoch: 66 [35456/50048]	Loss: 0.1574
Training Epoch: 66 [35584/50048]	Loss: 0.1803
Training Epoch: 66 [35712/50048]	Loss: 0.1758
Training Epoch: 66 [35840/50048]	Loss: 0.2798
Training Epoch: 66 [35968/50048]	Loss: 0.1550
Training Epoch: 66 [36096/50048]	Loss: 0.2345
Training Epoch: 66 [36224/50048]	Loss: 0.1773
Training Epoch: 66 [36352/50048]	Loss: 0.2098
Training Epoch: 66 [36480/50048]	Loss: 0.2159
Training Epoch: 66 [36608/50048]	Loss: 0.1913
Training Epoch: 66 [36736/50048]	Loss: 0.2428
Training Epoch: 66 [36864/50048]	Loss: 0.1948
Training Epoch: 66 [36992/50048]	Loss: 0.1887
Training Epoch: 66 [37120/50048]	Loss: 0.2171
Training Epoch: 66 [37248/50048]	Loss: 0.2074
Training Epoch: 66 [37376/50048]	Loss: 0.2385
Training Epoch: 66 [37504/50048]	Loss: 0.1143
Training Epoch: 66 [37632/50048]	Loss: 0.1752
Training Epoch: 66 [37760/50048]	Loss: 0.1739
Training Epoch: 66 [37888/50048]	Loss: 0.1286
Training Epoch: 66 [38016/50048]	Loss: 0.2353
Training Epoch: 66 [38144/50048]	Loss: 0.1881
Training Epoch: 66 [38272/50048]	Loss: 0.1267
Training Epoch: 66 [38400/50048]	Loss: 0.1788
Training Epoch: 66 [38528/50048]	Loss: 0.1856
Training Epoch: 66 [38656/50048]	Loss: 0.3104
Training Epoch: 66 [38784/50048]	Loss: 0.1270
Training Epoch: 66 [38912/50048]	Loss: 0.1850
Training Epoch: 66 [39040/50048]	Loss: 0.1784
Training Epoch: 66 [39168/50048]	Loss: 0.2045
Training Epoch: 66 [39296/50048]	Loss: 0.2369
Training Epoch: 66 [39424/50048]	Loss: 0.1342
Training Epoch: 66 [39552/50048]	Loss: 0.2197
Training Epoch: 66 [39680/50048]	Loss: 0.1332
Training Epoch: 66 [39808/50048]	Loss: 0.1939
Training Epoch: 66 [39936/50048]	Loss: 0.1083
Training Epoch: 66 [40064/50048]	Loss: 0.2297
Training Epoch: 66 [40192/50048]	Loss: 0.2055
Training Epoch: 66 [40320/50048]	Loss: 0.1606
Training Epoch: 66 [40448/50048]	Loss: 0.1671
Training Epoch: 66 [40576/50048]	Loss: 0.1886
Training Epoch: 66 [40704/50048]	Loss: 0.3205
Training Epoch: 66 [40832/50048]	Loss: 0.3372
Training Epoch: 66 [40960/50048]	Loss: 0.2536
Training Epoch: 66 [41088/50048]	Loss: 0.0960
Training Epoch: 66 [41216/50048]	Loss: 0.1464
Training Epoch: 66 [41344/50048]	Loss: 0.1871
Training Epoch: 66 [41472/50048]	Loss: 0.2316
Training Epoch: 66 [41600/50048]	Loss: 0.1755
Training Epoch: 66 [41728/50048]	Loss: 0.1415
Training Epoch: 66 [41856/50048]	Loss: 0.1870
Training Epoch: 66 [41984/50048]	Loss: 0.2126
Training Epoch: 66 [42112/50048]	Loss: 0.1650
Training Epoch: 66 [42240/50048]	Loss: 0.1510
Training Epoch: 66 [42368/50048]	Loss: 0.1641
Training Epoch: 66 [42496/50048]	Loss: 0.2009
Training Epoch: 66 [42624/50048]	Loss: 0.2453
Training Epoch: 66 [42752/50048]	Loss: 0.2254
Training Epoch: 66 [42880/50048]	Loss: 0.1163
Training Epoch: 66 [43008/50048]	Loss: 0.0941
Training Epoch: 66 [43136/50048]	Loss: 0.1858
Training Epoch: 66 [43264/50048]	Loss: 0.1917
Training Epoch: 66 [43392/50048]	Loss: 0.2260
Training Epoch: 66 [43520/50048]	Loss: 0.1755
Training Epoch: 66 [43648/50048]	Loss: 0.0845
Training Epoch: 66 [43776/50048]	Loss: 0.2038
Training Epoch: 66 [43904/50048]	Loss: 0.2212
Training Epoch: 66 [44032/50048]	Loss: 0.1642
Training Epoch: 66 [44160/50048]	Loss: 0.1334
Training Epoch: 66 [44288/50048]	Loss: 0.1199
Training Epoch: 66 [44416/50048]	Loss: 0.1560
Training Epoch: 66 [44544/50048]	Loss: 0.1753
Training Epoch: 66 [44672/50048]	Loss: 0.1873
Training Epoch: 66 [44800/50048]	Loss: 0.1828
Training Epoch: 66 [44928/50048]	Loss: 0.1260
Training Epoch: 66 [45056/50048]	Loss: 0.2527
Training Epoch: 66 [45184/50048]	Loss: 0.2278
Training Epoch: 66 [45312/50048]	Loss: 0.1160
Training Epoch: 66 [45440/50048]	Loss: 0.1149
Training Epoch: 66 [45568/50048]	Loss: 0.1199
Training Epoch: 66 [45696/50048]	Loss: 0.2265
2022-12-06 07:48:34,553 [ZeusDataLoader(train)] train epoch 67 done: time=86.50 energy=10501.89
2022-12-06 07:48:34,554 [ZeusDataLoader(eval)] Epoch 67 begin.
Training Epoch: 66 [45824/50048]	Loss: 0.2058
Training Epoch: 66 [45952/50048]	Loss: 0.1782
Training Epoch: 66 [46080/50048]	Loss: 0.3043
Training Epoch: 66 [46208/50048]	Loss: 0.1788
Training Epoch: 66 [46336/50048]	Loss: 0.1431
Training Epoch: 66 [46464/50048]	Loss: 0.1735
Training Epoch: 66 [46592/50048]	Loss: 0.1578
Training Epoch: 66 [46720/50048]	Loss: 0.1819
Training Epoch: 66 [46848/50048]	Loss: 0.1596
Training Epoch: 66 [46976/50048]	Loss: 0.2334
Training Epoch: 66 [47104/50048]	Loss: 0.1652
Training Epoch: 66 [47232/50048]	Loss: 0.1828
Training Epoch: 66 [47360/50048]	Loss: 0.1940
Training Epoch: 66 [47488/50048]	Loss: 0.1719
Training Epoch: 66 [47616/50048]	Loss: 0.1586
Training Epoch: 66 [47744/50048]	Loss: 0.1518
Training Epoch: 66 [47872/50048]	Loss: 0.2500
Training Epoch: 66 [48000/50048]	Loss: 0.1479
Training Epoch: 66 [48128/50048]	Loss: 0.2007
Training Epoch: 66 [48256/50048]	Loss: 0.1767
Training Epoch: 66 [48384/50048]	Loss: 0.2076
Training Epoch: 66 [48512/50048]	Loss: 0.1981
Training Epoch: 66 [48640/50048]	Loss: 0.1813
Training Epoch: 66 [48768/50048]	Loss: 0.2745
Training Epoch: 66 [48896/50048]	Loss: 0.2259
Training Epoch: 66 [49024/50048]	Loss: 0.1919
Training Epoch: 66 [49152/50048]	Loss: 0.1621
Training Epoch: 66 [49280/50048]	Loss: 0.2113
Training Epoch: 66 [49408/50048]	Loss: 0.2291
Training Epoch: 66 [49536/50048]	Loss: 0.2074
Training Epoch: 66 [49664/50048]	Loss: 0.1787
Training Epoch: 66 [49792/50048]	Loss: 0.2404
Training Epoch: 66 [49920/50048]	Loss: 0.0939
Training Epoch: 66 [50048/50048]	Loss: 0.2325
2022-12-06 12:48:38.203 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:48:38,214 [ZeusDataLoader(eval)] eval epoch 67 done: time=3.65 energy=440.34
2022-12-06 07:48:38,214 [ZeusDataLoader(train)] Up to epoch 67: time=6043.12, energy=733566.58, cost=895556.43
2022-12-06 07:48:38,214 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:48:38,214 [ZeusDataLoader(train)] Expected next epoch: time=6132.92, energy=744364.59, cost=908812.81
2022-12-06 07:48:38,215 [ZeusDataLoader(train)] Epoch 68 begin.
Validation Epoch: 66, Average loss: 0.0167, Accuracy: 0.6322
2022-12-06 07:48:38,405 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:48:38,406 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:48:38.416 [ZeusMonitor] Monitor started.
2022-12-06 12:48:38.416 [ZeusMonitor] Running indefinitely. 2022-12-06 12:48:38.416 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:48:38.416 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e68+gpu0.power.log
Training Epoch: 67 [128/50048]	Loss: 0.1703
Training Epoch: 67 [256/50048]	Loss: 0.0719
Training Epoch: 67 [384/50048]	Loss: 0.1259
Training Epoch: 67 [512/50048]	Loss: 0.1638
Training Epoch: 67 [640/50048]	Loss: 0.0731
Training Epoch: 67 [768/50048]	Loss: 0.1514
Training Epoch: 67 [896/50048]	Loss: 0.1315
Training Epoch: 67 [1024/50048]	Loss: 0.1354
Training Epoch: 67 [1152/50048]	Loss: 0.1830
Training Epoch: 67 [1280/50048]	Loss: 0.1974
Training Epoch: 67 [1408/50048]	Loss: 0.1361
Training Epoch: 67 [1536/50048]	Loss: 0.0994
Training Epoch: 67 [1664/50048]	Loss: 0.1801
Training Epoch: 67 [1792/50048]	Loss: 0.1637
Training Epoch: 67 [1920/50048]	Loss: 0.0796
Training Epoch: 67 [2048/50048]	Loss: 0.1209
Training Epoch: 67 [2176/50048]	Loss: 0.1848
Training Epoch: 67 [2304/50048]	Loss: 0.1436
Training Epoch: 67 [2432/50048]	Loss: 0.1903
Training Epoch: 67 [2560/50048]	Loss: 0.1224
Training Epoch: 67 [2688/50048]	Loss: 0.0771
Training Epoch: 67 [2816/50048]	Loss: 0.1514
Training Epoch: 67 [2944/50048]	Loss: 0.1655
Training Epoch: 67 [3072/50048]	Loss: 0.1728
Training Epoch: 67 [3200/50048]	Loss: 0.1460
Training Epoch: 67 [3328/50048]	Loss: 0.1349
Training Epoch: 67 [3456/50048]	Loss: 0.1262
Training Epoch: 67 [3584/50048]	Loss: 0.1285
Training Epoch: 67 [3712/50048]	Loss: 0.1791
Training Epoch: 67 [3840/50048]	Loss: 0.1711
Training Epoch: 67 [3968/50048]	Loss: 0.1879
Training Epoch: 67 [4096/50048]	Loss: 0.1048
Training Epoch: 67 [4224/50048]	Loss: 0.1666
Training Epoch: 67 [4352/50048]	Loss: 0.1177
Training Epoch: 67 [4480/50048]	Loss: 0.0843
Training Epoch: 67 [4608/50048]	Loss: 0.1616
Training Epoch: 67 [4736/50048]	Loss: 0.1368
Training Epoch: 67 [4864/50048]	Loss: 0.1551
Training Epoch: 67 [4992/50048]	Loss: 0.1095
Training Epoch: 67 [5120/50048]	Loss: 0.1273
Training Epoch: 67 [5248/50048]	Loss: 0.1653
Training Epoch: 67 [5376/50048]	Loss: 0.2206
Training Epoch: 67 [5504/50048]	Loss: 0.0956
Training Epoch: 67 [5632/50048]	Loss: 0.1424
Training Epoch: 67 [5760/50048]	Loss: 0.1509
Training Epoch: 67 [5888/50048]	Loss: 0.2036
Training Epoch: 67 [6016/50048]	Loss: 0.1041
Training Epoch: 67 [6144/50048]	Loss: 0.0857
Training Epoch: 67 [6272/50048]	Loss: 0.1209
Training Epoch: 67 [6400/50048]	Loss: 0.1432
Training Epoch: 67 [6528/50048]	Loss: 0.1856
Training Epoch: 67 [6656/50048]	Loss: 0.1356
Training Epoch: 67 [6784/50048]	Loss: 0.2195
Training Epoch: 67 [6912/50048]	Loss: 0.1676
Training Epoch: 67 [7040/50048]	Loss: 0.1701
Training Epoch: 67 [7168/50048]	Loss: 0.1611
Training Epoch: 67 [7296/50048]	Loss: 0.1627
Training Epoch: 67 [7424/50048]	Loss: 0.2063
Training Epoch: 67 [7552/50048]	Loss: 0.2033
Training Epoch: 67 [7680/50048]	Loss: 0.1481
Training Epoch: 67 [7808/50048]	Loss: 0.2173
Training Epoch: 67 [7936/50048]	Loss: 0.1315
Training Epoch: 67 [8064/50048]	Loss: 0.1511
Training Epoch: 67 [8192/50048]	Loss: 0.0745
Training Epoch: 67 [8320/50048]	Loss: 0.1249
Training Epoch: 67 [8448/50048]	Loss: 0.0801
Training Epoch: 67 [8576/50048]	Loss: 0.1557
Training Epoch: 67 [8704/50048]	Loss: 0.1143
Training Epoch: 67 [8832/50048]	Loss: 0.1843
Training Epoch: 67 [8960/50048]	Loss: 0.1240
Training Epoch: 67 [9088/50048]	Loss: 0.1879
Training Epoch: 67 [9216/50048]	Loss: 0.1271
Training Epoch: 67 [9344/50048]	Loss: 0.2052
Training Epoch: 67 [9472/50048]	Loss: 0.2253
Training Epoch: 67 [9600/50048]	Loss: 0.1733
Training Epoch: 67 [9728/50048]	Loss: 0.0929
Training Epoch: 67 [9856/50048]	Loss: 0.1496
Training Epoch: 67 [9984/50048]	Loss: 0.0718
Training Epoch: 67 [10112/50048]	Loss: 0.1298
Training Epoch: 67 [10240/50048]	Loss: 0.2016
Training Epoch: 67 [10368/50048]	Loss: 0.1751
Training Epoch: 67 [10496/50048]	Loss: 0.1495
Training Epoch: 67 [10624/50048]	Loss: 0.1209
Training Epoch: 67 [10752/50048]	Loss: 0.1173
Training Epoch: 67 [10880/50048]	Loss: 0.2120
Training Epoch: 67 [11008/50048]	Loss: 0.1190
Training Epoch: 67 [11136/50048]	Loss: 0.1975
Training Epoch: 67 [11264/50048]	Loss: 0.0932
Training Epoch: 67 [11392/50048]	Loss: 0.0983
Training Epoch: 67 [11520/50048]	Loss: 0.1278
Training Epoch: 67 [11648/50048]	Loss: 0.1155
Training Epoch: 67 [11776/50048]	Loss: 0.1457
Training Epoch: 67 [11904/50048]	Loss: 0.0919
Training Epoch: 67 [12032/50048]	Loss: 0.0897
Training Epoch: 67 [12160/50048]	Loss: 0.1437
Training Epoch: 67 [12288/50048]	Loss: 0.1031
Training Epoch: 67 [12416/50048]	Loss: 0.1390
Training Epoch: 67 [12544/50048]	Loss: 0.1205
Training Epoch: 67 [12672/50048]	Loss: 0.1219
Training Epoch: 67 [12800/50048]	Loss: 0.1386
Training Epoch: 67 [12928/50048]	Loss: 0.1529
Training Epoch: 67 [13056/50048]	Loss: 0.2270
Training Epoch: 67 [13184/50048]	Loss: 0.1733
Training Epoch: 67 [13312/50048]	Loss: 0.1277
Training Epoch: 67 [13440/50048]	Loss: 0.1566
Training Epoch: 67 [13568/50048]	Loss: 0.0891
Training Epoch: 67 [13696/50048]	Loss: 0.1821
Training Epoch: 67 [13824/50048]	Loss: 0.1021
Training Epoch: 67 [13952/50048]	Loss: 0.1876
Training Epoch: 67 [14080/50048]	Loss: 0.1382
Training Epoch: 67 [14208/50048]	Loss: 0.2262
Training Epoch: 67 [14336/50048]	Loss: 0.1926
Training Epoch: 67 [14464/50048]	Loss: 0.1560
Training Epoch: 67 [14592/50048]	Loss: 0.1054
Training Epoch: 67 [14720/50048]	Loss: 0.2120
Training Epoch: 67 [14848/50048]	Loss: 0.2533
Training Epoch: 67 [14976/50048]	Loss: 0.1106
Training Epoch: 67 [15104/50048]	Loss: 0.1308
Training Epoch: 67 [15232/50048]	Loss: 0.1918
Training Epoch: 67 [15360/50048]	Loss: 0.1438
Training Epoch: 67 [15488/50048]	Loss: 0.2402
Training Epoch: 67 [15616/50048]	Loss: 0.1182
Training Epoch: 67 [15744/50048]	Loss: 0.2227
Training Epoch: 67 [15872/50048]	Loss: 0.1226
Training Epoch: 67 [16000/50048]	Loss: 0.2053
Training Epoch: 67 [16128/50048]	Loss: 0.1611
Training Epoch: 67 [16256/50048]	Loss: 0.1695
Training Epoch: 67 [16384/50048]	Loss: 0.1422
Training Epoch: 67 [16512/50048]	Loss: 0.1498
Training Epoch: 67 [16640/50048]	Loss: 0.2049
Training Epoch: 67 [16768/50048]	Loss: 0.1190
Training Epoch: 67 [16896/50048]	Loss: 0.1773
Training Epoch: 67 [17024/50048]	Loss: 0.1495
Training Epoch: 67 [17152/50048]	Loss: 0.1600
Training Epoch: 67 [17280/50048]	Loss: 0.1965
Training Epoch: 67 [17408/50048]	Loss: 0.2351
Training Epoch: 67 [17536/50048]	Loss: 0.2345
Training Epoch: 67 [17664/50048]	Loss: 0.1961
Training Epoch: 67 [17792/50048]	Loss: 0.1088
Training Epoch: 67 [17920/50048]	Loss: 0.1585
Training Epoch: 67 [18048/50048]	Loss: 0.1467
Training Epoch: 67 [18176/50048]	Loss: 0.2556
Training Epoch: 67 [18304/50048]	Loss: 0.2147
Training Epoch: 67 [18432/50048]	Loss: 0.2123
Training Epoch: 67 [18560/50048]	Loss: 0.1151
Training Epoch: 67 [18688/50048]	Loss: 0.2414
Training Epoch: 67 [18816/50048]	Loss: 0.2241
Training Epoch: 67 [18944/50048]	Loss: 0.1395
Training Epoch: 67 [19072/50048]	Loss: 0.2154
Training Epoch: 67 [19200/50048]	Loss: 0.2513
Training Epoch: 67 [19328/50048]	Loss: 0.1359
Training Epoch: 67 [19456/50048]	Loss: 0.1958
Training Epoch: 67 [19584/50048]	Loss: 0.1655
Training Epoch: 67 [19712/50048]	Loss: 0.1643
Training Epoch: 67 [19840/50048]	Loss: 0.1259
Training Epoch: 67 [19968/50048]	Loss: 0.1298
Training Epoch: 67 [20096/50048]	Loss: 0.1129
Training Epoch: 67 [20224/50048]	Loss: 0.2200
Training Epoch: 67 [20352/50048]	Loss: 0.1755
Training Epoch: 67 [20480/50048]	Loss: 0.1593
Training Epoch: 67 [20608/50048]	Loss: 0.1209
Training Epoch: 67 [20736/50048]	Loss: 0.1245
Training Epoch: 67 [20864/50048]	Loss: 0.1429
Training Epoch: 67 [20992/50048]	Loss: 0.1902
Training Epoch: 67 [21120/50048]	Loss: 0.0831
Training Epoch: 67 [21248/50048]	Loss: 0.1158
Training Epoch: 67 [21376/50048]	Loss: 0.1560
Training Epoch: 67 [21504/50048]	Loss: 0.1264
Training Epoch: 67 [21632/50048]	Loss: 0.1203
Training Epoch: 67 [21760/50048]	Loss: 0.1301
Training Epoch: 67 [21888/50048]	Loss: 0.1771
Training Epoch: 67 [22016/50048]	Loss: 0.1663
Training Epoch: 67 [22144/50048]	Loss: 0.2567
Training Epoch: 67 [22272/50048]	Loss: 0.1232
Training Epoch: 67 [22400/50048]	Loss: 0.1652
Training Epoch: 67 [22528/50048]	Loss: 0.1709
Training Epoch: 67 [22656/50048]	Loss: 0.0895
Training Epoch: 67 [22784/50048]	Loss: 0.1427
Training Epoch: 67 [22912/50048]	Loss: 0.1874
Training Epoch: 67 [23040/50048]	Loss: 0.1661
Training Epoch: 67 [23168/50048]	Loss: 0.2395
Training Epoch: 67 [23296/50048]	Loss: 0.0975
Training Epoch: 67 [23424/50048]	Loss: 0.1383
Training Epoch: 67 [23552/50048]	Loss: 0.2308
Training Epoch: 67 [23680/50048]	Loss: 0.2684
Training Epoch: 67 [23808/50048]	Loss: 0.2162
Training Epoch: 67 [23936/50048]	Loss: 0.1322
Training Epoch: 67 [24064/50048]	Loss: 0.1467
Training Epoch: 67 [24192/50048]	Loss: 0.1332
Training Epoch: 67 [24320/50048]	Loss: 0.1433
Training Epoch: 67 [24448/50048]	Loss: 0.2073
Training Epoch: 67 [24576/50048]	Loss: 0.0872
Training Epoch: 67 [24704/50048]	Loss: 0.1663
Training Epoch: 67 [24832/50048]	Loss: 0.1223
Training Epoch: 67 [24960/50048]	Loss: 0.1895
Training Epoch: 67 [25088/50048]	Loss: 0.1277
Training Epoch: 67 [25216/50048]	Loss: 0.2314
Training Epoch: 67 [25344/50048]	Loss: 0.1985
Training Epoch: 67 [25472/50048]	Loss: 0.1299
Training Epoch: 67 [25600/50048]	Loss: 0.1159
Training Epoch: 67 [25728/50048]	Loss: 0.1980
Training Epoch: 67 [25856/50048]	Loss: 0.2145
Training Epoch: 67 [25984/50048]	Loss: 0.0809
Training Epoch: 67 [26112/50048]	Loss: 0.2304
Training Epoch: 67 [26240/50048]	Loss: 0.1903
Training Epoch: 67 [26368/50048]	Loss: 0.2446
Training Epoch: 67 [26496/50048]	Loss: 0.1638
Training Epoch: 67 [26624/50048]	Loss: 0.2500
Training Epoch: 67 [26752/50048]	Loss: 0.2341
Training Epoch: 67 [26880/50048]	Loss: 0.1001
Training Epoch: 67 [27008/50048]	Loss: 0.1256
Training Epoch: 67 [27136/50048]	Loss: 0.1950
Training Epoch: 67 [27264/50048]	Loss: 0.2069
Training Epoch: 67 [27392/50048]	Loss: 0.2024
Training Epoch: 67 [27520/50048]	Loss: 0.1300
Training Epoch: 67 [27648/50048]	Loss: 0.1336
Training Epoch: 67 [27776/50048]	Loss: 0.2178
Training Epoch: 67 [27904/50048]	Loss: 0.1687
Training Epoch: 67 [28032/50048]	Loss: 0.2067
Training Epoch: 67 [28160/50048]	Loss: 0.1548
Training Epoch: 67 [28288/50048]	Loss: 0.1713
Training Epoch: 67 [28416/50048]	Loss: 0.1786
Training Epoch: 67 [28544/50048]	Loss: 0.1852
Training Epoch: 67 [28672/50048]	Loss: 0.0970
Training Epoch: 67 [28800/50048]	Loss: 0.1550
Training Epoch: 67 [28928/50048]	Loss: 0.1014
Training Epoch: 67 [29056/50048]	Loss: 0.1551
Training Epoch: 67 [29184/50048]	Loss: 0.1769
Training Epoch: 67 [29312/50048]	Loss: 0.1551
Training Epoch: 67 [29440/50048]	Loss: 0.1539
Training Epoch: 67 [29568/50048]	Loss: 0.1147
Training Epoch: 67 [29696/50048]	Loss: 0.1972
Training Epoch: 67 [29824/50048]	Loss: 0.1195
Training Epoch: 67 [29952/50048]	Loss: 0.1593
Training Epoch: 67 [30080/50048]	Loss: 0.1999
Training Epoch: 67 [30208/50048]	Loss: 0.1381
Training Epoch: 67 [30336/50048]	Loss: 0.0910
Training Epoch: 67 [30464/50048]	Loss: 0.1956
Training Epoch: 67 [30592/50048]	Loss: 0.2413
Training Epoch: 67 [30720/50048]	Loss: 0.1379
Training Epoch: 67 [30848/50048]	Loss: 0.2030
Training Epoch: 67 [30976/50048]	Loss: 0.1919
Training Epoch: 67 [31104/50048]	Loss: 0.1428
Training Epoch: 67 [31232/50048]	Loss: 0.2877
Training Epoch: 67 [31360/50048]	Loss: 0.2086
Training Epoch: 67 [31488/50048]	Loss: 0.2212
Training Epoch: 67 [31616/50048]	Loss: 0.1369
Training Epoch: 67 [31744/50048]	Loss: 0.1920
Training Epoch: 67 [31872/50048]	Loss: 0.2136
Training Epoch: 67 [32000/50048]	Loss: 0.2060
Training Epoch: 67 [32128/50048]	Loss: 0.1554
Training Epoch: 67 [32256/50048]	Loss: 0.2252
Training Epoch: 67 [32384/50048]	Loss: 0.2035
Training Epoch: 67 [32512/50048]	Loss: 0.2522
Training Epoch: 67 [32640/50048]	Loss: 0.2116
Training Epoch: 67 [32768/50048]	Loss: 0.2239
Training Epoch: 67 [32896/50048]	Loss: 0.1703
Training Epoch: 67 [33024/50048]	Loss: 0.2315
Training Epoch: 67 [33152/50048]	Loss: 0.2556
Training Epoch: 67 [33280/50048]	Loss: 0.2498
Training Epoch: 67 [33408/50048]	Loss: 0.1451
Training Epoch: 67 [33536/50048]	Loss: 0.1973
Training Epoch: 67 [33664/50048]	Loss: 0.2566
Training Epoch: 67 [33792/50048]	Loss: 0.1160
Training Epoch: 67 [33920/50048]	Loss: 0.1863
Training Epoch: 67 [34048/50048]	Loss: 0.1893
Training Epoch: 67 [34176/50048]	Loss: 0.2124
Training Epoch: 67 [34304/50048]	Loss: 0.2269
Training Epoch: 67 [34432/50048]	Loss: 0.2560
Training Epoch: 67 [34560/50048]	Loss: 0.1939
Training Epoch: 67 [34688/50048]	Loss: 0.2697
Training Epoch: 67 [34816/50048]	Loss: 0.1432
Training Epoch: 67 [34944/50048]	Loss: 0.0952
Training Epoch: 67 [35072/50048]	Loss: 0.1682
Training Epoch: 67 [35200/50048]	Loss: 0.1736
Training Epoch: 67 [35328/50048]	Loss: 0.1395
Training Epoch: 67 [35456/50048]	Loss: 0.1225
Training Epoch: 67 [35584/50048]	Loss: 0.1543
Training Epoch: 67 [35712/50048]	Loss: 0.1614
Training Epoch: 67 [35840/50048]	Loss: 0.1092
Training Epoch: 67 [35968/50048]	Loss: 0.0747
Training Epoch: 67 [36096/50048]	Loss: 0.1300
Training Epoch: 67 [36224/50048]	Loss: 0.2035
Training Epoch: 67 [36352/50048]	Loss: 0.1626
Training Epoch: 67 [36480/50048]	Loss: 0.1196
Training Epoch: 67 [36608/50048]	Loss: 0.1001
Training Epoch: 67 [36736/50048]	Loss: 0.1302
Training Epoch: 67 [36864/50048]	Loss: 0.1117
Training Epoch: 67 [36992/50048]	Loss: 0.1986
Training Epoch: 67 [37120/50048]	Loss: 0.2030
Training Epoch: 67 [37248/50048]	Loss: 0.2149
Training Epoch: 67 [37376/50048]	Loss: 0.2034
Training Epoch: 67 [37504/50048]	Loss: 0.2652
Training Epoch: 67 [37632/50048]	Loss: 0.1478
Training Epoch: 67 [37760/50048]	Loss: 0.1665
Training Epoch: 67 [37888/50048]	Loss: 0.1868
Training Epoch: 67 [38016/50048]	Loss: 0.1263
Training Epoch: 67 [38144/50048]	Loss: 0.0917
Training Epoch: 67 [38272/50048]	Loss: 0.1692
Training Epoch: 67 [38400/50048]	Loss: 0.1724
Training Epoch: 67 [38528/50048]	Loss: 0.1978
Training Epoch: 67 [38656/50048]	Loss: 0.1922
Training Epoch: 67 [38784/50048]	Loss: 0.2187
Training Epoch: 67 [38912/50048]	Loss: 0.0931
Training Epoch: 67 [39040/50048]	Loss: 0.1135
Training Epoch: 67 [39168/50048]	Loss: 0.1834
Training Epoch: 67 [39296/50048]	Loss: 0.1795
Training Epoch: 67 [39424/50048]	Loss: 0.1246
Training Epoch: 67 [39552/50048]	Loss: 0.2733
Training Epoch: 67 [39680/50048]	Loss: 0.2891
Training Epoch: 67 [39808/50048]	Loss: 0.0694
Training Epoch: 67 [39936/50048]	Loss: 0.1260
Training Epoch: 67 [40064/50048]	Loss: 0.1727
Training Epoch: 67 [40192/50048]	Loss: 0.1172
Training Epoch: 67 [40320/50048]	Loss: 0.3247
Training Epoch: 67 [40448/50048]	Loss: 0.2146
Training Epoch: 67 [40576/50048]	Loss: 0.1303
Training Epoch: 67 [40704/50048]	Loss: 0.0778
Training Epoch: 67 [40832/50048]	Loss: 0.2591
Training Epoch: 67 [40960/50048]	Loss: 0.1763
Training Epoch: 67 [41088/50048]	Loss: 0.2071
Training Epoch: 67 [41216/50048]	Loss: 0.1646
Training Epoch: 67 [41344/50048]	Loss: 0.0765
Training Epoch: 67 [41472/50048]	Loss: 0.1017
Training Epoch: 67 [41600/50048]	Loss: 0.2104
Training Epoch: 67 [41728/50048]	Loss: 0.1503
Training Epoch: 67 [41856/50048]	Loss: 0.2482
Training Epoch: 67 [41984/50048]	Loss: 0.2013
Training Epoch: 67 [42112/50048]	Loss: 0.0880
Training Epoch: 67 [42240/50048]	Loss: 0.1349
Training Epoch: 67 [42368/50048]	Loss: 0.1851
Training Epoch: 67 [42496/50048]	Loss: 0.1446
Training Epoch: 67 [42624/50048]	Loss: 0.0991
Training Epoch: 67 [42752/50048]	Loss: 0.3095
Training Epoch: 67 [42880/50048]	Loss: 0.1672
Training Epoch: 67 [43008/50048]	Loss: 0.1715
Training Epoch: 67 [43136/50048]	Loss: 0.4113
Training Epoch: 67 [43264/50048]	Loss: 0.1004
Training Epoch: 67 [43392/50048]	Loss: 0.1413
Training Epoch: 67 [43520/50048]	Loss: 0.2505
Training Epoch: 67 [43648/50048]	Loss: 0.1447
Training Epoch: 67 [43776/50048]	Loss: 0.1810
Training Epoch: 67 [43904/50048]	Loss: 0.2264
Training Epoch: 67 [44032/50048]	Loss: 0.1170
Training Epoch: 67 [44160/50048]	Loss: 0.1893
Training Epoch: 67 [44288/50048]	Loss: 0.2535
Training Epoch: 67 [44416/50048]	Loss: 0.1656
Training Epoch: 67 [44544/50048]	Loss: 0.1071
Training Epoch: 67 [44672/50048]	Loss: 0.1441
Training Epoch: 67 [44800/50048]	Loss: 0.2490
Training Epoch: 67 [44928/50048]	Loss: 0.1570
Training Epoch: 67 [45056/50048]	Loss: 0.1305
Training Epoch: 67 [45184/50048]	Loss: 0.1689
Training Epoch: 67 [45312/50048]	Loss: 0.1518
Training Epoch: 67 [45440/50048]	Loss: 0.1456
Training Epoch: 67 [45568/50048]	Loss: 0.2034
Training Epoch: 67 [45696/50048]	Loss: 0.1872
2022-12-06 07:50:04,744 [ZeusDataLoader(train)] train epoch 68 done: time=86.52 energy=10504.48
2022-12-06 07:50:04,745 [ZeusDataLoader(eval)] Epoch 68 begin.
Training Epoch: 67 [45824/50048]	Loss: 0.3323
Training Epoch: 67 [45952/50048]	Loss: 0.1890
Training Epoch: 67 [46080/50048]	Loss: 0.1366
Training Epoch: 67 [46208/50048]	Loss: 0.2022
Training Epoch: 67 [46336/50048]	Loss: 0.1563
Training Epoch: 67 [46464/50048]	Loss: 0.2078
Training Epoch: 67 [46592/50048]	Loss: 0.1382
Training Epoch: 67 [46720/50048]	Loss: 0.2195
Training Epoch: 67 [46848/50048]	Loss: 0.1144
Training Epoch: 67 [46976/50048]	Loss: 0.1688
Training Epoch: 67 [47104/50048]	Loss: 0.2106
Training Epoch: 67 [47232/50048]	Loss: 0.1589
Training Epoch: 67 [47360/50048]	Loss: 0.2009
Training Epoch: 67 [47488/50048]	Loss: 0.2029
Training Epoch: 67 [47616/50048]	Loss: 0.1478
Training Epoch: 67 [47744/50048]	Loss: 0.1829
Training Epoch: 67 [47872/50048]	Loss: 0.2055
Training Epoch: 67 [48000/50048]	Loss: 0.1420
Training Epoch: 67 [48128/50048]	Loss: 0.1094
Training Epoch: 67 [48256/50048]	Loss: 0.2288
Training Epoch: 67 [48384/50048]	Loss: 0.1940
Training Epoch: 67 [48512/50048]	Loss: 0.1967
Training Epoch: 67 [48640/50048]	Loss: 0.0952
Training Epoch: 67 [48768/50048]	Loss: 0.1940
Training Epoch: 67 [48896/50048]	Loss: 0.1070
Training Epoch: 67 [49024/50048]	Loss: 0.2690
Training Epoch: 67 [49152/50048]	Loss: 0.1054
Training Epoch: 67 [49280/50048]	Loss: 0.2195
Training Epoch: 67 [49408/50048]	Loss: 0.2513
Training Epoch: 67 [49536/50048]	Loss: 0.1495
Training Epoch: 67 [49664/50048]	Loss: 0.1284
Training Epoch: 67 [49792/50048]	Loss: 0.1551
Training Epoch: 67 [49920/50048]	Loss: 0.2004
Training Epoch: 67 [50048/50048]	Loss: 0.2411
2022-12-06 12:50:08.405 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:50:08,418 [ZeusDataLoader(eval)] eval epoch 68 done: time=3.66 energy=444.65
2022-12-06 07:50:08,418 [ZeusDataLoader(train)] Up to epoch 68: time=6133.30, energy=744515.71, cost=908921.93
2022-12-06 07:50:08,418 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:50:08,418 [ZeusDataLoader(train)] Expected next epoch: time=6223.10, energy=755313.73, cost=922178.32
2022-12-06 07:50:08,419 [ZeusDataLoader(train)] Epoch 69 begin.
Validation Epoch: 67, Average loss: 0.0165, Accuracy: 0.6364
2022-12-06 07:50:08,623 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:50:08,624 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:50:08.625 [ZeusMonitor] Monitor started.
2022-12-06 12:50:08.626 [ZeusMonitor] Running indefinitely. 2022-12-06 12:50:08.626 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:50:08.626 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e69+gpu0.power.log
Training Epoch: 68 [128/50048]	Loss: 0.2740
Training Epoch: 68 [256/50048]	Loss: 0.1001
Training Epoch: 68 [384/50048]	Loss: 0.2564
Training Epoch: 68 [512/50048]	Loss: 0.1785
Training Epoch: 68 [640/50048]	Loss: 0.0920
Training Epoch: 68 [768/50048]	Loss: 0.2411
Training Epoch: 68 [896/50048]	Loss: 0.1716
Training Epoch: 68 [1024/50048]	Loss: 0.1000
Training Epoch: 68 [1152/50048]	Loss: 0.1698
Training Epoch: 68 [1280/50048]	Loss: 0.1237
Training Epoch: 68 [1408/50048]	Loss: 0.0942
Training Epoch: 68 [1536/50048]	Loss: 0.1142
Training Epoch: 68 [1664/50048]	Loss: 0.1374
Training Epoch: 68 [1792/50048]	Loss: 0.1665
Training Epoch: 68 [1920/50048]	Loss: 0.1917
Training Epoch: 68 [2048/50048]	Loss: 0.1850
Training Epoch: 68 [2176/50048]	Loss: 0.1386
Training Epoch: 68 [2304/50048]	Loss: 0.1134
Training Epoch: 68 [2432/50048]	Loss: 0.1972
Training Epoch: 68 [2560/50048]	Loss: 0.1583
Training Epoch: 68 [2688/50048]	Loss: 0.1727
Training Epoch: 68 [2816/50048]	Loss: 0.1420
Training Epoch: 68 [2944/50048]	Loss: 0.1045
Training Epoch: 68 [3072/50048]	Loss: 0.0677
Training Epoch: 68 [3200/50048]	Loss: 0.0956
Training Epoch: 68 [3328/50048]	Loss: 0.1620
Training Epoch: 68 [3456/50048]	Loss: 0.1061
Training Epoch: 68 [3584/50048]	Loss: 0.1770
Training Epoch: 68 [3712/50048]	Loss: 0.1663
Training Epoch: 68 [3840/50048]	Loss: 0.1429
Training Epoch: 68 [3968/50048]	Loss: 0.1246
Training Epoch: 68 [4096/50048]	Loss: 0.1714
Training Epoch: 68 [4224/50048]	Loss: 0.1934
Training Epoch: 68 [4352/50048]	Loss: 0.0757
Training Epoch: 68 [4480/50048]	Loss: 0.1530
Training Epoch: 68 [4608/50048]	Loss: 0.1008
Training Epoch: 68 [4736/50048]	Loss: 0.1495
Training Epoch: 68 [4864/50048]	Loss: 0.1251
Training Epoch: 68 [4992/50048]	Loss: 0.1908
Training Epoch: 68 [5120/50048]	Loss: 0.2188
Training Epoch: 68 [5248/50048]	Loss: 0.1531
Training Epoch: 68 [5376/50048]	Loss: 0.1121
Training Epoch: 68 [5504/50048]	Loss: 0.1903
Training Epoch: 68 [5632/50048]	Loss: 0.1346
Training Epoch: 68 [5760/50048]	Loss: 0.1685
Training Epoch: 68 [5888/50048]	Loss: 0.1093
Training Epoch: 68 [6016/50048]	Loss: 0.1717
Training Epoch: 68 [6144/50048]	Loss: 0.1918
Training Epoch: 68 [6272/50048]	Loss: 0.1475
Training Epoch: 68 [6400/50048]	Loss: 0.0716
Training Epoch: 68 [6528/50048]	Loss: 0.1006
Training Epoch: 68 [6656/50048]	Loss: 0.1198
Training Epoch: 68 [6784/50048]	Loss: 0.1155
Training Epoch: 68 [6912/50048]	Loss: 0.0893
Training Epoch: 68 [7040/50048]	Loss: 0.1782
Training Epoch: 68 [7168/50048]	Loss: 0.0874
Training Epoch: 68 [7296/50048]	Loss: 0.0931
Training Epoch: 68 [7424/50048]	Loss: 0.1292
Training Epoch: 68 [7552/50048]	Loss: 0.0732
Training Epoch: 68 [7680/50048]	Loss: 0.0619
Training Epoch: 68 [7808/50048]	Loss: 0.1691
Training Epoch: 68 [7936/50048]	Loss: 0.2382
Training Epoch: 68 [8064/50048]	Loss: 0.1981
Training Epoch: 68 [8192/50048]	Loss: 0.2329
Training Epoch: 68 [8320/50048]	Loss: 0.1269
Training Epoch: 68 [8448/50048]	Loss: 0.1862
Training Epoch: 68 [8576/50048]	Loss: 0.1414
Training Epoch: 68 [8704/50048]	Loss: 0.1359
Training Epoch: 68 [8832/50048]	Loss: 0.1090
Training Epoch: 68 [8960/50048]	Loss: 0.1191
Training Epoch: 68 [9088/50048]	Loss: 0.1311
Training Epoch: 68 [9216/50048]	Loss: 0.1272
Training Epoch: 68 [9344/50048]	Loss: 0.1025
Training Epoch: 68 [9472/50048]	Loss: 0.1225
Training Epoch: 68 [9600/50048]	Loss: 0.1744
Training Epoch: 68 [9728/50048]	Loss: 0.1490
Training Epoch: 68 [9856/50048]	Loss: 0.1026
Training Epoch: 68 [9984/50048]	Loss: 0.1963
Training Epoch: 68 [10112/50048]	Loss: 0.1491
Training Epoch: 68 [10240/50048]	Loss: 0.1280
Training Epoch: 68 [10368/50048]	Loss: 0.0816
Training Epoch: 68 [10496/50048]	Loss: 0.1505
Training Epoch: 68 [10624/50048]	Loss: 0.1161
Training Epoch: 68 [10752/50048]	Loss: 0.1148
Training Epoch: 68 [10880/50048]	Loss: 0.1040
Training Epoch: 68 [11008/50048]	Loss: 0.1521
Training Epoch: 68 [11136/50048]	Loss: 0.1636
Training Epoch: 68 [11264/50048]	Loss: 0.1079
Training Epoch: 68 [11392/50048]	Loss: 0.1871
Training Epoch: 68 [11520/50048]	Loss: 0.1514
Training Epoch: 68 [11648/50048]	Loss: 0.1668
Training Epoch: 68 [11776/50048]	Loss: 0.1726
Training Epoch: 68 [11904/50048]	Loss: 0.1748
Training Epoch: 68 [12032/50048]	Loss: 0.1803
Training Epoch: 68 [12160/50048]	Loss: 0.1281
Training Epoch: 68 [12288/50048]	Loss: 0.1637
Training Epoch: 68 [12416/50048]	Loss: 0.1358
Training Epoch: 68 [12544/50048]	Loss: 0.1677
Training Epoch: 68 [12672/50048]	Loss: 0.1085
Training Epoch: 68 [12800/50048]	Loss: 0.1541
Training Epoch: 68 [12928/50048]	Loss: 0.1547
Training Epoch: 68 [13056/50048]	Loss: 0.1499
Training Epoch: 68 [13184/50048]	Loss: 0.2371
Training Epoch: 68 [13312/50048]	Loss: 0.1113
Training Epoch: 68 [13440/50048]	Loss: 0.1492
Training Epoch: 68 [13568/50048]	Loss: 0.1155
Training Epoch: 68 [13696/50048]	Loss: 0.1213
Training Epoch: 68 [13824/50048]	Loss: 0.0801
Training Epoch: 68 [13952/50048]	Loss: 0.1398
Training Epoch: 68 [14080/50048]	Loss: 0.0703
Training Epoch: 68 [14208/50048]	Loss: 0.1168
Training Epoch: 68 [14336/50048]	Loss: 0.1136
Training Epoch: 68 [14464/50048]	Loss: 0.1639
Training Epoch: 68 [14592/50048]	Loss: 0.2401
Training Epoch: 68 [14720/50048]	Loss: 0.0913
Training Epoch: 68 [14848/50048]	Loss: 0.2798
Training Epoch: 68 [14976/50048]	Loss: 0.1854
Training Epoch: 68 [15104/50048]	Loss: 0.1365
Training Epoch: 68 [15232/50048]	Loss: 0.2281
Training Epoch: 68 [15360/50048]	Loss: 0.0944
Training Epoch: 68 [15488/50048]	Loss: 0.1272
Training Epoch: 68 [15616/50048]	Loss: 0.1766
Training Epoch: 68 [15744/50048]	Loss: 0.1886
Training Epoch: 68 [15872/50048]	Loss: 0.1599
Training Epoch: 68 [16000/50048]	Loss: 0.1844
Training Epoch: 68 [16128/50048]	Loss: 0.1380
Training Epoch: 68 [16256/50048]	Loss: 0.1834
Training Epoch: 68 [16384/50048]	Loss: 0.1003
Training Epoch: 68 [16512/50048]	Loss: 0.1637
Training Epoch: 68 [16640/50048]	Loss: 0.1743
Training Epoch: 68 [16768/50048]	Loss: 0.1642
Training Epoch: 68 [16896/50048]	Loss: 0.1519
Training Epoch: 68 [17024/50048]	Loss: 0.2226
Training Epoch: 68 [17152/50048]	Loss: 0.1430
Training Epoch: 68 [17280/50048]	Loss: 0.1392
Training Epoch: 68 [17408/50048]	Loss: 0.1047
Training Epoch: 68 [17536/50048]	Loss: 0.2052
Training Epoch: 68 [17664/50048]	Loss: 0.1240
Training Epoch: 68 [17792/50048]	Loss: 0.0987
Training Epoch: 68 [17920/50048]	Loss: 0.0660
Training Epoch: 68 [18048/50048]	Loss: 0.1069
Training Epoch: 68 [18176/50048]	Loss: 0.0671
Training Epoch: 68 [18304/50048]	Loss: 0.2683
Training Epoch: 68 [18432/50048]	Loss: 0.1407
Training Epoch: 68 [18560/50048]	Loss: 0.0751
Training Epoch: 68 [18688/50048]	Loss: 0.2120
Training Epoch: 68 [18816/50048]	Loss: 0.0927
Training Epoch: 68 [18944/50048]	Loss: 0.1471
Training Epoch: 68 [19072/50048]	Loss: 0.0729
Training Epoch: 68 [19200/50048]	Loss: 0.1015
Training Epoch: 68 [19328/50048]	Loss: 0.0887
Training Epoch: 68 [19456/50048]	Loss: 0.1070
Training Epoch: 68 [19584/50048]	Loss: 0.1592
Training Epoch: 68 [19712/50048]	Loss: 0.1668
Training Epoch: 68 [19840/50048]	Loss: 0.1897
Training Epoch: 68 [19968/50048]	Loss: 0.0996
Training Epoch: 68 [20096/50048]	Loss: 0.0947
Training Epoch: 68 [20224/50048]	Loss: 0.0693
Training Epoch: 68 [20352/50048]	Loss: 0.1658
Training Epoch: 68 [20480/50048]	Loss: 0.1413
Training Epoch: 68 [20608/50048]	Loss: 0.1195
Training Epoch: 68 [20736/50048]	Loss: 0.1424
Training Epoch: 68 [20864/50048]	Loss: 0.1299
Training Epoch: 68 [20992/50048]	Loss: 0.0902
Training Epoch: 68 [21120/50048]	Loss: 0.1648
Training Epoch: 68 [21248/50048]	Loss: 0.1191
Training Epoch: 68 [21376/50048]	Loss: 0.2306
Training Epoch: 68 [21504/50048]	Loss: 0.1332
Training Epoch: 68 [21632/50048]	Loss: 0.2008
Training Epoch: 68 [21760/50048]	Loss: 0.1440
Training Epoch: 68 [21888/50048]	Loss: 0.1593
Training Epoch: 68 [22016/50048]	Loss: 0.0745
Training Epoch: 68 [22144/50048]	Loss: 0.2273
Training Epoch: 68 [22272/50048]	Loss: 0.1393
Training Epoch: 68 [22400/50048]	Loss: 0.1057
Training Epoch: 68 [22528/50048]	Loss: 0.1861
Training Epoch: 68 [22656/50048]	Loss: 0.1455
Training Epoch: 68 [22784/50048]	Loss: 0.1592
Training Epoch: 68 [22912/50048]	Loss: 0.1084
Training Epoch: 68 [23040/50048]	Loss: 0.1223
Training Epoch: 68 [23168/50048]	Loss: 0.1943
Training Epoch: 68 [23296/50048]	Loss: 0.1414
Training Epoch: 68 [23424/50048]	Loss: 0.0883
Training Epoch: 68 [23552/50048]	Loss: 0.1748
Training Epoch: 68 [23680/50048]	Loss: 0.1339
Training Epoch: 68 [23808/50048]	Loss: 0.1206
Training Epoch: 68 [23936/50048]	Loss: 0.1555
Training Epoch: 68 [24064/50048]	Loss: 0.2044
Training Epoch: 68 [24192/50048]	Loss: 0.1344
Training Epoch: 68 [24320/50048]	Loss: 0.1122
Training Epoch: 68 [24448/50048]	Loss: 0.1607
Training Epoch: 68 [24576/50048]	Loss: 0.1140
Training Epoch: 68 [24704/50048]	Loss: 0.1278
Training Epoch: 68 [24832/50048]	Loss: 0.2196
Training Epoch: 68 [24960/50048]	Loss: 0.1305
Training Epoch: 68 [25088/50048]	Loss: 0.0917
Training Epoch: 68 [25216/50048]	Loss: 0.1692
Training Epoch: 68 [25344/50048]	Loss: 0.1338
Training Epoch: 68 [25472/50048]	Loss: 0.2038
Training Epoch: 68 [25600/50048]	Loss: 0.1417
Training Epoch: 68 [25728/50048]	Loss: 0.1795
Training Epoch: 68 [25856/50048]	Loss: 0.1151
Training Epoch: 68 [25984/50048]	Loss: 0.1223
Training Epoch: 68 [26112/50048]	Loss: 0.1462
Training Epoch: 68 [26240/50048]	Loss: 0.1434
Training Epoch: 68 [26368/50048]	Loss: 0.2579
Training Epoch: 68 [26496/50048]	Loss: 0.1628
Training Epoch: 68 [26624/50048]	Loss: 0.0874
Training Epoch: 68 [26752/50048]	Loss: 0.1670
Training Epoch: 68 [26880/50048]	Loss: 0.0743
Training Epoch: 68 [27008/50048]	Loss: 0.2071
Training Epoch: 68 [27136/50048]	Loss: 0.1725
Training Epoch: 68 [27264/50048]	Loss: 0.1466
Training Epoch: 68 [27392/50048]	Loss: 0.1493
Training Epoch: 68 [27520/50048]	Loss: 0.1280
Training Epoch: 68 [27648/50048]	Loss: 0.1187
Training Epoch: 68 [27776/50048]	Loss: 0.1556
Training Epoch: 68 [27904/50048]	Loss: 0.1465
Training Epoch: 68 [28032/50048]	Loss: 0.0926
Training Epoch: 68 [28160/50048]	Loss: 0.2967
Training Epoch: 68 [28288/50048]	Loss: 0.0788
Training Epoch: 68 [28416/50048]	Loss: 0.1390
Training Epoch: 68 [28544/50048]	Loss: 0.1842
Training Epoch: 68 [28672/50048]	Loss: 0.1357
Training Epoch: 68 [28800/50048]	Loss: 0.1134
Training Epoch: 68 [28928/50048]	Loss: 0.1143
Training Epoch: 68 [29056/50048]	Loss: 0.1368
Training Epoch: 68 [29184/50048]	Loss: 0.1822
Training Epoch: 68 [29312/50048]	Loss: 0.2138
Training Epoch: 68 [29440/50048]	Loss: 0.1634
Training Epoch: 68 [29568/50048]	Loss: 0.1716
Training Epoch: 68 [29696/50048]	Loss: 0.2178
Training Epoch: 68 [29824/50048]	Loss: 0.2012
Training Epoch: 68 [29952/50048]	Loss: 0.2212
Training Epoch: 68 [30080/50048]	Loss: 0.0808
Training Epoch: 68 [30208/50048]	Loss: 0.2611
Training Epoch: 68 [30336/50048]	Loss: 0.2181
Training Epoch: 68 [30464/50048]	Loss: 0.2811
Training Epoch: 68 [30592/50048]	Loss: 0.1746
Training Epoch: 68 [30720/50048]	Loss: 0.1527
Training Epoch: 68 [30848/50048]	Loss: 0.1148
Training Epoch: 68 [30976/50048]	Loss: 0.0871
Training Epoch: 68 [31104/50048]	Loss: 0.1071
Training Epoch: 68 [31232/50048]	Loss: 0.1354
Training Epoch: 68 [31360/50048]	Loss: 0.1157
Training Epoch: 68 [31488/50048]	Loss: 0.1177
Training Epoch: 68 [31616/50048]	Loss: 0.0969
Training Epoch: 68 [31744/50048]	Loss: 0.1629
Training Epoch: 68 [31872/50048]	Loss: 0.1989
Training Epoch: 68 [32000/50048]	Loss: 0.1911
Training Epoch: 68 [32128/50048]	Loss: 0.0843
Training Epoch: 68 [32256/50048]	Loss: 0.1640
Training Epoch: 68 [32384/50048]	Loss: 0.1250
Training Epoch: 68 [32512/50048]	Loss: 0.1854
Training Epoch: 68 [32640/50048]	Loss: 0.1865
Training Epoch: 68 [32768/50048]	Loss: 0.1830
Training Epoch: 68 [32896/50048]	Loss: 0.1855
Training Epoch: 68 [33024/50048]	Loss: 0.1554
Training Epoch: 68 [33152/50048]	Loss: 0.2638
Training Epoch: 68 [33280/50048]	Loss: 0.1429
Training Epoch: 68 [33408/50048]	Loss: 0.1101
Training Epoch: 68 [33536/50048]	Loss: 0.1331
Training Epoch: 68 [33664/50048]	Loss: 0.1475
Training Epoch: 68 [33792/50048]	Loss: 0.1552
Training Epoch: 68 [33920/50048]	Loss: 0.1548
Training Epoch: 68 [34048/50048]	Loss: 0.1236
Training Epoch: 68 [34176/50048]	Loss: 0.2050
Training Epoch: 68 [34304/50048]	Loss: 0.2273
Training Epoch: 68 [34432/50048]	Loss: 0.1553
Training Epoch: 68 [34560/50048]	Loss: 0.1263
Training Epoch: 68 [34688/50048]	Loss: 0.2099
Training Epoch: 68 [34816/50048]	Loss: 0.1668
Training Epoch: 68 [34944/50048]	Loss: 0.1530
Training Epoch: 68 [35072/50048]	Loss: 0.2245
Training Epoch: 68 [35200/50048]	Loss: 0.1749
Training Epoch: 68 [35328/50048]	Loss: 0.2204
Training Epoch: 68 [35456/50048]	Loss: 0.2579
Training Epoch: 68 [35584/50048]	Loss: 0.1558
Training Epoch: 68 [35712/50048]	Loss: 0.1437
Training Epoch: 68 [35840/50048]	Loss: 0.1893
Training Epoch: 68 [35968/50048]	Loss: 0.1626
Training Epoch: 68 [36096/50048]	Loss: 0.1245
Training Epoch: 68 [36224/50048]	Loss: 0.2572
Training Epoch: 68 [36352/50048]	Loss: 0.1758
Training Epoch: 68 [36480/50048]	Loss: 0.1537
Training Epoch: 68 [36608/50048]	Loss: 0.1988
Training Epoch: 68 [36736/50048]	Loss: 0.1308
Training Epoch: 68 [36864/50048]	Loss: 0.1414
Training Epoch: 68 [36992/50048]	Loss: 0.2683
Training Epoch: 68 [37120/50048]	Loss: 0.2171
Training Epoch: 68 [37248/50048]	Loss: 0.2368
Training Epoch: 68 [37376/50048]	Loss: 0.1733
Training Epoch: 68 [37504/50048]	Loss: 0.1340
Training Epoch: 68 [37632/50048]	Loss: 0.3563
Training Epoch: 68 [37760/50048]	Loss: 0.1552
Training Epoch: 68 [37888/50048]	Loss: 0.1319
Training Epoch: 68 [38016/50048]	Loss: 0.1205
Training Epoch: 68 [38144/50048]	Loss: 0.1099
Training Epoch: 68 [38272/50048]	Loss: 0.0991
Training Epoch: 68 [38400/50048]	Loss: 0.1114
Training Epoch: 68 [38528/50048]	Loss: 0.1663
Training Epoch: 68 [38656/50048]	Loss: 0.1163
Training Epoch: 68 [38784/50048]	Loss: 0.0679
Training Epoch: 68 [38912/50048]	Loss: 0.2891
Training Epoch: 68 [39040/50048]	Loss: 0.2853
Training Epoch: 68 [39168/50048]	Loss: 0.1381
Training Epoch: 68 [39296/50048]	Loss: 0.1823
Training Epoch: 68 [39424/50048]	Loss: 0.1069
Training Epoch: 68 [39552/50048]	Loss: 0.1543
Training Epoch: 68 [39680/50048]	Loss: 0.2075
Training Epoch: 68 [39808/50048]	Loss: 0.1737
Training Epoch: 68 [39936/50048]	Loss: 0.2262
Training Epoch: 68 [40064/50048]	Loss: 0.2706
Training Epoch: 68 [40192/50048]	Loss: 0.1714
Training Epoch: 68 [40320/50048]	Loss: 0.1429
Training Epoch: 68 [40448/50048]	Loss: 0.0784
Training Epoch: 68 [40576/50048]	Loss: 0.1303
Training Epoch: 68 [40704/50048]	Loss: 0.2058
Training Epoch: 68 [40832/50048]	Loss: 0.2593
Training Epoch: 68 [40960/50048]	Loss: 0.1607
Training Epoch: 68 [41088/50048]	Loss: 0.3348
Training Epoch: 68 [41216/50048]	Loss: 0.1054
Training Epoch: 68 [41344/50048]	Loss: 0.1368
Training Epoch: 68 [41472/50048]	Loss: 0.1269
Training Epoch: 68 [41600/50048]	Loss: 0.2890
Training Epoch: 68 [41728/50048]	Loss: 0.1274
Training Epoch: 68 [41856/50048]	Loss: 0.2507
Training Epoch: 68 [41984/50048]	Loss: 0.1947
Training Epoch: 68 [42112/50048]	Loss: 0.2606
Training Epoch: 68 [42240/50048]	Loss: 0.1292
Training Epoch: 68 [42368/50048]	Loss: 0.2485
Training Epoch: 68 [42496/50048]	Loss: 0.1909
Training Epoch: 68 [42624/50048]	Loss: 0.1307
Training Epoch: 68 [42752/50048]	Loss: 0.1405
Training Epoch: 68 [42880/50048]	Loss: 0.1262
Training Epoch: 68 [43008/50048]	Loss: 0.1063
Training Epoch: 68 [43136/50048]	Loss: 0.1969
Training Epoch: 68 [43264/50048]	Loss: 0.1908
Training Epoch: 68 [43392/50048]	Loss: 0.1400
Training Epoch: 68 [43520/50048]	Loss: 0.0853
Training Epoch: 68 [43648/50048]	Loss: 0.0985
Training Epoch: 68 [43776/50048]	Loss: 0.1665
Training Epoch: 68 [43904/50048]	Loss: 0.2478
Training Epoch: 68 [44032/50048]	Loss: 0.1497
Training Epoch: 68 [44160/50048]	Loss: 0.0673
Training Epoch: 68 [44288/50048]	Loss: 0.1575
Training Epoch: 68 [44416/50048]	Loss: 0.1888
Training Epoch: 68 [44544/50048]	Loss: 0.1087
Training Epoch: 68 [44672/50048]	Loss: 0.1616
Training Epoch: 68 [44800/50048]	Loss: 0.2304
Training Epoch: 68 [44928/50048]	Loss: 0.2445
Training Epoch: 68 [45056/50048]	Loss: 0.0909
Training Epoch: 68 [45184/50048]	Loss: 0.1286
Training Epoch: 68 [45312/50048]	Loss: 0.2305
Training Epoch: 68 [45440/50048]	Loss: 0.2327
Training Epoch: 68 [45568/50048]	Loss: 0.3144
Training Epoch: 68 [45696/50048]	Loss: 0.1860
2022-12-06 07:51:34,854 [ZeusDataLoader(train)] train epoch 69 done: time=86.42 energy=10497.69
2022-12-06 07:51:34,855 [ZeusDataLoader(eval)] Epoch 69 begin.
Training Epoch: 68 [45824/50048]	Loss: 0.1545
Training Epoch: 68 [45952/50048]	Loss: 0.0856
Training Epoch: 68 [46080/50048]	Loss: 0.1241
Training Epoch: 68 [46208/50048]	Loss: 0.1301
Training Epoch: 68 [46336/50048]	Loss: 0.1000
Training Epoch: 68 [46464/50048]	Loss: 0.1444
Training Epoch: 68 [46592/50048]	Loss: 0.1735
Training Epoch: 68 [46720/50048]	Loss: 0.1405
Training Epoch: 68 [46848/50048]	Loss: 0.1130
Training Epoch: 68 [46976/50048]	Loss: 0.0881
Training Epoch: 68 [47104/50048]	Loss: 0.1582
Training Epoch: 68 [47232/50048]	Loss: 0.1516
Training Epoch: 68 [47360/50048]	Loss: 0.1728
Training Epoch: 68 [47488/50048]	Loss: 0.2073
Training Epoch: 68 [47616/50048]	Loss: 0.1625
Training Epoch: 68 [47744/50048]	Loss: 0.2151
Training Epoch: 68 [47872/50048]	Loss: 0.2268
Training Epoch: 68 [48000/50048]	Loss: 0.1824
Training Epoch: 68 [48128/50048]	Loss: 0.1996
Training Epoch: 68 [48256/50048]	Loss: 0.2332
Training Epoch: 68 [48384/50048]	Loss: 0.1877
Training Epoch: 68 [48512/50048]	Loss: 0.1378
Training Epoch: 68 [48640/50048]	Loss: 0.2106
Training Epoch: 68 [48768/50048]	Loss: 0.1989
Training Epoch: 68 [48896/50048]	Loss: 0.1437
Training Epoch: 68 [49024/50048]	Loss: 0.2885
Training Epoch: 68 [49152/50048]	Loss: 0.1378
Training Epoch: 68 [49280/50048]	Loss: 0.1517
Training Epoch: 68 [49408/50048]	Loss: 0.1660
Training Epoch: 68 [49536/50048]	Loss: 0.2187
Training Epoch: 68 [49664/50048]	Loss: 0.1906
Training Epoch: 68 [49792/50048]	Loss: 0.0628
Training Epoch: 68 [49920/50048]	Loss: 0.1871
Training Epoch: 68 [50048/50048]	Loss: 0.1853
2022-12-06 12:51:38.530 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:51:38,540 [ZeusDataLoader(eval)] eval epoch 69 done: time=3.68 energy=441.16
2022-12-06 07:51:38,540 [ZeusDataLoader(train)] Up to epoch 69: time=6223.40, energy=755454.57, cost=922275.12
2022-12-06 07:51:38,540 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:51:38,540 [ZeusDataLoader(train)] Expected next epoch: time=6313.20, energy=766252.58, cost=935531.50
2022-12-06 07:51:38,541 [ZeusDataLoader(train)] Epoch 70 begin.
Validation Epoch: 68, Average loss: 0.0168, Accuracy: 0.6327
2022-12-06 07:51:38,754 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:51:38,755 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:51:38.761 [ZeusMonitor] Monitor started.
2022-12-06 12:51:38.761 [ZeusMonitor] Running indefinitely. 2022-12-06 12:51:38.761 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:51:38.761 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e70+gpu0.power.log
Training Epoch: 69 [128/50048]	Loss: 0.0729
Training Epoch: 69 [256/50048]	Loss: 0.0952
Training Epoch: 69 [384/50048]	Loss: 0.1601
Training Epoch: 69 [512/50048]	Loss: 0.1027
Training Epoch: 69 [640/50048]	Loss: 0.0918
Training Epoch: 69 [768/50048]	Loss: 0.2441
Training Epoch: 69 [896/50048]	Loss: 0.1488
Training Epoch: 69 [1024/50048]	Loss: 0.1455
Training Epoch: 69 [1152/50048]	Loss: 0.2175
Training Epoch: 69 [1280/50048]	Loss: 0.0740
Training Epoch: 69 [1408/50048]	Loss: 0.2292
Training Epoch: 69 [1536/50048]	Loss: 0.1394
Training Epoch: 69 [1664/50048]	Loss: 0.2045
Training Epoch: 69 [1792/50048]	Loss: 0.1519
Training Epoch: 69 [1920/50048]	Loss: 0.1216
Training Epoch: 69 [2048/50048]	Loss: 0.1524
Training Epoch: 69 [2176/50048]	Loss: 0.2083
Training Epoch: 69 [2304/50048]	Loss: 0.1318
Training Epoch: 69 [2432/50048]	Loss: 0.1883
Training Epoch: 69 [2560/50048]	Loss: 0.2172
Training Epoch: 69 [2688/50048]	Loss: 0.0986
Training Epoch: 69 [2816/50048]	Loss: 0.1713
Training Epoch: 69 [2944/50048]	Loss: 0.1301
Training Epoch: 69 [3072/50048]	Loss: 0.2351
Training Epoch: 69 [3200/50048]	Loss: 0.1136
Training Epoch: 69 [3328/50048]	Loss: 0.1560
Training Epoch: 69 [3456/50048]	Loss: 0.1148
Training Epoch: 69 [3584/50048]	Loss: 0.1078
Training Epoch: 69 [3712/50048]	Loss: 0.1085
Training Epoch: 69 [3840/50048]	Loss: 0.0879
Training Epoch: 69 [3968/50048]	Loss: 0.1166
Training Epoch: 69 [4096/50048]	Loss: 0.1470
Training Epoch: 69 [4224/50048]	Loss: 0.0972
Training Epoch: 69 [4352/50048]	Loss: 0.1457
Training Epoch: 69 [4480/50048]	Loss: 0.0859
Training Epoch: 69 [4608/50048]	Loss: 0.1259
Training Epoch: 69 [4736/50048]	Loss: 0.1281
Training Epoch: 69 [4864/50048]	Loss: 0.2302
Training Epoch: 69 [4992/50048]	Loss: 0.0963
Training Epoch: 69 [5120/50048]	Loss: 0.1707
Training Epoch: 69 [5248/50048]	Loss: 0.0662
Training Epoch: 69 [5376/50048]	Loss: 0.1403
Training Epoch: 69 [5504/50048]	Loss: 0.1571
Training Epoch: 69 [5632/50048]	Loss: 0.2130
Training Epoch: 69 [5760/50048]	Loss: 0.1320
Training Epoch: 69 [5888/50048]	Loss: 0.1271
Training Epoch: 69 [6016/50048]	Loss: 0.1059
Training Epoch: 69 [6144/50048]	Loss: 0.1525
Training Epoch: 69 [6272/50048]	Loss: 0.1993
Training Epoch: 69 [6400/50048]	Loss: 0.0802
Training Epoch: 69 [6528/50048]	Loss: 0.1459
Training Epoch: 69 [6656/50048]	Loss: 0.0971
Training Epoch: 69 [6784/50048]	Loss: 0.0942
Training Epoch: 69 [6912/50048]	Loss: 0.1360
Training Epoch: 69 [7040/50048]	Loss: 0.1442
Training Epoch: 69 [7168/50048]	Loss: 0.1648
Training Epoch: 69 [7296/50048]	Loss: 0.1407
Training Epoch: 69 [7424/50048]	Loss: 0.1494
Training Epoch: 69 [7552/50048]	Loss: 0.0907
Training Epoch: 69 [7680/50048]	Loss: 0.1038
Training Epoch: 69 [7808/50048]	Loss: 0.1388
Training Epoch: 69 [7936/50048]	Loss: 0.1052
Training Epoch: 69 [8064/50048]	Loss: 0.1203
Training Epoch: 69 [8192/50048]	Loss: 0.0846
Training Epoch: 69 [8320/50048]	Loss: 0.0961
Training Epoch: 69 [8448/50048]	Loss: 0.1863
Training Epoch: 69 [8576/50048]	Loss: 0.1516
Training Epoch: 69 [8704/50048]	Loss: 0.1391
Training Epoch: 69 [8832/50048]	Loss: 0.1204
Training Epoch: 69 [8960/50048]	Loss: 0.1045
Training Epoch: 69 [9088/50048]	Loss: 0.1543
Training Epoch: 69 [9216/50048]	Loss: 0.1221
Training Epoch: 69 [9344/50048]	Loss: 0.1453
Training Epoch: 69 [9472/50048]	Loss: 0.1925
Training Epoch: 69 [9600/50048]	Loss: 0.2339
Training Epoch: 69 [9728/50048]	Loss: 0.1707
Training Epoch: 69 [9856/50048]	Loss: 0.1045
Training Epoch: 69 [9984/50048]	Loss: 0.1122
Training Epoch: 69 [10112/50048]	Loss: 0.2107
Training Epoch: 69 [10240/50048]	Loss: 0.1765
Training Epoch: 69 [10368/50048]	Loss: 0.1435
Training Epoch: 69 [10496/50048]	Loss: 0.2353
Training Epoch: 69 [10624/50048]	Loss: 0.1483
Training Epoch: 69 [10752/50048]	Loss: 0.0723
Training Epoch: 69 [10880/50048]	Loss: 0.1445
Training Epoch: 69 [11008/50048]	Loss: 0.1026
Training Epoch: 69 [11136/50048]	Loss: 0.2473
Training Epoch: 69 [11264/50048]	Loss: 0.0871
Training Epoch: 69 [11392/50048]	Loss: 0.1239
Training Epoch: 69 [11520/50048]	Loss: 0.1250
Training Epoch: 69 [11648/50048]	Loss: 0.0988
Training Epoch: 69 [11776/50048]	Loss: 0.1395
Training Epoch: 69 [11904/50048]	Loss: 0.1569
Training Epoch: 69 [12032/50048]	Loss: 0.1118
Training Epoch: 69 [12160/50048]	Loss: 0.1443
Training Epoch: 69 [12288/50048]	Loss: 0.1482
Training Epoch: 69 [12416/50048]	Loss: 0.1352
Training Epoch: 69 [12544/50048]	Loss: 0.1255
Training Epoch: 69 [12672/50048]	Loss: 0.1563
Training Epoch: 69 [12800/50048]	Loss: 0.1236
Training Epoch: 69 [12928/50048]	Loss: 0.1552
Training Epoch: 69 [13056/50048]	Loss: 0.1219
Training Epoch: 69 [13184/50048]	Loss: 0.1371
Training Epoch: 69 [13312/50048]	Loss: 0.1261
Training Epoch: 69 [13440/50048]	Loss: 0.1547
Training Epoch: 69 [13568/50048]	Loss: 0.2287
Training Epoch: 69 [13696/50048]	Loss: 0.1627
Training Epoch: 69 [13824/50048]	Loss: 0.1186
Training Epoch: 69 [13952/50048]	Loss: 0.1759
Training Epoch: 69 [14080/50048]	Loss: 0.0908
Training Epoch: 69 [14208/50048]	Loss: 0.1800
Training Epoch: 69 [14336/50048]	Loss: 0.1632
Training Epoch: 69 [14464/50048]	Loss: 0.1630
Training Epoch: 69 [14592/50048]	Loss: 0.1107
Training Epoch: 69 [14720/50048]	Loss: 0.2179
Training Epoch: 69 [14848/50048]	Loss: 0.1645
Training Epoch: 69 [14976/50048]	Loss: 0.2125
Training Epoch: 69 [15104/50048]	Loss: 0.1723
Training Epoch: 69 [15232/50048]	Loss: 0.1277
Training Epoch: 69 [15360/50048]	Loss: 0.1697
Training Epoch: 69 [15488/50048]	Loss: 0.1835
Training Epoch: 69 [15616/50048]	Loss: 0.0755
Training Epoch: 69 [15744/50048]	Loss: 0.1332
Training Epoch: 69 [15872/50048]	Loss: 0.1353
Training Epoch: 69 [16000/50048]	Loss: 0.1789
Training Epoch: 69 [16128/50048]	Loss: 0.1954
Training Epoch: 69 [16256/50048]	Loss: 0.1285
Training Epoch: 69 [16384/50048]	Loss: 0.0931
Training Epoch: 69 [16512/50048]	Loss: 0.1397
Training Epoch: 69 [16640/50048]	Loss: 0.1152
Training Epoch: 69 [16768/50048]	Loss: 0.1859
Training Epoch: 69 [16896/50048]	Loss: 0.1831
Training Epoch: 69 [17024/50048]	Loss: 0.1698
Training Epoch: 69 [17152/50048]	Loss: 0.2189
Training Epoch: 69 [17280/50048]	Loss: 0.1849
Training Epoch: 69 [17408/50048]	Loss: 0.2383
Training Epoch: 69 [17536/50048]	Loss: 0.1350
Training Epoch: 69 [17664/50048]	Loss: 0.1156
Training Epoch: 69 [17792/50048]	Loss: 0.0970
Training Epoch: 69 [17920/50048]	Loss: 0.0816
Training Epoch: 69 [18048/50048]	Loss: 0.1251
Training Epoch: 69 [18176/50048]	Loss: 0.1570
Training Epoch: 69 [18304/50048]	Loss: 0.1234
Training Epoch: 69 [18432/50048]	Loss: 0.0805
Training Epoch: 69 [18560/50048]	Loss: 0.1260
Training Epoch: 69 [18688/50048]	Loss: 0.1609
Training Epoch: 69 [18816/50048]	Loss: 0.1761
Training Epoch: 69 [18944/50048]	Loss: 0.1202
Training Epoch: 69 [19072/50048]	Loss: 0.1416
Training Epoch: 69 [19200/50048]	Loss: 0.2022
Training Epoch: 69 [19328/50048]	Loss: 0.1436
Training Epoch: 69 [19456/50048]	Loss: 0.1102
Training Epoch: 69 [19584/50048]	Loss: 0.2371
Training Epoch: 69 [19712/50048]	Loss: 0.1299
Training Epoch: 69 [19840/50048]	Loss: 0.1653
Training Epoch: 69 [19968/50048]	Loss: 0.1217
Training Epoch: 69 [20096/50048]	Loss: 0.0874
Training Epoch: 69 [20224/50048]	Loss: 0.1064
Training Epoch: 69 [20352/50048]	Loss: 0.1444
Training Epoch: 69 [20480/50048]	Loss: 0.1688
Training Epoch: 69 [20608/50048]	Loss: 0.1184
Training Epoch: 69 [20736/50048]	Loss: 0.0547
Training Epoch: 69 [20864/50048]	Loss: 0.0917
Training Epoch: 69 [20992/50048]	Loss: 0.2451
Training Epoch: 69 [21120/50048]	Loss: 0.1892
Training Epoch: 69 [21248/50048]	Loss: 0.1482
Training Epoch: 69 [21376/50048]	Loss: 0.1224
Training Epoch: 69 [21504/50048]	Loss: 0.1489
Training Epoch: 69 [21632/50048]	Loss: 0.1467
Training Epoch: 69 [21760/50048]	Loss: 0.1227
Training Epoch: 69 [21888/50048]	Loss: 0.1447
Training Epoch: 69 [22016/50048]	Loss: 0.2253
Training Epoch: 69 [22144/50048]	Loss: 0.1776
Training Epoch: 69 [22272/50048]	Loss: 0.1885
Training Epoch: 69 [22400/50048]	Loss: 0.1055
Training Epoch: 69 [22528/50048]	Loss: 0.1604
Training Epoch: 69 [22656/50048]	Loss: 0.1188
Training Epoch: 69 [22784/50048]	Loss: 0.2042
Training Epoch: 69 [22912/50048]	Loss: 0.0995
Training Epoch: 69 [23040/50048]	Loss: 0.1925
Training Epoch: 69 [23168/50048]	Loss: 0.1144
Training Epoch: 69 [23296/50048]	Loss: 0.1649
Training Epoch: 69 [23424/50048]	Loss: 0.2160
Training Epoch: 69 [23552/50048]	Loss: 0.1555
Training Epoch: 69 [23680/50048]	Loss: 0.1071
Training Epoch: 69 [23808/50048]	Loss: 0.1945
Training Epoch: 69 [23936/50048]	Loss: 0.1841
Training Epoch: 69 [24064/50048]	Loss: 0.0579
Training Epoch: 69 [24192/50048]	Loss: 0.1230
Training Epoch: 69 [24320/50048]	Loss: 0.2415
Training Epoch: 69 [24448/50048]	Loss: 0.1676
Training Epoch: 69 [24576/50048]	Loss: 0.1126
Training Epoch: 69 [24704/50048]	Loss: 0.1475
Training Epoch: 69 [24832/50048]	Loss: 0.1910
Training Epoch: 69 [24960/50048]	Loss: 0.1180
Training Epoch: 69 [25088/50048]	Loss: 0.2023
Training Epoch: 69 [25216/50048]	Loss: 0.2301
Training Epoch: 69 [25344/50048]	Loss: 0.1880
Training Epoch: 69 [25472/50048]	Loss: 0.1914
Training Epoch: 69 [25600/50048]	Loss: 0.1480
Training Epoch: 69 [25728/50048]	Loss: 0.1347
Training Epoch: 69 [25856/50048]	Loss: 0.1344
Training Epoch: 69 [25984/50048]	Loss: 0.1465
Training Epoch: 69 [26112/50048]	Loss: 0.1639
Training Epoch: 69 [26240/50048]	Loss: 0.1242
Training Epoch: 69 [26368/50048]	Loss: 0.0898
Training Epoch: 69 [26496/50048]	Loss: 0.0850
Training Epoch: 69 [26624/50048]	Loss: 0.1313
Training Epoch: 69 [26752/50048]	Loss: 0.2580
Training Epoch: 69 [26880/50048]	Loss: 0.0949
Training Epoch: 69 [27008/50048]	Loss: 0.1153
Training Epoch: 69 [27136/50048]	Loss: 0.1204
Training Epoch: 69 [27264/50048]	Loss: 0.1927
Training Epoch: 69 [27392/50048]	Loss: 0.1719
Training Epoch: 69 [27520/50048]	Loss: 0.2101
Training Epoch: 69 [27648/50048]	Loss: 0.1384
Training Epoch: 69 [27776/50048]	Loss: 0.1717
Training Epoch: 69 [27904/50048]	Loss: 0.1693
Training Epoch: 69 [28032/50048]	Loss: 0.1104
Training Epoch: 69 [28160/50048]	Loss: 0.1749
Training Epoch: 69 [28288/50048]	Loss: 0.2438
Training Epoch: 69 [28416/50048]	Loss: 0.1209
Training Epoch: 69 [28544/50048]	Loss: 0.2003
Training Epoch: 69 [28672/50048]	Loss: 0.1288
Training Epoch: 69 [28800/50048]	Loss: 0.1100
Training Epoch: 69 [28928/50048]	Loss: 0.0848
Training Epoch: 69 [29056/50048]	Loss: 0.1283
Training Epoch: 69 [29184/50048]	Loss: 0.1130
Training Epoch: 69 [29312/50048]	Loss: 0.1221
Training Epoch: 69 [29440/50048]	Loss: 0.1224
Training Epoch: 69 [29568/50048]	Loss: 0.2058
Training Epoch: 69 [29696/50048]	Loss: 0.1812
Training Epoch: 69 [29824/50048]	Loss: 0.1500
Training Epoch: 69 [29952/50048]	Loss: 0.2174
Training Epoch: 69 [30080/50048]	Loss: 0.1568
Training Epoch: 69 [30208/50048]	Loss: 0.1649
Training Epoch: 69 [30336/50048]	Loss: 0.1390
Training Epoch: 69 [30464/50048]	Loss: 0.1615
Training Epoch: 69 [30592/50048]	Loss: 0.1690
Training Epoch: 69 [30720/50048]	Loss: 0.0710
Training Epoch: 69 [30848/50048]	Loss: 0.2132
Training Epoch: 69 [30976/50048]	Loss: 0.1225
Training Epoch: 69 [31104/50048]	Loss: 0.1492
Training Epoch: 69 [31232/50048]	Loss: 0.1327
Training Epoch: 69 [31360/50048]	Loss: 0.2199
Training Epoch: 69 [31488/50048]	Loss: 0.2234
Training Epoch: 69 [31616/50048]	Loss: 0.2335
Training Epoch: 69 [31744/50048]	Loss: 0.2569
Training Epoch: 69 [31872/50048]	Loss: 0.1060
Training Epoch: 69 [32000/50048]	Loss: 0.1018
Training Epoch: 69 [32128/50048]	Loss: 0.1740
Training Epoch: 69 [32256/50048]	Loss: 0.1303
Training Epoch: 69 [32384/50048]	Loss: 0.2112
Training Epoch: 69 [32512/50048]	Loss: 0.1746
Training Epoch: 69 [32640/50048]	Loss: 0.1041
Training Epoch: 69 [32768/50048]	Loss: 0.1109
Training Epoch: 69 [32896/50048]	Loss: 0.3286
Training Epoch: 69 [33024/50048]	Loss: 0.1356
Training Epoch: 69 [33152/50048]	Loss: 0.4108
Training Epoch: 69 [33280/50048]	Loss: 0.1203
Training Epoch: 69 [33408/50048]	Loss: 0.0860
Training Epoch: 69 [33536/50048]	Loss: 0.1897
Training Epoch: 69 [33664/50048]	Loss: 0.1956
Training Epoch: 69 [33792/50048]	Loss: 0.0973
Training Epoch: 69 [33920/50048]	Loss: 0.1663
Training Epoch: 69 [34048/50048]	Loss: 0.0947
Training Epoch: 69 [34176/50048]	Loss: 0.2098
Training Epoch: 69 [34304/50048]	Loss: 0.2070
Training Epoch: 69 [34432/50048]	Loss: 0.1367
Training Epoch: 69 [34560/50048]	Loss: 0.1918
Training Epoch: 69 [34688/50048]	Loss: 0.0851
Training Epoch: 69 [34816/50048]	Loss: 0.1594
Training Epoch: 69 [34944/50048]	Loss: 0.1247
Training Epoch: 69 [35072/50048]	Loss: 0.1849
Training Epoch: 69 [35200/50048]	Loss: 0.1156
Training Epoch: 69 [35328/50048]	Loss: 0.1706
Training Epoch: 69 [35456/50048]	Loss: 0.0981
Training Epoch: 69 [35584/50048]	Loss: 0.1393
Training Epoch: 69 [35712/50048]	Loss: 0.2042
Training Epoch: 69 [35840/50048]	Loss: 0.2387
Training Epoch: 69 [35968/50048]	Loss: 0.1850
Training Epoch: 69 [36096/50048]	Loss: 0.1822
Training Epoch: 69 [36224/50048]	Loss: 0.1726
Training Epoch: 69 [36352/50048]	Loss: 0.1432
Training Epoch: 69 [36480/50048]	Loss: 0.1520
Training Epoch: 69 [36608/50048]	Loss: 0.0763
Training Epoch: 69 [36736/50048]	Loss: 0.0887
Training Epoch: 69 [36864/50048]	Loss: 0.1698
Training Epoch: 69 [36992/50048]	Loss: 0.2389
Training Epoch: 69 [37120/50048]	Loss: 0.1332
Training Epoch: 69 [37248/50048]	Loss: 0.2050
Training Epoch: 69 [37376/50048]	Loss: 0.1474
Training Epoch: 69 [37504/50048]	Loss: 0.1722
Training Epoch: 69 [37632/50048]	Loss: 0.1616
Training Epoch: 69 [37760/50048]	Loss: 0.1676
Training Epoch: 69 [37888/50048]	Loss: 0.2169
Training Epoch: 69 [38016/50048]	Loss: 0.1065
Training Epoch: 69 [38144/50048]	Loss: 0.1385
Training Epoch: 69 [38272/50048]	Loss: 0.2176
Training Epoch: 69 [38400/50048]	Loss: 0.1565
Training Epoch: 69 [38528/50048]	Loss: 0.1952
Training Epoch: 69 [38656/50048]	Loss: 0.1286
Training Epoch: 69 [38784/50048]	Loss: 0.1742
Training Epoch: 69 [38912/50048]	Loss: 0.1400
Training Epoch: 69 [39040/50048]	Loss: 0.1215
Training Epoch: 69 [39168/50048]	Loss: 0.1436
Training Epoch: 69 [39296/50048]	Loss: 0.0981
Training Epoch: 69 [39424/50048]	Loss: 0.0916
Training Epoch: 69 [39552/50048]	Loss: 0.1824
Training Epoch: 69 [39680/50048]	Loss: 0.1860
Training Epoch: 69 [39808/50048]	Loss: 0.1207
Training Epoch: 69 [39936/50048]	Loss: 0.1920
Training Epoch: 69 [40064/50048]	Loss: 0.1689
Training Epoch: 69 [40192/50048]	Loss: 0.1023
Training Epoch: 69 [40320/50048]	Loss: 0.1113
Training Epoch: 69 [40448/50048]	Loss: 0.1097
Training Epoch: 69 [40576/50048]	Loss: 0.2073
Training Epoch: 69 [40704/50048]	Loss: 0.1362
Training Epoch: 69 [40832/50048]	Loss: 0.1255
Training Epoch: 69 [40960/50048]	Loss: 0.1115
Training Epoch: 69 [41088/50048]	Loss: 0.1999
Training Epoch: 69 [41216/50048]	Loss: 0.1208
Training Epoch: 69 [41344/50048]	Loss: 0.1668
Training Epoch: 69 [41472/50048]	Loss: 0.1497
Training Epoch: 69 [41600/50048]	Loss: 0.2509
Training Epoch: 69 [41728/50048]	Loss: 0.1634
Training Epoch: 69 [41856/50048]	Loss: 0.1237
Training Epoch: 69 [41984/50048]	Loss: 0.1933
Training Epoch: 69 [42112/50048]	Loss: 0.1568
Training Epoch: 69 [42240/50048]	Loss: 0.1012
Training Epoch: 69 [42368/50048]	Loss: 0.1655
Training Epoch: 69 [42496/50048]	Loss: 0.1951
Training Epoch: 69 [42624/50048]	Loss: 0.1472
Training Epoch: 69 [42752/50048]	Loss: 0.2440
Training Epoch: 69 [42880/50048]	Loss: 0.1287
Training Epoch: 69 [43008/50048]	Loss: 0.1334
Training Epoch: 69 [43136/50048]	Loss: 0.0906
Training Epoch: 69 [43264/50048]	Loss: 0.1277
Training Epoch: 69 [43392/50048]	Loss: 0.1707
Training Epoch: 69 [43520/50048]	Loss: 0.2250
Training Epoch: 69 [43648/50048]	Loss: 0.2369
Training Epoch: 69 [43776/50048]	Loss: 0.1700
Training Epoch: 69 [43904/50048]	Loss: 0.0997
Training Epoch: 69 [44032/50048]	Loss: 0.1622
Training Epoch: 69 [44160/50048]	Loss: 0.1968
Training Epoch: 69 [44288/50048]	Loss: 0.1256
Training Epoch: 69 [44416/50048]	Loss: 0.2481
Training Epoch: 69 [44544/50048]	Loss: 0.1688
Training Epoch: 69 [44672/50048]	Loss: 0.1852
Training Epoch: 69 [44800/50048]	Loss: 0.1801
Training Epoch: 69 [44928/50048]	Loss: 0.1197
Training Epoch: 69 [45056/50048]	Loss: 0.1782
Training Epoch: 69 [45184/50048]	Loss: 0.1815
Training Epoch: 69 [45312/50048]	Loss: 0.1937
Training Epoch: 69 [45440/50048]	Loss: 0.1702
Training Epoch: 69 [45568/50048]	Loss: 0.1221
Training Epoch: 69 [45696/50048]	Loss: 0.1123
2022-12-06 07:53:05,000 [ZeusDataLoader(train)] train epoch 70 done: time=86.45 energy=10494.82
2022-12-06 07:53:05,002 [ZeusDataLoader(eval)] Epoch 70 begin.
Training Epoch: 69 [45824/50048]	Loss: 0.1105
Training Epoch: 69 [45952/50048]	Loss: 0.2191
Training Epoch: 69 [46080/50048]	Loss: 0.1642
Training Epoch: 69 [46208/50048]	Loss: 0.1554
Training Epoch: 69 [46336/50048]	Loss: 0.2684
Training Epoch: 69 [46464/50048]	Loss: 0.1819
Training Epoch: 69 [46592/50048]	Loss: 0.1808
Training Epoch: 69 [46720/50048]	Loss: 0.1892
Training Epoch: 69 [46848/50048]	Loss: 0.4039
Training Epoch: 69 [46976/50048]	Loss: 0.1336
Training Epoch: 69 [47104/50048]	Loss: 0.1516
Training Epoch: 69 [47232/50048]	Loss: 0.1356
Training Epoch: 69 [47360/50048]	Loss: 0.2581
Training Epoch: 69 [47488/50048]	Loss: 0.1443
Training Epoch: 69 [47616/50048]	Loss: 0.2600
Training Epoch: 69 [47744/50048]	Loss: 0.2216
Training Epoch: 69 [47872/50048]	Loss: 0.2583
Training Epoch: 69 [48000/50048]	Loss: 0.1844
Training Epoch: 69 [48128/50048]	Loss: 0.1351
Training Epoch: 69 [48256/50048]	Loss: 0.1656
Training Epoch: 69 [48384/50048]	Loss: 0.1408
Training Epoch: 69 [48512/50048]	Loss: 0.1431
Training Epoch: 69 [48640/50048]	Loss: 0.0979
Training Epoch: 69 [48768/50048]	Loss: 0.1913
Training Epoch: 69 [48896/50048]	Loss: 0.1600
Training Epoch: 69 [49024/50048]	Loss: 0.2317
Training Epoch: 69 [49152/50048]	Loss: 0.1390
Training Epoch: 69 [49280/50048]	Loss: 0.2358
Training Epoch: 69 [49408/50048]	Loss: 0.0978
Training Epoch: 69 [49536/50048]	Loss: 0.1722
Training Epoch: 69 [49664/50048]	Loss: 0.2072
Training Epoch: 69 [49792/50048]	Loss: 0.1049
Training Epoch: 69 [49920/50048]	Loss: 0.2168
Training Epoch: 69 [50048/50048]	Loss: 0.1589
2022-12-06 12:53:08.692 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:53:08,717 [ZeusDataLoader(eval)] eval epoch 70 done: time=3.71 energy=453.61
2022-12-06 07:53:08,717 [ZeusDataLoader(train)] Up to epoch 70: time=6313.56, energy=766403.00, cost=935637.93
2022-12-06 07:53:08,717 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:53:08,717 [ZeusDataLoader(train)] Expected next epoch: time=6403.36, energy=777201.01, cost=948894.31
2022-12-06 07:53:08,718 [ZeusDataLoader(train)] Epoch 71 begin.
Validation Epoch: 69, Average loss: 0.0168, Accuracy: 0.6375
2022-12-06 07:53:08,866 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:53:08,867 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:53:08.869 [ZeusMonitor] Monitor started.
2022-12-06 12:53:08.869 [ZeusMonitor] Running indefinitely. 2022-12-06 12:53:08.869 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:53:08.869 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e71+gpu0.power.log
Training Epoch: 70 [128/50048]	Loss: 0.1450
Training Epoch: 70 [256/50048]	Loss: 0.1548
Training Epoch: 70 [384/50048]	Loss: 0.2162
Training Epoch: 70 [512/50048]	Loss: 0.1253
Training Epoch: 70 [640/50048]	Loss: 0.1299
Training Epoch: 70 [768/50048]	Loss: 0.1261
Training Epoch: 70 [896/50048]	Loss: 0.2361
Training Epoch: 70 [1024/50048]	Loss: 0.1691
Training Epoch: 70 [1152/50048]	Loss: 0.1680
Training Epoch: 70 [1280/50048]	Loss: 0.1752
Training Epoch: 70 [1408/50048]	Loss: 0.1510
Training Epoch: 70 [1536/50048]	Loss: 0.0754
Training Epoch: 70 [1664/50048]	Loss: 0.1416
Training Epoch: 70 [1792/50048]	Loss: 0.1099
Training Epoch: 70 [1920/50048]	Loss: 0.1742
Training Epoch: 70 [2048/50048]	Loss: 0.2272
Training Epoch: 70 [2176/50048]	Loss: 0.0914
Training Epoch: 70 [2304/50048]	Loss: 0.1202
Training Epoch: 70 [2432/50048]	Loss: 0.2081
Training Epoch: 70 [2560/50048]	Loss: 0.1576
Training Epoch: 70 [2688/50048]	Loss: 0.1039
Training Epoch: 70 [2816/50048]	Loss: 0.0764
Training Epoch: 70 [2944/50048]	Loss: 0.2436
Training Epoch: 70 [3072/50048]	Loss: 0.1735
Training Epoch: 70 [3200/50048]	Loss: 0.1204
Training Epoch: 70 [3328/50048]	Loss: 0.1199
Training Epoch: 70 [3456/50048]	Loss: 0.1179
Training Epoch: 70 [3584/50048]	Loss: 0.1522
Training Epoch: 70 [3712/50048]	Loss: 0.1692
Training Epoch: 70 [3840/50048]	Loss: 0.0847
Training Epoch: 70 [3968/50048]	Loss: 0.0825
Training Epoch: 70 [4096/50048]	Loss: 0.1275
Training Epoch: 70 [4224/50048]	Loss: 0.1661
Training Epoch: 70 [4352/50048]	Loss: 0.1023
Training Epoch: 70 [4480/50048]	Loss: 0.2080
Training Epoch: 70 [4608/50048]	Loss: 0.1603
Training Epoch: 70 [4736/50048]	Loss: 0.1404
Training Epoch: 70 [4864/50048]	Loss: 0.1375
Training Epoch: 70 [4992/50048]	Loss: 0.1136
Training Epoch: 70 [5120/50048]	Loss: 0.2049
Training Epoch: 70 [5248/50048]	Loss: 0.0746
Training Epoch: 70 [5376/50048]	Loss: 0.1266
Training Epoch: 70 [5504/50048]	Loss: 0.1778
Training Epoch: 70 [5632/50048]	Loss: 0.1217
Training Epoch: 70 [5760/50048]	Loss: 0.1259
Training Epoch: 70 [5888/50048]	Loss: 0.0895
Training Epoch: 70 [6016/50048]	Loss: 0.1786
Training Epoch: 70 [6144/50048]	Loss: 0.0823
Training Epoch: 70 [6272/50048]	Loss: 0.1456
Training Epoch: 70 [6400/50048]	Loss: 0.1925
Training Epoch: 70 [6528/50048]	Loss: 0.1501
Training Epoch: 70 [6656/50048]	Loss: 0.1883
Training Epoch: 70 [6784/50048]	Loss: 0.0677
Training Epoch: 70 [6912/50048]	Loss: 0.1323
Training Epoch: 70 [7040/50048]	Loss: 0.1547
Training Epoch: 70 [7168/50048]	Loss: 0.1194
Training Epoch: 70 [7296/50048]	Loss: 0.1607
Training Epoch: 70 [7424/50048]	Loss: 0.1381
Training Epoch: 70 [7552/50048]	Loss: 0.1440
Training Epoch: 70 [7680/50048]	Loss: 0.1385
Training Epoch: 70 [7808/50048]	Loss: 0.1063
Training Epoch: 70 [7936/50048]	Loss: 0.1535
Training Epoch: 70 [8064/50048]	Loss: 0.0787
Training Epoch: 70 [8192/50048]	Loss: 0.1190
Training Epoch: 70 [8320/50048]	Loss: 0.1317
Training Epoch: 70 [8448/50048]	Loss: 0.0792
Training Epoch: 70 [8576/50048]	Loss: 0.1604
Training Epoch: 70 [8704/50048]	Loss: 0.1430
Training Epoch: 70 [8832/50048]	Loss: 0.0942
Training Epoch: 70 [8960/50048]	Loss: 0.1224
Training Epoch: 70 [9088/50048]	Loss: 0.1232
Training Epoch: 70 [9216/50048]	Loss: 0.1290
Training Epoch: 70 [9344/50048]	Loss: 0.1738
Training Epoch: 70 [9472/50048]	Loss: 0.0556
Training Epoch: 70 [9600/50048]	Loss: 0.0940
Training Epoch: 70 [9728/50048]	Loss: 0.0867
Training Epoch: 70 [9856/50048]	Loss: 0.1162
Training Epoch: 70 [9984/50048]	Loss: 0.1091
Training Epoch: 70 [10112/50048]	Loss: 0.1364
Training Epoch: 70 [10240/50048]	Loss: 0.1437
Training Epoch: 70 [10368/50048]	Loss: 0.1517
Training Epoch: 70 [10496/50048]	Loss: 0.1070
Training Epoch: 70 [10624/50048]	Loss: 0.1054
Training Epoch: 70 [10752/50048]	Loss: 0.1762
Training Epoch: 70 [10880/50048]	Loss: 0.1393
Training Epoch: 70 [11008/50048]	Loss: 0.0823
Training Epoch: 70 [11136/50048]	Loss: 0.1398
Training Epoch: 70 [11264/50048]	Loss: 0.0681
Training Epoch: 70 [11392/50048]	Loss: 0.1245
Training Epoch: 70 [11520/50048]	Loss: 0.1597
Training Epoch: 70 [11648/50048]	Loss: 0.2017
Training Epoch: 70 [11776/50048]	Loss: 0.1423
Training Epoch: 70 [11904/50048]	Loss: 0.0745
Training Epoch: 70 [12032/50048]	Loss: 0.0801
Training Epoch: 70 [12160/50048]	Loss: 0.0862
Training Epoch: 70 [12288/50048]	Loss: 0.1488
Training Epoch: 70 [12416/50048]	Loss: 0.0829
Training Epoch: 70 [12544/50048]	Loss: 0.1161
Training Epoch: 70 [12672/50048]	Loss: 0.1004
Training Epoch: 70 [12800/50048]	Loss: 0.2542
Training Epoch: 70 [12928/50048]	Loss: 0.1685
Training Epoch: 70 [13056/50048]	Loss: 0.1418
Training Epoch: 70 [13184/50048]	Loss: 0.1667
Training Epoch: 70 [13312/50048]	Loss: 0.1681
Training Epoch: 70 [13440/50048]	Loss: 0.1575
Training Epoch: 70 [13568/50048]	Loss: 0.1047
Training Epoch: 70 [13696/50048]	Loss: 0.1203
Training Epoch: 70 [13824/50048]	Loss: 0.1002
Training Epoch: 70 [13952/50048]	Loss: 0.1099
Training Epoch: 70 [14080/50048]	Loss: 0.1179
Training Epoch: 70 [14208/50048]	Loss: 0.2715
Training Epoch: 70 [14336/50048]	Loss: 0.1810
Training Epoch: 70 [14464/50048]	Loss: 0.1568
Training Epoch: 70 [14592/50048]	Loss: 0.1038
Training Epoch: 70 [14720/50048]	Loss: 0.1218
Training Epoch: 70 [14848/50048]	Loss: 0.1493
Training Epoch: 70 [14976/50048]	Loss: 0.1166
Training Epoch: 70 [15104/50048]	Loss: 0.1138
Training Epoch: 70 [15232/50048]	Loss: 0.1475
Training Epoch: 70 [15360/50048]	Loss: 0.1331
Training Epoch: 70 [15488/50048]	Loss: 0.2139
Training Epoch: 70 [15616/50048]	Loss: 0.1623
Training Epoch: 70 [15744/50048]	Loss: 0.1683
Training Epoch: 70 [15872/50048]	Loss: 0.1404
Training Epoch: 70 [16000/50048]	Loss: 0.1229
Training Epoch: 70 [16128/50048]	Loss: 0.1197
Training Epoch: 70 [16256/50048]	Loss: 0.1013
Training Epoch: 70 [16384/50048]	Loss: 0.2046
Training Epoch: 70 [16512/50048]	Loss: 0.1038
Training Epoch: 70 [16640/50048]	Loss: 0.0812
Training Epoch: 70 [16768/50048]	Loss: 0.1748
Training Epoch: 70 [16896/50048]	Loss: 0.1614
Training Epoch: 70 [17024/50048]	Loss: 0.1146
Training Epoch: 70 [17152/50048]	Loss: 0.1370
Training Epoch: 70 [17280/50048]	Loss: 0.1251
Training Epoch: 70 [17408/50048]	Loss: 0.1797
Training Epoch: 70 [17536/50048]	Loss: 0.1293
Training Epoch: 70 [17664/50048]	Loss: 0.2013
Training Epoch: 70 [17792/50048]	Loss: 0.1282
Training Epoch: 70 [17920/50048]	Loss: 0.1043
Training Epoch: 70 [18048/50048]	Loss: 0.1430
Training Epoch: 70 [18176/50048]	Loss: 0.1951
Training Epoch: 70 [18304/50048]	Loss: 0.1430
Training Epoch: 70 [18432/50048]	Loss: 0.1668
Training Epoch: 70 [18560/50048]	Loss: 0.0901
Training Epoch: 70 [18688/50048]	Loss: 0.1874
Training Epoch: 70 [18816/50048]	Loss: 0.0886
Training Epoch: 70 [18944/50048]	Loss: 0.0908
Training Epoch: 70 [19072/50048]	Loss: 0.1307
Training Epoch: 70 [19200/50048]	Loss: 0.0853
Training Epoch: 70 [19328/50048]	Loss: 0.1408
Training Epoch: 70 [19456/50048]	Loss: 0.1195
Training Epoch: 70 [19584/50048]	Loss: 0.0787
Training Epoch: 70 [19712/50048]	Loss: 0.1487
Training Epoch: 70 [19840/50048]	Loss: 0.1459
Training Epoch: 70 [19968/50048]	Loss: 0.0892
Training Epoch: 70 [20096/50048]	Loss: 0.0815
Training Epoch: 70 [20224/50048]	Loss: 0.1989
Training Epoch: 70 [20352/50048]	Loss: 0.0971
Training Epoch: 70 [20480/50048]	Loss: 0.1343
Training Epoch: 70 [20608/50048]	Loss: 0.0406
Training Epoch: 70 [20736/50048]	Loss: 0.1938
Training Epoch: 70 [20864/50048]	Loss: 0.1329
Training Epoch: 70 [20992/50048]	Loss: 0.0718
Training Epoch: 70 [21120/50048]	Loss: 0.1709
Training Epoch: 70 [21248/50048]	Loss: 0.1106
Training Epoch: 70 [21376/50048]	Loss: 0.2304
Training Epoch: 70 [21504/50048]	Loss: 0.0758
Training Epoch: 70 [21632/50048]	Loss: 0.1591
Training Epoch: 70 [21760/50048]	Loss: 0.2273
Training Epoch: 70 [21888/50048]	Loss: 0.1652
Training Epoch: 70 [22016/50048]	Loss: 0.0804
Training Epoch: 70 [22144/50048]	Loss: 0.1281
Training Epoch: 70 [22272/50048]	Loss: 0.0915
Training Epoch: 70 [22400/50048]	Loss: 0.1841
Training Epoch: 70 [22528/50048]	Loss: 0.1202
Training Epoch: 70 [22656/50048]	Loss: 0.0744
Training Epoch: 70 [22784/50048]	Loss: 0.1311
Training Epoch: 70 [22912/50048]	Loss: 0.1967
Training Epoch: 70 [23040/50048]	Loss: 0.2316
Training Epoch: 70 [23168/50048]	Loss: 0.1095
Training Epoch: 70 [23296/50048]	Loss: 0.2507
Training Epoch: 70 [23424/50048]	Loss: 0.1121
Training Epoch: 70 [23552/50048]	Loss: 0.1823
Training Epoch: 70 [23680/50048]	Loss: 0.1858
Training Epoch: 70 [23808/50048]	Loss: 0.1370
Training Epoch: 70 [23936/50048]	Loss: 0.1520
Training Epoch: 70 [24064/50048]	Loss: 0.1167
Training Epoch: 70 [24192/50048]	Loss: 0.1044
Training Epoch: 70 [24320/50048]	Loss: 0.2208
Training Epoch: 70 [24448/50048]	Loss: 0.1744
Training Epoch: 70 [24576/50048]	Loss: 0.1204
Training Epoch: 70 [24704/50048]	Loss: 0.1338
Training Epoch: 70 [24832/50048]	Loss: 0.1586
Training Epoch: 70 [24960/50048]	Loss: 0.1806
Training Epoch: 70 [25088/50048]	Loss: 0.1009
Training Epoch: 70 [25216/50048]	Loss: 0.2000
Training Epoch: 70 [25344/50048]	Loss: 0.1201
Training Epoch: 70 [25472/50048]	Loss: 0.1207
Training Epoch: 70 [25600/50048]	Loss: 0.1689
Training Epoch: 70 [25728/50048]	Loss: 0.0620
Training Epoch: 70 [25856/50048]	Loss: 0.1201
Training Epoch: 70 [25984/50048]	Loss: 0.1550
Training Epoch: 70 [26112/50048]	Loss: 0.0941
Training Epoch: 70 [26240/50048]	Loss: 0.2297
Training Epoch: 70 [26368/50048]	Loss: 0.1918
Training Epoch: 70 [26496/50048]	Loss: 0.1147
Training Epoch: 70 [26624/50048]	Loss: 0.1996
Training Epoch: 70 [26752/50048]	Loss: 0.0898
Training Epoch: 70 [26880/50048]	Loss: 0.1818
Training Epoch: 70 [27008/50048]	Loss: 0.2561
Training Epoch: 70 [27136/50048]	Loss: 0.1356
Training Epoch: 70 [27264/50048]	Loss: 0.1085
Training Epoch: 70 [27392/50048]	Loss: 0.2112
Training Epoch: 70 [27520/50048]	Loss: 0.0937
Training Epoch: 70 [27648/50048]	Loss: 0.0928
Training Epoch: 70 [27776/50048]	Loss: 0.1917
Training Epoch: 70 [27904/50048]	Loss: 0.1195
Training Epoch: 70 [28032/50048]	Loss: 0.1267
Training Epoch: 70 [28160/50048]	Loss: 0.0893
Training Epoch: 70 [28288/50048]	Loss: 0.1158
Training Epoch: 70 [28416/50048]	Loss: 0.1178
Training Epoch: 70 [28544/50048]	Loss: 0.0993
Training Epoch: 70 [28672/50048]	Loss: 0.1499
Training Epoch: 70 [28800/50048]	Loss: 0.1541
Training Epoch: 70 [28928/50048]	Loss: 0.1504
Training Epoch: 70 [29056/50048]	Loss: 0.1563
Training Epoch: 70 [29184/50048]	Loss: 0.0509
Training Epoch: 70 [29312/50048]	Loss: 0.1122
Training Epoch: 70 [29440/50048]	Loss: 0.1076
Training Epoch: 70 [29568/50048]	Loss: 0.1336
Training Epoch: 70 [29696/50048]	Loss: 0.1555
Training Epoch: 70 [29824/50048]	Loss: 0.0950
Training Epoch: 70 [29952/50048]	Loss: 0.1322
Training Epoch: 70 [30080/50048]	Loss: 0.0979
Training Epoch: 70 [30208/50048]	Loss: 0.1501
Training Epoch: 70 [30336/50048]	Loss: 0.1561
Training Epoch: 70 [30464/50048]	Loss: 0.1380
Training Epoch: 70 [30592/50048]	Loss: 0.2213
Training Epoch: 70 [30720/50048]	Loss: 0.1818
Training Epoch: 70 [30848/50048]	Loss: 0.1337
Training Epoch: 70 [30976/50048]	Loss: 0.1522
Training Epoch: 70 [31104/50048]	Loss: 0.1278
Training Epoch: 70 [31232/50048]	Loss: 0.1218
Training Epoch: 70 [31360/50048]	Loss: 0.1167
Training Epoch: 70 [31488/50048]	Loss: 0.1262
Training Epoch: 70 [31616/50048]	Loss: 0.0750
Training Epoch: 70 [31744/50048]	Loss: 0.1886
Training Epoch: 70 [31872/50048]	Loss: 0.1117
Training Epoch: 70 [32000/50048]	Loss: 0.1935
Training Epoch: 70 [32128/50048]	Loss: 0.1931
Training Epoch: 70 [32256/50048]	Loss: 0.1546
Training Epoch: 70 [32384/50048]	Loss: 0.1503
Training Epoch: 70 [32512/50048]	Loss: 0.1128
Training Epoch: 70 [32640/50048]	Loss: 0.1257
Training Epoch: 70 [32768/50048]	Loss: 0.1395
Training Epoch: 70 [32896/50048]	Loss: 0.1876
Training Epoch: 70 [33024/50048]	Loss: 0.1247
Training Epoch: 70 [33152/50048]	Loss: 0.1345
Training Epoch: 70 [33280/50048]	Loss: 0.1102
Training Epoch: 70 [33408/50048]	Loss: 0.1505
Training Epoch: 70 [33536/50048]	Loss: 0.1243
Training Epoch: 70 [33664/50048]	Loss: 0.2073
Training Epoch: 70 [33792/50048]	Loss: 0.1627
Training Epoch: 70 [33920/50048]	Loss: 0.1181
Training Epoch: 70 [34048/50048]	Loss: 0.1367
Training Epoch: 70 [34176/50048]	Loss: 0.1673
Training Epoch: 70 [34304/50048]	Loss: 0.1534
Training Epoch: 70 [34432/50048]	Loss: 0.0864
Training Epoch: 70 [34560/50048]	Loss: 0.1339
Training Epoch: 70 [34688/50048]	Loss: 0.1133
Training Epoch: 70 [34816/50048]	Loss: 0.1174
Training Epoch: 70 [34944/50048]	Loss: 0.1721
Training Epoch: 70 [35072/50048]	Loss: 0.1454
Training Epoch: 70 [35200/50048]	Loss: 0.1075
Training Epoch: 70 [35328/50048]	Loss: 0.0964
Training Epoch: 70 [35456/50048]	Loss: 0.1466
Training Epoch: 70 [35584/50048]	Loss: 0.0852
Training Epoch: 70 [35712/50048]	Loss: 0.2090
Training Epoch: 70 [35840/50048]	Loss: 0.1635
Training Epoch: 70 [35968/50048]	Loss: 0.2183
Training Epoch: 70 [36096/50048]	Loss: 0.0757
Training Epoch: 70 [36224/50048]	Loss: 0.1582
Training Epoch: 70 [36352/50048]	Loss: 0.2607
Training Epoch: 70 [36480/50048]	Loss: 0.2151
Training Epoch: 70 [36608/50048]	Loss: 0.1312
Training Epoch: 70 [36736/50048]	Loss: 0.1686
Training Epoch: 70 [36864/50048]	Loss: 0.1213
Training Epoch: 70 [36992/50048]	Loss: 0.2418
Training Epoch: 70 [37120/50048]	Loss: 0.1236
Training Epoch: 70 [37248/50048]	Loss: 0.2931
Training Epoch: 70 [37376/50048]	Loss: 0.1728
Training Epoch: 70 [37504/50048]	Loss: 0.2232
Training Epoch: 70 [37632/50048]	Loss: 0.2371
Training Epoch: 70 [37760/50048]	Loss: 0.1431
Training Epoch: 70 [37888/50048]	Loss: 0.2614
Training Epoch: 70 [38016/50048]	Loss: 0.1161
Training Epoch: 70 [38144/50048]	Loss: 0.1770
Training Epoch: 70 [38272/50048]	Loss: 0.1682
Training Epoch: 70 [38400/50048]	Loss: 0.2296
Training Epoch: 70 [38528/50048]	Loss: 0.1104
Training Epoch: 70 [38656/50048]	Loss: 0.0759
Training Epoch: 70 [38784/50048]	Loss: 0.1910
Training Epoch: 70 [38912/50048]	Loss: 0.1785
Training Epoch: 70 [39040/50048]	Loss: 0.1743
Training Epoch: 70 [39168/50048]	Loss: 0.1612
Training Epoch: 70 [39296/50048]	Loss: 0.1818
Training Epoch: 70 [39424/50048]	Loss: 0.1969
Training Epoch: 70 [39552/50048]	Loss: 0.1822
Training Epoch: 70 [39680/50048]	Loss: 0.1660
Training Epoch: 70 [39808/50048]	Loss: 0.1630
Training Epoch: 70 [39936/50048]	Loss: 0.1855
Training Epoch: 70 [40064/50048]	Loss: 0.1332
Training Epoch: 70 [40192/50048]	Loss: 0.1742
Training Epoch: 70 [40320/50048]	Loss: 0.1579
Training Epoch: 70 [40448/50048]	Loss: 0.1023
Training Epoch: 70 [40576/50048]	Loss: 0.1600
Training Epoch: 70 [40704/50048]	Loss: 0.1227
Training Epoch: 70 [40832/50048]	Loss: 0.1319
Training Epoch: 70 [40960/50048]	Loss: 0.1345
Training Epoch: 70 [41088/50048]	Loss: 0.1492
Training Epoch: 70 [41216/50048]	Loss: 0.1823
Training Epoch: 70 [41344/50048]	Loss: 0.1439
Training Epoch: 70 [41472/50048]	Loss: 0.1072
Training Epoch: 70 [41600/50048]	Loss: 0.1522
Training Epoch: 70 [41728/50048]	Loss: 0.0936
Training Epoch: 70 [41856/50048]	Loss: 0.1785
Training Epoch: 70 [41984/50048]	Loss: 0.1057
Training Epoch: 70 [42112/50048]	Loss: 0.2782
Training Epoch: 70 [42240/50048]	Loss: 0.1794
Training Epoch: 70 [42368/50048]	Loss: 0.2019
Training Epoch: 70 [42496/50048]	Loss: 0.1029
Training Epoch: 70 [42624/50048]	Loss: 0.2014
Training Epoch: 70 [42752/50048]	Loss: 0.1038
Training Epoch: 70 [42880/50048]	Loss: 0.1667
Training Epoch: 70 [43008/50048]	Loss: 0.1808
Training Epoch: 70 [43136/50048]	Loss: 0.1763
Training Epoch: 70 [43264/50048]	Loss: 0.0978
Training Epoch: 70 [43392/50048]	Loss: 0.1169
Training Epoch: 70 [43520/50048]	Loss: 0.1143
Training Epoch: 70 [43648/50048]	Loss: 0.2154
Training Epoch: 70 [43776/50048]	Loss: 0.1351
Training Epoch: 70 [43904/50048]	Loss: 0.0961
Training Epoch: 70 [44032/50048]	Loss: 0.1256
Training Epoch: 70 [44160/50048]	Loss: 0.1606
Training Epoch: 70 [44288/50048]	Loss: 0.2245
Training Epoch: 70 [44416/50048]	Loss: 0.0871
Training Epoch: 70 [44544/50048]	Loss: 0.0890
Training Epoch: 70 [44672/50048]	Loss: 0.1311
Training Epoch: 70 [44800/50048]	Loss: 0.1697
Training Epoch: 70 [44928/50048]	Loss: 0.2310
Training Epoch: 70 [45056/50048]	Loss: 0.2065
Training Epoch: 70 [45184/50048]	Loss: 0.1651
Training Epoch: 70 [45312/50048]	Loss: 0.1492
Training Epoch: 70 [45440/50048]	Loss: 0.1348
Training Epoch: 70 [45568/50048]	Loss: 0.1627
Training Epoch: 70 [45696/50048]	Loss: 0.2056
2022-12-06 07:54:35,198 [ZeusDataLoader(train)] train epoch 71 done: time=86.47 energy=10506.12
2022-12-06 07:54:35,199 [ZeusDataLoader(eval)] Epoch 71 begin.
Training Epoch: 70 [45824/50048]	Loss: 0.1502
Training Epoch: 70 [45952/50048]	Loss: 0.1222
Training Epoch: 70 [46080/50048]	Loss: 0.0936
Training Epoch: 70 [46208/50048]	Loss: 0.1526
Training Epoch: 70 [46336/50048]	Loss: 0.2070
Training Epoch: 70 [46464/50048]	Loss: 0.1210
Training Epoch: 70 [46592/50048]	Loss: 0.1310
Training Epoch: 70 [46720/50048]	Loss: 0.1499
Training Epoch: 70 [46848/50048]	Loss: 0.1156
Training Epoch: 70 [46976/50048]	Loss: 0.0963
Training Epoch: 70 [47104/50048]	Loss: 0.1367
Training Epoch: 70 [47232/50048]	Loss: 0.1773
Training Epoch: 70 [47360/50048]	Loss: 0.1299
Training Epoch: 70 [47488/50048]	Loss: 0.1572
Training Epoch: 70 [47616/50048]	Loss: 0.1160
Training Epoch: 70 [47744/50048]	Loss: 0.1646
Training Epoch: 70 [47872/50048]	Loss: 0.1755
Training Epoch: 70 [48000/50048]	Loss: 0.1564
Training Epoch: 70 [48128/50048]	Loss: 0.1091
Training Epoch: 70 [48256/50048]	Loss: 0.1595
Training Epoch: 70 [48384/50048]	Loss: 0.0897
Training Epoch: 70 [48512/50048]	Loss: 0.2214
Training Epoch: 70 [48640/50048]	Loss: 0.1362
Training Epoch: 70 [48768/50048]	Loss: 0.1182
Training Epoch: 70 [48896/50048]	Loss: 0.0951
Training Epoch: 70 [49024/50048]	Loss: 0.2116
Training Epoch: 70 [49152/50048]	Loss: 0.1550
Training Epoch: 70 [49280/50048]	Loss: 0.1355
Training Epoch: 70 [49408/50048]	Loss: 0.1585
Training Epoch: 70 [49536/50048]	Loss: 0.1060
Training Epoch: 70 [49664/50048]	Loss: 0.2185
Training Epoch: 70 [49792/50048]	Loss: 0.1651
Training Epoch: 70 [49920/50048]	Loss: 0.2837
Training Epoch: 70 [50048/50048]	Loss: 0.1951
2022-12-06 12:54:38.948 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:54:38,978 [ZeusDataLoader(eval)] eval epoch 71 done: time=3.77 energy=453.67
2022-12-06 07:54:38,978 [ZeusDataLoader(train)] Up to epoch 71: time=6403.80, energy=777362.78, cost=949013.73
2022-12-06 07:54:38,978 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:54:38,979 [ZeusDataLoader(train)] Expected next epoch: time=6493.60, energy=788160.80, cost=962270.11
2022-12-06 07:54:38,979 [ZeusDataLoader(train)] Epoch 72 begin.
Validation Epoch: 70, Average loss: 0.0173, Accuracy: 0.6343
2022-12-06 07:54:39,175 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:54:39,176 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:54:39.177 [ZeusMonitor] Monitor started.
2022-12-06 12:54:39.178 [ZeusMonitor] Running indefinitely. 2022-12-06 12:54:39.178 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:54:39.178 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e72+gpu0.power.log
Training Epoch: 71 [128/50048]	Loss: 0.0990
Training Epoch: 71 [256/50048]	Loss: 0.1527
Training Epoch: 71 [384/50048]	Loss: 0.1491
Training Epoch: 71 [512/50048]	Loss: 0.0756
Training Epoch: 71 [640/50048]	Loss: 0.1456
Training Epoch: 71 [768/50048]	Loss: 0.1147
Training Epoch: 71 [896/50048]	Loss: 0.1452
Training Epoch: 71 [1024/50048]	Loss: 0.1024
Training Epoch: 71 [1152/50048]	Loss: 0.0594
Training Epoch: 71 [1280/50048]	Loss: 0.1494
Training Epoch: 71 [1408/50048]	Loss: 0.1103
Training Epoch: 71 [1536/50048]	Loss: 0.1581
Training Epoch: 71 [1664/50048]	Loss: 0.0926
Training Epoch: 71 [1792/50048]	Loss: 0.0862
Training Epoch: 71 [1920/50048]	Loss: 0.0700
Training Epoch: 71 [2048/50048]	Loss: 0.1587
Training Epoch: 71 [2176/50048]	Loss: 0.0556
Training Epoch: 71 [2304/50048]	Loss: 0.1774
Training Epoch: 71 [2432/50048]	Loss: 0.1315
Training Epoch: 71 [2560/50048]	Loss: 0.1005
Training Epoch: 71 [2688/50048]	Loss: 0.0549
Training Epoch: 71 [2816/50048]	Loss: 0.1855
Training Epoch: 71 [2944/50048]	Loss: 0.0976
Training Epoch: 71 [3072/50048]	Loss: 0.0886
Training Epoch: 71 [3200/50048]	Loss: 0.1691
Training Epoch: 71 [3328/50048]	Loss: 0.1281
Training Epoch: 71 [3456/50048]	Loss: 0.1110
Training Epoch: 71 [3584/50048]	Loss: 0.1160
Training Epoch: 71 [3712/50048]	Loss: 0.0757
Training Epoch: 71 [3840/50048]	Loss: 0.1042
Training Epoch: 71 [3968/50048]	Loss: 0.1086
Training Epoch: 71 [4096/50048]	Loss: 0.0809
Training Epoch: 71 [4224/50048]	Loss: 0.1484
Training Epoch: 71 [4352/50048]	Loss: 0.0934
Training Epoch: 71 [4480/50048]	Loss: 0.0830
Training Epoch: 71 [4608/50048]	Loss: 0.0496
Training Epoch: 71 [4736/50048]	Loss: 0.1191
Training Epoch: 71 [4864/50048]	Loss: 0.1617
Training Epoch: 71 [4992/50048]	Loss: 0.1803
Training Epoch: 71 [5120/50048]	Loss: 0.1268
Training Epoch: 71 [5248/50048]	Loss: 0.1765
Training Epoch: 71 [5376/50048]	Loss: 0.1086
Training Epoch: 71 [5504/50048]	Loss: 0.0967
Training Epoch: 71 [5632/50048]	Loss: 0.1041
Training Epoch: 71 [5760/50048]	Loss: 0.1673
Training Epoch: 71 [5888/50048]	Loss: 0.1097
Training Epoch: 71 [6016/50048]	Loss: 0.0659
Training Epoch: 71 [6144/50048]	Loss: 0.1055
Training Epoch: 71 [6272/50048]	Loss: 0.1237
Training Epoch: 71 [6400/50048]	Loss: 0.1158
Training Epoch: 71 [6528/50048]	Loss: 0.0537
Training Epoch: 71 [6656/50048]	Loss: 0.1033
Training Epoch: 71 [6784/50048]	Loss: 0.1047
Training Epoch: 71 [6912/50048]	Loss: 0.1896
Training Epoch: 71 [7040/50048]	Loss: 0.1727
Training Epoch: 71 [7168/50048]	Loss: 0.1501
Training Epoch: 71 [7296/50048]	Loss: 0.1293
Training Epoch: 71 [7424/50048]	Loss: 0.2094
Training Epoch: 71 [7552/50048]	Loss: 0.0920
Training Epoch: 71 [7680/50048]	Loss: 0.1389
Training Epoch: 71 [7808/50048]	Loss: 0.1240
Training Epoch: 71 [7936/50048]	Loss: 0.1305
Training Epoch: 71 [8064/50048]	Loss: 0.1142
Training Epoch: 71 [8192/50048]	Loss: 0.1625
Training Epoch: 71 [8320/50048]	Loss: 0.1823
Training Epoch: 71 [8448/50048]	Loss: 0.1062
Training Epoch: 71 [8576/50048]	Loss: 0.1178
Training Epoch: 71 [8704/50048]	Loss: 0.0863
Training Epoch: 71 [8832/50048]	Loss: 0.1191
Training Epoch: 71 [8960/50048]	Loss: 0.1499
Training Epoch: 71 [9088/50048]	Loss: 0.1042
Training Epoch: 71 [9216/50048]	Loss: 0.1635
Training Epoch: 71 [9344/50048]	Loss: 0.0981
Training Epoch: 71 [9472/50048]	Loss: 0.1254
Training Epoch: 71 [9600/50048]	Loss: 0.1266
Training Epoch: 71 [9728/50048]	Loss: 0.1178
Training Epoch: 71 [9856/50048]	Loss: 0.1148
Training Epoch: 71 [9984/50048]	Loss: 0.1242
Training Epoch: 71 [10112/50048]	Loss: 0.1675
Training Epoch: 71 [10240/50048]	Loss: 0.0664
Training Epoch: 71 [10368/50048]	Loss: 0.1361
Training Epoch: 71 [10496/50048]	Loss: 0.1741
Training Epoch: 71 [10624/50048]	Loss: 0.1487
Training Epoch: 71 [10752/50048]	Loss: 0.1007
Training Epoch: 71 [10880/50048]	Loss: 0.1120
Training Epoch: 71 [11008/50048]	Loss: 0.1780
Training Epoch: 71 [11136/50048]	Loss: 0.2383
Training Epoch: 71 [11264/50048]	Loss: 0.1780
Training Epoch: 71 [11392/50048]	Loss: 0.0889
Training Epoch: 71 [11520/50048]	Loss: 0.2431
Training Epoch: 71 [11648/50048]	Loss: 0.0616
Training Epoch: 71 [11776/50048]	Loss: 0.1171
Training Epoch: 71 [11904/50048]	Loss: 0.0768
Training Epoch: 71 [12032/50048]	Loss: 0.1416
Training Epoch: 71 [12160/50048]	Loss: 0.1824
Training Epoch: 71 [12288/50048]	Loss: 0.1657
Training Epoch: 71 [12416/50048]	Loss: 0.1977
Training Epoch: 71 [12544/50048]	Loss: 0.1337
Training Epoch: 71 [12672/50048]	Loss: 0.1026
Training Epoch: 71 [12800/50048]	Loss: 0.0506
Training Epoch: 71 [12928/50048]	Loss: 0.1416
Training Epoch: 71 [13056/50048]	Loss: 0.1142
Training Epoch: 71 [13184/50048]	Loss: 0.1250
Training Epoch: 71 [13312/50048]	Loss: 0.1938
Training Epoch: 71 [13440/50048]	Loss: 0.1066
Training Epoch: 71 [13568/50048]	Loss: 0.1223
Training Epoch: 71 [13696/50048]	Loss: 0.1011
Training Epoch: 71 [13824/50048]	Loss: 0.1330
Training Epoch: 71 [13952/50048]	Loss: 0.0992
Training Epoch: 71 [14080/50048]	Loss: 0.1198
Training Epoch: 71 [14208/50048]	Loss: 0.1830
Training Epoch: 71 [14336/50048]	Loss: 0.0833
Training Epoch: 71 [14464/50048]	Loss: 0.1142
Training Epoch: 71 [14592/50048]	Loss: 0.1405
Training Epoch: 71 [14720/50048]	Loss: 0.0906
Training Epoch: 71 [14848/50048]	Loss: 0.1157
Training Epoch: 71 [14976/50048]	Loss: 0.1014
Training Epoch: 71 [15104/50048]	Loss: 0.1340
Training Epoch: 71 [15232/50048]	Loss: 0.0811
Training Epoch: 71 [15360/50048]	Loss: 0.1171
Training Epoch: 71 [15488/50048]	Loss: 0.0986
Training Epoch: 71 [15616/50048]	Loss: 0.1175
Training Epoch: 71 [15744/50048]	Loss: 0.1601
Training Epoch: 71 [15872/50048]	Loss: 0.2166
Training Epoch: 71 [16000/50048]	Loss: 0.1986
Training Epoch: 71 [16128/50048]	Loss: 0.1454
Training Epoch: 71 [16256/50048]	Loss: 0.1115
Training Epoch: 71 [16384/50048]	Loss: 0.0783
Training Epoch: 71 [16512/50048]	Loss: 0.0748
Training Epoch: 71 [16640/50048]	Loss: 0.0785
Training Epoch: 71 [16768/50048]	Loss: 0.1105
Training Epoch: 71 [16896/50048]	Loss: 0.0917
Training Epoch: 71 [17024/50048]	Loss: 0.0951
Training Epoch: 71 [17152/50048]	Loss: 0.1905
Training Epoch: 71 [17280/50048]	Loss: 0.1868
Training Epoch: 71 [17408/50048]	Loss: 0.0738
Training Epoch: 71 [17536/50048]	Loss: 0.1135
Training Epoch: 71 [17664/50048]	Loss: 0.1394
Training Epoch: 71 [17792/50048]	Loss: 0.0427
Training Epoch: 71 [17920/50048]	Loss: 0.0808
Training Epoch: 71 [18048/50048]	Loss: 0.0795
Training Epoch: 71 [18176/50048]	Loss: 0.1273
Training Epoch: 71 [18304/50048]	Loss: 0.0842
Training Epoch: 71 [18432/50048]	Loss: 0.1872
Training Epoch: 71 [18560/50048]	Loss: 0.1020
Training Epoch: 71 [18688/50048]	Loss: 0.0929
Training Epoch: 71 [18816/50048]	Loss: 0.1152
Training Epoch: 71 [18944/50048]	Loss: 0.1198
Training Epoch: 71 [19072/50048]	Loss: 0.0579
Training Epoch: 71 [19200/50048]	Loss: 0.0871
Training Epoch: 71 [19328/50048]	Loss: 0.1932
Training Epoch: 71 [19456/50048]	Loss: 0.0708
Training Epoch: 71 [19584/50048]	Loss: 0.1403
Training Epoch: 71 [19712/50048]	Loss: 0.0824
Training Epoch: 71 [19840/50048]	Loss: 0.1038
Training Epoch: 71 [19968/50048]	Loss: 0.0991
Training Epoch: 71 [20096/50048]	Loss: 0.0784
Training Epoch: 71 [20224/50048]	Loss: 0.0650
Training Epoch: 71 [20352/50048]	Loss: 0.1323
Training Epoch: 71 [20480/50048]	Loss: 0.1264
Training Epoch: 71 [20608/50048]	Loss: 0.1523
Training Epoch: 71 [20736/50048]	Loss: 0.0875
Training Epoch: 71 [20864/50048]	Loss: 0.1123
Training Epoch: 71 [20992/50048]	Loss: 0.1555
Training Epoch: 71 [21120/50048]	Loss: 0.1648
Training Epoch: 71 [21248/50048]	Loss: 0.1335
Training Epoch: 71 [21376/50048]	Loss: 0.1319
Training Epoch: 71 [21504/50048]	Loss: 0.1860
Training Epoch: 71 [21632/50048]	Loss: 0.1033
Training Epoch: 71 [21760/50048]	Loss: 0.1501
Training Epoch: 71 [21888/50048]	Loss: 0.1317
Training Epoch: 71 [22016/50048]	Loss: 0.1249
Training Epoch: 71 [22144/50048]	Loss: 0.1536
Training Epoch: 71 [22272/50048]	Loss: 0.1263
Training Epoch: 71 [22400/50048]	Loss: 0.1189
Training Epoch: 71 [22528/50048]	Loss: 0.1221
Training Epoch: 71 [22656/50048]	Loss: 0.1071
Training Epoch: 71 [22784/50048]	Loss: 0.0913
Training Epoch: 71 [22912/50048]	Loss: 0.2646
Training Epoch: 71 [23040/50048]	Loss: 0.0787
Training Epoch: 71 [23168/50048]	Loss: 0.2036
Training Epoch: 71 [23296/50048]	Loss: 0.1691
Training Epoch: 71 [23424/50048]	Loss: 0.1859
Training Epoch: 71 [23552/50048]	Loss: 0.1045
Training Epoch: 71 [23680/50048]	Loss: 0.1216
Training Epoch: 71 [23808/50048]	Loss: 0.1317
Training Epoch: 71 [23936/50048]	Loss: 0.1804
Training Epoch: 71 [24064/50048]	Loss: 0.1420
Training Epoch: 71 [24192/50048]	Loss: 0.2000
Training Epoch: 71 [24320/50048]	Loss: 0.0780
Training Epoch: 71 [24448/50048]	Loss: 0.0812
Training Epoch: 71 [24576/50048]	Loss: 0.1684
Training Epoch: 71 [24704/50048]	Loss: 0.2488
Training Epoch: 71 [24832/50048]	Loss: 0.1311
Training Epoch: 71 [24960/50048]	Loss: 0.1284
Training Epoch: 71 [25088/50048]	Loss: 0.0861
Training Epoch: 71 [25216/50048]	Loss: 0.1253
Training Epoch: 71 [25344/50048]	Loss: 0.1738
Training Epoch: 71 [25472/50048]	Loss: 0.1231
Training Epoch: 71 [25600/50048]	Loss: 0.2698
Training Epoch: 71 [25728/50048]	Loss: 0.1081
Training Epoch: 71 [25856/50048]	Loss: 0.1540
Training Epoch: 71 [25984/50048]	Loss: 0.0874
Training Epoch: 71 [26112/50048]	Loss: 0.1254
Training Epoch: 71 [26240/50048]	Loss: 0.0894
Training Epoch: 71 [26368/50048]	Loss: 0.1992
Training Epoch: 71 [26496/50048]	Loss: 0.2298
Training Epoch: 71 [26624/50048]	Loss: 0.1038
Training Epoch: 71 [26752/50048]	Loss: 0.1001
Training Epoch: 71 [26880/50048]	Loss: 0.2163
Training Epoch: 71 [27008/50048]	Loss: 0.0961
Training Epoch: 71 [27136/50048]	Loss: 0.1501
Training Epoch: 71 [27264/50048]	Loss: 0.1017
Training Epoch: 71 [27392/50048]	Loss: 0.1861
Training Epoch: 71 [27520/50048]	Loss: 0.1390
Training Epoch: 71 [27648/50048]	Loss: 0.1670
Training Epoch: 71 [27776/50048]	Loss: 0.2070
Training Epoch: 71 [27904/50048]	Loss: 0.2303
Training Epoch: 71 [28032/50048]	Loss: 0.2192
Training Epoch: 71 [28160/50048]	Loss: 0.1432
Training Epoch: 71 [28288/50048]	Loss: 0.1785
Training Epoch: 71 [28416/50048]	Loss: 0.1787
Training Epoch: 71 [28544/50048]	Loss: 0.1707
Training Epoch: 71 [28672/50048]	Loss: 0.0766
Training Epoch: 71 [28800/50048]	Loss: 0.1336
Training Epoch: 71 [28928/50048]	Loss: 0.1378
Training Epoch: 71 [29056/50048]	Loss: 0.2098
Training Epoch: 71 [29184/50048]	Loss: 0.1562
Training Epoch: 71 [29312/50048]	Loss: 0.1236
Training Epoch: 71 [29440/50048]	Loss: 0.1330
Training Epoch: 71 [29568/50048]	Loss: 0.1514
Training Epoch: 71 [29696/50048]	Loss: 0.1410
Training Epoch: 71 [29824/50048]	Loss: 0.1928
Training Epoch: 71 [29952/50048]	Loss: 0.2089
Training Epoch: 71 [30080/50048]	Loss: 0.2560
Training Epoch: 71 [30208/50048]	Loss: 0.1840
Training Epoch: 71 [30336/50048]	Loss: 0.0934
Training Epoch: 71 [30464/50048]	Loss: 0.1314
Training Epoch: 71 [30592/50048]	Loss: 0.1036
Training Epoch: 71 [30720/50048]	Loss: 0.1385
Training Epoch: 71 [30848/50048]	Loss: 0.1232
Training Epoch: 71 [30976/50048]	Loss: 0.1163
Training Epoch: 71 [31104/50048]	Loss: 0.0457
Training Epoch: 71 [31232/50048]	Loss: 0.1397
Training Epoch: 71 [31360/50048]	Loss: 0.1771
Training Epoch: 71 [31488/50048]	Loss: 0.1471
Training Epoch: 71 [31616/50048]	Loss: 0.1652
Training Epoch: 71 [31744/50048]	Loss: 0.1515
Training Epoch: 71 [31872/50048]	Loss: 0.1980
Training Epoch: 71 [32000/50048]	Loss: 0.1280
Training Epoch: 71 [32128/50048]	Loss: 0.0926
Training Epoch: 71 [32256/50048]	Loss: 0.1317
Training Epoch: 71 [32384/50048]	Loss: 0.0958
Training Epoch: 71 [32512/50048]	Loss: 0.1846
Training Epoch: 71 [32640/50048]	Loss: 0.2051
Training Epoch: 71 [32768/50048]	Loss: 0.1693
Training Epoch: 71 [32896/50048]	Loss: 0.1274
Training Epoch: 71 [33024/50048]	Loss: 0.1916
Training Epoch: 71 [33152/50048]	Loss: 0.0516
Training Epoch: 71 [33280/50048]	Loss: 0.1742
Training Epoch: 71 [33408/50048]	Loss: 0.1078
Training Epoch: 71 [33536/50048]	Loss: 0.1472
Training Epoch: 71 [33664/50048]	Loss: 0.1305
Training Epoch: 71 [33792/50048]	Loss: 0.1321
Training Epoch: 71 [33920/50048]	Loss: 0.2100
Training Epoch: 71 [34048/50048]	Loss: 0.1265
Training Epoch: 71 [34176/50048]	Loss: 0.2041
Training Epoch: 71 [34304/50048]	Loss: 0.1104
Training Epoch: 71 [34432/50048]	Loss: 0.1205
Training Epoch: 71 [34560/50048]	Loss: 0.1043
Training Epoch: 71 [34688/50048]	Loss: 0.1902
Training Epoch: 71 [34816/50048]	Loss: 0.1775
Training Epoch: 71 [34944/50048]	Loss: 0.1063
Training Epoch: 71 [35072/50048]	Loss: 0.1223
Training Epoch: 71 [35200/50048]	Loss: 0.1241
Training Epoch: 71 [35328/50048]	Loss: 0.1040
Training Epoch: 71 [35456/50048]	Loss: 0.1960
Training Epoch: 71 [35584/50048]	Loss: 0.1924
Training Epoch: 71 [35712/50048]	Loss: 0.0629
Training Epoch: 71 [35840/50048]	Loss: 0.0941
Training Epoch: 71 [35968/50048]	Loss: 0.0824
Training Epoch: 71 [36096/50048]	Loss: 0.1659
Training Epoch: 71 [36224/50048]	Loss: 0.1787
Training Epoch: 71 [36352/50048]	Loss: 0.1962
Training Epoch: 71 [36480/50048]	Loss: 0.1610
Training Epoch: 71 [36608/50048]	Loss: 0.2338
Training Epoch: 71 [36736/50048]	Loss: 0.1038
Training Epoch: 71 [36864/50048]	Loss: 0.0866
Training Epoch: 71 [36992/50048]	Loss: 0.0741
Training Epoch: 71 [37120/50048]	Loss: 0.1099
Training Epoch: 71 [37248/50048]	Loss: 0.1501
Training Epoch: 71 [37376/50048]	Loss: 0.2673
Training Epoch: 71 [37504/50048]	Loss: 0.2206
Training Epoch: 71 [37632/50048]	Loss: 0.1520
Training Epoch: 71 [37760/50048]	Loss: 0.2061
Training Epoch: 71 [37888/50048]	Loss: 0.1168
Training Epoch: 71 [38016/50048]	Loss: 0.2487
Training Epoch: 71 [38144/50048]	Loss: 0.1172
Training Epoch: 71 [38272/50048]	Loss: 0.1108
Training Epoch: 71 [38400/50048]	Loss: 0.2181
Training Epoch: 71 [38528/50048]	Loss: 0.1400
Training Epoch: 71 [38656/50048]	Loss: 0.2179
Training Epoch: 71 [38784/50048]	Loss: 0.0997
Training Epoch: 71 [38912/50048]	Loss: 0.1935
Training Epoch: 71 [39040/50048]	Loss: 0.1404
Training Epoch: 71 [39168/50048]	Loss: 0.1256
Training Epoch: 71 [39296/50048]	Loss: 0.1069
Training Epoch: 71 [39424/50048]	Loss: 0.1146
Training Epoch: 71 [39552/50048]	Loss: 0.1836
Training Epoch: 71 [39680/50048]	Loss: 0.0883
Training Epoch: 71 [39808/50048]	Loss: 0.1856
Training Epoch: 71 [39936/50048]	Loss: 0.1727
Training Epoch: 71 [40064/50048]	Loss: 0.0979
Training Epoch: 71 [40192/50048]	Loss: 0.1818
Training Epoch: 71 [40320/50048]	Loss: 0.1532
Training Epoch: 71 [40448/50048]	Loss: 0.2022
Training Epoch: 71 [40576/50048]	Loss: 0.1223
Training Epoch: 71 [40704/50048]	Loss: 0.1986
Training Epoch: 71 [40832/50048]	Loss: 0.1098
Training Epoch: 71 [40960/50048]	Loss: 0.1836
Training Epoch: 71 [41088/50048]	Loss: 0.1297
Training Epoch: 71 [41216/50048]	Loss: 0.1914
Training Epoch: 71 [41344/50048]	Loss: 0.2386
Training Epoch: 71 [41472/50048]	Loss: 0.1131
Training Epoch: 71 [41600/50048]	Loss: 0.0910
Training Epoch: 71 [41728/50048]	Loss: 0.2532
Training Epoch: 71 [41856/50048]	Loss: 0.1256
Training Epoch: 71 [41984/50048]	Loss: 0.1672
Training Epoch: 71 [42112/50048]	Loss: 0.1729
Training Epoch: 71 [42240/50048]	Loss: 0.1510
Training Epoch: 71 [42368/50048]	Loss: 0.1224
Training Epoch: 71 [42496/50048]	Loss: 0.0941
Training Epoch: 71 [42624/50048]	Loss: 0.1031
Training Epoch: 71 [42752/50048]	Loss: 0.2224
Training Epoch: 71 [42880/50048]	Loss: 0.1715
Training Epoch: 71 [43008/50048]	Loss: 0.1142
Training Epoch: 71 [43136/50048]	Loss: 0.2089
Training Epoch: 71 [43264/50048]	Loss: 0.2123
Training Epoch: 71 [43392/50048]	Loss: 0.1185
Training Epoch: 71 [43520/50048]	Loss: 0.1682
Training Epoch: 71 [43648/50048]	Loss: 0.1157
Training Epoch: 71 [43776/50048]	Loss: 0.0787
Training Epoch: 71 [43904/50048]	Loss: 0.2685
Training Epoch: 71 [44032/50048]	Loss: 0.1174
Training Epoch: 71 [44160/50048]	Loss: 0.1314
Training Epoch: 71 [44288/50048]	Loss: 0.1139
Training Epoch: 71 [44416/50048]	Loss: 0.1390
Training Epoch: 71 [44544/50048]	Loss: 0.0659
Training Epoch: 71 [44672/50048]	Loss: 0.1104
Training Epoch: 71 [44800/50048]	Loss: 0.2161
Training Epoch: 71 [44928/50048]	Loss: 0.1852
Training Epoch: 71 [45056/50048]	Loss: 0.1882
Training Epoch: 71 [45184/50048]	Loss: 0.1396
Training Epoch: 71 [45312/50048]	Loss: 0.1712
Training Epoch: 71 [45440/50048]	Loss: 0.0937
Training Epoch: 71 [45568/50048]	Loss: 0.1730
Training Epoch: 71 [45696/50048]	Loss: 0.1270
2022-12-06 07:56:05,620 [ZeusDataLoader(train)] train epoch 72 done: time=86.59 energy=10503.56
2022-12-06 07:56:05,622 [ZeusDataLoader(eval)] Epoch 72 begin.
Training Epoch: 71 [45824/50048]	Loss: 0.0875
Training Epoch: 71 [45952/50048]	Loss: 0.1285
Training Epoch: 71 [46080/50048]	Loss: 0.1250
Training Epoch: 71 [46208/50048]	Loss: 0.1166
Training Epoch: 71 [46336/50048]	Loss: 0.1502
Training Epoch: 71 [46464/50048]	Loss: 0.1315
Training Epoch: 71 [46592/50048]	Loss: 0.1743
Training Epoch: 71 [46720/50048]	Loss: 0.2015
Training Epoch: 71 [46848/50048]	Loss: 0.1630
Training Epoch: 71 [46976/50048]	Loss: 0.1265
Training Epoch: 71 [47104/50048]	Loss: 0.1482
Training Epoch: 71 [47232/50048]	Loss: 0.1848
Training Epoch: 71 [47360/50048]	Loss: 0.1149
Training Epoch: 71 [47488/50048]	Loss: 0.0958
Training Epoch: 71 [47616/50048]	Loss: 0.1881
Training Epoch: 71 [47744/50048]	Loss: 0.1651
Training Epoch: 71 [47872/50048]	Loss: 0.1615
Training Epoch: 71 [48000/50048]	Loss: 0.1038
Training Epoch: 71 [48128/50048]	Loss: 0.1120
Training Epoch: 71 [48256/50048]	Loss: 0.1525
Training Epoch: 71 [48384/50048]	Loss: 0.1566
Training Epoch: 71 [48512/50048]	Loss: 0.1620
Training Epoch: 71 [48640/50048]	Loss: 0.1324
Training Epoch: 71 [48768/50048]	Loss: 0.2063
Training Epoch: 71 [48896/50048]	Loss: 0.1234
Training Epoch: 71 [49024/50048]	Loss: 0.2767
Training Epoch: 71 [49152/50048]	Loss: 0.2213
Training Epoch: 71 [49280/50048]	Loss: 0.1070
Training Epoch: 71 [49408/50048]	Loss: 0.2504
Training Epoch: 71 [49536/50048]	Loss: 0.1781
Training Epoch: 71 [49664/50048]	Loss: 0.2349
Training Epoch: 71 [49792/50048]	Loss: 0.2545
Training Epoch: 71 [49920/50048]	Loss: 0.2200
Training Epoch: 71 [50048/50048]	Loss: 0.1887
2022-12-06 12:56:09.323 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:56:09,351 [ZeusDataLoader(eval)] eval epoch 72 done: time=3.72 energy=450.01
2022-12-06 07:56:09,352 [ZeusDataLoader(train)] Up to epoch 72: time=6494.10, energy=788316.35, cost=962392.36
2022-12-06 07:56:09,352 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:56:09,352 [ZeusDataLoader(train)] Expected next epoch: time=6583.90, energy=799114.37, cost=975648.75
2022-12-06 07:56:09,353 [ZeusDataLoader(train)] Epoch 73 begin.
Validation Epoch: 71, Average loss: 0.0176, Accuracy: 0.6306
2022-12-06 07:56:09,500 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:56:09,501 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:56:09.503 [ZeusMonitor] Monitor started.
2022-12-06 12:56:09.503 [ZeusMonitor] Running indefinitely. 2022-12-06 12:56:09.503 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:56:09.503 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e73+gpu0.power.log
Training Epoch: 72 [128/50048]	Loss: 0.0836
Training Epoch: 72 [256/50048]	Loss: 0.2146
Training Epoch: 72 [384/50048]	Loss: 0.1865
Training Epoch: 72 [512/50048]	Loss: 0.0788
Training Epoch: 72 [640/50048]	Loss: 0.1184
Training Epoch: 72 [768/50048]	Loss: 0.1914
Training Epoch: 72 [896/50048]	Loss: 0.1166
Training Epoch: 72 [1024/50048]	Loss: 0.1052
Training Epoch: 72 [1152/50048]	Loss: 0.1638
Training Epoch: 72 [1280/50048]	Loss: 0.2061
Training Epoch: 72 [1408/50048]	Loss: 0.1021
Training Epoch: 72 [1536/50048]	Loss: 0.1400
Training Epoch: 72 [1664/50048]	Loss: 0.1029
Training Epoch: 72 [1792/50048]	Loss: 0.1503
Training Epoch: 72 [1920/50048]	Loss: 0.1030
Training Epoch: 72 [2048/50048]	Loss: 0.1521
Training Epoch: 72 [2176/50048]	Loss: 0.0826
Training Epoch: 72 [2304/50048]	Loss: 0.1354
Training Epoch: 72 [2432/50048]	Loss: 0.1007
Training Epoch: 72 [2560/50048]	Loss: 0.1731
Training Epoch: 72 [2688/50048]	Loss: 0.0755
Training Epoch: 72 [2816/50048]	Loss: 0.1896
Training Epoch: 72 [2944/50048]	Loss: 0.1119
Training Epoch: 72 [3072/50048]	Loss: 0.1971
Training Epoch: 72 [3200/50048]	Loss: 0.1195
Training Epoch: 72 [3328/50048]	Loss: 0.0761
Training Epoch: 72 [3456/50048]	Loss: 0.1270
Training Epoch: 72 [3584/50048]	Loss: 0.1047
Training Epoch: 72 [3712/50048]	Loss: 0.0486
Training Epoch: 72 [3840/50048]	Loss: 0.1425
Training Epoch: 72 [3968/50048]	Loss: 0.1385
Training Epoch: 72 [4096/50048]	Loss: 0.0723
Training Epoch: 72 [4224/50048]	Loss: 0.1212
Training Epoch: 72 [4352/50048]	Loss: 0.0801
Training Epoch: 72 [4480/50048]	Loss: 0.1075
Training Epoch: 72 [4608/50048]	Loss: 0.0991
Training Epoch: 72 [4736/50048]	Loss: 0.0404
Training Epoch: 72 [4864/50048]	Loss: 0.0956
Training Epoch: 72 [4992/50048]	Loss: 0.1114
Training Epoch: 72 [5120/50048]	Loss: 0.0854
Training Epoch: 72 [5248/50048]	Loss: 0.1235
Training Epoch: 72 [5376/50048]	Loss: 0.0725
Training Epoch: 72 [5504/50048]	Loss: 0.0750
Training Epoch: 72 [5632/50048]	Loss: 0.0911
Training Epoch: 72 [5760/50048]	Loss: 0.1270
Training Epoch: 72 [5888/50048]	Loss: 0.1394
Training Epoch: 72 [6016/50048]	Loss: 0.1039
Training Epoch: 72 [6144/50048]	Loss: 0.1011
Training Epoch: 72 [6272/50048]	Loss: 0.1295
Training Epoch: 72 [6400/50048]	Loss: 0.0747
Training Epoch: 72 [6528/50048]	Loss: 0.0479
Training Epoch: 72 [6656/50048]	Loss: 0.1537
Training Epoch: 72 [6784/50048]	Loss: 0.0890
Training Epoch: 72 [6912/50048]	Loss: 0.0502
Training Epoch: 72 [7040/50048]	Loss: 0.0999
Training Epoch: 72 [7168/50048]	Loss: 0.0902
Training Epoch: 72 [7296/50048]	Loss: 0.1035
Training Epoch: 72 [7424/50048]	Loss: 0.0820
Training Epoch: 72 [7552/50048]	Loss: 0.2282
Training Epoch: 72 [7680/50048]	Loss: 0.1242
Training Epoch: 72 [7808/50048]	Loss: 0.0958
Training Epoch: 72 [7936/50048]	Loss: 0.2613
Training Epoch: 72 [8064/50048]	Loss: 0.0808
Training Epoch: 72 [8192/50048]	Loss: 0.1572
Training Epoch: 72 [8320/50048]	Loss: 0.1333
Training Epoch: 72 [8448/50048]	Loss: 0.0961
Training Epoch: 72 [8576/50048]	Loss: 0.1249
Training Epoch: 72 [8704/50048]	Loss: 0.1372
Training Epoch: 72 [8832/50048]	Loss: 0.1392
Training Epoch: 72 [8960/50048]	Loss: 0.1147
Training Epoch: 72 [9088/50048]	Loss: 0.1253
Training Epoch: 72 [9216/50048]	Loss: 0.1686
Training Epoch: 72 [9344/50048]	Loss: 0.1435
Training Epoch: 72 [9472/50048]	Loss: 0.0773
Training Epoch: 72 [9600/50048]	Loss: 0.1350
Training Epoch: 72 [9728/50048]	Loss: 0.2027
Training Epoch: 72 [9856/50048]	Loss: 0.1564
Training Epoch: 72 [9984/50048]	Loss: 0.1073
Training Epoch: 72 [10112/50048]	Loss: 0.0951
Training Epoch: 72 [10240/50048]	Loss: 0.0951
Training Epoch: 72 [10368/50048]	Loss: 0.1258
Training Epoch: 72 [10496/50048]	Loss: 0.1985
Training Epoch: 72 [10624/50048]	Loss: 0.1166
Training Epoch: 72 [10752/50048]	Loss: 0.0841
Training Epoch: 72 [10880/50048]	Loss: 0.1170
Training Epoch: 72 [11008/50048]	Loss: 0.1865
Training Epoch: 72 [11136/50048]	Loss: 0.1908
Training Epoch: 72 [11264/50048]	Loss: 0.1789
Training Epoch: 72 [11392/50048]	Loss: 0.1461
Training Epoch: 72 [11520/50048]	Loss: 0.0842
Training Epoch: 72 [11648/50048]	Loss: 0.1078
Training Epoch: 72 [11776/50048]	Loss: 0.0947
Training Epoch: 72 [11904/50048]	Loss: 0.1193
Training Epoch: 72 [12032/50048]	Loss: 0.1697
Training Epoch: 72 [12160/50048]	Loss: 0.2101
Training Epoch: 72 [12288/50048]	Loss: 0.1131
Training Epoch: 72 [12416/50048]	Loss: 0.1810
Training Epoch: 72 [12544/50048]	Loss: 0.0882
Training Epoch: 72 [12672/50048]	Loss: 0.1537
Training Epoch: 72 [12800/50048]	Loss: 0.1433
Training Epoch: 72 [12928/50048]	Loss: 0.1553
Training Epoch: 72 [13056/50048]	Loss: 0.1902
Training Epoch: 72 [13184/50048]	Loss: 0.1034
Training Epoch: 72 [13312/50048]	Loss: 0.1754
Training Epoch: 72 [13440/50048]	Loss: 0.1009
Training Epoch: 72 [13568/50048]	Loss: 0.1066
Training Epoch: 72 [13696/50048]	Loss: 0.1143
Training Epoch: 72 [13824/50048]	Loss: 0.0934
Training Epoch: 72 [13952/50048]	Loss: 0.1147
Training Epoch: 72 [14080/50048]	Loss: 0.1054
Training Epoch: 72 [14208/50048]	Loss: 0.1542
Training Epoch: 72 [14336/50048]	Loss: 0.1204
Training Epoch: 72 [14464/50048]	Loss: 0.0679
Training Epoch: 72 [14592/50048]	Loss: 0.0862
Training Epoch: 72 [14720/50048]	Loss: 0.1106
Training Epoch: 72 [14848/50048]	Loss: 0.1514
Training Epoch: 72 [14976/50048]	Loss: 0.1826
Training Epoch: 72 [15104/50048]	Loss: 0.1619
Training Epoch: 72 [15232/50048]	Loss: 0.0971
Training Epoch: 72 [15360/50048]	Loss: 0.0615
Training Epoch: 72 [15488/50048]	Loss: 0.1262
Training Epoch: 72 [15616/50048]	Loss: 0.1069
Training Epoch: 72 [15744/50048]	Loss: 0.0902
Training Epoch: 72 [15872/50048]	Loss: 0.1106
Training Epoch: 72 [16000/50048]	Loss: 0.1030
Training Epoch: 72 [16128/50048]	Loss: 0.1581
Training Epoch: 72 [16256/50048]	Loss: 0.0996
Training Epoch: 72 [16384/50048]	Loss: 0.2321
Training Epoch: 72 [16512/50048]	Loss: 0.1542
Training Epoch: 72 [16640/50048]	Loss: 0.1288
Training Epoch: 72 [16768/50048]	Loss: 0.1710
Training Epoch: 72 [16896/50048]	Loss: 0.0674
Training Epoch: 72 [17024/50048]	Loss: 0.1556
Training Epoch: 72 [17152/50048]	Loss: 0.1617
Training Epoch: 72 [17280/50048]	Loss: 0.0811
Training Epoch: 72 [17408/50048]	Loss: 0.1822
Training Epoch: 72 [17536/50048]	Loss: 0.0663
Training Epoch: 72 [17664/50048]	Loss: 0.1500
Training Epoch: 72 [17792/50048]	Loss: 0.1666
Training Epoch: 72 [17920/50048]	Loss: 0.1916
Training Epoch: 72 [18048/50048]	Loss: 0.1952
Training Epoch: 72 [18176/50048]	Loss: 0.0812
Training Epoch: 72 [18304/50048]	Loss: 0.1289
Training Epoch: 72 [18432/50048]	Loss: 0.1334
Training Epoch: 72 [18560/50048]	Loss: 0.1635
Training Epoch: 72 [18688/50048]	Loss: 0.1227
Training Epoch: 72 [18816/50048]	Loss: 0.1222
Training Epoch: 72 [18944/50048]	Loss: 0.1000
Training Epoch: 72 [19072/50048]	Loss: 0.1529
Training Epoch: 72 [19200/50048]	Loss: 0.0938
Training Epoch: 72 [19328/50048]	Loss: 0.1169
Training Epoch: 72 [19456/50048]	Loss: 0.0771
Training Epoch: 72 [19584/50048]	Loss: 0.0993
Training Epoch: 72 [19712/50048]	Loss: 0.1005
Training Epoch: 72 [19840/50048]	Loss: 0.1965
Training Epoch: 72 [19968/50048]	Loss: 0.0909
Training Epoch: 72 [20096/50048]	Loss: 0.1421
Training Epoch: 72 [20224/50048]	Loss: 0.1424
Training Epoch: 72 [20352/50048]	Loss: 0.1337
Training Epoch: 72 [20480/50048]	Loss: 0.1491
Training Epoch: 72 [20608/50048]	Loss: 0.0767
Training Epoch: 72 [20736/50048]	Loss: 0.2053
Training Epoch: 72 [20864/50048]	Loss: 0.1164
Training Epoch: 72 [20992/50048]	Loss: 0.2522
Training Epoch: 72 [21120/50048]	Loss: 0.1472
Training Epoch: 72 [21248/50048]	Loss: 0.0987
Training Epoch: 72 [21376/50048]	Loss: 0.1546
Training Epoch: 72 [21504/50048]	Loss: 0.0919
Training Epoch: 72 [21632/50048]	Loss: 0.1271
Training Epoch: 72 [21760/50048]	Loss: 0.1197
Training Epoch: 72 [21888/50048]	Loss: 0.1859
Training Epoch: 72 [22016/50048]	Loss: 0.1120
Training Epoch: 72 [22144/50048]	Loss: 0.0931
Training Epoch: 72 [22272/50048]	Loss: 0.1966
Training Epoch: 72 [22400/50048]	Loss: 0.1013
Training Epoch: 72 [22528/50048]	Loss: 0.1414
Training Epoch: 72 [22656/50048]	Loss: 0.1172
Training Epoch: 72 [22784/50048]	Loss: 0.1363
Training Epoch: 72 [22912/50048]	Loss: 0.2177
Training Epoch: 72 [23040/50048]	Loss: 0.1321
Training Epoch: 72 [23168/50048]	Loss: 0.1764
Training Epoch: 72 [23296/50048]	Loss: 0.1640
Training Epoch: 72 [23424/50048]	Loss: 0.1307
Training Epoch: 72 [23552/50048]	Loss: 0.1177
Training Epoch: 72 [23680/50048]	Loss: 0.0974
Training Epoch: 72 [23808/50048]	Loss: 0.1797
Training Epoch: 72 [23936/50048]	Loss: 0.1106
Training Epoch: 72 [24064/50048]	Loss: 0.1539
Training Epoch: 72 [24192/50048]	Loss: 0.1482
Training Epoch: 72 [24320/50048]	Loss: 0.1846
Training Epoch: 72 [24448/50048]	Loss: 0.1025
Training Epoch: 72 [24576/50048]	Loss: 0.0769
Training Epoch: 72 [24704/50048]	Loss: 0.1324
Training Epoch: 72 [24832/50048]	Loss: 0.1681
Training Epoch: 72 [24960/50048]	Loss: 0.0927
Training Epoch: 72 [25088/50048]	Loss: 0.2654
Training Epoch: 72 [25216/50048]	Loss: 0.0987
Training Epoch: 72 [25344/50048]	Loss: 0.0916
Training Epoch: 72 [25472/50048]	Loss: 0.1346
Training Epoch: 72 [25600/50048]	Loss: 0.2101
Training Epoch: 72 [25728/50048]	Loss: 0.1370
Training Epoch: 72 [25856/50048]	Loss: 0.1305
Training Epoch: 72 [25984/50048]	Loss: 0.0750
Training Epoch: 72 [26112/50048]	Loss: 0.1145
Training Epoch: 72 [26240/50048]	Loss: 0.1210
Training Epoch: 72 [26368/50048]	Loss: 0.1792
Training Epoch: 72 [26496/50048]	Loss: 0.0911
Training Epoch: 72 [26624/50048]	Loss: 0.1403
Training Epoch: 72 [26752/50048]	Loss: 0.1245
Training Epoch: 72 [26880/50048]	Loss: 0.2193
Training Epoch: 72 [27008/50048]	Loss: 0.1475
Training Epoch: 72 [27136/50048]	Loss: 0.0859
Training Epoch: 72 [27264/50048]	Loss: 0.1291
Training Epoch: 72 [27392/50048]	Loss: 0.2131
Training Epoch: 72 [27520/50048]	Loss: 0.1618
Training Epoch: 72 [27648/50048]	Loss: 0.1512
Training Epoch: 72 [27776/50048]	Loss: 0.1376
Training Epoch: 72 [27904/50048]	Loss: 0.1297
Training Epoch: 72 [28032/50048]	Loss: 0.2046
Training Epoch: 72 [28160/50048]	Loss: 0.1308
Training Epoch: 72 [28288/50048]	Loss: 0.1281
Training Epoch: 72 [28416/50048]	Loss: 0.2008
Training Epoch: 72 [28544/50048]	Loss: 0.1742
Training Epoch: 72 [28672/50048]	Loss: 0.2078
Training Epoch: 72 [28800/50048]	Loss: 0.1034
Training Epoch: 72 [28928/50048]	Loss: 0.1493
Training Epoch: 72 [29056/50048]	Loss: 0.1967
Training Epoch: 72 [29184/50048]	Loss: 0.2340
Training Epoch: 72 [29312/50048]	Loss: 0.1063
Training Epoch: 72 [29440/50048]	Loss: 0.0862
Training Epoch: 72 [29568/50048]	Loss: 0.1470
Training Epoch: 72 [29696/50048]	Loss: 0.1232
Training Epoch: 72 [29824/50048]	Loss: 0.1084
Training Epoch: 72 [29952/50048]	Loss: 0.1898
Training Epoch: 72 [30080/50048]	Loss: 0.1147
Training Epoch: 72 [30208/50048]	Loss: 0.2036
Training Epoch: 72 [30336/50048]	Loss: 0.2465
Training Epoch: 72 [30464/50048]	Loss: 0.2187
Training Epoch: 72 [30592/50048]	Loss: 0.1226
Training Epoch: 72 [30720/50048]	Loss: 0.2554
Training Epoch: 72 [30848/50048]	Loss: 0.0796
Training Epoch: 72 [30976/50048]	Loss: 0.0644
Training Epoch: 72 [31104/50048]	Loss: 0.1085
Training Epoch: 72 [31232/50048]	Loss: 0.2206
Training Epoch: 72 [31360/50048]	Loss: 0.1311
Training Epoch: 72 [31488/50048]	Loss: 0.2568
Training Epoch: 72 [31616/50048]	Loss: 0.1319
Training Epoch: 72 [31744/50048]	Loss: 0.1681
Training Epoch: 72 [31872/50048]	Loss: 0.0953
Training Epoch: 72 [32000/50048]	Loss: 0.1500
Training Epoch: 72 [32128/50048]	Loss: 0.1392
Training Epoch: 72 [32256/50048]	Loss: 0.1953
Training Epoch: 72 [32384/50048]	Loss: 0.1308
Training Epoch: 72 [32512/50048]	Loss: 0.0583
Training Epoch: 72 [32640/50048]	Loss: 0.1242
Training Epoch: 72 [32768/50048]	Loss: 0.1661
Training Epoch: 72 [32896/50048]	Loss: 0.1597
Training Epoch: 72 [33024/50048]	Loss: 0.1243
Training Epoch: 72 [33152/50048]	Loss: 0.1176
Training Epoch: 72 [33280/50048]	Loss: 0.0740
Training Epoch: 72 [33408/50048]	Loss: 0.1493
Training Epoch: 72 [33536/50048]	Loss: 0.0918
Training Epoch: 72 [33664/50048]	Loss: 0.1631
Training Epoch: 72 [33792/50048]	Loss: 0.1422
Training Epoch: 72 [33920/50048]	Loss: 0.1739
Training Epoch: 72 [34048/50048]	Loss: 0.0831
Training Epoch: 72 [34176/50048]	Loss: 0.1072
Training Epoch: 72 [34304/50048]	Loss: 0.0752
Training Epoch: 72 [34432/50048]	Loss: 0.1223
Training Epoch: 72 [34560/50048]	Loss: 0.1270
Training Epoch: 72 [34688/50048]	Loss: 0.1695
Training Epoch: 72 [34816/50048]	Loss: 0.1234
Training Epoch: 72 [34944/50048]	Loss: 0.1983
Training Epoch: 72 [35072/50048]	Loss: 0.2088
Training Epoch: 72 [35200/50048]	Loss: 0.1077
Training Epoch: 72 [35328/50048]	Loss: 0.1405
Training Epoch: 72 [35456/50048]	Loss: 0.1221
Training Epoch: 72 [35584/50048]	Loss: 0.1239
Training Epoch: 72 [35712/50048]	Loss: 0.0952
Training Epoch: 72 [35840/50048]	Loss: 0.2024
Training Epoch: 72 [35968/50048]	Loss: 0.1854
Training Epoch: 72 [36096/50048]	Loss: 0.0985
Training Epoch: 72 [36224/50048]	Loss: 0.0416
Training Epoch: 72 [36352/50048]	Loss: 0.1030
Training Epoch: 72 [36480/50048]	Loss: 0.1466
Training Epoch: 72 [36608/50048]	Loss: 0.1119
Training Epoch: 72 [36736/50048]	Loss: 0.2435
Training Epoch: 72 [36864/50048]	Loss: 0.1313
Training Epoch: 72 [36992/50048]	Loss: 0.1318
Training Epoch: 72 [37120/50048]	Loss: 0.1177
Training Epoch: 72 [37248/50048]	Loss: 0.2317
Training Epoch: 72 [37376/50048]	Loss: 0.1108
Training Epoch: 72 [37504/50048]	Loss: 0.1484
Training Epoch: 72 [37632/50048]	Loss: 0.1030
Training Epoch: 72 [37760/50048]	Loss: 0.1523
Training Epoch: 72 [37888/50048]	Loss: 0.1165
Training Epoch: 72 [38016/50048]	Loss: 0.1040
Training Epoch: 72 [38144/50048]	Loss: 0.1089
Training Epoch: 72 [38272/50048]	Loss: 0.1329
Training Epoch: 72 [38400/50048]	Loss: 0.1375
Training Epoch: 72 [38528/50048]	Loss: 0.1238
Training Epoch: 72 [38656/50048]	Loss: 0.1474
Training Epoch: 72 [38784/50048]	Loss: 0.1489
Training Epoch: 72 [38912/50048]	Loss: 0.1134
Training Epoch: 72 [39040/50048]	Loss: 0.1559
Training Epoch: 72 [39168/50048]	Loss: 0.1532
Training Epoch: 72 [39296/50048]	Loss: 0.1669
Training Epoch: 72 [39424/50048]	Loss: 0.0860
Training Epoch: 72 [39552/50048]	Loss: 0.1443
Training Epoch: 72 [39680/50048]	Loss: 0.1769
Training Epoch: 72 [39808/50048]	Loss: 0.1784
Training Epoch: 72 [39936/50048]	Loss: 0.1745
Training Epoch: 72 [40064/50048]	Loss: 0.1255
Training Epoch: 72 [40192/50048]	Loss: 0.1154
Training Epoch: 72 [40320/50048]	Loss: 0.1853
Training Epoch: 72 [40448/50048]	Loss: 0.1457
Training Epoch: 72 [40576/50048]	Loss: 0.2002
Training Epoch: 72 [40704/50048]	Loss: 0.2101
Training Epoch: 72 [40832/50048]	Loss: 0.1394
Training Epoch: 72 [40960/50048]	Loss: 0.1319
Training Epoch: 72 [41088/50048]	Loss: 0.1185
Training Epoch: 72 [41216/50048]	Loss: 0.1864
Training Epoch: 72 [41344/50048]	Loss: 0.1133
Training Epoch: 72 [41472/50048]	Loss: 0.2340
Training Epoch: 72 [41600/50048]	Loss: 0.0892
Training Epoch: 72 [41728/50048]	Loss: 0.1313
Training Epoch: 72 [41856/50048]	Loss: 0.1984
Training Epoch: 72 [41984/50048]	Loss: 0.1952
Training Epoch: 72 [42112/50048]	Loss: 0.2222
Training Epoch: 72 [42240/50048]	Loss: 0.1396
Training Epoch: 72 [42368/50048]	Loss: 0.1432
Training Epoch: 72 [42496/50048]	Loss: 0.1694
Training Epoch: 72 [42624/50048]	Loss: 0.1195
Training Epoch: 72 [42752/50048]	Loss: 0.1470
Training Epoch: 72 [42880/50048]	Loss: 0.1073
Training Epoch: 72 [43008/50048]	Loss: 0.0986
Training Epoch: 72 [43136/50048]	Loss: 0.2326
Training Epoch: 72 [43264/50048]	Loss: 0.1438
Training Epoch: 72 [43392/50048]	Loss: 0.1366
Training Epoch: 72 [43520/50048]	Loss: 0.1714
Training Epoch: 72 [43648/50048]	Loss: 0.1897
Training Epoch: 72 [43776/50048]	Loss: 0.0954
Training Epoch: 72 [43904/50048]	Loss: 0.1362
Training Epoch: 72 [44032/50048]	Loss: 0.1230
Training Epoch: 72 [44160/50048]	Loss: 0.1036
Training Epoch: 72 [44288/50048]	Loss: 0.1220
Training Epoch: 72 [44416/50048]	Loss: 0.1029
Training Epoch: 72 [44544/50048]	Loss: 0.3412
Training Epoch: 72 [44672/50048]	Loss: 0.1378
Training Epoch: 72 [44800/50048]	Loss: 0.1151
Training Epoch: 72 [44928/50048]	Loss: 0.1092
Training Epoch: 72 [45056/50048]	Loss: 0.1514
Training Epoch: 72 [45184/50048]	Loss: 0.1590
Training Epoch: 72 [45312/50048]	Loss: 0.1780
Training Epoch: 72 [45440/50048]	Loss: 0.1162
Training Epoch: 72 [45568/50048]	Loss: 0.1832
Training Epoch: 72 [45696/50048]	Loss: 0.1640
2022-12-06 07:57:36,250 [ZeusDataLoader(train)] train epoch 73 done: time=86.89 energy=10521.59
2022-12-06 07:57:36,252 [ZeusDataLoader(eval)] Epoch 73 begin.
Training Epoch: 72 [45824/50048]	Loss: 0.2118
Training Epoch: 72 [45952/50048]	Loss: 0.1558
Training Epoch: 72 [46080/50048]	Loss: 0.2566
Training Epoch: 72 [46208/50048]	Loss: 0.1246
Training Epoch: 72 [46336/50048]	Loss: 0.1455
Training Epoch: 72 [46464/50048]	Loss: 0.1176
Training Epoch: 72 [46592/50048]	Loss: 0.1527
Training Epoch: 72 [46720/50048]	Loss: 0.2546
Training Epoch: 72 [46848/50048]	Loss: 0.1177
Training Epoch: 72 [46976/50048]	Loss: 0.1458
Training Epoch: 72 [47104/50048]	Loss: 0.2068
Training Epoch: 72 [47232/50048]	Loss: 0.3162
Training Epoch: 72 [47360/50048]	Loss: 0.1415
Training Epoch: 72 [47488/50048]	Loss: 0.1124
Training Epoch: 72 [47616/50048]	Loss: 0.1966
Training Epoch: 72 [47744/50048]	Loss: 0.1473
Training Epoch: 72 [47872/50048]	Loss: 0.1222
Training Epoch: 72 [48000/50048]	Loss: 0.1400
Training Epoch: 72 [48128/50048]	Loss: 0.1524
Training Epoch: 72 [48256/50048]	Loss: 0.1240
Training Epoch: 72 [48384/50048]	Loss: 0.1161
Training Epoch: 72 [48512/50048]	Loss: 0.1653
Training Epoch: 72 [48640/50048]	Loss: 0.2089
Training Epoch: 72 [48768/50048]	Loss: 0.1353
Training Epoch: 72 [48896/50048]	Loss: 0.1795
Training Epoch: 72 [49024/50048]	Loss: 0.2289
Training Epoch: 72 [49152/50048]	Loss: 0.1927
Training Epoch: 72 [49280/50048]	Loss: 0.1910
Training Epoch: 72 [49408/50048]	Loss: 0.2490
Training Epoch: 72 [49536/50048]	Loss: 0.1404
Training Epoch: 72 [49664/50048]	Loss: 0.1292
Training Epoch: 72 [49792/50048]	Loss: 0.1334
Training Epoch: 72 [49920/50048]	Loss: 0.1519
Training Epoch: 72 [50048/50048]	Loss: 0.1455
2022-12-06 12:57:39.932 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:57:39,960 [ZeusDataLoader(eval)] eval epoch 73 done: time=3.70 energy=453.15
2022-12-06 07:57:39,961 [ZeusDataLoader(train)] Up to epoch 73: time=6584.69, energy=799291.09, cost=975806.11
2022-12-06 07:57:39,961 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:57:39,961 [ZeusDataLoader(train)] Expected next epoch: time=6674.49, energy=810089.11, cost=989062.49
2022-12-06 07:57:39,962 [ZeusDataLoader(train)] Epoch 74 begin.
Validation Epoch: 72, Average loss: 0.0177, Accuracy: 0.6269
2022-12-06 07:57:40,175 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:57:40,175 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:57:40.181 [ZeusMonitor] Monitor started.
2022-12-06 12:57:40.181 [ZeusMonitor] Running indefinitely. 2022-12-06 12:57:40.181 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:57:40.181 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e74+gpu0.power.log
Training Epoch: 73 [128/50048]	Loss: 0.1287
Training Epoch: 73 [256/50048]	Loss: 0.1471
Training Epoch: 73 [384/50048]	Loss: 0.1448
Training Epoch: 73 [512/50048]	Loss: 0.1130
Training Epoch: 73 [640/50048]	Loss: 0.0489
Training Epoch: 73 [768/50048]	Loss: 0.1472
Training Epoch: 73 [896/50048]	Loss: 0.1379
Training Epoch: 73 [1024/50048]	Loss: 0.1062
Training Epoch: 73 [1152/50048]	Loss: 0.1514
Training Epoch: 73 [1280/50048]	Loss: 0.0792
Training Epoch: 73 [1408/50048]	Loss: 0.1304
Training Epoch: 73 [1536/50048]	Loss: 0.0962
Training Epoch: 73 [1664/50048]	Loss: 0.0988
Training Epoch: 73 [1792/50048]	Loss: 0.0980
Training Epoch: 73 [1920/50048]	Loss: 0.1300
Training Epoch: 73 [2048/50048]	Loss: 0.1117
Training Epoch: 73 [2176/50048]	Loss: 0.0781
Training Epoch: 73 [2304/50048]	Loss: 0.0743
Training Epoch: 73 [2432/50048]	Loss: 0.0793
Training Epoch: 73 [2560/50048]	Loss: 0.1365
Training Epoch: 73 [2688/50048]	Loss: 0.1466
Training Epoch: 73 [2816/50048]	Loss: 0.1422
Training Epoch: 73 [2944/50048]	Loss: 0.0601
Training Epoch: 73 [3072/50048]	Loss: 0.1461
Training Epoch: 73 [3200/50048]	Loss: 0.1358
Training Epoch: 73 [3328/50048]	Loss: 0.2083
Training Epoch: 73 [3456/50048]	Loss: 0.1038
Training Epoch: 73 [3584/50048]	Loss: 0.1188
Training Epoch: 73 [3712/50048]	Loss: 0.1314
Training Epoch: 73 [3840/50048]	Loss: 0.1015
Training Epoch: 73 [3968/50048]	Loss: 0.1573
Training Epoch: 73 [4096/50048]	Loss: 0.0487
Training Epoch: 73 [4224/50048]	Loss: 0.1156
Training Epoch: 73 [4352/50048]	Loss: 0.0399
Training Epoch: 73 [4480/50048]	Loss: 0.1058
Training Epoch: 73 [4608/50048]	Loss: 0.1328
Training Epoch: 73 [4736/50048]	Loss: 0.1181
Training Epoch: 73 [4864/50048]	Loss: 0.1143
Training Epoch: 73 [4992/50048]	Loss: 0.1456
Training Epoch: 73 [5120/50048]	Loss: 0.0826
Training Epoch: 73 [5248/50048]	Loss: 0.1101
Training Epoch: 73 [5376/50048]	Loss: 0.1124
Training Epoch: 73 [5504/50048]	Loss: 0.1232
Training Epoch: 73 [5632/50048]	Loss: 0.0995
Training Epoch: 73 [5760/50048]	Loss: 0.1348
Training Epoch: 73 [5888/50048]	Loss: 0.1944
Training Epoch: 73 [6016/50048]	Loss: 0.1135
Training Epoch: 73 [6144/50048]	Loss: 0.1546
Training Epoch: 73 [6272/50048]	Loss: 0.1466
Training Epoch: 73 [6400/50048]	Loss: 0.1302
Training Epoch: 73 [6528/50048]	Loss: 0.1451
Training Epoch: 73 [6656/50048]	Loss: 0.1351
Training Epoch: 73 [6784/50048]	Loss: 0.1644
Training Epoch: 73 [6912/50048]	Loss: 0.1066
Training Epoch: 73 [7040/50048]	Loss: 0.1261
Training Epoch: 73 [7168/50048]	Loss: 0.0830
Training Epoch: 73 [7296/50048]	Loss: 0.0723
Training Epoch: 73 [7424/50048]	Loss: 0.0550
Training Epoch: 73 [7552/50048]	Loss: 0.0672
Training Epoch: 73 [7680/50048]	Loss: 0.1554
Training Epoch: 73 [7808/50048]	Loss: 0.1667
Training Epoch: 73 [7936/50048]	Loss: 0.0509
Training Epoch: 73 [8064/50048]	Loss: 0.1231
Training Epoch: 73 [8192/50048]	Loss: 0.1356
Training Epoch: 73 [8320/50048]	Loss: 0.1260
Training Epoch: 73 [8448/50048]	Loss: 0.1144
Training Epoch: 73 [8576/50048]	Loss: 0.1079
Training Epoch: 73 [8704/50048]	Loss: 0.0998
Training Epoch: 73 [8832/50048]	Loss: 0.1105
Training Epoch: 73 [8960/50048]	Loss: 0.1003
Training Epoch: 73 [9088/50048]	Loss: 0.1616
Training Epoch: 73 [9216/50048]	Loss: 0.1081
Training Epoch: 73 [9344/50048]	Loss: 0.1170
Training Epoch: 73 [9472/50048]	Loss: 0.1504
Training Epoch: 73 [9600/50048]	Loss: 0.1495
Training Epoch: 73 [9728/50048]	Loss: 0.1405
Training Epoch: 73 [9856/50048]	Loss: 0.1139
Training Epoch: 73 [9984/50048]	Loss: 0.0754
Training Epoch: 73 [10112/50048]	Loss: 0.1248
Training Epoch: 73 [10240/50048]	Loss: 0.1382
Training Epoch: 73 [10368/50048]	Loss: 0.0783
Training Epoch: 73 [10496/50048]	Loss: 0.1627
Training Epoch: 73 [10624/50048]	Loss: 0.1048
Training Epoch: 73 [10752/50048]	Loss: 0.0927
Training Epoch: 73 [10880/50048]	Loss: 0.0733
Training Epoch: 73 [11008/50048]	Loss: 0.0747
Training Epoch: 73 [11136/50048]	Loss: 0.1439
Training Epoch: 73 [11264/50048]	Loss: 0.1021
Training Epoch: 73 [11392/50048]	Loss: 0.1711
Training Epoch: 73 [11520/50048]	Loss: 0.2173
Training Epoch: 73 [11648/50048]	Loss: 0.1213
Training Epoch: 73 [11776/50048]	Loss: 0.0811
Training Epoch: 73 [11904/50048]	Loss: 0.1103
Training Epoch: 73 [12032/50048]	Loss: 0.1583
Training Epoch: 73 [12160/50048]	Loss: 0.1090
Training Epoch: 73 [12288/50048]	Loss: 0.1575
Training Epoch: 73 [12416/50048]	Loss: 0.0828
Training Epoch: 73 [12544/50048]	Loss: 0.1481
Training Epoch: 73 [12672/50048]	Loss: 0.0966
Training Epoch: 73 [12800/50048]	Loss: 0.0738
Training Epoch: 73 [12928/50048]	Loss: 0.1473
Training Epoch: 73 [13056/50048]	Loss: 0.0688
Training Epoch: 73 [13184/50048]	Loss: 0.1345
Training Epoch: 73 [13312/50048]	Loss: 0.1748
Training Epoch: 73 [13440/50048]	Loss: 0.2421
Training Epoch: 73 [13568/50048]	Loss: 0.1120
Training Epoch: 73 [13696/50048]	Loss: 0.1738
Training Epoch: 73 [13824/50048]	Loss: 0.2085
Training Epoch: 73 [13952/50048]	Loss: 0.1427
Training Epoch: 73 [14080/50048]	Loss: 0.1186
Training Epoch: 73 [14208/50048]	Loss: 0.0948
Training Epoch: 73 [14336/50048]	Loss: 0.0736
Training Epoch: 73 [14464/50048]	Loss: 0.1231
Training Epoch: 73 [14592/50048]	Loss: 0.1148
Training Epoch: 73 [14720/50048]	Loss: 0.1302
Training Epoch: 73 [14848/50048]	Loss: 0.1066
Training Epoch: 73 [14976/50048]	Loss: 0.0923
Training Epoch: 73 [15104/50048]	Loss: 0.0440
Training Epoch: 73 [15232/50048]	Loss: 0.2404
Training Epoch: 73 [15360/50048]	Loss: 0.1341
Training Epoch: 73 [15488/50048]	Loss: 0.0882
Training Epoch: 73 [15616/50048]	Loss: 0.1494
Training Epoch: 73 [15744/50048]	Loss: 0.1150
Training Epoch: 73 [15872/50048]	Loss: 0.1214
Training Epoch: 73 [16000/50048]	Loss: 0.2508
Training Epoch: 73 [16128/50048]	Loss: 0.1236
Training Epoch: 73 [16256/50048]	Loss: 0.1566
Training Epoch: 73 [16384/50048]	Loss: 0.1735
Training Epoch: 73 [16512/50048]	Loss: 0.0991
Training Epoch: 73 [16640/50048]	Loss: 0.1326
Training Epoch: 73 [16768/50048]	Loss: 0.1151
Training Epoch: 73 [16896/50048]	Loss: 0.2362
Training Epoch: 73 [17024/50048]	Loss: 0.1781
Training Epoch: 73 [17152/50048]	Loss: 0.0749
Training Epoch: 73 [17280/50048]	Loss: 0.2367
Training Epoch: 73 [17408/50048]	Loss: 0.1170
Training Epoch: 73 [17536/50048]	Loss: 0.1468
Training Epoch: 73 [17664/50048]	Loss: 0.1941
Training Epoch: 73 [17792/50048]	Loss: 0.1826
Training Epoch: 73 [17920/50048]	Loss: 0.1995
Training Epoch: 73 [18048/50048]	Loss: 0.1287
Training Epoch: 73 [18176/50048]	Loss: 0.1212
Training Epoch: 73 [18304/50048]	Loss: 0.1093
Training Epoch: 73 [18432/50048]	Loss: 0.0699
Training Epoch: 73 [18560/50048]	Loss: 0.0794
Training Epoch: 73 [18688/50048]	Loss: 0.1235
Training Epoch: 73 [18816/50048]	Loss: 0.1554
Training Epoch: 73 [18944/50048]	Loss: 0.1706
Training Epoch: 73 [19072/50048]	Loss: 0.1431
Training Epoch: 73 [19200/50048]	Loss: 0.1240
Training Epoch: 73 [19328/50048]	Loss: 0.1117
Training Epoch: 73 [19456/50048]	Loss: 0.1023
Training Epoch: 73 [19584/50048]	Loss: 0.1338
Training Epoch: 73 [19712/50048]	Loss: 0.0565
Training Epoch: 73 [19840/50048]	Loss: 0.0985
Training Epoch: 73 [19968/50048]	Loss: 0.0731
Training Epoch: 73 [20096/50048]	Loss: 0.1285
Training Epoch: 73 [20224/50048]	Loss: 0.1697
Training Epoch: 73 [20352/50048]	Loss: 0.1366
Training Epoch: 73 [20480/50048]	Loss: 0.1687
Training Epoch: 73 [20608/50048]	Loss: 0.0935
Training Epoch: 73 [20736/50048]	Loss: 0.0904
Training Epoch: 73 [20864/50048]	Loss: 0.1867
Training Epoch: 73 [20992/50048]	Loss: 0.1118
Training Epoch: 73 [21120/50048]	Loss: 0.1755
Training Epoch: 73 [21248/50048]	Loss: 0.1396
Training Epoch: 73 [21376/50048]	Loss: 0.1878
Training Epoch: 73 [21504/50048]	Loss: 0.2213
Training Epoch: 73 [21632/50048]	Loss: 0.0653
Training Epoch: 73 [21760/50048]	Loss: 0.1440
Training Epoch: 73 [21888/50048]	Loss: 0.1371
Training Epoch: 73 [22016/50048]	Loss: 0.1059
Training Epoch: 73 [22144/50048]	Loss: 0.2171
Training Epoch: 73 [22272/50048]	Loss: 0.1459
Training Epoch: 73 [22400/50048]	Loss: 0.1199
Training Epoch: 73 [22528/50048]	Loss: 0.0795
Training Epoch: 73 [22656/50048]	Loss: 0.1314
Training Epoch: 73 [22784/50048]	Loss: 0.0617
Training Epoch: 73 [22912/50048]	Loss: 0.1303
Training Epoch: 73 [23040/50048]	Loss: 0.1096
Training Epoch: 73 [23168/50048]	Loss: 0.1145
Training Epoch: 73 [23296/50048]	Loss: 0.1771
Training Epoch: 73 [23424/50048]	Loss: 0.0714
Training Epoch: 73 [23552/50048]	Loss: 0.0751
Training Epoch: 73 [23680/50048]	Loss: 0.0838
Training Epoch: 73 [23808/50048]	Loss: 0.1315
Training Epoch: 73 [23936/50048]	Loss: 0.1388
Training Epoch: 73 [24064/50048]	Loss: 0.1578
Training Epoch: 73 [24192/50048]	Loss: 0.1260
Training Epoch: 73 [24320/50048]	Loss: 0.1021
Training Epoch: 73 [24448/50048]	Loss: 0.0986
Training Epoch: 73 [24576/50048]	Loss: 0.2141
Training Epoch: 73 [24704/50048]	Loss: 0.1375
Training Epoch: 73 [24832/50048]	Loss: 0.0902
Training Epoch: 73 [24960/50048]	Loss: 0.1133
Training Epoch: 73 [25088/50048]	Loss: 0.1857
Training Epoch: 73 [25216/50048]	Loss: 0.2188
Training Epoch: 73 [25344/50048]	Loss: 0.0736
Training Epoch: 73 [25472/50048]	Loss: 0.1558
Training Epoch: 73 [25600/50048]	Loss: 0.1097
Training Epoch: 73 [25728/50048]	Loss: 0.1974
Training Epoch: 73 [25856/50048]	Loss: 0.1499
Training Epoch: 73 [25984/50048]	Loss: 0.1717
Training Epoch: 73 [26112/50048]	Loss: 0.1994
Training Epoch: 73 [26240/50048]	Loss: 0.1141
Training Epoch: 73 [26368/50048]	Loss: 0.1809
Training Epoch: 73 [26496/50048]	Loss: 0.1038
Training Epoch: 73 [26624/50048]	Loss: 0.0908
Training Epoch: 73 [26752/50048]	Loss: 0.1889
Training Epoch: 73 [26880/50048]	Loss: 0.1113
Training Epoch: 73 [27008/50048]	Loss: 0.1751
Training Epoch: 73 [27136/50048]	Loss: 0.0603
Training Epoch: 73 [27264/50048]	Loss: 0.1107
Training Epoch: 73 [27392/50048]	Loss: 0.0942
Training Epoch: 73 [27520/50048]	Loss: 0.0968
Training Epoch: 73 [27648/50048]	Loss: 0.3508
Training Epoch: 73 [27776/50048]	Loss: 0.1457
Training Epoch: 73 [27904/50048]	Loss: 0.1490
Training Epoch: 73 [28032/50048]	Loss: 0.2023
Training Epoch: 73 [28160/50048]	Loss: 0.1232
Training Epoch: 73 [28288/50048]	Loss: 0.1013
Training Epoch: 73 [28416/50048]	Loss: 0.1381
Training Epoch: 73 [28544/50048]	Loss: 0.1304
Training Epoch: 73 [28672/50048]	Loss: 0.1239
Training Epoch: 73 [28800/50048]	Loss: 0.1429
Training Epoch: 73 [28928/50048]	Loss: 0.1599
Training Epoch: 73 [29056/50048]	Loss: 0.1617
Training Epoch: 73 [29184/50048]	Loss: 0.1124
Training Epoch: 73 [29312/50048]	Loss: 0.0907
Training Epoch: 73 [29440/50048]	Loss: 0.0835
Training Epoch: 73 [29568/50048]	Loss: 0.1007
Training Epoch: 73 [29696/50048]	Loss: 0.1653
Training Epoch: 73 [29824/50048]	Loss: 0.1522
Training Epoch: 73 [29952/50048]	Loss: 0.1416
Training Epoch: 73 [30080/50048]	Loss: 0.1261
Training Epoch: 73 [30208/50048]	Loss: 0.1285
Training Epoch: 73 [30336/50048]	Loss: 0.1082
Training Epoch: 73 [30464/50048]	Loss: 0.0931
Training Epoch: 73 [30592/50048]	Loss: 0.1886
Training Epoch: 73 [30720/50048]	Loss: 0.0514
Training Epoch: 73 [30848/50048]	Loss: 0.1830
Training Epoch: 73 [30976/50048]	Loss: 0.1450
Training Epoch: 73 [31104/50048]	Loss: 0.1208
Training Epoch: 73 [31232/50048]	Loss: 0.1218
Training Epoch: 73 [31360/50048]	Loss: 0.1761
Training Epoch: 73 [31488/50048]	Loss: 0.1982
Training Epoch: 73 [31616/50048]	Loss: 0.1316
Training Epoch: 73 [31744/50048]	Loss: 0.1529
Training Epoch: 73 [31872/50048]	Loss: 0.1097
Training Epoch: 73 [32000/50048]	Loss: 0.0458
Training Epoch: 73 [32128/50048]	Loss: 0.1452
Training Epoch: 73 [32256/50048]	Loss: 0.1648
Training Epoch: 73 [32384/50048]	Loss: 0.1041
Training Epoch: 73 [32512/50048]	Loss: 0.1151
Training Epoch: 73 [32640/50048]	Loss: 0.2112
Training Epoch: 73 [32768/50048]	Loss: 0.2197
Training Epoch: 73 [32896/50048]	Loss: 0.1439
Training Epoch: 73 [33024/50048]	Loss: 0.1394
Training Epoch: 73 [33152/50048]	Loss: 0.1006
Training Epoch: 73 [33280/50048]	Loss: 0.1399
Training Epoch: 73 [33408/50048]	Loss: 0.1652
Training Epoch: 73 [33536/50048]	Loss: 0.1768
Training Epoch: 73 [33664/50048]	Loss: 0.1392
Training Epoch: 73 [33792/50048]	Loss: 0.1519
Training Epoch: 73 [33920/50048]	Loss: 0.1504
Training Epoch: 73 [34048/50048]	Loss: 0.1282
Training Epoch: 73 [34176/50048]	Loss: 0.1290
Training Epoch: 73 [34304/50048]	Loss: 0.1099
Training Epoch: 73 [34432/50048]	Loss: 0.1264
Training Epoch: 73 [34560/50048]	Loss: 0.1602
Training Epoch: 73 [34688/50048]	Loss: 0.1152
Training Epoch: 73 [34816/50048]	Loss: 0.1998
Training Epoch: 73 [34944/50048]	Loss: 0.1160
Training Epoch: 73 [35072/50048]	Loss: 0.1872
Training Epoch: 73 [35200/50048]	Loss: 0.2169
Training Epoch: 73 [35328/50048]	Loss: 0.2289
Training Epoch: 73 [35456/50048]	Loss: 0.1495
Training Epoch: 73 [35584/50048]	Loss: 0.2280
Training Epoch: 73 [35712/50048]	Loss: 0.0644
Training Epoch: 73 [35840/50048]	Loss: 0.1593
Training Epoch: 73 [35968/50048]	Loss: 0.1035
Training Epoch: 73 [36096/50048]	Loss: 0.1261
Training Epoch: 73 [36224/50048]	Loss: 0.1565
Training Epoch: 73 [36352/50048]	Loss: 0.0921
Training Epoch: 73 [36480/50048]	Loss: 0.1101
Training Epoch: 73 [36608/50048]	Loss: 0.1290
Training Epoch: 73 [36736/50048]	Loss: 0.1128
Training Epoch: 73 [36864/50048]	Loss: 0.0858
Training Epoch: 73 [36992/50048]	Loss: 0.1565
Training Epoch: 73 [37120/50048]	Loss: 0.1419
Training Epoch: 73 [37248/50048]	Loss: 0.0522
Training Epoch: 73 [37376/50048]	Loss: 0.1786
Training Epoch: 73 [37504/50048]	Loss: 0.1232
Training Epoch: 73 [37632/50048]	Loss: 0.1223
Training Epoch: 73 [37760/50048]	Loss: 0.1474
Training Epoch: 73 [37888/50048]	Loss: 0.2865
Training Epoch: 73 [38016/50048]	Loss: 0.2939
Training Epoch: 73 [38144/50048]	Loss: 0.1279
Training Epoch: 73 [38272/50048]	Loss: 0.1366
Training Epoch: 73 [38400/50048]	Loss: 0.1653
Training Epoch: 73 [38528/50048]	Loss: 0.1015
Training Epoch: 73 [38656/50048]	Loss: 0.1790
Training Epoch: 73 [38784/50048]	Loss: 0.1080
Training Epoch: 73 [38912/50048]	Loss: 0.1495
Training Epoch: 73 [39040/50048]	Loss: 0.1568
Training Epoch: 73 [39168/50048]	Loss: 0.1126
Training Epoch: 73 [39296/50048]	Loss: 0.0961
Training Epoch: 73 [39424/50048]	Loss: 0.1076
Training Epoch: 73 [39552/50048]	Loss: 0.1579
Training Epoch: 73 [39680/50048]	Loss: 0.2981
Training Epoch: 73 [39808/50048]	Loss: 0.1575
Training Epoch: 73 [39936/50048]	Loss: 0.1979
Training Epoch: 73 [40064/50048]	Loss: 0.1153
Training Epoch: 73 [40192/50048]	Loss: 0.1300
Training Epoch: 73 [40320/50048]	Loss: 0.1426
Training Epoch: 73 [40448/50048]	Loss: 0.2446
Training Epoch: 73 [40576/50048]	Loss: 0.2348
Training Epoch: 73 [40704/50048]	Loss: 0.1497
Training Epoch: 73 [40832/50048]	Loss: 0.1849
Training Epoch: 73 [40960/50048]	Loss: 0.1281
Training Epoch: 73 [41088/50048]	Loss: 0.1798
Training Epoch: 73 [41216/50048]	Loss: 0.1649
Training Epoch: 73 [41344/50048]	Loss: 0.1969
Training Epoch: 73 [41472/50048]	Loss: 0.1659
Training Epoch: 73 [41600/50048]	Loss: 0.2421
Training Epoch: 73 [41728/50048]	Loss: 0.2415
Training Epoch: 73 [41856/50048]	Loss: 0.1574
Training Epoch: 73 [41984/50048]	Loss: 0.1467
Training Epoch: 73 [42112/50048]	Loss: 0.2019
Training Epoch: 73 [42240/50048]	Loss: 0.2290
Training Epoch: 73 [42368/50048]	Loss: 0.1732
Training Epoch: 73 [42496/50048]	Loss: 0.1083
Training Epoch: 73 [42624/50048]	Loss: 0.1317
Training Epoch: 73 [42752/50048]	Loss: 0.1099
Training Epoch: 73 [42880/50048]	Loss: 0.1524
Training Epoch: 73 [43008/50048]	Loss: 0.2110
Training Epoch: 73 [43136/50048]	Loss: 0.0983
Training Epoch: 73 [43264/50048]	Loss: 0.1902
Training Epoch: 73 [43392/50048]	Loss: 0.1622
Training Epoch: 73 [43520/50048]	Loss: 0.1222
Training Epoch: 73 [43648/50048]	Loss: 0.1853
Training Epoch: 73 [43776/50048]	Loss: 0.2112
Training Epoch: 73 [43904/50048]	Loss: 0.1707
Training Epoch: 73 [44032/50048]	Loss: 0.1719
Training Epoch: 73 [44160/50048]	Loss: 0.1132
Training Epoch: 73 [44288/50048]	Loss: 0.1758
Training Epoch: 73 [44416/50048]	Loss: 0.1036
Training Epoch: 73 [44544/50048]	Loss: 0.1474
Training Epoch: 73 [44672/50048]	Loss: 0.1031
Training Epoch: 73 [44800/50048]	Loss: 0.1325
Training Epoch: 73 [44928/50048]	Loss: 0.1400
Training Epoch: 73 [45056/50048]	Loss: 0.1487
Training Epoch: 73 [45184/50048]	Loss: 0.2139
Training Epoch: 73 [45312/50048]	Loss: 0.1750
Training Epoch: 73 [45440/50048]	Loss: 0.0962
Training Epoch: 73 [45568/50048]	Loss: 0.1032
Training Epoch: 73 [45696/50048]	Loss: 0.1226
2022-12-06 07:59:06,438 [ZeusDataLoader(train)] train epoch 74 done: time=86.47 energy=10499.84
2022-12-06 07:59:06,440 [ZeusDataLoader(eval)] Epoch 74 begin.
Training Epoch: 73 [45824/50048]	Loss: 0.1203
Training Epoch: 73 [45952/50048]	Loss: 0.2255
Training Epoch: 73 [46080/50048]	Loss: 0.0927
Training Epoch: 73 [46208/50048]	Loss: 0.1470
Training Epoch: 73 [46336/50048]	Loss: 0.1440
Training Epoch: 73 [46464/50048]	Loss: 0.1434
Training Epoch: 73 [46592/50048]	Loss: 0.1187
Training Epoch: 73 [46720/50048]	Loss: 0.2248
Training Epoch: 73 [46848/50048]	Loss: 0.1727
Training Epoch: 73 [46976/50048]	Loss: 0.1849
Training Epoch: 73 [47104/50048]	Loss: 0.0956
Training Epoch: 73 [47232/50048]	Loss: 0.2012
Training Epoch: 73 [47360/50048]	Loss: 0.1237
Training Epoch: 73 [47488/50048]	Loss: 0.1372
Training Epoch: 73 [47616/50048]	Loss: 0.1055
Training Epoch: 73 [47744/50048]	Loss: 0.1285
Training Epoch: 73 [47872/50048]	Loss: 0.1010
Training Epoch: 73 [48000/50048]	Loss: 0.1139
Training Epoch: 73 [48128/50048]	Loss: 0.1585
Training Epoch: 73 [48256/50048]	Loss: 0.0885
Training Epoch: 73 [48384/50048]	Loss: 0.2358
Training Epoch: 73 [48512/50048]	Loss: 0.1440
Training Epoch: 73 [48640/50048]	Loss: 0.1134
Training Epoch: 73 [48768/50048]	Loss: 0.0990
Training Epoch: 73 [48896/50048]	Loss: 0.0809
Training Epoch: 73 [49024/50048]	Loss: 0.2169
Training Epoch: 73 [49152/50048]	Loss: 0.1869
Training Epoch: 73 [49280/50048]	Loss: 0.1746
Training Epoch: 73 [49408/50048]	Loss: 0.1981
Training Epoch: 73 [49536/50048]	Loss: 0.1107
Training Epoch: 73 [49664/50048]	Loss: 0.0840
Training Epoch: 73 [49792/50048]	Loss: 0.1943
Training Epoch: 73 [49920/50048]	Loss: 0.1688
Training Epoch: 73 [50048/50048]	Loss: 0.1271
2022-12-06 12:59:10.113 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 07:59:10,151 [ZeusDataLoader(eval)] eval epoch 74 done: time=3.70 energy=457.59
2022-12-06 07:59:10,151 [ZeusDataLoader(train)] Up to epoch 74: time=6674.86, energy=810248.52, cost=989174.56
2022-12-06 07:59:10,151 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 07:59:10,151 [ZeusDataLoader(train)] Expected next epoch: time=6764.66, energy=821046.54, cost=1002430.94
2022-12-06 07:59:10,152 [ZeusDataLoader(train)] Epoch 75 begin.
Validation Epoch: 73, Average loss: 0.0174, Accuracy: 0.6308
2022-12-06 07:59:10,300 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 07:59:10,301 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 12:59:10.302 [ZeusMonitor] Monitor started.
2022-12-06 12:59:10.302 [ZeusMonitor] Running indefinitely. 2022-12-06 12:59:10.302 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 12:59:10.302 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e75+gpu0.power.log
Training Epoch: 74 [128/50048]	Loss: 0.1290
Training Epoch: 74 [256/50048]	Loss: 0.1276
Training Epoch: 74 [384/50048]	Loss: 0.1184
Training Epoch: 74 [512/50048]	Loss: 0.1111
Training Epoch: 74 [640/50048]	Loss: 0.0958
Training Epoch: 74 [768/50048]	Loss: 0.1751
Training Epoch: 74 [896/50048]	Loss: 0.0895
Training Epoch: 74 [1024/50048]	Loss: 0.1324
Training Epoch: 74 [1152/50048]	Loss: 0.1315
Training Epoch: 74 [1280/50048]	Loss: 0.1939
Training Epoch: 74 [1408/50048]	Loss: 0.1502
Training Epoch: 74 [1536/50048]	Loss: 0.0586
Training Epoch: 74 [1664/50048]	Loss: 0.1146
Training Epoch: 74 [1792/50048]	Loss: 0.1410
Training Epoch: 74 [1920/50048]	Loss: 0.1629
Training Epoch: 74 [2048/50048]	Loss: 0.1066
Training Epoch: 74 [2176/50048]	Loss: 0.0553
Training Epoch: 74 [2304/50048]	Loss: 0.1245
Training Epoch: 74 [2432/50048]	Loss: 0.1416
Training Epoch: 74 [2560/50048]	Loss: 0.1447
Training Epoch: 74 [2688/50048]	Loss: 0.1862
Training Epoch: 74 [2816/50048]	Loss: 0.1240
Training Epoch: 74 [2944/50048]	Loss: 0.1610
Training Epoch: 74 [3072/50048]	Loss: 0.0949
Training Epoch: 74 [3200/50048]	Loss: 0.0547
Training Epoch: 74 [3328/50048]	Loss: 0.0648
Training Epoch: 74 [3456/50048]	Loss: 0.0821
Training Epoch: 74 [3584/50048]	Loss: 0.1193
Training Epoch: 74 [3712/50048]	Loss: 0.0858
Training Epoch: 74 [3840/50048]	Loss: 0.0762
Training Epoch: 74 [3968/50048]	Loss: 0.1676
Training Epoch: 74 [4096/50048]	Loss: 0.1261
Training Epoch: 74 [4224/50048]	Loss: 0.1930
Training Epoch: 74 [4352/50048]	Loss: 0.1014
Training Epoch: 74 [4480/50048]	Loss: 0.0821
Training Epoch: 74 [4608/50048]	Loss: 0.1244
Training Epoch: 74 [4736/50048]	Loss: 0.1419
Training Epoch: 74 [4864/50048]	Loss: 0.0700
Training Epoch: 74 [4992/50048]	Loss: 0.0686
Training Epoch: 74 [5120/50048]	Loss: 0.1352
Training Epoch: 74 [5248/50048]	Loss: 0.0517
Training Epoch: 74 [5376/50048]	Loss: 0.0743
Training Epoch: 74 [5504/50048]	Loss: 0.0937
Training Epoch: 74 [5632/50048]	Loss: 0.1119
Training Epoch: 74 [5760/50048]	Loss: 0.1345
Training Epoch: 74 [5888/50048]	Loss: 0.1713
Training Epoch: 74 [6016/50048]	Loss: 0.0840
Training Epoch: 74 [6144/50048]	Loss: 0.1297
Training Epoch: 74 [6272/50048]	Loss: 0.0949
Training Epoch: 74 [6400/50048]	Loss: 0.1069
Training Epoch: 74 [6528/50048]	Loss: 0.1561
Training Epoch: 74 [6656/50048]	Loss: 0.1762
Training Epoch: 74 [6784/50048]	Loss: 0.1199
Training Epoch: 74 [6912/50048]	Loss: 0.1799
Training Epoch: 74 [7040/50048]	Loss: 0.1137
Training Epoch: 74 [7168/50048]	Loss: 0.1058
Training Epoch: 74 [7296/50048]	Loss: 0.0771
Training Epoch: 74 [7424/50048]	Loss: 0.1449
Training Epoch: 74 [7552/50048]	Loss: 0.0869
Training Epoch: 74 [7680/50048]	Loss: 0.1268
Training Epoch: 74 [7808/50048]	Loss: 0.0820
Training Epoch: 74 [7936/50048]	Loss: 0.2516
Training Epoch: 74 [8064/50048]	Loss: 0.1366
Training Epoch: 74 [8192/50048]	Loss: 0.1086
Training Epoch: 74 [8320/50048]	Loss: 0.1153
Training Epoch: 74 [8448/50048]	Loss: 0.1547
Training Epoch: 74 [8576/50048]	Loss: 0.2771
Training Epoch: 74 [8704/50048]	Loss: 0.2283
Training Epoch: 74 [8832/50048]	Loss: 0.1704
Training Epoch: 74 [8960/50048]	Loss: 0.1771
Training Epoch: 74 [9088/50048]	Loss: 0.1704
Training Epoch: 74 [9216/50048]	Loss: 0.1117
Training Epoch: 74 [9344/50048]	Loss: 0.1343
Training Epoch: 74 [9472/50048]	Loss: 0.1681
Training Epoch: 74 [9600/50048]	Loss: 0.1412
Training Epoch: 74 [9728/50048]	Loss: 0.1919
Training Epoch: 74 [9856/50048]	Loss: 0.1282
Training Epoch: 74 [9984/50048]	Loss: 0.1218
Training Epoch: 74 [10112/50048]	Loss: 0.1932
Training Epoch: 74 [10240/50048]	Loss: 0.1315
Training Epoch: 74 [10368/50048]	Loss: 0.1475
Training Epoch: 74 [10496/50048]	Loss: 0.0829
Training Epoch: 74 [10624/50048]	Loss: 0.0950
Training Epoch: 74 [10752/50048]	Loss: 0.1076
Training Epoch: 74 [10880/50048]	Loss: 0.1038
Training Epoch: 74 [11008/50048]	Loss: 0.1466
Training Epoch: 74 [11136/50048]	Loss: 0.1259
Training Epoch: 74 [11264/50048]	Loss: 0.1237
Training Epoch: 74 [11392/50048]	Loss: 0.1676
Training Epoch: 74 [11520/50048]	Loss: 0.2107
Training Epoch: 74 [11648/50048]	Loss: 0.1469
Training Epoch: 74 [11776/50048]	Loss: 0.1477
Training Epoch: 74 [11904/50048]	Loss: 0.1333
Training Epoch: 74 [12032/50048]	Loss: 0.1194
Training Epoch: 74 [12160/50048]	Loss: 0.1596
Training Epoch: 74 [12288/50048]	Loss: 0.1273
Training Epoch: 74 [12416/50048]	Loss: 0.0884
Training Epoch: 74 [12544/50048]	Loss: 0.1509
Training Epoch: 74 [12672/50048]	Loss: 0.0952
Training Epoch: 74 [12800/50048]	Loss: 0.2097
Training Epoch: 74 [12928/50048]	Loss: 0.2226
Training Epoch: 74 [13056/50048]	Loss: 0.1514
Training Epoch: 74 [13184/50048]	Loss: 0.0779
Training Epoch: 74 [13312/50048]	Loss: 0.1420
Training Epoch: 74 [13440/50048]	Loss: 0.1643
Training Epoch: 74 [13568/50048]	Loss: 0.1090
Training Epoch: 74 [13696/50048]	Loss: 0.2265
Training Epoch: 74 [13824/50048]	Loss: 0.1160
Training Epoch: 74 [13952/50048]	Loss: 0.0962
Training Epoch: 74 [14080/50048]	Loss: 0.1254
Training Epoch: 74 [14208/50048]	Loss: 0.0606
Training Epoch: 74 [14336/50048]	Loss: 0.0784
Training Epoch: 74 [14464/50048]	Loss: 0.0745
Training Epoch: 74 [14592/50048]	Loss: 0.0971
Training Epoch: 74 [14720/50048]	Loss: 0.0943
Training Epoch: 74 [14848/50048]	Loss: 0.0872
Training Epoch: 74 [14976/50048]	Loss: 0.0916
Training Epoch: 74 [15104/50048]	Loss: 0.1120
Training Epoch: 74 [15232/50048]	Loss: 0.1720
Training Epoch: 74 [15360/50048]	Loss: 0.1483
Training Epoch: 74 [15488/50048]	Loss: 0.0816
Training Epoch: 74 [15616/50048]	Loss: 0.1415
Training Epoch: 74 [15744/50048]	Loss: 0.1137
Training Epoch: 74 [15872/50048]	Loss: 0.1833
Training Epoch: 74 [16000/50048]	Loss: 0.0913
Training Epoch: 74 [16128/50048]	Loss: 0.1509
Training Epoch: 74 [16256/50048]	Loss: 0.1023
Training Epoch: 74 [16384/50048]	Loss: 0.1357
Training Epoch: 74 [16512/50048]	Loss: 0.1516
Training Epoch: 74 [16640/50048]	Loss: 0.1698
Training Epoch: 74 [16768/50048]	Loss: 0.2272
Training Epoch: 74 [16896/50048]	Loss: 0.1364
Training Epoch: 74 [17024/50048]	Loss: 0.1214
Training Epoch: 74 [17152/50048]	Loss: 0.1183
Training Epoch: 74 [17280/50048]	Loss: 0.1136
Training Epoch: 74 [17408/50048]	Loss: 0.1791
Training Epoch: 74 [17536/50048]	Loss: 0.1332
Training Epoch: 74 [17664/50048]	Loss: 0.1729
Training Epoch: 74 [17792/50048]	Loss: 0.1234
Training Epoch: 74 [17920/50048]	Loss: 0.0826
Training Epoch: 74 [18048/50048]	Loss: 0.1925
Training Epoch: 74 [18176/50048]	Loss: 0.1531
Training Epoch: 74 [18304/50048]	Loss: 0.1383
Training Epoch: 74 [18432/50048]	Loss: 0.1504
Training Epoch: 74 [18560/50048]	Loss: 0.1543
Training Epoch: 74 [18688/50048]	Loss: 0.1165
Training Epoch: 74 [18816/50048]	Loss: 0.1216
Training Epoch: 74 [18944/50048]	Loss: 0.0599
Training Epoch: 74 [19072/50048]	Loss: 0.1095
Training Epoch: 74 [19200/50048]	Loss: 0.0785
Training Epoch: 74 [19328/50048]	Loss: 0.1204
Training Epoch: 74 [19456/50048]	Loss: 0.1974
Training Epoch: 74 [19584/50048]	Loss: 0.2534
Training Epoch: 74 [19712/50048]	Loss: 0.1202
Training Epoch: 74 [19840/50048]	Loss: 0.2450
Training Epoch: 74 [19968/50048]	Loss: 0.1733
Training Epoch: 74 [20096/50048]	Loss: 0.1149
Training Epoch: 74 [20224/50048]	Loss: 0.1124
Training Epoch: 74 [20352/50048]	Loss: 0.1598
Training Epoch: 74 [20480/50048]	Loss: 0.1040
Training Epoch: 74 [20608/50048]	Loss: 0.0755
Training Epoch: 74 [20736/50048]	Loss: 0.1610
Training Epoch: 74 [20864/50048]	Loss: 0.1132
Training Epoch: 74 [20992/50048]	Loss: 0.1069
Training Epoch: 74 [21120/50048]	Loss: 0.0823
Training Epoch: 74 [21248/50048]	Loss: 0.1438
Training Epoch: 74 [21376/50048]	Loss: 0.1840
Training Epoch: 74 [21504/50048]	Loss: 0.0946
Training Epoch: 74 [21632/50048]	Loss: 0.1677
Training Epoch: 74 [21760/50048]	Loss: 0.1492
Training Epoch: 74 [21888/50048]	Loss: 0.1187
Training Epoch: 74 [22016/50048]	Loss: 0.0755
Training Epoch: 74 [22144/50048]	Loss: 0.1538
Training Epoch: 74 [22272/50048]	Loss: 0.1541
Training Epoch: 74 [22400/50048]	Loss: 0.1664
Training Epoch: 74 [22528/50048]	Loss: 0.1235
Training Epoch: 74 [22656/50048]	Loss: 0.0973
Training Epoch: 74 [22784/50048]	Loss: 0.1685
Training Epoch: 74 [22912/50048]	Loss: 0.1522
Training Epoch: 74 [23040/50048]	Loss: 0.0624
Training Epoch: 74 [23168/50048]	Loss: 0.0838
Training Epoch: 74 [23296/50048]	Loss: 0.1294
Training Epoch: 74 [23424/50048]	Loss: 0.0988
Training Epoch: 74 [23552/50048]	Loss: 0.1352
Training Epoch: 74 [23680/50048]	Loss: 0.1923
Training Epoch: 74 [23808/50048]	Loss: 0.0935
Training Epoch: 74 [23936/50048]	Loss: 0.1588
Training Epoch: 74 [24064/50048]	Loss: 0.0855
Training Epoch: 74 [24192/50048]	Loss: 0.1566
Training Epoch: 74 [24320/50048]	Loss: 0.1098
Training Epoch: 74 [24448/50048]	Loss: 0.1394
Training Epoch: 74 [24576/50048]	Loss: 0.1962
Training Epoch: 74 [24704/50048]	Loss: 0.1436
Training Epoch: 74 [24832/50048]	Loss: 0.0832
Training Epoch: 74 [24960/50048]	Loss: 0.1324
Training Epoch: 74 [25088/50048]	Loss: 0.1121
Training Epoch: 74 [25216/50048]	Loss: 0.1887
Training Epoch: 74 [25344/50048]	Loss: 0.2199
Training Epoch: 74 [25472/50048]	Loss: 0.1270
Training Epoch: 74 [25600/50048]	Loss: 0.1417
Training Epoch: 74 [25728/50048]	Loss: 0.1053
Training Epoch: 74 [25856/50048]	Loss: 0.1313
Training Epoch: 74 [25984/50048]	Loss: 0.1095
Training Epoch: 74 [26112/50048]	Loss: 0.2009
Training Epoch: 74 [26240/50048]	Loss: 0.0758
Training Epoch: 74 [26368/50048]	Loss: 0.1346
Training Epoch: 74 [26496/50048]	Loss: 0.1717
Training Epoch: 74 [26624/50048]	Loss: 0.1805
Training Epoch: 74 [26752/50048]	Loss: 0.0625
Training Epoch: 74 [26880/50048]	Loss: 0.1744
Training Epoch: 74 [27008/50048]	Loss: 0.1937
Training Epoch: 74 [27136/50048]	Loss: 0.1084
Training Epoch: 74 [27264/50048]	Loss: 0.1740
Training Epoch: 74 [27392/50048]	Loss: 0.0937
Training Epoch: 74 [27520/50048]	Loss: 0.1287
Training Epoch: 74 [27648/50048]	Loss: 0.1943
Training Epoch: 74 [27776/50048]	Loss: 0.1188
Training Epoch: 74 [27904/50048]	Loss: 0.0831
Training Epoch: 74 [28032/50048]	Loss: 0.1283
Training Epoch: 74 [28160/50048]	Loss: 0.1265
Training Epoch: 74 [28288/50048]	Loss: 0.1943
Training Epoch: 74 [28416/50048]	Loss: 0.1259
Training Epoch: 74 [28544/50048]	Loss: 0.0778
Training Epoch: 74 [28672/50048]	Loss: 0.1149
Training Epoch: 74 [28800/50048]	Loss: 0.0509
Training Epoch: 74 [28928/50048]	Loss: 0.1013
Training Epoch: 74 [29056/50048]	Loss: 0.2261
Training Epoch: 74 [29184/50048]	Loss: 0.1902
Training Epoch: 74 [29312/50048]	Loss: 0.0750
Training Epoch: 74 [29440/50048]	Loss: 0.1684
Training Epoch: 74 [29568/50048]	Loss: 0.1538
Training Epoch: 74 [29696/50048]	Loss: 0.0935
Training Epoch: 74 [29824/50048]	Loss: 0.0955
Training Epoch: 74 [29952/50048]	Loss: 0.1093
Training Epoch: 74 [30080/50048]	Loss: 0.0641
Training Epoch: 74 [30208/50048]	Loss: 0.0885
Training Epoch: 74 [30336/50048]	Loss: 0.1251
Training Epoch: 74 [30464/50048]	Loss: 0.0792
Training Epoch: 74 [30592/50048]	Loss: 0.0976
Training Epoch: 74 [30720/50048]	Loss: 0.1790
Training Epoch: 74 [30848/50048]	Loss: 0.1959
Training Epoch: 74 [30976/50048]	Loss: 0.1777
Training Epoch: 74 [31104/50048]	Loss: 0.1066
Training Epoch: 74 [31232/50048]	Loss: 0.1478
Training Epoch: 74 [31360/50048]	Loss: 0.1312
Training Epoch: 74 [31488/50048]	Loss: 0.1007
Training Epoch: 74 [31616/50048]	Loss: 0.2303
Training Epoch: 74 [31744/50048]	Loss: 0.1388
Training Epoch: 74 [31872/50048]	Loss: 0.1671
Training Epoch: 74 [32000/50048]	Loss: 0.0983
Training Epoch: 74 [32128/50048]	Loss: 0.1195
Training Epoch: 74 [32256/50048]	Loss: 0.1962
Training Epoch: 74 [32384/50048]	Loss: 0.1438
Training Epoch: 74 [32512/50048]	Loss: 0.0975
Training Epoch: 74 [32640/50048]	Loss: 0.1003
Training Epoch: 74 [32768/50048]	Loss: 0.1913
Training Epoch: 74 [32896/50048]	Loss: 0.1982
Training Epoch: 74 [33024/50048]	Loss: 0.1089
Training Epoch: 74 [33152/50048]	Loss: 0.1687
Training Epoch: 74 [33280/50048]	Loss: 0.1059
Training Epoch: 74 [33408/50048]	Loss: 0.0652
Training Epoch: 74 [33536/50048]	Loss: 0.1303
Training Epoch: 74 [33664/50048]	Loss: 0.1965
Training Epoch: 74 [33792/50048]	Loss: 0.2070
Training Epoch: 74 [33920/50048]	Loss: 0.1411
Training Epoch: 74 [34048/50048]	Loss: 0.2192
Training Epoch: 74 [34176/50048]	Loss: 0.2776
Training Epoch: 74 [34304/50048]	Loss: 0.1803
Training Epoch: 74 [34432/50048]	Loss: 0.1102
Training Epoch: 74 [34560/50048]	Loss: 0.1114
Training Epoch: 74 [34688/50048]	Loss: 0.1125
Training Epoch: 74 [34816/50048]	Loss: 0.1280
Training Epoch: 74 [34944/50048]	Loss: 0.1555
Training Epoch: 74 [35072/50048]	Loss: 0.0979
Training Epoch: 74 [35200/50048]	Loss: 0.1385
Training Epoch: 74 [35328/50048]	Loss: 0.1669
Training Epoch: 74 [35456/50048]	Loss: 0.1653
Training Epoch: 74 [35584/50048]	Loss: 0.1579
Training Epoch: 74 [35712/50048]	Loss: 0.1233
Training Epoch: 74 [35840/50048]	Loss: 0.1015
Training Epoch: 74 [35968/50048]	Loss: 0.1960
Training Epoch: 74 [36096/50048]	Loss: 0.1905
Training Epoch: 74 [36224/50048]	Loss: 0.1354
Training Epoch: 74 [36352/50048]	Loss: 0.0848
Training Epoch: 74 [36480/50048]	Loss: 0.1114
Training Epoch: 74 [36608/50048]	Loss: 0.0976
Training Epoch: 74 [36736/50048]	Loss: 0.0662
Training Epoch: 74 [36864/50048]	Loss: 0.1548
Training Epoch: 74 [36992/50048]	Loss: 0.1934
Training Epoch: 74 [37120/50048]	Loss: 0.2414
Training Epoch: 74 [37248/50048]	Loss: 0.1125
Training Epoch: 74 [37376/50048]	Loss: 0.2017
Training Epoch: 74 [37504/50048]	Loss: 0.0945
Training Epoch: 74 [37632/50048]	Loss: 0.1605
Training Epoch: 74 [37760/50048]	Loss: 0.1964
Training Epoch: 74 [37888/50048]	Loss: 0.0771
Training Epoch: 74 [38016/50048]	Loss: 0.1200
Training Epoch: 74 [38144/50048]	Loss: 0.2088
Training Epoch: 74 [38272/50048]	Loss: 0.1783
Training Epoch: 74 [38400/50048]	Loss: 0.1418
Training Epoch: 74 [38528/50048]	Loss: 0.0913
Training Epoch: 74 [38656/50048]	Loss: 0.1420
Training Epoch: 74 [38784/50048]	Loss: 0.1419
Training Epoch: 74 [38912/50048]	Loss: 0.1006
Training Epoch: 74 [39040/50048]	Loss: 0.1890
Training Epoch: 74 [39168/50048]	Loss: 0.1249
Training Epoch: 74 [39296/50048]	Loss: 0.1332
Training Epoch: 74 [39424/50048]	Loss: 0.1962
Training Epoch: 74 [39552/50048]	Loss: 0.1562
Training Epoch: 74 [39680/50048]	Loss: 0.1267
Training Epoch: 74 [39808/50048]	Loss: 0.1864
Training Epoch: 74 [39936/50048]	Loss: 0.2314
Training Epoch: 74 [40064/50048]	Loss: 0.1489
Training Epoch: 74 [40192/50048]	Loss: 0.2771
Training Epoch: 74 [40320/50048]	Loss: 0.0916
Training Epoch: 74 [40448/50048]	Loss: 0.1016
Training Epoch: 74 [40576/50048]	Loss: 0.1170
Training Epoch: 74 [40704/50048]	Loss: 0.1671
Training Epoch: 74 [40832/50048]	Loss: 0.1443
Training Epoch: 74 [40960/50048]	Loss: 0.1091
Training Epoch: 74 [41088/50048]	Loss: 0.0921
Training Epoch: 74 [41216/50048]	Loss: 0.1992
Training Epoch: 74 [41344/50048]	Loss: 0.1777
Training Epoch: 74 [41472/50048]	Loss: 0.1426
Training Epoch: 74 [41600/50048]	Loss: 0.1623
Training Epoch: 74 [41728/50048]	Loss: 0.1301
Training Epoch: 74 [41856/50048]	Loss: 0.1809
Training Epoch: 74 [41984/50048]	Loss: 0.1330
Training Epoch: 74 [42112/50048]	Loss: 0.1095
Training Epoch: 74 [42240/50048]	Loss: 0.1656
Training Epoch: 74 [42368/50048]	Loss: 0.1470
Training Epoch: 74 [42496/50048]	Loss: 0.1712
Training Epoch: 74 [42624/50048]	Loss: 0.1513
Training Epoch: 74 [42752/50048]	Loss: 0.1350
Training Epoch: 74 [42880/50048]	Loss: 0.1080
Training Epoch: 74 [43008/50048]	Loss: 0.1698
Training Epoch: 74 [43136/50048]	Loss: 0.1912
Training Epoch: 74 [43264/50048]	Loss: 0.1987
Training Epoch: 74 [43392/50048]	Loss: 0.1334
Training Epoch: 74 [43520/50048]	Loss: 0.2924
Training Epoch: 74 [43648/50048]	Loss: 0.1145
Training Epoch: 74 [43776/50048]	Loss: 0.0976
Training Epoch: 74 [43904/50048]	Loss: 0.0620
Training Epoch: 74 [44032/50048]	Loss: 0.1054
Training Epoch: 74 [44160/50048]	Loss: 0.1172
Training Epoch: 74 [44288/50048]	Loss: 0.1471
Training Epoch: 74 [44416/50048]	Loss: 0.1684
Training Epoch: 74 [44544/50048]	Loss: 0.1952
Training Epoch: 74 [44672/50048]	Loss: 0.1316
Training Epoch: 74 [44800/50048]	Loss: 0.1858
Training Epoch: 74 [44928/50048]	Loss: 0.1248
Training Epoch: 74 [45056/50048]	Loss: 0.0995
Training Epoch: 74 [45184/50048]	Loss: 0.2082
Training Epoch: 74 [45312/50048]	Loss: 0.2157
Training Epoch: 74 [45440/50048]	Loss: 0.1192
Training Epoch: 74 [45568/50048]	Loss: 0.1587
Training Epoch: 74 [45696/50048]	Loss: 0.1466
2022-12-06 08:00:36,671 [ZeusDataLoader(train)] train epoch 75 done: time=86.51 energy=10491.97
2022-12-06 08:00:36,673 [ZeusDataLoader(eval)] Epoch 75 begin.
Training Epoch: 74 [45824/50048]	Loss: 0.1614
Training Epoch: 74 [45952/50048]	Loss: 0.1622
Training Epoch: 74 [46080/50048]	Loss: 0.1328
Training Epoch: 74 [46208/50048]	Loss: 0.1850
Training Epoch: 74 [46336/50048]	Loss: 0.0920
Training Epoch: 74 [46464/50048]	Loss: 0.1423
Training Epoch: 74 [46592/50048]	Loss: 0.1764
Training Epoch: 74 [46720/50048]	Loss: 0.1378
Training Epoch: 74 [46848/50048]	Loss: 0.1057
Training Epoch: 74 [46976/50048]	Loss: 0.0298
Training Epoch: 74 [47104/50048]	Loss: 0.1706
Training Epoch: 74 [47232/50048]	Loss: 0.1878
Training Epoch: 74 [47360/50048]	Loss: 0.1311
Training Epoch: 74 [47488/50048]	Loss: 0.0866
Training Epoch: 74 [47616/50048]	Loss: 0.2134
Training Epoch: 74 [47744/50048]	Loss: 0.1121
Training Epoch: 74 [47872/50048]	Loss: 0.1772
Training Epoch: 74 [48000/50048]	Loss: 0.0913
Training Epoch: 74 [48128/50048]	Loss: 0.0728
Training Epoch: 74 [48256/50048]	Loss: 0.2003
Training Epoch: 74 [48384/50048]	Loss: 0.1187
Training Epoch: 74 [48512/50048]	Loss: 0.1172
Training Epoch: 74 [48640/50048]	Loss: 0.1773
Training Epoch: 74 [48768/50048]	Loss: 0.1409
Training Epoch: 74 [48896/50048]	Loss: 0.1574
Training Epoch: 74 [49024/50048]	Loss: 0.1190
Training Epoch: 74 [49152/50048]	Loss: 0.2140
Training Epoch: 74 [49280/50048]	Loss: 0.1503
Training Epoch: 74 [49408/50048]	Loss: 0.1763
Training Epoch: 74 [49536/50048]	Loss: 0.1344
Training Epoch: 74 [49664/50048]	Loss: 0.0736
Training Epoch: 74 [49792/50048]	Loss: 0.2073
Training Epoch: 74 [49920/50048]	Loss: 0.1976
Training Epoch: 74 [50048/50048]	Loss: 0.1521
2022-12-06 13:00:40.374 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 08:00:40,428 [ZeusDataLoader(eval)] eval epoch 75 done: time=3.75 energy=453.98
2022-12-06 08:00:40,428 [ZeusDataLoader(train)] Up to epoch 75: time=6765.12, energy=821194.47, cost=1002544.83
2022-12-06 08:00:40,428 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 08:00:40,428 [ZeusDataLoader(train)] Expected next epoch: time=6854.91, energy=831992.48, cost=1015801.21
2022-12-06 08:00:40,429 [ZeusDataLoader(train)] Epoch 76 begin.
Validation Epoch: 74, Average loss: 0.0177, Accuracy: 0.6338
2022-12-06 08:00:40,619 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:00:40,620 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:00:40.632 [ZeusMonitor] Monitor started.
2022-12-06 13:00:40.632 [ZeusMonitor] Running indefinitely. 2022-12-06 13:00:40.632 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:00:40.632 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e76+gpu0.power.log
Training Epoch: 75 [128/50048]	Loss: 0.1543
Training Epoch: 75 [256/50048]	Loss: 0.1313
Training Epoch: 75 [384/50048]	Loss: 0.1117
Training Epoch: 75 [512/50048]	Loss: 0.0569
Training Epoch: 75 [640/50048]	Loss: 0.1835
Training Epoch: 75 [768/50048]	Loss: 0.0929
Training Epoch: 75 [896/50048]	Loss: 0.1117
Training Epoch: 75 [1024/50048]	Loss: 0.0583
Training Epoch: 75 [1152/50048]	Loss: 0.0999
Training Epoch: 75 [1280/50048]	Loss: 0.1029
Training Epoch: 75 [1408/50048]	Loss: 0.0483
Training Epoch: 75 [1536/50048]	Loss: 0.1269
Training Epoch: 75 [1664/50048]	Loss: 0.1596
Training Epoch: 75 [1792/50048]	Loss: 0.1350
Training Epoch: 75 [1920/50048]	Loss: 0.1334
Training Epoch: 75 [2048/50048]	Loss: 0.0830
Training Epoch: 75 [2176/50048]	Loss: 0.1047
Training Epoch: 75 [2304/50048]	Loss: 0.0708
Training Epoch: 75 [2432/50048]	Loss: 0.1247
Training Epoch: 75 [2560/50048]	Loss: 0.1479
Training Epoch: 75 [2688/50048]	Loss: 0.1453
Training Epoch: 75 [2816/50048]	Loss: 0.1210
Training Epoch: 75 [2944/50048]	Loss: 0.0523
Training Epoch: 75 [3072/50048]	Loss: 0.1196
Training Epoch: 75 [3200/50048]	Loss: 0.2081
Training Epoch: 75 [3328/50048]	Loss: 0.1033
Training Epoch: 75 [3456/50048]	Loss: 0.0779
Training Epoch: 75 [3584/50048]	Loss: 0.1202
Training Epoch: 75 [3712/50048]	Loss: 0.1487
Training Epoch: 75 [3840/50048]	Loss: 0.0518
Training Epoch: 75 [3968/50048]	Loss: 0.2263
Training Epoch: 75 [4096/50048]	Loss: 0.1223
Training Epoch: 75 [4224/50048]	Loss: 0.1176
Training Epoch: 75 [4352/50048]	Loss: 0.1516
Training Epoch: 75 [4480/50048]	Loss: 0.0739
Training Epoch: 75 [4608/50048]	Loss: 0.1009
Training Epoch: 75 [4736/50048]	Loss: 0.0954
Training Epoch: 75 [4864/50048]	Loss: 0.0721
Training Epoch: 75 [4992/50048]	Loss: 0.0791
Training Epoch: 75 [5120/50048]	Loss: 0.0958
Training Epoch: 75 [5248/50048]	Loss: 0.1086
Training Epoch: 75 [5376/50048]	Loss: 0.0731
Training Epoch: 75 [5504/50048]	Loss: 0.1335
Training Epoch: 75 [5632/50048]	Loss: 0.1529
Training Epoch: 75 [5760/50048]	Loss: 0.0987
Training Epoch: 75 [5888/50048]	Loss: 0.1237
Training Epoch: 75 [6016/50048]	Loss: 0.1107
Training Epoch: 75 [6144/50048]	Loss: 0.1071
Training Epoch: 75 [6272/50048]	Loss: 0.1179
Training Epoch: 75 [6400/50048]	Loss: 0.0770
Training Epoch: 75 [6528/50048]	Loss: 0.0583
Training Epoch: 75 [6656/50048]	Loss: 0.0745
Training Epoch: 75 [6784/50048]	Loss: 0.1796
Training Epoch: 75 [6912/50048]	Loss: 0.2100
Training Epoch: 75 [7040/50048]	Loss: 0.0575
Training Epoch: 75 [7168/50048]	Loss: 0.1488
Training Epoch: 75 [7296/50048]	Loss: 0.0550
Training Epoch: 75 [7424/50048]	Loss: 0.1441
Training Epoch: 75 [7552/50048]	Loss: 0.0915
Training Epoch: 75 [7680/50048]	Loss: 0.1147
Training Epoch: 75 [7808/50048]	Loss: 0.1125
Training Epoch: 75 [7936/50048]	Loss: 0.1751
Training Epoch: 75 [8064/50048]	Loss: 0.1393
Training Epoch: 75 [8192/50048]	Loss: 0.0579
Training Epoch: 75 [8320/50048]	Loss: 0.1239
Training Epoch: 75 [8448/50048]	Loss: 0.0808
Training Epoch: 75 [8576/50048]	Loss: 0.1502
Training Epoch: 75 [8704/50048]	Loss: 0.0752
Training Epoch: 75 [8832/50048]	Loss: 0.1005
Training Epoch: 75 [8960/50048]	Loss: 0.1295
Training Epoch: 75 [9088/50048]	Loss: 0.0616
Training Epoch: 75 [9216/50048]	Loss: 0.1468
Training Epoch: 75 [9344/50048]	Loss: 0.1060
Training Epoch: 75 [9472/50048]	Loss: 0.0875
Training Epoch: 75 [9600/50048]	Loss: 0.1204
Training Epoch: 75 [9728/50048]	Loss: 0.1101
Training Epoch: 75 [9856/50048]	Loss: 0.2844
Training Epoch: 75 [9984/50048]	Loss: 0.1161
Training Epoch: 75 [10112/50048]	Loss: 0.1147
Training Epoch: 75 [10240/50048]	Loss: 0.0672
Training Epoch: 75 [10368/50048]	Loss: 0.1070
Training Epoch: 75 [10496/50048]	Loss: 0.1822
Training Epoch: 75 [10624/50048]	Loss: 0.1216
Training Epoch: 75 [10752/50048]	Loss: 0.0772
Training Epoch: 75 [10880/50048]	Loss: 0.1567
Training Epoch: 75 [11008/50048]	Loss: 0.0825
Training Epoch: 75 [11136/50048]	Loss: 0.0570
Training Epoch: 75 [11264/50048]	Loss: 0.0886
Training Epoch: 75 [11392/50048]	Loss: 0.0940
Training Epoch: 75 [11520/50048]	Loss: 0.1471
Training Epoch: 75 [11648/50048]	Loss: 0.1555
Training Epoch: 75 [11776/50048]	Loss: 0.1215
Training Epoch: 75 [11904/50048]	Loss: 0.1178
Training Epoch: 75 [12032/50048]	Loss: 0.1270
Training Epoch: 75 [12160/50048]	Loss: 0.1164
Training Epoch: 75 [12288/50048]	Loss: 0.1097
Training Epoch: 75 [12416/50048]	Loss: 0.1451
Training Epoch: 75 [12544/50048]	Loss: 0.1621
Training Epoch: 75 [12672/50048]	Loss: 0.1224
Training Epoch: 75 [12800/50048]	Loss: 0.1094
Training Epoch: 75 [12928/50048]	Loss: 0.1296
Training Epoch: 75 [13056/50048]	Loss: 0.0966
Training Epoch: 75 [13184/50048]	Loss: 0.1303
Training Epoch: 75 [13312/50048]	Loss: 0.1819
Training Epoch: 75 [13440/50048]	Loss: 0.1200
Training Epoch: 75 [13568/50048]	Loss: 0.0737
Training Epoch: 75 [13696/50048]	Loss: 0.0763
Training Epoch: 75 [13824/50048]	Loss: 0.1183
Training Epoch: 75 [13952/50048]	Loss: 0.1529
Training Epoch: 75 [14080/50048]	Loss: 0.0778
Training Epoch: 75 [14208/50048]	Loss: 0.1330
Training Epoch: 75 [14336/50048]	Loss: 0.1596
Training Epoch: 75 [14464/50048]	Loss: 0.1630
Training Epoch: 75 [14592/50048]	Loss: 0.0504
Training Epoch: 75 [14720/50048]	Loss: 0.0888
Training Epoch: 75 [14848/50048]	Loss: 0.0843
Training Epoch: 75 [14976/50048]	Loss: 0.1493
Training Epoch: 75 [15104/50048]	Loss: 0.0890
Training Epoch: 75 [15232/50048]	Loss: 0.0827
Training Epoch: 75 [15360/50048]	Loss: 0.1293
Training Epoch: 75 [15488/50048]	Loss: 0.1869
Training Epoch: 75 [15616/50048]	Loss: 0.1329
Training Epoch: 75 [15744/50048]	Loss: 0.1592
Training Epoch: 75 [15872/50048]	Loss: 0.0849
Training Epoch: 75 [16000/50048]	Loss: 0.0739
Training Epoch: 75 [16128/50048]	Loss: 0.1028
Training Epoch: 75 [16256/50048]	Loss: 0.1504
Training Epoch: 75 [16384/50048]	Loss: 0.0794
Training Epoch: 75 [16512/50048]	Loss: 0.0637
Training Epoch: 75 [16640/50048]	Loss: 0.1363
Training Epoch: 75 [16768/50048]	Loss: 0.1106
Training Epoch: 75 [16896/50048]	Loss: 0.0422
Training Epoch: 75 [17024/50048]	Loss: 0.0678
Training Epoch: 75 [17152/50048]	Loss: 0.0844
Training Epoch: 75 [17280/50048]	Loss: 0.1337
Training Epoch: 75 [17408/50048]	Loss: 0.0896
Training Epoch: 75 [17536/50048]	Loss: 0.0926
Training Epoch: 75 [17664/50048]	Loss: 0.1854
Training Epoch: 75 [17792/50048]	Loss: 0.1114
Training Epoch: 75 [17920/50048]	Loss: 0.0707
Training Epoch: 75 [18048/50048]	Loss: 0.1483
Training Epoch: 75 [18176/50048]	Loss: 0.1920
Training Epoch: 75 [18304/50048]	Loss: 0.1056
Training Epoch: 75 [18432/50048]	Loss: 0.0424
Training Epoch: 75 [18560/50048]	Loss: 0.1262
Training Epoch: 75 [18688/50048]	Loss: 0.0755
Training Epoch: 75 [18816/50048]	Loss: 0.1377
Training Epoch: 75 [18944/50048]	Loss: 0.1797
Training Epoch: 75 [19072/50048]	Loss: 0.1117
Training Epoch: 75 [19200/50048]	Loss: 0.1228
Training Epoch: 75 [19328/50048]	Loss: 0.1507
Training Epoch: 75 [19456/50048]	Loss: 0.2920
Training Epoch: 75 [19584/50048]	Loss: 0.1292
Training Epoch: 75 [19712/50048]	Loss: 0.1958
Training Epoch: 75 [19840/50048]	Loss: 0.1028
Training Epoch: 75 [19968/50048]	Loss: 0.1289
Training Epoch: 75 [20096/50048]	Loss: 0.1553
Training Epoch: 75 [20224/50048]	Loss: 0.1033
Training Epoch: 75 [20352/50048]	Loss: 0.0837
Training Epoch: 75 [20480/50048]	Loss: 0.0666
Training Epoch: 75 [20608/50048]	Loss: 0.0770
Training Epoch: 75 [20736/50048]	Loss: 0.1805
Training Epoch: 75 [20864/50048]	Loss: 0.0859
Training Epoch: 75 [20992/50048]	Loss: 0.1148
Training Epoch: 75 [21120/50048]	Loss: 0.1279
Training Epoch: 75 [21248/50048]	Loss: 0.1509
Training Epoch: 75 [21376/50048]	Loss: 0.1302
Training Epoch: 75 [21504/50048]	Loss: 0.1424
Training Epoch: 75 [21632/50048]	Loss: 0.1045
Training Epoch: 75 [21760/50048]	Loss: 0.1978
Training Epoch: 75 [21888/50048]	Loss: 0.1992
Training Epoch: 75 [22016/50048]	Loss: 0.1423
Training Epoch: 75 [22144/50048]	Loss: 0.1181
Training Epoch: 75 [22272/50048]	Loss: 0.0725
Training Epoch: 75 [22400/50048]	Loss: 0.1820
Training Epoch: 75 [22528/50048]	Loss: 0.1227
Training Epoch: 75 [22656/50048]	Loss: 0.1433
Training Epoch: 75 [22784/50048]	Loss: 0.1720
Training Epoch: 75 [22912/50048]	Loss: 0.0911
Training Epoch: 75 [23040/50048]	Loss: 0.1608
Training Epoch: 75 [23168/50048]	Loss: 0.2791
Training Epoch: 75 [23296/50048]	Loss: 0.1335
Training Epoch: 75 [23424/50048]	Loss: 0.2024
Training Epoch: 75 [23552/50048]	Loss: 0.0922
Training Epoch: 75 [23680/50048]	Loss: 0.2059
Training Epoch: 75 [23808/50048]	Loss: 0.1127
Training Epoch: 75 [23936/50048]	Loss: 0.2665
Training Epoch: 75 [24064/50048]	Loss: 0.1022
Training Epoch: 75 [24192/50048]	Loss: 0.1510
Training Epoch: 75 [24320/50048]	Loss: 0.1295
Training Epoch: 75 [24448/50048]	Loss: 0.2106
Training Epoch: 75 [24576/50048]	Loss: 0.1426
Training Epoch: 75 [24704/50048]	Loss: 0.1787
Training Epoch: 75 [24832/50048]	Loss: 0.0952
Training Epoch: 75 [24960/50048]	Loss: 0.1808
Training Epoch: 75 [25088/50048]	Loss: 0.1767
Training Epoch: 75 [25216/50048]	Loss: 0.1114
Training Epoch: 75 [25344/50048]	Loss: 0.1109
Training Epoch: 75 [25472/50048]	Loss: 0.1218
Training Epoch: 75 [25600/50048]	Loss: 0.1580
Training Epoch: 75 [25728/50048]	Loss: 0.1720
Training Epoch: 75 [25856/50048]	Loss: 0.1519
Training Epoch: 75 [25984/50048]	Loss: 0.1382
Training Epoch: 75 [26112/50048]	Loss: 0.0996
Training Epoch: 75 [26240/50048]	Loss: 0.2132
Training Epoch: 75 [26368/50048]	Loss: 0.1653
Training Epoch: 75 [26496/50048]	Loss: 0.0859
Training Epoch: 75 [26624/50048]	Loss: 0.1048
Training Epoch: 75 [26752/50048]	Loss: 0.1514
Training Epoch: 75 [26880/50048]	Loss: 0.1277
Training Epoch: 75 [27008/50048]	Loss: 0.1929
Training Epoch: 75 [27136/50048]	Loss: 0.0785
Training Epoch: 75 [27264/50048]	Loss: 0.0948
Training Epoch: 75 [27392/50048]	Loss: 0.0895
Training Epoch: 75 [27520/50048]	Loss: 0.1361
Training Epoch: 75 [27648/50048]	Loss: 0.1794
Training Epoch: 75 [27776/50048]	Loss: 0.1808
Training Epoch: 75 [27904/50048]	Loss: 0.2274
Training Epoch: 75 [28032/50048]	Loss: 0.1512
Training Epoch: 75 [28160/50048]	Loss: 0.1590
Training Epoch: 75 [28288/50048]	Loss: 0.1247
Training Epoch: 75 [28416/50048]	Loss: 0.1405
Training Epoch: 75 [28544/50048]	Loss: 0.0965
Training Epoch: 75 [28672/50048]	Loss: 0.1200
Training Epoch: 75 [28800/50048]	Loss: 0.1562
Training Epoch: 75 [28928/50048]	Loss: 0.1183
Training Epoch: 75 [29056/50048]	Loss: 0.1205
Training Epoch: 75 [29184/50048]	Loss: 0.1096
Training Epoch: 75 [29312/50048]	Loss: 0.1807
Training Epoch: 75 [29440/50048]	Loss: 0.1187
Training Epoch: 75 [29568/50048]	Loss: 0.1864
Training Epoch: 75 [29696/50048]	Loss: 0.1060
Training Epoch: 75 [29824/50048]	Loss: 0.1524
Training Epoch: 75 [29952/50048]	Loss: 0.1279
Training Epoch: 75 [30080/50048]	Loss: 0.1182
Training Epoch: 75 [30208/50048]	Loss: 0.1182
Training Epoch: 75 [30336/50048]	Loss: 0.1354
Training Epoch: 75 [30464/50048]	Loss: 0.0559
Training Epoch: 75 [30592/50048]	Loss: 0.1212
Training Epoch: 75 [30720/50048]	Loss: 0.0613
Training Epoch: 75 [30848/50048]	Loss: 0.1313
Training Epoch: 75 [30976/50048]	Loss: 0.1286
Training Epoch: 75 [31104/50048]	Loss: 0.0853
Training Epoch: 75 [31232/50048]	Loss: 0.0864
Training Epoch: 75 [31360/50048]	Loss: 0.2051
Training Epoch: 75 [31488/50048]	Loss: 0.0943
Training Epoch: 75 [31616/50048]	Loss: 0.1786
Training Epoch: 75 [31744/50048]	Loss: 0.1036
Training Epoch: 75 [31872/50048]	Loss: 0.0891
Training Epoch: 75 [32000/50048]	Loss: 0.0768
Training Epoch: 75 [32128/50048]	Loss: 0.2038
Training Epoch: 75 [32256/50048]	Loss: 0.1651
Training Epoch: 75 [32384/50048]	Loss: 0.1044
Training Epoch: 75 [32512/50048]	Loss: 0.1447
Training Epoch: 75 [32640/50048]	Loss: 0.1669
Training Epoch: 75 [32768/50048]	Loss: 0.0935
Training Epoch: 75 [32896/50048]	Loss: 0.0770
Training Epoch: 75 [33024/50048]	Loss: 0.0554
Training Epoch: 75 [33152/50048]	Loss: 0.0991
Training Epoch: 75 [33280/50048]	Loss: 0.1179
Training Epoch: 75 [33408/50048]	Loss: 0.1379
Training Epoch: 75 [33536/50048]	Loss: 0.1085
Training Epoch: 75 [33664/50048]	Loss: 0.1248
Training Epoch: 75 [33792/50048]	Loss: 0.1098
Training Epoch: 75 [33920/50048]	Loss: 0.1120
Training Epoch: 75 [34048/50048]	Loss: 0.1149
Training Epoch: 75 [34176/50048]	Loss: 0.0628
Training Epoch: 75 [34304/50048]	Loss: 0.0942
Training Epoch: 75 [34432/50048]	Loss: 0.0650
Training Epoch: 75 [34560/50048]	Loss: 0.1964
Training Epoch: 75 [34688/50048]	Loss: 0.0980
Training Epoch: 75 [34816/50048]	Loss: 0.2076
Training Epoch: 75 [34944/50048]	Loss: 0.0764
Training Epoch: 75 [35072/50048]	Loss: 0.1035
Training Epoch: 75 [35200/50048]	Loss: 0.1171
Training Epoch: 75 [35328/50048]	Loss: 0.1274
Training Epoch: 75 [35456/50048]	Loss: 0.0353
Training Epoch: 75 [35584/50048]	Loss: 0.2140
Training Epoch: 75 [35712/50048]	Loss: 0.1220
Training Epoch: 75 [35840/50048]	Loss: 0.0663
Training Epoch: 75 [35968/50048]	Loss: 0.0558
Training Epoch: 75 [36096/50048]	Loss: 0.0835
Training Epoch: 75 [36224/50048]	Loss: 0.0780
Training Epoch: 75 [36352/50048]	Loss: 0.1072
Training Epoch: 75 [36480/50048]	Loss: 0.1206
Training Epoch: 75 [36608/50048]	Loss: 0.2241
Training Epoch: 75 [36736/50048]	Loss: 0.1897
Training Epoch: 75 [36864/50048]	Loss: 0.1694
Training Epoch: 75 [36992/50048]	Loss: 0.1546
Training Epoch: 75 [37120/50048]	Loss: 0.1944
Training Epoch: 75 [37248/50048]	Loss: 0.1098
Training Epoch: 75 [37376/50048]	Loss: 0.1775
Training Epoch: 75 [37504/50048]	Loss: 0.1474
Training Epoch: 75 [37632/50048]	Loss: 0.1727
Training Epoch: 75 [37760/50048]	Loss: 0.1117
Training Epoch: 75 [37888/50048]	Loss: 0.1218
Training Epoch: 75 [38016/50048]	Loss: 0.1578
Training Epoch: 75 [38144/50048]	Loss: 0.1265
Training Epoch: 75 [38272/50048]	Loss: 0.1999
Training Epoch: 75 [38400/50048]	Loss: 0.1917
Training Epoch: 75 [38528/50048]	Loss: 0.0841
Training Epoch: 75 [38656/50048]	Loss: 0.1955
Training Epoch: 75 [38784/50048]	Loss: 0.1258
Training Epoch: 75 [38912/50048]	Loss: 0.1902
Training Epoch: 75 [39040/50048]	Loss: 0.2682
Training Epoch: 75 [39168/50048]	Loss: 0.1425
Training Epoch: 75 [39296/50048]	Loss: 0.1201
Training Epoch: 75 [39424/50048]	Loss: 0.1495
Training Epoch: 75 [39552/50048]	Loss: 0.1669
Training Epoch: 75 [39680/50048]	Loss: 0.1525
Training Epoch: 75 [39808/50048]	Loss: 0.1298
Training Epoch: 75 [39936/50048]	Loss: 0.0699
Training Epoch: 75 [40064/50048]	Loss: 0.0566
Training Epoch: 75 [40192/50048]	Loss: 0.2211
Training Epoch: 75 [40320/50048]	Loss: 0.1212
Training Epoch: 75 [40448/50048]	Loss: 0.1130
Training Epoch: 75 [40576/50048]	Loss: 0.1275
Training Epoch: 75 [40704/50048]	Loss: 0.0603
Training Epoch: 75 [40832/50048]	Loss: 0.1206
Training Epoch: 75 [40960/50048]	Loss: 0.1162
Training Epoch: 75 [41088/50048]	Loss: 0.1620
Training Epoch: 75 [41216/50048]	Loss: 0.0367
Training Epoch: 75 [41344/50048]	Loss: 0.2010
Training Epoch: 75 [41472/50048]	Loss: 0.2117
Training Epoch: 75 [41600/50048]	Loss: 0.1231
Training Epoch: 75 [41728/50048]	Loss: 0.1548
Training Epoch: 75 [41856/50048]	Loss: 0.1786
Training Epoch: 75 [41984/50048]	Loss: 0.2210
Training Epoch: 75 [42112/50048]	Loss: 0.0825
Training Epoch: 75 [42240/50048]	Loss: 0.1549
Training Epoch: 75 [42368/50048]	Loss: 0.1468
Training Epoch: 75 [42496/50048]	Loss: 0.1309
Training Epoch: 75 [42624/50048]	Loss: 0.0857
Training Epoch: 75 [42752/50048]	Loss: 0.0497
Training Epoch: 75 [42880/50048]	Loss: 0.1386
Training Epoch: 75 [43008/50048]	Loss: 0.1963
Training Epoch: 75 [43136/50048]	Loss: 0.1782
Training Epoch: 75 [43264/50048]	Loss: 0.0895
Training Epoch: 75 [43392/50048]	Loss: 0.1512
Training Epoch: 75 [43520/50048]	Loss: 0.1318
Training Epoch: 75 [43648/50048]	Loss: 0.1358
Training Epoch: 75 [43776/50048]	Loss: 0.0808
Training Epoch: 75 [43904/50048]	Loss: 0.2018
Training Epoch: 75 [44032/50048]	Loss: 0.1765
Training Epoch: 75 [44160/50048]	Loss: 0.1359
Training Epoch: 75 [44288/50048]	Loss: 0.1040
Training Epoch: 75 [44416/50048]	Loss: 0.1116
Training Epoch: 75 [44544/50048]	Loss: 0.2032
Training Epoch: 75 [44672/50048]	Loss: 0.0532
Training Epoch: 75 [44800/50048]	Loss: 0.1283
Training Epoch: 75 [44928/50048]	Loss: 0.3227
Training Epoch: 75 [45056/50048]	Loss: 0.2012
Training Epoch: 75 [45184/50048]	Loss: 0.1179
Training Epoch: 75 [45312/50048]	Loss: 0.1024
Training Epoch: 75 [45440/50048]	Loss: 0.1015
Training Epoch: 75 [45568/50048]	Loss: 0.0923
Training Epoch: 75 [45696/50048]	Loss: 0.1618
2022-12-06 08:02:06,923 [ZeusDataLoader(train)] train epoch 76 done: time=86.48 energy=10507.45
2022-12-06 08:02:06,924 [ZeusDataLoader(eval)] Epoch 76 begin.
Training Epoch: 75 [45824/50048]	Loss: 0.1002
Training Epoch: 75 [45952/50048]	Loss: 0.1150
Training Epoch: 75 [46080/50048]	Loss: 0.0605
Training Epoch: 75 [46208/50048]	Loss: 0.1638
Training Epoch: 75 [46336/50048]	Loss: 0.1544
Training Epoch: 75 [46464/50048]	Loss: 0.1017
Training Epoch: 75 [46592/50048]	Loss: 0.2155
Training Epoch: 75 [46720/50048]	Loss: 0.1685
Training Epoch: 75 [46848/50048]	Loss: 0.1570
Training Epoch: 75 [46976/50048]	Loss: 0.2288
Training Epoch: 75 [47104/50048]	Loss: 0.1934
Training Epoch: 75 [47232/50048]	Loss: 0.1655
Training Epoch: 75 [47360/50048]	Loss: 0.2097
Training Epoch: 75 [47488/50048]	Loss: 0.1671
Training Epoch: 75 [47616/50048]	Loss: 0.2106
Training Epoch: 75 [47744/50048]	Loss: 0.0973
Training Epoch: 75 [47872/50048]	Loss: 0.2075
Training Epoch: 75 [48000/50048]	Loss: 0.0819
Training Epoch: 75 [48128/50048]	Loss: 0.1918
Training Epoch: 75 [48256/50048]	Loss: 0.1652
Training Epoch: 75 [48384/50048]	Loss: 0.1477
Training Epoch: 75 [48512/50048]	Loss: 0.1007
Training Epoch: 75 [48640/50048]	Loss: 0.1194
Training Epoch: 75 [48768/50048]	Loss: 0.1531
Training Epoch: 75 [48896/50048]	Loss: 0.0777
Training Epoch: 75 [49024/50048]	Loss: 0.2158
Training Epoch: 75 [49152/50048]	Loss: 0.0909
Training Epoch: 75 [49280/50048]	Loss: 0.0992
Training Epoch: 75 [49408/50048]	Loss: 0.1139
Training Epoch: 75 [49536/50048]	Loss: 0.0608
Training Epoch: 75 [49664/50048]	Loss: 0.1446
Training Epoch: 75 [49792/50048]	Loss: 0.1003
Training Epoch: 75 [49920/50048]	Loss: 0.1553
Training Epoch: 75 [50048/50048]	Loss: 0.1943
2022-12-06 13:02:10.651 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 08:02:10,686 [ZeusDataLoader(eval)] eval epoch 76 done: time=3.75 energy=453.29
2022-12-06 08:02:10,686 [ZeusDataLoader(train)] Up to epoch 76: time=6855.35, energy=832155.21, cost=1015920.85
2022-12-06 08:02:10,686 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 08:02:10,686 [ZeusDataLoader(train)] Expected next epoch: time=6945.15, energy=842953.23, cost=1029177.23
2022-12-06 08:02:10,687 [ZeusDataLoader(train)] Epoch 77 begin.
Validation Epoch: 75, Average loss: 0.0175, Accuracy: 0.6325
2022-12-06 08:02:10,874 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:02:10,874 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:02:10.876 [ZeusMonitor] Monitor started.
2022-12-06 13:02:10.876 [ZeusMonitor] Running indefinitely. 2022-12-06 13:02:10.876 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:02:10.876 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e77+gpu0.power.log
Training Epoch: 76 [128/50048]	Loss: 0.1240
Training Epoch: 76 [256/50048]	Loss: 0.0972
Training Epoch: 76 [384/50048]	Loss: 0.1010
Training Epoch: 76 [512/50048]	Loss: 0.0556
Training Epoch: 76 [640/50048]	Loss: 0.0641
Training Epoch: 76 [768/50048]	Loss: 0.1052
Training Epoch: 76 [896/50048]	Loss: 0.1325
Training Epoch: 76 [1024/50048]	Loss: 0.1168
Training Epoch: 76 [1152/50048]	Loss: 0.1429
Training Epoch: 76 [1280/50048]	Loss: 0.1130
Training Epoch: 76 [1408/50048]	Loss: 0.1414
Training Epoch: 76 [1536/50048]	Loss: 0.1349
Training Epoch: 76 [1664/50048]	Loss: 0.1184
Training Epoch: 76 [1792/50048]	Loss: 0.0680
Training Epoch: 76 [1920/50048]	Loss: 0.1587
Training Epoch: 76 [2048/50048]	Loss: 0.0818
Training Epoch: 76 [2176/50048]	Loss: 0.1345
Training Epoch: 76 [2304/50048]	Loss: 0.1230
Training Epoch: 76 [2432/50048]	Loss: 0.1352
Training Epoch: 76 [2560/50048]	Loss: 0.1394
Training Epoch: 76 [2688/50048]	Loss: 0.1745
Training Epoch: 76 [2816/50048]	Loss: 0.1259
Training Epoch: 76 [2944/50048]	Loss: 0.0864
Training Epoch: 76 [3072/50048]	Loss: 0.1044
Training Epoch: 76 [3200/50048]	Loss: 0.0906
Training Epoch: 76 [3328/50048]	Loss: 0.1864
Training Epoch: 76 [3456/50048]	Loss: 0.1296
Training Epoch: 76 [3584/50048]	Loss: 0.1585
Training Epoch: 76 [3712/50048]	Loss: 0.0960
Training Epoch: 76 [3840/50048]	Loss: 0.0962
Training Epoch: 76 [3968/50048]	Loss: 0.1173
Training Epoch: 76 [4096/50048]	Loss: 0.1455
Training Epoch: 76 [4224/50048]	Loss: 0.1053
Training Epoch: 76 [4352/50048]	Loss: 0.1271
Training Epoch: 76 [4480/50048]	Loss: 0.0638
Training Epoch: 76 [4608/50048]	Loss: 0.1407
Training Epoch: 76 [4736/50048]	Loss: 0.0940
Training Epoch: 76 [4864/50048]	Loss: 0.0799
Training Epoch: 76 [4992/50048]	Loss: 0.0796
Training Epoch: 76 [5120/50048]	Loss: 0.0662
Training Epoch: 76 [5248/50048]	Loss: 0.0722
Training Epoch: 76 [5376/50048]	Loss: 0.1095
Training Epoch: 76 [5504/50048]	Loss: 0.1460
Training Epoch: 76 [5632/50048]	Loss: 0.1422
Training Epoch: 76 [5760/50048]	Loss: 0.1139
Training Epoch: 76 [5888/50048]	Loss: 0.1292
Training Epoch: 76 [6016/50048]	Loss: 0.1142
Training Epoch: 76 [6144/50048]	Loss: 0.0839
Training Epoch: 76 [6272/50048]	Loss: 0.2058
Training Epoch: 76 [6400/50048]	Loss: 0.1858
Training Epoch: 76 [6528/50048]	Loss: 0.1789
Training Epoch: 76 [6656/50048]	Loss: 0.0832
Training Epoch: 76 [6784/50048]	Loss: 0.0802
Training Epoch: 76 [6912/50048]	Loss: 0.0994
Training Epoch: 76 [7040/50048]	Loss: 0.1037
Training Epoch: 76 [7168/50048]	Loss: 0.0876
Training Epoch: 76 [7296/50048]	Loss: 0.1453
Training Epoch: 76 [7424/50048]	Loss: 0.0754
Training Epoch: 76 [7552/50048]	Loss: 0.0864
Training Epoch: 76 [7680/50048]	Loss: 0.0818
Training Epoch: 76 [7808/50048]	Loss: 0.0964
Training Epoch: 76 [7936/50048]	Loss: 0.0559
Training Epoch: 76 [8064/50048]	Loss: 0.0869
Training Epoch: 76 [8192/50048]	Loss: 0.0600
Training Epoch: 76 [8320/50048]	Loss: 0.0966
Training Epoch: 76 [8448/50048]	Loss: 0.0766
Training Epoch: 76 [8576/50048]	Loss: 0.1205
Training Epoch: 76 [8704/50048]	Loss: 0.1572
Training Epoch: 76 [8832/50048]	Loss: 0.0847
Training Epoch: 76 [8960/50048]	Loss: 0.2305
Training Epoch: 76 [9088/50048]	Loss: 0.1402
Training Epoch: 76 [9216/50048]	Loss: 0.1370
Training Epoch: 76 [9344/50048]	Loss: 0.0710
Training Epoch: 76 [9472/50048]	Loss: 0.0739
Training Epoch: 76 [9600/50048]	Loss: 0.1114
Training Epoch: 76 [9728/50048]	Loss: 0.1130
Training Epoch: 76 [9856/50048]	Loss: 0.0615
Training Epoch: 76 [9984/50048]	Loss: 0.1271
Training Epoch: 76 [10112/50048]	Loss: 0.1635
Training Epoch: 76 [10240/50048]	Loss: 0.1261
Training Epoch: 76 [10368/50048]	Loss: 0.0731
Training Epoch: 76 [10496/50048]	Loss: 0.0976
Training Epoch: 76 [10624/50048]	Loss: 0.0752
Training Epoch: 76 [10752/50048]	Loss: 0.0897
Training Epoch: 76 [10880/50048]	Loss: 0.0570
Training Epoch: 76 [11008/50048]	Loss: 0.0819
Training Epoch: 76 [11136/50048]	Loss: 0.0972
Training Epoch: 76 [11264/50048]	Loss: 0.0517
Training Epoch: 76 [11392/50048]	Loss: 0.0806
Training Epoch: 76 [11520/50048]	Loss: 0.1216
Training Epoch: 76 [11648/50048]	Loss: 0.1140
Training Epoch: 76 [11776/50048]	Loss: 0.0750
Training Epoch: 76 [11904/50048]	Loss: 0.0925
Training Epoch: 76 [12032/50048]	Loss: 0.0638
Training Epoch: 76 [12160/50048]	Loss: 0.1808
Training Epoch: 76 [12288/50048]	Loss: 0.0400
Training Epoch: 76 [12416/50048]	Loss: 0.1948
Training Epoch: 76 [12544/50048]	Loss: 0.1568
Training Epoch: 76 [12672/50048]	Loss: 0.1267
Training Epoch: 76 [12800/50048]	Loss: 0.0707
Training Epoch: 76 [12928/50048]	Loss: 0.0797
Training Epoch: 76 [13056/50048]	Loss: 0.2158
Training Epoch: 76 [13184/50048]	Loss: 0.1166
Training Epoch: 76 [13312/50048]	Loss: 0.0978
Training Epoch: 76 [13440/50048]	Loss: 0.1061
Training Epoch: 76 [13568/50048]	Loss: 0.1262
Training Epoch: 76 [13696/50048]	Loss: 0.1012
Training Epoch: 76 [13824/50048]	Loss: 0.0870
Training Epoch: 76 [13952/50048]	Loss: 0.1488
Training Epoch: 76 [14080/50048]	Loss: 0.0731
Training Epoch: 76 [14208/50048]	Loss: 0.0533
Training Epoch: 76 [14336/50048]	Loss: 0.1301
Training Epoch: 76 [14464/50048]	Loss: 0.1883
Training Epoch: 76 [14592/50048]	Loss: 0.1534
Training Epoch: 76 [14720/50048]	Loss: 0.1941
Training Epoch: 76 [14848/50048]	Loss: 0.1784
Training Epoch: 76 [14976/50048]	Loss: 0.0911
Training Epoch: 76 [15104/50048]	Loss: 0.1108
Training Epoch: 76 [15232/50048]	Loss: 0.0935
Training Epoch: 76 [15360/50048]	Loss: 0.0904
Training Epoch: 76 [15488/50048]	Loss: 0.1062
Training Epoch: 76 [15616/50048]	Loss: 0.0777
Training Epoch: 76 [15744/50048]	Loss: 0.0806
Training Epoch: 76 [15872/50048]	Loss: 0.1155
Training Epoch: 76 [16000/50048]	Loss: 0.1562
Training Epoch: 76 [16128/50048]	Loss: 0.1402
Training Epoch: 76 [16256/50048]	Loss: 0.1187
Training Epoch: 76 [16384/50048]	Loss: 0.0601
Training Epoch: 76 [16512/50048]	Loss: 0.0512
Training Epoch: 76 [16640/50048]	Loss: 0.1527
Training Epoch: 76 [16768/50048]	Loss: 0.1092
Training Epoch: 76 [16896/50048]	Loss: 0.0546
Training Epoch: 76 [17024/50048]	Loss: 0.1067
Training Epoch: 76 [17152/50048]	Loss: 0.0938
Training Epoch: 76 [17280/50048]	Loss: 0.0949
Training Epoch: 76 [17408/50048]	Loss: 0.1120
Training Epoch: 76 [17536/50048]	Loss: 0.1799
Training Epoch: 76 [17664/50048]	Loss: 0.1411
Training Epoch: 76 [17792/50048]	Loss: 0.0637
Training Epoch: 76 [17920/50048]	Loss: 0.0765
Training Epoch: 76 [18048/50048]	Loss: 0.0935
Training Epoch: 76 [18176/50048]	Loss: 0.1773
Training Epoch: 76 [18304/50048]	Loss: 0.1619
Training Epoch: 76 [18432/50048]	Loss: 0.1366
Training Epoch: 76 [18560/50048]	Loss: 0.0914
Training Epoch: 76 [18688/50048]	Loss: 0.1354
Training Epoch: 76 [18816/50048]	Loss: 0.1163
Training Epoch: 76 [18944/50048]	Loss: 0.1512
Training Epoch: 76 [19072/50048]	Loss: 0.1001
Training Epoch: 76 [19200/50048]	Loss: 0.1328
Training Epoch: 76 [19328/50048]	Loss: 0.2249
Training Epoch: 76 [19456/50048]	Loss: 0.1732
Training Epoch: 76 [19584/50048]	Loss: 0.1087
Training Epoch: 76 [19712/50048]	Loss: 0.1033
Training Epoch: 76 [19840/50048]	Loss: 0.1484
Training Epoch: 76 [19968/50048]	Loss: 0.1250
Training Epoch: 76 [20096/50048]	Loss: 0.1462
Training Epoch: 76 [20224/50048]	Loss: 0.1393
Training Epoch: 76 [20352/50048]	Loss: 0.1027
Training Epoch: 76 [20480/50048]	Loss: 0.0397
Training Epoch: 76 [20608/50048]	Loss: 0.1246
Training Epoch: 76 [20736/50048]	Loss: 0.2246
Training Epoch: 76 [20864/50048]	Loss: 0.1089
Training Epoch: 76 [20992/50048]	Loss: 0.0899
Training Epoch: 76 [21120/50048]	Loss: 0.1654
Training Epoch: 76 [21248/50048]	Loss: 0.1527
Training Epoch: 76 [21376/50048]	Loss: 0.1104
Training Epoch: 76 [21504/50048]	Loss: 0.0653
Training Epoch: 76 [21632/50048]	Loss: 0.1120
Training Epoch: 76 [21760/50048]	Loss: 0.1058
Training Epoch: 76 [21888/50048]	Loss: 0.1374
Training Epoch: 76 [22016/50048]	Loss: 0.1384
Training Epoch: 76 [22144/50048]	Loss: 0.1791
Training Epoch: 76 [22272/50048]	Loss: 0.0946
Training Epoch: 76 [22400/50048]	Loss: 0.1483
Training Epoch: 76 [22528/50048]	Loss: 0.1152
Training Epoch: 76 [22656/50048]	Loss: 0.1301
Training Epoch: 76 [22784/50048]	Loss: 0.1060
Training Epoch: 76 [22912/50048]	Loss: 0.1315
Training Epoch: 76 [23040/50048]	Loss: 0.2029
Training Epoch: 76 [23168/50048]	Loss: 0.1238
Training Epoch: 76 [23296/50048]	Loss: 0.1198
Training Epoch: 76 [23424/50048]	Loss: 0.0787
Training Epoch: 76 [23552/50048]	Loss: 0.0780
Training Epoch: 76 [23680/50048]	Loss: 0.2168
Training Epoch: 76 [23808/50048]	Loss: 0.1790
Training Epoch: 76 [23936/50048]	Loss: 0.2014
Training Epoch: 76 [24064/50048]	Loss: 0.1812
Training Epoch: 76 [24192/50048]	Loss: 0.1095
Training Epoch: 76 [24320/50048]	Loss: 0.1253
Training Epoch: 76 [24448/50048]	Loss: 0.0723
Training Epoch: 76 [24576/50048]	Loss: 0.1268
Training Epoch: 76 [24704/50048]	Loss: 0.1610
Training Epoch: 76 [24832/50048]	Loss: 0.1682
Training Epoch: 76 [24960/50048]	Loss: 0.1302
Training Epoch: 76 [25088/50048]	Loss: 0.1161
Training Epoch: 76 [25216/50048]	Loss: 0.1906
Training Epoch: 76 [25344/50048]	Loss: 0.0987
Training Epoch: 76 [25472/50048]	Loss: 0.1237
Training Epoch: 76 [25600/50048]	Loss: 0.0671
Training Epoch: 76 [25728/50048]	Loss: 0.1683
Training Epoch: 76 [25856/50048]	Loss: 0.1865
Training Epoch: 76 [25984/50048]	Loss: 0.0755
Training Epoch: 76 [26112/50048]	Loss: 0.1317
Training Epoch: 76 [26240/50048]	Loss: 0.0877
Training Epoch: 76 [26368/50048]	Loss: 0.1468
Training Epoch: 76 [26496/50048]	Loss: 0.0964
Training Epoch: 76 [26624/50048]	Loss: 0.1135
Training Epoch: 76 [26752/50048]	Loss: 0.0961
Training Epoch: 76 [26880/50048]	Loss: 0.1622
Training Epoch: 76 [27008/50048]	Loss: 0.1090
Training Epoch: 76 [27136/50048]	Loss: 0.1095
Training Epoch: 76 [27264/50048]	Loss: 0.2139
Training Epoch: 76 [27392/50048]	Loss: 0.1048
Training Epoch: 76 [27520/50048]	Loss: 0.1475
Training Epoch: 76 [27648/50048]	Loss: 0.0981
Training Epoch: 76 [27776/50048]	Loss: 0.1037
Training Epoch: 76 [27904/50048]	Loss: 0.0487
Training Epoch: 76 [28032/50048]	Loss: 0.1317
Training Epoch: 76 [28160/50048]	Loss: 0.0715
Training Epoch: 76 [28288/50048]	Loss: 0.0899
Training Epoch: 76 [28416/50048]	Loss: 0.1942
Training Epoch: 76 [28544/50048]	Loss: 0.1156
Training Epoch: 76 [28672/50048]	Loss: 0.0933
Training Epoch: 76 [28800/50048]	Loss: 0.1859
Training Epoch: 76 [28928/50048]	Loss: 0.1102
Training Epoch: 76 [29056/50048]	Loss: 0.1453
Training Epoch: 76 [29184/50048]	Loss: 0.0983
Training Epoch: 76 [29312/50048]	Loss: 0.1932
Training Epoch: 76 [29440/50048]	Loss: 0.0904
Training Epoch: 76 [29568/50048]	Loss: 0.1098
Training Epoch: 76 [29696/50048]	Loss: 0.1697
Training Epoch: 76 [29824/50048]	Loss: 0.1187
Training Epoch: 76 [29952/50048]	Loss: 0.1087
Training Epoch: 76 [30080/50048]	Loss: 0.1322
Training Epoch: 76 [30208/50048]	Loss: 0.1484
Training Epoch: 76 [30336/50048]	Loss: 0.1096
Training Epoch: 76 [30464/50048]	Loss: 0.1013
Training Epoch: 76 [30592/50048]	Loss: 0.2365
Training Epoch: 76 [30720/50048]	Loss: 0.1008
Training Epoch: 76 [30848/50048]	Loss: 0.1006
Training Epoch: 76 [30976/50048]	Loss: 0.0889
Training Epoch: 76 [31104/50048]	Loss: 0.1199
Training Epoch: 76 [31232/50048]	Loss: 0.1433
Training Epoch: 76 [31360/50048]	Loss: 0.0660
Training Epoch: 76 [31488/50048]	Loss: 0.1403
Training Epoch: 76 [31616/50048]	Loss: 0.1247
Training Epoch: 76 [31744/50048]	Loss: 0.0424
Training Epoch: 76 [31872/50048]	Loss: 0.0948
Training Epoch: 76 [32000/50048]	Loss: 0.0766
Training Epoch: 76 [32128/50048]	Loss: 0.1756
Training Epoch: 76 [32256/50048]	Loss: 0.1024
Training Epoch: 76 [32384/50048]	Loss: 0.1396
Training Epoch: 76 [32512/50048]	Loss: 0.0752
Training Epoch: 76 [32640/50048]	Loss: 0.1201
Training Epoch: 76 [32768/50048]	Loss: 0.1823
Training Epoch: 76 [32896/50048]	Loss: 0.0716
Training Epoch: 76 [33024/50048]	Loss: 0.1012
Training Epoch: 76 [33152/50048]	Loss: 0.1134
Training Epoch: 76 [33280/50048]	Loss: 0.1374
Training Epoch: 76 [33408/50048]	Loss: 0.1322
Training Epoch: 76 [33536/50048]	Loss: 0.1351
Training Epoch: 76 [33664/50048]	Loss: 0.1614
Training Epoch: 76 [33792/50048]	Loss: 0.0521
Training Epoch: 76 [33920/50048]	Loss: 0.1672
Training Epoch: 76 [34048/50048]	Loss: 0.1975
Training Epoch: 76 [34176/50048]	Loss: 0.0837
Training Epoch: 76 [34304/50048]	Loss: 0.1234
Training Epoch: 76 [34432/50048]	Loss: 0.1209
Training Epoch: 76 [34560/50048]	Loss: 0.1290
Training Epoch: 76 [34688/50048]	Loss: 0.1105
Training Epoch: 76 [34816/50048]	Loss: 0.1049
Training Epoch: 76 [34944/50048]	Loss: 0.0815
Training Epoch: 76 [35072/50048]	Loss: 0.1893
Training Epoch: 76 [35200/50048]	Loss: 0.1161
Training Epoch: 76 [35328/50048]	Loss: 0.1154
Training Epoch: 76 [35456/50048]	Loss: 0.1427
Training Epoch: 76 [35584/50048]	Loss: 0.0511
Training Epoch: 76 [35712/50048]	Loss: 0.1317
Training Epoch: 76 [35840/50048]	Loss: 0.1624
Training Epoch: 76 [35968/50048]	Loss: 0.1179
Training Epoch: 76 [36096/50048]	Loss: 0.1312
Training Epoch: 76 [36224/50048]	Loss: 0.0882
Training Epoch: 76 [36352/50048]	Loss: 0.0655
Training Epoch: 76 [36480/50048]	Loss: 0.1292
Training Epoch: 76 [36608/50048]	Loss: 0.1261
Training Epoch: 76 [36736/50048]	Loss: 0.1497
Training Epoch: 76 [36864/50048]	Loss: 0.1334
Training Epoch: 76 [36992/50048]	Loss: 0.0778
Training Epoch: 76 [37120/50048]	Loss: 0.1714
Training Epoch: 76 [37248/50048]	Loss: 0.1457
Training Epoch: 76 [37376/50048]	Loss: 0.0838
Training Epoch: 76 [37504/50048]	Loss: 0.1522
Training Epoch: 76 [37632/50048]	Loss: 0.1629
Training Epoch: 76 [37760/50048]	Loss: 0.1213
Training Epoch: 76 [37888/50048]	Loss: 0.1112
Training Epoch: 76 [38016/50048]	Loss: 0.0974
Training Epoch: 76 [38144/50048]	Loss: 0.1135
Training Epoch: 76 [38272/50048]	Loss: 0.0973
Training Epoch: 76 [38400/50048]	Loss: 0.1132
Training Epoch: 76 [38528/50048]	Loss: 0.1433
Training Epoch: 76 [38656/50048]	Loss: 0.0843
Training Epoch: 76 [38784/50048]	Loss: 0.0952
Training Epoch: 76 [38912/50048]	Loss: 0.1502
Training Epoch: 76 [39040/50048]	Loss: 0.0670
Training Epoch: 76 [39168/50048]	Loss: 0.2093
Training Epoch: 76 [39296/50048]	Loss: 0.1103
Training Epoch: 76 [39424/50048]	Loss: 0.1671
Training Epoch: 76 [39552/50048]	Loss: 0.1595
Training Epoch: 76 [39680/50048]	Loss: 0.1051
Training Epoch: 76 [39808/50048]	Loss: 0.1063
Training Epoch: 76 [39936/50048]	Loss: 0.1229
Training Epoch: 76 [40064/50048]	Loss: 0.1413
Training Epoch: 76 [40192/50048]	Loss: 0.1003
Training Epoch: 76 [40320/50048]	Loss: 0.0866
Training Epoch: 76 [40448/50048]	Loss: 0.1377
Training Epoch: 76 [40576/50048]	Loss: 0.1224
Training Epoch: 76 [40704/50048]	Loss: 0.0777
Training Epoch: 76 [40832/50048]	Loss: 0.1364
Training Epoch: 76 [40960/50048]	Loss: 0.1463
Training Epoch: 76 [41088/50048]	Loss: 0.1351
Training Epoch: 76 [41216/50048]	Loss: 0.1395
Training Epoch: 76 [41344/50048]	Loss: 0.2385
Training Epoch: 76 [41472/50048]	Loss: 0.1154
Training Epoch: 76 [41600/50048]	Loss: 0.1403
Training Epoch: 76 [41728/50048]	Loss: 0.1027
Training Epoch: 76 [41856/50048]	Loss: 0.1226
Training Epoch: 76 [41984/50048]	Loss: 0.0977
Training Epoch: 76 [42112/50048]	Loss: 0.2086
Training Epoch: 76 [42240/50048]	Loss: 0.2116
Training Epoch: 76 [42368/50048]	Loss: 0.1645
Training Epoch: 76 [42496/50048]	Loss: 0.2386
Training Epoch: 76 [42624/50048]	Loss: 0.1155
Training Epoch: 76 [42752/50048]	Loss: 0.1521
Training Epoch: 76 [42880/50048]	Loss: 0.1098
Training Epoch: 76 [43008/50048]	Loss: 0.1268
Training Epoch: 76 [43136/50048]	Loss: 0.2644
Training Epoch: 76 [43264/50048]	Loss: 0.1153
Training Epoch: 76 [43392/50048]	Loss: 0.1700
Training Epoch: 76 [43520/50048]	Loss: 0.1310
Training Epoch: 76 [43648/50048]	Loss: 0.1429
Training Epoch: 76 [43776/50048]	Loss: 0.1527
Training Epoch: 76 [43904/50048]	Loss: 0.0972
Training Epoch: 76 [44032/50048]	Loss: 0.1180
Training Epoch: 76 [44160/50048]	Loss: 0.1634
Training Epoch: 76 [44288/50048]	Loss: 0.1304
Training Epoch: 76 [44416/50048]	Loss: 0.1122
Training Epoch: 76 [44544/50048]	Loss: 0.1220
Training Epoch: 76 [44672/50048]	Loss: 0.1281
Training Epoch: 76 [44800/50048]	Loss: 0.1382
Training Epoch: 76 [44928/50048]	Loss: 0.2028
Training Epoch: 76 [45056/50048]	Loss: 0.0884
Training Epoch: 76 [45184/50048]	Loss: 0.1843
Training Epoch: 76 [45312/50048]	Loss: 0.1012
Training Epoch: 76 [45440/50048]	Loss: 0.1259
Training Epoch: 76 [45568/50048]	Loss: 0.0675
Training Epoch: 76 [45696/50048]	Loss: 0.1640
2022-12-06 08:03:37,253 [ZeusDataLoader(train)] train epoch 77 done: time=86.56 energy=10512.52
2022-12-06 08:03:37,254 [ZeusDataLoader(eval)] Epoch 77 begin.
Training Epoch: 76 [45824/50048]	Loss: 0.1049
Training Epoch: 76 [45952/50048]	Loss: 0.0644
Training Epoch: 76 [46080/50048]	Loss: 0.1531
Training Epoch: 76 [46208/50048]	Loss: 0.1168
Training Epoch: 76 [46336/50048]	Loss: 0.0690
Training Epoch: 76 [46464/50048]	Loss: 0.1731
Training Epoch: 76 [46592/50048]	Loss: 0.1474
Training Epoch: 76 [46720/50048]	Loss: 0.0479
Training Epoch: 76 [46848/50048]	Loss: 0.1770
Training Epoch: 76 [46976/50048]	Loss: 0.0856
Training Epoch: 76 [47104/50048]	Loss: 0.1521
Training Epoch: 76 [47232/50048]	Loss: 0.1765
Training Epoch: 76 [47360/50048]	Loss: 0.1741
Training Epoch: 76 [47488/50048]	Loss: 0.2019
Training Epoch: 76 [47616/50048]	Loss: 0.1165
Training Epoch: 76 [47744/50048]	Loss: 0.2364
Training Epoch: 76 [47872/50048]	Loss: 0.0720
Training Epoch: 76 [48000/50048]	Loss: 0.1006
Training Epoch: 76 [48128/50048]	Loss: 0.2040
Training Epoch: 76 [48256/50048]	Loss: 0.1181
Training Epoch: 76 [48384/50048]	Loss: 0.1511
Training Epoch: 76 [48512/50048]	Loss: 0.1450
Training Epoch: 76 [48640/50048]	Loss: 0.1792
Training Epoch: 76 [48768/50048]	Loss: 0.1137
Training Epoch: 76 [48896/50048]	Loss: 0.0812
Training Epoch: 76 [49024/50048]	Loss: 0.1362
Training Epoch: 76 [49152/50048]	Loss: 0.1546
Training Epoch: 76 [49280/50048]	Loss: 0.1594
Training Epoch: 76 [49408/50048]	Loss: 0.1364
Training Epoch: 76 [49536/50048]	Loss: 0.1785
Training Epoch: 76 [49664/50048]	Loss: 0.1437
Training Epoch: 76 [49792/50048]	Loss: 0.1173
Training Epoch: 76 [49920/50048]	Loss: 0.1849
Training Epoch: 76 [50048/50048]	Loss: 0.1458
2022-12-06 13:03:40.919 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 08:03:40,931 [ZeusDataLoader(eval)] eval epoch 77 done: time=3.67 energy=440.46
2022-12-06 08:03:40,932 [ZeusDataLoader(train)] Up to epoch 77: time=6945.58, energy=843108.19, cost=1029291.91
2022-12-06 08:03:40,932 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 08:03:40,932 [ZeusDataLoader(train)] Expected next epoch: time=7035.37, energy=853906.20, cost=1042548.30
2022-12-06 08:03:40,933 [ZeusDataLoader(train)] Epoch 78 begin.
Validation Epoch: 76, Average loss: 0.0177, Accuracy: 0.6382
2022-12-06 08:03:41,126 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:03:41,126 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:03:41.140 [ZeusMonitor] Monitor started.
2022-12-06 13:03:41.140 [ZeusMonitor] Running indefinitely. 2022-12-06 13:03:41.140 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:03:41.140 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e78+gpu0.power.log
Training Epoch: 77 [128/50048]	Loss: 0.1211
Training Epoch: 77 [256/50048]	Loss: 0.1707
Training Epoch: 77 [384/50048]	Loss: 0.0786
Training Epoch: 77 [512/50048]	Loss: 0.0973
Training Epoch: 77 [640/50048]	Loss: 0.1669
Training Epoch: 77 [768/50048]	Loss: 0.1209
Training Epoch: 77 [896/50048]	Loss: 0.1362
Training Epoch: 77 [1024/50048]	Loss: 0.0571
Training Epoch: 77 [1152/50048]	Loss: 0.2406
Training Epoch: 77 [1280/50048]	Loss: 0.0940
Training Epoch: 77 [1408/50048]	Loss: 0.0759
Training Epoch: 77 [1536/50048]	Loss: 0.1259
Training Epoch: 77 [1664/50048]	Loss: 0.1029
Training Epoch: 77 [1792/50048]	Loss: 0.0882
Training Epoch: 77 [1920/50048]	Loss: 0.1069
Training Epoch: 77 [2048/50048]	Loss: 0.1251
Training Epoch: 77 [2176/50048]	Loss: 0.1202
Training Epoch: 77 [2304/50048]	Loss: 0.0664
Training Epoch: 77 [2432/50048]	Loss: 0.0975
Training Epoch: 77 [2560/50048]	Loss: 0.0834
Training Epoch: 77 [2688/50048]	Loss: 0.0902
Training Epoch: 77 [2816/50048]	Loss: 0.0854
Training Epoch: 77 [2944/50048]	Loss: 0.0973
Training Epoch: 77 [3072/50048]	Loss: 0.1099
Training Epoch: 77 [3200/50048]	Loss: 0.1147
Training Epoch: 77 [3328/50048]	Loss: 0.0789
Training Epoch: 77 [3456/50048]	Loss: 0.0978
Training Epoch: 77 [3584/50048]	Loss: 0.1060
Training Epoch: 77 [3712/50048]	Loss: 0.1415
Training Epoch: 77 [3840/50048]	Loss: 0.1863
Training Epoch: 77 [3968/50048]	Loss: 0.1215
Training Epoch: 77 [4096/50048]	Loss: 0.0740
Training Epoch: 77 [4224/50048]	Loss: 0.0943
Training Epoch: 77 [4352/50048]	Loss: 0.1217
Training Epoch: 77 [4480/50048]	Loss: 0.1169
Training Epoch: 77 [4608/50048]	Loss: 0.1795
Training Epoch: 77 [4736/50048]	Loss: 0.0992
Training Epoch: 77 [4864/50048]	Loss: 0.1150
Training Epoch: 77 [4992/50048]	Loss: 0.0796
Training Epoch: 77 [5120/50048]	Loss: 0.1531
Training Epoch: 77 [5248/50048]	Loss: 0.0814
Training Epoch: 77 [5376/50048]	Loss: 0.0691
Training Epoch: 77 [5504/50048]	Loss: 0.1686
Training Epoch: 77 [5632/50048]	Loss: 0.0694
Training Epoch: 77 [5760/50048]	Loss: 0.1272
Training Epoch: 77 [5888/50048]	Loss: 0.1347
Training Epoch: 77 [6016/50048]	Loss: 0.1312
Training Epoch: 77 [6144/50048]	Loss: 0.0869
Training Epoch: 77 [6272/50048]	Loss: 0.0823
Training Epoch: 77 [6400/50048]	Loss: 0.0724
Training Epoch: 77 [6528/50048]	Loss: 0.0464
Training Epoch: 77 [6656/50048]	Loss: 0.1295
Training Epoch: 77 [6784/50048]	Loss: 0.1470
Training Epoch: 77 [6912/50048]	Loss: 0.1606
Training Epoch: 77 [7040/50048]	Loss: 0.0582
Training Epoch: 77 [7168/50048]	Loss: 0.0517
Training Epoch: 77 [7296/50048]	Loss: 0.1273
Training Epoch: 77 [7424/50048]	Loss: 0.1204
Training Epoch: 77 [7552/50048]	Loss: 0.0787
Training Epoch: 77 [7680/50048]	Loss: 0.1370
Training Epoch: 77 [7808/50048]	Loss: 0.1042
Training Epoch: 77 [7936/50048]	Loss: 0.0559
Training Epoch: 77 [8064/50048]	Loss: 0.1542
Training Epoch: 77 [8192/50048]	Loss: 0.0740
Training Epoch: 77 [8320/50048]	Loss: 0.1371
Training Epoch: 77 [8448/50048]	Loss: 0.0852
Training Epoch: 77 [8576/50048]	Loss: 0.1196
Training Epoch: 77 [8704/50048]	Loss: 0.0700
Training Epoch: 77 [8832/50048]	Loss: 0.1113
Training Epoch: 77 [8960/50048]	Loss: 0.0922
Training Epoch: 77 [9088/50048]	Loss: 0.0809
Training Epoch: 77 [9216/50048]	Loss: 0.0924
Training Epoch: 77 [9344/50048]	Loss: 0.0762
Training Epoch: 77 [9472/50048]	Loss: 0.0536
Training Epoch: 77 [9600/50048]	Loss: 0.0897
Training Epoch: 77 [9728/50048]	Loss: 0.0696
Training Epoch: 77 [9856/50048]	Loss: 0.0643
Training Epoch: 77 [9984/50048]	Loss: 0.1556
Training Epoch: 77 [10112/50048]	Loss: 0.1153
Training Epoch: 77 [10240/50048]	Loss: 0.1067
Training Epoch: 77 [10368/50048]	Loss: 0.0709
Training Epoch: 77 [10496/50048]	Loss: 0.1115
Training Epoch: 77 [10624/50048]	Loss: 0.0844
Training Epoch: 77 [10752/50048]	Loss: 0.0633
Training Epoch: 77 [10880/50048]	Loss: 0.0476
Training Epoch: 77 [11008/50048]	Loss: 0.0941
Training Epoch: 77 [11136/50048]	Loss: 0.0667
Training Epoch: 77 [11264/50048]	Loss: 0.0826
Training Epoch: 77 [11392/50048]	Loss: 0.1072
Training Epoch: 77 [11520/50048]	Loss: 0.1036
Training Epoch: 77 [11648/50048]	Loss: 0.0911
Training Epoch: 77 [11776/50048]	Loss: 0.1740
Training Epoch: 77 [11904/50048]	Loss: 0.0809
Training Epoch: 77 [12032/50048]	Loss: 0.1521
Training Epoch: 77 [12160/50048]	Loss: 0.1308
Training Epoch: 77 [12288/50048]	Loss: 0.1031
Training Epoch: 77 [12416/50048]	Loss: 0.1098
Training Epoch: 77 [12544/50048]	Loss: 0.1977
Training Epoch: 77 [12672/50048]	Loss: 0.1604
Training Epoch: 77 [12800/50048]	Loss: 0.0690
Training Epoch: 77 [12928/50048]	Loss: 0.1201
Training Epoch: 77 [13056/50048]	Loss: 0.1504
Training Epoch: 77 [13184/50048]	Loss: 0.1265
Training Epoch: 77 [13312/50048]	Loss: 0.1366
Training Epoch: 77 [13440/50048]	Loss: 0.1026
Training Epoch: 77 [13568/50048]	Loss: 0.1199
Training Epoch: 77 [13696/50048]	Loss: 0.1440
Training Epoch: 77 [13824/50048]	Loss: 0.0909
Training Epoch: 77 [13952/50048]	Loss: 0.0924
Training Epoch: 77 [14080/50048]	Loss: 0.1041
Training Epoch: 77 [14208/50048]	Loss: 0.0952
Training Epoch: 77 [14336/50048]	Loss: 0.0834
Training Epoch: 77 [14464/50048]	Loss: 0.0713
Training Epoch: 77 [14592/50048]	Loss: 0.0903
Training Epoch: 77 [14720/50048]	Loss: 0.1389
Training Epoch: 77 [14848/50048]	Loss: 0.0986
Training Epoch: 77 [14976/50048]	Loss: 0.1652
Training Epoch: 77 [15104/50048]	Loss: 0.1913
Training Epoch: 77 [15232/50048]	Loss: 0.0888
Training Epoch: 77 [15360/50048]	Loss: 0.0993
Training Epoch: 77 [15488/50048]	Loss: 0.1074
Training Epoch: 77 [15616/50048]	Loss: 0.1142
Training Epoch: 77 [15744/50048]	Loss: 0.1440
Training Epoch: 77 [15872/50048]	Loss: 0.1600
Training Epoch: 77 [16000/50048]	Loss: 0.1218
Training Epoch: 77 [16128/50048]	Loss: 0.0744
Training Epoch: 77 [16256/50048]	Loss: 0.1224
Training Epoch: 77 [16384/50048]	Loss: 0.0844
Training Epoch: 77 [16512/50048]	Loss: 0.1102
Training Epoch: 77 [16640/50048]	Loss: 0.1718
Training Epoch: 77 [16768/50048]	Loss: 0.0716
Training Epoch: 77 [16896/50048]	Loss: 0.0507
Training Epoch: 77 [17024/50048]	Loss: 0.1211
Training Epoch: 77 [17152/50048]	Loss: 0.1436
Training Epoch: 77 [17280/50048]	Loss: 0.0827
Training Epoch: 77 [17408/50048]	Loss: 0.1271
Training Epoch: 77 [17536/50048]	Loss: 0.0836
Training Epoch: 77 [17664/50048]	Loss: 0.0958
Training Epoch: 77 [17792/50048]	Loss: 0.1311
Training Epoch: 77 [17920/50048]	Loss: 0.1532
Training Epoch: 77 [18048/50048]	Loss: 0.1870
Training Epoch: 77 [18176/50048]	Loss: 0.1834
Training Epoch: 77 [18304/50048]	Loss: 0.0960
Training Epoch: 77 [18432/50048]	Loss: 0.1166
Training Epoch: 77 [18560/50048]	Loss: 0.0808
Training Epoch: 77 [18688/50048]	Loss: 0.1205
Training Epoch: 77 [18816/50048]	Loss: 0.1073
Training Epoch: 77 [18944/50048]	Loss: 0.0631
Training Epoch: 77 [19072/50048]	Loss: 0.1131
Training Epoch: 77 [19200/50048]	Loss: 0.1319
Training Epoch: 77 [19328/50048]	Loss: 0.0823
Training Epoch: 77 [19456/50048]	Loss: 0.0983
Training Epoch: 77 [19584/50048]	Loss: 0.1255
Training Epoch: 77 [19712/50048]	Loss: 0.1140
Training Epoch: 77 [19840/50048]	Loss: 0.1126
Training Epoch: 77 [19968/50048]	Loss: 0.1102
Training Epoch: 77 [20096/50048]	Loss: 0.1243
Training Epoch: 77 [20224/50048]	Loss: 0.1433
Training Epoch: 77 [20352/50048]	Loss: 0.0487
Training Epoch: 77 [20480/50048]	Loss: 0.1222
Training Epoch: 77 [20608/50048]	Loss: 0.0867
Training Epoch: 77 [20736/50048]	Loss: 0.1025
Training Epoch: 77 [20864/50048]	Loss: 0.0871
Training Epoch: 77 [20992/50048]	Loss: 0.1331
Training Epoch: 77 [21120/50048]	Loss: 0.0938
Training Epoch: 77 [21248/50048]	Loss: 0.0851
Training Epoch: 77 [21376/50048]	Loss: 0.1199
Training Epoch: 77 [21504/50048]	Loss: 0.0903
Training Epoch: 77 [21632/50048]	Loss: 0.1330
Training Epoch: 77 [21760/50048]	Loss: 0.0977
Training Epoch: 77 [21888/50048]	Loss: 0.1032
Training Epoch: 77 [22016/50048]	Loss: 0.1012
Training Epoch: 77 [22144/50048]	Loss: 0.0977
Training Epoch: 77 [22272/50048]	Loss: 0.1139
Training Epoch: 77 [22400/50048]	Loss: 0.1755
Training Epoch: 77 [22528/50048]	Loss: 0.1082
Training Epoch: 77 [22656/50048]	Loss: 0.1064
Training Epoch: 77 [22784/50048]	Loss: 0.1026
Training Epoch: 77 [22912/50048]	Loss: 0.0647
Training Epoch: 77 [23040/50048]	Loss: 0.1498
Training Epoch: 77 [23168/50048]	Loss: 0.1251
Training Epoch: 77 [23296/50048]	Loss: 0.1488
Training Epoch: 77 [23424/50048]	Loss: 0.0766
Training Epoch: 77 [23552/50048]	Loss: 0.0538
Training Epoch: 77 [23680/50048]	Loss: 0.1560
Training Epoch: 77 [23808/50048]	Loss: 0.1793
Training Epoch: 77 [23936/50048]	Loss: 0.1423
Training Epoch: 77 [24064/50048]	Loss: 0.0883
Training Epoch: 77 [24192/50048]	Loss: 0.1326
Training Epoch: 77 [24320/50048]	Loss: 0.1713
Training Epoch: 77 [24448/50048]	Loss: 0.0984
Training Epoch: 77 [24576/50048]	Loss: 0.1166
Training Epoch: 77 [24704/50048]	Loss: 0.2197
Training Epoch: 77 [24832/50048]	Loss: 0.0979
Training Epoch: 77 [24960/50048]	Loss: 0.1194
Training Epoch: 77 [25088/50048]	Loss: 0.0789
Training Epoch: 77 [25216/50048]	Loss: 0.1335
Training Epoch: 77 [25344/50048]	Loss: 0.0706
Training Epoch: 77 [25472/50048]	Loss: 0.0947
Training Epoch: 77 [25600/50048]	Loss: 0.1305
Training Epoch: 77 [25728/50048]	Loss: 0.1493
Training Epoch: 77 [25856/50048]	Loss: 0.1280
Training Epoch: 77 [25984/50048]	Loss: 0.1261
Training Epoch: 77 [26112/50048]	Loss: 0.0671
Training Epoch: 77 [26240/50048]	Loss: 0.1369
Training Epoch: 77 [26368/50048]	Loss: 0.1588
Training Epoch: 77 [26496/50048]	Loss: 0.1317
Training Epoch: 77 [26624/50048]	Loss: 0.0978
Training Epoch: 77 [26752/50048]	Loss: 0.1259
Training Epoch: 77 [26880/50048]	Loss: 0.0838
Training Epoch: 77 [27008/50048]	Loss: 0.1756
Training Epoch: 77 [27136/50048]	Loss: 0.0514
Training Epoch: 77 [27264/50048]	Loss: 0.0827
Training Epoch: 77 [27392/50048]	Loss: 0.1278
Training Epoch: 77 [27520/50048]	Loss: 0.2609
Training Epoch: 77 [27648/50048]	Loss: 0.0850
Training Epoch: 77 [27776/50048]	Loss: 0.1103
Training Epoch: 77 [27904/50048]	Loss: 0.1530
Training Epoch: 77 [28032/50048]	Loss: 0.1718
Training Epoch: 77 [28160/50048]	Loss: 0.0895
Training Epoch: 77 [28288/50048]	Loss: 0.1076
Training Epoch: 77 [28416/50048]	Loss: 0.0716
Training Epoch: 77 [28544/50048]	Loss: 0.1846
Training Epoch: 77 [28672/50048]	Loss: 0.1467
Training Epoch: 77 [28800/50048]	Loss: 0.2189
Training Epoch: 77 [28928/50048]	Loss: 0.1088
Training Epoch: 77 [29056/50048]	Loss: 0.0831
Training Epoch: 77 [29184/50048]	Loss: 0.1168
Training Epoch: 77 [29312/50048]	Loss: 0.1459
Training Epoch: 77 [29440/50048]	Loss: 0.2567
Training Epoch: 77 [29568/50048]	Loss: 0.1341
Training Epoch: 77 [29696/50048]	Loss: 0.1382
Training Epoch: 77 [29824/50048]	Loss: 0.0766
Training Epoch: 77 [29952/50048]	Loss: 0.0833
Training Epoch: 77 [30080/50048]	Loss: 0.1201
Training Epoch: 77 [30208/50048]	Loss: 0.1862
Training Epoch: 77 [30336/50048]	Loss: 0.1139
Training Epoch: 77 [30464/50048]	Loss: 0.0769
Training Epoch: 77 [30592/50048]	Loss: 0.0890
Training Epoch: 77 [30720/50048]	Loss: 0.1214
Training Epoch: 77 [30848/50048]	Loss: 0.1059
Training Epoch: 77 [30976/50048]	Loss: 0.1210
Training Epoch: 77 [31104/50048]	Loss: 0.0969
Training Epoch: 77 [31232/50048]	Loss: 0.1265
Training Epoch: 77 [31360/50048]	Loss: 0.0964
Training Epoch: 77 [31488/50048]	Loss: 0.1509
Training Epoch: 77 [31616/50048]	Loss: 0.1153
Training Epoch: 77 [31744/50048]	Loss: 0.0875
Training Epoch: 77 [31872/50048]	Loss: 0.1455
Training Epoch: 77 [32000/50048]	Loss: 0.1500
Training Epoch: 77 [32128/50048]	Loss: 0.0734
Training Epoch: 77 [32256/50048]	Loss: 0.1173
Training Epoch: 77 [32384/50048]	Loss: 0.1493
Training Epoch: 77 [32512/50048]	Loss: 0.1841
Training Epoch: 77 [32640/50048]	Loss: 0.1666
Training Epoch: 77 [32768/50048]	Loss: 0.0705
Training Epoch: 77 [32896/50048]	Loss: 0.0931
Training Epoch: 77 [33024/50048]	Loss: 0.1274
Training Epoch: 77 [33152/50048]	Loss: 0.1041
Training Epoch: 77 [33280/50048]	Loss: 0.1342
Training Epoch: 77 [33408/50048]	Loss: 0.1638
Training Epoch: 77 [33536/50048]	Loss: 0.0995
Training Epoch: 77 [33664/50048]	Loss: 0.0798
Training Epoch: 77 [33792/50048]	Loss: 0.1571
Training Epoch: 77 [33920/50048]	Loss: 0.0991
Training Epoch: 77 [34048/50048]	Loss: 0.1285
Training Epoch: 77 [34176/50048]	Loss: 0.2693
Training Epoch: 77 [34304/50048]	Loss: 0.1828
Training Epoch: 77 [34432/50048]	Loss: 0.0877
Training Epoch: 77 [34560/50048]	Loss: 0.1356
Training Epoch: 77 [34688/50048]	Loss: 0.1222
Training Epoch: 77 [34816/50048]	Loss: 0.0998
Training Epoch: 77 [34944/50048]	Loss: 0.1403
Training Epoch: 77 [35072/50048]	Loss: 0.1723
Training Epoch: 77 [35200/50048]	Loss: 0.0965
Training Epoch: 77 [35328/50048]	Loss: 0.1965
Training Epoch: 77 [35456/50048]	Loss: 0.0811
Training Epoch: 77 [35584/50048]	Loss: 0.1223
Training Epoch: 77 [35712/50048]	Loss: 0.1880
Training Epoch: 77 [35840/50048]	Loss: 0.1260
Training Epoch: 77 [35968/50048]	Loss: 0.1135
Training Epoch: 77 [36096/50048]	Loss: 0.2329
Training Epoch: 77 [36224/50048]	Loss: 0.1271
Training Epoch: 77 [36352/50048]	Loss: 0.0708
Training Epoch: 77 [36480/50048]	Loss: 0.1420
Training Epoch: 77 [36608/50048]	Loss: 0.1520
Training Epoch: 77 [36736/50048]	Loss: 0.0739
Training Epoch: 77 [36864/50048]	Loss: 0.0806
Training Epoch: 77 [36992/50048]	Loss: 0.0842
Training Epoch: 77 [37120/50048]	Loss: 0.0901
Training Epoch: 77 [37248/50048]	Loss: 0.1840
Training Epoch: 77 [37376/50048]	Loss: 0.0732
Training Epoch: 77 [37504/50048]	Loss: 0.1694
Training Epoch: 77 [37632/50048]	Loss: 0.0921
Training Epoch: 77 [37760/50048]	Loss: 0.1387
Training Epoch: 77 [37888/50048]	Loss: 0.1439
Training Epoch: 77 [38016/50048]	Loss: 0.1875
Training Epoch: 77 [38144/50048]	Loss: 0.1585
Training Epoch: 77 [38272/50048]	Loss: 0.1410
Training Epoch: 77 [38400/50048]	Loss: 0.1528
Training Epoch: 77 [38528/50048]	Loss: 0.0697
Training Epoch: 77 [38656/50048]	Loss: 0.1047
Training Epoch: 77 [38784/50048]	Loss: 0.1309
Training Epoch: 77 [38912/50048]	Loss: 0.1962
Training Epoch: 77 [39040/50048]	Loss: 0.1144
Training Epoch: 77 [39168/50048]	Loss: 0.1206
Training Epoch: 77 [39296/50048]	Loss: 0.2449
Training Epoch: 77 [39424/50048]	Loss: 0.1344
Training Epoch: 77 [39552/50048]	Loss: 0.2133
Training Epoch: 77 [39680/50048]	Loss: 0.1459
Training Epoch: 77 [39808/50048]	Loss: 0.1486
Training Epoch: 77 [39936/50048]	Loss: 0.1638
Training Epoch: 77 [40064/50048]	Loss: 0.1286
Training Epoch: 77 [40192/50048]	Loss: 0.1874
Training Epoch: 77 [40320/50048]	Loss: 0.1444
Training Epoch: 77 [40448/50048]	Loss: 0.1064
Training Epoch: 77 [40576/50048]	Loss: 0.1233
Training Epoch: 77 [40704/50048]	Loss: 0.1117
Training Epoch: 77 [40832/50048]	Loss: 0.0646
Training Epoch: 77 [40960/50048]	Loss: 0.1517
Training Epoch: 77 [41088/50048]	Loss: 0.1138
Training Epoch: 77 [41216/50048]	Loss: 0.1631
Training Epoch: 77 [41344/50048]	Loss: 0.1814
Training Epoch: 77 [41472/50048]	Loss: 0.2125
Training Epoch: 77 [41600/50048]	Loss: 0.1472
Training Epoch: 77 [41728/50048]	Loss: 0.0782
Training Epoch: 77 [41856/50048]	Loss: 0.1522
Training Epoch: 77 [41984/50048]	Loss: 0.0818
Training Epoch: 77 [42112/50048]	Loss: 0.1573
Training Epoch: 77 [42240/50048]	Loss: 0.0796
Training Epoch: 77 [42368/50048]	Loss: 0.1060
Training Epoch: 77 [42496/50048]	Loss: 0.0827
Training Epoch: 77 [42624/50048]	Loss: 0.1365
Training Epoch: 77 [42752/50048]	Loss: 0.0701
Training Epoch: 77 [42880/50048]	Loss: 0.1114
Training Epoch: 77 [43008/50048]	Loss: 0.1555
Training Epoch: 77 [43136/50048]	Loss: 0.0942
Training Epoch: 77 [43264/50048]	Loss: 0.1410
Training Epoch: 77 [43392/50048]	Loss: 0.1856
Training Epoch: 77 [43520/50048]	Loss: 0.0931
Training Epoch: 77 [43648/50048]	Loss: 0.1789
Training Epoch: 77 [43776/50048]	Loss: 0.0978
Training Epoch: 77 [43904/50048]	Loss: 0.2029
Training Epoch: 77 [44032/50048]	Loss: 0.1215
Training Epoch: 77 [44160/50048]	Loss: 0.1331
Training Epoch: 77 [44288/50048]	Loss: 0.1087
Training Epoch: 77 [44416/50048]	Loss: 0.1627
Training Epoch: 77 [44544/50048]	Loss: 0.2313
Training Epoch: 77 [44672/50048]	Loss: 0.0482
Training Epoch: 77 [44800/50048]	Loss: 0.1354
Training Epoch: 77 [44928/50048]	Loss: 0.1230
Training Epoch: 77 [45056/50048]	Loss: 0.1759
Training Epoch: 77 [45184/50048]	Loss: 0.0891
Training Epoch: 77 [45312/50048]	Loss: 0.0862
Training Epoch: 77 [45440/50048]	Loss: 0.1399
Training Epoch: 77 [45568/50048]	Loss: 0.1149
Training Epoch: 77 [45696/50048]	Loss: 0.1439
2022-12-06 08:05:07,383 [ZeusDataLoader(train)] train epoch 78 done: time=86.44 energy=10487.93
2022-12-06 08:05:07,384 [ZeusDataLoader(eval)] Epoch 78 begin.
Training Epoch: 77 [45824/50048]	Loss: 0.1412
Training Epoch: 77 [45952/50048]	Loss: 0.1099
Training Epoch: 77 [46080/50048]	Loss: 0.1959
Training Epoch: 77 [46208/50048]	Loss: 0.1116
Training Epoch: 77 [46336/50048]	Loss: 0.1334
Training Epoch: 77 [46464/50048]	Loss: 0.1900
Training Epoch: 77 [46592/50048]	Loss: 0.1877
Training Epoch: 77 [46720/50048]	Loss: 0.1442
Training Epoch: 77 [46848/50048]	Loss: 0.1042
Training Epoch: 77 [46976/50048]	Loss: 0.1212
Training Epoch: 77 [47104/50048]	Loss: 0.1685
Training Epoch: 77 [47232/50048]	Loss: 0.1546
Training Epoch: 77 [47360/50048]	Loss: 0.0999
Training Epoch: 77 [47488/50048]	Loss: 0.0905
Training Epoch: 77 [47616/50048]	Loss: 0.0927
Training Epoch: 77 [47744/50048]	Loss: 0.1372
Training Epoch: 77 [47872/50048]	Loss: 0.1113
Training Epoch: 77 [48000/50048]	Loss: 0.1955
Training Epoch: 77 [48128/50048]	Loss: 0.1302
Training Epoch: 77 [48256/50048]	Loss: 0.1480
Training Epoch: 77 [48384/50048]	Loss: 0.0234
Training Epoch: 77 [48512/50048]	Loss: 0.1123
Training Epoch: 77 [48640/50048]	Loss: 0.2974
Training Epoch: 77 [48768/50048]	Loss: 0.1911
Training Epoch: 77 [48896/50048]	Loss: 0.1579
Training Epoch: 77 [49024/50048]	Loss: 0.0703
Training Epoch: 77 [49152/50048]	Loss: 0.1425
Training Epoch: 77 [49280/50048]	Loss: 0.0730
Training Epoch: 77 [49408/50048]	Loss: 0.1116
Training Epoch: 77 [49536/50048]	Loss: 0.1166
Training Epoch: 77 [49664/50048]	Loss: 0.1049
Training Epoch: 77 [49792/50048]	Loss: 0.0692
Training Epoch: 77 [49920/50048]	Loss: 0.0655
Training Epoch: 77 [50048/50048]	Loss: 0.1397
2022-12-06 13:05:11.065 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 08:05:11,099 [ZeusDataLoader(eval)] eval epoch 78 done: time=3.71 energy=455.47
2022-12-06 08:05:11,100 [ZeusDataLoader(train)] Up to epoch 78: time=7035.72, energy=854051.59, cost=1042651.40
2022-12-06 08:05:11,100 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 08:05:11,100 [ZeusDataLoader(train)] Expected next epoch: time=7125.52, energy=864849.60, cost=1055907.78
2022-12-06 08:05:11,101 [ZeusDataLoader(train)] Epoch 79 begin.
Validation Epoch: 77, Average loss: 0.0175, Accuracy: 0.6406
2022-12-06 08:05:11,296 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:05:11,297 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:05:11.299 [ZeusMonitor] Monitor started.
2022-12-06 13:05:11.299 [ZeusMonitor] Running indefinitely. 2022-12-06 13:05:11.307 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:05:11.307 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e79+gpu0.power.log
Training Epoch: 78 [128/50048]	Loss: 0.0977
Training Epoch: 78 [256/50048]	Loss: 0.0884
Training Epoch: 78 [384/50048]	Loss: 0.0709
Training Epoch: 78 [512/50048]	Loss: 0.1157
Training Epoch: 78 [640/50048]	Loss: 0.1178
Training Epoch: 78 [768/50048]	Loss: 0.1357
Training Epoch: 78 [896/50048]	Loss: 0.1161
Training Epoch: 78 [1024/50048]	Loss: 0.0535
Training Epoch: 78 [1152/50048]	Loss: 0.0679
Training Epoch: 78 [1280/50048]	Loss: 0.0606
Training Epoch: 78 [1408/50048]	Loss: 0.1418
Training Epoch: 78 [1536/50048]	Loss: 0.0438
Training Epoch: 78 [1664/50048]	Loss: 0.0439
Training Epoch: 78 [1792/50048]	Loss: 0.0530
Training Epoch: 78 [1920/50048]	Loss: 0.0416
Training Epoch: 78 [2048/50048]	Loss: 0.0845
Training Epoch: 78 [2176/50048]	Loss: 0.0690
Training Epoch: 78 [2304/50048]	Loss: 0.0494
Training Epoch: 78 [2432/50048]	Loss: 0.1319
Training Epoch: 78 [2560/50048]	Loss: 0.0923
Training Epoch: 78 [2688/50048]	Loss: 0.1898
Training Epoch: 78 [2816/50048]	Loss: 0.1500
Training Epoch: 78 [2944/50048]	Loss: 0.0686
Training Epoch: 78 [3072/50048]	Loss: 0.0697
Training Epoch: 78 [3200/50048]	Loss: 0.1103
Training Epoch: 78 [3328/50048]	Loss: 0.0932
Training Epoch: 78 [3456/50048]	Loss: 0.1628
Training Epoch: 78 [3584/50048]	Loss: 0.1319
Training Epoch: 78 [3712/50048]	Loss: 0.1561
Training Epoch: 78 [3840/50048]	Loss: 0.1298
Training Epoch: 78 [3968/50048]	Loss: 0.0828
Training Epoch: 78 [4096/50048]	Loss: 0.0810
Training Epoch: 78 [4224/50048]	Loss: 0.0969
Training Epoch: 78 [4352/50048]	Loss: 0.0606
Training Epoch: 78 [4480/50048]	Loss: 0.1036
Training Epoch: 78 [4608/50048]	Loss: 0.1253
Training Epoch: 78 [4736/50048]	Loss: 0.0429
Training Epoch: 78 [4864/50048]	Loss: 0.0529
Training Epoch: 78 [4992/50048]	Loss: 0.0252
Training Epoch: 78 [5120/50048]	Loss: 0.0647
Training Epoch: 78 [5248/50048]	Loss: 0.0882
Training Epoch: 78 [5376/50048]	Loss: 0.0622
Training Epoch: 78 [5504/50048]	Loss: 0.1046
Training Epoch: 78 [5632/50048]	Loss: 0.0651
Training Epoch: 78 [5760/50048]	Loss: 0.0459
Training Epoch: 78 [5888/50048]	Loss: 0.1209
Training Epoch: 78 [6016/50048]	Loss: 0.1203
Training Epoch: 78 [6144/50048]	Loss: 0.1108
Training Epoch: 78 [6272/50048]	Loss: 0.1598
Training Epoch: 78 [6400/50048]	Loss: 0.1554
Training Epoch: 78 [6528/50048]	Loss: 0.1191
Training Epoch: 78 [6656/50048]	Loss: 0.0735
Training Epoch: 78 [6784/50048]	Loss: 0.0668
Training Epoch: 78 [6912/50048]	Loss: 0.1373
Training Epoch: 78 [7040/50048]	Loss: 0.1025
Training Epoch: 78 [7168/50048]	Loss: 0.0759
Training Epoch: 78 [7296/50048]	Loss: 0.0827
Training Epoch: 78 [7424/50048]	Loss: 0.1475
Training Epoch: 78 [7552/50048]	Loss: 0.0602
Training Epoch: 78 [7680/50048]	Loss: 0.1195
Training Epoch: 78 [7808/50048]	Loss: 0.0759
Training Epoch: 78 [7936/50048]	Loss: 0.0530
Training Epoch: 78 [8064/50048]	Loss: 0.1723
Training Epoch: 78 [8192/50048]	Loss: 0.1043
Training Epoch: 78 [8320/50048]	Loss: 0.1227
Training Epoch: 78 [8448/50048]	Loss: 0.0321
Training Epoch: 78 [8576/50048]	Loss: 0.1174
Training Epoch: 78 [8704/50048]	Loss: 0.0807
Training Epoch: 78 [8832/50048]	Loss: 0.1215
Training Epoch: 78 [8960/50048]	Loss: 0.0578
Training Epoch: 78 [9088/50048]	Loss: 0.1205
Training Epoch: 78 [9216/50048]	Loss: 0.1000
Training Epoch: 78 [9344/50048]	Loss: 0.0873
Training Epoch: 78 [9472/50048]	Loss: 0.0668
Training Epoch: 78 [9600/50048]	Loss: 0.1038
Training Epoch: 78 [9728/50048]	Loss: 0.0456
Training Epoch: 78 [9856/50048]	Loss: 0.0595
Training Epoch: 78 [9984/50048]	Loss: 0.0783
Training Epoch: 78 [10112/50048]	Loss: 0.0837
Training Epoch: 78 [10240/50048]	Loss: 0.0764
Training Epoch: 78 [10368/50048]	Loss: 0.0673
Training Epoch: 78 [10496/50048]	Loss: 0.0840
Training Epoch: 78 [10624/50048]	Loss: 0.1477
Training Epoch: 78 [10752/50048]	Loss: 0.1285
Training Epoch: 78 [10880/50048]	Loss: 0.1025
Training Epoch: 78 [11008/50048]	Loss: 0.1402
Training Epoch: 78 [11136/50048]	Loss: 0.0549
Training Epoch: 78 [11264/50048]	Loss: 0.0802
Training Epoch: 78 [11392/50048]	Loss: 0.1170
Training Epoch: 78 [11520/50048]	Loss: 0.1053
Training Epoch: 78 [11648/50048]	Loss: 0.1276
Training Epoch: 78 [11776/50048]	Loss: 0.1063
Training Epoch: 78 [11904/50048]	Loss: 0.0737
Training Epoch: 78 [12032/50048]	Loss: 0.0586
Training Epoch: 78 [12160/50048]	Loss: 0.1991
Training Epoch: 78 [12288/50048]	Loss: 0.0998
Training Epoch: 78 [12416/50048]	Loss: 0.1296
Training Epoch: 78 [12544/50048]	Loss: 0.0830
Training Epoch: 78 [12672/50048]	Loss: 0.0874
Training Epoch: 78 [12800/50048]	Loss: 0.0842
Training Epoch: 78 [12928/50048]	Loss: 0.1500
Training Epoch: 78 [13056/50048]	Loss: 0.0842
Training Epoch: 78 [13184/50048]	Loss: 0.1088
Training Epoch: 78 [13312/50048]	Loss: 0.0920
Training Epoch: 78 [13440/50048]	Loss: 0.1318
Training Epoch: 78 [13568/50048]	Loss: 0.0510
Training Epoch: 78 [13696/50048]	Loss: 0.0764
Training Epoch: 78 [13824/50048]	Loss: 0.0873
Training Epoch: 78 [13952/50048]	Loss: 0.0905
Training Epoch: 78 [14080/50048]	Loss: 0.0464
Training Epoch: 78 [14208/50048]	Loss: 0.1538
Training Epoch: 78 [14336/50048]	Loss: 0.0861
Training Epoch: 78 [14464/50048]	Loss: 0.1443
Training Epoch: 78 [14592/50048]	Loss: 0.1086
Training Epoch: 78 [14720/50048]	Loss: 0.0763
Training Epoch: 78 [14848/50048]	Loss: 0.2491
Training Epoch: 78 [14976/50048]	Loss: 0.0959
Training Epoch: 78 [15104/50048]	Loss: 0.1006
Training Epoch: 78 [15232/50048]	Loss: 0.0741
Training Epoch: 78 [15360/50048]	Loss: 0.0630
Training Epoch: 78 [15488/50048]	Loss: 0.1386
Training Epoch: 78 [15616/50048]	Loss: 0.0827
Training Epoch: 78 [15744/50048]	Loss: 0.2606
Training Epoch: 78 [15872/50048]	Loss: 0.0732
Training Epoch: 78 [16000/50048]	Loss: 0.1516
Training Epoch: 78 [16128/50048]	Loss: 0.0479
Training Epoch: 78 [16256/50048]	Loss: 0.1176
Training Epoch: 78 [16384/50048]	Loss: 0.0703
Training Epoch: 78 [16512/50048]	Loss: 0.0752
Training Epoch: 78 [16640/50048]	Loss: 0.1345
Training Epoch: 78 [16768/50048]	Loss: 0.1169
Training Epoch: 78 [16896/50048]	Loss: 0.0629
Training Epoch: 78 [17024/50048]	Loss: 0.1794
Training Epoch: 78 [17152/50048]	Loss: 0.1398
Training Epoch: 78 [17280/50048]	Loss: 0.1282
Training Epoch: 78 [17408/50048]	Loss: 0.0862
Training Epoch: 78 [17536/50048]	Loss: 0.0797
Training Epoch: 78 [17664/50048]	Loss: 0.0925
Training Epoch: 78 [17792/50048]	Loss: 0.1164
Training Epoch: 78 [17920/50048]	Loss: 0.0850
Training Epoch: 78 [18048/50048]	Loss: 0.1012
Training Epoch: 78 [18176/50048]	Loss: 0.1107
Training Epoch: 78 [18304/50048]	Loss: 0.1207
Training Epoch: 78 [18432/50048]	Loss: 0.0828
Training Epoch: 78 [18560/50048]	Loss: 0.0931
Training Epoch: 78 [18688/50048]	Loss: 0.0828
Training Epoch: 78 [18816/50048]	Loss: 0.0930
Training Epoch: 78 [18944/50048]	Loss: 0.1136
Training Epoch: 78 [19072/50048]	Loss: 0.1058
Training Epoch: 78 [19200/50048]	Loss: 0.1699
Training Epoch: 78 [19328/50048]	Loss: 0.1547
Training Epoch: 78 [19456/50048]	Loss: 0.0867
Training Epoch: 78 [19584/50048]	Loss: 0.0535
Training Epoch: 78 [19712/50048]	Loss: 0.1447
Training Epoch: 78 [19840/50048]	Loss: 0.0753
Training Epoch: 78 [19968/50048]	Loss: 0.1016
Training Epoch: 78 [20096/50048]	Loss: 0.0491
Training Epoch: 78 [20224/50048]	Loss: 0.0533
Training Epoch: 78 [20352/50048]	Loss: 0.1573
Training Epoch: 78 [20480/50048]	Loss: 0.1341
Training Epoch: 78 [20608/50048]	Loss: 0.1479
Training Epoch: 78 [20736/50048]	Loss: 0.1669
Training Epoch: 78 [20864/50048]	Loss: 0.1182
Training Epoch: 78 [20992/50048]	Loss: 0.0791
Training Epoch: 78 [21120/50048]	Loss: 0.1302
Training Epoch: 78 [21248/50048]	Loss: 0.1178
Training Epoch: 78 [21376/50048]	Loss: 0.1152
Training Epoch: 78 [21504/50048]	Loss: 0.0598
Training Epoch: 78 [21632/50048]	Loss: 0.1713
Training Epoch: 78 [21760/50048]	Loss: 0.1288
Training Epoch: 78 [21888/50048]	Loss: 0.1346
Training Epoch: 78 [22016/50048]	Loss: 0.1792
Training Epoch: 78 [22144/50048]	Loss: 0.1253
Training Epoch: 78 [22272/50048]	Loss: 0.0914
Training Epoch: 78 [22400/50048]	Loss: 0.1731
Training Epoch: 78 [22528/50048]	Loss: 0.1393
Training Epoch: 78 [22656/50048]	Loss: 0.0940
Training Epoch: 78 [22784/50048]	Loss: 0.0558
Training Epoch: 78 [22912/50048]	Loss: 0.2147
Training Epoch: 78 [23040/50048]	Loss: 0.0857
Training Epoch: 78 [23168/50048]	Loss: 0.1896
Training Epoch: 78 [23296/50048]	Loss: 0.1602
Training Epoch: 78 [23424/50048]	Loss: 0.1242
Training Epoch: 78 [23552/50048]	Loss: 0.1107
Training Epoch: 78 [23680/50048]	Loss: 0.1235
Training Epoch: 78 [23808/50048]	Loss: 0.1723
Training Epoch: 78 [23936/50048]	Loss: 0.1156
Training Epoch: 78 [24064/50048]	Loss: 0.1401
Training Epoch: 78 [24192/50048]	Loss: 0.1255
Training Epoch: 78 [24320/50048]	Loss: 0.1733
Training Epoch: 78 [24448/50048]	Loss: 0.1180
Training Epoch: 78 [24576/50048]	Loss: 0.0799
Training Epoch: 78 [24704/50048]	Loss: 0.1088
Training Epoch: 78 [24832/50048]	Loss: 0.1457
Training Epoch: 78 [24960/50048]	Loss: 0.1575
Training Epoch: 78 [25088/50048]	Loss: 0.0914
Training Epoch: 78 [25216/50048]	Loss: 0.0976
Training Epoch: 78 [25344/50048]	Loss: 0.1370
Training Epoch: 78 [25472/50048]	Loss: 0.1917
Training Epoch: 78 [25600/50048]	Loss: 0.1971
Training Epoch: 78 [25728/50048]	Loss: 0.0762
Training Epoch: 78 [25856/50048]	Loss: 0.0935
Training Epoch: 78 [25984/50048]	Loss: 0.0905
Training Epoch: 78 [26112/50048]	Loss: 0.1609
Training Epoch: 78 [26240/50048]	Loss: 0.0713
Training Epoch: 78 [26368/50048]	Loss: 0.0918
Training Epoch: 78 [26496/50048]	Loss: 0.1123
Training Epoch: 78 [26624/50048]	Loss: 0.1059
Training Epoch: 78 [26752/50048]	Loss: 0.1489
Training Epoch: 78 [26880/50048]	Loss: 0.0862
Training Epoch: 78 [27008/50048]	Loss: 0.0903
Training Epoch: 78 [27136/50048]	Loss: 0.1364
Training Epoch: 78 [27264/50048]	Loss: 0.1183
Training Epoch: 78 [27392/50048]	Loss: 0.1449
Training Epoch: 78 [27520/50048]	Loss: 0.1176
Training Epoch: 78 [27648/50048]	Loss: 0.1203
Training Epoch: 78 [27776/50048]	Loss: 0.1420
Training Epoch: 78 [27904/50048]	Loss: 0.1087
Training Epoch: 78 [28032/50048]	Loss: 0.1064
Training Epoch: 78 [28160/50048]	Loss: 0.0888
Training Epoch: 78 [28288/50048]	Loss: 0.1044
Training Epoch: 78 [28416/50048]	Loss: 0.1107
Training Epoch: 78 [28544/50048]	Loss: 0.1272
Training Epoch: 78 [28672/50048]	Loss: 0.0920
Training Epoch: 78 [28800/50048]	Loss: 0.2330
Training Epoch: 78 [28928/50048]	Loss: 0.0899
Training Epoch: 78 [29056/50048]	Loss: 0.1300
Training Epoch: 78 [29184/50048]	Loss: 0.0812
Training Epoch: 78 [29312/50048]	Loss: 0.1283
Training Epoch: 78 [29440/50048]	Loss: 0.1088
Training Epoch: 78 [29568/50048]	Loss: 0.2234
Training Epoch: 78 [29696/50048]	Loss: 0.1069
Training Epoch: 78 [29824/50048]	Loss: 0.0777
Training Epoch: 78 [29952/50048]	Loss: 0.1007
Training Epoch: 78 [30080/50048]	Loss: 0.0818
Training Epoch: 78 [30208/50048]	Loss: 0.1789
Training Epoch: 78 [30336/50048]	Loss: 0.1168
Training Epoch: 78 [30464/50048]	Loss: 0.0512
Training Epoch: 78 [30592/50048]	Loss: 0.0323
Training Epoch: 78 [30720/50048]	Loss: 0.1473
Training Epoch: 78 [30848/50048]	Loss: 0.1381
Training Epoch: 78 [30976/50048]	Loss: 0.1213
Training Epoch: 78 [31104/50048]	Loss: 0.1069
Training Epoch: 78 [31232/50048]	Loss: 0.2003
Training Epoch: 78 [31360/50048]	Loss: 0.1453
Training Epoch: 78 [31488/50048]	Loss: 0.1078
Training Epoch: 78 [31616/50048]	Loss: 0.1161
Training Epoch: 78 [31744/50048]	Loss: 0.1119
Training Epoch: 78 [31872/50048]	Loss: 0.0854
Training Epoch: 78 [32000/50048]	Loss: 0.0998
Training Epoch: 78 [32128/50048]	Loss: 0.0562
Training Epoch: 78 [32256/50048]	Loss: 0.0522
Training Epoch: 78 [32384/50048]	Loss: 0.0550
Training Epoch: 78 [32512/50048]	Loss: 0.2340
Training Epoch: 78 [32640/50048]	Loss: 0.1108
Training Epoch: 78 [32768/50048]	Loss: 0.1120
Training Epoch: 78 [32896/50048]	Loss: 0.0966
Training Epoch: 78 [33024/50048]	Loss: 0.1292
Training Epoch: 78 [33152/50048]	Loss: 0.1512
Training Epoch: 78 [33280/50048]	Loss: 0.0469
Training Epoch: 78 [33408/50048]	Loss: 0.0982
Training Epoch: 78 [33536/50048]	Loss: 0.1783
Training Epoch: 78 [33664/50048]	Loss: 0.0827
Training Epoch: 78 [33792/50048]	Loss: 0.2040
Training Epoch: 78 [33920/50048]	Loss: 0.1969
Training Epoch: 78 [34048/50048]	Loss: 0.0841
Training Epoch: 78 [34176/50048]	Loss: 0.0798
Training Epoch: 78 [34304/50048]	Loss: 0.0847
Training Epoch: 78 [34432/50048]	Loss: 0.1244
Training Epoch: 78 [34560/50048]	Loss: 0.1225
Training Epoch: 78 [34688/50048]	Loss: 0.1119
Training Epoch: 78 [34816/50048]	Loss: 0.0761
Training Epoch: 78 [34944/50048]	Loss: 0.1024
Training Epoch: 78 [35072/50048]	Loss: 0.0722
Training Epoch: 78 [35200/50048]	Loss: 0.1516
Training Epoch: 78 [35328/50048]	Loss: 0.1067
Training Epoch: 78 [35456/50048]	Loss: 0.1488
Training Epoch: 78 [35584/50048]	Loss: 0.0711
Training Epoch: 78 [35712/50048]	Loss: 0.1276
Training Epoch: 78 [35840/50048]	Loss: 0.0811
Training Epoch: 78 [35968/50048]	Loss: 0.1181
Training Epoch: 78 [36096/50048]	Loss: 0.0687
Training Epoch: 78 [36224/50048]	Loss: 0.1694
Training Epoch: 78 [36352/50048]	Loss: 0.1264
Training Epoch: 78 [36480/50048]	Loss: 0.0784
Training Epoch: 78 [36608/50048]	Loss: 0.1187
Training Epoch: 78 [36736/50048]	Loss: 0.1025
Training Epoch: 78 [36864/50048]	Loss: 0.0920
Training Epoch: 78 [36992/50048]	Loss: 0.1019
Training Epoch: 78 [37120/50048]	Loss: 0.1424
Training Epoch: 78 [37248/50048]	Loss: 0.0693
Training Epoch: 78 [37376/50048]	Loss: 0.1259
Training Epoch: 78 [37504/50048]	Loss: 0.1504
Training Epoch: 78 [37632/50048]	Loss: 0.1154
Training Epoch: 78 [37760/50048]	Loss: 0.1310
Training Epoch: 78 [37888/50048]	Loss: 0.1081
Training Epoch: 78 [38016/50048]	Loss: 0.1021
Training Epoch: 78 [38144/50048]	Loss: 0.1612
Training Epoch: 78 [38272/50048]	Loss: 0.1237
Training Epoch: 78 [38400/50048]	Loss: 0.0369
Training Epoch: 78 [38528/50048]	Loss: 0.1896
Training Epoch: 78 [38656/50048]	Loss: 0.1246
Training Epoch: 78 [38784/50048]	Loss: 0.1884
Training Epoch: 78 [38912/50048]	Loss: 0.0798
Training Epoch: 78 [39040/50048]	Loss: 0.0738
Training Epoch: 78 [39168/50048]	Loss: 0.1140
Training Epoch: 78 [39296/50048]	Loss: 0.1498
Training Epoch: 78 [39424/50048]	Loss: 0.0893
Training Epoch: 78 [39552/50048]	Loss: 0.0647
Training Epoch: 78 [39680/50048]	Loss: 0.1399
Training Epoch: 78 [39808/50048]	Loss: 0.1591
Training Epoch: 78 [39936/50048]	Loss: 0.2015
Training Epoch: 78 [40064/50048]	Loss: 0.0579
Training Epoch: 78 [40192/50048]	Loss: 0.1179
Training Epoch: 78 [40320/50048]	Loss: 0.1192
Training Epoch: 78 [40448/50048]	Loss: 0.1335
Training Epoch: 78 [40576/50048]	Loss: 0.1952
Training Epoch: 78 [40704/50048]	Loss: 0.1192
Training Epoch: 78 [40832/50048]	Loss: 0.0884
Training Epoch: 78 [40960/50048]	Loss: 0.0835
Training Epoch: 78 [41088/50048]	Loss: 0.0914
Training Epoch: 78 [41216/50048]	Loss: 0.1579
Training Epoch: 78 [41344/50048]	Loss: 0.1663
Training Epoch: 78 [41472/50048]	Loss: 0.1282
Training Epoch: 78 [41600/50048]	Loss: 0.0908
Training Epoch: 78 [41728/50048]	Loss: 0.2182
Training Epoch: 78 [41856/50048]	Loss: 0.1079
Training Epoch: 78 [41984/50048]	Loss: 0.1224
Training Epoch: 78 [42112/50048]	Loss: 0.1252
Training Epoch: 78 [42240/50048]	Loss: 0.2630
Training Epoch: 78 [42368/50048]	Loss: 0.1574
Training Epoch: 78 [42496/50048]	Loss: 0.0608
Training Epoch: 78 [42624/50048]	Loss: 0.2262
Training Epoch: 78 [42752/50048]	Loss: 0.1077
Training Epoch: 78 [42880/50048]	Loss: 0.1120
Training Epoch: 78 [43008/50048]	Loss: 0.1126
Training Epoch: 78 [43136/50048]	Loss: 0.0700
Training Epoch: 78 [43264/50048]	Loss: 0.1215
Training Epoch: 78 [43392/50048]	Loss: 0.1541
Training Epoch: 78 [43520/50048]	Loss: 0.0845
Training Epoch: 78 [43648/50048]	Loss: 0.0772
Training Epoch: 78 [43776/50048]	Loss: 0.1128
Training Epoch: 78 [43904/50048]	Loss: 0.1719
Training Epoch: 78 [44032/50048]	Loss: 0.0771
Training Epoch: 78 [44160/50048]	Loss: 0.0979
Training Epoch: 78 [44288/50048]	Loss: 0.1727
Training Epoch: 78 [44416/50048]	Loss: 0.2286
Training Epoch: 78 [44544/50048]	Loss: 0.1646
Training Epoch: 78 [44672/50048]	Loss: 0.1130
Training Epoch: 78 [44800/50048]	Loss: 0.0755
Training Epoch: 78 [44928/50048]	Loss: 0.1575
Training Epoch: 78 [45056/50048]	Loss: 0.1274
Training Epoch: 78 [45184/50048]	Loss: 0.1459
Training Epoch: 78 [45312/50048]	Loss: 0.1077
Training Epoch: 78 [45440/50048]	Loss: 0.0887
Training Epoch: 78 [45568/50048]	Loss: 0.2268
Training Epoch: 78 [45696/50048]	Loss: 0.1420
2022-12-06 08:06:37,642 [ZeusDataLoader(train)] train epoch 79 done: time=86.53 energy=10502.54
2022-12-06 08:06:37,643 [ZeusDataLoader(eval)] Epoch 79 begin.
Training Epoch: 78 [45824/50048]	Loss: 0.1621
Training Epoch: 78 [45952/50048]	Loss: 0.0792
Training Epoch: 78 [46080/50048]	Loss: 0.2133
Training Epoch: 78 [46208/50048]	Loss: 0.2743
Training Epoch: 78 [46336/50048]	Loss: 0.1317
Training Epoch: 78 [46464/50048]	Loss: 0.1603
Training Epoch: 78 [46592/50048]	Loss: 0.0785
Training Epoch: 78 [46720/50048]	Loss: 0.1825
Training Epoch: 78 [46848/50048]	Loss: 0.1573
Training Epoch: 78 [46976/50048]	Loss: 0.1308
Training Epoch: 78 [47104/50048]	Loss: 0.1565
Training Epoch: 78 [47232/50048]	Loss: 0.1601
Training Epoch: 78 [47360/50048]	Loss: 0.1887
Training Epoch: 78 [47488/50048]	Loss: 0.2619
Training Epoch: 78 [47616/50048]	Loss: 0.1012
Training Epoch: 78 [47744/50048]	Loss: 0.1641
Training Epoch: 78 [47872/50048]	Loss: 0.1176
Training Epoch: 78 [48000/50048]	Loss: 0.0849
Training Epoch: 78 [48128/50048]	Loss: 0.0880
Training Epoch: 78 [48256/50048]	Loss: 0.2477
Training Epoch: 78 [48384/50048]	Loss: 0.0705
Training Epoch: 78 [48512/50048]	Loss: 0.1277
Training Epoch: 78 [48640/50048]	Loss: 0.1154
Training Epoch: 78 [48768/50048]	Loss: 0.1320
Training Epoch: 78 [48896/50048]	Loss: 0.1628
Training Epoch: 78 [49024/50048]	Loss: 0.0342
Training Epoch: 78 [49152/50048]	Loss: 0.1595
Training Epoch: 78 [49280/50048]	Loss: 0.2382
Training Epoch: 78 [49408/50048]	Loss: 0.0565
Training Epoch: 78 [49536/50048]	Loss: 0.1176
Training Epoch: 78 [49664/50048]	Loss: 0.1254
Training Epoch: 78 [49792/50048]	Loss: 0.1590
Training Epoch: 78 [49920/50048]	Loss: 0.0984
Training Epoch: 78 [50048/50048]	Loss: 0.0810
2022-12-06 13:06:41.365 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 08:06:41,410 [ZeusDataLoader(eval)] eval epoch 79 done: time=3.76 energy=454.27
2022-12-06 08:06:41,410 [ZeusDataLoader(train)] Up to epoch 79: time=7126.01, energy=865008.40, cost=1056030.04
2022-12-06 08:06:41,410 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 08:06:41,410 [ZeusDataLoader(train)] Expected next epoch: time=7215.81, energy=875806.42, cost=1069286.42
2022-12-06 08:06:41,411 [ZeusDataLoader(train)] Epoch 80 begin.
Validation Epoch: 78, Average loss: 0.0178, Accuracy: 0.6394
2022-12-06 08:06:41,561 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:06:41,561 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:06:41.563 [ZeusMonitor] Monitor started.
2022-12-06 13:06:41.563 [ZeusMonitor] Running indefinitely. 2022-12-06 13:06:41.563 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:06:41.563 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e80+gpu0.power.log
Training Epoch: 79 [128/50048]	Loss: 0.0639
Training Epoch: 79 [256/50048]	Loss: 0.0520
Training Epoch: 79 [384/50048]	Loss: 0.0972
Training Epoch: 79 [512/50048]	Loss: 0.0693
Training Epoch: 79 [640/50048]	Loss: 0.0999
Training Epoch: 79 [768/50048]	Loss: 0.1331
Training Epoch: 79 [896/50048]	Loss: 0.0601
Training Epoch: 79 [1024/50048]	Loss: 0.1202
Training Epoch: 79 [1152/50048]	Loss: 0.1183
Training Epoch: 79 [1280/50048]	Loss: 0.0912
Training Epoch: 79 [1408/50048]	Loss: 0.0604
Training Epoch: 79 [1536/50048]	Loss: 0.0613
Training Epoch: 79 [1664/50048]	Loss: 0.0963
Training Epoch: 79 [1792/50048]	Loss: 0.1269
Training Epoch: 79 [1920/50048]	Loss: 0.0704
Training Epoch: 79 [2048/50048]	Loss: 0.1056
Training Epoch: 79 [2176/50048]	Loss: 0.0650
Training Epoch: 79 [2304/50048]	Loss: 0.1399
Training Epoch: 79 [2432/50048]	Loss: 0.0645
Training Epoch: 79 [2560/50048]	Loss: 0.0643
Training Epoch: 79 [2688/50048]	Loss: 0.1422
Training Epoch: 79 [2816/50048]	Loss: 0.1387
Training Epoch: 79 [2944/50048]	Loss: 0.0698
Training Epoch: 79 [3072/50048]	Loss: 0.1615
Training Epoch: 79 [3200/50048]	Loss: 0.1336
Training Epoch: 79 [3328/50048]	Loss: 0.0715
Training Epoch: 79 [3456/50048]	Loss: 0.0489
Training Epoch: 79 [3584/50048]	Loss: 0.0572
Training Epoch: 79 [3712/50048]	Loss: 0.0908
Training Epoch: 79 [3840/50048]	Loss: 0.0999
Training Epoch: 79 [3968/50048]	Loss: 0.1120
Training Epoch: 79 [4096/50048]	Loss: 0.0929
Training Epoch: 79 [4224/50048]	Loss: 0.0734
Training Epoch: 79 [4352/50048]	Loss: 0.1407
Training Epoch: 79 [4480/50048]	Loss: 0.1269
Training Epoch: 79 [4608/50048]	Loss: 0.0798
Training Epoch: 79 [4736/50048]	Loss: 0.1441
Training Epoch: 79 [4864/50048]	Loss: 0.0610
Training Epoch: 79 [4992/50048]	Loss: 0.0617
Training Epoch: 79 [5120/50048]	Loss: 0.1397
Training Epoch: 79 [5248/50048]	Loss: 0.1591
Training Epoch: 79 [5376/50048]	Loss: 0.1024
Training Epoch: 79 [5504/50048]	Loss: 0.1651
Training Epoch: 79 [5632/50048]	Loss: 0.1416
Training Epoch: 79 [5760/50048]	Loss: 0.0805
Training Epoch: 79 [5888/50048]	Loss: 0.0499
Training Epoch: 79 [6016/50048]	Loss: 0.1687
Training Epoch: 79 [6144/50048]	Loss: 0.0760
Training Epoch: 79 [6272/50048]	Loss: 0.0837
Training Epoch: 79 [6400/50048]	Loss: 0.0813
Training Epoch: 79 [6528/50048]	Loss: 0.1021
Training Epoch: 79 [6656/50048]	Loss: 0.0775
Training Epoch: 79 [6784/50048]	Loss: 0.1029
Training Epoch: 79 [6912/50048]	Loss: 0.0500
Training Epoch: 79 [7040/50048]	Loss: 0.0661
Training Epoch: 79 [7168/50048]	Loss: 0.0442
Training Epoch: 79 [7296/50048]	Loss: 0.1661
Training Epoch: 79 [7424/50048]	Loss: 0.1446
Training Epoch: 79 [7552/50048]	Loss: 0.1500
Training Epoch: 79 [7680/50048]	Loss: 0.0757
Training Epoch: 79 [7808/50048]	Loss: 0.0926
Training Epoch: 79 [7936/50048]	Loss: 0.0878
Training Epoch: 79 [8064/50048]	Loss: 0.0929
Training Epoch: 79 [8192/50048]	Loss: 0.0939
Training Epoch: 79 [8320/50048]	Loss: 0.0615
Training Epoch: 79 [8448/50048]	Loss: 0.1250
Training Epoch: 79 [8576/50048]	Loss: 0.1847
Training Epoch: 79 [8704/50048]	Loss: 0.1145
Training Epoch: 79 [8832/50048]	Loss: 0.0205
Training Epoch: 79 [8960/50048]	Loss: 0.1701
Training Epoch: 79 [9088/50048]	Loss: 0.0805
Training Epoch: 79 [9216/50048]	Loss: 0.1125
Training Epoch: 79 [9344/50048]	Loss: 0.1645
Training Epoch: 79 [9472/50048]	Loss: 0.0940
Training Epoch: 79 [9600/50048]	Loss: 0.1299
Training Epoch: 79 [9728/50048]	Loss: 0.1510
Training Epoch: 79 [9856/50048]	Loss: 0.0912
Training Epoch: 79 [9984/50048]	Loss: 0.0503
Training Epoch: 79 [10112/50048]	Loss: 0.0639
Training Epoch: 79 [10240/50048]	Loss: 0.1110
Training Epoch: 79 [10368/50048]	Loss: 0.1092
Training Epoch: 79 [10496/50048]	Loss: 0.0775
Training Epoch: 79 [10624/50048]	Loss: 0.0649
Training Epoch: 79 [10752/50048]	Loss: 0.0689
Training Epoch: 79 [10880/50048]	Loss: 0.0603
Training Epoch: 79 [11008/50048]	Loss: 0.1390
Training Epoch: 79 [11136/50048]	Loss: 0.0558
Training Epoch: 79 [11264/50048]	Loss: 0.0715
Training Epoch: 79 [11392/50048]	Loss: 0.0663
Training Epoch: 79 [11520/50048]	Loss: 0.1086
Training Epoch: 79 [11648/50048]	Loss: 0.0892
Training Epoch: 79 [11776/50048]	Loss: 0.0784
Training Epoch: 79 [11904/50048]	Loss: 0.0452
Training Epoch: 79 [12032/50048]	Loss: 0.1206
Training Epoch: 79 [12160/50048]	Loss: 0.0762
Training Epoch: 79 [12288/50048]	Loss: 0.1331
Training Epoch: 79 [12416/50048]	Loss: 0.0444
Training Epoch: 79 [12544/50048]	Loss: 0.1182
Training Epoch: 79 [12672/50048]	Loss: 0.1101
Training Epoch: 79 [12800/50048]	Loss: 0.0739
Training Epoch: 79 [12928/50048]	Loss: 0.0923
Training Epoch: 79 [13056/50048]	Loss: 0.1197
Training Epoch: 79 [13184/50048]	Loss: 0.0949
Training Epoch: 79 [13312/50048]	Loss: 0.0766
Training Epoch: 79 [13440/50048]	Loss: 0.1039
Training Epoch: 79 [13568/50048]	Loss: 0.1070
Training Epoch: 79 [13696/50048]	Loss: 0.0628
Training Epoch: 79 [13824/50048]	Loss: 0.0571
Training Epoch: 79 [13952/50048]	Loss: 0.0766
Training Epoch: 79 [14080/50048]	Loss: 0.1418
Training Epoch: 79 [14208/50048]	Loss: 0.0755
Training Epoch: 79 [14336/50048]	Loss: 0.0810
Training Epoch: 79 [14464/50048]	Loss: 0.1225
Training Epoch: 79 [14592/50048]	Loss: 0.1311
Training Epoch: 79 [14720/50048]	Loss: 0.0755
Training Epoch: 79 [14848/50048]	Loss: 0.0836
Training Epoch: 79 [14976/50048]	Loss: 0.1123
Training Epoch: 79 [15104/50048]	Loss: 0.1042
Training Epoch: 79 [15232/50048]	Loss: 0.1416
Training Epoch: 79 [15360/50048]	Loss: 0.0729
Training Epoch: 79 [15488/50048]	Loss: 0.0616
Training Epoch: 79 [15616/50048]	Loss: 0.0726
Training Epoch: 79 [15744/50048]	Loss: 0.1245
Training Epoch: 79 [15872/50048]	Loss: 0.1584
Training Epoch: 79 [16000/50048]	Loss: 0.0905
Training Epoch: 79 [16128/50048]	Loss: 0.1013
Training Epoch: 79 [16256/50048]	Loss: 0.0934
Training Epoch: 79 [16384/50048]	Loss: 0.1230
Training Epoch: 79 [16512/50048]	Loss: 0.1190
Training Epoch: 79 [16640/50048]	Loss: 0.1640
Training Epoch: 79 [16768/50048]	Loss: 0.0714
Training Epoch: 79 [16896/50048]	Loss: 0.1195
Training Epoch: 79 [17024/50048]	Loss: 0.0699
Training Epoch: 79 [17152/50048]	Loss: 0.0665
Training Epoch: 79 [17280/50048]	Loss: 0.0347
Training Epoch: 79 [17408/50048]	Loss: 0.1292
Training Epoch: 79 [17536/50048]	Loss: 0.0663
Training Epoch: 79 [17664/50048]	Loss: 0.1385
Training Epoch: 79 [17792/50048]	Loss: 0.1007
Training Epoch: 79 [17920/50048]	Loss: 0.0578
Training Epoch: 79 [18048/50048]	Loss: 0.0532
Training Epoch: 79 [18176/50048]	Loss: 0.0857
Training Epoch: 79 [18304/50048]	Loss: 0.1341
Training Epoch: 79 [18432/50048]	Loss: 0.0806
Training Epoch: 79 [18560/50048]	Loss: 0.1635
Training Epoch: 79 [18688/50048]	Loss: 0.1028
Training Epoch: 79 [18816/50048]	Loss: 0.1342
Training Epoch: 79 [18944/50048]	Loss: 0.0620
Training Epoch: 79 [19072/50048]	Loss: 0.0736
Training Epoch: 79 [19200/50048]	Loss: 0.1214
Training Epoch: 79 [19328/50048]	Loss: 0.0998
Training Epoch: 79 [19456/50048]	Loss: 0.0902
Training Epoch: 79 [19584/50048]	Loss: 0.0850
Training Epoch: 79 [19712/50048]	Loss: 0.0619
Training Epoch: 79 [19840/50048]	Loss: 0.1013
Training Epoch: 79 [19968/50048]	Loss: 0.0865
Training Epoch: 79 [20096/50048]	Loss: 0.1248
Training Epoch: 79 [20224/50048]	Loss: 0.1381
Training Epoch: 79 [20352/50048]	Loss: 0.0810
Training Epoch: 79 [20480/50048]	Loss: 0.1463
Training Epoch: 79 [20608/50048]	Loss: 0.0671
Training Epoch: 79 [20736/50048]	Loss: 0.0818
Training Epoch: 79 [20864/50048]	Loss: 0.0554
Training Epoch: 79 [20992/50048]	Loss: 0.0721
Training Epoch: 79 [21120/50048]	Loss: 0.1933
Training Epoch: 79 [21248/50048]	Loss: 0.1920
Training Epoch: 79 [21376/50048]	Loss: 0.1488
Training Epoch: 79 [21504/50048]	Loss: 0.1066
Training Epoch: 79 [21632/50048]	Loss: 0.1059
Training Epoch: 79 [21760/50048]	Loss: 0.0999
Training Epoch: 79 [21888/50048]	Loss: 0.2390
Training Epoch: 79 [22016/50048]	Loss: 0.1113
Training Epoch: 79 [22144/50048]	Loss: 0.1240
Training Epoch: 79 [22272/50048]	Loss: 0.0773
Training Epoch: 79 [22400/50048]	Loss: 0.1658
Training Epoch: 79 [22528/50048]	Loss: 0.0777
Training Epoch: 79 [22656/50048]	Loss: 0.1439
Training Epoch: 79 [22784/50048]	Loss: 0.1035
Training Epoch: 79 [22912/50048]	Loss: 0.1268
Training Epoch: 79 [23040/50048]	Loss: 0.1587
Training Epoch: 79 [23168/50048]	Loss: 0.2244
Training Epoch: 79 [23296/50048]	Loss: 0.1343
Training Epoch: 79 [23424/50048]	Loss: 0.1533
Training Epoch: 79 [23552/50048]	Loss: 0.1577
Training Epoch: 79 [23680/50048]	Loss: 0.1512
Training Epoch: 79 [23808/50048]	Loss: 0.1556
Training Epoch: 79 [23936/50048]	Loss: 0.0612
Training Epoch: 79 [24064/50048]	Loss: 0.1372
Training Epoch: 79 [24192/50048]	Loss: 0.0962
Training Epoch: 79 [24320/50048]	Loss: 0.0990
Training Epoch: 79 [24448/50048]	Loss: 0.0578
Training Epoch: 79 [24576/50048]	Loss: 0.1251
Training Epoch: 79 [24704/50048]	Loss: 0.0773
Training Epoch: 79 [24832/50048]	Loss: 0.1389
Training Epoch: 79 [24960/50048]	Loss: 0.1343
Training Epoch: 79 [25088/50048]	Loss: 0.0679
Training Epoch: 79 [25216/50048]	Loss: 0.0606
Training Epoch: 79 [25344/50048]	Loss: 0.1258
Training Epoch: 79 [25472/50048]	Loss: 0.1729
Training Epoch: 79 [25600/50048]	Loss: 0.1336
Training Epoch: 79 [25728/50048]	Loss: 0.1141
Training Epoch: 79 [25856/50048]	Loss: 0.2016
Training Epoch: 79 [25984/50048]	Loss: 0.1460
Training Epoch: 79 [26112/50048]	Loss: 0.1393
Training Epoch: 79 [26240/50048]	Loss: 0.1075
Training Epoch: 79 [26368/50048]	Loss: 0.1169
Training Epoch: 79 [26496/50048]	Loss: 0.1458
Training Epoch: 79 [26624/50048]	Loss: 0.0843
Training Epoch: 79 [26752/50048]	Loss: 0.0776
Training Epoch: 79 [26880/50048]	Loss: 0.1183
Training Epoch: 79 [27008/50048]	Loss: 0.0793
Training Epoch: 79 [27136/50048]	Loss: 0.1542
Training Epoch: 79 [27264/50048]	Loss: 0.1963
Training Epoch: 79 [27392/50048]	Loss: 0.0936
Training Epoch: 79 [27520/50048]	Loss: 0.0660
Training Epoch: 79 [27648/50048]	Loss: 0.1407
Training Epoch: 79 [27776/50048]	Loss: 0.1533
Training Epoch: 79 [27904/50048]	Loss: 0.0653
Training Epoch: 79 [28032/50048]	Loss: 0.1432
Training Epoch: 79 [28160/50048]	Loss: 0.0670
Training Epoch: 79 [28288/50048]	Loss: 0.0663
Training Epoch: 79 [28416/50048]	Loss: 0.0977
Training Epoch: 79 [28544/50048]	Loss: 0.0634
Training Epoch: 79 [28672/50048]	Loss: 0.0769
Training Epoch: 79 [28800/50048]	Loss: 0.0931
Training Epoch: 79 [28928/50048]	Loss: 0.1151
Training Epoch: 79 [29056/50048]	Loss: 0.1128
Training Epoch: 79 [29184/50048]	Loss: 0.0741
Training Epoch: 79 [29312/50048]	Loss: 0.1513
Training Epoch: 79 [29440/50048]	Loss: 0.0553
Training Epoch: 79 [29568/50048]	Loss: 0.0783
Training Epoch: 79 [29696/50048]	Loss: 0.1877
Training Epoch: 79 [29824/50048]	Loss: 0.0945
Training Epoch: 79 [29952/50048]	Loss: 0.1264
Training Epoch: 79 [30080/50048]	Loss: 0.1433
Training Epoch: 79 [30208/50048]	Loss: 0.1345
Training Epoch: 79 [30336/50048]	Loss: 0.1710
Training Epoch: 79 [30464/50048]	Loss: 0.1280
Training Epoch: 79 [30592/50048]	Loss: 0.0835
Training Epoch: 79 [30720/50048]	Loss: 0.0908
Training Epoch: 79 [30848/50048]	Loss: 0.1164
Training Epoch: 79 [30976/50048]	Loss: 0.1189
Training Epoch: 79 [31104/50048]	Loss: 0.0994
Training Epoch: 79 [31232/50048]	Loss: 0.1039
Training Epoch: 79 [31360/50048]	Loss: 0.1632
Training Epoch: 79 [31488/50048]	Loss: 0.1602
Training Epoch: 79 [31616/50048]	Loss: 0.1201
Training Epoch: 79 [31744/50048]	Loss: 0.0840
Training Epoch: 79 [31872/50048]	Loss: 0.0836
Training Epoch: 79 [32000/50048]	Loss: 0.0640
Training Epoch: 79 [32128/50048]	Loss: 0.1534
Training Epoch: 79 [32256/50048]	Loss: 0.0588
Training Epoch: 79 [32384/50048]	Loss: 0.1325
Training Epoch: 79 [32512/50048]	Loss: 0.1606
Training Epoch: 79 [32640/50048]	Loss: 0.2204
Training Epoch: 79 [32768/50048]	Loss: 0.1160
Training Epoch: 79 [32896/50048]	Loss: 0.1046
Training Epoch: 79 [33024/50048]	Loss: 0.0675
Training Epoch: 79 [33152/50048]	Loss: 0.1174
Training Epoch: 79 [33280/50048]	Loss: 0.0731
Training Epoch: 79 [33408/50048]	Loss: 0.1509
Training Epoch: 79 [33536/50048]	Loss: 0.1697
Training Epoch: 79 [33664/50048]	Loss: 0.0714
Training Epoch: 79 [33792/50048]	Loss: 0.0860
Training Epoch: 79 [33920/50048]	Loss: 0.1383
Training Epoch: 79 [34048/50048]	Loss: 0.1214
Training Epoch: 79 [34176/50048]	Loss: 0.1282
Training Epoch: 79 [34304/50048]	Loss: 0.0852
Training Epoch: 79 [34432/50048]	Loss: 0.2258
Training Epoch: 79 [34560/50048]	Loss: 0.0659
Training Epoch: 79 [34688/50048]	Loss: 0.1370
Training Epoch: 79 [34816/50048]	Loss: 0.1227
Training Epoch: 79 [34944/50048]	Loss: 0.1483
Training Epoch: 79 [35072/50048]	Loss: 0.1409
Training Epoch: 79 [35200/50048]	Loss: 0.1213
Training Epoch: 79 [35328/50048]	Loss: 0.1696
Training Epoch: 79 [35456/50048]	Loss: 0.0869
Training Epoch: 79 [35584/50048]	Loss: 0.1770
Training Epoch: 79 [35712/50048]	Loss: 0.0523
Training Epoch: 79 [35840/50048]	Loss: 0.1429
Training Epoch: 79 [35968/50048]	Loss: 0.1144
Training Epoch: 79 [36096/50048]	Loss: 0.0998
Training Epoch: 79 [36224/50048]	Loss: 0.0868
Training Epoch: 79 [36352/50048]	Loss: 0.1047
Training Epoch: 79 [36480/50048]	Loss: 0.1672
Training Epoch: 79 [36608/50048]	Loss: 0.0971
Training Epoch: 79 [36736/50048]	Loss: 0.1482
Training Epoch: 79 [36864/50048]	Loss: 0.1252
Training Epoch: 79 [36992/50048]	Loss: 0.1718
Training Epoch: 79 [37120/50048]	Loss: 0.2200
Training Epoch: 79 [37248/50048]	Loss: 0.0794
Training Epoch: 79 [37376/50048]	Loss: 0.1021
Training Epoch: 79 [37504/50048]	Loss: 0.1322
Training Epoch: 79 [37632/50048]	Loss: 0.0997
Training Epoch: 79 [37760/50048]	Loss: 0.0626
Training Epoch: 79 [37888/50048]	Loss: 0.0704
Training Epoch: 79 [38016/50048]	Loss: 0.2965
Training Epoch: 79 [38144/50048]	Loss: 0.1230
Training Epoch: 79 [38272/50048]	Loss: 0.0847
Training Epoch: 79 [38400/50048]	Loss: 0.0828
Training Epoch: 79 [38528/50048]	Loss: 0.0913
Training Epoch: 79 [38656/50048]	Loss: 0.0745
Training Epoch: 79 [38784/50048]	Loss: 0.0994
Training Epoch: 79 [38912/50048]	Loss: 0.2002
Training Epoch: 79 [39040/50048]	Loss: 0.1341
Training Epoch: 79 [39168/50048]	Loss: 0.1316
Training Epoch: 79 [39296/50048]	Loss: 0.0714
Training Epoch: 79 [39424/50048]	Loss: 0.0990
Training Epoch: 79 [39552/50048]	Loss: 0.1567
Training Epoch: 79 [39680/50048]	Loss: 0.1571
Training Epoch: 79 [39808/50048]	Loss: 0.1130
Training Epoch: 79 [39936/50048]	Loss: 0.1981
Training Epoch: 79 [40064/50048]	Loss: 0.0781
Training Epoch: 79 [40192/50048]	Loss: 0.1067
Training Epoch: 79 [40320/50048]	Loss: 0.1609
Training Epoch: 79 [40448/50048]	Loss: 0.0838
Training Epoch: 79 [40576/50048]	Loss: 0.0722
Training Epoch: 79 [40704/50048]	Loss: 0.0976
Training Epoch: 79 [40832/50048]	Loss: 0.1780
Training Epoch: 79 [40960/50048]	Loss: 0.1311
Training Epoch: 79 [41088/50048]	Loss: 0.1897
Training Epoch: 79 [41216/50048]	Loss: 0.1353
Training Epoch: 79 [41344/50048]	Loss: 0.1083
Training Epoch: 79 [41472/50048]	Loss: 0.2253
Training Epoch: 79 [41600/50048]	Loss: 0.1653
Training Epoch: 79 [41728/50048]	Loss: 0.0919
Training Epoch: 79 [41856/50048]	Loss: 0.1349
Training Epoch: 79 [41984/50048]	Loss: 0.1450
Training Epoch: 79 [42112/50048]	Loss: 0.1355
Training Epoch: 79 [42240/50048]	Loss: 0.1841
Training Epoch: 79 [42368/50048]	Loss: 0.0409
Training Epoch: 79 [42496/50048]	Loss: 0.1285
Training Epoch: 79 [42624/50048]	Loss: 0.0613
Training Epoch: 79 [42752/50048]	Loss: 0.2155
Training Epoch: 79 [42880/50048]	Loss: 0.1539
Training Epoch: 79 [43008/50048]	Loss: 0.1564
Training Epoch: 79 [43136/50048]	Loss: 0.1030
Training Epoch: 79 [43264/50048]	Loss: 0.1575
Training Epoch: 79 [43392/50048]	Loss: 0.0985
Training Epoch: 79 [43520/50048]	Loss: 0.0743
Training Epoch: 79 [43648/50048]	Loss: 0.0867
Training Epoch: 79 [43776/50048]	Loss: 0.0999
Training Epoch: 79 [43904/50048]	Loss: 0.0838
Training Epoch: 79 [44032/50048]	Loss: 0.1608
Training Epoch: 79 [44160/50048]	Loss: 0.0475
Training Epoch: 79 [44288/50048]	Loss: 0.1588
Training Epoch: 79 [44416/50048]	Loss: 0.0700
Training Epoch: 79 [44544/50048]	Loss: 0.1047
Training Epoch: 79 [44672/50048]	Loss: 0.1233
Training Epoch: 79 [44800/50048]	Loss: 0.0846
Training Epoch: 79 [44928/50048]	Loss: 0.0720
Training Epoch: 79 [45056/50048]	Loss: 0.0929
Training Epoch: 79 [45184/50048]	Loss: 0.1073
Training Epoch: 79 [45312/50048]	Loss: 0.1349
Training Epoch: 79 [45440/50048]	Loss: 0.0752
Training Epoch: 79 [45568/50048]	Loss: 0.1643
Training Epoch: 79 [45696/50048]	Loss: 0.1267
2022-12-06 08:08:07,931 [ZeusDataLoader(train)] train epoch 80 done: time=86.51 energy=10499.31
2022-12-06 08:08:07,933 [ZeusDataLoader(eval)] Epoch 80 begin.
Training Epoch: 79 [45824/50048]	Loss: 0.1555
Training Epoch: 79 [45952/50048]	Loss: 0.1167
Training Epoch: 79 [46080/50048]	Loss: 0.0530
Training Epoch: 79 [46208/50048]	Loss: 0.0631
Training Epoch: 79 [46336/50048]	Loss: 0.2058
Training Epoch: 79 [46464/50048]	Loss: 0.0857
Training Epoch: 79 [46592/50048]	Loss: 0.0562
Training Epoch: 79 [46720/50048]	Loss: 0.1177
Training Epoch: 79 [46848/50048]	Loss: 0.1739
Training Epoch: 79 [46976/50048]	Loss: 0.1353
Training Epoch: 79 [47104/50048]	Loss: 0.1214
Training Epoch: 79 [47232/50048]	Loss: 0.1702
Training Epoch: 79 [47360/50048]	Loss: 0.1935
Training Epoch: 79 [47488/50048]	Loss: 0.0662
Training Epoch: 79 [47616/50048]	Loss: 0.1524
Training Epoch: 79 [47744/50048]	Loss: 0.0892
Training Epoch: 79 [47872/50048]	Loss: 0.1637
Training Epoch: 79 [48000/50048]	Loss: 0.1953
Training Epoch: 79 [48128/50048]	Loss: 0.0770
Training Epoch: 79 [48256/50048]	Loss: 0.0821
Training Epoch: 79 [48384/50048]	Loss: 0.0886
Training Epoch: 79 [48512/50048]	Loss: 0.1629
Training Epoch: 79 [48640/50048]	Loss: 0.2128
Training Epoch: 79 [48768/50048]	Loss: 0.1364
Training Epoch: 79 [48896/50048]	Loss: 0.1220
Training Epoch: 79 [49024/50048]	Loss: 0.1146
Training Epoch: 79 [49152/50048]	Loss: 0.0898
Training Epoch: 79 [49280/50048]	Loss: 0.1419
Training Epoch: 79 [49408/50048]	Loss: 0.2260
Training Epoch: 79 [49536/50048]	Loss: 0.1354
Training Epoch: 79 [49664/50048]	Loss: 0.1314
Training Epoch: 79 [49792/50048]	Loss: 0.1411
Training Epoch: 79 [49920/50048]	Loss: 0.1223
Training Epoch: 79 [50048/50048]	Loss: 0.1068
2022-12-06 13:08:11.577 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 08:08:11,601 [ZeusDataLoader(eval)] eval epoch 80 done: time=3.66 energy=442.49
2022-12-06 08:08:11,601 [ZeusDataLoader(train)] Up to epoch 80: time=7216.18, energy=875950.20, cost=1069390.69
2022-12-06 08:08:11,601 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 08:08:11,601 [ZeusDataLoader(train)] Expected next epoch: time=7305.98, energy=886748.21, cost=1082647.07
2022-12-06 08:08:11,602 [ZeusDataLoader(train)] Epoch 81 begin.
Validation Epoch: 79, Average loss: 0.0179, Accuracy: 0.6381
2022-12-06 08:08:11,784 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:08:11,785 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:08:11.786 [ZeusMonitor] Monitor started.
2022-12-06 13:08:11.786 [ZeusMonitor] Running indefinitely. 2022-12-06 13:08:11.787 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:08:11.787 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e81+gpu0.power.log
Training Epoch: 80 [128/50048]	Loss: 0.1396
Training Epoch: 80 [256/50048]	Loss: 0.1114
Training Epoch: 80 [384/50048]	Loss: 0.1078
Training Epoch: 80 [512/50048]	Loss: 0.1531
Training Epoch: 80 [640/50048]	Loss: 0.0771
Training Epoch: 80 [768/50048]	Loss: 0.0538
Training Epoch: 80 [896/50048]	Loss: 0.1756
Training Epoch: 80 [1024/50048]	Loss: 0.0858
Training Epoch: 80 [1152/50048]	Loss: 0.0674
Training Epoch: 80 [1280/50048]	Loss: 0.0768
Training Epoch: 80 [1408/50048]	Loss: 0.1491
Training Epoch: 80 [1536/50048]	Loss: 0.0899
Training Epoch: 80 [1664/50048]	Loss: 0.0823
Training Epoch: 80 [1792/50048]	Loss: 0.0736
Training Epoch: 80 [1920/50048]	Loss: 0.1177
Training Epoch: 80 [2048/50048]	Loss: 0.0648
Training Epoch: 80 [2176/50048]	Loss: 0.0624
Training Epoch: 80 [2304/50048]	Loss: 0.0984
Training Epoch: 80 [2432/50048]	Loss: 0.0595
Training Epoch: 80 [2560/50048]	Loss: 0.0538
Training Epoch: 80 [2688/50048]	Loss: 0.1252
Training Epoch: 80 [2816/50048]	Loss: 0.1501
Training Epoch: 80 [2944/50048]	Loss: 0.1236
Training Epoch: 80 [3072/50048]	Loss: 0.1074
Training Epoch: 80 [3200/50048]	Loss: 0.1412
Training Epoch: 80 [3328/50048]	Loss: 0.1171
Training Epoch: 80 [3456/50048]	Loss: 0.1206
Training Epoch: 80 [3584/50048]	Loss: 0.1162
Training Epoch: 80 [3712/50048]	Loss: 0.0988
Training Epoch: 80 [3840/50048]	Loss: 0.0826
Training Epoch: 80 [3968/50048]	Loss: 0.0851
Training Epoch: 80 [4096/50048]	Loss: 0.0752
Training Epoch: 80 [4224/50048]	Loss: 0.1366
Training Epoch: 80 [4352/50048]	Loss: 0.1001
Training Epoch: 80 [4480/50048]	Loss: 0.1103
Training Epoch: 80 [4608/50048]	Loss: 0.0919
Training Epoch: 80 [4736/50048]	Loss: 0.1024
Training Epoch: 80 [4864/50048]	Loss: 0.0854
Training Epoch: 80 [4992/50048]	Loss: 0.0678
Training Epoch: 80 [5120/50048]	Loss: 0.1039
Training Epoch: 80 [5248/50048]	Loss: 0.0523
Training Epoch: 80 [5376/50048]	Loss: 0.0716
Training Epoch: 80 [5504/50048]	Loss: 0.1733
Training Epoch: 80 [5632/50048]	Loss: 0.0634
Training Epoch: 80 [5760/50048]	Loss: 0.0737
Training Epoch: 80 [5888/50048]	Loss: 0.1333
Training Epoch: 80 [6016/50048]	Loss: 0.0941
Training Epoch: 80 [6144/50048]	Loss: 0.0394
Training Epoch: 80 [6272/50048]	Loss: 0.1173
Training Epoch: 80 [6400/50048]	Loss: 0.0932
Training Epoch: 80 [6528/50048]	Loss: 0.1192
Training Epoch: 80 [6656/50048]	Loss: 0.1116
Training Epoch: 80 [6784/50048]	Loss: 0.1437
Training Epoch: 80 [6912/50048]	Loss: 0.1180
Training Epoch: 80 [7040/50048]	Loss: 0.1655
Training Epoch: 80 [7168/50048]	Loss: 0.2050
Training Epoch: 80 [7296/50048]	Loss: 0.1240
Training Epoch: 80 [7424/50048]	Loss: 0.1181
Training Epoch: 80 [7552/50048]	Loss: 0.1395
Training Epoch: 80 [7680/50048]	Loss: 0.0707
Training Epoch: 80 [7808/50048]	Loss: 0.0898
Training Epoch: 80 [7936/50048]	Loss: 0.1104
Training Epoch: 80 [8064/50048]	Loss: 0.0801
Training Epoch: 80 [8192/50048]	Loss: 0.0384
Training Epoch: 80 [8320/50048]	Loss: 0.1035
Training Epoch: 80 [8448/50048]	Loss: 0.1049
Training Epoch: 80 [8576/50048]	Loss: 0.1260
Training Epoch: 80 [8704/50048]	Loss: 0.1407
Training Epoch: 80 [8832/50048]	Loss: 0.0822
Training Epoch: 80 [8960/50048]	Loss: 0.0799
Training Epoch: 80 [9088/50048]	Loss: 0.0645
Training Epoch: 80 [9216/50048]	Loss: 0.1373
Training Epoch: 80 [9344/50048]	Loss: 0.1228
Training Epoch: 80 [9472/50048]	Loss: 0.1142
Training Epoch: 80 [9600/50048]	Loss: 0.1373
Training Epoch: 80 [9728/50048]	Loss: 0.1031
Training Epoch: 80 [9856/50048]	Loss: 0.1191
Training Epoch: 80 [9984/50048]	Loss: 0.0779
Training Epoch: 80 [10112/50048]	Loss: 0.1412
Training Epoch: 80 [10240/50048]	Loss: 0.0544
Training Epoch: 80 [10368/50048]	Loss: 0.1509
Training Epoch: 80 [10496/50048]	Loss: 0.1532
Training Epoch: 80 [10624/50048]	Loss: 0.1355
Training Epoch: 80 [10752/50048]	Loss: 0.1074
Training Epoch: 80 [10880/50048]	Loss: 0.0558
Training Epoch: 80 [11008/50048]	Loss: 0.1484
Training Epoch: 80 [11136/50048]	Loss: 0.1478
Training Epoch: 80 [11264/50048]	Loss: 0.0787
Training Epoch: 80 [11392/50048]	Loss: 0.0792
Training Epoch: 80 [11520/50048]	Loss: 0.1104
Training Epoch: 80 [11648/50048]	Loss: 0.0949
Training Epoch: 80 [11776/50048]	Loss: 0.1581
Training Epoch: 80 [11904/50048]	Loss: 0.0785
Training Epoch: 80 [12032/50048]	Loss: 0.1135
Training Epoch: 80 [12160/50048]	Loss: 0.0614
Training Epoch: 80 [12288/50048]	Loss: 0.0559
Training Epoch: 80 [12416/50048]	Loss: 0.1098
Training Epoch: 80 [12544/50048]	Loss: 0.1545
Training Epoch: 80 [12672/50048]	Loss: 0.0601
Training Epoch: 80 [12800/50048]	Loss: 0.0470
Training Epoch: 80 [12928/50048]	Loss: 0.0974
Training Epoch: 80 [13056/50048]	Loss: 0.0788
Training Epoch: 80 [13184/50048]	Loss: 0.0688
Training Epoch: 80 [13312/50048]	Loss: 0.1392
Training Epoch: 80 [13440/50048]	Loss: 0.1080
Training Epoch: 80 [13568/50048]	Loss: 0.1110
Training Epoch: 80 [13696/50048]	Loss: 0.0727
Training Epoch: 80 [13824/50048]	Loss: 0.1413
Training Epoch: 80 [13952/50048]	Loss: 0.0744
Training Epoch: 80 [14080/50048]	Loss: 0.1131
Training Epoch: 80 [14208/50048]	Loss: 0.0511
Training Epoch: 80 [14336/50048]	Loss: 0.1498
Training Epoch: 80 [14464/50048]	Loss: 0.1013
Training Epoch: 80 [14592/50048]	Loss: 0.0640
Training Epoch: 80 [14720/50048]	Loss: 0.1029
Training Epoch: 80 [14848/50048]	Loss: 0.0961
Training Epoch: 80 [14976/50048]	Loss: 0.1358
Training Epoch: 80 [15104/50048]	Loss: 0.1920
Training Epoch: 80 [15232/50048]	Loss: 0.1044
Training Epoch: 80 [15360/50048]	Loss: 0.1512
Training Epoch: 80 [15488/50048]	Loss: 0.0507
Training Epoch: 80 [15616/50048]	Loss: 0.1063
Training Epoch: 80 [15744/50048]	Loss: 0.1654
Training Epoch: 80 [15872/50048]	Loss: 0.0950
Training Epoch: 80 [16000/50048]	Loss: 0.2150
Training Epoch: 80 [16128/50048]	Loss: 0.1170
Training Epoch: 80 [16256/50048]	Loss: 0.1667
Training Epoch: 80 [16384/50048]	Loss: 0.1069
Training Epoch: 80 [16512/50048]	Loss: 0.1105
Training Epoch: 80 [16640/50048]	Loss: 0.1108
Training Epoch: 80 [16768/50048]	Loss: 0.0953
Training Epoch: 80 [16896/50048]	Loss: 0.0909
Training Epoch: 80 [17024/50048]	Loss: 0.1482
Training Epoch: 80 [17152/50048]	Loss: 0.0730
Training Epoch: 80 [17280/50048]	Loss: 0.0462
Training Epoch: 80 [17408/50048]	Loss: 0.0701
Training Epoch: 80 [17536/50048]	Loss: 0.0988
Training Epoch: 80 [17664/50048]	Loss: 0.1309
Training Epoch: 80 [17792/50048]	Loss: 0.0989
Training Epoch: 80 [17920/50048]	Loss: 0.1435
Training Epoch: 80 [18048/50048]	Loss: 0.1300
Training Epoch: 80 [18176/50048]	Loss: 0.1171
Training Epoch: 80 [18304/50048]	Loss: 0.2617
Training Epoch: 80 [18432/50048]	Loss: 0.1231
Training Epoch: 80 [18560/50048]	Loss: 0.1256
Training Epoch: 80 [18688/50048]	Loss: 0.0565
Training Epoch: 80 [18816/50048]	Loss: 0.1260
Training Epoch: 80 [18944/50048]	Loss: 0.0926
Training Epoch: 80 [19072/50048]	Loss: 0.0667
Training Epoch: 80 [19200/50048]	Loss: 0.1267
Training Epoch: 80 [19328/50048]	Loss: 0.0571
Training Epoch: 80 [19456/50048]	Loss: 0.1115
Training Epoch: 80 [19584/50048]	Loss: 0.1020
Training Epoch: 80 [19712/50048]	Loss: 0.0593
Training Epoch: 80 [19840/50048]	Loss: 0.0720
Training Epoch: 80 [19968/50048]	Loss: 0.1080
Training Epoch: 80 [20096/50048]	Loss: 0.1131
Training Epoch: 80 [20224/50048]	Loss: 0.1570
Training Epoch: 80 [20352/50048]	Loss: 0.1475
Training Epoch: 80 [20480/50048]	Loss: 0.1997
Training Epoch: 80 [20608/50048]	Loss: 0.0794
Training Epoch: 80 [20736/50048]	Loss: 0.1128
Training Epoch: 80 [20864/50048]	Loss: 0.1146
Training Epoch: 80 [20992/50048]	Loss: 0.1802
Training Epoch: 80 [21120/50048]	Loss: 0.0933
Training Epoch: 80 [21248/50048]	Loss: 0.1511
Training Epoch: 80 [21376/50048]	Loss: 0.1727
Training Epoch: 80 [21504/50048]	Loss: 0.0787
Training Epoch: 80 [21632/50048]	Loss: 0.0496
Training Epoch: 80 [21760/50048]	Loss: 0.1055
Training Epoch: 80 [21888/50048]	Loss: 0.1172
Training Epoch: 80 [22016/50048]	Loss: 0.1393
Training Epoch: 80 [22144/50048]	Loss: 0.0638
Training Epoch: 80 [22272/50048]	Loss: 0.1771
Training Epoch: 80 [22400/50048]	Loss: 0.0993
Training Epoch: 80 [22528/50048]	Loss: 0.0782
Training Epoch: 80 [22656/50048]	Loss: 0.0889
Training Epoch: 80 [22784/50048]	Loss: 0.0742
Training Epoch: 80 [22912/50048]	Loss: 0.0907
Training Epoch: 80 [23040/50048]	Loss: 0.0614
Training Epoch: 80 [23168/50048]	Loss: 0.1407
Training Epoch: 80 [23296/50048]	Loss: 0.1162
Training Epoch: 80 [23424/50048]	Loss: 0.1389
Training Epoch: 80 [23552/50048]	Loss: 0.0714
Training Epoch: 80 [23680/50048]	Loss: 0.1457
Training Epoch: 80 [23808/50048]	Loss: 0.1459
Training Epoch: 80 [23936/50048]	Loss: 0.0961
Training Epoch: 80 [24064/50048]	Loss: 0.0939
Training Epoch: 80 [24192/50048]	Loss: 0.0681
Training Epoch: 80 [24320/50048]	Loss: 0.1628
Training Epoch: 80 [24448/50048]	Loss: 0.1226
Training Epoch: 80 [24576/50048]	Loss: 0.0806
Training Epoch: 80 [24704/50048]	Loss: 0.0318
Training Epoch: 80 [24832/50048]	Loss: 0.0862
Training Epoch: 80 [24960/50048]	Loss: 0.0911
Training Epoch: 80 [25088/50048]	Loss: 0.1350
Training Epoch: 80 [25216/50048]	Loss: 0.0930
Training Epoch: 80 [25344/50048]	Loss: 0.1218
Training Epoch: 80 [25472/50048]	Loss: 0.0585
Training Epoch: 80 [25600/50048]	Loss: 0.1214
Training Epoch: 80 [25728/50048]	Loss: 0.0696
Training Epoch: 80 [25856/50048]	Loss: 0.0775
Training Epoch: 80 [25984/50048]	Loss: 0.0986
Training Epoch: 80 [26112/50048]	Loss: 0.1192
Training Epoch: 80 [26240/50048]	Loss: 0.0706
Training Epoch: 80 [26368/50048]	Loss: 0.1198
Training Epoch: 80 [26496/50048]	Loss: 0.0848
Training Epoch: 80 [26624/50048]	Loss: 0.0607
Training Epoch: 80 [26752/50048]	Loss: 0.1247
Training Epoch: 80 [26880/50048]	Loss: 0.1047
Training Epoch: 80 [27008/50048]	Loss: 0.0946
Training Epoch: 80 [27136/50048]	Loss: 0.0693
Training Epoch: 80 [27264/50048]	Loss: 0.0737
Training Epoch: 80 [27392/50048]	Loss: 0.1269
Training Epoch: 80 [27520/50048]	Loss: 0.1440
Training Epoch: 80 [27648/50048]	Loss: 0.1160
Training Epoch: 80 [27776/50048]	Loss: 0.0885
Training Epoch: 80 [27904/50048]	Loss: 0.0851
Training Epoch: 80 [28032/50048]	Loss: 0.1570
Training Epoch: 80 [28160/50048]	Loss: 0.0983
Training Epoch: 80 [28288/50048]	Loss: 0.0991
Training Epoch: 80 [28416/50048]	Loss: 0.0769
Training Epoch: 80 [28544/50048]	Loss: 0.1250
Training Epoch: 80 [28672/50048]	Loss: 0.0692
Training Epoch: 80 [28800/50048]	Loss: 0.0738
Training Epoch: 80 [28928/50048]	Loss: 0.0934
Training Epoch: 80 [29056/50048]	Loss: 0.0823
Training Epoch: 80 [29184/50048]	Loss: 0.0812
Training Epoch: 80 [29312/50048]	Loss: 0.1470
Training Epoch: 80 [29440/50048]	Loss: 0.1206
Training Epoch: 80 [29568/50048]	Loss: 0.2450
Training Epoch: 80 [29696/50048]	Loss: 0.1423
Training Epoch: 80 [29824/50048]	Loss: 0.1655
Training Epoch: 80 [29952/50048]	Loss: 0.1455
Training Epoch: 80 [30080/50048]	Loss: 0.0757
Training Epoch: 80 [30208/50048]	Loss: 0.0863
Training Epoch: 80 [30336/50048]	Loss: 0.1326
Training Epoch: 80 [30464/50048]	Loss: 0.1006
Training Epoch: 80 [30592/50048]	Loss: 0.0709
Training Epoch: 80 [30720/50048]	Loss: 0.1547
Training Epoch: 80 [30848/50048]	Loss: 0.1214
Training Epoch: 80 [30976/50048]	Loss: 0.0656
Training Epoch: 80 [31104/50048]	Loss: 0.0384
Training Epoch: 80 [31232/50048]	Loss: 0.0891
Training Epoch: 80 [31360/50048]	Loss: 0.1345
Training Epoch: 80 [31488/50048]	Loss: 0.1109
Training Epoch: 80 [31616/50048]	Loss: 0.1428
Training Epoch: 80 [31744/50048]	Loss: 0.1156
Training Epoch: 80 [31872/50048]	Loss: 0.1471
Training Epoch: 80 [32000/50048]	Loss: 0.0714
Training Epoch: 80 [32128/50048]	Loss: 0.1611
Training Epoch: 80 [32256/50048]	Loss: 0.2381
Training Epoch: 80 [32384/50048]	Loss: 0.0945
Training Epoch: 80 [32512/50048]	Loss: 0.1249
Training Epoch: 80 [32640/50048]	Loss: 0.1496
Training Epoch: 80 [32768/50048]	Loss: 0.0706
Training Epoch: 80 [32896/50048]	Loss: 0.1024
Training Epoch: 80 [33024/50048]	Loss: 0.0921
Training Epoch: 80 [33152/50048]	Loss: 0.0923
Training Epoch: 80 [33280/50048]	Loss: 0.1205
Training Epoch: 80 [33408/50048]	Loss: 0.1393
Training Epoch: 80 [33536/50048]	Loss: 0.0679
Training Epoch: 80 [33664/50048]	Loss: 0.0828
Training Epoch: 80 [33792/50048]	Loss: 0.1274
Training Epoch: 80 [33920/50048]	Loss: 0.1127
Training Epoch: 80 [34048/50048]	Loss: 0.1116
Training Epoch: 80 [34176/50048]	Loss: 0.1229
Training Epoch: 80 [34304/50048]	Loss: 0.0391
Training Epoch: 80 [34432/50048]	Loss: 0.0655
Training Epoch: 80 [34560/50048]	Loss: 0.1401
Training Epoch: 80 [34688/50048]	Loss: 0.1586
Training Epoch: 80 [34816/50048]	Loss: 0.1386
Training Epoch: 80 [34944/50048]	Loss: 0.1212
Training Epoch: 80 [35072/50048]	Loss: 0.0686
Training Epoch: 80 [35200/50048]	Loss: 0.0565
Training Epoch: 80 [35328/50048]	Loss: 0.1448
Training Epoch: 80 [35456/50048]	Loss: 0.2287
Training Epoch: 80 [35584/50048]	Loss: 0.1103
Training Epoch: 80 [35712/50048]	Loss: 0.1043
Training Epoch: 80 [35840/50048]	Loss: 0.1315
Training Epoch: 80 [35968/50048]	Loss: 0.1502
Training Epoch: 80 [36096/50048]	Loss: 0.1343
Training Epoch: 80 [36224/50048]	Loss: 0.1366
Training Epoch: 80 [36352/50048]	Loss: 0.1083
Training Epoch: 80 [36480/50048]	Loss: 0.0627
Training Epoch: 80 [36608/50048]	Loss: 0.0853
Training Epoch: 80 [36736/50048]	Loss: 0.2071
Training Epoch: 80 [36864/50048]	Loss: 0.1290
Training Epoch: 80 [36992/50048]	Loss: 0.0884
Training Epoch: 80 [37120/50048]	Loss: 0.1791
Training Epoch: 80 [37248/50048]	Loss: 0.1229
Training Epoch: 80 [37376/50048]	Loss: 0.1924
Training Epoch: 80 [37504/50048]	Loss: 0.1137
Training Epoch: 80 [37632/50048]	Loss: 0.0693
Training Epoch: 80 [37760/50048]	Loss: 0.0479
Training Epoch: 80 [37888/50048]	Loss: 0.0833
Training Epoch: 80 [38016/50048]	Loss: 0.1364
Training Epoch: 80 [38144/50048]	Loss: 0.0805
Training Epoch: 80 [38272/50048]	Loss: 0.0822
Training Epoch: 80 [38400/50048]	Loss: 0.1491
Training Epoch: 80 [38528/50048]	Loss: 0.1077
Training Epoch: 80 [38656/50048]	Loss: 0.1069
Training Epoch: 80 [38784/50048]	Loss: 0.0859
Training Epoch: 80 [38912/50048]	Loss: 0.2083
Training Epoch: 80 [39040/50048]	Loss: 0.0769
Training Epoch: 80 [39168/50048]	Loss: 0.1668
Training Epoch: 80 [39296/50048]	Loss: 0.1461
Training Epoch: 80 [39424/50048]	Loss: 0.1145
Training Epoch: 80 [39552/50048]	Loss: 0.1081
Training Epoch: 80 [39680/50048]	Loss: 0.1212
Training Epoch: 80 [39808/50048]	Loss: 0.1623
Training Epoch: 80 [39936/50048]	Loss: 0.1151
Training Epoch: 80 [40064/50048]	Loss: 0.0940
Training Epoch: 80 [40192/50048]	Loss: 0.1108
Training Epoch: 80 [40320/50048]	Loss: 0.1229
Training Epoch: 80 [40448/50048]	Loss: 0.1510
Training Epoch: 80 [40576/50048]	Loss: 0.1272
Training Epoch: 80 [40704/50048]	Loss: 0.2256
Training Epoch: 80 [40832/50048]	Loss: 0.0748
Training Epoch: 80 [40960/50048]	Loss: 0.1280
Training Epoch: 80 [41088/50048]	Loss: 0.1970
Training Epoch: 80 [41216/50048]	Loss: 0.2868
Training Epoch: 80 [41344/50048]	Loss: 0.0679
Training Epoch: 80 [41472/50048]	Loss: 0.1636
Training Epoch: 80 [41600/50048]	Loss: 0.0793
Training Epoch: 80 [41728/50048]	Loss: 0.1422
Training Epoch: 80 [41856/50048]	Loss: 0.1255
Training Epoch: 80 [41984/50048]	Loss: 0.1052
Training Epoch: 80 [42112/50048]	Loss: 0.1446
Training Epoch: 80 [42240/50048]	Loss: 0.1200
Training Epoch: 80 [42368/50048]	Loss: 0.0820
Training Epoch: 80 [42496/50048]	Loss: 0.0883
Training Epoch: 80 [42624/50048]	Loss: 0.2555
Training Epoch: 80 [42752/50048]	Loss: 0.1241
Training Epoch: 80 [42880/50048]	Loss: 0.0816
Training Epoch: 80 [43008/50048]	Loss: 0.1095
Training Epoch: 80 [43136/50048]	Loss: 0.1107
Training Epoch: 80 [43264/50048]	Loss: 0.1231
Training Epoch: 80 [43392/50048]	Loss: 0.2257
Training Epoch: 80 [43520/50048]	Loss: 0.1042
Training Epoch: 80 [43648/50048]	Loss: 0.1885
Training Epoch: 80 [43776/50048]	Loss: 0.0646
Training Epoch: 80 [43904/50048]	Loss: 0.1548
Training Epoch: 80 [44032/50048]	Loss: 0.1450
Training Epoch: 80 [44160/50048]	Loss: 0.1347
Training Epoch: 80 [44288/50048]	Loss: 0.2539
Training Epoch: 80 [44416/50048]	Loss: 0.1164
Training Epoch: 80 [44544/50048]	Loss: 0.1759
Training Epoch: 80 [44672/50048]	Loss: 0.0785
Training Epoch: 80 [44800/50048]	Loss: 0.1168
Training Epoch: 80 [44928/50048]	Loss: 0.0738
Training Epoch: 80 [45056/50048]	Loss: 0.1097
Training Epoch: 80 [45184/50048]	Loss: 0.1522
Training Epoch: 80 [45312/50048]	Loss: 0.1384
Training Epoch: 80 [45440/50048]	Loss: 0.0889
Training Epoch: 80 [45568/50048]	Loss: 0.1346
Training Epoch: 80 [45696/50048]	Loss: 0.1511
2022-12-06 08:09:38,196 [ZeusDataLoader(train)] train epoch 81 done: time=86.58 energy=10500.23
2022-12-06 08:09:38,198 [ZeusDataLoader(eval)] Epoch 81 begin.
Training Epoch: 80 [45824/50048]	Loss: 0.1498
Training Epoch: 80 [45952/50048]	Loss: 0.0705
Training Epoch: 80 [46080/50048]	Loss: 0.1776
Training Epoch: 80 [46208/50048]	Loss: 0.1552
Training Epoch: 80 [46336/50048]	Loss: 0.1671
Training Epoch: 80 [46464/50048]	Loss: 0.1477
Training Epoch: 80 [46592/50048]	Loss: 0.1092
Training Epoch: 80 [46720/50048]	Loss: 0.2455
Training Epoch: 80 [46848/50048]	Loss: 0.1204
Training Epoch: 80 [46976/50048]	Loss: 0.0836
Training Epoch: 80 [47104/50048]	Loss: 0.1222
Training Epoch: 80 [47232/50048]	Loss: 0.1323
Training Epoch: 80 [47360/50048]	Loss: 0.0649
Training Epoch: 80 [47488/50048]	Loss: 0.1302
Training Epoch: 80 [47616/50048]	Loss: 0.1664
Training Epoch: 80 [47744/50048]	Loss: 0.1181
Training Epoch: 80 [47872/50048]	Loss: 0.0862
Training Epoch: 80 [48000/50048]	Loss: 0.1320
Training Epoch: 80 [48128/50048]	Loss: 0.0946
Training Epoch: 80 [48256/50048]	Loss: 0.1693
Training Epoch: 80 [48384/50048]	Loss: 0.1843
Training Epoch: 80 [48512/50048]	Loss: 0.0683
Training Epoch: 80 [48640/50048]	Loss: 0.1349
Training Epoch: 80 [48768/50048]	Loss: 0.1769
Training Epoch: 80 [48896/50048]	Loss: 0.1149
Training Epoch: 80 [49024/50048]	Loss: 0.1221
Training Epoch: 80 [49152/50048]	Loss: 0.0922
Training Epoch: 80 [49280/50048]	Loss: 0.1480
Training Epoch: 80 [49408/50048]	Loss: 0.1163
Training Epoch: 80 [49536/50048]	Loss: 0.1932
Training Epoch: 80 [49664/50048]	Loss: 0.0997
Training Epoch: 80 [49792/50048]	Loss: 0.1235
Training Epoch: 80 [49920/50048]	Loss: 0.0753
Training Epoch: 80 [50048/50048]	Loss: 0.1794
2022-12-06 13:09:41.888 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 08:09:41,917 [ZeusDataLoader(eval)] eval epoch 81 done: time=3.71 energy=451.84
2022-12-06 08:09:41,917 [ZeusDataLoader(train)] Up to epoch 81: time=7306.47, energy=886902.26, cost=1082767.47
2022-12-06 08:09:41,917 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 08:09:41,917 [ZeusDataLoader(train)] Expected next epoch: time=7396.27, energy=897700.28, cost=1096023.85
2022-12-06 08:09:41,918 [ZeusDataLoader(train)] Epoch 82 begin.
Validation Epoch: 80, Average loss: 0.0180, Accuracy: 0.6390
2022-12-06 08:09:42,077 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:09:42,077 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:09:42.079 [ZeusMonitor] Monitor started.
2022-12-06 13:09:42.079 [ZeusMonitor] Running indefinitely. 2022-12-06 13:09:42.079 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:09:42.079 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e82+gpu0.power.log
Training Epoch: 81 [128/50048]	Loss: 0.1557
Training Epoch: 81 [256/50048]	Loss: 0.0943
Training Epoch: 81 [384/50048]	Loss: 0.1031
Training Epoch: 81 [512/50048]	Loss: 0.0772
Training Epoch: 81 [640/50048]	Loss: 0.0983
Training Epoch: 81 [768/50048]	Loss: 0.1026
Training Epoch: 81 [896/50048]	Loss: 0.1693
Training Epoch: 81 [1024/50048]	Loss: 0.0959
Training Epoch: 81 [1152/50048]	Loss: 0.1601
Training Epoch: 81 [1280/50048]	Loss: 0.0603
Training Epoch: 81 [1408/50048]	Loss: 0.1198
Training Epoch: 81 [1536/50048]	Loss: 0.0572
Training Epoch: 81 [1664/50048]	Loss: 0.1284
Training Epoch: 81 [1792/50048]	Loss: 0.0988
Training Epoch: 81 [1920/50048]	Loss: 0.0818
Training Epoch: 81 [2048/50048]	Loss: 0.1497
Training Epoch: 81 [2176/50048]	Loss: 0.0992
Training Epoch: 81 [2304/50048]	Loss: 0.0634
Training Epoch: 81 [2432/50048]	Loss: 0.1405
Training Epoch: 81 [2560/50048]	Loss: 0.1023
Training Epoch: 81 [2688/50048]	Loss: 0.1831
Training Epoch: 81 [2816/50048]	Loss: 0.0919
Training Epoch: 81 [2944/50048]	Loss: 0.1183
Training Epoch: 81 [3072/50048]	Loss: 0.1058
Training Epoch: 81 [3200/50048]	Loss: 0.1172
Training Epoch: 81 [3328/50048]	Loss: 0.1166
Training Epoch: 81 [3456/50048]	Loss: 0.1196
Training Epoch: 81 [3584/50048]	Loss: 0.0678
Training Epoch: 81 [3712/50048]	Loss: 0.1378
Training Epoch: 81 [3840/50048]	Loss: 0.1392
Training Epoch: 81 [3968/50048]	Loss: 0.1668
Training Epoch: 81 [4096/50048]	Loss: 0.1900
Training Epoch: 81 [4224/50048]	Loss: 0.0704
Training Epoch: 81 [4352/50048]	Loss: 0.1210
Training Epoch: 81 [4480/50048]	Loss: 0.1897
Training Epoch: 81 [4608/50048]	Loss: 0.1282
Training Epoch: 81 [4736/50048]	Loss: 0.0579
Training Epoch: 81 [4864/50048]	Loss: 0.0586
Training Epoch: 81 [4992/50048]	Loss: 0.0979
Training Epoch: 81 [5120/50048]	Loss: 0.0971
Training Epoch: 81 [5248/50048]	Loss: 0.1054
Training Epoch: 81 [5376/50048]	Loss: 0.1911
Training Epoch: 81 [5504/50048]	Loss: 0.0995
Training Epoch: 81 [5632/50048]	Loss: 0.0906
Training Epoch: 81 [5760/50048]	Loss: 0.1031
Training Epoch: 81 [5888/50048]	Loss: 0.0794
Training Epoch: 81 [6016/50048]	Loss: 0.0873
Training Epoch: 81 [6144/50048]	Loss: 0.1791
Training Epoch: 81 [6272/50048]	Loss: 0.0473
Training Epoch: 81 [6400/50048]	Loss: 0.1247
Training Epoch: 81 [6528/50048]	Loss: 0.1273
Training Epoch: 81 [6656/50048]	Loss: 0.0839
Training Epoch: 81 [6784/50048]	Loss: 0.0920
Training Epoch: 81 [6912/50048]	Loss: 0.1949
Training Epoch: 81 [7040/50048]	Loss: 0.0839
Training Epoch: 81 [7168/50048]	Loss: 0.0848
Training Epoch: 81 [7296/50048]	Loss: 0.1053
Training Epoch: 81 [7424/50048]	Loss: 0.0754
Training Epoch: 81 [7552/50048]	Loss: 0.1376
Training Epoch: 81 [7680/50048]	Loss: 0.1308
Training Epoch: 81 [7808/50048]	Loss: 0.0595
Training Epoch: 81 [7936/50048]	Loss: 0.1561
Training Epoch: 81 [8064/50048]	Loss: 0.1379
Training Epoch: 81 [8192/50048]	Loss: 0.0751
Training Epoch: 81 [8320/50048]	Loss: 0.0911
Training Epoch: 81 [8448/50048]	Loss: 0.0551
Training Epoch: 81 [8576/50048]	Loss: 0.1723
Training Epoch: 81 [8704/50048]	Loss: 0.1003
Training Epoch: 81 [8832/50048]	Loss: 0.1673
Training Epoch: 81 [8960/50048]	Loss: 0.0949
Training Epoch: 81 [9088/50048]	Loss: 0.1097
Training Epoch: 81 [9216/50048]	Loss: 0.0816
Training Epoch: 81 [9344/50048]	Loss: 0.0782
Training Epoch: 81 [9472/50048]	Loss: 0.1005
Training Epoch: 81 [9600/50048]	Loss: 0.1343
Training Epoch: 81 [9728/50048]	Loss: 0.1564
Training Epoch: 81 [9856/50048]	Loss: 0.0718
Training Epoch: 81 [9984/50048]	Loss: 0.0586
Training Epoch: 81 [10112/50048]	Loss: 0.0802
Training Epoch: 81 [10240/50048]	Loss: 0.0803
Training Epoch: 81 [10368/50048]	Loss: 0.0946
Training Epoch: 81 [10496/50048]	Loss: 0.0529
Training Epoch: 81 [10624/50048]	Loss: 0.1732
Training Epoch: 81 [10752/50048]	Loss: 0.1476
Training Epoch: 81 [10880/50048]	Loss: 0.1156
Training Epoch: 81 [11008/50048]	Loss: 0.0573
Training Epoch: 81 [11136/50048]	Loss: 0.0458
Training Epoch: 81 [11264/50048]	Loss: 0.0527
Training Epoch: 81 [11392/50048]	Loss: 0.1331
Training Epoch: 81 [11520/50048]	Loss: 0.0775
Training Epoch: 81 [11648/50048]	Loss: 0.0389
Training Epoch: 81 [11776/50048]	Loss: 0.0259
Training Epoch: 81 [11904/50048]	Loss: 0.1301
Training Epoch: 81 [12032/50048]	Loss: 0.0824
Training Epoch: 81 [12160/50048]	Loss: 0.1026
Training Epoch: 81 [12288/50048]	Loss: 0.0585
Training Epoch: 81 [12416/50048]	Loss: 0.0763
Training Epoch: 81 [12544/50048]	Loss: 0.0389
Training Epoch: 81 [12672/50048]	Loss: 0.0875
Training Epoch: 81 [12800/50048]	Loss: 0.0986
Training Epoch: 81 [12928/50048]	Loss: 0.0935
Training Epoch: 81 [13056/50048]	Loss: 0.0610
Training Epoch: 81 [13184/50048]	Loss: 0.1252
Training Epoch: 81 [13312/50048]	Loss: 0.1231
Training Epoch: 81 [13440/50048]	Loss: 0.1110
Training Epoch: 81 [13568/50048]	Loss: 0.1479
Training Epoch: 81 [13696/50048]	Loss: 0.0535
Training Epoch: 81 [13824/50048]	Loss: 0.1078
Training Epoch: 81 [13952/50048]	Loss: 0.0535
Training Epoch: 81 [14080/50048]	Loss: 0.1107
Training Epoch: 81 [14208/50048]	Loss: 0.1021
Training Epoch: 81 [14336/50048]	Loss: 0.0452
Training Epoch: 81 [14464/50048]	Loss: 0.0725
Training Epoch: 81 [14592/50048]	Loss: 0.0859
Training Epoch: 81 [14720/50048]	Loss: 0.0792
Training Epoch: 81 [14848/50048]	Loss: 0.2223
Training Epoch: 81 [14976/50048]	Loss: 0.0621
Training Epoch: 81 [15104/50048]	Loss: 0.1475
Training Epoch: 81 [15232/50048]	Loss: 0.1370
Training Epoch: 81 [15360/50048]	Loss: 0.1205
Training Epoch: 81 [15488/50048]	Loss: 0.1329
Training Epoch: 81 [15616/50048]	Loss: 0.1025
Training Epoch: 81 [15744/50048]	Loss: 0.0289
Training Epoch: 81 [15872/50048]	Loss: 0.2111
Training Epoch: 81 [16000/50048]	Loss: 0.0764
Training Epoch: 81 [16128/50048]	Loss: 0.1128
Training Epoch: 81 [16256/50048]	Loss: 0.1378
Training Epoch: 81 [16384/50048]	Loss: 0.0564
Training Epoch: 81 [16512/50048]	Loss: 0.1149
Training Epoch: 81 [16640/50048]	Loss: 0.0963
Training Epoch: 81 [16768/50048]	Loss: 0.0823
Training Epoch: 81 [16896/50048]	Loss: 0.0879
Training Epoch: 81 [17024/50048]	Loss: 0.0987
Training Epoch: 81 [17152/50048]	Loss: 0.1188
Training Epoch: 81 [17280/50048]	Loss: 0.0914
Training Epoch: 81 [17408/50048]	Loss: 0.1576
Training Epoch: 81 [17536/50048]	Loss: 0.0761
Training Epoch: 81 [17664/50048]	Loss: 0.0719
Training Epoch: 81 [17792/50048]	Loss: 0.1561
Training Epoch: 81 [17920/50048]	Loss: 0.1080
Training Epoch: 81 [18048/50048]	Loss: 0.1418
Training Epoch: 81 [18176/50048]	Loss: 0.0946
Training Epoch: 81 [18304/50048]	Loss: 0.0910
Training Epoch: 81 [18432/50048]	Loss: 0.0630
Training Epoch: 81 [18560/50048]	Loss: 0.1015
Training Epoch: 81 [18688/50048]	Loss: 0.0810
Training Epoch: 81 [18816/50048]	Loss: 0.0297
Training Epoch: 81 [18944/50048]	Loss: 0.1381
Training Epoch: 81 [19072/50048]	Loss: 0.0466
Training Epoch: 81 [19200/50048]	Loss: 0.0714
Training Epoch: 81 [19328/50048]	Loss: 0.0711
Training Epoch: 81 [19456/50048]	Loss: 0.0526
Training Epoch: 81 [19584/50048]	Loss: 0.0457
Training Epoch: 81 [19712/50048]	Loss: 0.1203
Training Epoch: 81 [19840/50048]	Loss: 0.1380
Training Epoch: 81 [19968/50048]	Loss: 0.0986
Training Epoch: 81 [20096/50048]	Loss: 0.1005
Training Epoch: 81 [20224/50048]	Loss: 0.0845
Training Epoch: 81 [20352/50048]	Loss: 0.1121
Training Epoch: 81 [20480/50048]	Loss: 0.1592
Training Epoch: 81 [20608/50048]	Loss: 0.0697
Training Epoch: 81 [20736/50048]	Loss: 0.2097
Training Epoch: 81 [20864/50048]	Loss: 0.1172
Training Epoch: 81 [20992/50048]	Loss: 0.1166
Training Epoch: 81 [21120/50048]	Loss: 0.1102
Training Epoch: 81 [21248/50048]	Loss: 0.1156
Training Epoch: 81 [21376/50048]	Loss: 0.0813
Training Epoch: 81 [21504/50048]	Loss: 0.1937
Training Epoch: 81 [21632/50048]	Loss: 0.0834
Training Epoch: 81 [21760/50048]	Loss: 0.1107
Training Epoch: 81 [21888/50048]	Loss: 0.1247
Training Epoch: 81 [22016/50048]	Loss: 0.0935
Training Epoch: 81 [22144/50048]	Loss: 0.0740
Training Epoch: 81 [22272/50048]	Loss: 0.0412
Training Epoch: 81 [22400/50048]	Loss: 0.1297
Training Epoch: 81 [22528/50048]	Loss: 0.1311
Training Epoch: 81 [22656/50048]	Loss: 0.0427
Training Epoch: 81 [22784/50048]	Loss: 0.1325
Training Epoch: 81 [22912/50048]	Loss: 0.1103
Training Epoch: 81 [23040/50048]	Loss: 0.1092
Training Epoch: 81 [23168/50048]	Loss: 0.0679
Training Epoch: 81 [23296/50048]	Loss: 0.0496
Training Epoch: 81 [23424/50048]	Loss: 0.1325
Training Epoch: 81 [23552/50048]	Loss: 0.0982
Training Epoch: 81 [23680/50048]	Loss: 0.0679
Training Epoch: 81 [23808/50048]	Loss: 0.0504
Training Epoch: 81 [23936/50048]	Loss: 0.0960
Training Epoch: 81 [24064/50048]	Loss: 0.1323
Training Epoch: 81 [24192/50048]	Loss: 0.0690
Training Epoch: 81 [24320/50048]	Loss: 0.0884
Training Epoch: 81 [24448/50048]	Loss: 0.0736
Training Epoch: 81 [24576/50048]	Loss: 0.2187
Training Epoch: 81 [24704/50048]	Loss: 0.1088
Training Epoch: 81 [24832/50048]	Loss: 0.1378
Training Epoch: 81 [24960/50048]	Loss: 0.1137
Training Epoch: 81 [25088/50048]	Loss: 0.0854
Training Epoch: 81 [25216/50048]	Loss: 0.1126
Training Epoch: 81 [25344/50048]	Loss: 0.0568
Training Epoch: 81 [25472/50048]	Loss: 0.1107
Training Epoch: 81 [25600/50048]	Loss: 0.1043
Training Epoch: 81 [25728/50048]	Loss: 0.1136
Training Epoch: 81 [25856/50048]	Loss: 0.1255
Training Epoch: 81 [25984/50048]	Loss: 0.2009
Training Epoch: 81 [26112/50048]	Loss: 0.1178
Training Epoch: 81 [26240/50048]	Loss: 0.1050
Training Epoch: 81 [26368/50048]	Loss: 0.1215
Training Epoch: 81 [26496/50048]	Loss: 0.2274
Training Epoch: 81 [26624/50048]	Loss: 0.1046
Training Epoch: 81 [26752/50048]	Loss: 0.1301
Training Epoch: 81 [26880/50048]	Loss: 0.2045
Training Epoch: 81 [27008/50048]	Loss: 0.1087
Training Epoch: 81 [27136/50048]	Loss: 0.1265
Training Epoch: 81 [27264/50048]	Loss: 0.1472
Training Epoch: 81 [27392/50048]	Loss: 0.1144
Training Epoch: 81 [27520/50048]	Loss: 0.1534
Training Epoch: 81 [27648/50048]	Loss: 0.0875
Training Epoch: 81 [27776/50048]	Loss: 0.1247
Training Epoch: 81 [27904/50048]	Loss: 0.1524
Training Epoch: 81 [28032/50048]	Loss: 0.1187
Training Epoch: 81 [28160/50048]	Loss: 0.1005
Training Epoch: 81 [28288/50048]	Loss: 0.0558
Training Epoch: 81 [28416/50048]	Loss: 0.1356
Training Epoch: 81 [28544/50048]	Loss: 0.0857
Training Epoch: 81 [28672/50048]	Loss: 0.1368
Training Epoch: 81 [28800/50048]	Loss: 0.0807
Training Epoch: 81 [28928/50048]	Loss: 0.1210
Training Epoch: 81 [29056/50048]	Loss: 0.1530
Training Epoch: 81 [29184/50048]	Loss: 0.0702
Training Epoch: 81 [29312/50048]	Loss: 0.1761
Training Epoch: 81 [29440/50048]	Loss: 0.0822
Training Epoch: 81 [29568/50048]	Loss: 0.1001
Training Epoch: 81 [29696/50048]	Loss: 0.0904
Training Epoch: 81 [29824/50048]	Loss: 0.1159
Training Epoch: 81 [29952/50048]	Loss: 0.0833
Training Epoch: 81 [30080/50048]	Loss: 0.0670
Training Epoch: 81 [30208/50048]	Loss: 0.0596
Training Epoch: 81 [30336/50048]	Loss: 0.0762
Training Epoch: 81 [30464/50048]	Loss: 0.0602
Training Epoch: 81 [30592/50048]	Loss: 0.1110
Training Epoch: 81 [30720/50048]	Loss: 0.1571
Training Epoch: 81 [30848/50048]	Loss: 0.1227
Training Epoch: 81 [30976/50048]	Loss: 0.1369
Training Epoch: 81 [31104/50048]	Loss: 0.0543
Training Epoch: 81 [31232/50048]	Loss: 0.0812
Training Epoch: 81 [31360/50048]	Loss: 0.1545
Training Epoch: 81 [31488/50048]	Loss: 0.0632
Training Epoch: 81 [31616/50048]	Loss: 0.0984
Training Epoch: 81 [31744/50048]	Loss: 0.0504
Training Epoch: 81 [31872/50048]	Loss: 0.0522
Training Epoch: 81 [32000/50048]	Loss: 0.1061
Training Epoch: 81 [32128/50048]	Loss: 0.1542
Training Epoch: 81 [32256/50048]	Loss: 0.0900
Training Epoch: 81 [32384/50048]	Loss: 0.0894
Training Epoch: 81 [32512/50048]	Loss: 0.1381
Training Epoch: 81 [32640/50048]	Loss: 0.0755
Training Epoch: 81 [32768/50048]	Loss: 0.1739
Training Epoch: 81 [32896/50048]	Loss: 0.1396
Training Epoch: 81 [33024/50048]	Loss: 0.1675
Training Epoch: 81 [33152/50048]	Loss: 0.2354
Training Epoch: 81 [33280/50048]	Loss: 0.0691
Training Epoch: 81 [33408/50048]	Loss: 0.1263
Training Epoch: 81 [33536/50048]	Loss: 0.1210
Training Epoch: 81 [33664/50048]	Loss: 0.0933
Training Epoch: 81 [33792/50048]	Loss: 0.1040
Training Epoch: 81 [33920/50048]	Loss: 0.0849
Training Epoch: 81 [34048/50048]	Loss: 0.1071
Training Epoch: 81 [34176/50048]	Loss: 0.1021
Training Epoch: 81 [34304/50048]	Loss: 0.2072
Training Epoch: 81 [34432/50048]	Loss: 0.0949
Training Epoch: 81 [34560/50048]	Loss: 0.1191
Training Epoch: 81 [34688/50048]	Loss: 0.1271
Training Epoch: 81 [34816/50048]	Loss: 0.1021
Training Epoch: 81 [34944/50048]	Loss: 0.1173
Training Epoch: 81 [35072/50048]	Loss: 0.1077
Training Epoch: 81 [35200/50048]	Loss: 0.1470
Training Epoch: 81 [35328/50048]	Loss: 0.1415
Training Epoch: 81 [35456/50048]	Loss: 0.1465
Training Epoch: 81 [35584/50048]	Loss: 0.0377
Training Epoch: 81 [35712/50048]	Loss: 0.0876
Training Epoch: 81 [35840/50048]	Loss: 0.1067
Training Epoch: 81 [35968/50048]	Loss: 0.1329
Training Epoch: 81 [36096/50048]	Loss: 0.0659
Training Epoch: 81 [36224/50048]	Loss: 0.0766
Training Epoch: 81 [36352/50048]	Loss: 0.0734
Training Epoch: 81 [36480/50048]	Loss: 0.0496
Training Epoch: 81 [36608/50048]	Loss: 0.0647
Training Epoch: 81 [36736/50048]	Loss: 0.1409
Training Epoch: 81 [36864/50048]	Loss: 0.0501
Training Epoch: 81 [36992/50048]	Loss: 0.1308
Training Epoch: 81 [37120/50048]	Loss: 0.1017
Training Epoch: 81 [37248/50048]	Loss: 0.1461
Training Epoch: 81 [37376/50048]	Loss: 0.2531
Training Epoch: 81 [37504/50048]	Loss: 0.0925
Training Epoch: 81 [37632/50048]	Loss: 0.1917
Training Epoch: 81 [37760/50048]	Loss: 0.0622
Training Epoch: 81 [37888/50048]	Loss: 0.1455
Training Epoch: 81 [38016/50048]	Loss: 0.1451
Training Epoch: 81 [38144/50048]	Loss: 0.0611
Training Epoch: 81 [38272/50048]	Loss: 0.0891
Training Epoch: 81 [38400/50048]	Loss: 0.0739
Training Epoch: 81 [38528/50048]	Loss: 0.0944
Training Epoch: 81 [38656/50048]	Loss: 0.0788
Training Epoch: 81 [38784/50048]	Loss: 0.0907
Training Epoch: 81 [38912/50048]	Loss: 0.0957
Training Epoch: 81 [39040/50048]	Loss: 0.0894
Training Epoch: 81 [39168/50048]	Loss: 0.1924
Training Epoch: 81 [39296/50048]	Loss: 0.0958
Training Epoch: 81 [39424/50048]	Loss: 0.1432
Training Epoch: 81 [39552/50048]	Loss: 0.0515
Training Epoch: 81 [39680/50048]	Loss: 0.1670
Training Epoch: 81 [39808/50048]	Loss: 0.1672
Training Epoch: 81 [39936/50048]	Loss: 0.1259
Training Epoch: 81 [40064/50048]	Loss: 0.1640
Training Epoch: 81 [40192/50048]	Loss: 0.1475
Training Epoch: 81 [40320/50048]	Loss: 0.0749
Training Epoch: 81 [40448/50048]	Loss: 0.1106
Training Epoch: 81 [40576/50048]	Loss: 0.1530
Training Epoch: 81 [40704/50048]	Loss: 0.1096
Training Epoch: 81 [40832/50048]	Loss: 0.1477
Training Epoch: 81 [40960/50048]	Loss: 0.1694
Training Epoch: 81 [41088/50048]	Loss: 0.1663
Training Epoch: 81 [41216/50048]	Loss: 0.1162
Training Epoch: 81 [41344/50048]	Loss: 0.0621
Training Epoch: 81 [41472/50048]	Loss: 0.0752
Training Epoch: 81 [41600/50048]	Loss: 0.1954
Training Epoch: 81 [41728/50048]	Loss: 0.1190
Training Epoch: 81 [41856/50048]	Loss: 0.0939
Training Epoch: 81 [41984/50048]	Loss: 0.0575
Training Epoch: 81 [42112/50048]	Loss: 0.1762
Training Epoch: 81 [42240/50048]	Loss: 0.0672
Training Epoch: 81 [42368/50048]	Loss: 0.1013
Training Epoch: 81 [42496/50048]	Loss: 0.1330
Training Epoch: 81 [42624/50048]	Loss: 0.2340
Training Epoch: 81 [42752/50048]	Loss: 0.1257
Training Epoch: 81 [42880/50048]	Loss: 0.1272
Training Epoch: 81 [43008/50048]	Loss: 0.0784
Training Epoch: 81 [43136/50048]	Loss: 0.0747
Training Epoch: 81 [43264/50048]	Loss: 0.1747
Training Epoch: 81 [43392/50048]	Loss: 0.1848
Training Epoch: 81 [43520/50048]	Loss: 0.1140
Training Epoch: 81 [43648/50048]	Loss: 0.1791
Training Epoch: 81 [43776/50048]	Loss: 0.0828
Training Epoch: 81 [43904/50048]	Loss: 0.1174
Training Epoch: 81 [44032/50048]	Loss: 0.1188
Training Epoch: 81 [44160/50048]	Loss: 0.1445
Training Epoch: 81 [44288/50048]	Loss: 0.1353
Training Epoch: 81 [44416/50048]	Loss: 0.0941
Training Epoch: 81 [44544/50048]	Loss: 0.1686
Training Epoch: 81 [44672/50048]	Loss: 0.2232
Training Epoch: 81 [44800/50048]	Loss: 0.1011
Training Epoch: 81 [44928/50048]	Loss: 0.1619
Training Epoch: 81 [45056/50048]	Loss: 0.0748
Training Epoch: 81 [45184/50048]	Loss: 0.1455
Training Epoch: 81 [45312/50048]	Loss: 0.1537
Training Epoch: 81 [45440/50048]	Loss: 0.1446
Training Epoch: 81 [45568/50048]	Loss: 0.1255
Training Epoch: 81 [45696/50048]	Loss: 0.0741
2022-12-06 08:11:08,548 [ZeusDataLoader(train)] train epoch 82 done: time=86.62 energy=10515.34
2022-12-06 08:11:08,550 [ZeusDataLoader(eval)] Epoch 82 begin.
Training Epoch: 81 [45824/50048]	Loss: 0.1079
Training Epoch: 81 [45952/50048]	Loss: 0.1376
Training Epoch: 81 [46080/50048]	Loss: 0.1335
Training Epoch: 81 [46208/50048]	Loss: 0.1362
Training Epoch: 81 [46336/50048]	Loss: 0.1222
Training Epoch: 81 [46464/50048]	Loss: 0.1658
Training Epoch: 81 [46592/50048]	Loss: 0.0814
Training Epoch: 81 [46720/50048]	Loss: 0.1093
Training Epoch: 81 [46848/50048]	Loss: 0.0856
Training Epoch: 81 [46976/50048]	Loss: 0.1362
Training Epoch: 81 [47104/50048]	Loss: 0.0734
Training Epoch: 81 [47232/50048]	Loss: 0.0912
Training Epoch: 81 [47360/50048]	Loss: 0.2584
Training Epoch: 81 [47488/50048]	Loss: 0.1794
Training Epoch: 81 [47616/50048]	Loss: 0.1814
Training Epoch: 81 [47744/50048]	Loss: 0.1452
Training Epoch: 81 [47872/50048]	Loss: 0.0703
Training Epoch: 81 [48000/50048]	Loss: 0.0770
Training Epoch: 81 [48128/50048]	Loss: 0.1425
Training Epoch: 81 [48256/50048]	Loss: 0.1184
Training Epoch: 81 [48384/50048]	Loss: 0.1240
Training Epoch: 81 [48512/50048]	Loss: 0.0541
Training Epoch: 81 [48640/50048]	Loss: 0.1667
Training Epoch: 81 [48768/50048]	Loss: 0.0928
Training Epoch: 81 [48896/50048]	Loss: 0.0935
Training Epoch: 81 [49024/50048]	Loss: 0.0885
Training Epoch: 81 [49152/50048]	Loss: 0.1562
Training Epoch: 81 [49280/50048]	Loss: 0.0760
Training Epoch: 81 [49408/50048]	Loss: 0.1020
Training Epoch: 81 [49536/50048]	Loss: 0.1084
Training Epoch: 81 [49664/50048]	Loss: 0.0408
Training Epoch: 81 [49792/50048]	Loss: 0.1231
Training Epoch: 81 [49920/50048]	Loss: 0.1250
Training Epoch: 81 [50048/50048]	Loss: 0.1225
2022-12-06 13:11:12.258 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 08:11:12,313 [ZeusDataLoader(eval)] eval epoch 82 done: time=3.75 energy=453.28
2022-12-06 08:11:12,313 [ZeusDataLoader(train)] Up to epoch 82: time=7396.85, energy=897870.88, cost=1096159.53
2022-12-06 08:11:12,313 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 08:11:12,314 [ZeusDataLoader(train)] Expected next epoch: time=7486.65, energy=908668.89, cost=1109415.92
2022-12-06 08:11:12,314 [ZeusDataLoader(train)] Epoch 83 begin.
Validation Epoch: 81, Average loss: 0.0179, Accuracy: 0.6368
2022-12-06 08:11:12,496 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:11:12,497 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:11:12.499 [ZeusMonitor] Monitor started.
2022-12-06 13:11:12.499 [ZeusMonitor] Running indefinitely. 2022-12-06 13:11:12.500 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:11:12.500 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e83+gpu0.power.log
Training Epoch: 82 [128/50048]	Loss: 0.0962
Training Epoch: 82 [256/50048]	Loss: 0.1474
Training Epoch: 82 [384/50048]	Loss: 0.1992
Training Epoch: 82 [512/50048]	Loss: 0.0495
Training Epoch: 82 [640/50048]	Loss: 0.1319
Training Epoch: 82 [768/50048]	Loss: 0.0965
Training Epoch: 82 [896/50048]	Loss: 0.0899
Training Epoch: 82 [1024/50048]	Loss: 0.0723
Training Epoch: 82 [1152/50048]	Loss: 0.1259
Training Epoch: 82 [1280/50048]	Loss: 0.1173
Training Epoch: 82 [1408/50048]	Loss: 0.1346
Training Epoch: 82 [1536/50048]	Loss: 0.0656
Training Epoch: 82 [1664/50048]	Loss: 0.1091
Training Epoch: 82 [1792/50048]	Loss: 0.1901
Training Epoch: 82 [1920/50048]	Loss: 0.0685
Training Epoch: 82 [2048/50048]	Loss: 0.0654
Training Epoch: 82 [2176/50048]	Loss: 0.0631
Training Epoch: 82 [2304/50048]	Loss: 0.1338
Training Epoch: 82 [2432/50048]	Loss: 0.1347
Training Epoch: 82 [2560/50048]	Loss: 0.1014
Training Epoch: 82 [2688/50048]	Loss: 0.0751
Training Epoch: 82 [2816/50048]	Loss: 0.1153
Training Epoch: 82 [2944/50048]	Loss: 0.1077
Training Epoch: 82 [3072/50048]	Loss: 0.0609
Training Epoch: 82 [3200/50048]	Loss: 0.1183
Training Epoch: 82 [3328/50048]	Loss: 0.0897
Training Epoch: 82 [3456/50048]	Loss: 0.0629
Training Epoch: 82 [3584/50048]	Loss: 0.0690
Training Epoch: 82 [3712/50048]	Loss: 0.0766
Training Epoch: 82 [3840/50048]	Loss: 0.0825
Training Epoch: 82 [3968/50048]	Loss: 0.0595
Training Epoch: 82 [4096/50048]	Loss: 0.0849
Training Epoch: 82 [4224/50048]	Loss: 0.1833
Training Epoch: 82 [4352/50048]	Loss: 0.0985
Training Epoch: 82 [4480/50048]	Loss: 0.1299
Training Epoch: 82 [4608/50048]	Loss: 0.0998
Training Epoch: 82 [4736/50048]	Loss: 0.0748
Training Epoch: 82 [4864/50048]	Loss: 0.1213
Training Epoch: 82 [4992/50048]	Loss: 0.1302
Training Epoch: 82 [5120/50048]	Loss: 0.0586
Training Epoch: 82 [5248/50048]	Loss: 0.1101
Training Epoch: 82 [5376/50048]	Loss: 0.0378
Training Epoch: 82 [5504/50048]	Loss: 0.0925
Training Epoch: 82 [5632/50048]	Loss: 0.0986
Training Epoch: 82 [5760/50048]	Loss: 0.0485
Training Epoch: 82 [5888/50048]	Loss: 0.0566
Training Epoch: 82 [6016/50048]	Loss: 0.1438
Training Epoch: 82 [6144/50048]	Loss: 0.0748
Training Epoch: 82 [6272/50048]	Loss: 0.1520
Training Epoch: 82 [6400/50048]	Loss: 0.1311
Training Epoch: 82 [6528/50048]	Loss: 0.1260
Training Epoch: 82 [6656/50048]	Loss: 0.1440
Training Epoch: 82 [6784/50048]	Loss: 0.1288
Training Epoch: 82 [6912/50048]	Loss: 0.0903
Training Epoch: 82 [7040/50048]	Loss: 0.1250
Training Epoch: 82 [7168/50048]	Loss: 0.1850
Training Epoch: 82 [7296/50048]	Loss: 0.1234
Training Epoch: 82 [7424/50048]	Loss: 0.0879
Training Epoch: 82 [7552/50048]	Loss: 0.0650
Training Epoch: 82 [7680/50048]	Loss: 0.1212
Training Epoch: 82 [7808/50048]	Loss: 0.0706
Training Epoch: 82 [7936/50048]	Loss: 0.0704
Training Epoch: 82 [8064/50048]	Loss: 0.0673
Training Epoch: 82 [8192/50048]	Loss: 0.1325
Training Epoch: 82 [8320/50048]	Loss: 0.1332
Training Epoch: 82 [8448/50048]	Loss: 0.1002
Training Epoch: 82 [8576/50048]	Loss: 0.0929
Training Epoch: 82 [8704/50048]	Loss: 0.0631
Training Epoch: 82 [8832/50048]	Loss: 0.1121
Training Epoch: 82 [8960/50048]	Loss: 0.1129
Training Epoch: 82 [9088/50048]	Loss: 0.1250
Training Epoch: 82 [9216/50048]	Loss: 0.0846
Training Epoch: 82 [9344/50048]	Loss: 0.1346
Training Epoch: 82 [9472/50048]	Loss: 0.0657
Training Epoch: 82 [9600/50048]	Loss: 0.1353
Training Epoch: 82 [9728/50048]	Loss: 0.0661
Training Epoch: 82 [9856/50048]	Loss: 0.0628
Training Epoch: 82 [9984/50048]	Loss: 0.1011
Training Epoch: 82 [10112/50048]	Loss: 0.1062
Training Epoch: 82 [10240/50048]	Loss: 0.0704
Training Epoch: 82 [10368/50048]	Loss: 0.0925
Training Epoch: 82 [10496/50048]	Loss: 0.1057
Training Epoch: 82 [10624/50048]	Loss: 0.1589
Training Epoch: 82 [10752/50048]	Loss: 0.0822
Training Epoch: 82 [10880/50048]	Loss: 0.0387
Training Epoch: 82 [11008/50048]	Loss: 0.0849
Training Epoch: 82 [11136/50048]	Loss: 0.0645
Training Epoch: 82 [11264/50048]	Loss: 0.0966
Training Epoch: 82 [11392/50048]	Loss: 0.0913
Training Epoch: 82 [11520/50048]	Loss: 0.1683
Training Epoch: 82 [11648/50048]	Loss: 0.1126
Training Epoch: 82 [11776/50048]	Loss: 0.0902
Training Epoch: 82 [11904/50048]	Loss: 0.0830
Training Epoch: 82 [12032/50048]	Loss: 0.0893
Training Epoch: 82 [12160/50048]	Loss: 0.0836
Training Epoch: 82 [12288/50048]	Loss: 0.2372
Training Epoch: 82 [12416/50048]	Loss: 0.0833
Training Epoch: 82 [12544/50048]	Loss: 0.0593
Training Epoch: 82 [12672/50048]	Loss: 0.0513
Training Epoch: 82 [12800/50048]	Loss: 0.1013
Training Epoch: 82 [12928/50048]	Loss: 0.0704
Training Epoch: 82 [13056/50048]	Loss: 0.1384
Training Epoch: 82 [13184/50048]	Loss: 0.0957
Training Epoch: 82 [13312/50048]	Loss: 0.1480
Training Epoch: 82 [13440/50048]	Loss: 0.0753
Training Epoch: 82 [13568/50048]	Loss: 0.0446
Training Epoch: 82 [13696/50048]	Loss: 0.1296
Training Epoch: 82 [13824/50048]	Loss: 0.1536
Training Epoch: 82 [13952/50048]	Loss: 0.1317
Training Epoch: 82 [14080/50048]	Loss: 0.1227
Training Epoch: 82 [14208/50048]	Loss: 0.1488
Training Epoch: 82 [14336/50048]	Loss: 0.1392
Training Epoch: 82 [14464/50048]	Loss: 0.0682
Training Epoch: 82 [14592/50048]	Loss: 0.1283
Training Epoch: 82 [14720/50048]	Loss: 0.0405
Training Epoch: 82 [14848/50048]	Loss: 0.1373
Training Epoch: 82 [14976/50048]	Loss: 0.1424
Training Epoch: 82 [15104/50048]	Loss: 0.1631
Training Epoch: 82 [15232/50048]	Loss: 0.1095
Training Epoch: 82 [15360/50048]	Loss: 0.0977
Training Epoch: 82 [15488/50048]	Loss: 0.0969
Training Epoch: 82 [15616/50048]	Loss: 0.0750
Training Epoch: 82 [15744/50048]	Loss: 0.0894
Training Epoch: 82 [15872/50048]	Loss: 0.0958
Training Epoch: 82 [16000/50048]	Loss: 0.1629
Training Epoch: 82 [16128/50048]	Loss: 0.1606
Training Epoch: 82 [16256/50048]	Loss: 0.0493
Training Epoch: 82 [16384/50048]	Loss: 0.0757
Training Epoch: 82 [16512/50048]	Loss: 0.1350
Training Epoch: 82 [16640/50048]	Loss: 0.0745
Training Epoch: 82 [16768/50048]	Loss: 0.0994
Training Epoch: 82 [16896/50048]	Loss: 0.1366
Training Epoch: 82 [17024/50048]	Loss: 0.1000
Training Epoch: 82 [17152/50048]	Loss: 0.1294
Training Epoch: 82 [17280/50048]	Loss: 0.0742
Training Epoch: 82 [17408/50048]	Loss: 0.0994
Training Epoch: 82 [17536/50048]	Loss: 0.1167
Training Epoch: 82 [17664/50048]	Loss: 0.1152
Training Epoch: 82 [17792/50048]	Loss: 0.1528
Training Epoch: 82 [17920/50048]	Loss: 0.0633
Training Epoch: 82 [18048/50048]	Loss: 0.1470
Training Epoch: 82 [18176/50048]	Loss: 0.1566
Training Epoch: 82 [18304/50048]	Loss: 0.0682
Training Epoch: 82 [18432/50048]	Loss: 0.0817
Training Epoch: 82 [18560/50048]	Loss: 0.1104
Training Epoch: 82 [18688/50048]	Loss: 0.1361
Training Epoch: 82 [18816/50048]	Loss: 0.0433
Training Epoch: 82 [18944/50048]	Loss: 0.1040
Training Epoch: 82 [19072/50048]	Loss: 0.0967
Training Epoch: 82 [19200/50048]	Loss: 0.1610
Training Epoch: 82 [19328/50048]	Loss: 0.0647
Training Epoch: 82 [19456/50048]	Loss: 0.1059
Training Epoch: 82 [19584/50048]	Loss: 0.1495
Training Epoch: 82 [19712/50048]	Loss: 0.0494
Training Epoch: 82 [19840/50048]	Loss: 0.0790
Training Epoch: 82 [19968/50048]	Loss: 0.1347
Training Epoch: 82 [20096/50048]	Loss: 0.0828
Training Epoch: 82 [20224/50048]	Loss: 0.1423
Training Epoch: 82 [20352/50048]	Loss: 0.1324
Training Epoch: 82 [20480/50048]	Loss: 0.0975
Training Epoch: 82 [20608/50048]	Loss: 0.0813
Training Epoch: 82 [20736/50048]	Loss: 0.1223
Training Epoch: 82 [20864/50048]	Loss: 0.0695
Training Epoch: 82 [20992/50048]	Loss: 0.0998
Training Epoch: 82 [21120/50048]	Loss: 0.0453
Training Epoch: 82 [21248/50048]	Loss: 0.0902
Training Epoch: 82 [21376/50048]	Loss: 0.0925
Training Epoch: 82 [21504/50048]	Loss: 0.0884
Training Epoch: 82 [21632/50048]	Loss: 0.1571
Training Epoch: 82 [21760/50048]	Loss: 0.1296
Training Epoch: 82 [21888/50048]	Loss: 0.1383
Training Epoch: 82 [22016/50048]	Loss: 0.1320
Training Epoch: 82 [22144/50048]	Loss: 0.0497
Training Epoch: 82 [22272/50048]	Loss: 0.0939
Training Epoch: 82 [22400/50048]	Loss: 0.1063
Training Epoch: 82 [22528/50048]	Loss: 0.1422
Training Epoch: 82 [22656/50048]	Loss: 0.1007
Training Epoch: 82 [22784/50048]	Loss: 0.0761
Training Epoch: 82 [22912/50048]	Loss: 0.1189
Training Epoch: 82 [23040/50048]	Loss: 0.0985
Training Epoch: 82 [23168/50048]	Loss: 0.1089
Training Epoch: 82 [23296/50048]	Loss: 0.0905
Training Epoch: 82 [23424/50048]	Loss: 0.1225
Training Epoch: 82 [23552/50048]	Loss: 0.0323
Training Epoch: 82 [23680/50048]	Loss: 0.2048
Training Epoch: 82 [23808/50048]	Loss: 0.1000
Training Epoch: 82 [23936/50048]	Loss: 0.0431
Training Epoch: 82 [24064/50048]	Loss: 0.1231
Training Epoch: 82 [24192/50048]	Loss: 0.0537
Training Epoch: 82 [24320/50048]	Loss: 0.0564
Training Epoch: 82 [24448/50048]	Loss: 0.0744
Training Epoch: 82 [24576/50048]	Loss: 0.0720
Training Epoch: 82 [24704/50048]	Loss: 0.1654
Training Epoch: 82 [24832/50048]	Loss: 0.1020
Training Epoch: 82 [24960/50048]	Loss: 0.1030
Training Epoch: 82 [25088/50048]	Loss: 0.0746
Training Epoch: 82 [25216/50048]	Loss: 0.0856
Training Epoch: 82 [25344/50048]	Loss: 0.1197
Training Epoch: 82 [25472/50048]	Loss: 0.0898
Training Epoch: 82 [25600/50048]	Loss: 0.1881
Training Epoch: 82 [25728/50048]	Loss: 0.0654
Training Epoch: 82 [25856/50048]	Loss: 0.1058
Training Epoch: 82 [25984/50048]	Loss: 0.0524
Training Epoch: 82 [26112/50048]	Loss: 0.1122
Training Epoch: 82 [26240/50048]	Loss: 0.1099
Training Epoch: 82 [26368/50048]	Loss: 0.0647
Training Epoch: 82 [26496/50048]	Loss: 0.0719
Training Epoch: 82 [26624/50048]	Loss: 0.1085
Training Epoch: 82 [26752/50048]	Loss: 0.0615
Training Epoch: 82 [26880/50048]	Loss: 0.2004
Training Epoch: 82 [27008/50048]	Loss: 0.1313
Training Epoch: 82 [27136/50048]	Loss: 0.1357
Training Epoch: 82 [27264/50048]	Loss: 0.0518
Training Epoch: 82 [27392/50048]	Loss: 0.1254
Training Epoch: 82 [27520/50048]	Loss: 0.1547
Training Epoch: 82 [27648/50048]	Loss: 0.0661
Training Epoch: 82 [27776/50048]	Loss: 0.1645
Training Epoch: 82 [27904/50048]	Loss: 0.1395
Training Epoch: 82 [28032/50048]	Loss: 0.1333
Training Epoch: 82 [28160/50048]	Loss: 0.0788
Training Epoch: 82 [28288/50048]	Loss: 0.0675
Training Epoch: 82 [28416/50048]	Loss: 0.1456
Training Epoch: 82 [28544/50048]	Loss: 0.1101
Training Epoch: 82 [28672/50048]	Loss: 0.0566
Training Epoch: 82 [28800/50048]	Loss: 0.1174
Training Epoch: 82 [28928/50048]	Loss: 0.1545
Training Epoch: 82 [29056/50048]	Loss: 0.1082
Training Epoch: 82 [29184/50048]	Loss: 0.0842
Training Epoch: 82 [29312/50048]	Loss: 0.0512
Training Epoch: 82 [29440/50048]	Loss: 0.1306
Training Epoch: 82 [29568/50048]	Loss: 0.0786
Training Epoch: 82 [29696/50048]	Loss: 0.1436
Training Epoch: 82 [29824/50048]	Loss: 0.0959
Training Epoch: 82 [29952/50048]	Loss: 0.1289
Training Epoch: 82 [30080/50048]	Loss: 0.0504
Training Epoch: 82 [30208/50048]	Loss: 0.0971
Training Epoch: 82 [30336/50048]	Loss: 0.1011
Training Epoch: 82 [30464/50048]	Loss: 0.0507
Training Epoch: 82 [30592/50048]	Loss: 0.0839
Training Epoch: 82 [30720/50048]	Loss: 0.1796
Training Epoch: 82 [30848/50048]	Loss: 0.0962
Training Epoch: 82 [30976/50048]	Loss: 0.1086
Training Epoch: 82 [31104/50048]	Loss: 0.1248
Training Epoch: 82 [31232/50048]	Loss: 0.1082
Training Epoch: 82 [31360/50048]	Loss: 0.1932
Training Epoch: 82 [31488/50048]	Loss: 0.0922
Training Epoch: 82 [31616/50048]	Loss: 0.0792
Training Epoch: 82 [31744/50048]	Loss: 0.1920
Training Epoch: 82 [31872/50048]	Loss: 0.0857
Training Epoch: 82 [32000/50048]	Loss: 0.0542
Training Epoch: 82 [32128/50048]	Loss: 0.0535
Training Epoch: 82 [32256/50048]	Loss: 0.0492
Training Epoch: 82 [32384/50048]	Loss: 0.1537
Training Epoch: 82 [32512/50048]	Loss: 0.0800
Training Epoch: 82 [32640/50048]	Loss: 0.0478
Training Epoch: 82 [32768/50048]	Loss: 0.1149
Training Epoch: 82 [32896/50048]	Loss: 0.0397
Training Epoch: 82 [33024/50048]	Loss: 0.0821
Training Epoch: 82 [33152/50048]	Loss: 0.1418
Training Epoch: 82 [33280/50048]	Loss: 0.0784
Training Epoch: 82 [33408/50048]	Loss: 0.1614
Training Epoch: 82 [33536/50048]	Loss: 0.1211
Training Epoch: 82 [33664/50048]	Loss: 0.1112
Training Epoch: 82 [33792/50048]	Loss: 0.0979
Training Epoch: 82 [33920/50048]	Loss: 0.1658
Training Epoch: 82 [34048/50048]	Loss: 0.1070
Training Epoch: 82 [34176/50048]	Loss: 0.0590
Training Epoch: 82 [34304/50048]	Loss: 0.0654
Training Epoch: 82 [34432/50048]	Loss: 0.1684
Training Epoch: 82 [34560/50048]	Loss: 0.0513
Training Epoch: 82 [34688/50048]	Loss: 0.1082
Training Epoch: 82 [34816/50048]	Loss: 0.1103
Training Epoch: 82 [34944/50048]	Loss: 0.0766
Training Epoch: 82 [35072/50048]	Loss: 0.0536
Training Epoch: 82 [35200/50048]	Loss: 0.1126
Training Epoch: 82 [35328/50048]	Loss: 0.0781
Training Epoch: 82 [35456/50048]	Loss: 0.1691
Training Epoch: 82 [35584/50048]	Loss: 0.1382
Training Epoch: 82 [35712/50048]	Loss: 0.1586
Training Epoch: 82 [35840/50048]	Loss: 0.1039
Training Epoch: 82 [35968/50048]	Loss: 0.1158
Training Epoch: 82 [36096/50048]	Loss: 0.0952
Training Epoch: 82 [36224/50048]	Loss: 0.1327
Training Epoch: 82 [36352/50048]	Loss: 0.0630
Training Epoch: 82 [36480/50048]	Loss: 0.1328
Training Epoch: 82 [36608/50048]	Loss: 0.0954
Training Epoch: 82 [36736/50048]	Loss: 0.1885
Training Epoch: 82 [36864/50048]	Loss: 0.1085
Training Epoch: 82 [36992/50048]	Loss: 0.1764
Training Epoch: 82 [37120/50048]	Loss: 0.1521
Training Epoch: 82 [37248/50048]	Loss: 0.1156
Training Epoch: 82 [37376/50048]	Loss: 0.0620
Training Epoch: 82 [37504/50048]	Loss: 0.0820
Training Epoch: 82 [37632/50048]	Loss: 0.0672
Training Epoch: 82 [37760/50048]	Loss: 0.0694
Training Epoch: 82 [37888/50048]	Loss: 0.0475
Training Epoch: 82 [38016/50048]	Loss: 0.1110
Training Epoch: 82 [38144/50048]	Loss: 0.1177
Training Epoch: 82 [38272/50048]	Loss: 0.1021
Training Epoch: 82 [38400/50048]	Loss: 0.1017
Training Epoch: 82 [38528/50048]	Loss: 0.0908
Training Epoch: 82 [38656/50048]	Loss: 0.1513
Training Epoch: 82 [38784/50048]	Loss: 0.1843
Training Epoch: 82 [38912/50048]	Loss: 0.1275
Training Epoch: 82 [39040/50048]	Loss: 0.1806
Training Epoch: 82 [39168/50048]	Loss: 0.1435
Training Epoch: 82 [39296/50048]	Loss: 0.0845
Training Epoch: 82 [39424/50048]	Loss: 0.0996
Training Epoch: 82 [39552/50048]	Loss: 0.1808
Training Epoch: 82 [39680/50048]	Loss: 0.1489
Training Epoch: 82 [39808/50048]	Loss: 0.1033
Training Epoch: 82 [39936/50048]	Loss: 0.0723
Training Epoch: 82 [40064/50048]	Loss: 0.1651
Training Epoch: 82 [40192/50048]	Loss: 0.1841
Training Epoch: 82 [40320/50048]	Loss: 0.1471
Training Epoch: 82 [40448/50048]	Loss: 0.1149
Training Epoch: 82 [40576/50048]	Loss: 0.1607
Training Epoch: 82 [40704/50048]	Loss: 0.1879
Training Epoch: 82 [40832/50048]	Loss: 0.2163
Training Epoch: 82 [40960/50048]	Loss: 0.1691
Training Epoch: 82 [41088/50048]	Loss: 0.1256
Training Epoch: 82 [41216/50048]	Loss: 0.1143
Training Epoch: 82 [41344/50048]	Loss: 0.1460
Training Epoch: 82 [41472/50048]	Loss: 0.0931
Training Epoch: 82 [41600/50048]	Loss: 0.0939
Training Epoch: 82 [41728/50048]	Loss: 0.1063
Training Epoch: 82 [41856/50048]	Loss: 0.0887
Training Epoch: 82 [41984/50048]	Loss: 0.0615
Training Epoch: 82 [42112/50048]	Loss: 0.0740
Training Epoch: 82 [42240/50048]	Loss: 0.1383
Training Epoch: 82 [42368/50048]	Loss: 0.1322
Training Epoch: 82 [42496/50048]	Loss: 0.1227
Training Epoch: 82 [42624/50048]	Loss: 0.1364
Training Epoch: 82 [42752/50048]	Loss: 0.1543
Training Epoch: 82 [42880/50048]	Loss: 0.1930
Training Epoch: 82 [43008/50048]	Loss: 0.0703
Training Epoch: 82 [43136/50048]	Loss: 0.1569
Training Epoch: 82 [43264/50048]	Loss: 0.1282
Training Epoch: 82 [43392/50048]	Loss: 0.1118
Training Epoch: 82 [43520/50048]	Loss: 0.1754
Training Epoch: 82 [43648/50048]	Loss: 0.0863
Training Epoch: 82 [43776/50048]	Loss: 0.0829
Training Epoch: 82 [43904/50048]	Loss: 0.1617
Training Epoch: 82 [44032/50048]	Loss: 0.0893
Training Epoch: 82 [44160/50048]	Loss: 0.1010
Training Epoch: 82 [44288/50048]	Loss: 0.2913
Training Epoch: 82 [44416/50048]	Loss: 0.0985
Training Epoch: 82 [44544/50048]	Loss: 0.0741
Training Epoch: 82 [44672/50048]	Loss: 0.1317
Training Epoch: 82 [44800/50048]	Loss: 0.0561
Training Epoch: 82 [44928/50048]	Loss: 0.1467
Training Epoch: 82 [45056/50048]	Loss: 0.1521
Training Epoch: 82 [45184/50048]	Loss: 0.1075
Training Epoch: 82 [45312/50048]	Loss: 0.1289
Training Epoch: 82 [45440/50048]	Loss: 0.1286
Training Epoch: 82 [45568/50048]	Loss: 0.1067
Training Epoch: 82 [45696/50048]	Loss: 0.0975
2022-12-06 08:12:38,764 [ZeusDataLoader(train)] train epoch 83 done: time=86.44 energy=10504.84
2022-12-06 08:12:38,765 [ZeusDataLoader(eval)] Epoch 83 begin.
Training Epoch: 82 [45824/50048]	Loss: 0.0801
Training Epoch: 82 [45952/50048]	Loss: 0.0774
Training Epoch: 82 [46080/50048]	Loss: 0.1420
Training Epoch: 82 [46208/50048]	Loss: 0.2553
Training Epoch: 82 [46336/50048]	Loss: 0.0837
Training Epoch: 82 [46464/50048]	Loss: 0.1115
Training Epoch: 82 [46592/50048]	Loss: 0.1994
Training Epoch: 82 [46720/50048]	Loss: 0.0813
Training Epoch: 82 [46848/50048]	Loss: 0.0734
Training Epoch: 82 [46976/50048]	Loss: 0.1583
Training Epoch: 82 [47104/50048]	Loss: 0.1498
Training Epoch: 82 [47232/50048]	Loss: 0.1240
Training Epoch: 82 [47360/50048]	Loss: 0.0741
Training Epoch: 82 [47488/50048]	Loss: 0.1274
Training Epoch: 82 [47616/50048]	Loss: 0.1011
Training Epoch: 82 [47744/50048]	Loss: 0.1116
Training Epoch: 82 [47872/50048]	Loss: 0.1326
Training Epoch: 82 [48000/50048]	Loss: 0.1264
Training Epoch: 82 [48128/50048]	Loss: 0.2201
Training Epoch: 82 [48256/50048]	Loss: 0.1179
Training Epoch: 82 [48384/50048]	Loss: 0.0952
Training Epoch: 82 [48512/50048]	Loss: 0.1600
Training Epoch: 82 [48640/50048]	Loss: 0.1482
Training Epoch: 82 [48768/50048]	Loss: 0.0574
Training Epoch: 82 [48896/50048]	Loss: 0.1395
Training Epoch: 82 [49024/50048]	Loss: 0.1048
Training Epoch: 82 [49152/50048]	Loss: 0.1201
Training Epoch: 82 [49280/50048]	Loss: 0.0890
Training Epoch: 82 [49408/50048]	Loss: 0.2211
Training Epoch: 82 [49536/50048]	Loss: 0.2087
Training Epoch: 82 [49664/50048]	Loss: 0.0827
Training Epoch: 82 [49792/50048]	Loss: 0.1306
Training Epoch: 82 [49920/50048]	Loss: 0.0801
Training Epoch: 82 [50048/50048]	Loss: 0.1542
2022-12-06 13:12:42.434 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 08:12:42,452 [ZeusDataLoader(eval)] eval epoch 83 done: time=3.68 energy=440.79
2022-12-06 08:12:42,452 [ZeusDataLoader(train)] Up to epoch 83: time=7486.96, energy=908816.52, cost=1109517.57
2022-12-06 08:12:42,452 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 08:12:42,452 [ZeusDataLoader(train)] Expected next epoch: time=7576.76, energy=919614.53, cost=1122773.96
2022-12-06 08:12:42,453 [ZeusDataLoader(train)] Epoch 84 begin.
Validation Epoch: 82, Average loss: 0.0183, Accuracy: 0.6389
2022-12-06 08:12:42,641 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:12:42,641 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:12:42.645 [ZeusMonitor] Monitor started.
2022-12-06 13:12:42.645 [ZeusMonitor] Running indefinitely. 2022-12-06 13:12:42.645 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:12:42.645 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e84+gpu0.power.log
Training Epoch: 83 [128/50048]	Loss: 0.1942
Training Epoch: 83 [256/50048]	Loss: 0.1311
Training Epoch: 83 [384/50048]	Loss: 0.1014
Training Epoch: 83 [512/50048]	Loss: 0.1170
Training Epoch: 83 [640/50048]	Loss: 0.1067
Training Epoch: 83 [768/50048]	Loss: 0.1267
Training Epoch: 83 [896/50048]	Loss: 0.0910
Training Epoch: 83 [1024/50048]	Loss: 0.0544
Training Epoch: 83 [1152/50048]	Loss: 0.1183
Training Epoch: 83 [1280/50048]	Loss: 0.0547
Training Epoch: 83 [1408/50048]	Loss: 0.0523
Training Epoch: 83 [1536/50048]	Loss: 0.0871
Training Epoch: 83 [1664/50048]	Loss: 0.0316
Training Epoch: 83 [1792/50048]	Loss: 0.0948
Training Epoch: 83 [1920/50048]	Loss: 0.0575
Training Epoch: 83 [2048/50048]	Loss: 0.1371
Training Epoch: 83 [2176/50048]	Loss: 0.0988
Training Epoch: 83 [2304/50048]	Loss: 0.0496
Training Epoch: 83 [2432/50048]	Loss: 0.0712
Training Epoch: 83 [2560/50048]	Loss: 0.1353
Training Epoch: 83 [2688/50048]	Loss: 0.0786
Training Epoch: 83 [2816/50048]	Loss: 0.1118
Training Epoch: 83 [2944/50048]	Loss: 0.0914
Training Epoch: 83 [3072/50048]	Loss: 0.1105
Training Epoch: 83 [3200/50048]	Loss: 0.1408
Training Epoch: 83 [3328/50048]	Loss: 0.0551
Training Epoch: 83 [3456/50048]	Loss: 0.1436
Training Epoch: 83 [3584/50048]	Loss: 0.1018
Training Epoch: 83 [3712/50048]	Loss: 0.1312
Training Epoch: 83 [3840/50048]	Loss: 0.0656
Training Epoch: 83 [3968/50048]	Loss: 0.1028
Training Epoch: 83 [4096/50048]	Loss: 0.1566
Training Epoch: 83 [4224/50048]	Loss: 0.1140
Training Epoch: 83 [4352/50048]	Loss: 0.0979
Training Epoch: 83 [4480/50048]	Loss: 0.0868
Training Epoch: 83 [4608/50048]	Loss: 0.1154
Training Epoch: 83 [4736/50048]	Loss: 0.0619
Training Epoch: 83 [4864/50048]	Loss: 0.0965
Training Epoch: 83 [4992/50048]	Loss: 0.1083
Training Epoch: 83 [5120/50048]	Loss: 0.0607
Training Epoch: 83 [5248/50048]	Loss: 0.1055
Training Epoch: 83 [5376/50048]	Loss: 0.0733
Training Epoch: 83 [5504/50048]	Loss: 0.1205
Training Epoch: 83 [5632/50048]	Loss: 0.1675
Training Epoch: 83 [5760/50048]	Loss: 0.1455
Training Epoch: 83 [5888/50048]	Loss: 0.0699
Training Epoch: 83 [6016/50048]	Loss: 0.1914
Training Epoch: 83 [6144/50048]	Loss: 0.0802
Training Epoch: 83 [6272/50048]	Loss: 0.1310
Training Epoch: 83 [6400/50048]	Loss: 0.0829
Training Epoch: 83 [6528/50048]	Loss: 0.0330
Training Epoch: 83 [6656/50048]	Loss: 0.1543
Training Epoch: 83 [6784/50048]	Loss: 0.0843
Training Epoch: 83 [6912/50048]	Loss: 0.1068
Training Epoch: 83 [7040/50048]	Loss: 0.2074
Training Epoch: 83 [7168/50048]	Loss: 0.0788
Training Epoch: 83 [7296/50048]	Loss: 0.0846
Training Epoch: 83 [7424/50048]	Loss: 0.0543
Training Epoch: 83 [7552/50048]	Loss: 0.0940
Training Epoch: 83 [7680/50048]	Loss: 0.0590
Training Epoch: 83 [7808/50048]	Loss: 0.0858
Training Epoch: 83 [7936/50048]	Loss: 0.0994
Training Epoch: 83 [8064/50048]	Loss: 0.1895
Training Epoch: 83 [8192/50048]	Loss: 0.1152
Training Epoch: 83 [8320/50048]	Loss: 0.0974
Training Epoch: 83 [8448/50048]	Loss: 0.1347
Training Epoch: 83 [8576/50048]	Loss: 0.1145
Training Epoch: 83 [8704/50048]	Loss: 0.0410
Training Epoch: 83 [8832/50048]	Loss: 0.0853
Training Epoch: 83 [8960/50048]	Loss: 0.1061
Training Epoch: 83 [9088/50048]	Loss: 0.0607
Training Epoch: 83 [9216/50048]	Loss: 0.0720
Training Epoch: 83 [9344/50048]	Loss: 0.0727
Training Epoch: 83 [9472/50048]	Loss: 0.0531
Training Epoch: 83 [9600/50048]	Loss: 0.0920
Training Epoch: 83 [9728/50048]	Loss: 0.0959
Training Epoch: 83 [9856/50048]	Loss: 0.0500
Training Epoch: 83 [9984/50048]	Loss: 0.0934
Training Epoch: 83 [10112/50048]	Loss: 0.0655
Training Epoch: 83 [10240/50048]	Loss: 0.0641
Training Epoch: 83 [10368/50048]	Loss: 0.1073
Training Epoch: 83 [10496/50048]	Loss: 0.1159
Training Epoch: 83 [10624/50048]	Loss: 0.0776
Training Epoch: 83 [10752/50048]	Loss: 0.1034
Training Epoch: 83 [10880/50048]	Loss: 0.1407
Training Epoch: 83 [11008/50048]	Loss: 0.1023
Training Epoch: 83 [11136/50048]	Loss: 0.1039
Training Epoch: 83 [11264/50048]	Loss: 0.0735
Training Epoch: 83 [11392/50048]	Loss: 0.1073
Training Epoch: 83 [11520/50048]	Loss: 0.1217
Training Epoch: 83 [11648/50048]	Loss: 0.0412
Training Epoch: 83 [11776/50048]	Loss: 0.0797
Training Epoch: 83 [11904/50048]	Loss: 0.1091
Training Epoch: 83 [12032/50048]	Loss: 0.0503
Training Epoch: 83 [12160/50048]	Loss: 0.1165
Training Epoch: 83 [12288/50048]	Loss: 0.0556
Training Epoch: 83 [12416/50048]	Loss: 0.1019
Training Epoch: 83 [12544/50048]	Loss: 0.0372
Training Epoch: 83 [12672/50048]	Loss: 0.0882
Training Epoch: 83 [12800/50048]	Loss: 0.0858
Training Epoch: 83 [12928/50048]	Loss: 0.0811
Training Epoch: 83 [13056/50048]	Loss: 0.0985
Training Epoch: 83 [13184/50048]	Loss: 0.1046
Training Epoch: 83 [13312/50048]	Loss: 0.0684
Training Epoch: 83 [13440/50048]	Loss: 0.0429
Training Epoch: 83 [13568/50048]	Loss: 0.0485
Training Epoch: 83 [13696/50048]	Loss: 0.0878
Training Epoch: 83 [13824/50048]	Loss: 0.0844
Training Epoch: 83 [13952/50048]	Loss: 0.0745
Training Epoch: 83 [14080/50048]	Loss: 0.0927
Training Epoch: 83 [14208/50048]	Loss: 0.1215
Training Epoch: 83 [14336/50048]	Loss: 0.0513
Training Epoch: 83 [14464/50048]	Loss: 0.0702
Training Epoch: 83 [14592/50048]	Loss: 0.1261
Training Epoch: 83 [14720/50048]	Loss: 0.0780
Training Epoch: 83 [14848/50048]	Loss: 0.1175
Training Epoch: 83 [14976/50048]	Loss: 0.0583
Training Epoch: 83 [15104/50048]	Loss: 0.0757
Training Epoch: 83 [15232/50048]	Loss: 0.1047
Training Epoch: 83 [15360/50048]	Loss: 0.0817
Training Epoch: 83 [15488/50048]	Loss: 0.0825
Training Epoch: 83 [15616/50048]	Loss: 0.1206
Training Epoch: 83 [15744/50048]	Loss: 0.0829
Training Epoch: 83 [15872/50048]	Loss: 0.0674
Training Epoch: 83 [16000/50048]	Loss: 0.1144
Training Epoch: 83 [16128/50048]	Loss: 0.0948
Training Epoch: 83 [16256/50048]	Loss: 0.0355
Training Epoch: 83 [16384/50048]	Loss: 0.1310
Training Epoch: 83 [16512/50048]	Loss: 0.0515
Training Epoch: 83 [16640/50048]	Loss: 0.1091
Training Epoch: 83 [16768/50048]	Loss: 0.0676
Training Epoch: 83 [16896/50048]	Loss: 0.0694
Training Epoch: 83 [17024/50048]	Loss: 0.0755
Training Epoch: 83 [17152/50048]	Loss: 0.0768
Training Epoch: 83 [17280/50048]	Loss: 0.0965
Training Epoch: 83 [17408/50048]	Loss: 0.0929
Training Epoch: 83 [17536/50048]	Loss: 0.1451
Training Epoch: 83 [17664/50048]	Loss: 0.1137
Training Epoch: 83 [17792/50048]	Loss: 0.0859
Training Epoch: 83 [17920/50048]	Loss: 0.0547
Training Epoch: 83 [18048/50048]	Loss: 0.0408
Training Epoch: 83 [18176/50048]	Loss: 0.0853
Training Epoch: 83 [18304/50048]	Loss: 0.1182
Training Epoch: 83 [18432/50048]	Loss: 0.1371
Training Epoch: 83 [18560/50048]	Loss: 0.0791
Training Epoch: 83 [18688/50048]	Loss: 0.1915
Training Epoch: 83 [18816/50048]	Loss: 0.0626
Training Epoch: 83 [18944/50048]	Loss: 0.0942
Training Epoch: 83 [19072/50048]	Loss: 0.0559
Training Epoch: 83 [19200/50048]	Loss: 0.1347
Training Epoch: 83 [19328/50048]	Loss: 0.0614
Training Epoch: 83 [19456/50048]	Loss: 0.0952
Training Epoch: 83 [19584/50048]	Loss: 0.1356
Training Epoch: 83 [19712/50048]	Loss: 0.0793
Training Epoch: 83 [19840/50048]	Loss: 0.1327
Training Epoch: 83 [19968/50048]	Loss: 0.0888
Training Epoch: 83 [20096/50048]	Loss: 0.1057
Training Epoch: 83 [20224/50048]	Loss: 0.0793
Training Epoch: 83 [20352/50048]	Loss: 0.1251
Training Epoch: 83 [20480/50048]	Loss: 0.0411
Training Epoch: 83 [20608/50048]	Loss: 0.1927
Training Epoch: 83 [20736/50048]	Loss: 0.1543
Training Epoch: 83 [20864/50048]	Loss: 0.1310
Training Epoch: 83 [20992/50048]	Loss: 0.1596
Training Epoch: 83 [21120/50048]	Loss: 0.0673
Training Epoch: 83 [21248/50048]	Loss: 0.0882
Training Epoch: 83 [21376/50048]	Loss: 0.1160
Training Epoch: 83 [21504/50048]	Loss: 0.0742
Training Epoch: 83 [21632/50048]	Loss: 0.1012
Training Epoch: 83 [21760/50048]	Loss: 0.0860
Training Epoch: 83 [21888/50048]	Loss: 0.0621
Training Epoch: 83 [22016/50048]	Loss: 0.0979
Training Epoch: 83 [22144/50048]	Loss: 0.1006
Training Epoch: 83 [22272/50048]	Loss: 0.0977
Training Epoch: 83 [22400/50048]	Loss: 0.1419
Training Epoch: 83 [22528/50048]	Loss: 0.0731
Training Epoch: 83 [22656/50048]	Loss: 0.0524
Training Epoch: 83 [22784/50048]	Loss: 0.0461
Training Epoch: 83 [22912/50048]	Loss: 0.0570
Training Epoch: 83 [23040/50048]	Loss: 0.0443
Training Epoch: 83 [23168/50048]	Loss: 0.0987
Training Epoch: 83 [23296/50048]	Loss: 0.1054
Training Epoch: 83 [23424/50048]	Loss: 0.1379
Training Epoch: 83 [23552/50048]	Loss: 0.1211
Training Epoch: 83 [23680/50048]	Loss: 0.1394
Training Epoch: 83 [23808/50048]	Loss: 0.0628
Training Epoch: 83 [23936/50048]	Loss: 0.0652
Training Epoch: 83 [24064/50048]	Loss: 0.0573
Training Epoch: 83 [24192/50048]	Loss: 0.0511
Training Epoch: 83 [24320/50048]	Loss: 0.1047
Training Epoch: 83 [24448/50048]	Loss: 0.1260
Training Epoch: 83 [24576/50048]	Loss: 0.0917
Training Epoch: 83 [24704/50048]	Loss: 0.1710
Training Epoch: 83 [24832/50048]	Loss: 0.0881
Training Epoch: 83 [24960/50048]	Loss: 0.1887
Training Epoch: 83 [25088/50048]	Loss: 0.1623
Training Epoch: 83 [25216/50048]	Loss: 0.0820
Training Epoch: 83 [25344/50048]	Loss: 0.1304
Training Epoch: 83 [25472/50048]	Loss: 0.1592
Training Epoch: 83 [25600/50048]	Loss: 0.0603
Training Epoch: 83 [25728/50048]	Loss: 0.0582
Training Epoch: 83 [25856/50048]	Loss: 0.0708
Training Epoch: 83 [25984/50048]	Loss: 0.1071
Training Epoch: 83 [26112/50048]	Loss: 0.0660
Training Epoch: 83 [26240/50048]	Loss: 0.0338
Training Epoch: 83 [26368/50048]	Loss: 0.1459
Training Epoch: 83 [26496/50048]	Loss: 0.1591
Training Epoch: 83 [26624/50048]	Loss: 0.1009
Training Epoch: 83 [26752/50048]	Loss: 0.0661
Training Epoch: 83 [26880/50048]	Loss: 0.1035
Training Epoch: 83 [27008/50048]	Loss: 0.0583
Training Epoch: 83 [27136/50048]	Loss: 0.1033
Training Epoch: 83 [27264/50048]	Loss: 0.1101
Training Epoch: 83 [27392/50048]	Loss: 0.0990
Training Epoch: 83 [27520/50048]	Loss: 0.0969
Training Epoch: 83 [27648/50048]	Loss: 0.0632
Training Epoch: 83 [27776/50048]	Loss: 0.0786
Training Epoch: 83 [27904/50048]	Loss: 0.0543
Training Epoch: 83 [28032/50048]	Loss: 0.0915
Training Epoch: 83 [28160/50048]	Loss: 0.0485
Training Epoch: 83 [28288/50048]	Loss: 0.0569
Training Epoch: 83 [28416/50048]	Loss: 0.0813
Training Epoch: 83 [28544/50048]	Loss: 0.0795
Training Epoch: 83 [28672/50048]	Loss: 0.1292
Training Epoch: 83 [28800/50048]	Loss: 0.0417
Training Epoch: 83 [28928/50048]	Loss: 0.1325
Training Epoch: 83 [29056/50048]	Loss: 0.0966
Training Epoch: 83 [29184/50048]	Loss: 0.0749
Training Epoch: 83 [29312/50048]	Loss: 0.1031
Training Epoch: 83 [29440/50048]	Loss: 0.1121
Training Epoch: 83 [29568/50048]	Loss: 0.0728
Training Epoch: 83 [29696/50048]	Loss: 0.0711
Training Epoch: 83 [29824/50048]	Loss: 0.0921
Training Epoch: 83 [29952/50048]	Loss: 0.1278
Training Epoch: 83 [30080/50048]	Loss: 0.0847
Training Epoch: 83 [30208/50048]	Loss: 0.0802
Training Epoch: 83 [30336/50048]	Loss: 0.1037
Training Epoch: 83 [30464/50048]	Loss: 0.1168
Training Epoch: 83 [30592/50048]	Loss: 0.1226
Training Epoch: 83 [30720/50048]	Loss: 0.0744
Training Epoch: 83 [30848/50048]	Loss: 0.0800
Training Epoch: 83 [30976/50048]	Loss: 0.0866
Training Epoch: 83 [31104/50048]	Loss: 0.1434
Training Epoch: 83 [31232/50048]	Loss: 0.1012
Training Epoch: 83 [31360/50048]	Loss: 0.0753
Training Epoch: 83 [31488/50048]	Loss: 0.0484
Training Epoch: 83 [31616/50048]	Loss: 0.1073
Training Epoch: 83 [31744/50048]	Loss: 0.0736
Training Epoch: 83 [31872/50048]	Loss: 0.1142
Training Epoch: 83 [32000/50048]	Loss: 0.1127
Training Epoch: 83 [32128/50048]	Loss: 0.0653
Training Epoch: 83 [32256/50048]	Loss: 0.0607
Training Epoch: 83 [32384/50048]	Loss: 0.0721
Training Epoch: 83 [32512/50048]	Loss: 0.1864
Training Epoch: 83 [32640/50048]	Loss: 0.0824
Training Epoch: 83 [32768/50048]	Loss: 0.1077
Training Epoch: 83 [32896/50048]	Loss: 0.0553
Training Epoch: 83 [33024/50048]	Loss: 0.1371
Training Epoch: 83 [33152/50048]	Loss: 0.1744
Training Epoch: 83 [33280/50048]	Loss: 0.1862
Training Epoch: 83 [33408/50048]	Loss: 0.0471
Training Epoch: 83 [33536/50048]	Loss: 0.1409
Training Epoch: 83 [33664/50048]	Loss: 0.1055
Training Epoch: 83 [33792/50048]	Loss: 0.0722
Training Epoch: 83 [33920/50048]	Loss: 0.0480
Training Epoch: 83 [34048/50048]	Loss: 0.0375
Training Epoch: 83 [34176/50048]	Loss: 0.0829
Training Epoch: 83 [34304/50048]	Loss: 0.0911
Training Epoch: 83 [34432/50048]	Loss: 0.0957
Training Epoch: 83 [34560/50048]	Loss: 0.1538
Training Epoch: 83 [34688/50048]	Loss: 0.0947
Training Epoch: 83 [34816/50048]	Loss: 0.1049
Training Epoch: 83 [34944/50048]	Loss: 0.1015
Training Epoch: 83 [35072/50048]	Loss: 0.0875
Training Epoch: 83 [35200/50048]	Loss: 0.0481
Training Epoch: 83 [35328/50048]	Loss: 0.0980
Training Epoch: 83 [35456/50048]	Loss: 0.1259
Training Epoch: 83 [35584/50048]	Loss: 0.1097
Training Epoch: 83 [35712/50048]	Loss: 0.0778
Training Epoch: 83 [35840/50048]	Loss: 0.0983
Training Epoch: 83 [35968/50048]	Loss: 0.0656
Training Epoch: 83 [36096/50048]	Loss: 0.0759
Training Epoch: 83 [36224/50048]	Loss: 0.1337
Training Epoch: 83 [36352/50048]	Loss: 0.1069
Training Epoch: 83 [36480/50048]	Loss: 0.0607
Training Epoch: 83 [36608/50048]	Loss: 0.1210
Training Epoch: 83 [36736/50048]	Loss: 0.1078
Training Epoch: 83 [36864/50048]	Loss: 0.1038
Training Epoch: 83 [36992/50048]	Loss: 0.0910
Training Epoch: 83 [37120/50048]	Loss: 0.1164
Training Epoch: 83 [37248/50048]	Loss: 0.0983
Training Epoch: 83 [37376/50048]	Loss: 0.1055
Training Epoch: 83 [37504/50048]	Loss: 0.1454
Training Epoch: 83 [37632/50048]	Loss: 0.0814
Training Epoch: 83 [37760/50048]	Loss: 0.1634
Training Epoch: 83 [37888/50048]	Loss: 0.1343
Training Epoch: 83 [38016/50048]	Loss: 0.1059
Training Epoch: 83 [38144/50048]	Loss: 0.1588
Training Epoch: 83 [38272/50048]	Loss: 0.1786
Training Epoch: 83 [38400/50048]	Loss: 0.0788
Training Epoch: 83 [38528/50048]	Loss: 0.0909
Training Epoch: 83 [38656/50048]	Loss: 0.0776
Training Epoch: 83 [38784/50048]	Loss: 0.1675
Training Epoch: 83 [38912/50048]	Loss: 0.0235
Training Epoch: 83 [39040/50048]	Loss: 0.0828
Training Epoch: 83 [39168/50048]	Loss: 0.1138
Training Epoch: 83 [39296/50048]	Loss: 0.0414
Training Epoch: 83 [39424/50048]	Loss: 0.0741
Training Epoch: 83 [39552/50048]	Loss: 0.0679
Training Epoch: 83 [39680/50048]	Loss: 0.1045
Training Epoch: 83 [39808/50048]	Loss: 0.1199
Training Epoch: 83 [39936/50048]	Loss: 0.0866
Training Epoch: 83 [40064/50048]	Loss: 0.1575
Training Epoch: 83 [40192/50048]	Loss: 0.1078
Training Epoch: 83 [40320/50048]	Loss: 0.1543
Training Epoch: 83 [40448/50048]	Loss: 0.1745
Training Epoch: 83 [40576/50048]	Loss: 0.1831
Training Epoch: 83 [40704/50048]	Loss: 0.1386
Training Epoch: 83 [40832/50048]	Loss: 0.0700
Training Epoch: 83 [40960/50048]	Loss: 0.0837
Training Epoch: 83 [41088/50048]	Loss: 0.0985
Training Epoch: 83 [41216/50048]	Loss: 0.1877
Training Epoch: 83 [41344/50048]	Loss: 0.1260
Training Epoch: 83 [41472/50048]	Loss: 0.0918
Training Epoch: 83 [41600/50048]	Loss: 0.1248
Training Epoch: 83 [41728/50048]	Loss: 0.0525
Training Epoch: 83 [41856/50048]	Loss: 0.1056
Training Epoch: 83 [41984/50048]	Loss: 0.0759
Training Epoch: 83 [42112/50048]	Loss: 0.1510
Training Epoch: 83 [42240/50048]	Loss: 0.0984
Training Epoch: 83 [42368/50048]	Loss: 0.1488
Training Epoch: 83 [42496/50048]	Loss: 0.0820
Training Epoch: 83 [42624/50048]	Loss: 0.0815
Training Epoch: 83 [42752/50048]	Loss: 0.1215
Training Epoch: 83 [42880/50048]	Loss: 0.0959
Training Epoch: 83 [43008/50048]	Loss: 0.1379
Training Epoch: 83 [43136/50048]	Loss: 0.0919
Training Epoch: 83 [43264/50048]	Loss: 0.0580
Training Epoch: 83 [43392/50048]	Loss: 0.1139
Training Epoch: 83 [43520/50048]	Loss: 0.1646
Training Epoch: 83 [43648/50048]	Loss: 0.1434
Training Epoch: 83 [43776/50048]	Loss: 0.1013
Training Epoch: 83 [43904/50048]	Loss: 0.0489
Training Epoch: 83 [44032/50048]	Loss: 0.1010
Training Epoch: 83 [44160/50048]	Loss: 0.0704
Training Epoch: 83 [44288/50048]	Loss: 0.1558
Training Epoch: 83 [44416/50048]	Loss: 0.0608
Training Epoch: 83 [44544/50048]	Loss: 0.1177
Training Epoch: 83 [44672/50048]	Loss: 0.0804
Training Epoch: 83 [44800/50048]	Loss: 0.1385
Training Epoch: 83 [44928/50048]	Loss: 0.1282
Training Epoch: 83 [45056/50048]	Loss: 0.0698
Training Epoch: 83 [45184/50048]	Loss: 0.1905
Training Epoch: 83 [45312/50048]	Loss: 0.1148
Training Epoch: 83 [45440/50048]	Loss: 0.0704
Training Epoch: 83 [45568/50048]	Loss: 0.1392
Training Epoch: 83 [45696/50048]	Loss: 0.1117
2022-12-06 08:14:08,918 [ZeusDataLoader(train)] train epoch 84 done: time=86.45 energy=10499.02
2022-12-06 08:14:08,919 [ZeusDataLoader(eval)] Epoch 84 begin.
Training Epoch: 83 [45824/50048]	Loss: 0.0852
Training Epoch: 83 [45952/50048]	Loss: 0.0653
Training Epoch: 83 [46080/50048]	Loss: 0.0635
Training Epoch: 83 [46208/50048]	Loss: 0.0464
Training Epoch: 83 [46336/50048]	Loss: 0.0536
Training Epoch: 83 [46464/50048]	Loss: 0.1100
Training Epoch: 83 [46592/50048]	Loss: 0.1436
Training Epoch: 83 [46720/50048]	Loss: 0.0497
Training Epoch: 83 [46848/50048]	Loss: 0.0856
Training Epoch: 83 [46976/50048]	Loss: 0.1365
Training Epoch: 83 [47104/50048]	Loss: 0.1369
Training Epoch: 83 [47232/50048]	Loss: 0.1220
Training Epoch: 83 [47360/50048]	Loss: 0.1245
Training Epoch: 83 [47488/50048]	Loss: 0.0782
Training Epoch: 83 [47616/50048]	Loss: 0.1707
Training Epoch: 83 [47744/50048]	Loss: 0.1100
Training Epoch: 83 [47872/50048]	Loss: 0.0972
Training Epoch: 83 [48000/50048]	Loss: 0.1141
Training Epoch: 83 [48128/50048]	Loss: 0.0942
Training Epoch: 83 [48256/50048]	Loss: 0.1388
Training Epoch: 83 [48384/50048]	Loss: 0.1195
Training Epoch: 83 [48512/50048]	Loss: 0.1067
Training Epoch: 83 [48640/50048]	Loss: 0.1386
Training Epoch: 83 [48768/50048]	Loss: 0.0631
Training Epoch: 83 [48896/50048]	Loss: 0.1247
Training Epoch: 83 [49024/50048]	Loss: 0.1051
Training Epoch: 83 [49152/50048]	Loss: 0.2122
Training Epoch: 83 [49280/50048]	Loss: 0.0792
Training Epoch: 83 [49408/50048]	Loss: 0.1159
Training Epoch: 83 [49536/50048]	Loss: 0.0564
Training Epoch: 83 [49664/50048]	Loss: 0.0624
Training Epoch: 83 [49792/50048]	Loss: 0.0711
Training Epoch: 83 [49920/50048]	Loss: 0.0812
Training Epoch: 83 [50048/50048]	Loss: 0.1325
2022-12-06 13:14:12.558 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 08:14:12,570 [ZeusDataLoader(eval)] eval epoch 84 done: time=3.64 energy=441.75
2022-12-06 08:14:12,570 [ZeusDataLoader(train)] Up to epoch 84: time=7577.06, energy=919757.28, cost=1122871.35
2022-12-06 08:14:12,570 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 08:14:12,570 [ZeusDataLoader(train)] Expected next epoch: time=7666.86, energy=930555.30, cost=1136127.73
2022-12-06 08:14:12,571 [ZeusDataLoader(train)] Epoch 85 begin.
Validation Epoch: 83, Average loss: 0.0181, Accuracy: 0.6383
2022-12-06 08:14:12,757 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:14:12,757 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:14:12.761 [ZeusMonitor] Monitor started.
2022-12-06 13:14:12.761 [ZeusMonitor] Running indefinitely. 2022-12-06 13:14:12.761 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:14:12.761 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e85+gpu0.power.log
Training Epoch: 84 [128/50048]	Loss: 0.0585
Training Epoch: 84 [256/50048]	Loss: 0.1338
Training Epoch: 84 [384/50048]	Loss: 0.1212
Training Epoch: 84 [512/50048]	Loss: 0.0449
Training Epoch: 84 [640/50048]	Loss: 0.0531
Training Epoch: 84 [768/50048]	Loss: 0.0979
Training Epoch: 84 [896/50048]	Loss: 0.0948
Training Epoch: 84 [1024/50048]	Loss: 0.0622
Training Epoch: 84 [1152/50048]	Loss: 0.1563
Training Epoch: 84 [1280/50048]	Loss: 0.0746
Training Epoch: 84 [1408/50048]	Loss: 0.1816
Training Epoch: 84 [1536/50048]	Loss: 0.0687
Training Epoch: 84 [1664/50048]	Loss: 0.0807
Training Epoch: 84 [1792/50048]	Loss: 0.0418
Training Epoch: 84 [1920/50048]	Loss: 0.1192
Training Epoch: 84 [2048/50048]	Loss: 0.0957
Training Epoch: 84 [2176/50048]	Loss: 0.0801
Training Epoch: 84 [2304/50048]	Loss: 0.1752
Training Epoch: 84 [2432/50048]	Loss: 0.0851
Training Epoch: 84 [2560/50048]	Loss: 0.0751
Training Epoch: 84 [2688/50048]	Loss: 0.0918
Training Epoch: 84 [2816/50048]	Loss: 0.0648
Training Epoch: 84 [2944/50048]	Loss: 0.0639
Training Epoch: 84 [3072/50048]	Loss: 0.0640
Training Epoch: 84 [3200/50048]	Loss: 0.0731
Training Epoch: 84 [3328/50048]	Loss: 0.1091
Training Epoch: 84 [3456/50048]	Loss: 0.1050
Training Epoch: 84 [3584/50048]	Loss: 0.0725
Training Epoch: 84 [3712/50048]	Loss: 0.1425
Training Epoch: 84 [3840/50048]	Loss: 0.0849
Training Epoch: 84 [3968/50048]	Loss: 0.1060
Training Epoch: 84 [4096/50048]	Loss: 0.0458
Training Epoch: 84 [4224/50048]	Loss: 0.1958
Training Epoch: 84 [4352/50048]	Loss: 0.1174
Training Epoch: 84 [4480/50048]	Loss: 0.0776
Training Epoch: 84 [4608/50048]	Loss: 0.0830
Training Epoch: 84 [4736/50048]	Loss: 0.0791
Training Epoch: 84 [4864/50048]	Loss: 0.0377
Training Epoch: 84 [4992/50048]	Loss: 0.1282
Training Epoch: 84 [5120/50048]	Loss: 0.0706
Training Epoch: 84 [5248/50048]	Loss: 0.0757
Training Epoch: 84 [5376/50048]	Loss: 0.0441
Training Epoch: 84 [5504/50048]	Loss: 0.1069
Training Epoch: 84 [5632/50048]	Loss: 0.1376
Training Epoch: 84 [5760/50048]	Loss: 0.1337
Training Epoch: 84 [5888/50048]	Loss: 0.0955
Training Epoch: 84 [6016/50048]	Loss: 0.1638
Training Epoch: 84 [6144/50048]	Loss: 0.1182
Training Epoch: 84 [6272/50048]	Loss: 0.1146
Training Epoch: 84 [6400/50048]	Loss: 0.1377
Training Epoch: 84 [6528/50048]	Loss: 0.1446
Training Epoch: 84 [6656/50048]	Loss: 0.1100
Training Epoch: 84 [6784/50048]	Loss: 0.0976
Training Epoch: 84 [6912/50048]	Loss: 0.0373
Training Epoch: 84 [7040/50048]	Loss: 0.0617
Training Epoch: 84 [7168/50048]	Loss: 0.0681
Training Epoch: 84 [7296/50048]	Loss: 0.1039
Training Epoch: 84 [7424/50048]	Loss: 0.1080
Training Epoch: 84 [7552/50048]	Loss: 0.1085
Training Epoch: 84 [7680/50048]	Loss: 0.0731
Training Epoch: 84 [7808/50048]	Loss: 0.0891
Training Epoch: 84 [7936/50048]	Loss: 0.1045
Training Epoch: 84 [8064/50048]	Loss: 0.0800
Training Epoch: 84 [8192/50048]	Loss: 0.0790
Training Epoch: 84 [8320/50048]	Loss: 0.0815
Training Epoch: 84 [8448/50048]	Loss: 0.1755
Training Epoch: 84 [8576/50048]	Loss: 0.1270
Training Epoch: 84 [8704/50048]	Loss: 0.0966
Training Epoch: 84 [8832/50048]	Loss: 0.0947
Training Epoch: 84 [8960/50048]	Loss: 0.1106
Training Epoch: 84 [9088/50048]	Loss: 0.0765
Training Epoch: 84 [9216/50048]	Loss: 0.0808
Training Epoch: 84 [9344/50048]	Loss: 0.0787
Training Epoch: 84 [9472/50048]	Loss: 0.0644
Training Epoch: 84 [9600/50048]	Loss: 0.0627
Training Epoch: 84 [9728/50048]	Loss: 0.1010
Training Epoch: 84 [9856/50048]	Loss: 0.0711
Training Epoch: 84 [9984/50048]	Loss: 0.0477
Training Epoch: 84 [10112/50048]	Loss: 0.1715
Training Epoch: 84 [10240/50048]	Loss: 0.0843
Training Epoch: 84 [10368/50048]	Loss: 0.0637
Training Epoch: 84 [10496/50048]	Loss: 0.0862
Training Epoch: 84 [10624/50048]	Loss: 0.1445
Training Epoch: 84 [10752/50048]	Loss: 0.0888
Training Epoch: 84 [10880/50048]	Loss: 0.0828
Training Epoch: 84 [11008/50048]	Loss: 0.0468
Training Epoch: 84 [11136/50048]	Loss: 0.0888
Training Epoch: 84 [11264/50048]	Loss: 0.1140
Training Epoch: 84 [11392/50048]	Loss: 0.1239
Training Epoch: 84 [11520/50048]	Loss: 0.1700
Training Epoch: 84 [11648/50048]	Loss: 0.1168
Training Epoch: 84 [11776/50048]	Loss: 0.1070
Training Epoch: 84 [11904/50048]	Loss: 0.0854
Training Epoch: 84 [12032/50048]	Loss: 0.1092
Training Epoch: 84 [12160/50048]	Loss: 0.1243
Training Epoch: 84 [12288/50048]	Loss: 0.0655
Training Epoch: 84 [12416/50048]	Loss: 0.0886
Training Epoch: 84 [12544/50048]	Loss: 0.0674
Training Epoch: 84 [12672/50048]	Loss: 0.1336
Training Epoch: 84 [12800/50048]	Loss: 0.0777
Training Epoch: 84 [12928/50048]	Loss: 0.0707
Training Epoch: 84 [13056/50048]	Loss: 0.0458
Training Epoch: 84 [13184/50048]	Loss: 0.0825
Training Epoch: 84 [13312/50048]	Loss: 0.1632
Training Epoch: 84 [13440/50048]	Loss: 0.1094
Training Epoch: 84 [13568/50048]	Loss: 0.1550
Training Epoch: 84 [13696/50048]	Loss: 0.1263
Training Epoch: 84 [13824/50048]	Loss: 0.0957
Training Epoch: 84 [13952/50048]	Loss: 0.0949
Training Epoch: 84 [14080/50048]	Loss: 0.1664
Training Epoch: 84 [14208/50048]	Loss: 0.1035
Training Epoch: 84 [14336/50048]	Loss: 0.0819
Training Epoch: 84 [14464/50048]	Loss: 0.0512
Training Epoch: 84 [14592/50048]	Loss: 0.1262
Training Epoch: 84 [14720/50048]	Loss: 0.0774
Training Epoch: 84 [14848/50048]	Loss: 0.1169
Training Epoch: 84 [14976/50048]	Loss: 0.1306
Training Epoch: 84 [15104/50048]	Loss: 0.1088
Training Epoch: 84 [15232/50048]	Loss: 0.0892
Training Epoch: 84 [15360/50048]	Loss: 0.1051
Training Epoch: 84 [15488/50048]	Loss: 0.2089
Training Epoch: 84 [15616/50048]	Loss: 0.1261
Training Epoch: 84 [15744/50048]	Loss: 0.1100
Training Epoch: 84 [15872/50048]	Loss: 0.0749
Training Epoch: 84 [16000/50048]	Loss: 0.0716
Training Epoch: 84 [16128/50048]	Loss: 0.0742
Training Epoch: 84 [16256/50048]	Loss: 0.0628
Training Epoch: 84 [16384/50048]	Loss: 0.1463
Training Epoch: 84 [16512/50048]	Loss: 0.0720
Training Epoch: 84 [16640/50048]	Loss: 0.0809
Training Epoch: 84 [16768/50048]	Loss: 0.1556
Training Epoch: 84 [16896/50048]	Loss: 0.1124
Training Epoch: 84 [17024/50048]	Loss: 0.0682
Training Epoch: 84 [17152/50048]	Loss: 0.0769
Training Epoch: 84 [17280/50048]	Loss: 0.1701
Training Epoch: 84 [17408/50048]	Loss: 0.0948
Training Epoch: 84 [17536/50048]	Loss: 0.0761
Training Epoch: 84 [17664/50048]	Loss: 0.0458
Training Epoch: 84 [17792/50048]	Loss: 0.1888
Training Epoch: 84 [17920/50048]	Loss: 0.0603
Training Epoch: 84 [18048/50048]	Loss: 0.1565
Training Epoch: 84 [18176/50048]	Loss: 0.0933
Training Epoch: 84 [18304/50048]	Loss: 0.0768
Training Epoch: 84 [18432/50048]	Loss: 0.1136
Training Epoch: 84 [18560/50048]	Loss: 0.1978
Training Epoch: 84 [18688/50048]	Loss: 0.1328
Training Epoch: 84 [18816/50048]	Loss: 0.0524
Training Epoch: 84 [18944/50048]	Loss: 0.1321
Training Epoch: 84 [19072/50048]	Loss: 0.1183
Training Epoch: 84 [19200/50048]	Loss: 0.1058
Training Epoch: 84 [19328/50048]	Loss: 0.1181
Training Epoch: 84 [19456/50048]	Loss: 0.0835
Training Epoch: 84 [19584/50048]	Loss: 0.1282
Training Epoch: 84 [19712/50048]	Loss: 0.0725
Training Epoch: 84 [19840/50048]	Loss: 0.1469
Training Epoch: 84 [19968/50048]	Loss: 0.1067
Training Epoch: 84 [20096/50048]	Loss: 0.1584
Training Epoch: 84 [20224/50048]	Loss: 0.1813
Training Epoch: 84 [20352/50048]	Loss: 0.0871
Training Epoch: 84 [20480/50048]	Loss: 0.1189
Training Epoch: 84 [20608/50048]	Loss: 0.0958
Training Epoch: 84 [20736/50048]	Loss: 0.1374
Training Epoch: 84 [20864/50048]	Loss: 0.0980
Training Epoch: 84 [20992/50048]	Loss: 0.1058
Training Epoch: 84 [21120/50048]	Loss: 0.1099
Training Epoch: 84 [21248/50048]	Loss: 0.0750
Training Epoch: 84 [21376/50048]	Loss: 0.0516
Training Epoch: 84 [21504/50048]	Loss: 0.1062
Training Epoch: 84 [21632/50048]	Loss: 0.0627
Training Epoch: 84 [21760/50048]	Loss: 0.1010
Training Epoch: 84 [21888/50048]	Loss: 0.0893
Training Epoch: 84 [22016/50048]	Loss: 0.0384
Training Epoch: 84 [22144/50048]	Loss: 0.0920
Training Epoch: 84 [22272/50048]	Loss: 0.0727
Training Epoch: 84 [22400/50048]	Loss: 0.0788
Training Epoch: 84 [22528/50048]	Loss: 0.1475
Training Epoch: 84 [22656/50048]	Loss: 0.2193
Training Epoch: 84 [22784/50048]	Loss: 0.0811
Training Epoch: 84 [22912/50048]	Loss: 0.0813
Training Epoch: 84 [23040/50048]	Loss: 0.0896
Training Epoch: 84 [23168/50048]	Loss: 0.1162
Training Epoch: 84 [23296/50048]	Loss: 0.1367
Training Epoch: 84 [23424/50048]	Loss: 0.0943
Training Epoch: 84 [23552/50048]	Loss: 0.0793
Training Epoch: 84 [23680/50048]	Loss: 0.0958
Training Epoch: 84 [23808/50048]	Loss: 0.1449
Training Epoch: 84 [23936/50048]	Loss: 0.1126
Training Epoch: 84 [24064/50048]	Loss: 0.0900
Training Epoch: 84 [24192/50048]	Loss: 0.0565
Training Epoch: 84 [24320/50048]	Loss: 0.1351
Training Epoch: 84 [24448/50048]	Loss: 0.0808
Training Epoch: 84 [24576/50048]	Loss: 0.0565
Training Epoch: 84 [24704/50048]	Loss: 0.0307
Training Epoch: 84 [24832/50048]	Loss: 0.0921
Training Epoch: 84 [24960/50048]	Loss: 0.1688
Training Epoch: 84 [25088/50048]	Loss: 0.1763
Training Epoch: 84 [25216/50048]	Loss: 0.0870
Training Epoch: 84 [25344/50048]	Loss: 0.0581
Training Epoch: 84 [25472/50048]	Loss: 0.0635
Training Epoch: 84 [25600/50048]	Loss: 0.1238
Training Epoch: 84 [25728/50048]	Loss: 0.1646
Training Epoch: 84 [25856/50048]	Loss: 0.0821
Training Epoch: 84 [25984/50048]	Loss: 0.1265
Training Epoch: 84 [26112/50048]	Loss: 0.1167
Training Epoch: 84 [26240/50048]	Loss: 0.1111
Training Epoch: 84 [26368/50048]	Loss: 0.1210
Training Epoch: 84 [26496/50048]	Loss: 0.0773
Training Epoch: 84 [26624/50048]	Loss: 0.1837
Training Epoch: 84 [26752/50048]	Loss: 0.0998
Training Epoch: 84 [26880/50048]	Loss: 0.0662
Training Epoch: 84 [27008/50048]	Loss: 0.0922
Training Epoch: 84 [27136/50048]	Loss: 0.0498
Training Epoch: 84 [27264/50048]	Loss: 0.0841
Training Epoch: 84 [27392/50048]	Loss: 0.1251
Training Epoch: 84 [27520/50048]	Loss: 0.0931
Training Epoch: 84 [27648/50048]	Loss: 0.1096
Training Epoch: 84 [27776/50048]	Loss: 0.0636
Training Epoch: 84 [27904/50048]	Loss: 0.1077
Training Epoch: 84 [28032/50048]	Loss: 0.0664
Training Epoch: 84 [28160/50048]	Loss: 0.1227
Training Epoch: 84 [28288/50048]	Loss: 0.0924
Training Epoch: 84 [28416/50048]	Loss: 0.0894
Training Epoch: 84 [28544/50048]	Loss: 0.0914
Training Epoch: 84 [28672/50048]	Loss: 0.0965
Training Epoch: 84 [28800/50048]	Loss: 0.1250
Training Epoch: 84 [28928/50048]	Loss: 0.1059
Training Epoch: 84 [29056/50048]	Loss: 0.1498
Training Epoch: 84 [29184/50048]	Loss: 0.2310
Training Epoch: 84 [29312/50048]	Loss: 0.1207
Training Epoch: 84 [29440/50048]	Loss: 0.1384
Training Epoch: 84 [29568/50048]	Loss: 0.1081
Training Epoch: 84 [29696/50048]	Loss: 0.1420
Training Epoch: 84 [29824/50048]	Loss: 0.1219
Training Epoch: 84 [29952/50048]	Loss: 0.1561
Training Epoch: 84 [30080/50048]	Loss: 0.1281
Training Epoch: 84 [30208/50048]	Loss: 0.1028
Training Epoch: 84 [30336/50048]	Loss: 0.0690
Training Epoch: 84 [30464/50048]	Loss: 0.1272
Training Epoch: 84 [30592/50048]	Loss: 0.1427
Training Epoch: 84 [30720/50048]	Loss: 0.1014
Training Epoch: 84 [30848/50048]	Loss: 0.0575
Training Epoch: 84 [30976/50048]	Loss: 0.1559
Training Epoch: 84 [31104/50048]	Loss: 0.0666
Training Epoch: 84 [31232/50048]	Loss: 0.1552
Training Epoch: 84 [31360/50048]	Loss: 0.0851
Training Epoch: 84 [31488/50048]	Loss: 0.1032
Training Epoch: 84 [31616/50048]	Loss: 0.0850
Training Epoch: 84 [31744/50048]	Loss: 0.1703
Training Epoch: 84 [31872/50048]	Loss: 0.0963
Training Epoch: 84 [32000/50048]	Loss: 0.2032
Training Epoch: 84 [32128/50048]	Loss: 0.0472
Training Epoch: 84 [32256/50048]	Loss: 0.1073
Training Epoch: 84 [32384/50048]	Loss: 0.0522
Training Epoch: 84 [32512/50048]	Loss: 0.0811
Training Epoch: 84 [32640/50048]	Loss: 0.0656
Training Epoch: 84 [32768/50048]	Loss: 0.0982
Training Epoch: 84 [32896/50048]	Loss: 0.1063
Training Epoch: 84 [33024/50048]	Loss: 0.0925
Training Epoch: 84 [33152/50048]	Loss: 0.0920
Training Epoch: 84 [33280/50048]	Loss: 0.1443
Training Epoch: 84 [33408/50048]	Loss: 0.0658
Training Epoch: 84 [33536/50048]	Loss: 0.1440
Training Epoch: 84 [33664/50048]	Loss: 0.2058
Training Epoch: 84 [33792/50048]	Loss: 0.0998
Training Epoch: 84 [33920/50048]	Loss: 0.1891
Training Epoch: 84 [34048/50048]	Loss: 0.1889
Training Epoch: 84 [34176/50048]	Loss: 0.1347
Training Epoch: 84 [34304/50048]	Loss: 0.1110
Training Epoch: 84 [34432/50048]	Loss: 0.1599
Training Epoch: 84 [34560/50048]	Loss: 0.1672
Training Epoch: 84 [34688/50048]	Loss: 0.1087
Training Epoch: 84 [34816/50048]	Loss: 0.0715
Training Epoch: 84 [34944/50048]	Loss: 0.1186
Training Epoch: 84 [35072/50048]	Loss: 0.1065
Training Epoch: 84 [35200/50048]	Loss: 0.0765
Training Epoch: 84 [35328/50048]	Loss: 0.0742
Training Epoch: 84 [35456/50048]	Loss: 0.1181
Training Epoch: 84 [35584/50048]	Loss: 0.1704
Training Epoch: 84 [35712/50048]	Loss: 0.2565
Training Epoch: 84 [35840/50048]	Loss: 0.1583
Training Epoch: 84 [35968/50048]	Loss: 0.1243
Training Epoch: 84 [36096/50048]	Loss: 0.1373
Training Epoch: 84 [36224/50048]	Loss: 0.1462
Training Epoch: 84 [36352/50048]	Loss: 0.1505
Training Epoch: 84 [36480/50048]	Loss: 0.0977
Training Epoch: 84 [36608/50048]	Loss: 0.1232
Training Epoch: 84 [36736/50048]	Loss: 0.2593
Training Epoch: 84 [36864/50048]	Loss: 0.1254
Training Epoch: 84 [36992/50048]	Loss: 0.1047
Training Epoch: 84 [37120/50048]	Loss: 0.0909
Training Epoch: 84 [37248/50048]	Loss: 0.1465
Training Epoch: 84 [37376/50048]	Loss: 0.0771
Training Epoch: 84 [37504/50048]	Loss: 0.1308
Training Epoch: 84 [37632/50048]	Loss: 0.1005
Training Epoch: 84 [37760/50048]	Loss: 0.0849
Training Epoch: 84 [37888/50048]	Loss: 0.1587
Training Epoch: 84 [38016/50048]	Loss: 0.0800
Training Epoch: 84 [38144/50048]	Loss: 0.1783
Training Epoch: 84 [38272/50048]	Loss: 0.1512
Training Epoch: 84 [38400/50048]	Loss: 0.1016
Training Epoch: 84 [38528/50048]	Loss: 0.0308
Training Epoch: 84 [38656/50048]	Loss: 0.1375
Training Epoch: 84 [38784/50048]	Loss: 0.1003
Training Epoch: 84 [38912/50048]	Loss: 0.0992
Training Epoch: 84 [39040/50048]	Loss: 0.1409
Training Epoch: 84 [39168/50048]	Loss: 0.0542
Training Epoch: 84 [39296/50048]	Loss: 0.0930
Training Epoch: 84 [39424/50048]	Loss: 0.1032
Training Epoch: 84 [39552/50048]	Loss: 0.1489
Training Epoch: 84 [39680/50048]	Loss: 0.0885
Training Epoch: 84 [39808/50048]	Loss: 0.1214
Training Epoch: 84 [39936/50048]	Loss: 0.1107
Training Epoch: 84 [40064/50048]	Loss: 0.1478
Training Epoch: 84 [40192/50048]	Loss: 0.0442
Training Epoch: 84 [40320/50048]	Loss: 0.1261
Training Epoch: 84 [40448/50048]	Loss: 0.1747
Training Epoch: 84 [40576/50048]	Loss: 0.1236
Training Epoch: 84 [40704/50048]	Loss: 0.0845
Training Epoch: 84 [40832/50048]	Loss: 0.0832
Training Epoch: 84 [40960/50048]	Loss: 0.1865
Training Epoch: 84 [41088/50048]	Loss: 0.2005
Training Epoch: 84 [41216/50048]	Loss: 0.1671
Training Epoch: 84 [41344/50048]	Loss: 0.1179
Training Epoch: 84 [41472/50048]	Loss: 0.2081
Training Epoch: 84 [41600/50048]	Loss: 0.1186
Training Epoch: 84 [41728/50048]	Loss: 0.0965
Training Epoch: 84 [41856/50048]	Loss: 0.1191
Training Epoch: 84 [41984/50048]	Loss: 0.0948
Training Epoch: 84 [42112/50048]	Loss: 0.1715
Training Epoch: 84 [42240/50048]	Loss: 0.1039
Training Epoch: 84 [42368/50048]	Loss: 0.2702
Training Epoch: 84 [42496/50048]	Loss: 0.1612
Training Epoch: 84 [42624/50048]	Loss: 0.1060
Training Epoch: 84 [42752/50048]	Loss: 0.0988
Training Epoch: 84 [42880/50048]	Loss: 0.0909
Training Epoch: 84 [43008/50048]	Loss: 0.1448
Training Epoch: 84 [43136/50048]	Loss: 0.1365
Training Epoch: 84 [43264/50048]	Loss: 0.1427
Training Epoch: 84 [43392/50048]	Loss: 0.0949
Training Epoch: 84 [43520/50048]	Loss: 0.1093
Training Epoch: 84 [43648/50048]	Loss: 0.1319
Training Epoch: 84 [43776/50048]	Loss: 0.1731
Training Epoch: 84 [43904/50048]	Loss: 0.1215
Training Epoch: 84 [44032/50048]	Loss: 0.1974
Training Epoch: 84 [44160/50048]	Loss: 0.1497
Training Epoch: 84 [44288/50048]	Loss: 0.0722
Training Epoch: 84 [44416/50048]	Loss: 0.1094
Training Epoch: 84 [44544/50048]	Loss: 0.0863
Training Epoch: 84 [44672/50048]	Loss: 0.0397
Training Epoch: 84 [44800/50048]	Loss: 0.1031
Training Epoch: 84 [44928/50048]	Loss: 0.1180
Training Epoch: 84 [45056/50048]	Loss: 0.0847
Training Epoch: 84 [45184/50048]	Loss: 0.1830
Training Epoch: 84 [45312/50048]	Loss: 0.0675
Training Epoch: 84 [45440/50048]	Loss: 0.1230
Training Epoch: 84 [45568/50048]	Loss: 0.1109
Training Epoch: 84 [45696/50048]	Loss: 0.1038
2022-12-06 08:15:39,030 [ZeusDataLoader(train)] train epoch 85 done: time=86.45 energy=10500.46
2022-12-06 08:15:39,032 [ZeusDataLoader(eval)] Epoch 85 begin.
Training Epoch: 84 [45824/50048]	Loss: 0.0692
Training Epoch: 84 [45952/50048]	Loss: 0.1180
Training Epoch: 84 [46080/50048]	Loss: 0.0683
Training Epoch: 84 [46208/50048]	Loss: 0.0969
Training Epoch: 84 [46336/50048]	Loss: 0.1325
Training Epoch: 84 [46464/50048]	Loss: 0.1095
Training Epoch: 84 [46592/50048]	Loss: 0.1296
Training Epoch: 84 [46720/50048]	Loss: 0.0617
Training Epoch: 84 [46848/50048]	Loss: 0.1101
Training Epoch: 84 [46976/50048]	Loss: 0.1328
Training Epoch: 84 [47104/50048]	Loss: 0.1367
Training Epoch: 84 [47232/50048]	Loss: 0.2036
Training Epoch: 84 [47360/50048]	Loss: 0.1480
Training Epoch: 84 [47488/50048]	Loss: 0.1244
Training Epoch: 84 [47616/50048]	Loss: 0.1778
Training Epoch: 84 [47744/50048]	Loss: 0.1252
Training Epoch: 84 [47872/50048]	Loss: 0.1025
Training Epoch: 84 [48000/50048]	Loss: 0.1020
Training Epoch: 84 [48128/50048]	Loss: 0.1684
Training Epoch: 84 [48256/50048]	Loss: 0.1032
Training Epoch: 84 [48384/50048]	Loss: 0.1071
Training Epoch: 84 [48512/50048]	Loss: 0.1237
Training Epoch: 84 [48640/50048]	Loss: 0.1147
Training Epoch: 84 [48768/50048]	Loss: 0.0693
Training Epoch: 84 [48896/50048]	Loss: 0.1443
Training Epoch: 84 [49024/50048]	Loss: 0.1364
Training Epoch: 84 [49152/50048]	Loss: 0.1148
Training Epoch: 84 [49280/50048]	Loss: 0.1155
Training Epoch: 84 [49408/50048]	Loss: 0.1037
Training Epoch: 84 [49536/50048]	Loss: 0.1163
Training Epoch: 84 [49664/50048]	Loss: 0.0518
Training Epoch: 84 [49792/50048]	Loss: 0.1710
Training Epoch: 84 [49920/50048]	Loss: 0.0949
Training Epoch: 84 [50048/50048]	Loss: 0.1108
2022-12-06 13:15:42.724 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 08:15:42,736 [ZeusDataLoader(eval)] eval epoch 85 done: time=3.70 energy=453.01
2022-12-06 08:15:42,736 [ZeusDataLoader(train)] Up to epoch 85: time=7667.20, energy=930710.76, cost=1136235.70
2022-12-06 08:15:42,736 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 08:15:42,736 [ZeusDataLoader(train)] Expected next epoch: time=7757.00, energy=941508.77, cost=1149492.08
2022-12-06 08:15:42,737 [ZeusDataLoader(train)] Epoch 86 begin.
Validation Epoch: 84, Average loss: 0.0181, Accuracy: 0.6411
2022-12-06 08:15:42,929 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:15:42,930 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:15:42.931 [ZeusMonitor] Monitor started.
2022-12-06 13:15:42.932 [ZeusMonitor] Running indefinitely. 2022-12-06 13:15:42.932 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:15:42.932 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e86+gpu0.power.log
Training Epoch: 85 [128/50048]	Loss: 0.0918
Training Epoch: 85 [256/50048]	Loss: 0.0619
Training Epoch: 85 [384/50048]	Loss: 0.1012
Training Epoch: 85 [512/50048]	Loss: 0.0766
Training Epoch: 85 [640/50048]	Loss: 0.0371
Training Epoch: 85 [768/50048]	Loss: 0.0977
Training Epoch: 85 [896/50048]	Loss: 0.1227
Training Epoch: 85 [1024/50048]	Loss: 0.1014
Training Epoch: 85 [1152/50048]	Loss: 0.0377
Training Epoch: 85 [1280/50048]	Loss: 0.0332
Training Epoch: 85 [1408/50048]	Loss: 0.0410
Training Epoch: 85 [1536/50048]	Loss: 0.0319
Training Epoch: 85 [1664/50048]	Loss: 0.0732
Training Epoch: 85 [1792/50048]	Loss: 0.0443
Training Epoch: 85 [1920/50048]	Loss: 0.0753
Training Epoch: 85 [2048/50048]	Loss: 0.0874
Training Epoch: 85 [2176/50048]	Loss: 0.0628
Training Epoch: 85 [2304/50048]	Loss: 0.0349
Training Epoch: 85 [2432/50048]	Loss: 0.0792
Training Epoch: 85 [2560/50048]	Loss: 0.0640
Training Epoch: 85 [2688/50048]	Loss: 0.1681
Training Epoch: 85 [2816/50048]	Loss: 0.0714
Training Epoch: 85 [2944/50048]	Loss: 0.0835
Training Epoch: 85 [3072/50048]	Loss: 0.0662
Training Epoch: 85 [3200/50048]	Loss: 0.1009
Training Epoch: 85 [3328/50048]	Loss: 0.0476
Training Epoch: 85 [3456/50048]	Loss: 0.0650
Training Epoch: 85 [3584/50048]	Loss: 0.1159
Training Epoch: 85 [3712/50048]	Loss: 0.0953
Training Epoch: 85 [3840/50048]	Loss: 0.0449
Training Epoch: 85 [3968/50048]	Loss: 0.0651
Training Epoch: 85 [4096/50048]	Loss: 0.1219
Training Epoch: 85 [4224/50048]	Loss: 0.0889
Training Epoch: 85 [4352/50048]	Loss: 0.1195
Training Epoch: 85 [4480/50048]	Loss: 0.0678
Training Epoch: 85 [4608/50048]	Loss: 0.0447
Training Epoch: 85 [4736/50048]	Loss: 0.1607
Training Epoch: 85 [4864/50048]	Loss: 0.0772
Training Epoch: 85 [4992/50048]	Loss: 0.0801
Training Epoch: 85 [5120/50048]	Loss: 0.1244
Training Epoch: 85 [5248/50048]	Loss: 0.0485
Training Epoch: 85 [5376/50048]	Loss: 0.0517
Training Epoch: 85 [5504/50048]	Loss: 0.0439
Training Epoch: 85 [5632/50048]	Loss: 0.1303
Training Epoch: 85 [5760/50048]	Loss: 0.1055
Training Epoch: 85 [5888/50048]	Loss: 0.0680
Training Epoch: 85 [6016/50048]	Loss: 0.0745
Training Epoch: 85 [6144/50048]	Loss: 0.0450
Training Epoch: 85 [6272/50048]	Loss: 0.0475
Training Epoch: 85 [6400/50048]	Loss: 0.0824
Training Epoch: 85 [6528/50048]	Loss: 0.1470
Training Epoch: 85 [6656/50048]	Loss: 0.1119
Training Epoch: 85 [6784/50048]	Loss: 0.1097
Training Epoch: 85 [6912/50048]	Loss: 0.1526
Training Epoch: 85 [7040/50048]	Loss: 0.0754
Training Epoch: 85 [7168/50048]	Loss: 0.1489
Training Epoch: 85 [7296/50048]	Loss: 0.1068
Training Epoch: 85 [7424/50048]	Loss: 0.0513
Training Epoch: 85 [7552/50048]	Loss: 0.0911
Training Epoch: 85 [7680/50048]	Loss: 0.0802
Training Epoch: 85 [7808/50048]	Loss: 0.0552
Training Epoch: 85 [7936/50048]	Loss: 0.0711
Training Epoch: 85 [8064/50048]	Loss: 0.1270
Training Epoch: 85 [8192/50048]	Loss: 0.0533
Training Epoch: 85 [8320/50048]	Loss: 0.0517
Training Epoch: 85 [8448/50048]	Loss: 0.0774
Training Epoch: 85 [8576/50048]	Loss: 0.1091
Training Epoch: 85 [8704/50048]	Loss: 0.1498
Training Epoch: 85 [8832/50048]	Loss: 0.0789
Training Epoch: 85 [8960/50048]	Loss: 0.1600
Training Epoch: 85 [9088/50048]	Loss: 0.1362
Training Epoch: 85 [9216/50048]	Loss: 0.1175
Training Epoch: 85 [9344/50048]	Loss: 0.0907
Training Epoch: 85 [9472/50048]	Loss: 0.0870
Training Epoch: 85 [9600/50048]	Loss: 0.1409
Training Epoch: 85 [9728/50048]	Loss: 0.0823
Training Epoch: 85 [9856/50048]	Loss: 0.0808
Training Epoch: 85 [9984/50048]	Loss: 0.0562
Training Epoch: 85 [10112/50048]	Loss: 0.0576
Training Epoch: 85 [10240/50048]	Loss: 0.0920
Training Epoch: 85 [10368/50048]	Loss: 0.1775
Training Epoch: 85 [10496/50048]	Loss: 0.1054
Training Epoch: 85 [10624/50048]	Loss: 0.0375
Training Epoch: 85 [10752/50048]	Loss: 0.1346
Training Epoch: 85 [10880/50048]	Loss: 0.1073
Training Epoch: 85 [11008/50048]	Loss: 0.1309
Training Epoch: 85 [11136/50048]	Loss: 0.1591
Training Epoch: 85 [11264/50048]	Loss: 0.0628
Training Epoch: 85 [11392/50048]	Loss: 0.0916
Training Epoch: 85 [11520/50048]	Loss: 0.0642
Training Epoch: 85 [11648/50048]	Loss: 0.1508
Training Epoch: 85 [11776/50048]	Loss: 0.1511
Training Epoch: 85 [11904/50048]	Loss: 0.1133
Training Epoch: 85 [12032/50048]	Loss: 0.0724
Training Epoch: 85 [12160/50048]	Loss: 0.0827
Training Epoch: 85 [12288/50048]	Loss: 0.1061
Training Epoch: 85 [12416/50048]	Loss: 0.0965
Training Epoch: 85 [12544/50048]	Loss: 0.0504
Training Epoch: 85 [12672/50048]	Loss: 0.0388
Training Epoch: 85 [12800/50048]	Loss: 0.1556
Training Epoch: 85 [12928/50048]	Loss: 0.0682
Training Epoch: 85 [13056/50048]	Loss: 0.0811
Training Epoch: 85 [13184/50048]	Loss: 0.0927
Training Epoch: 85 [13312/50048]	Loss: 0.0922
Training Epoch: 85 [13440/50048]	Loss: 0.0830
Training Epoch: 85 [13568/50048]	Loss: 0.0765
Training Epoch: 85 [13696/50048]	Loss: 0.0991
Training Epoch: 85 [13824/50048]	Loss: 0.0484
Training Epoch: 85 [13952/50048]	Loss: 0.1282
Training Epoch: 85 [14080/50048]	Loss: 0.0809
Training Epoch: 85 [14208/50048]	Loss: 0.0518
Training Epoch: 85 [14336/50048]	Loss: 0.0464
Training Epoch: 85 [14464/50048]	Loss: 0.0941
Training Epoch: 85 [14592/50048]	Loss: 0.1687
Training Epoch: 85 [14720/50048]	Loss: 0.1050
Training Epoch: 85 [14848/50048]	Loss: 0.0712
Training Epoch: 85 [14976/50048]	Loss: 0.1035
Training Epoch: 85 [15104/50048]	Loss: 0.1012
Training Epoch: 85 [15232/50048]	Loss: 0.1014
Training Epoch: 85 [15360/50048]	Loss: 0.0437
Training Epoch: 85 [15488/50048]	Loss: 0.0791
Training Epoch: 85 [15616/50048]	Loss: 0.1712
Training Epoch: 85 [15744/50048]	Loss: 0.1058
Training Epoch: 85 [15872/50048]	Loss: 0.0745
Training Epoch: 85 [16000/50048]	Loss: 0.1101
Training Epoch: 85 [16128/50048]	Loss: 0.0754
Training Epoch: 85 [16256/50048]	Loss: 0.1171
Training Epoch: 85 [16384/50048]	Loss: 0.1262
Training Epoch: 85 [16512/50048]	Loss: 0.2167
Training Epoch: 85 [16640/50048]	Loss: 0.0897
Training Epoch: 85 [16768/50048]	Loss: 0.1664
Training Epoch: 85 [16896/50048]	Loss: 0.0830
Training Epoch: 85 [17024/50048]	Loss: 0.0830
Training Epoch: 85 [17152/50048]	Loss: 0.0529
Training Epoch: 85 [17280/50048]	Loss: 0.1023
Training Epoch: 85 [17408/50048]	Loss: 0.0712
Training Epoch: 85 [17536/50048]	Loss: 0.0902
Training Epoch: 85 [17664/50048]	Loss: 0.0256
Training Epoch: 85 [17792/50048]	Loss: 0.0718
Training Epoch: 85 [17920/50048]	Loss: 0.0851
Training Epoch: 85 [18048/50048]	Loss: 0.0851
Training Epoch: 85 [18176/50048]	Loss: 0.0859
Training Epoch: 85 [18304/50048]	Loss: 0.0936
Training Epoch: 85 [18432/50048]	Loss: 0.0577
Training Epoch: 85 [18560/50048]	Loss: 0.0639
Training Epoch: 85 [18688/50048]	Loss: 0.0786
Training Epoch: 85 [18816/50048]	Loss: 0.1008
Training Epoch: 85 [18944/50048]	Loss: 0.0627
Training Epoch: 85 [19072/50048]	Loss: 0.2351
Training Epoch: 85 [19200/50048]	Loss: 0.1189
Training Epoch: 85 [19328/50048]	Loss: 0.0567
Training Epoch: 85 [19456/50048]	Loss: 0.0739
Training Epoch: 85 [19584/50048]	Loss: 0.0982
Training Epoch: 85 [19712/50048]	Loss: 0.1504
Training Epoch: 85 [19840/50048]	Loss: 0.1382
Training Epoch: 85 [19968/50048]	Loss: 0.1257
Training Epoch: 85 [20096/50048]	Loss: 0.1872
Training Epoch: 85 [20224/50048]	Loss: 0.0674
Training Epoch: 85 [20352/50048]	Loss: 0.1461
Training Epoch: 85 [20480/50048]	Loss: 0.0355
Training Epoch: 85 [20608/50048]	Loss: 0.1134
Training Epoch: 85 [20736/50048]	Loss: 0.1788
Training Epoch: 85 [20864/50048]	Loss: 0.1416
Training Epoch: 85 [20992/50048]	Loss: 0.0885
Training Epoch: 85 [21120/50048]	Loss: 0.0817
Training Epoch: 85 [21248/50048]	Loss: 0.1171
Training Epoch: 85 [21376/50048]	Loss: 0.1523
Training Epoch: 85 [21504/50048]	Loss: 0.1050
Training Epoch: 85 [21632/50048]	Loss: 0.1144
Training Epoch: 85 [21760/50048]	Loss: 0.1095
Training Epoch: 85 [21888/50048]	Loss: 0.0638
Training Epoch: 85 [22016/50048]	Loss: 0.0753
Training Epoch: 85 [22144/50048]	Loss: 0.0783
Training Epoch: 85 [22272/50048]	Loss: 0.1077
Training Epoch: 85 [22400/50048]	Loss: 0.0440
Training Epoch: 85 [22528/50048]	Loss: 0.0714
Training Epoch: 85 [22656/50048]	Loss: 0.0767
Training Epoch: 85 [22784/50048]	Loss: 0.0333
Training Epoch: 85 [22912/50048]	Loss: 0.0911
Training Epoch: 85 [23040/50048]	Loss: 0.0806
Training Epoch: 85 [23168/50048]	Loss: 0.0256
Training Epoch: 85 [23296/50048]	Loss: 0.0846
Training Epoch: 85 [23424/50048]	Loss: 0.1221
Training Epoch: 85 [23552/50048]	Loss: 0.1163
Training Epoch: 85 [23680/50048]	Loss: 0.1218
Training Epoch: 85 [23808/50048]	Loss: 0.1152
Training Epoch: 85 [23936/50048]	Loss: 0.0732
Training Epoch: 85 [24064/50048]	Loss: 0.0881
Training Epoch: 85 [24192/50048]	Loss: 0.1020
Training Epoch: 85 [24320/50048]	Loss: 0.0901
Training Epoch: 85 [24448/50048]	Loss: 0.0895
Training Epoch: 85 [24576/50048]	Loss: 0.1928
Training Epoch: 85 [24704/50048]	Loss: 0.1391
Training Epoch: 85 [24832/50048]	Loss: 0.0875
Training Epoch: 85 [24960/50048]	Loss: 0.0559
Training Epoch: 85 [25088/50048]	Loss: 0.1393
Training Epoch: 85 [25216/50048]	Loss: 0.1170
Training Epoch: 85 [25344/50048]	Loss: 0.0749
Training Epoch: 85 [25472/50048]	Loss: 0.1014
Training Epoch: 85 [25600/50048]	Loss: 0.1426
Training Epoch: 85 [25728/50048]	Loss: 0.1261
Training Epoch: 85 [25856/50048]	Loss: 0.0825
Training Epoch: 85 [25984/50048]	Loss: 0.1329
Training Epoch: 85 [26112/50048]	Loss: 0.0758
Training Epoch: 85 [26240/50048]	Loss: 0.0658
Training Epoch: 85 [26368/50048]	Loss: 0.0833
Training Epoch: 85 [26496/50048]	Loss: 0.1471
Training Epoch: 85 [26624/50048]	Loss: 0.1408
Training Epoch: 85 [26752/50048]	Loss: 0.1148
Training Epoch: 85 [26880/50048]	Loss: 0.0951
Training Epoch: 85 [27008/50048]	Loss: 0.1253
Training Epoch: 85 [27136/50048]	Loss: 0.0909
Training Epoch: 85 [27264/50048]	Loss: 0.1324
Training Epoch: 85 [27392/50048]	Loss: 0.1195
Training Epoch: 85 [27520/50048]	Loss: 0.0707
Training Epoch: 85 [27648/50048]	Loss: 0.1348
Training Epoch: 85 [27776/50048]	Loss: 0.0855
Training Epoch: 85 [27904/50048]	Loss: 0.1271
Training Epoch: 85 [28032/50048]	Loss: 0.0627
Training Epoch: 85 [28160/50048]	Loss: 0.1015
Training Epoch: 85 [28288/50048]	Loss: 0.1098
Training Epoch: 85 [28416/50048]	Loss: 0.0408
Training Epoch: 85 [28544/50048]	Loss: 0.1288
Training Epoch: 85 [28672/50048]	Loss: 0.1361
Training Epoch: 85 [28800/50048]	Loss: 0.0682
Training Epoch: 85 [28928/50048]	Loss: 0.1671
Training Epoch: 85 [29056/50048]	Loss: 0.2349
Training Epoch: 85 [29184/50048]	Loss: 0.2052
Training Epoch: 85 [29312/50048]	Loss: 0.1526
Training Epoch: 85 [29440/50048]	Loss: 0.1051
Training Epoch: 85 [29568/50048]	Loss: 0.1063
Training Epoch: 85 [29696/50048]	Loss: 0.1036
Training Epoch: 85 [29824/50048]	Loss: 0.1155
Training Epoch: 85 [29952/50048]	Loss: 0.1042
Training Epoch: 85 [30080/50048]	Loss: 0.1566
Training Epoch: 85 [30208/50048]	Loss: 0.0847
Training Epoch: 85 [30336/50048]	Loss: 0.0847
Training Epoch: 85 [30464/50048]	Loss: 0.1195
Training Epoch: 85 [30592/50048]	Loss: 0.1628
Training Epoch: 85 [30720/50048]	Loss: 0.0515
Training Epoch: 85 [30848/50048]	Loss: 0.0381
Training Epoch: 85 [30976/50048]	Loss: 0.1634
Training Epoch: 85 [31104/50048]	Loss: 0.0757
Training Epoch: 85 [31232/50048]	Loss: 0.0869
Training Epoch: 85 [31360/50048]	Loss: 0.0677
Training Epoch: 85 [31488/50048]	Loss: 0.1224
Training Epoch: 85 [31616/50048]	Loss: 0.0730
Training Epoch: 85 [31744/50048]	Loss: 0.1629
Training Epoch: 85 [31872/50048]	Loss: 0.1844
Training Epoch: 85 [32000/50048]	Loss: 0.1505
Training Epoch: 85 [32128/50048]	Loss: 0.0758
Training Epoch: 85 [32256/50048]	Loss: 0.1621
Training Epoch: 85 [32384/50048]	Loss: 0.0614
Training Epoch: 85 [32512/50048]	Loss: 0.1118
Training Epoch: 85 [32640/50048]	Loss: 0.1233
Training Epoch: 85 [32768/50048]	Loss: 0.0743
Training Epoch: 85 [32896/50048]	Loss: 0.1077
Training Epoch: 85 [33024/50048]	Loss: 0.0911
Training Epoch: 85 [33152/50048]	Loss: 0.0841
Training Epoch: 85 [33280/50048]	Loss: 0.1128
Training Epoch: 85 [33408/50048]	Loss: 0.1146
Training Epoch: 85 [33536/50048]	Loss: 0.0694
Training Epoch: 85 [33664/50048]	Loss: 0.0810
Training Epoch: 85 [33792/50048]	Loss: 0.0848
Training Epoch: 85 [33920/50048]	Loss: 0.0641
Training Epoch: 85 [34048/50048]	Loss: 0.1613
Training Epoch: 85 [34176/50048]	Loss: 0.1204
Training Epoch: 85 [34304/50048]	Loss: 0.1041
Training Epoch: 85 [34432/50048]	Loss: 0.0634
Training Epoch: 85 [34560/50048]	Loss: 0.1161
Training Epoch: 85 [34688/50048]	Loss: 0.0759
Training Epoch: 85 [34816/50048]	Loss: 0.0967
Training Epoch: 85 [34944/50048]	Loss: 0.0994
Training Epoch: 85 [35072/50048]	Loss: 0.0839
Training Epoch: 85 [35200/50048]	Loss: 0.1048
Training Epoch: 85 [35328/50048]	Loss: 0.1316
Training Epoch: 85 [35456/50048]	Loss: 0.0527
Training Epoch: 85 [35584/50048]	Loss: 0.2190
Training Epoch: 85 [35712/50048]	Loss: 0.1003
Training Epoch: 85 [35840/50048]	Loss: 0.1000
Training Epoch: 85 [35968/50048]	Loss: 0.1387
Training Epoch: 85 [36096/50048]	Loss: 0.0997
Training Epoch: 85 [36224/50048]	Loss: 0.1417
Training Epoch: 85 [36352/50048]	Loss: 0.1101
Training Epoch: 85 [36480/50048]	Loss: 0.1744
Training Epoch: 85 [36608/50048]	Loss: 0.0758
Training Epoch: 85 [36736/50048]	Loss: 0.0669
Training Epoch: 85 [36864/50048]	Loss: 0.1325
Training Epoch: 85 [36992/50048]	Loss: 0.1565
Training Epoch: 85 [37120/50048]	Loss: 0.1219
Training Epoch: 85 [37248/50048]	Loss: 0.0865
Training Epoch: 85 [37376/50048]	Loss: 0.0433
Training Epoch: 85 [37504/50048]	Loss: 0.1016
Training Epoch: 85 [37632/50048]	Loss: 0.0330
Training Epoch: 85 [37760/50048]	Loss: 0.1159
Training Epoch: 85 [37888/50048]	Loss: 0.1478
Training Epoch: 85 [38016/50048]	Loss: 0.1467
Training Epoch: 85 [38144/50048]	Loss: 0.0846
Training Epoch: 85 [38272/50048]	Loss: 0.1006
Training Epoch: 85 [38400/50048]	Loss: 0.1650
Training Epoch: 85 [38528/50048]	Loss: 0.1472
Training Epoch: 85 [38656/50048]	Loss: 0.0695
Training Epoch: 85 [38784/50048]	Loss: 0.1071
Training Epoch: 85 [38912/50048]	Loss: 0.0777
Training Epoch: 85 [39040/50048]	Loss: 0.1390
Training Epoch: 85 [39168/50048]	Loss: 0.1159
Training Epoch: 85 [39296/50048]	Loss: 0.1648
Training Epoch: 85 [39424/50048]	Loss: 0.1146
Training Epoch: 85 [39552/50048]	Loss: 0.0562
Training Epoch: 85 [39680/50048]	Loss: 0.1021
Training Epoch: 85 [39808/50048]	Loss: 0.2065
Training Epoch: 85 [39936/50048]	Loss: 0.0631
Training Epoch: 85 [40064/50048]	Loss: 0.1150
Training Epoch: 85 [40192/50048]	Loss: 0.0579
Training Epoch: 85 [40320/50048]	Loss: 0.1043
Training Epoch: 85 [40448/50048]	Loss: 0.1006
Training Epoch: 85 [40576/50048]	Loss: 0.1199
Training Epoch: 85 [40704/50048]	Loss: 0.1939
Training Epoch: 85 [40832/50048]	Loss: 0.0749
Training Epoch: 85 [40960/50048]	Loss: 0.0905
Training Epoch: 85 [41088/50048]	Loss: 0.1010
Training Epoch: 85 [41216/50048]	Loss: 0.1103
Training Epoch: 85 [41344/50048]	Loss: 0.1226
Training Epoch: 85 [41472/50048]	Loss: 0.1071
Training Epoch: 85 [41600/50048]	Loss: 0.1065
Training Epoch: 85 [41728/50048]	Loss: 0.0690
Training Epoch: 85 [41856/50048]	Loss: 0.1032
Training Epoch: 85 [41984/50048]	Loss: 0.0680
Training Epoch: 85 [42112/50048]	Loss: 0.0830
Training Epoch: 85 [42240/50048]	Loss: 0.1349
Training Epoch: 85 [42368/50048]	Loss: 0.1002
Training Epoch: 85 [42496/50048]	Loss: 0.2488
Training Epoch: 85 [42624/50048]	Loss: 0.1278
Training Epoch: 85 [42752/50048]	Loss: 0.1480
Training Epoch: 85 [42880/50048]	Loss: 0.1844
Training Epoch: 85 [43008/50048]	Loss: 0.1211
Training Epoch: 85 [43136/50048]	Loss: 0.1744
Training Epoch: 85 [43264/50048]	Loss: 0.1498
Training Epoch: 85 [43392/50048]	Loss: 0.0582
Training Epoch: 85 [43520/50048]	Loss: 0.1106
Training Epoch: 85 [43648/50048]	Loss: 0.1047
Training Epoch: 85 [43776/50048]	Loss: 0.0992
Training Epoch: 85 [43904/50048]	Loss: 0.1293
Training Epoch: 85 [44032/50048]	Loss: 0.0907
Training Epoch: 85 [44160/50048]	Loss: 0.0428
Training Epoch: 85 [44288/50048]	Loss: 0.0925
Training Epoch: 85 [44416/50048]	Loss: 0.2581
Training Epoch: 85 [44544/50048]	Loss: 0.1866
Training Epoch: 85 [44672/50048]	Loss: 0.0862
Training Epoch: 85 [44800/50048]	Loss: 0.0810
Training Epoch: 85 [44928/50048]	Loss: 0.0817
Training Epoch: 85 [45056/50048]	Loss: 0.0671
Training Epoch: 85 [45184/50048]	Loss: 0.1611
Training Epoch: 85 [45312/50048]	Loss: 0.1088
Training Epoch: 85 [45440/50048]	Loss: 0.1061
Training Epoch: 85 [45568/50048]	Loss: 0.0987
Training Epoch: 85 [45696/50048]	Loss: 0.1593
2022-12-06 08:17:09,281 [ZeusDataLoader(train)] train epoch 86 done: time=86.53 energy=10503.95
2022-12-06 08:17:09,282 [ZeusDataLoader(eval)] Epoch 86 begin.
Training Epoch: 85 [45824/50048]	Loss: 0.1146
Training Epoch: 85 [45952/50048]	Loss: 0.1089
Training Epoch: 85 [46080/50048]	Loss: 0.1442
Training Epoch: 85 [46208/50048]	Loss: 0.0980
Training Epoch: 85 [46336/50048]	Loss: 0.1381
Training Epoch: 85 [46464/50048]	Loss: 0.0456
Training Epoch: 85 [46592/50048]	Loss: 0.0300
Training Epoch: 85 [46720/50048]	Loss: 0.0890
Training Epoch: 85 [46848/50048]	Loss: 0.0874
Training Epoch: 85 [46976/50048]	Loss: 0.1615
Training Epoch: 85 [47104/50048]	Loss: 0.1614
Training Epoch: 85 [47232/50048]	Loss: 0.1046
Training Epoch: 85 [47360/50048]	Loss: 0.1045
Training Epoch: 85 [47488/50048]	Loss: 0.1181
Training Epoch: 85 [47616/50048]	Loss: 0.0834
Training Epoch: 85 [47744/50048]	Loss: 0.0994
Training Epoch: 85 [47872/50048]	Loss: 0.0904
Training Epoch: 85 [48000/50048]	Loss: 0.1281
Training Epoch: 85 [48128/50048]	Loss: 0.1220
Training Epoch: 85 [48256/50048]	Loss: 0.0924
Training Epoch: 85 [48384/50048]	Loss: 0.1237
Training Epoch: 85 [48512/50048]	Loss: 0.2340
Training Epoch: 85 [48640/50048]	Loss: 0.0698
Training Epoch: 85 [48768/50048]	Loss: 0.0453
Training Epoch: 85 [48896/50048]	Loss: 0.0794
Training Epoch: 85 [49024/50048]	Loss: 0.1056
Training Epoch: 85 [49152/50048]	Loss: 0.0745
Training Epoch: 85 [49280/50048]	Loss: 0.2493
Training Epoch: 85 [49408/50048]	Loss: 0.0951
Training Epoch: 85 [49536/50048]	Loss: 0.0642
Training Epoch: 85 [49664/50048]	Loss: 0.1293
Training Epoch: 85 [49792/50048]	Loss: 0.0582
Training Epoch: 85 [49920/50048]	Loss: 0.0666
Training Epoch: 85 [50048/50048]	Loss: 0.0520
2022-12-06 13:17:13.042 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 08:17:13,062 [ZeusDataLoader(eval)] eval epoch 86 done: time=3.77 energy=452.36
2022-12-06 08:17:13,063 [ZeusDataLoader(train)] Up to epoch 86: time=7757.51, energy=941667.07, cost=1149615.52
2022-12-06 08:17:13,063 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 08:17:13,063 [ZeusDataLoader(train)] Expected next epoch: time=7847.31, energy=952465.09, cost=1162871.90
2022-12-06 08:17:13,064 [ZeusDataLoader(train)] Epoch 87 begin.
Validation Epoch: 85, Average loss: 0.0178, Accuracy: 0.6388
2022-12-06 08:17:13,252 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:17:13,253 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:17:13.254 [ZeusMonitor] Monitor started.
2022-12-06 13:17:13.254 [ZeusMonitor] Running indefinitely. 2022-12-06 13:17:13.254 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:17:13.254 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e87+gpu0.power.log
Training Epoch: 86 [128/50048]	Loss: 0.1736
Training Epoch: 86 [256/50048]	Loss: 0.1406
Training Epoch: 86 [384/50048]	Loss: 0.0541
Training Epoch: 86 [512/50048]	Loss: 0.0788
Training Epoch: 86 [640/50048]	Loss: 0.0617
Training Epoch: 86 [768/50048]	Loss: 0.0433
Training Epoch: 86 [896/50048]	Loss: 0.1058
Training Epoch: 86 [1024/50048]	Loss: 0.1160
Training Epoch: 86 [1152/50048]	Loss: 0.0477
Training Epoch: 86 [1280/50048]	Loss: 0.0618
Training Epoch: 86 [1408/50048]	Loss: 0.0735
Training Epoch: 86 [1536/50048]	Loss: 0.0390
Training Epoch: 86 [1664/50048]	Loss: 0.0734
Training Epoch: 86 [1792/50048]	Loss: 0.0566
Training Epoch: 86 [1920/50048]	Loss: 0.0807
Training Epoch: 86 [2048/50048]	Loss: 0.0747
Training Epoch: 86 [2176/50048]	Loss: 0.1173
Training Epoch: 86 [2304/50048]	Loss: 0.0645
Training Epoch: 86 [2432/50048]	Loss: 0.0653
Training Epoch: 86 [2560/50048]	Loss: 0.0803
Training Epoch: 86 [2688/50048]	Loss: 0.1033
Training Epoch: 86 [2816/50048]	Loss: 0.0811
Training Epoch: 86 [2944/50048]	Loss: 0.0873
Training Epoch: 86 [3072/50048]	Loss: 0.0443
Training Epoch: 86 [3200/50048]	Loss: 0.0917
Training Epoch: 86 [3328/50048]	Loss: 0.0808
Training Epoch: 86 [3456/50048]	Loss: 0.0653
Training Epoch: 86 [3584/50048]	Loss: 0.1101
Training Epoch: 86 [3712/50048]	Loss: 0.0462
Training Epoch: 86 [3840/50048]	Loss: 0.0408
Training Epoch: 86 [3968/50048]	Loss: 0.0415
Training Epoch: 86 [4096/50048]	Loss: 0.0317
Training Epoch: 86 [4224/50048]	Loss: 0.0611
Training Epoch: 86 [4352/50048]	Loss: 0.0906
Training Epoch: 86 [4480/50048]	Loss: 0.1088
Training Epoch: 86 [4608/50048]	Loss: 0.1010
Training Epoch: 86 [4736/50048]	Loss: 0.0821
Training Epoch: 86 [4864/50048]	Loss: 0.1140
Training Epoch: 86 [4992/50048]	Loss: 0.0660
Training Epoch: 86 [5120/50048]	Loss: 0.0875
Training Epoch: 86 [5248/50048]	Loss: 0.0716
Training Epoch: 86 [5376/50048]	Loss: 0.0673
Training Epoch: 86 [5504/50048]	Loss: 0.0935
Training Epoch: 86 [5632/50048]	Loss: 0.1308
Training Epoch: 86 [5760/50048]	Loss: 0.0858
Training Epoch: 86 [5888/50048]	Loss: 0.0902
Training Epoch: 86 [6016/50048]	Loss: 0.0567
Training Epoch: 86 [6144/50048]	Loss: 0.0678
Training Epoch: 86 [6272/50048]	Loss: 0.0386
Training Epoch: 86 [6400/50048]	Loss: 0.0679
Training Epoch: 86 [6528/50048]	Loss: 0.0943
Training Epoch: 86 [6656/50048]	Loss: 0.1087
Training Epoch: 86 [6784/50048]	Loss: 0.0700
Training Epoch: 86 [6912/50048]	Loss: 0.0803
Training Epoch: 86 [7040/50048]	Loss: 0.0829
Training Epoch: 86 [7168/50048]	Loss: 0.0890
Training Epoch: 86 [7296/50048]	Loss: 0.0747
Training Epoch: 86 [7424/50048]	Loss: 0.0981
Training Epoch: 86 [7552/50048]	Loss: 0.1403
Training Epoch: 86 [7680/50048]	Loss: 0.1122
Training Epoch: 86 [7808/50048]	Loss: 0.1107
Training Epoch: 86 [7936/50048]	Loss: 0.1055
Training Epoch: 86 [8064/50048]	Loss: 0.0659
Training Epoch: 86 [8192/50048]	Loss: 0.1294
Training Epoch: 86 [8320/50048]	Loss: 0.2423
Training Epoch: 86 [8448/50048]	Loss: 0.0708
Training Epoch: 86 [8576/50048]	Loss: 0.0670
Training Epoch: 86 [8704/50048]	Loss: 0.0885
Training Epoch: 86 [8832/50048]	Loss: 0.1009
Training Epoch: 86 [8960/50048]	Loss: 0.0340
Training Epoch: 86 [9088/50048]	Loss: 0.0747
Training Epoch: 86 [9216/50048]	Loss: 0.1297
Training Epoch: 86 [9344/50048]	Loss: 0.1396
Training Epoch: 86 [9472/50048]	Loss: 0.0735
Training Epoch: 86 [9600/50048]	Loss: 0.0685
Training Epoch: 86 [9728/50048]	Loss: 0.0830
Training Epoch: 86 [9856/50048]	Loss: 0.1208
Training Epoch: 86 [9984/50048]	Loss: 0.0543
Training Epoch: 86 [10112/50048]	Loss: 0.0837
Training Epoch: 86 [10240/50048]	Loss: 0.0377
Training Epoch: 86 [10368/50048]	Loss: 0.0633
Training Epoch: 86 [10496/50048]	Loss: 0.0445
Training Epoch: 86 [10624/50048]	Loss: 0.0948
Training Epoch: 86 [10752/50048]	Loss: 0.1342
Training Epoch: 86 [10880/50048]	Loss: 0.1152
Training Epoch: 86 [11008/50048]	Loss: 0.0717
Training Epoch: 86 [11136/50048]	Loss: 0.1025
Training Epoch: 86 [11264/50048]	Loss: 0.0750
Training Epoch: 86 [11392/50048]	Loss: 0.1540
Training Epoch: 86 [11520/50048]	Loss: 0.0903
Training Epoch: 86 [11648/50048]	Loss: 0.1529
Training Epoch: 86 [11776/50048]	Loss: 0.1131
Training Epoch: 86 [11904/50048]	Loss: 0.0796
Training Epoch: 86 [12032/50048]	Loss: 0.0559
Training Epoch: 86 [12160/50048]	Loss: 0.0968
Training Epoch: 86 [12288/50048]	Loss: 0.0636
Training Epoch: 86 [12416/50048]	Loss: 0.0941
Training Epoch: 86 [12544/50048]	Loss: 0.1439
Training Epoch: 86 [12672/50048]	Loss: 0.1370
Training Epoch: 86 [12800/50048]	Loss: 0.1295
Training Epoch: 86 [12928/50048]	Loss: 0.0509
Training Epoch: 86 [13056/50048]	Loss: 0.0955
Training Epoch: 86 [13184/50048]	Loss: 0.0985
Training Epoch: 86 [13312/50048]	Loss: 0.0583
Training Epoch: 86 [13440/50048]	Loss: 0.0969
Training Epoch: 86 [13568/50048]	Loss: 0.0561
Training Epoch: 86 [13696/50048]	Loss: 0.0760
Training Epoch: 86 [13824/50048]	Loss: 0.0839
Training Epoch: 86 [13952/50048]	Loss: 0.0990
Training Epoch: 86 [14080/50048]	Loss: 0.1155
Training Epoch: 86 [14208/50048]	Loss: 0.0364
Training Epoch: 86 [14336/50048]	Loss: 0.1122
Training Epoch: 86 [14464/50048]	Loss: 0.1098
Training Epoch: 86 [14592/50048]	Loss: 0.0676
Training Epoch: 86 [14720/50048]	Loss: 0.1182
Training Epoch: 86 [14848/50048]	Loss: 0.1251
Training Epoch: 86 [14976/50048]	Loss: 0.0804
Training Epoch: 86 [15104/50048]	Loss: 0.1502
Training Epoch: 86 [15232/50048]	Loss: 0.1853
Training Epoch: 86 [15360/50048]	Loss: 0.0647
Training Epoch: 86 [15488/50048]	Loss: 0.1111
Training Epoch: 86 [15616/50048]	Loss: 0.1848
Training Epoch: 86 [15744/50048]	Loss: 0.0842
Training Epoch: 86 [15872/50048]	Loss: 0.1417
Training Epoch: 86 [16000/50048]	Loss: 0.1169
Training Epoch: 86 [16128/50048]	Loss: 0.0792
Training Epoch: 86 [16256/50048]	Loss: 0.1414
Training Epoch: 86 [16384/50048]	Loss: 0.0716
Training Epoch: 86 [16512/50048]	Loss: 0.1163
Training Epoch: 86 [16640/50048]	Loss: 0.0954
Training Epoch: 86 [16768/50048]	Loss: 0.0424
Training Epoch: 86 [16896/50048]	Loss: 0.1445
Training Epoch: 86 [17024/50048]	Loss: 0.0811
Training Epoch: 86 [17152/50048]	Loss: 0.1184
Training Epoch: 86 [17280/50048]	Loss: 0.1463
Training Epoch: 86 [17408/50048]	Loss: 0.1036
Training Epoch: 86 [17536/50048]	Loss: 0.0531
Training Epoch: 86 [17664/50048]	Loss: 0.0770
Training Epoch: 86 [17792/50048]	Loss: 0.1358
Training Epoch: 86 [17920/50048]	Loss: 0.0651
Training Epoch: 86 [18048/50048]	Loss: 0.0692
Training Epoch: 86 [18176/50048]	Loss: 0.2162
Training Epoch: 86 [18304/50048]	Loss: 0.1015
Training Epoch: 86 [18432/50048]	Loss: 0.0748
Training Epoch: 86 [18560/50048]	Loss: 0.0913
Training Epoch: 86 [18688/50048]	Loss: 0.0681
Training Epoch: 86 [18816/50048]	Loss: 0.2086
Training Epoch: 86 [18944/50048]	Loss: 0.1020
Training Epoch: 86 [19072/50048]	Loss: 0.0736
Training Epoch: 86 [19200/50048]	Loss: 0.0991
Training Epoch: 86 [19328/50048]	Loss: 0.1180
Training Epoch: 86 [19456/50048]	Loss: 0.0811
Training Epoch: 86 [19584/50048]	Loss: 0.0692
Training Epoch: 86 [19712/50048]	Loss: 0.1040
Training Epoch: 86 [19840/50048]	Loss: 0.0976
Training Epoch: 86 [19968/50048]	Loss: 0.1016
Training Epoch: 86 [20096/50048]	Loss: 0.0860
Training Epoch: 86 [20224/50048]	Loss: 0.0443
Training Epoch: 86 [20352/50048]	Loss: 0.0977
Training Epoch: 86 [20480/50048]	Loss: 0.0798
Training Epoch: 86 [20608/50048]	Loss: 0.1776
Training Epoch: 86 [20736/50048]	Loss: 0.0549
Training Epoch: 86 [20864/50048]	Loss: 0.0644
Training Epoch: 86 [20992/50048]	Loss: 0.0949
Training Epoch: 86 [21120/50048]	Loss: 0.0608
Training Epoch: 86 [21248/50048]	Loss: 0.1003
Training Epoch: 86 [21376/50048]	Loss: 0.0410
Training Epoch: 86 [21504/50048]	Loss: 0.0989
Training Epoch: 86 [21632/50048]	Loss: 0.0572
Training Epoch: 86 [21760/50048]	Loss: 0.1061
Training Epoch: 86 [21888/50048]	Loss: 0.1482
Training Epoch: 86 [22016/50048]	Loss: 0.0442
Training Epoch: 86 [22144/50048]	Loss: 0.0573
Training Epoch: 86 [22272/50048]	Loss: 0.1433
Training Epoch: 86 [22400/50048]	Loss: 0.0664
Training Epoch: 86 [22528/50048]	Loss: 0.0725
Training Epoch: 86 [22656/50048]	Loss: 0.0553
Training Epoch: 86 [22784/50048]	Loss: 0.1099
Training Epoch: 86 [22912/50048]	Loss: 0.0837
Training Epoch: 86 [23040/50048]	Loss: 0.1089
Training Epoch: 86 [23168/50048]	Loss: 0.0983
Training Epoch: 86 [23296/50048]	Loss: 0.0435
Training Epoch: 86 [23424/50048]	Loss: 0.1009
Training Epoch: 86 [23552/50048]	Loss: 0.1137
Training Epoch: 86 [23680/50048]	Loss: 0.0271
Training Epoch: 86 [23808/50048]	Loss: 0.0472
Training Epoch: 86 [23936/50048]	Loss: 0.1182
Training Epoch: 86 [24064/50048]	Loss: 0.1843
Training Epoch: 86 [24192/50048]	Loss: 0.0577
Training Epoch: 86 [24320/50048]	Loss: 0.0664
Training Epoch: 86 [24448/50048]	Loss: 0.0801
Training Epoch: 86 [24576/50048]	Loss: 0.0588
Training Epoch: 86 [24704/50048]	Loss: 0.0567
Training Epoch: 86 [24832/50048]	Loss: 0.0580
Training Epoch: 86 [24960/50048]	Loss: 0.0452
Training Epoch: 86 [25088/50048]	Loss: 0.1730
Training Epoch: 86 [25216/50048]	Loss: 0.0473
Training Epoch: 86 [25344/50048]	Loss: 0.0901
Training Epoch: 86 [25472/50048]	Loss: 0.1538
Training Epoch: 86 [25600/50048]	Loss: 0.0709
Training Epoch: 86 [25728/50048]	Loss: 0.0626
Training Epoch: 86 [25856/50048]	Loss: 0.1031
Training Epoch: 86 [25984/50048]	Loss: 0.0993
Training Epoch: 86 [26112/50048]	Loss: 0.1641
Training Epoch: 86 [26240/50048]	Loss: 0.1282
Training Epoch: 86 [26368/50048]	Loss: 0.1169
Training Epoch: 86 [26496/50048]	Loss: 0.1518
Training Epoch: 86 [26624/50048]	Loss: 0.0683
Training Epoch: 86 [26752/50048]	Loss: 0.0942
Training Epoch: 86 [26880/50048]	Loss: 0.1284
Training Epoch: 86 [27008/50048]	Loss: 0.1034
Training Epoch: 86 [27136/50048]	Loss: 0.0835
Training Epoch: 86 [27264/50048]	Loss: 0.0513
Training Epoch: 86 [27392/50048]	Loss: 0.0801
Training Epoch: 86 [27520/50048]	Loss: 0.1428
Training Epoch: 86 [27648/50048]	Loss: 0.1497
Training Epoch: 86 [27776/50048]	Loss: 0.1502
Training Epoch: 86 [27904/50048]	Loss: 0.1356
Training Epoch: 86 [28032/50048]	Loss: 0.0807
Training Epoch: 86 [28160/50048]	Loss: 0.1026
Training Epoch: 86 [28288/50048]	Loss: 0.0883
Training Epoch: 86 [28416/50048]	Loss: 0.1273
Training Epoch: 86 [28544/50048]	Loss: 0.1016
Training Epoch: 86 [28672/50048]	Loss: 0.1234
Training Epoch: 86 [28800/50048]	Loss: 0.0660
Training Epoch: 86 [28928/50048]	Loss: 0.0956
Training Epoch: 86 [29056/50048]	Loss: 0.0560
Training Epoch: 86 [29184/50048]	Loss: 0.1134
Training Epoch: 86 [29312/50048]	Loss: 0.1442
Training Epoch: 86 [29440/50048]	Loss: 0.0647
Training Epoch: 86 [29568/50048]	Loss: 0.0512
Training Epoch: 86 [29696/50048]	Loss: 0.0678
Training Epoch: 86 [29824/50048]	Loss: 0.1756
Training Epoch: 86 [29952/50048]	Loss: 0.1245
Training Epoch: 86 [30080/50048]	Loss: 0.1295
Training Epoch: 86 [30208/50048]	Loss: 0.1493
Training Epoch: 86 [30336/50048]	Loss: 0.1134
Training Epoch: 86 [30464/50048]	Loss: 0.0831
Training Epoch: 86 [30592/50048]	Loss: 0.1260
Training Epoch: 86 [30720/50048]	Loss: 0.1149
Training Epoch: 86 [30848/50048]	Loss: 0.1316
Training Epoch: 86 [30976/50048]	Loss: 0.0795
Training Epoch: 86 [31104/50048]	Loss: 0.0940
Training Epoch: 86 [31232/50048]	Loss: 0.1473
Training Epoch: 86 [31360/50048]	Loss: 0.1150
Training Epoch: 86 [31488/50048]	Loss: 0.0646
Training Epoch: 86 [31616/50048]	Loss: 0.0871
Training Epoch: 86 [31744/50048]	Loss: 0.0777
Training Epoch: 86 [31872/50048]	Loss: 0.1071
Training Epoch: 86 [32000/50048]	Loss: 0.1176
Training Epoch: 86 [32128/50048]	Loss: 0.0984
Training Epoch: 86 [32256/50048]	Loss: 0.0957
Training Epoch: 86 [32384/50048]	Loss: 0.0700
Training Epoch: 86 [32512/50048]	Loss: 0.0625
Training Epoch: 86 [32640/50048]	Loss: 0.1446
Training Epoch: 86 [32768/50048]	Loss: 0.1260
Training Epoch: 86 [32896/50048]	Loss: 0.1541
Training Epoch: 86 [33024/50048]	Loss: 0.1262
Training Epoch: 86 [33152/50048]	Loss: 0.1081
Training Epoch: 86 [33280/50048]	Loss: 0.1345
Training Epoch: 86 [33408/50048]	Loss: 0.0423
Training Epoch: 86 [33536/50048]	Loss: 0.1013
Training Epoch: 86 [33664/50048]	Loss: 0.0798
Training Epoch: 86 [33792/50048]	Loss: 0.0677
Training Epoch: 86 [33920/50048]	Loss: 0.0978
Training Epoch: 86 [34048/50048]	Loss: 0.0762
Training Epoch: 86 [34176/50048]	Loss: 0.1112
Training Epoch: 86 [34304/50048]	Loss: 0.0596
Training Epoch: 86 [34432/50048]	Loss: 0.0709
Training Epoch: 86 [34560/50048]	Loss: 0.1453
Training Epoch: 86 [34688/50048]	Loss: 0.1706
Training Epoch: 86 [34816/50048]	Loss: 0.0668
Training Epoch: 86 [34944/50048]	Loss: 0.0738
Training Epoch: 86 [35072/50048]	Loss: 0.0958
Training Epoch: 86 [35200/50048]	Loss: 0.1100
Training Epoch: 86 [35328/50048]	Loss: 0.1055
Training Epoch: 86 [35456/50048]	Loss: 0.1202
Training Epoch: 86 [35584/50048]	Loss: 0.1075
Training Epoch: 86 [35712/50048]	Loss: 0.1047
Training Epoch: 86 [35840/50048]	Loss: 0.1920
Training Epoch: 86 [35968/50048]	Loss: 0.1331
Training Epoch: 86 [36096/50048]	Loss: 0.1517
Training Epoch: 86 [36224/50048]	Loss: 0.0818
Training Epoch: 86 [36352/50048]	Loss: 0.1215
Training Epoch: 86 [36480/50048]	Loss: 0.0658
Training Epoch: 86 [36608/50048]	Loss: 0.2181
Training Epoch: 86 [36736/50048]	Loss: 0.1479
Training Epoch: 86 [36864/50048]	Loss: 0.1239
Training Epoch: 86 [36992/50048]	Loss: 0.0670
Training Epoch: 86 [37120/50048]	Loss: 0.0947
Training Epoch: 86 [37248/50048]	Loss: 0.2007
Training Epoch: 86 [37376/50048]	Loss: 0.1373
Training Epoch: 86 [37504/50048]	Loss: 0.0968
Training Epoch: 86 [37632/50048]	Loss: 0.0808
Training Epoch: 86 [37760/50048]	Loss: 0.0484
Training Epoch: 86 [37888/50048]	Loss: 0.0803
Training Epoch: 86 [38016/50048]	Loss: 0.0805
Training Epoch: 86 [38144/50048]	Loss: 0.0915
Training Epoch: 86 [38272/50048]	Loss: 0.2301
Training Epoch: 86 [38400/50048]	Loss: 0.0816
Training Epoch: 86 [38528/50048]	Loss: 0.1282
Training Epoch: 86 [38656/50048]	Loss: 0.0979
Training Epoch: 86 [38784/50048]	Loss: 0.1300
Training Epoch: 86 [38912/50048]	Loss: 0.1127
Training Epoch: 86 [39040/50048]	Loss: 0.1171
Training Epoch: 86 [39168/50048]	Loss: 0.1267
Training Epoch: 86 [39296/50048]	Loss: 0.0834
Training Epoch: 86 [39424/50048]	Loss: 0.2150
Training Epoch: 86 [39552/50048]	Loss: 0.0991
Training Epoch: 86 [39680/50048]	Loss: 0.1613
Training Epoch: 86 [39808/50048]	Loss: 0.0821
Training Epoch: 86 [39936/50048]	Loss: 0.0677
Training Epoch: 86 [40064/50048]	Loss: 0.2155
Training Epoch: 86 [40192/50048]	Loss: 0.0683
Training Epoch: 86 [40320/50048]	Loss: 0.1190
Training Epoch: 86 [40448/50048]	Loss: 0.0978
Training Epoch: 86 [40576/50048]	Loss: 0.0629
Training Epoch: 86 [40704/50048]	Loss: 0.0965
Training Epoch: 86 [40832/50048]	Loss: 0.1135
Training Epoch: 86 [40960/50048]	Loss: 0.1004
Training Epoch: 86 [41088/50048]	Loss: 0.1405
Training Epoch: 86 [41216/50048]	Loss: 0.1100
Training Epoch: 86 [41344/50048]	Loss: 0.0845
Training Epoch: 86 [41472/50048]	Loss: 0.1399
Training Epoch: 86 [41600/50048]	Loss: 0.1130
Training Epoch: 86 [41728/50048]	Loss: 0.1188
Training Epoch: 86 [41856/50048]	Loss: 0.0651
Training Epoch: 86 [41984/50048]	Loss: 0.0977
Training Epoch: 86 [42112/50048]	Loss: 0.0537
Training Epoch: 86 [42240/50048]	Loss: 0.0937
Training Epoch: 86 [42368/50048]	Loss: 0.1803
Training Epoch: 86 [42496/50048]	Loss: 0.0544
Training Epoch: 86 [42624/50048]	Loss: 0.1450
Training Epoch: 86 [42752/50048]	Loss: 0.0259
Training Epoch: 86 [42880/50048]	Loss: 0.0929
Training Epoch: 86 [43008/50048]	Loss: 0.1265
Training Epoch: 86 [43136/50048]	Loss: 0.1781
Training Epoch: 86 [43264/50048]	Loss: 0.1558
Training Epoch: 86 [43392/50048]	Loss: 0.0903
Training Epoch: 86 [43520/50048]	Loss: 0.2005
Training Epoch: 86 [43648/50048]	Loss: 0.0772
Training Epoch: 86 [43776/50048]	Loss: 0.0980
Training Epoch: 86 [43904/50048]	Loss: 0.1302
Training Epoch: 86 [44032/50048]	Loss: 0.1097
Training Epoch: 86 [44160/50048]	Loss: 0.1294
Training Epoch: 86 [44288/50048]	Loss: 0.0884
Training Epoch: 86 [44416/50048]	Loss: 0.1474
Training Epoch: 86 [44544/50048]	Loss: 0.1109
Training Epoch: 86 [44672/50048]	Loss: 0.0956
Training Epoch: 86 [44800/50048]	Loss: 0.1647
Training Epoch: 86 [44928/50048]	Loss: 0.1754
Training Epoch: 86 [45056/50048]	Loss: 0.1440
Training Epoch: 86 [45184/50048]	Loss: 0.0910
Training Epoch: 86 [45312/50048]	Loss: 0.1597
Training Epoch: 86 [45440/50048]	Loss: 0.1127
Training Epoch: 86 [45568/50048]	Loss: 0.1124
Training Epoch: 86 [45696/50048]	Loss: 0.2231
2022-12-06 08:18:39,554 [ZeusDataLoader(train)] train epoch 87 done: time=86.48 energy=10495.44
2022-12-06 08:18:39,556 [ZeusDataLoader(eval)] Epoch 87 begin.
Training Epoch: 86 [45824/50048]	Loss: 0.0961
Training Epoch: 86 [45952/50048]	Loss: 0.1176
Training Epoch: 86 [46080/50048]	Loss: 0.1858
Training Epoch: 86 [46208/50048]	Loss: 0.0835
Training Epoch: 86 [46336/50048]	Loss: 0.1402
Training Epoch: 86 [46464/50048]	Loss: 0.0976
Training Epoch: 86 [46592/50048]	Loss: 0.1070
Training Epoch: 86 [46720/50048]	Loss: 0.1025
Training Epoch: 86 [46848/50048]	Loss: 0.0963
Training Epoch: 86 [46976/50048]	Loss: 0.1349
Training Epoch: 86 [47104/50048]	Loss: 0.1155
Training Epoch: 86 [47232/50048]	Loss: 0.1601
Training Epoch: 86 [47360/50048]	Loss: 0.0960
Training Epoch: 86 [47488/50048]	Loss: 0.1322
Training Epoch: 86 [47616/50048]	Loss: 0.1070
Training Epoch: 86 [47744/50048]	Loss: 0.0886
Training Epoch: 86 [47872/50048]	Loss: 0.1262
Training Epoch: 86 [48000/50048]	Loss: 0.1131
Training Epoch: 86 [48128/50048]	Loss: 0.1877
Training Epoch: 86 [48256/50048]	Loss: 0.0689
Training Epoch: 86 [48384/50048]	Loss: 0.0829
Training Epoch: 86 [48512/50048]	Loss: 0.0915
Training Epoch: 86 [48640/50048]	Loss: 0.1141
Training Epoch: 86 [48768/50048]	Loss: 0.1092
Training Epoch: 86 [48896/50048]	Loss: 0.1567
Training Epoch: 86 [49024/50048]	Loss: 0.1521
Training Epoch: 86 [49152/50048]	Loss: 0.1167
Training Epoch: 86 [49280/50048]	Loss: 0.0756
Training Epoch: 86 [49408/50048]	Loss: 0.1120
Training Epoch: 86 [49536/50048]	Loss: 0.0747
Training Epoch: 86 [49664/50048]	Loss: 0.1126
Training Epoch: 86 [49792/50048]	Loss: 0.1138
Training Epoch: 86 [49920/50048]	Loss: 0.0783
Training Epoch: 86 [50048/50048]	Loss: 0.2073
2022-12-06 13:18:43.221 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 08:18:43,240 [ZeusDataLoader(eval)] eval epoch 87 done: time=3.68 energy=440.19
2022-12-06 08:18:43,241 [ZeusDataLoader(train)] Up to epoch 87: time=7847.66, energy=952602.70, cost=1162971.97
2022-12-06 08:18:43,241 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 08:18:43,241 [ZeusDataLoader(train)] Expected next epoch: time=7937.46, energy=963400.71, cost=1176228.35
2022-12-06 08:18:43,242 [ZeusDataLoader(train)] Epoch 88 begin.
Validation Epoch: 86, Average loss: 0.0185, Accuracy: 0.6380
2022-12-06 08:18:43,389 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:18:43,390 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:18:43.392 [ZeusMonitor] Monitor started.
2022-12-06 13:18:43.392 [ZeusMonitor] Running indefinitely. 2022-12-06 13:18:43.392 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:18:43.392 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e88+gpu0.power.log
Training Epoch: 87 [128/50048]	Loss: 0.0809
Training Epoch: 87 [256/50048]	Loss: 0.0781
Training Epoch: 87 [384/50048]	Loss: 0.1081
Training Epoch: 87 [512/50048]	Loss: 0.1932
Training Epoch: 87 [640/50048]	Loss: 0.1743
Training Epoch: 87 [768/50048]	Loss: 0.0800
Training Epoch: 87 [896/50048]	Loss: 0.1366
Training Epoch: 87 [1024/50048]	Loss: 0.0737
Training Epoch: 87 [1152/50048]	Loss: 0.0745
Training Epoch: 87 [1280/50048]	Loss: 0.0623
Training Epoch: 87 [1408/50048]	Loss: 0.0938
Training Epoch: 87 [1536/50048]	Loss: 0.1035
Training Epoch: 87 [1664/50048]	Loss: 0.0938
Training Epoch: 87 [1792/50048]	Loss: 0.0620
Training Epoch: 87 [1920/50048]	Loss: 0.1188
Training Epoch: 87 [2048/50048]	Loss: 0.1356
Training Epoch: 87 [2176/50048]	Loss: 0.1017
Training Epoch: 87 [2304/50048]	Loss: 0.0553
Training Epoch: 87 [2432/50048]	Loss: 0.0471
Training Epoch: 87 [2560/50048]	Loss: 0.0566
Training Epoch: 87 [2688/50048]	Loss: 0.0635
Training Epoch: 87 [2816/50048]	Loss: 0.0882
Training Epoch: 87 [2944/50048]	Loss: 0.1698
Training Epoch: 87 [3072/50048]	Loss: 0.0455
Training Epoch: 87 [3200/50048]	Loss: 0.0691
Training Epoch: 87 [3328/50048]	Loss: 0.0935
Training Epoch: 87 [3456/50048]	Loss: 0.1108
Training Epoch: 87 [3584/50048]	Loss: 0.0685
Training Epoch: 87 [3712/50048]	Loss: 0.0832
Training Epoch: 87 [3840/50048]	Loss: 0.0802
Training Epoch: 87 [3968/50048]	Loss: 0.0360
Training Epoch: 87 [4096/50048]	Loss: 0.1243
Training Epoch: 87 [4224/50048]	Loss: 0.1459
Training Epoch: 87 [4352/50048]	Loss: 0.0617
Training Epoch: 87 [4480/50048]	Loss: 0.1106
Training Epoch: 87 [4608/50048]	Loss: 0.0559
Training Epoch: 87 [4736/50048]	Loss: 0.1317
Training Epoch: 87 [4864/50048]	Loss: 0.0882
Training Epoch: 87 [4992/50048]	Loss: 0.1191
Training Epoch: 87 [5120/50048]	Loss: 0.1165
Training Epoch: 87 [5248/50048]	Loss: 0.0885
Training Epoch: 87 [5376/50048]	Loss: 0.0553
Training Epoch: 87 [5504/50048]	Loss: 0.1075
Training Epoch: 87 [5632/50048]	Loss: 0.0795
Training Epoch: 87 [5760/50048]	Loss: 0.0416
Training Epoch: 87 [5888/50048]	Loss: 0.0383
Training Epoch: 87 [6016/50048]	Loss: 0.0317
Training Epoch: 87 [6144/50048]	Loss: 0.0685
Training Epoch: 87 [6272/50048]	Loss: 0.2638
Training Epoch: 87 [6400/50048]	Loss: 0.1240
Training Epoch: 87 [6528/50048]	Loss: 0.0465
Training Epoch: 87 [6656/50048]	Loss: 0.0872
Training Epoch: 87 [6784/50048]	Loss: 0.0305
Training Epoch: 87 [6912/50048]	Loss: 0.0663
Training Epoch: 87 [7040/50048]	Loss: 0.0659
Training Epoch: 87 [7168/50048]	Loss: 0.1533
Training Epoch: 87 [7296/50048]	Loss: 0.1690
Training Epoch: 87 [7424/50048]	Loss: 0.1122
Training Epoch: 87 [7552/50048]	Loss: 0.0748
Training Epoch: 87 [7680/50048]	Loss: 0.0564
Training Epoch: 87 [7808/50048]	Loss: 0.1808
Training Epoch: 87 [7936/50048]	Loss: 0.0585
Training Epoch: 87 [8064/50048]	Loss: 0.0853
Training Epoch: 87 [8192/50048]	Loss: 0.0989
Training Epoch: 87 [8320/50048]	Loss: 0.0681
Training Epoch: 87 [8448/50048]	Loss: 0.0813
Training Epoch: 87 [8576/50048]	Loss: 0.1257
Training Epoch: 87 [8704/50048]	Loss: 0.0767
Training Epoch: 87 [8832/50048]	Loss: 0.0749
Training Epoch: 87 [8960/50048]	Loss: 0.0942
Training Epoch: 87 [9088/50048]	Loss: 0.1684
Training Epoch: 87 [9216/50048]	Loss: 0.1386
Training Epoch: 87 [9344/50048]	Loss: 0.0578
Training Epoch: 87 [9472/50048]	Loss: 0.1113
Training Epoch: 87 [9600/50048]	Loss: 0.0557
Training Epoch: 87 [9728/50048]	Loss: 0.0701
Training Epoch: 87 [9856/50048]	Loss: 0.0913
Training Epoch: 87 [9984/50048]	Loss: 0.1064
Training Epoch: 87 [10112/50048]	Loss: 0.1468
Training Epoch: 87 [10240/50048]	Loss: 0.1752
Training Epoch: 87 [10368/50048]	Loss: 0.0939
Training Epoch: 87 [10496/50048]	Loss: 0.0790
Training Epoch: 87 [10624/50048]	Loss: 0.1010
Training Epoch: 87 [10752/50048]	Loss: 0.0624
Training Epoch: 87 [10880/50048]	Loss: 0.0919
Training Epoch: 87 [11008/50048]	Loss: 0.0907
Training Epoch: 87 [11136/50048]	Loss: 0.0622
Training Epoch: 87 [11264/50048]	Loss: 0.0719
Training Epoch: 87 [11392/50048]	Loss: 0.1020
Training Epoch: 87 [11520/50048]	Loss: 0.1329
Training Epoch: 87 [11648/50048]	Loss: 0.0976
Training Epoch: 87 [11776/50048]	Loss: 0.0642
Training Epoch: 87 [11904/50048]	Loss: 0.1051
Training Epoch: 87 [12032/50048]	Loss: 0.0787
Training Epoch: 87 [12160/50048]	Loss: 0.0849
Training Epoch: 87 [12288/50048]	Loss: 0.0697
Training Epoch: 87 [12416/50048]	Loss: 0.0623
Training Epoch: 87 [12544/50048]	Loss: 0.1375
Training Epoch: 87 [12672/50048]	Loss: 0.0533
Training Epoch: 87 [12800/50048]	Loss: 0.1291
Training Epoch: 87 [12928/50048]	Loss: 0.0673
Training Epoch: 87 [13056/50048]	Loss: 0.0700
Training Epoch: 87 [13184/50048]	Loss: 0.0627
Training Epoch: 87 [13312/50048]	Loss: 0.0622
Training Epoch: 87 [13440/50048]	Loss: 0.1075
Training Epoch: 87 [13568/50048]	Loss: 0.0762
Training Epoch: 87 [13696/50048]	Loss: 0.1237
Training Epoch: 87 [13824/50048]	Loss: 0.1188
Training Epoch: 87 [13952/50048]	Loss: 0.0512
Training Epoch: 87 [14080/50048]	Loss: 0.1304
Training Epoch: 87 [14208/50048]	Loss: 0.1377
Training Epoch: 87 [14336/50048]	Loss: 0.0978
Training Epoch: 87 [14464/50048]	Loss: 0.1026
Training Epoch: 87 [14592/50048]	Loss: 0.0454
Training Epoch: 87 [14720/50048]	Loss: 0.0567
Training Epoch: 87 [14848/50048]	Loss: 0.0697
Training Epoch: 87 [14976/50048]	Loss: 0.0851
Training Epoch: 87 [15104/50048]	Loss: 0.1161
Training Epoch: 87 [15232/50048]	Loss: 0.1206
Training Epoch: 87 [15360/50048]	Loss: 0.0649
Training Epoch: 87 [15488/50048]	Loss: 0.1243
Training Epoch: 87 [15616/50048]	Loss: 0.0771
Training Epoch: 87 [15744/50048]	Loss: 0.1736
Training Epoch: 87 [15872/50048]	Loss: 0.0615
Training Epoch: 87 [16000/50048]	Loss: 0.1776
Training Epoch: 87 [16128/50048]	Loss: 0.0708
Training Epoch: 87 [16256/50048]	Loss: 0.1241
Training Epoch: 87 [16384/50048]	Loss: 0.1596
Training Epoch: 87 [16512/50048]	Loss: 0.0696
Training Epoch: 87 [16640/50048]	Loss: 0.0679
Training Epoch: 87 [16768/50048]	Loss: 0.0597
Training Epoch: 87 [16896/50048]	Loss: 0.0853
Training Epoch: 87 [17024/50048]	Loss: 0.1202
Training Epoch: 87 [17152/50048]	Loss: 0.0602
Training Epoch: 87 [17280/50048]	Loss: 0.1469
Training Epoch: 87 [17408/50048]	Loss: 0.0651
Training Epoch: 87 [17536/50048]	Loss: 0.1520
Training Epoch: 87 [17664/50048]	Loss: 0.0477
Training Epoch: 87 [17792/50048]	Loss: 0.0981
Training Epoch: 87 [17920/50048]	Loss: 0.1062
Training Epoch: 87 [18048/50048]	Loss: 0.0787
Training Epoch: 87 [18176/50048]	Loss: 0.0865
Training Epoch: 87 [18304/50048]	Loss: 0.1189
Training Epoch: 87 [18432/50048]	Loss: 0.0763
Training Epoch: 87 [18560/50048]	Loss: 0.0783
Training Epoch: 87 [18688/50048]	Loss: 0.1424
Training Epoch: 87 [18816/50048]	Loss: 0.1193
Training Epoch: 87 [18944/50048]	Loss: 0.0591
Training Epoch: 87 [19072/50048]	Loss: 0.0845
Training Epoch: 87 [19200/50048]	Loss: 0.1096
Training Epoch: 87 [19328/50048]	Loss: 0.1256
Training Epoch: 87 [19456/50048]	Loss: 0.0877
Training Epoch: 87 [19584/50048]	Loss: 0.1176
Training Epoch: 87 [19712/50048]	Loss: 0.0382
Training Epoch: 87 [19840/50048]	Loss: 0.0876
Training Epoch: 87 [19968/50048]	Loss: 0.0659
Training Epoch: 87 [20096/50048]	Loss: 0.0700
Training Epoch: 87 [20224/50048]	Loss: 0.0646
Training Epoch: 87 [20352/50048]	Loss: 0.0995
Training Epoch: 87 [20480/50048]	Loss: 0.1204
Training Epoch: 87 [20608/50048]	Loss: 0.0715
Training Epoch: 87 [20736/50048]	Loss: 0.0476
Training Epoch: 87 [20864/50048]	Loss: 0.0539
Training Epoch: 87 [20992/50048]	Loss: 0.1326
Training Epoch: 87 [21120/50048]	Loss: 0.0794
Training Epoch: 87 [21248/50048]	Loss: 0.1847
Training Epoch: 87 [21376/50048]	Loss: 0.0422
Training Epoch: 87 [21504/50048]	Loss: 0.0834
Training Epoch: 87 [21632/50048]	Loss: 0.1344
Training Epoch: 87 [21760/50048]	Loss: 0.1116
Training Epoch: 87 [21888/50048]	Loss: 0.0871
Training Epoch: 87 [22016/50048]	Loss: 0.0684
Training Epoch: 87 [22144/50048]	Loss: 0.1172
Training Epoch: 87 [22272/50048]	Loss: 0.0713
Training Epoch: 87 [22400/50048]	Loss: 0.0633
Training Epoch: 87 [22528/50048]	Loss: 0.0583
Training Epoch: 87 [22656/50048]	Loss: 0.0837
Training Epoch: 87 [22784/50048]	Loss: 0.1055
Training Epoch: 87 [22912/50048]	Loss: 0.0673
Training Epoch: 87 [23040/50048]	Loss: 0.0800
Training Epoch: 87 [23168/50048]	Loss: 0.0853
Training Epoch: 87 [23296/50048]	Loss: 0.0692
Training Epoch: 87 [23424/50048]	Loss: 0.0767
Training Epoch: 87 [23552/50048]	Loss: 0.0383
Training Epoch: 87 [23680/50048]	Loss: 0.0741
Training Epoch: 87 [23808/50048]	Loss: 0.0898
Training Epoch: 87 [23936/50048]	Loss: 0.0678
Training Epoch: 87 [24064/50048]	Loss: 0.0638
Training Epoch: 87 [24192/50048]	Loss: 0.1016
Training Epoch: 87 [24320/50048]	Loss: 0.0776
Training Epoch: 87 [24448/50048]	Loss: 0.0680
Training Epoch: 87 [24576/50048]	Loss: 0.1295
Training Epoch: 87 [24704/50048]	Loss: 0.0870
Training Epoch: 87 [24832/50048]	Loss: 0.0789
Training Epoch: 87 [24960/50048]	Loss: 0.0648
Training Epoch: 87 [25088/50048]	Loss: 0.0898
Training Epoch: 87 [25216/50048]	Loss: 0.0929
Training Epoch: 87 [25344/50048]	Loss: 0.1595
Training Epoch: 87 [25472/50048]	Loss: 0.1520
Training Epoch: 87 [25600/50048]	Loss: 0.0617
Training Epoch: 87 [25728/50048]	Loss: 0.0622
Training Epoch: 87 [25856/50048]	Loss: 0.0925
Training Epoch: 87 [25984/50048]	Loss: 0.0670
Training Epoch: 87 [26112/50048]	Loss: 0.1302
Training Epoch: 87 [26240/50048]	Loss: 0.0863
Training Epoch: 87 [26368/50048]	Loss: 0.0722
Training Epoch: 87 [26496/50048]	Loss: 0.1491
Training Epoch: 87 [26624/50048]	Loss: 0.0462
Training Epoch: 87 [26752/50048]	Loss: 0.0545
Training Epoch: 87 [26880/50048]	Loss: 0.0611
Training Epoch: 87 [27008/50048]	Loss: 0.0966
Training Epoch: 87 [27136/50048]	Loss: 0.0802
Training Epoch: 87 [27264/50048]	Loss: 0.0768
Training Epoch: 87 [27392/50048]	Loss: 0.0757
Training Epoch: 87 [27520/50048]	Loss: 0.0995
Training Epoch: 87 [27648/50048]	Loss: 0.1347
Training Epoch: 87 [27776/50048]	Loss: 0.1053
Training Epoch: 87 [27904/50048]	Loss: 0.0736
Training Epoch: 87 [28032/50048]	Loss: 0.1398
Training Epoch: 87 [28160/50048]	Loss: 0.1295
Training Epoch: 87 [28288/50048]	Loss: 0.0715
Training Epoch: 87 [28416/50048]	Loss: 0.0917
Training Epoch: 87 [28544/50048]	Loss: 0.1021
Training Epoch: 87 [28672/50048]	Loss: 0.0931
Training Epoch: 87 [28800/50048]	Loss: 0.0801
Training Epoch: 87 [28928/50048]	Loss: 0.0508
Training Epoch: 87 [29056/50048]	Loss: 0.0724
Training Epoch: 87 [29184/50048]	Loss: 0.0732
Training Epoch: 87 [29312/50048]	Loss: 0.0632
Training Epoch: 87 [29440/50048]	Loss: 0.0505
Training Epoch: 87 [29568/50048]	Loss: 0.0911
Training Epoch: 87 [29696/50048]	Loss: 0.0482
Training Epoch: 87 [29824/50048]	Loss: 0.0534
Training Epoch: 87 [29952/50048]	Loss: 0.0509
Training Epoch: 87 [30080/50048]	Loss: 0.0617
Training Epoch: 87 [30208/50048]	Loss: 0.1116
Training Epoch: 87 [30336/50048]	Loss: 0.1409
Training Epoch: 87 [30464/50048]	Loss: 0.1224
Training Epoch: 87 [30592/50048]	Loss: 0.0649
Training Epoch: 87 [30720/50048]	Loss: 0.0718
Training Epoch: 87 [30848/50048]	Loss: 0.1143
Training Epoch: 87 [30976/50048]	Loss: 0.0817
Training Epoch: 87 [31104/50048]	Loss: 0.1165
Training Epoch: 87 [31232/50048]	Loss: 0.0659
Training Epoch: 87 [31360/50048]	Loss: 0.0798
Training Epoch: 87 [31488/50048]	Loss: 0.0751
Training Epoch: 87 [31616/50048]	Loss: 0.0736
Training Epoch: 87 [31744/50048]	Loss: 0.0992
Training Epoch: 87 [31872/50048]	Loss: 0.1634
Training Epoch: 87 [32000/50048]	Loss: 0.1341
Training Epoch: 87 [32128/50048]	Loss: 0.1932
Training Epoch: 87 [32256/50048]	Loss: 0.1869
Training Epoch: 87 [32384/50048]	Loss: 0.1061
Training Epoch: 87 [32512/50048]	Loss: 0.1037
Training Epoch: 87 [32640/50048]	Loss: 0.0602
Training Epoch: 87 [32768/50048]	Loss: 0.0641
Training Epoch: 87 [32896/50048]	Loss: 0.0696
Training Epoch: 87 [33024/50048]	Loss: 0.0597
Training Epoch: 87 [33152/50048]	Loss: 0.1187
Training Epoch: 87 [33280/50048]	Loss: 0.0801
Training Epoch: 87 [33408/50048]	Loss: 0.0646
Training Epoch: 87 [33536/50048]	Loss: 0.1083
Training Epoch: 87 [33664/50048]	Loss: 0.1304
Training Epoch: 87 [33792/50048]	Loss: 0.1182
Training Epoch: 87 [33920/50048]	Loss: 0.1114
Training Epoch: 87 [34048/50048]	Loss: 0.0914
Training Epoch: 87 [34176/50048]	Loss: 0.0842
Training Epoch: 87 [34304/50048]	Loss: 0.0773
Training Epoch: 87 [34432/50048]	Loss: 0.1105
Training Epoch: 87 [34560/50048]	Loss: 0.1156
Training Epoch: 87 [34688/50048]	Loss: 0.0897
Training Epoch: 87 [34816/50048]	Loss: 0.0844
Training Epoch: 87 [34944/50048]	Loss: 0.0618
Training Epoch: 87 [35072/50048]	Loss: 0.1458
Training Epoch: 87 [35200/50048]	Loss: 0.1272
Training Epoch: 87 [35328/50048]	Loss: 0.0456
Training Epoch: 87 [35456/50048]	Loss: 0.0755
Training Epoch: 87 [35584/50048]	Loss: 0.1240
Training Epoch: 87 [35712/50048]	Loss: 0.0770
Training Epoch: 87 [35840/50048]	Loss: 0.1060
Training Epoch: 87 [35968/50048]	Loss: 0.0364
Training Epoch: 87 [36096/50048]	Loss: 0.0579
Training Epoch: 87 [36224/50048]	Loss: 0.0359
Training Epoch: 87 [36352/50048]	Loss: 0.0702
Training Epoch: 87 [36480/50048]	Loss: 0.0791
Training Epoch: 87 [36608/50048]	Loss: 0.0858
Training Epoch: 87 [36736/50048]	Loss: 0.1454
Training Epoch: 87 [36864/50048]	Loss: 0.1355
Training Epoch: 87 [36992/50048]	Loss: 0.1565
Training Epoch: 87 [37120/50048]	Loss: 0.0492
Training Epoch: 87 [37248/50048]	Loss: 0.0880
Training Epoch: 87 [37376/50048]	Loss: 0.0640
Training Epoch: 87 [37504/50048]	Loss: 0.0966
Training Epoch: 87 [37632/50048]	Loss: 0.0672
Training Epoch: 87 [37760/50048]	Loss: 0.0930
Training Epoch: 87 [37888/50048]	Loss: 0.0921
Training Epoch: 87 [38016/50048]	Loss: 0.0959
Training Epoch: 87 [38144/50048]	Loss: 0.0624
Training Epoch: 87 [38272/50048]	Loss: 0.0526
Training Epoch: 87 [38400/50048]	Loss: 0.1971
Training Epoch: 87 [38528/50048]	Loss: 0.0776
Training Epoch: 87 [38656/50048]	Loss: 0.0495
Training Epoch: 87 [38784/50048]	Loss: 0.0994
Training Epoch: 87 [38912/50048]	Loss: 0.1307
Training Epoch: 87 [39040/50048]	Loss: 0.0875
Training Epoch: 87 [39168/50048]	Loss: 0.1153
Training Epoch: 87 [39296/50048]	Loss: 0.1309
Training Epoch: 87 [39424/50048]	Loss: 0.0596
Training Epoch: 87 [39552/50048]	Loss: 0.0636
Training Epoch: 87 [39680/50048]	Loss: 0.0338
Training Epoch: 87 [39808/50048]	Loss: 0.0678
Training Epoch: 87 [39936/50048]	Loss: 0.0750
Training Epoch: 87 [40064/50048]	Loss: 0.1151
Training Epoch: 87 [40192/50048]	Loss: 0.1235
Training Epoch: 87 [40320/50048]	Loss: 0.1013
Training Epoch: 87 [40448/50048]	Loss: 0.0866
Training Epoch: 87 [40576/50048]	Loss: 0.0741
Training Epoch: 87 [40704/50048]	Loss: 0.1968
Training Epoch: 87 [40832/50048]	Loss: 0.1266
Training Epoch: 87 [40960/50048]	Loss: 0.0590
Training Epoch: 87 [41088/50048]	Loss: 0.1003
Training Epoch: 87 [41216/50048]	Loss: 0.1017
Training Epoch: 87 [41344/50048]	Loss: 0.1579
Training Epoch: 87 [41472/50048]	Loss: 0.1363
Training Epoch: 87 [41600/50048]	Loss: 0.1504
Training Epoch: 87 [41728/50048]	Loss: 0.1161
Training Epoch: 87 [41856/50048]	Loss: 0.1314
Training Epoch: 87 [41984/50048]	Loss: 0.1362
Training Epoch: 87 [42112/50048]	Loss: 0.1801
Training Epoch: 87 [42240/50048]	Loss: 0.1122
Training Epoch: 87 [42368/50048]	Loss: 0.0848
Training Epoch: 87 [42496/50048]	Loss: 0.0940
Training Epoch: 87 [42624/50048]	Loss: 0.1165
Training Epoch: 87 [42752/50048]	Loss: 0.1135
Training Epoch: 87 [42880/50048]	Loss: 0.0965
Training Epoch: 87 [43008/50048]	Loss: 0.1275
Training Epoch: 87 [43136/50048]	Loss: 0.1065
Training Epoch: 87 [43264/50048]	Loss: 0.1701
Training Epoch: 87 [43392/50048]	Loss: 0.0897
Training Epoch: 87 [43520/50048]	Loss: 0.1423
Training Epoch: 87 [43648/50048]	Loss: 0.1308
Training Epoch: 87 [43776/50048]	Loss: 0.0868
Training Epoch: 87 [43904/50048]	Loss: 0.0457
Training Epoch: 87 [44032/50048]	Loss: 0.0825
Training Epoch: 87 [44160/50048]	Loss: 0.0893
Training Epoch: 87 [44288/50048]	Loss: 0.0730
Training Epoch: 87 [44416/50048]	Loss: 0.0445
Training Epoch: 87 [44544/50048]	Loss: 0.0771
Training Epoch: 87 [44672/50048]	Loss: 0.1494
Training Epoch: 87 [44800/50048]	Loss: 0.0672
Training Epoch: 87 [44928/50048]	Loss: 0.0995
Training Epoch: 87 [45056/50048]	Loss: 0.1173
Training Epoch: 87 [45184/50048]	Loss: 0.0908
Training Epoch: 87 [45312/50048]	Loss: 0.2392
Training Epoch: 87 [45440/50048]	Loss: 0.0841
Training Epoch: 87 [45568/50048]	Loss: 0.1107
Training Epoch: 87 [45696/50048]	Loss: 0.1765
2022-12-06 08:20:09,756 [ZeusDataLoader(train)] train epoch 88 done: time=86.50 energy=10508.86
2022-12-06 08:20:09,757 [ZeusDataLoader(eval)] Epoch 88 begin.
Training Epoch: 87 [45824/50048]	Loss: 0.1620
Training Epoch: 87 [45952/50048]	Loss: 0.0597
Training Epoch: 87 [46080/50048]	Loss: 0.0849
Training Epoch: 87 [46208/50048]	Loss: 0.0782
Training Epoch: 87 [46336/50048]	Loss: 0.0933
Training Epoch: 87 [46464/50048]	Loss: 0.1387
Training Epoch: 87 [46592/50048]	Loss: 0.1799
Training Epoch: 87 [46720/50048]	Loss: 0.1024
Training Epoch: 87 [46848/50048]	Loss: 0.0809
Training Epoch: 87 [46976/50048]	Loss: 0.0798
Training Epoch: 87 [47104/50048]	Loss: 0.1266
Training Epoch: 87 [47232/50048]	Loss: 0.0615
Training Epoch: 87 [47360/50048]	Loss: 0.0900
Training Epoch: 87 [47488/50048]	Loss: 0.1090
Training Epoch: 87 [47616/50048]	Loss: 0.1614
Training Epoch: 87 [47744/50048]	Loss: 0.1055
Training Epoch: 87 [47872/50048]	Loss: 0.1204
Training Epoch: 87 [48000/50048]	Loss: 0.1050
Training Epoch: 87 [48128/50048]	Loss: 0.1341
Training Epoch: 87 [48256/50048]	Loss: 0.1487
Training Epoch: 87 [48384/50048]	Loss: 0.1396
Training Epoch: 87 [48512/50048]	Loss: 0.0707
Training Epoch: 87 [48640/50048]	Loss: 0.0558
Training Epoch: 87 [48768/50048]	Loss: 0.0817
Training Epoch: 87 [48896/50048]	Loss: 0.0935
Training Epoch: 87 [49024/50048]	Loss: 0.1492
Training Epoch: 87 [49152/50048]	Loss: 0.0683
Training Epoch: 87 [49280/50048]	Loss: 0.0730
Training Epoch: 87 [49408/50048]	Loss: 0.0492
Training Epoch: 87 [49536/50048]	Loss: 0.0578
Training Epoch: 87 [49664/50048]	Loss: 0.0970
Training Epoch: 87 [49792/50048]	Loss: 0.0974
Training Epoch: 87 [49920/50048]	Loss: 0.0611
Training Epoch: 87 [50048/50048]	Loss: 0.1745
2022-12-06 13:20:13.436 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 08:20:13,448 [ZeusDataLoader(eval)] eval epoch 88 done: time=3.68 energy=440.49
2022-12-06 08:20:13,448 [ZeusDataLoader(train)] Up to epoch 88: time=7937.85, energy=963552.05, cost=1176337.91
2022-12-06 08:20:13,448 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 08:20:13,449 [ZeusDataLoader(train)] Expected next epoch: time=8027.65, energy=974350.07, cost=1189594.30
2022-12-06 08:20:13,449 [ZeusDataLoader(train)] Epoch 89 begin.
Validation Epoch: 87, Average loss: 0.0184, Accuracy: 0.6363
2022-12-06 08:20:13,652 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:20:13,652 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:20:13.654 [ZeusMonitor] Monitor started.
2022-12-06 13:20:13.654 [ZeusMonitor] Running indefinitely. 2022-12-06 13:20:13.654 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:20:13.654 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e89+gpu0.power.log
Training Epoch: 88 [128/50048]	Loss: 0.0903
Training Epoch: 88 [256/50048]	Loss: 0.0866
Training Epoch: 88 [384/50048]	Loss: 0.1103
Training Epoch: 88 [512/50048]	Loss: 0.0924
Training Epoch: 88 [640/50048]	Loss: 0.1297
Training Epoch: 88 [768/50048]	Loss: 0.1049
Training Epoch: 88 [896/50048]	Loss: 0.0872
Training Epoch: 88 [1024/50048]	Loss: 0.0745
Training Epoch: 88 [1152/50048]	Loss: 0.0667
Training Epoch: 88 [1280/50048]	Loss: 0.0619
Training Epoch: 88 [1408/50048]	Loss: 0.0687
Training Epoch: 88 [1536/50048]	Loss: 0.0912
Training Epoch: 88 [1664/50048]	Loss: 0.0464
Training Epoch: 88 [1792/50048]	Loss: 0.0666
Training Epoch: 88 [1920/50048]	Loss: 0.0815
Training Epoch: 88 [2048/50048]	Loss: 0.1326
Training Epoch: 88 [2176/50048]	Loss: 0.0295
Training Epoch: 88 [2304/50048]	Loss: 0.1247
Training Epoch: 88 [2432/50048]	Loss: 0.1901
Training Epoch: 88 [2560/50048]	Loss: 0.1456
Training Epoch: 88 [2688/50048]	Loss: 0.1137
Training Epoch: 88 [2816/50048]	Loss: 0.0622
Training Epoch: 88 [2944/50048]	Loss: 0.1051
Training Epoch: 88 [3072/50048]	Loss: 0.0839
Training Epoch: 88 [3200/50048]	Loss: 0.0847
Training Epoch: 88 [3328/50048]	Loss: 0.0951
Training Epoch: 88 [3456/50048]	Loss: 0.0582
Training Epoch: 88 [3584/50048]	Loss: 0.0785
Training Epoch: 88 [3712/50048]	Loss: 0.0809
Training Epoch: 88 [3840/50048]	Loss: 0.0511
Training Epoch: 88 [3968/50048]	Loss: 0.1248
Training Epoch: 88 [4096/50048]	Loss: 0.1121
Training Epoch: 88 [4224/50048]	Loss: 0.1078
Training Epoch: 88 [4352/50048]	Loss: 0.0523
Training Epoch: 88 [4480/50048]	Loss: 0.1367
Training Epoch: 88 [4608/50048]	Loss: 0.0857
Training Epoch: 88 [4736/50048]	Loss: 0.0466
Training Epoch: 88 [4864/50048]	Loss: 0.0749
Training Epoch: 88 [4992/50048]	Loss: 0.0435
Training Epoch: 88 [5120/50048]	Loss: 0.0760
Training Epoch: 88 [5248/50048]	Loss: 0.1420
Training Epoch: 88 [5376/50048]	Loss: 0.0718
Training Epoch: 88 [5504/50048]	Loss: 0.0357
Training Epoch: 88 [5632/50048]	Loss: 0.0478
Training Epoch: 88 [5760/50048]	Loss: 0.1339
Training Epoch: 88 [5888/50048]	Loss: 0.0826
Training Epoch: 88 [6016/50048]	Loss: 0.0546
Training Epoch: 88 [6144/50048]	Loss: 0.1671
Training Epoch: 88 [6272/50048]	Loss: 0.1124
Training Epoch: 88 [6400/50048]	Loss: 0.1358
Training Epoch: 88 [6528/50048]	Loss: 0.0270
Training Epoch: 88 [6656/50048]	Loss: 0.0767
Training Epoch: 88 [6784/50048]	Loss: 0.0454
Training Epoch: 88 [6912/50048]	Loss: 0.1465
Training Epoch: 88 [7040/50048]	Loss: 0.1142
Training Epoch: 88 [7168/50048]	Loss: 0.0600
Training Epoch: 88 [7296/50048]	Loss: 0.0696
Training Epoch: 88 [7424/50048]	Loss: 0.0846
Training Epoch: 88 [7552/50048]	Loss: 0.0622
Training Epoch: 88 [7680/50048]	Loss: 0.1679
Training Epoch: 88 [7808/50048]	Loss: 0.0924
Training Epoch: 88 [7936/50048]	Loss: 0.1207
Training Epoch: 88 [8064/50048]	Loss: 0.0730
Training Epoch: 88 [8192/50048]	Loss: 0.2077
Training Epoch: 88 [8320/50048]	Loss: 0.1076
Training Epoch: 88 [8448/50048]	Loss: 0.0517
Training Epoch: 88 [8576/50048]	Loss: 0.0564
Training Epoch: 88 [8704/50048]	Loss: 0.0531
Training Epoch: 88 [8832/50048]	Loss: 0.0593
Training Epoch: 88 [8960/50048]	Loss: 0.1473
Training Epoch: 88 [9088/50048]	Loss: 0.0905
Training Epoch: 88 [9216/50048]	Loss: 0.0921
Training Epoch: 88 [9344/50048]	Loss: 0.1423
Training Epoch: 88 [9472/50048]	Loss: 0.1133
Training Epoch: 88 [9600/50048]	Loss: 0.1839
Training Epoch: 88 [9728/50048]	Loss: 0.0282
Training Epoch: 88 [9856/50048]	Loss: 0.0447
Training Epoch: 88 [9984/50048]	Loss: 0.0859
Training Epoch: 88 [10112/50048]	Loss: 0.0821
Training Epoch: 88 [10240/50048]	Loss: 0.1462
Training Epoch: 88 [10368/50048]	Loss: 0.0707
Training Epoch: 88 [10496/50048]	Loss: 0.0629
Training Epoch: 88 [10624/50048]	Loss: 0.1228
Training Epoch: 88 [10752/50048]	Loss: 0.0815
Training Epoch: 88 [10880/50048]	Loss: 0.2046
Training Epoch: 88 [11008/50048]	Loss: 0.0528
Training Epoch: 88 [11136/50048]	Loss: 0.0903
Training Epoch: 88 [11264/50048]	Loss: 0.1713
Training Epoch: 88 [11392/50048]	Loss: 0.0448
Training Epoch: 88 [11520/50048]	Loss: 0.0544
Training Epoch: 88 [11648/50048]	Loss: 0.0626
Training Epoch: 88 [11776/50048]	Loss: 0.0712
Training Epoch: 88 [11904/50048]	Loss: 0.0841
Training Epoch: 88 [12032/50048]	Loss: 0.0665
Training Epoch: 88 [12160/50048]	Loss: 0.0831
Training Epoch: 88 [12288/50048]	Loss: 0.0387
Training Epoch: 88 [12416/50048]	Loss: 0.0768
Training Epoch: 88 [12544/50048]	Loss: 0.0807
Training Epoch: 88 [12672/50048]	Loss: 0.0696
Training Epoch: 88 [12800/50048]	Loss: 0.1056
Training Epoch: 88 [12928/50048]	Loss: 0.1106
Training Epoch: 88 [13056/50048]	Loss: 0.0583
Training Epoch: 88 [13184/50048]	Loss: 0.1100
Training Epoch: 88 [13312/50048]	Loss: 0.0388
Training Epoch: 88 [13440/50048]	Loss: 0.1156
Training Epoch: 88 [13568/50048]	Loss: 0.0967
Training Epoch: 88 [13696/50048]	Loss: 0.0717
Training Epoch: 88 [13824/50048]	Loss: 0.0922
Training Epoch: 88 [13952/50048]	Loss: 0.1547
Training Epoch: 88 [14080/50048]	Loss: 0.0305
Training Epoch: 88 [14208/50048]	Loss: 0.1109
Training Epoch: 88 [14336/50048]	Loss: 0.1088
Training Epoch: 88 [14464/50048]	Loss: 0.0548
Training Epoch: 88 [14592/50048]	Loss: 0.0682
Training Epoch: 88 [14720/50048]	Loss: 0.0920
Training Epoch: 88 [14848/50048]	Loss: 0.0534
Training Epoch: 88 [14976/50048]	Loss: 0.0997
Training Epoch: 88 [15104/50048]	Loss: 0.0978
Training Epoch: 88 [15232/50048]	Loss: 0.1250
Training Epoch: 88 [15360/50048]	Loss: 0.1166
Training Epoch: 88 [15488/50048]	Loss: 0.0584
Training Epoch: 88 [15616/50048]	Loss: 0.1307
Training Epoch: 88 [15744/50048]	Loss: 0.1118
Training Epoch: 88 [15872/50048]	Loss: 0.0686
Training Epoch: 88 [16000/50048]	Loss: 0.1479
Training Epoch: 88 [16128/50048]	Loss: 0.0919
Training Epoch: 88 [16256/50048]	Loss: 0.0645
Training Epoch: 88 [16384/50048]	Loss: 0.0843
Training Epoch: 88 [16512/50048]	Loss: 0.0877
Training Epoch: 88 [16640/50048]	Loss: 0.0839
Training Epoch: 88 [16768/50048]	Loss: 0.0899
Training Epoch: 88 [16896/50048]	Loss: 0.0494
Training Epoch: 88 [17024/50048]	Loss: 0.1029
Training Epoch: 88 [17152/50048]	Loss: 0.0260
Training Epoch: 88 [17280/50048]	Loss: 0.0947
Training Epoch: 88 [17408/50048]	Loss: 0.0405
Training Epoch: 88 [17536/50048]	Loss: 0.0707
Training Epoch: 88 [17664/50048]	Loss: 0.1016
Training Epoch: 88 [17792/50048]	Loss: 0.0832
Training Epoch: 88 [17920/50048]	Loss: 0.0635
Training Epoch: 88 [18048/50048]	Loss: 0.0770
Training Epoch: 88 [18176/50048]	Loss: 0.1201
Training Epoch: 88 [18304/50048]	Loss: 0.1539
Training Epoch: 88 [18432/50048]	Loss: 0.1109
Training Epoch: 88 [18560/50048]	Loss: 0.0749
Training Epoch: 88 [18688/50048]	Loss: 0.1478
Training Epoch: 88 [18816/50048]	Loss: 0.0483
Training Epoch: 88 [18944/50048]	Loss: 0.0448
Training Epoch: 88 [19072/50048]	Loss: 0.0475
Training Epoch: 88 [19200/50048]	Loss: 0.1246
Training Epoch: 88 [19328/50048]	Loss: 0.1086
Training Epoch: 88 [19456/50048]	Loss: 0.0785
Training Epoch: 88 [19584/50048]	Loss: 0.0619
Training Epoch: 88 [19712/50048]	Loss: 0.0991
Training Epoch: 88 [19840/50048]	Loss: 0.0671
Training Epoch: 88 [19968/50048]	Loss: 0.1344
Training Epoch: 88 [20096/50048]	Loss: 0.1099
Training Epoch: 88 [20224/50048]	Loss: 0.0785
Training Epoch: 88 [20352/50048]	Loss: 0.0913
Training Epoch: 88 [20480/50048]	Loss: 0.0575
Training Epoch: 88 [20608/50048]	Loss: 0.1166
Training Epoch: 88 [20736/50048]	Loss: 0.1343
Training Epoch: 88 [20864/50048]	Loss: 0.0431
Training Epoch: 88 [20992/50048]	Loss: 0.1489
Training Epoch: 88 [21120/50048]	Loss: 0.0772
Training Epoch: 88 [21248/50048]	Loss: 0.1148
Training Epoch: 88 [21376/50048]	Loss: 0.0765
Training Epoch: 88 [21504/50048]	Loss: 0.0484
Training Epoch: 88 [21632/50048]	Loss: 0.0811
Training Epoch: 88 [21760/50048]	Loss: 0.0418
Training Epoch: 88 [21888/50048]	Loss: 0.1995
Training Epoch: 88 [22016/50048]	Loss: 0.0492
Training Epoch: 88 [22144/50048]	Loss: 0.0701
Training Epoch: 88 [22272/50048]	Loss: 0.0740
Training Epoch: 88 [22400/50048]	Loss: 0.1080
Training Epoch: 88 [22528/50048]	Loss: 0.0797
Training Epoch: 88 [22656/50048]	Loss: 0.0538
Training Epoch: 88 [22784/50048]	Loss: 0.1078
Training Epoch: 88 [22912/50048]	Loss: 0.1121
Training Epoch: 88 [23040/50048]	Loss: 0.0852
Training Epoch: 88 [23168/50048]	Loss: 0.0400
Training Epoch: 88 [23296/50048]	Loss: 0.0881
Training Epoch: 88 [23424/50048]	Loss: 0.0473
Training Epoch: 88 [23552/50048]	Loss: 0.0955
Training Epoch: 88 [23680/50048]	Loss: 0.0634
Training Epoch: 88 [23808/50048]	Loss: 0.1466
Training Epoch: 88 [23936/50048]	Loss: 0.1495
Training Epoch: 88 [24064/50048]	Loss: 0.1343
Training Epoch: 88 [24192/50048]	Loss: 0.1272
Training Epoch: 88 [24320/50048]	Loss: 0.0907
Training Epoch: 88 [24448/50048]	Loss: 0.0346
Training Epoch: 88 [24576/50048]	Loss: 0.0797
Training Epoch: 88 [24704/50048]	Loss: 0.0459
Training Epoch: 88 [24832/50048]	Loss: 0.0511
Training Epoch: 88 [24960/50048]	Loss: 0.1118
Training Epoch: 88 [25088/50048]	Loss: 0.0582
Training Epoch: 88 [25216/50048]	Loss: 0.1116
Training Epoch: 88 [25344/50048]	Loss: 0.1442
Training Epoch: 88 [25472/50048]	Loss: 0.0424
Training Epoch: 88 [25600/50048]	Loss: 0.0644
Training Epoch: 88 [25728/50048]	Loss: 0.0657
Training Epoch: 88 [25856/50048]	Loss: 0.0527
Training Epoch: 88 [25984/50048]	Loss: 0.0816
Training Epoch: 88 [26112/50048]	Loss: 0.0704
Training Epoch: 88 [26240/50048]	Loss: 0.0807
Training Epoch: 88 [26368/50048]	Loss: 0.1237
Training Epoch: 88 [26496/50048]	Loss: 0.0808
Training Epoch: 88 [26624/50048]	Loss: 0.0909
Training Epoch: 88 [26752/50048]	Loss: 0.1244
Training Epoch: 88 [26880/50048]	Loss: 0.0652
Training Epoch: 88 [27008/50048]	Loss: 0.1229
Training Epoch: 88 [27136/50048]	Loss: 0.1056
Training Epoch: 88 [27264/50048]	Loss: 0.1306
Training Epoch: 88 [27392/50048]	Loss: 0.1079
Training Epoch: 88 [27520/50048]	Loss: 0.0676
Training Epoch: 88 [27648/50048]	Loss: 0.0875
Training Epoch: 88 [27776/50048]	Loss: 0.0548
Training Epoch: 88 [27904/50048]	Loss: 0.1065
Training Epoch: 88 [28032/50048]	Loss: 0.0552
Training Epoch: 88 [28160/50048]	Loss: 0.1319
Training Epoch: 88 [28288/50048]	Loss: 0.0596
Training Epoch: 88 [28416/50048]	Loss: 0.0480
Training Epoch: 88 [28544/50048]	Loss: 0.2031
Training Epoch: 88 [28672/50048]	Loss: 0.1545
Training Epoch: 88 [28800/50048]	Loss: 0.0995
Training Epoch: 88 [28928/50048]	Loss: 0.0604
Training Epoch: 88 [29056/50048]	Loss: 0.1065
Training Epoch: 88 [29184/50048]	Loss: 0.1427
Training Epoch: 88 [29312/50048]	Loss: 0.0736
Training Epoch: 88 [29440/50048]	Loss: 0.0514
Training Epoch: 88 [29568/50048]	Loss: 0.1092
Training Epoch: 88 [29696/50048]	Loss: 0.1304
Training Epoch: 88 [29824/50048]	Loss: 0.0268
Training Epoch: 88 [29952/50048]	Loss: 0.1819
Training Epoch: 88 [30080/50048]	Loss: 0.0635
Training Epoch: 88 [30208/50048]	Loss: 0.0829
Training Epoch: 88 [30336/50048]	Loss: 0.1106
Training Epoch: 88 [30464/50048]	Loss: 0.0500
Training Epoch: 88 [30592/50048]	Loss: 0.0717
Training Epoch: 88 [30720/50048]	Loss: 0.1363
Training Epoch: 88 [30848/50048]	Loss: 0.0944
Training Epoch: 88 [30976/50048]	Loss: 0.0554
Training Epoch: 88 [31104/50048]	Loss: 0.1000
Training Epoch: 88 [31232/50048]	Loss: 0.1346
Training Epoch: 88 [31360/50048]	Loss: 0.0630
Training Epoch: 88 [31488/50048]	Loss: 0.0953
Training Epoch: 88 [31616/50048]	Loss: 0.1711
Training Epoch: 88 [31744/50048]	Loss: 0.0640
Training Epoch: 88 [31872/50048]	Loss: 0.1044
Training Epoch: 88 [32000/50048]	Loss: 0.1788
Training Epoch: 88 [32128/50048]	Loss: 0.0938
Training Epoch: 88 [32256/50048]	Loss: 0.0550
Training Epoch: 88 [32384/50048]	Loss: 0.1062
Training Epoch: 88 [32512/50048]	Loss: 0.0864
Training Epoch: 88 [32640/50048]	Loss: 0.1499
Training Epoch: 88 [32768/50048]	Loss: 0.1008
Training Epoch: 88 [32896/50048]	Loss: 0.1112
Training Epoch: 88 [33024/50048]	Loss: 0.0560
Training Epoch: 88 [33152/50048]	Loss: 0.1501
Training Epoch: 88 [33280/50048]	Loss: 0.1946
Training Epoch: 88 [33408/50048]	Loss: 0.1140
Training Epoch: 88 [33536/50048]	Loss: 0.0981
Training Epoch: 88 [33664/50048]	Loss: 0.0955
Training Epoch: 88 [33792/50048]	Loss: 0.1237
Training Epoch: 88 [33920/50048]	Loss: 0.0488
Training Epoch: 88 [34048/50048]	Loss: 0.0411
Training Epoch: 88 [34176/50048]	Loss: 0.0727
Training Epoch: 88 [34304/50048]	Loss: 0.0597
Training Epoch: 88 [34432/50048]	Loss: 0.1475
Training Epoch: 88 [34560/50048]	Loss: 0.0786
Training Epoch: 88 [34688/50048]	Loss: 0.1101
Training Epoch: 88 [34816/50048]	Loss: 0.0906
Training Epoch: 88 [34944/50048]	Loss: 0.0783
Training Epoch: 88 [35072/50048]	Loss: 0.2054
Training Epoch: 88 [35200/50048]	Loss: 0.1323
Training Epoch: 88 [35328/50048]	Loss: 0.0934
Training Epoch: 88 [35456/50048]	Loss: 0.0570
Training Epoch: 88 [35584/50048]	Loss: 0.1157
Training Epoch: 88 [35712/50048]	Loss: 0.0821
Training Epoch: 88 [35840/50048]	Loss: 0.1354
Training Epoch: 88 [35968/50048]	Loss: 0.1190
Training Epoch: 88 [36096/50048]	Loss: 0.0961
Training Epoch: 88 [36224/50048]	Loss: 0.1035
Training Epoch: 88 [36352/50048]	Loss: 0.1957
Training Epoch: 88 [36480/50048]	Loss: 0.0845
Training Epoch: 88 [36608/50048]	Loss: 0.1032
Training Epoch: 88 [36736/50048]	Loss: 0.1215
Training Epoch: 88 [36864/50048]	Loss: 0.0744
Training Epoch: 88 [36992/50048]	Loss: 0.0334
Training Epoch: 88 [37120/50048]	Loss: 0.0510
Training Epoch: 88 [37248/50048]	Loss: 0.0844
Training Epoch: 88 [37376/50048]	Loss: 0.0512
Training Epoch: 88 [37504/50048]	Loss: 0.0921
Training Epoch: 88 [37632/50048]	Loss: 0.1039
Training Epoch: 88 [37760/50048]	Loss: 0.0787
Training Epoch: 88 [37888/50048]	Loss: 0.0735
Training Epoch: 88 [38016/50048]	Loss: 0.0798
Training Epoch: 88 [38144/50048]	Loss: 0.2088
Training Epoch: 88 [38272/50048]	Loss: 0.0705
Training Epoch: 88 [38400/50048]	Loss: 0.1314
Training Epoch: 88 [38528/50048]	Loss: 0.1323
Training Epoch: 88 [38656/50048]	Loss: 0.1260
Training Epoch: 88 [38784/50048]	Loss: 0.0888
Training Epoch: 88 [38912/50048]	Loss: 0.1780
Training Epoch: 88 [39040/50048]	Loss: 0.1056
Training Epoch: 88 [39168/50048]	Loss: 0.1017
Training Epoch: 88 [39296/50048]	Loss: 0.0676
Training Epoch: 88 [39424/50048]	Loss: 0.1004
Training Epoch: 88 [39552/50048]	Loss: 0.0721
Training Epoch: 88 [39680/50048]	Loss: 0.0919
Training Epoch: 88 [39808/50048]	Loss: 0.1299
Training Epoch: 88 [39936/50048]	Loss: 0.0938
Training Epoch: 88 [40064/50048]	Loss: 0.1310
Training Epoch: 88 [40192/50048]	Loss: 0.0842
Training Epoch: 88 [40320/50048]	Loss: 0.0533
Training Epoch: 88 [40448/50048]	Loss: 0.1224
Training Epoch: 88 [40576/50048]	Loss: 0.0996
Training Epoch: 88 [40704/50048]	Loss: 0.1386
Training Epoch: 88 [40832/50048]	Loss: 0.1011
Training Epoch: 88 [40960/50048]	Loss: 0.0733
Training Epoch: 88 [41088/50048]	Loss: 0.0202
Training Epoch: 88 [41216/50048]	Loss: 0.1139
Training Epoch: 88 [41344/50048]	Loss: 0.0724
Training Epoch: 88 [41472/50048]	Loss: 0.0793
Training Epoch: 88 [41600/50048]	Loss: 0.0497
Training Epoch: 88 [41728/50048]	Loss: 0.1321
Training Epoch: 88 [41856/50048]	Loss: 0.0294
Training Epoch: 88 [41984/50048]	Loss: 0.1034
Training Epoch: 88 [42112/50048]	Loss: 0.1104
Training Epoch: 88 [42240/50048]	Loss: 0.0929
Training Epoch: 88 [42368/50048]	Loss: 0.0875
Training Epoch: 88 [42496/50048]	Loss: 0.1014
Training Epoch: 88 [42624/50048]	Loss: 0.1529
Training Epoch: 88 [42752/50048]	Loss: 0.1579
Training Epoch: 88 [42880/50048]	Loss: 0.1094
Training Epoch: 88 [43008/50048]	Loss: 0.1050
Training Epoch: 88 [43136/50048]	Loss: 0.0834
Training Epoch: 88 [43264/50048]	Loss: 0.0858
Training Epoch: 88 [43392/50048]	Loss: 0.0774
Training Epoch: 88 [43520/50048]	Loss: 0.1333
Training Epoch: 88 [43648/50048]	Loss: 0.1432
Training Epoch: 88 [43776/50048]	Loss: 0.0579
Training Epoch: 88 [43904/50048]	Loss: 0.0880
Training Epoch: 88 [44032/50048]	Loss: 0.0364
Training Epoch: 88 [44160/50048]	Loss: 0.0831
Training Epoch: 88 [44288/50048]	Loss: 0.0881
Training Epoch: 88 [44416/50048]	Loss: 0.0724
Training Epoch: 88 [44544/50048]	Loss: 0.0654
Training Epoch: 88 [44672/50048]	Loss: 0.0548
Training Epoch: 88 [44800/50048]	Loss: 0.0652
Training Epoch: 88 [44928/50048]	Loss: 0.0923
Training Epoch: 88 [45056/50048]	Loss: 0.0587
Training Epoch: 88 [45184/50048]	Loss: 0.1211
Training Epoch: 88 [45312/50048]	Loss: 0.0932
Training Epoch: 88 [45440/50048]	Loss: 0.1475
Training Epoch: 88 [45568/50048]	Loss: 0.0876
Training Epoch: 88 [45696/50048]	Loss: 0.1117
2022-12-06 08:21:40,031 [ZeusDataLoader(train)] train epoch 89 done: time=86.57 energy=10503.76
2022-12-06 08:21:40,032 [ZeusDataLoader(eval)] Epoch 89 begin.
Training Epoch: 88 [45824/50048]	Loss: 0.0849
Training Epoch: 88 [45952/50048]	Loss: 0.1895
Training Epoch: 88 [46080/50048]	Loss: 0.1173
Training Epoch: 88 [46208/50048]	Loss: 0.0722
Training Epoch: 88 [46336/50048]	Loss: 0.2510
Training Epoch: 88 [46464/50048]	Loss: 0.0585
Training Epoch: 88 [46592/50048]	Loss: 0.0998
Training Epoch: 88 [46720/50048]	Loss: 0.0642
Training Epoch: 88 [46848/50048]	Loss: 0.0999
Training Epoch: 88 [46976/50048]	Loss: 0.1419
Training Epoch: 88 [47104/50048]	Loss: 0.1177
Training Epoch: 88 [47232/50048]	Loss: 0.1368
Training Epoch: 88 [47360/50048]	Loss: 0.0981
Training Epoch: 88 [47488/50048]	Loss: 0.0633
Training Epoch: 88 [47616/50048]	Loss: 0.0715
Training Epoch: 88 [47744/50048]	Loss: 0.0829
Training Epoch: 88 [47872/50048]	Loss: 0.0791
Training Epoch: 88 [48000/50048]	Loss: 0.2387
Training Epoch: 88 [48128/50048]	Loss: 0.1540
Training Epoch: 88 [48256/50048]	Loss: 0.0674
Training Epoch: 88 [48384/50048]	Loss: 0.1054
Training Epoch: 88 [48512/50048]	Loss: 0.1162
Training Epoch: 88 [48640/50048]	Loss: 0.1625
Training Epoch: 88 [48768/50048]	Loss: 0.1520
Training Epoch: 88 [48896/50048]	Loss: 0.1029
Training Epoch: 88 [49024/50048]	Loss: 0.1036
Training Epoch: 88 [49152/50048]	Loss: 0.0962
Training Epoch: 88 [49280/50048]	Loss: 0.1051
Training Epoch: 88 [49408/50048]	Loss: 0.0589
Training Epoch: 88 [49536/50048]	Loss: 0.0598
Training Epoch: 88 [49664/50048]	Loss: 0.0300
Training Epoch: 88 [49792/50048]	Loss: 0.0833
Training Epoch: 88 [49920/50048]	Loss: 0.1018
Training Epoch: 88 [50048/50048]	Loss: 0.1524
2022-12-06 13:21:43.670 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 08:21:43,682 [ZeusDataLoader(eval)] eval epoch 89 done: time=3.64 energy=442.57
2022-12-06 08:21:43,683 [ZeusDataLoader(train)] Up to epoch 89: time=8028.06, energy=974498.39, cost=1189704.62
2022-12-06 08:21:43,683 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 08:21:43,683 [ZeusDataLoader(train)] Expected next epoch: time=8117.86, energy=985296.40, cost=1202961.01
2022-12-06 08:21:43,684 [ZeusDataLoader(train)] Epoch 90 begin.
Validation Epoch: 88, Average loss: 0.0182, Accuracy: 0.6452
2022-12-06 08:21:43,826 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:21:43,827 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:21:43.831 [ZeusMonitor] Monitor started.
2022-12-06 13:21:43.831 [ZeusMonitor] Running indefinitely. 2022-12-06 13:21:43.831 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:21:43.831 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e90+gpu0.power.log
Training Epoch: 89 [128/50048]	Loss: 0.1049
Training Epoch: 89 [256/50048]	Loss: 0.0625
Training Epoch: 89 [384/50048]	Loss: 0.1213
Training Epoch: 89 [512/50048]	Loss: 0.0411
Training Epoch: 89 [640/50048]	Loss: 0.0423
Training Epoch: 89 [768/50048]	Loss: 0.0559
Training Epoch: 89 [896/50048]	Loss: 0.0713
Training Epoch: 89 [1024/50048]	Loss: 0.1195
Training Epoch: 89 [1152/50048]	Loss: 0.1018
Training Epoch: 89 [1280/50048]	Loss: 0.0759
Training Epoch: 89 [1408/50048]	Loss: 0.0737
Training Epoch: 89 [1536/50048]	Loss: 0.0647
Training Epoch: 89 [1664/50048]	Loss: 0.0990
Training Epoch: 89 [1792/50048]	Loss: 0.0591
Training Epoch: 89 [1920/50048]	Loss: 0.0521
Training Epoch: 89 [2048/50048]	Loss: 0.1032
Training Epoch: 89 [2176/50048]	Loss: 0.0750
Training Epoch: 89 [2304/50048]	Loss: 0.0897
Training Epoch: 89 [2432/50048]	Loss: 0.0793
Training Epoch: 89 [2560/50048]	Loss: 0.0329
Training Epoch: 89 [2688/50048]	Loss: 0.1032
Training Epoch: 89 [2816/50048]	Loss: 0.0731
Training Epoch: 89 [2944/50048]	Loss: 0.0361
Training Epoch: 89 [3072/50048]	Loss: 0.0641
Training Epoch: 89 [3200/50048]	Loss: 0.1498
Training Epoch: 89 [3328/50048]	Loss: 0.0649
Training Epoch: 89 [3456/50048]	Loss: 0.0648
Training Epoch: 89 [3584/50048]	Loss: 0.0581
Training Epoch: 89 [3712/50048]	Loss: 0.0838
Training Epoch: 89 [3840/50048]	Loss: 0.1002
Training Epoch: 89 [3968/50048]	Loss: 0.0507
Training Epoch: 89 [4096/50048]	Loss: 0.0809
Training Epoch: 89 [4224/50048]	Loss: 0.0728
Training Epoch: 89 [4352/50048]	Loss: 0.0354
Training Epoch: 89 [4480/50048]	Loss: 0.1158
Training Epoch: 89 [4608/50048]	Loss: 0.1531
Training Epoch: 89 [4736/50048]	Loss: 0.0353
Training Epoch: 89 [4864/50048]	Loss: 0.0410
Training Epoch: 89 [4992/50048]	Loss: 0.1159
Training Epoch: 89 [5120/50048]	Loss: 0.0869
Training Epoch: 89 [5248/50048]	Loss: 0.0713
Training Epoch: 89 [5376/50048]	Loss: 0.1135
Training Epoch: 89 [5504/50048]	Loss: 0.0807
Training Epoch: 89 [5632/50048]	Loss: 0.0519
Training Epoch: 89 [5760/50048]	Loss: 0.0711
Training Epoch: 89 [5888/50048]	Loss: 0.0947
Training Epoch: 89 [6016/50048]	Loss: 0.0882
Training Epoch: 89 [6144/50048]	Loss: 0.1627
Training Epoch: 89 [6272/50048]	Loss: 0.1754
Training Epoch: 89 [6400/50048]	Loss: 0.1537
Training Epoch: 89 [6528/50048]	Loss: 0.1073
Training Epoch: 89 [6656/50048]	Loss: 0.0675
Training Epoch: 89 [6784/50048]	Loss: 0.0924
Training Epoch: 89 [6912/50048]	Loss: 0.0763
Training Epoch: 89 [7040/50048]	Loss: 0.0755
Training Epoch: 89 [7168/50048]	Loss: 0.0616
Training Epoch: 89 [7296/50048]	Loss: 0.1418
Training Epoch: 89 [7424/50048]	Loss: 0.0691
Training Epoch: 89 [7552/50048]	Loss: 0.1101
Training Epoch: 89 [7680/50048]	Loss: 0.1137
Training Epoch: 89 [7808/50048]	Loss: 0.1212
Training Epoch: 89 [7936/50048]	Loss: 0.0743
Training Epoch: 89 [8064/50048]	Loss: 0.0726
Training Epoch: 89 [8192/50048]	Loss: 0.0694
Training Epoch: 89 [8320/50048]	Loss: 0.0741
Training Epoch: 89 [8448/50048]	Loss: 0.0583
Training Epoch: 89 [8576/50048]	Loss: 0.1546
Training Epoch: 89 [8704/50048]	Loss: 0.0914
Training Epoch: 89 [8832/50048]	Loss: 0.1450
Training Epoch: 89 [8960/50048]	Loss: 0.0812
Training Epoch: 89 [9088/50048]	Loss: 0.1790
Training Epoch: 89 [9216/50048]	Loss: 0.1404
Training Epoch: 89 [9344/50048]	Loss: 0.0530
Training Epoch: 89 [9472/50048]	Loss: 0.1400
Training Epoch: 89 [9600/50048]	Loss: 0.0694
Training Epoch: 89 [9728/50048]	Loss: 0.0813
Training Epoch: 89 [9856/50048]	Loss: 0.1960
Training Epoch: 89 [9984/50048]	Loss: 0.1081
Training Epoch: 89 [10112/50048]	Loss: 0.1011
Training Epoch: 89 [10240/50048]	Loss: 0.0575
Training Epoch: 89 [10368/50048]	Loss: 0.0706
Training Epoch: 89 [10496/50048]	Loss: 0.0610
Training Epoch: 89 [10624/50048]	Loss: 0.2224
Training Epoch: 89 [10752/50048]	Loss: 0.1436
Training Epoch: 89 [10880/50048]	Loss: 0.0657
Training Epoch: 89 [11008/50048]	Loss: 0.0308
Training Epoch: 89 [11136/50048]	Loss: 0.1022
Training Epoch: 89 [11264/50048]	Loss: 0.0776
Training Epoch: 89 [11392/50048]	Loss: 0.1689
Training Epoch: 89 [11520/50048]	Loss: 0.0416
Training Epoch: 89 [11648/50048]	Loss: 0.0725
Training Epoch: 89 [11776/50048]	Loss: 0.0531
Training Epoch: 89 [11904/50048]	Loss: 0.1504
Training Epoch: 89 [12032/50048]	Loss: 0.0653
Training Epoch: 89 [12160/50048]	Loss: 0.0821
Training Epoch: 89 [12288/50048]	Loss: 0.1605
Training Epoch: 89 [12416/50048]	Loss: 0.1198
Training Epoch: 89 [12544/50048]	Loss: 0.0199
Training Epoch: 89 [12672/50048]	Loss: 0.1125
Training Epoch: 89 [12800/50048]	Loss: 0.1046
Training Epoch: 89 [12928/50048]	Loss: 0.0542
Training Epoch: 89 [13056/50048]	Loss: 0.1008
Training Epoch: 89 [13184/50048]	Loss: 0.0887
Training Epoch: 89 [13312/50048]	Loss: 0.0983
Training Epoch: 89 [13440/50048]	Loss: 0.0512
Training Epoch: 89 [13568/50048]	Loss: 0.0769
Training Epoch: 89 [13696/50048]	Loss: 0.0961
Training Epoch: 89 [13824/50048]	Loss: 0.1086
Training Epoch: 89 [13952/50048]	Loss: 0.0723
Training Epoch: 89 [14080/50048]	Loss: 0.0802
Training Epoch: 89 [14208/50048]	Loss: 0.0470
Training Epoch: 89 [14336/50048]	Loss: 0.1719
Training Epoch: 89 [14464/50048]	Loss: 0.0631
Training Epoch: 89 [14592/50048]	Loss: 0.0445
Training Epoch: 89 [14720/50048]	Loss: 0.0684
Training Epoch: 89 [14848/50048]	Loss: 0.0764
Training Epoch: 89 [14976/50048]	Loss: 0.0469
Training Epoch: 89 [15104/50048]	Loss: 0.0649
Training Epoch: 89 [15232/50048]	Loss: 0.0364
Training Epoch: 89 [15360/50048]	Loss: 0.0791
Training Epoch: 89 [15488/50048]	Loss: 0.1023
Training Epoch: 89 [15616/50048]	Loss: 0.1557
Training Epoch: 89 [15744/50048]	Loss: 0.0773
Training Epoch: 89 [15872/50048]	Loss: 0.1106
Training Epoch: 89 [16000/50048]	Loss: 0.1445
Training Epoch: 89 [16128/50048]	Loss: 0.0807
Training Epoch: 89 [16256/50048]	Loss: 0.0863
Training Epoch: 89 [16384/50048]	Loss: 0.0813
Training Epoch: 89 [16512/50048]	Loss: 0.0501
Training Epoch: 89 [16640/50048]	Loss: 0.1035
Training Epoch: 89 [16768/50048]	Loss: 0.0884
Training Epoch: 89 [16896/50048]	Loss: 0.0606
Training Epoch: 89 [17024/50048]	Loss: 0.1159
Training Epoch: 89 [17152/50048]	Loss: 0.0338
Training Epoch: 89 [17280/50048]	Loss: 0.0647
Training Epoch: 89 [17408/50048]	Loss: 0.0658
Training Epoch: 89 [17536/50048]	Loss: 0.0898
Training Epoch: 89 [17664/50048]	Loss: 0.1459
Training Epoch: 89 [17792/50048]	Loss: 0.0660
Training Epoch: 89 [17920/50048]	Loss: 0.1886
Training Epoch: 89 [18048/50048]	Loss: 0.0914
Training Epoch: 89 [18176/50048]	Loss: 0.0984
Training Epoch: 89 [18304/50048]	Loss: 0.1026
Training Epoch: 89 [18432/50048]	Loss: 0.0871
Training Epoch: 89 [18560/50048]	Loss: 0.0276
Training Epoch: 89 [18688/50048]	Loss: 0.0980
Training Epoch: 89 [18816/50048]	Loss: 0.1340
Training Epoch: 89 [18944/50048]	Loss: 0.1085
Training Epoch: 89 [19072/50048]	Loss: 0.0405
Training Epoch: 89 [19200/50048]	Loss: 0.0912
Training Epoch: 89 [19328/50048]	Loss: 0.0461
Training Epoch: 89 [19456/50048]	Loss: 0.0466
Training Epoch: 89 [19584/50048]	Loss: 0.0805
Training Epoch: 89 [19712/50048]	Loss: 0.0644
Training Epoch: 89 [19840/50048]	Loss: 0.0682
Training Epoch: 89 [19968/50048]	Loss: 0.0733
Training Epoch: 89 [20096/50048]	Loss: 0.1359
Training Epoch: 89 [20224/50048]	Loss: 0.0303
Training Epoch: 89 [20352/50048]	Loss: 0.0966
Training Epoch: 89 [20480/50048]	Loss: 0.1705
Training Epoch: 89 [20608/50048]	Loss: 0.0834
Training Epoch: 89 [20736/50048]	Loss: 0.1664
Training Epoch: 89 [20864/50048]	Loss: 0.0812
Training Epoch: 89 [20992/50048]	Loss: 0.1139
Training Epoch: 89 [21120/50048]	Loss: 0.0890
Training Epoch: 89 [21248/50048]	Loss: 0.0582
Training Epoch: 89 [21376/50048]	Loss: 0.1119
Training Epoch: 89 [21504/50048]	Loss: 0.0709
Training Epoch: 89 [21632/50048]	Loss: 0.1000
Training Epoch: 89 [21760/50048]	Loss: 0.0633
Training Epoch: 89 [21888/50048]	Loss: 0.0909
Training Epoch: 89 [22016/50048]	Loss: 0.0436
Training Epoch: 89 [22144/50048]	Loss: 0.1405
Training Epoch: 89 [22272/50048]	Loss: 0.1577
Training Epoch: 89 [22400/50048]	Loss: 0.0548
Training Epoch: 89 [22528/50048]	Loss: 0.0711
Training Epoch: 89 [22656/50048]	Loss: 0.0856
Training Epoch: 89 [22784/50048]	Loss: 0.1363
Training Epoch: 89 [22912/50048]	Loss: 0.0535
Training Epoch: 89 [23040/50048]	Loss: 0.1343
Training Epoch: 89 [23168/50048]	Loss: 0.0957
Training Epoch: 89 [23296/50048]	Loss: 0.0527
Training Epoch: 89 [23424/50048]	Loss: 0.0845
Training Epoch: 89 [23552/50048]	Loss: 0.1090
Training Epoch: 89 [23680/50048]	Loss: 0.0313
Training Epoch: 89 [23808/50048]	Loss: 0.0466
Training Epoch: 89 [23936/50048]	Loss: 0.0894
Training Epoch: 89 [24064/50048]	Loss: 0.1458
Training Epoch: 89 [24192/50048]	Loss: 0.1549
Training Epoch: 89 [24320/50048]	Loss: 0.0934
Training Epoch: 89 [24448/50048]	Loss: 0.1108
Training Epoch: 89 [24576/50048]	Loss: 0.1059
Training Epoch: 89 [24704/50048]	Loss: 0.0760
Training Epoch: 89 [24832/50048]	Loss: 0.1601
Training Epoch: 89 [24960/50048]	Loss: 0.0723
Training Epoch: 89 [25088/50048]	Loss: 0.1479
Training Epoch: 89 [25216/50048]	Loss: 0.0693
Training Epoch: 89 [25344/50048]	Loss: 0.0889
Training Epoch: 89 [25472/50048]	Loss: 0.1015
Training Epoch: 89 [25600/50048]	Loss: 0.1165
Training Epoch: 89 [25728/50048]	Loss: 0.1146
Training Epoch: 89 [25856/50048]	Loss: 0.1230
Training Epoch: 89 [25984/50048]	Loss: 0.1248
Training Epoch: 89 [26112/50048]	Loss: 0.1268
Training Epoch: 89 [26240/50048]	Loss: 0.0671
Training Epoch: 89 [26368/50048]	Loss: 0.1763
Training Epoch: 89 [26496/50048]	Loss: 0.0887
Training Epoch: 89 [26624/50048]	Loss: 0.1086
Training Epoch: 89 [26752/50048]	Loss: 0.0579
Training Epoch: 89 [26880/50048]	Loss: 0.1731
Training Epoch: 89 [27008/50048]	Loss: 0.0584
Training Epoch: 89 [27136/50048]	Loss: 0.1005
Training Epoch: 89 [27264/50048]	Loss: 0.1201
Training Epoch: 89 [27392/50048]	Loss: 0.0921
Training Epoch: 89 [27520/50048]	Loss: 0.0606
Training Epoch: 89 [27648/50048]	Loss: 0.1014
Training Epoch: 89 [27776/50048]	Loss: 0.1187
Training Epoch: 89 [27904/50048]	Loss: 0.0509
Training Epoch: 89 [28032/50048]	Loss: 0.1615
Training Epoch: 89 [28160/50048]	Loss: 0.0943
Training Epoch: 89 [28288/50048]	Loss: 0.1329
Training Epoch: 89 [28416/50048]	Loss: 0.0899
Training Epoch: 89 [28544/50048]	Loss: 0.0671
Training Epoch: 89 [28672/50048]	Loss: 0.0799
Training Epoch: 89 [28800/50048]	Loss: 0.0299
Training Epoch: 89 [28928/50048]	Loss: 0.0692
Training Epoch: 89 [29056/50048]	Loss: 0.1128
Training Epoch: 89 [29184/50048]	Loss: 0.2666
Training Epoch: 89 [29312/50048]	Loss: 0.1533
Training Epoch: 89 [29440/50048]	Loss: 0.1143
Training Epoch: 89 [29568/50048]	Loss: 0.0508
Training Epoch: 89 [29696/50048]	Loss: 0.2632
Training Epoch: 89 [29824/50048]	Loss: 0.0966
Training Epoch: 89 [29952/50048]	Loss: 0.0904
Training Epoch: 89 [30080/50048]	Loss: 0.0737
Training Epoch: 89 [30208/50048]	Loss: 0.1394
Training Epoch: 89 [30336/50048]	Loss: 0.0695
Training Epoch: 89 [30464/50048]	Loss: 0.1357
Training Epoch: 89 [30592/50048]	Loss: 0.0696
Training Epoch: 89 [30720/50048]	Loss: 0.0988
Training Epoch: 89 [30848/50048]	Loss: 0.1210
Training Epoch: 89 [30976/50048]	Loss: 0.1545
Training Epoch: 89 [31104/50048]	Loss: 0.0688
Training Epoch: 89 [31232/50048]	Loss: 0.1240
Training Epoch: 89 [31360/50048]	Loss: 0.0512
Training Epoch: 89 [31488/50048]	Loss: 0.0503
Training Epoch: 89 [31616/50048]	Loss: 0.0458
Training Epoch: 89 [31744/50048]	Loss: 0.0576
Training Epoch: 89 [31872/50048]	Loss: 0.0278
Training Epoch: 89 [32000/50048]	Loss: 0.1067
Training Epoch: 89 [32128/50048]	Loss: 0.0846
Training Epoch: 89 [32256/50048]	Loss: 0.0735
Training Epoch: 89 [32384/50048]	Loss: 0.0811
Training Epoch: 89 [32512/50048]	Loss: 0.1023
Training Epoch: 89 [32640/50048]	Loss: 0.1610
Training Epoch: 89 [32768/50048]	Loss: 0.1401
Training Epoch: 89 [32896/50048]	Loss: 0.1644
Training Epoch: 89 [33024/50048]	Loss: 0.0955
Training Epoch: 89 [33152/50048]	Loss: 0.1162
Training Epoch: 89 [33280/50048]	Loss: 0.0501
Training Epoch: 89 [33408/50048]	Loss: 0.0899
Training Epoch: 89 [33536/50048]	Loss: 0.1193
Training Epoch: 89 [33664/50048]	Loss: 0.0716
Training Epoch: 89 [33792/50048]	Loss: 0.0564
Training Epoch: 89 [33920/50048]	Loss: 0.0713
Training Epoch: 89 [34048/50048]	Loss: 0.1050
Training Epoch: 89 [34176/50048]	Loss: 0.0817
Training Epoch: 89 [34304/50048]	Loss: 0.0335
Training Epoch: 89 [34432/50048]	Loss: 0.0753
Training Epoch: 89 [34560/50048]	Loss: 0.0411
Training Epoch: 89 [34688/50048]	Loss: 0.0360
Training Epoch: 89 [34816/50048]	Loss: 0.0670
Training Epoch: 89 [34944/50048]	Loss: 0.1146
Training Epoch: 89 [35072/50048]	Loss: 0.0850
Training Epoch: 89 [35200/50048]	Loss: 0.1138
Training Epoch: 89 [35328/50048]	Loss: 0.0555
Training Epoch: 89 [35456/50048]	Loss: 0.1049
Training Epoch: 89 [35584/50048]	Loss: 0.1251
Training Epoch: 89 [35712/50048]	Loss: 0.0317
Training Epoch: 89 [35840/50048]	Loss: 0.0966
Training Epoch: 89 [35968/50048]	Loss: 0.0968
Training Epoch: 89 [36096/50048]	Loss: 0.0621
Training Epoch: 89 [36224/50048]	Loss: 0.1395
Training Epoch: 89 [36352/50048]	Loss: 0.0840
Training Epoch: 89 [36480/50048]	Loss: 0.0481
Training Epoch: 89 [36608/50048]	Loss: 0.0820
Training Epoch: 89 [36736/50048]	Loss: 0.0655
Training Epoch: 89 [36864/50048]	Loss: 0.0794
Training Epoch: 89 [36992/50048]	Loss: 0.0768
Training Epoch: 89 [37120/50048]	Loss: 0.0521
Training Epoch: 89 [37248/50048]	Loss: 0.0263
Training Epoch: 89 [37376/50048]	Loss: 0.1271
Training Epoch: 89 [37504/50048]	Loss: 0.0533
Training Epoch: 89 [37632/50048]	Loss: 0.0387
Training Epoch: 89 [37760/50048]	Loss: 0.0867
Training Epoch: 89 [37888/50048]	Loss: 0.1485
Training Epoch: 89 [38016/50048]	Loss: 0.1117
Training Epoch: 89 [38144/50048]	Loss: 0.0698
Training Epoch: 89 [38272/50048]	Loss: 0.0731
Training Epoch: 89 [38400/50048]	Loss: 0.1731
Training Epoch: 89 [38528/50048]	Loss: 0.0899
Training Epoch: 89 [38656/50048]	Loss: 0.0788
Training Epoch: 89 [38784/50048]	Loss: 0.1354
Training Epoch: 89 [38912/50048]	Loss: 0.0610
Training Epoch: 89 [39040/50048]	Loss: 0.1501
Training Epoch: 89 [39168/50048]	Loss: 0.0393
Training Epoch: 89 [39296/50048]	Loss: 0.1465
Training Epoch: 89 [39424/50048]	Loss: 0.1537
Training Epoch: 89 [39552/50048]	Loss: 0.0531
Training Epoch: 89 [39680/50048]	Loss: 0.0452
Training Epoch: 89 [39808/50048]	Loss: 0.0959
Training Epoch: 89 [39936/50048]	Loss: 0.1353
Training Epoch: 89 [40064/50048]	Loss: 0.0551
Training Epoch: 89 [40192/50048]	Loss: 0.0416
Training Epoch: 89 [40320/50048]	Loss: 0.0636
Training Epoch: 89 [40448/50048]	Loss: 0.1004
Training Epoch: 89 [40576/50048]	Loss: 0.0942
Training Epoch: 89 [40704/50048]	Loss: 0.1100
Training Epoch: 89 [40832/50048]	Loss: 0.1005
Training Epoch: 89 [40960/50048]	Loss: 0.0903
Training Epoch: 89 [41088/50048]	Loss: 0.1431
Training Epoch: 89 [41216/50048]	Loss: 0.0924
Training Epoch: 89 [41344/50048]	Loss: 0.1058
Training Epoch: 89 [41472/50048]	Loss: 0.1127
Training Epoch: 89 [41600/50048]	Loss: 0.0981
Training Epoch: 89 [41728/50048]	Loss: 0.1043
Training Epoch: 89 [41856/50048]	Loss: 0.1035
Training Epoch: 89 [41984/50048]	Loss: 0.1267
Training Epoch: 89 [42112/50048]	Loss: 0.1417
Training Epoch: 89 [42240/50048]	Loss: 0.1282
Training Epoch: 89 [42368/50048]	Loss: 0.1255
Training Epoch: 89 [42496/50048]	Loss: 0.0843
Training Epoch: 89 [42624/50048]	Loss: 0.1800
Training Epoch: 89 [42752/50048]	Loss: 0.0774
Training Epoch: 89 [42880/50048]	Loss: 0.1056
Training Epoch: 89 [43008/50048]	Loss: 0.1124
Training Epoch: 89 [43136/50048]	Loss: 0.0914
Training Epoch: 89 [43264/50048]	Loss: 0.0669
Training Epoch: 89 [43392/50048]	Loss: 0.0606
Training Epoch: 89 [43520/50048]	Loss: 0.1004
Training Epoch: 89 [43648/50048]	Loss: 0.1465
Training Epoch: 89 [43776/50048]	Loss: 0.0607
Training Epoch: 89 [43904/50048]	Loss: 0.0709
Training Epoch: 89 [44032/50048]	Loss: 0.0745
Training Epoch: 89 [44160/50048]	Loss: 0.1073
Training Epoch: 89 [44288/50048]	Loss: 0.0673
Training Epoch: 89 [44416/50048]	Loss: 0.0648
Training Epoch: 89 [44544/50048]	Loss: 0.0483
Training Epoch: 89 [44672/50048]	Loss: 0.0762
Training Epoch: 89 [44800/50048]	Loss: 0.1146
Training Epoch: 89 [44928/50048]	Loss: 0.0764
Training Epoch: 89 [45056/50048]	Loss: 0.1094
Training Epoch: 89 [45184/50048]	Loss: 0.0583
Training Epoch: 89 [45312/50048]	Loss: 0.1243
Training Epoch: 89 [45440/50048]	Loss: 0.1185
Training Epoch: 89 [45568/50048]	Loss: 0.0564
Training Epoch: 89 [45696/50048]	Loss: 0.1407
2022-12-06 08:23:10,143 [ZeusDataLoader(train)] train epoch 90 done: time=86.45 energy=10496.26
2022-12-06 08:23:10,144 [ZeusDataLoader(eval)] Epoch 90 begin.
Training Epoch: 89 [45824/50048]	Loss: 0.0992
Training Epoch: 89 [45952/50048]	Loss: 0.1037
Training Epoch: 89 [46080/50048]	Loss: 0.0875
Training Epoch: 89 [46208/50048]	Loss: 0.1813
Training Epoch: 89 [46336/50048]	Loss: 0.1671
Training Epoch: 89 [46464/50048]	Loss: 0.0997
Training Epoch: 89 [46592/50048]	Loss: 0.1291
Training Epoch: 89 [46720/50048]	Loss: 0.0905
Training Epoch: 89 [46848/50048]	Loss: 0.1416
Training Epoch: 89 [46976/50048]	Loss: 0.0469
Training Epoch: 89 [47104/50048]	Loss: 0.1929
Training Epoch: 89 [47232/50048]	Loss: 0.0621
Training Epoch: 89 [47360/50048]	Loss: 0.1422
Training Epoch: 89 [47488/50048]	Loss: 0.1022
Training Epoch: 89 [47616/50048]	Loss: 0.1830
Training Epoch: 89 [47744/50048]	Loss: 0.1369
Training Epoch: 89 [47872/50048]	Loss: 0.1039
Training Epoch: 89 [48000/50048]	Loss: 0.0572
Training Epoch: 89 [48128/50048]	Loss: 0.0938
Training Epoch: 89 [48256/50048]	Loss: 0.0802
Training Epoch: 89 [48384/50048]	Loss: 0.0842
Training Epoch: 89 [48512/50048]	Loss: 0.0972
Training Epoch: 89 [48640/50048]	Loss: 0.0818
Training Epoch: 89 [48768/50048]	Loss: 0.0647
Training Epoch: 89 [48896/50048]	Loss: 0.0331
Training Epoch: 89 [49024/50048]	Loss: 0.0933
Training Epoch: 89 [49152/50048]	Loss: 0.1137
Training Epoch: 89 [49280/50048]	Loss: 0.0517
Training Epoch: 89 [49408/50048]	Loss: 0.0924
Training Epoch: 89 [49536/50048]	Loss: 0.1671
Training Epoch: 89 [49664/50048]	Loss: 0.0766
Training Epoch: 89 [49792/50048]	Loss: 0.0867
Training Epoch: 89 [49920/50048]	Loss: 0.1280
Training Epoch: 89 [50048/50048]	Loss: 0.1562
2022-12-06 13:23:13.878 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 08:23:13,927 [ZeusDataLoader(eval)] eval epoch 90 done: time=3.77 energy=452.53
2022-12-06 08:23:13,927 [ZeusDataLoader(train)] Up to epoch 90: time=8118.28, energy=985447.17, cost=1203073.48
2022-12-06 08:23:13,927 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 08:23:13,928 [ZeusDataLoader(train)] Expected next epoch: time=8208.08, energy=996245.19, cost=1216329.86
2022-12-06 08:23:13,928 [ZeusDataLoader(train)] Epoch 91 begin.
Validation Epoch: 89, Average loss: 0.0184, Accuracy: 0.6384
2022-12-06 08:23:14,117 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:23:14,118 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:23:14.120 [ZeusMonitor] Monitor started.
2022-12-06 13:23:14.120 [ZeusMonitor] Running indefinitely. 2022-12-06 13:23:14.120 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:23:14.120 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e91+gpu0.power.log
Training Epoch: 90 [128/50048]	Loss: 0.0604
Training Epoch: 90 [256/50048]	Loss: 0.0898
Training Epoch: 90 [384/50048]	Loss: 0.0530
Training Epoch: 90 [512/50048]	Loss: 0.0393
Training Epoch: 90 [640/50048]	Loss: 0.0383
Training Epoch: 90 [768/50048]	Loss: 0.0935
Training Epoch: 90 [896/50048]	Loss: 0.0929
Training Epoch: 90 [1024/50048]	Loss: 0.1077
Training Epoch: 90 [1152/50048]	Loss: 0.1110
Training Epoch: 90 [1280/50048]	Loss: 0.1868
Training Epoch: 90 [1408/50048]	Loss: 0.1102
Training Epoch: 90 [1536/50048]	Loss: 0.1247
Training Epoch: 90 [1664/50048]	Loss: 0.0664
Training Epoch: 90 [1792/50048]	Loss: 0.0783
Training Epoch: 90 [1920/50048]	Loss: 0.0724
Training Epoch: 90 [2048/50048]	Loss: 0.0393
Training Epoch: 90 [2176/50048]	Loss: 0.0754
Training Epoch: 90 [2304/50048]	Loss: 0.0713
Training Epoch: 90 [2432/50048]	Loss: 0.1015
Training Epoch: 90 [2560/50048]	Loss: 0.0550
Training Epoch: 90 [2688/50048]	Loss: 0.0626
Training Epoch: 90 [2816/50048]	Loss: 0.0773
Training Epoch: 90 [2944/50048]	Loss: 0.0935
Training Epoch: 90 [3072/50048]	Loss: 0.0618
Training Epoch: 90 [3200/50048]	Loss: 0.1258
Training Epoch: 90 [3328/50048]	Loss: 0.0852
Training Epoch: 90 [3456/50048]	Loss: 0.0702
Training Epoch: 90 [3584/50048]	Loss: 0.0735
Training Epoch: 90 [3712/50048]	Loss: 0.0500
Training Epoch: 90 [3840/50048]	Loss: 0.0763
Training Epoch: 90 [3968/50048]	Loss: 0.1436
Training Epoch: 90 [4096/50048]	Loss: 0.0584
Training Epoch: 90 [4224/50048]	Loss: 0.0955
Training Epoch: 90 [4352/50048]	Loss: 0.1761
Training Epoch: 90 [4480/50048]	Loss: 0.0986
Training Epoch: 90 [4608/50048]	Loss: 0.1227
Training Epoch: 90 [4736/50048]	Loss: 0.0433
Training Epoch: 90 [4864/50048]	Loss: 0.0652
Training Epoch: 90 [4992/50048]	Loss: 0.0931
Training Epoch: 90 [5120/50048]	Loss: 0.1194
Training Epoch: 90 [5248/50048]	Loss: 0.0715
Training Epoch: 90 [5376/50048]	Loss: 0.0708
Training Epoch: 90 [5504/50048]	Loss: 0.0294
Training Epoch: 90 [5632/50048]	Loss: 0.0653
Training Epoch: 90 [5760/50048]	Loss: 0.0611
Training Epoch: 90 [5888/50048]	Loss: 0.0462
Training Epoch: 90 [6016/50048]	Loss: 0.0711
Training Epoch: 90 [6144/50048]	Loss: 0.0902
Training Epoch: 90 [6272/50048]	Loss: 0.0411
Training Epoch: 90 [6400/50048]	Loss: 0.1008
Training Epoch: 90 [6528/50048]	Loss: 0.1353
Training Epoch: 90 [6656/50048]	Loss: 0.0549
Training Epoch: 90 [6784/50048]	Loss: 0.0594
Training Epoch: 90 [6912/50048]	Loss: 0.1138
Training Epoch: 90 [7040/50048]	Loss: 0.0566
Training Epoch: 90 [7168/50048]	Loss: 0.0787
Training Epoch: 90 [7296/50048]	Loss: 0.0617
Training Epoch: 90 [7424/50048]	Loss: 0.1527
Training Epoch: 90 [7552/50048]	Loss: 0.0551
Training Epoch: 90 [7680/50048]	Loss: 0.0771
Training Epoch: 90 [7808/50048]	Loss: 0.0759
Training Epoch: 90 [7936/50048]	Loss: 0.1102
Training Epoch: 90 [8064/50048]	Loss: 0.0450
Training Epoch: 90 [8192/50048]	Loss: 0.0728
Training Epoch: 90 [8320/50048]	Loss: 0.1470
Training Epoch: 90 [8448/50048]	Loss: 0.0951
Training Epoch: 90 [8576/50048]	Loss: 0.1002
Training Epoch: 90 [8704/50048]	Loss: 0.0890
Training Epoch: 90 [8832/50048]	Loss: 0.0405
Training Epoch: 90 [8960/50048]	Loss: 0.1017
Training Epoch: 90 [9088/50048]	Loss: 0.0651
Training Epoch: 90 [9216/50048]	Loss: 0.0283
Training Epoch: 90 [9344/50048]	Loss: 0.0444
Training Epoch: 90 [9472/50048]	Loss: 0.1151
Training Epoch: 90 [9600/50048]	Loss: 0.0681
Training Epoch: 90 [9728/50048]	Loss: 0.0758
Training Epoch: 90 [9856/50048]	Loss: 0.0808
Training Epoch: 90 [9984/50048]	Loss: 0.0614
Training Epoch: 90 [10112/50048]	Loss: 0.0622
Training Epoch: 90 [10240/50048]	Loss: 0.0962
Training Epoch: 90 [10368/50048]	Loss: 0.0901
Training Epoch: 90 [10496/50048]	Loss: 0.1046
Training Epoch: 90 [10624/50048]	Loss: 0.0946
Training Epoch: 90 [10752/50048]	Loss: 0.1558
Training Epoch: 90 [10880/50048]	Loss: 0.0794
Training Epoch: 90 [11008/50048]	Loss: 0.0570
Training Epoch: 90 [11136/50048]	Loss: 0.0604
Training Epoch: 90 [11264/50048]	Loss: 0.1009
Training Epoch: 90 [11392/50048]	Loss: 0.0832
Training Epoch: 90 [11520/50048]	Loss: 0.1112
Training Epoch: 90 [11648/50048]	Loss: 0.0670
Training Epoch: 90 [11776/50048]	Loss: 0.1110
Training Epoch: 90 [11904/50048]	Loss: 0.1392
Training Epoch: 90 [12032/50048]	Loss: 0.0646
Training Epoch: 90 [12160/50048]	Loss: 0.0566
Training Epoch: 90 [12288/50048]	Loss: 0.0620
Training Epoch: 90 [12416/50048]	Loss: 0.0583
Training Epoch: 90 [12544/50048]	Loss: 0.1707
Training Epoch: 90 [12672/50048]	Loss: 0.0961
Training Epoch: 90 [12800/50048]	Loss: 0.0944
Training Epoch: 90 [12928/50048]	Loss: 0.1434
Training Epoch: 90 [13056/50048]	Loss: 0.0677
Training Epoch: 90 [13184/50048]	Loss: 0.0543
Training Epoch: 90 [13312/50048]	Loss: 0.0479
Training Epoch: 90 [13440/50048]	Loss: 0.0631
Training Epoch: 90 [13568/50048]	Loss: 0.0580
Training Epoch: 90 [13696/50048]	Loss: 0.1758
Training Epoch: 90 [13824/50048]	Loss: 0.0707
Training Epoch: 90 [13952/50048]	Loss: 0.0536
Training Epoch: 90 [14080/50048]	Loss: 0.1223
Training Epoch: 90 [14208/50048]	Loss: 0.1429
Training Epoch: 90 [14336/50048]	Loss: 0.0655
Training Epoch: 90 [14464/50048]	Loss: 0.0706
Training Epoch: 90 [14592/50048]	Loss: 0.0348
Training Epoch: 90 [14720/50048]	Loss: 0.0572
Training Epoch: 90 [14848/50048]	Loss: 0.1942
Training Epoch: 90 [14976/50048]	Loss: 0.0281
Training Epoch: 90 [15104/50048]	Loss: 0.0778
Training Epoch: 90 [15232/50048]	Loss: 0.1010
Training Epoch: 90 [15360/50048]	Loss: 0.0966
Training Epoch: 90 [15488/50048]	Loss: 0.1088
Training Epoch: 90 [15616/50048]	Loss: 0.0900
Training Epoch: 90 [15744/50048]	Loss: 0.1099
Training Epoch: 90 [15872/50048]	Loss: 0.0670
Training Epoch: 90 [16000/50048]	Loss: 0.0353
Training Epoch: 90 [16128/50048]	Loss: 0.1311
Training Epoch: 90 [16256/50048]	Loss: 0.0657
Training Epoch: 90 [16384/50048]	Loss: 0.1258
Training Epoch: 90 [16512/50048]	Loss: 0.0913
Training Epoch: 90 [16640/50048]	Loss: 0.0523
Training Epoch: 90 [16768/50048]	Loss: 0.0946
Training Epoch: 90 [16896/50048]	Loss: 0.0967
Training Epoch: 90 [17024/50048]	Loss: 0.0715
Training Epoch: 90 [17152/50048]	Loss: 0.1060
Training Epoch: 90 [17280/50048]	Loss: 0.0267
Training Epoch: 90 [17408/50048]	Loss: 0.1113
Training Epoch: 90 [17536/50048]	Loss: 0.0865
Training Epoch: 90 [17664/50048]	Loss: 0.1365
Training Epoch: 90 [17792/50048]	Loss: 0.0915
Training Epoch: 90 [17920/50048]	Loss: 0.0421
Training Epoch: 90 [18048/50048]	Loss: 0.1070
Training Epoch: 90 [18176/50048]	Loss: 0.0847
Training Epoch: 90 [18304/50048]	Loss: 0.0732
Training Epoch: 90 [18432/50048]	Loss: 0.0464
Training Epoch: 90 [18560/50048]	Loss: 0.0766
Training Epoch: 90 [18688/50048]	Loss: 0.0726
Training Epoch: 90 [18816/50048]	Loss: 0.1256
Training Epoch: 90 [18944/50048]	Loss: 0.0368
Training Epoch: 90 [19072/50048]	Loss: 0.1210
Training Epoch: 90 [19200/50048]	Loss: 0.0518
Training Epoch: 90 [19328/50048]	Loss: 0.1353
Training Epoch: 90 [19456/50048]	Loss: 0.1059
Training Epoch: 90 [19584/50048]	Loss: 0.0292
Training Epoch: 90 [19712/50048]	Loss: 0.0852
Training Epoch: 90 [19840/50048]	Loss: 0.1011
Training Epoch: 90 [19968/50048]	Loss: 0.0654
Training Epoch: 90 [20096/50048]	Loss: 0.0753
Training Epoch: 90 [20224/50048]	Loss: 0.0886
Training Epoch: 90 [20352/50048]	Loss: 0.1661
Training Epoch: 90 [20480/50048]	Loss: 0.0864
Training Epoch: 90 [20608/50048]	Loss: 0.0731
Training Epoch: 90 [20736/50048]	Loss: 0.0631
Training Epoch: 90 [20864/50048]	Loss: 0.0630
Training Epoch: 90 [20992/50048]	Loss: 0.2112
Training Epoch: 90 [21120/50048]	Loss: 0.0457
Training Epoch: 90 [21248/50048]	Loss: 0.0709
Training Epoch: 90 [21376/50048]	Loss: 0.1272
Training Epoch: 90 [21504/50048]	Loss: 0.1205
Training Epoch: 90 [21632/50048]	Loss: 0.0650
Training Epoch: 90 [21760/50048]	Loss: 0.1411
Training Epoch: 90 [21888/50048]	Loss: 0.0715
Training Epoch: 90 [22016/50048]	Loss: 0.0532
Training Epoch: 90 [22144/50048]	Loss: 0.0907
Training Epoch: 90 [22272/50048]	Loss: 0.0532
Training Epoch: 90 [22400/50048]	Loss: 0.0762
Training Epoch: 90 [22528/50048]	Loss: 0.0799
Training Epoch: 90 [22656/50048]	Loss: 0.0531
Training Epoch: 90 [22784/50048]	Loss: 0.1505
Training Epoch: 90 [22912/50048]	Loss: 0.1107
Training Epoch: 90 [23040/50048]	Loss: 0.1069
Training Epoch: 90 [23168/50048]	Loss: 0.0570
Training Epoch: 90 [23296/50048]	Loss: 0.0647
Training Epoch: 90 [23424/50048]	Loss: 0.0909
Training Epoch: 90 [23552/50048]	Loss: 0.1008
Training Epoch: 90 [23680/50048]	Loss: 0.0635
Training Epoch: 90 [23808/50048]	Loss: 0.0771
Training Epoch: 90 [23936/50048]	Loss: 0.0622
Training Epoch: 90 [24064/50048]	Loss: 0.0621
Training Epoch: 90 [24192/50048]	Loss: 0.0844
Training Epoch: 90 [24320/50048]	Loss: 0.0642
Training Epoch: 90 [24448/50048]	Loss: 0.1171
Training Epoch: 90 [24576/50048]	Loss: 0.0734
Training Epoch: 90 [24704/50048]	Loss: 0.0900
Training Epoch: 90 [24832/50048]	Loss: 0.1108
Training Epoch: 90 [24960/50048]	Loss: 0.0894
Training Epoch: 90 [25088/50048]	Loss: 0.0681
Training Epoch: 90 [25216/50048]	Loss: 0.0301
Training Epoch: 90 [25344/50048]	Loss: 0.0946
Training Epoch: 90 [25472/50048]	Loss: 0.0427
Training Epoch: 90 [25600/50048]	Loss: 0.0755
Training Epoch: 90 [25728/50048]	Loss: 0.1171
Training Epoch: 90 [25856/50048]	Loss: 0.0998
Training Epoch: 90 [25984/50048]	Loss: 0.1009
Training Epoch: 90 [26112/50048]	Loss: 0.1625
Training Epoch: 90 [26240/50048]	Loss: 0.1328
Training Epoch: 90 [26368/50048]	Loss: 0.1296
Training Epoch: 90 [26496/50048]	Loss: 0.0569
Training Epoch: 90 [26624/50048]	Loss: 0.0657
Training Epoch: 90 [26752/50048]	Loss: 0.1442
Training Epoch: 90 [26880/50048]	Loss: 0.1093
Training Epoch: 90 [27008/50048]	Loss: 0.1482
Training Epoch: 90 [27136/50048]	Loss: 0.0628
Training Epoch: 90 [27264/50048]	Loss: 0.1391
Training Epoch: 90 [27392/50048]	Loss: 0.0516
Training Epoch: 90 [27520/50048]	Loss: 0.1478
Training Epoch: 90 [27648/50048]	Loss: 0.1041
Training Epoch: 90 [27776/50048]	Loss: 0.0855
Training Epoch: 90 [27904/50048]	Loss: 0.0925
Training Epoch: 90 [28032/50048]	Loss: 0.1045
Training Epoch: 90 [28160/50048]	Loss: 0.1386
Training Epoch: 90 [28288/50048]	Loss: 0.0981
Training Epoch: 90 [28416/50048]	Loss: 0.0815
Training Epoch: 90 [28544/50048]	Loss: 0.1664
Training Epoch: 90 [28672/50048]	Loss: 0.1271
Training Epoch: 90 [28800/50048]	Loss: 0.0331
Training Epoch: 90 [28928/50048]	Loss: 0.1203
Training Epoch: 90 [29056/50048]	Loss: 0.1253
Training Epoch: 90 [29184/50048]	Loss: 0.0536
Training Epoch: 90 [29312/50048]	Loss: 0.1020
Training Epoch: 90 [29440/50048]	Loss: 0.0731
Training Epoch: 90 [29568/50048]	Loss: 0.1080
Training Epoch: 90 [29696/50048]	Loss: 0.0609
Training Epoch: 90 [29824/50048]	Loss: 0.0653
Training Epoch: 90 [29952/50048]	Loss: 0.0678
Training Epoch: 90 [30080/50048]	Loss: 0.0520
Training Epoch: 90 [30208/50048]	Loss: 0.0831
Training Epoch: 90 [30336/50048]	Loss: 0.0854
Training Epoch: 90 [30464/50048]	Loss: 0.0804
Training Epoch: 90 [30592/50048]	Loss: 0.0633
Training Epoch: 90 [30720/50048]	Loss: 0.1387
Training Epoch: 90 [30848/50048]	Loss: 0.0460
Training Epoch: 90 [30976/50048]	Loss: 0.1959
Training Epoch: 90 [31104/50048]	Loss: 0.1822
Training Epoch: 90 [31232/50048]	Loss: 0.0603
Training Epoch: 90 [31360/50048]	Loss: 0.0542
Training Epoch: 90 [31488/50048]	Loss: 0.0848
Training Epoch: 90 [31616/50048]	Loss: 0.0943
Training Epoch: 90 [31744/50048]	Loss: 0.0877
Training Epoch: 90 [31872/50048]	Loss: 0.0852
Training Epoch: 90 [32000/50048]	Loss: 0.0583
Training Epoch: 90 [32128/50048]	Loss: 0.1530
Training Epoch: 90 [32256/50048]	Loss: 0.1010
Training Epoch: 90 [32384/50048]	Loss: 0.0707
Training Epoch: 90 [32512/50048]	Loss: 0.0886
Training Epoch: 90 [32640/50048]	Loss: 0.1174
Training Epoch: 90 [32768/50048]	Loss: 0.1061
Training Epoch: 90 [32896/50048]	Loss: 0.1551
Training Epoch: 90 [33024/50048]	Loss: 0.0808
Training Epoch: 90 [33152/50048]	Loss: 0.0534
Training Epoch: 90 [33280/50048]	Loss: 0.0472
Training Epoch: 90 [33408/50048]	Loss: 0.0936
Training Epoch: 90 [33536/50048]	Loss: 0.1345
Training Epoch: 90 [33664/50048]	Loss: 0.0663
Training Epoch: 90 [33792/50048]	Loss: 0.0440
Training Epoch: 90 [33920/50048]	Loss: 0.0809
Training Epoch: 90 [34048/50048]	Loss: 0.0988
Training Epoch: 90 [34176/50048]	Loss: 0.1197
Training Epoch: 90 [34304/50048]	Loss: 0.1382
Training Epoch: 90 [34432/50048]	Loss: 0.1161
Training Epoch: 90 [34560/50048]	Loss: 0.0699
Training Epoch: 90 [34688/50048]	Loss: 0.1280
Training Epoch: 90 [34816/50048]	Loss: 0.0724
Training Epoch: 90 [34944/50048]	Loss: 0.0650
Training Epoch: 90 [35072/50048]	Loss: 0.1139
Training Epoch: 90 [35200/50048]	Loss: 0.0485
Training Epoch: 90 [35328/50048]	Loss: 0.0597
Training Epoch: 90 [35456/50048]	Loss: 0.1208
Training Epoch: 90 [35584/50048]	Loss: 0.1029
Training Epoch: 90 [35712/50048]	Loss: 0.0704
Training Epoch: 90 [35840/50048]	Loss: 0.0750
Training Epoch: 90 [35968/50048]	Loss: 0.1141
Training Epoch: 90 [36096/50048]	Loss: 0.1077
Training Epoch: 90 [36224/50048]	Loss: 0.0648
Training Epoch: 90 [36352/50048]	Loss: 0.0906
Training Epoch: 90 [36480/50048]	Loss: 0.1034
Training Epoch: 90 [36608/50048]	Loss: 0.0712
Training Epoch: 90 [36736/50048]	Loss: 0.0953
Training Epoch: 90 [36864/50048]	Loss: 0.1194
Training Epoch: 90 [36992/50048]	Loss: 0.1046
Training Epoch: 90 [37120/50048]	Loss: 0.0865
Training Epoch: 90 [37248/50048]	Loss: 0.0749
Training Epoch: 90 [37376/50048]	Loss: 0.0743
Training Epoch: 90 [37504/50048]	Loss: 0.0703
Training Epoch: 90 [37632/50048]	Loss: 0.0373
Training Epoch: 90 [37760/50048]	Loss: 0.1388
Training Epoch: 90 [37888/50048]	Loss: 0.1542
Training Epoch: 90 [38016/50048]	Loss: 0.0806
Training Epoch: 90 [38144/50048]	Loss: 0.2119
Training Epoch: 90 [38272/50048]	Loss: 0.0818
Training Epoch: 90 [38400/50048]	Loss: 0.1003
Training Epoch: 90 [38528/50048]	Loss: 0.0873
Training Epoch: 90 [38656/50048]	Loss: 0.0902
Training Epoch: 90 [38784/50048]	Loss: 0.0879
Training Epoch: 90 [38912/50048]	Loss: 0.0777
Training Epoch: 90 [39040/50048]	Loss: 0.0555
Training Epoch: 90 [39168/50048]	Loss: 0.1097
Training Epoch: 90 [39296/50048]	Loss: 0.1213
Training Epoch: 90 [39424/50048]	Loss: 0.0866
Training Epoch: 90 [39552/50048]	Loss: 0.1270
Training Epoch: 90 [39680/50048]	Loss: 0.1058
Training Epoch: 90 [39808/50048]	Loss: 0.1349
Training Epoch: 90 [39936/50048]	Loss: 0.0594
Training Epoch: 90 [40064/50048]	Loss: 0.1301
Training Epoch: 90 [40192/50048]	Loss: 0.0827
Training Epoch: 90 [40320/50048]	Loss: 0.1441
Training Epoch: 90 [40448/50048]	Loss: 0.1345
Training Epoch: 90 [40576/50048]	Loss: 0.0416
Training Epoch: 90 [40704/50048]	Loss: 0.0524
Training Epoch: 90 [40832/50048]	Loss: 0.1700
Training Epoch: 90 [40960/50048]	Loss: 0.1250
Training Epoch: 90 [41088/50048]	Loss: 0.1540
Training Epoch: 90 [41216/50048]	Loss: 0.0932
Training Epoch: 90 [41344/50048]	Loss: 0.1146
Training Epoch: 90 [41472/50048]	Loss: 0.1010
Training Epoch: 90 [41600/50048]	Loss: 0.0582
Training Epoch: 90 [41728/50048]	Loss: 0.0482
Training Epoch: 90 [41856/50048]	Loss: 0.1664
Training Epoch: 90 [41984/50048]	Loss: 0.1151
Training Epoch: 90 [42112/50048]	Loss: 0.1237
Training Epoch: 90 [42240/50048]	Loss: 0.0669
Training Epoch: 90 [42368/50048]	Loss: 0.0825
Training Epoch: 90 [42496/50048]	Loss: 0.0790
Training Epoch: 90 [42624/50048]	Loss: 0.1335
Training Epoch: 90 [42752/50048]	Loss: 0.1284
Training Epoch: 90 [42880/50048]	Loss: 0.1069
Training Epoch: 90 [43008/50048]	Loss: 0.0753
Training Epoch: 90 [43136/50048]	Loss: 0.0935
Training Epoch: 90 [43264/50048]	Loss: 0.1126
Training Epoch: 90 [43392/50048]	Loss: 0.0628
Training Epoch: 90 [43520/50048]	Loss: 0.1270
Training Epoch: 90 [43648/50048]	Loss: 0.0732
Training Epoch: 90 [43776/50048]	Loss: 0.0710
Training Epoch: 90 [43904/50048]	Loss: 0.0697
Training Epoch: 90 [44032/50048]	Loss: 0.1364
Training Epoch: 90 [44160/50048]	Loss: 0.1631
Training Epoch: 90 [44288/50048]	Loss: 0.0382
Training Epoch: 90 [44416/50048]	Loss: 0.1033
Training Epoch: 90 [44544/50048]	Loss: 0.0790
Training Epoch: 90 [44672/50048]	Loss: 0.1190
Training Epoch: 90 [44800/50048]	Loss: 0.1118
Training Epoch: 90 [44928/50048]	Loss: 0.0747
Training Epoch: 90 [45056/50048]	Loss: 0.0972
Training Epoch: 90 [45184/50048]	Loss: 0.1734
Training Epoch: 90 [45312/50048]	Loss: 0.1280
Training Epoch: 90 [45440/50048]	Loss: 0.1445
Training Epoch: 90 [45568/50048]	Loss: 0.1117
Training Epoch: 90 [45696/50048]	Loss: 0.0862
2022-12-06 08:24:40,480 [ZeusDataLoader(train)] train epoch 91 done: time=86.54 energy=10501.55
2022-12-06 08:24:40,481 [ZeusDataLoader(eval)] Epoch 91 begin.
Training Epoch: 90 [45824/50048]	Loss: 0.0783
Training Epoch: 90 [45952/50048]	Loss: 0.1378
Training Epoch: 90 [46080/50048]	Loss: 0.1031
Training Epoch: 90 [46208/50048]	Loss: 0.1414
Training Epoch: 90 [46336/50048]	Loss: 0.1306
Training Epoch: 90 [46464/50048]	Loss: 0.0716
Training Epoch: 90 [46592/50048]	Loss: 0.1901
Training Epoch: 90 [46720/50048]	Loss: 0.0595
Training Epoch: 90 [46848/50048]	Loss: 0.1157
Training Epoch: 90 [46976/50048]	Loss: 0.1454
Training Epoch: 90 [47104/50048]	Loss: 0.0928
Training Epoch: 90 [47232/50048]	Loss: 0.1509
Training Epoch: 90 [47360/50048]	Loss: 0.0563
Training Epoch: 90 [47488/50048]	Loss: 0.0481
Training Epoch: 90 [47616/50048]	Loss: 0.1458
Training Epoch: 90 [47744/50048]	Loss: 0.1213
Training Epoch: 90 [47872/50048]	Loss: 0.0728
Training Epoch: 90 [48000/50048]	Loss: 0.0665
Training Epoch: 90 [48128/50048]	Loss: 0.0774
Training Epoch: 90 [48256/50048]	Loss: 0.1024
Training Epoch: 90 [48384/50048]	Loss: 0.0890
Training Epoch: 90 [48512/50048]	Loss: 0.0698
Training Epoch: 90 [48640/50048]	Loss: 0.0695
Training Epoch: 90 [48768/50048]	Loss: 0.1445
Training Epoch: 90 [48896/50048]	Loss: 0.1094
Training Epoch: 90 [49024/50048]	Loss: 0.0637
Training Epoch: 90 [49152/50048]	Loss: 0.1073
Training Epoch: 90 [49280/50048]	Loss: 0.1694
Training Epoch: 90 [49408/50048]	Loss: 0.1285
Training Epoch: 90 [49536/50048]	Loss: 0.1245
Training Epoch: 90 [49664/50048]	Loss: 0.1153
Training Epoch: 90 [49792/50048]	Loss: 0.0700
Training Epoch: 90 [49920/50048]	Loss: 0.1300
Training Epoch: 90 [50048/50048]	Loss: 0.1984
2022-12-06 13:24:44.195 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 08:24:44,214 [ZeusDataLoader(eval)] eval epoch 91 done: time=3.72 energy=451.70
2022-12-06 08:24:44,215 [ZeusDataLoader(train)] Up to epoch 91: time=8208.55, energy=996400.43, cost=1216448.34
2022-12-06 08:24:44,215 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 08:24:44,215 [ZeusDataLoader(train)] Expected next epoch: time=8298.35, energy=1007198.44, cost=1229704.73
2022-12-06 08:24:44,216 [ZeusDataLoader(train)] Epoch 92 begin.
Validation Epoch: 90, Average loss: 0.0182, Accuracy: 0.6384
2022-12-06 08:24:44,393 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:24:44,394 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:24:44.398 [ZeusMonitor] Monitor started.
2022-12-06 13:24:44.398 [ZeusMonitor] Running indefinitely. 2022-12-06 13:24:44.398 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:24:44.398 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e92+gpu0.power.log
Training Epoch: 91 [128/50048]	Loss: 0.1127
Training Epoch: 91 [256/50048]	Loss: 0.0966
Training Epoch: 91 [384/50048]	Loss: 0.0310
Training Epoch: 91 [512/50048]	Loss: 0.0984
Training Epoch: 91 [640/50048]	Loss: 0.0587
Training Epoch: 91 [768/50048]	Loss: 0.1129
Training Epoch: 91 [896/50048]	Loss: 0.0346
Training Epoch: 91 [1024/50048]	Loss: 0.1135
Training Epoch: 91 [1152/50048]	Loss: 0.1044
Training Epoch: 91 [1280/50048]	Loss: 0.1306
Training Epoch: 91 [1408/50048]	Loss: 0.0544
Training Epoch: 91 [1536/50048]	Loss: 0.0967
Training Epoch: 91 [1664/50048]	Loss: 0.0365
Training Epoch: 91 [1792/50048]	Loss: 0.0647
Training Epoch: 91 [1920/50048]	Loss: 0.0564
Training Epoch: 91 [2048/50048]	Loss: 0.0834
Training Epoch: 91 [2176/50048]	Loss: 0.0946
Training Epoch: 91 [2304/50048]	Loss: 0.0699
Training Epoch: 91 [2432/50048]	Loss: 0.0464
Training Epoch: 91 [2560/50048]	Loss: 0.0540
Training Epoch: 91 [2688/50048]	Loss: 0.0815
Training Epoch: 91 [2816/50048]	Loss: 0.0603
Training Epoch: 91 [2944/50048]	Loss: 0.0726
Training Epoch: 91 [3072/50048]	Loss: 0.1229
Training Epoch: 91 [3200/50048]	Loss: 0.1418
Training Epoch: 91 [3328/50048]	Loss: 0.0327
Training Epoch: 91 [3456/50048]	Loss: 0.0616
Training Epoch: 91 [3584/50048]	Loss: 0.0547
Training Epoch: 91 [3712/50048]	Loss: 0.1073
Training Epoch: 91 [3840/50048]	Loss: 0.0685
Training Epoch: 91 [3968/50048]	Loss: 0.0893
Training Epoch: 91 [4096/50048]	Loss: 0.0665
Training Epoch: 91 [4224/50048]	Loss: 0.0815
Training Epoch: 91 [4352/50048]	Loss: 0.0906
Training Epoch: 91 [4480/50048]	Loss: 0.0284
Training Epoch: 91 [4608/50048]	Loss: 0.0513
Training Epoch: 91 [4736/50048]	Loss: 0.0800
Training Epoch: 91 [4864/50048]	Loss: 0.1172
Training Epoch: 91 [4992/50048]	Loss: 0.0846
Training Epoch: 91 [5120/50048]	Loss: 0.0708
Training Epoch: 91 [5248/50048]	Loss: 0.0686
Training Epoch: 91 [5376/50048]	Loss: 0.0447
Training Epoch: 91 [5504/50048]	Loss: 0.0881
Training Epoch: 91 [5632/50048]	Loss: 0.1147
Training Epoch: 91 [5760/50048]	Loss: 0.0988
Training Epoch: 91 [5888/50048]	Loss: 0.0775
Training Epoch: 91 [6016/50048]	Loss: 0.0401
Training Epoch: 91 [6144/50048]	Loss: 0.1082
Training Epoch: 91 [6272/50048]	Loss: 0.0420
Training Epoch: 91 [6400/50048]	Loss: 0.0626
Training Epoch: 91 [6528/50048]	Loss: 0.0599
Training Epoch: 91 [6656/50048]	Loss: 0.0538
Training Epoch: 91 [6784/50048]	Loss: 0.0492
Training Epoch: 91 [6912/50048]	Loss: 0.1576
Training Epoch: 91 [7040/50048]	Loss: 0.0326
Training Epoch: 91 [7168/50048]	Loss: 0.1261
Training Epoch: 91 [7296/50048]	Loss: 0.0817
Training Epoch: 91 [7424/50048]	Loss: 0.0852
Training Epoch: 91 [7552/50048]	Loss: 0.1340
Training Epoch: 91 [7680/50048]	Loss: 0.1229
Training Epoch: 91 [7808/50048]	Loss: 0.0840
Training Epoch: 91 [7936/50048]	Loss: 0.0891
Training Epoch: 91 [8064/50048]	Loss: 0.0307
Training Epoch: 91 [8192/50048]	Loss: 0.1063
Training Epoch: 91 [8320/50048]	Loss: 0.0709
Training Epoch: 91 [8448/50048]	Loss: 0.0924
Training Epoch: 91 [8576/50048]	Loss: 0.0488
Training Epoch: 91 [8704/50048]	Loss: 0.1252
Training Epoch: 91 [8832/50048]	Loss: 0.1298
Training Epoch: 91 [8960/50048]	Loss: 0.0587
Training Epoch: 91 [9088/50048]	Loss: 0.0845
Training Epoch: 91 [9216/50048]	Loss: 0.1347
Training Epoch: 91 [9344/50048]	Loss: 0.0544
Training Epoch: 91 [9472/50048]	Loss: 0.1072
Training Epoch: 91 [9600/50048]	Loss: 0.1060
Training Epoch: 91 [9728/50048]	Loss: 0.0764
Training Epoch: 91 [9856/50048]	Loss: 0.1036
Training Epoch: 91 [9984/50048]	Loss: 0.1269
Training Epoch: 91 [10112/50048]	Loss: 0.1410
Training Epoch: 91 [10240/50048]	Loss: 0.0989
Training Epoch: 91 [10368/50048]	Loss: 0.1078
Training Epoch: 91 [10496/50048]	Loss: 0.0402
Training Epoch: 91 [10624/50048]	Loss: 0.1626
Training Epoch: 91 [10752/50048]	Loss: 0.1139
Training Epoch: 91 [10880/50048]	Loss: 0.1005
Training Epoch: 91 [11008/50048]	Loss: 0.0414
Training Epoch: 91 [11136/50048]	Loss: 0.0204
Training Epoch: 91 [11264/50048]	Loss: 0.0881
Training Epoch: 91 [11392/50048]	Loss: 0.1197
Training Epoch: 91 [11520/50048]	Loss: 0.0586
Training Epoch: 91 [11648/50048]	Loss: 0.1146
Training Epoch: 91 [11776/50048]	Loss: 0.0750
Training Epoch: 91 [11904/50048]	Loss: 0.0441
Training Epoch: 91 [12032/50048]	Loss: 0.0252
Training Epoch: 91 [12160/50048]	Loss: 0.0582
Training Epoch: 91 [12288/50048]	Loss: 0.1169
Training Epoch: 91 [12416/50048]	Loss: 0.0521
Training Epoch: 91 [12544/50048]	Loss: 0.0622
Training Epoch: 91 [12672/50048]	Loss: 0.0630
Training Epoch: 91 [12800/50048]	Loss: 0.0773
Training Epoch: 91 [12928/50048]	Loss: 0.0811
Training Epoch: 91 [13056/50048]	Loss: 0.0908
Training Epoch: 91 [13184/50048]	Loss: 0.1139
Training Epoch: 91 [13312/50048]	Loss: 0.1057
Training Epoch: 91 [13440/50048]	Loss: 0.0793
Training Epoch: 91 [13568/50048]	Loss: 0.0565
Training Epoch: 91 [13696/50048]	Loss: 0.0590
Training Epoch: 91 [13824/50048]	Loss: 0.0916
Training Epoch: 91 [13952/50048]	Loss: 0.0812
Training Epoch: 91 [14080/50048]	Loss: 0.1225
Training Epoch: 91 [14208/50048]	Loss: 0.0684
Training Epoch: 91 [14336/50048]	Loss: 0.0495
Training Epoch: 91 [14464/50048]	Loss: 0.0776
Training Epoch: 91 [14592/50048]	Loss: 0.0521
Training Epoch: 91 [14720/50048]	Loss: 0.0337
Training Epoch: 91 [14848/50048]	Loss: 0.0618
Training Epoch: 91 [14976/50048]	Loss: 0.0684
Training Epoch: 91 [15104/50048]	Loss: 0.0813
Training Epoch: 91 [15232/50048]	Loss: 0.1570
Training Epoch: 91 [15360/50048]	Loss: 0.0797
Training Epoch: 91 [15488/50048]	Loss: 0.0420
Training Epoch: 91 [15616/50048]	Loss: 0.0933
Training Epoch: 91 [15744/50048]	Loss: 0.0721
Training Epoch: 91 [15872/50048]	Loss: 0.0749
Training Epoch: 91 [16000/50048]	Loss: 0.0931
Training Epoch: 91 [16128/50048]	Loss: 0.0878
Training Epoch: 91 [16256/50048]	Loss: 0.0698
Training Epoch: 91 [16384/50048]	Loss: 0.1490
Training Epoch: 91 [16512/50048]	Loss: 0.0792
Training Epoch: 91 [16640/50048]	Loss: 0.0435
Training Epoch: 91 [16768/50048]	Loss: 0.0606
Training Epoch: 91 [16896/50048]	Loss: 0.0549
Training Epoch: 91 [17024/50048]	Loss: 0.1081
Training Epoch: 91 [17152/50048]	Loss: 0.1503
Training Epoch: 91 [17280/50048]	Loss: 0.1720
Training Epoch: 91 [17408/50048]	Loss: 0.1561
Training Epoch: 91 [17536/50048]	Loss: 0.0651
Training Epoch: 91 [17664/50048]	Loss: 0.0836
Training Epoch: 91 [17792/50048]	Loss: 0.0864
Training Epoch: 91 [17920/50048]	Loss: 0.1114
Training Epoch: 91 [18048/50048]	Loss: 0.0743
Training Epoch: 91 [18176/50048]	Loss: 0.1183
Training Epoch: 91 [18304/50048]	Loss: 0.0461
Training Epoch: 91 [18432/50048]	Loss: 0.1377
Training Epoch: 91 [18560/50048]	Loss: 0.0635
Training Epoch: 91 [18688/50048]	Loss: 0.0710
Training Epoch: 91 [18816/50048]	Loss: 0.0547
Training Epoch: 91 [18944/50048]	Loss: 0.1035
Training Epoch: 91 [19072/50048]	Loss: 0.1260
Training Epoch: 91 [19200/50048]	Loss: 0.0809
Training Epoch: 91 [19328/50048]	Loss: 0.0673
Training Epoch: 91 [19456/50048]	Loss: 0.1095
Training Epoch: 91 [19584/50048]	Loss: 0.0546
Training Epoch: 91 [19712/50048]	Loss: 0.1127
Training Epoch: 91 [19840/50048]	Loss: 0.0478
Training Epoch: 91 [19968/50048]	Loss: 0.1025
Training Epoch: 91 [20096/50048]	Loss: 0.0705
Training Epoch: 91 [20224/50048]	Loss: 0.0678
Training Epoch: 91 [20352/50048]	Loss: 0.0969
Training Epoch: 91 [20480/50048]	Loss: 0.0823
Training Epoch: 91 [20608/50048]	Loss: 0.0591
Training Epoch: 91 [20736/50048]	Loss: 0.0855
Training Epoch: 91 [20864/50048]	Loss: 0.1306
Training Epoch: 91 [20992/50048]	Loss: 0.0460
Training Epoch: 91 [21120/50048]	Loss: 0.1054
Training Epoch: 91 [21248/50048]	Loss: 0.0529
Training Epoch: 91 [21376/50048]	Loss: 0.0774
Training Epoch: 91 [21504/50048]	Loss: 0.0594
Training Epoch: 91 [21632/50048]	Loss: 0.1162
Training Epoch: 91 [21760/50048]	Loss: 0.1335
Training Epoch: 91 [21888/50048]	Loss: 0.0974
Training Epoch: 91 [22016/50048]	Loss: 0.1183
Training Epoch: 91 [22144/50048]	Loss: 0.1302
Training Epoch: 91 [22272/50048]	Loss: 0.0917
Training Epoch: 91 [22400/50048]	Loss: 0.0492
Training Epoch: 91 [22528/50048]	Loss: 0.0708
Training Epoch: 91 [22656/50048]	Loss: 0.0980
Training Epoch: 91 [22784/50048]	Loss: 0.0756
Training Epoch: 91 [22912/50048]	Loss: 0.0337
Training Epoch: 91 [23040/50048]	Loss: 0.0429
Training Epoch: 91 [23168/50048]	Loss: 0.1306
Training Epoch: 91 [23296/50048]	Loss: 0.1101
Training Epoch: 91 [23424/50048]	Loss: 0.0677
Training Epoch: 91 [23552/50048]	Loss: 0.0983
Training Epoch: 91 [23680/50048]	Loss: 0.0877
Training Epoch: 91 [23808/50048]	Loss: 0.0483
Training Epoch: 91 [23936/50048]	Loss: 0.0899
Training Epoch: 91 [24064/50048]	Loss: 0.0464
Training Epoch: 91 [24192/50048]	Loss: 0.1052
Training Epoch: 91 [24320/50048]	Loss: 0.1538
Training Epoch: 91 [24448/50048]	Loss: 0.0813
Training Epoch: 91 [24576/50048]	Loss: 0.0271
Training Epoch: 91 [24704/50048]	Loss: 0.0870
Training Epoch: 91 [24832/50048]	Loss: 0.0909
Training Epoch: 91 [24960/50048]	Loss: 0.0378
Training Epoch: 91 [25088/50048]	Loss: 0.1110
Training Epoch: 91 [25216/50048]	Loss: 0.0906
Training Epoch: 91 [25344/50048]	Loss: 0.1923
Training Epoch: 91 [25472/50048]	Loss: 0.0952
Training Epoch: 91 [25600/50048]	Loss: 0.0574
Training Epoch: 91 [25728/50048]	Loss: 0.0958
Training Epoch: 91 [25856/50048]	Loss: 0.0756
Training Epoch: 91 [25984/50048]	Loss: 0.0513
Training Epoch: 91 [26112/50048]	Loss: 0.0965
Training Epoch: 91 [26240/50048]	Loss: 0.1383
Training Epoch: 91 [26368/50048]	Loss: 0.0437
Training Epoch: 91 [26496/50048]	Loss: 0.0983
Training Epoch: 91 [26624/50048]	Loss: 0.1026
Training Epoch: 91 [26752/50048]	Loss: 0.0667
Training Epoch: 91 [26880/50048]	Loss: 0.0845
Training Epoch: 91 [27008/50048]	Loss: 0.0584
Training Epoch: 91 [27136/50048]	Loss: 0.0891
Training Epoch: 91 [27264/50048]	Loss: 0.0770
Training Epoch: 91 [27392/50048]	Loss: 0.1132
Training Epoch: 91 [27520/50048]	Loss: 0.1008
Training Epoch: 91 [27648/50048]	Loss: 0.1479
Training Epoch: 91 [27776/50048]	Loss: 0.0408
Training Epoch: 91 [27904/50048]	Loss: 0.0548
Training Epoch: 91 [28032/50048]	Loss: 0.0627
Training Epoch: 91 [28160/50048]	Loss: 0.0522
Training Epoch: 91 [28288/50048]	Loss: 0.0874
Training Epoch: 91 [28416/50048]	Loss: 0.1343
Training Epoch: 91 [28544/50048]	Loss: 0.0757
Training Epoch: 91 [28672/50048]	Loss: 0.0307
Training Epoch: 91 [28800/50048]	Loss: 0.1547
Training Epoch: 91 [28928/50048]	Loss: 0.1269
Training Epoch: 91 [29056/50048]	Loss: 0.0536
Training Epoch: 91 [29184/50048]	Loss: 0.1878
Training Epoch: 91 [29312/50048]	Loss: 0.0525
Training Epoch: 91 [29440/50048]	Loss: 0.1536
Training Epoch: 91 [29568/50048]	Loss: 0.0821
Training Epoch: 91 [29696/50048]	Loss: 0.1053
Training Epoch: 91 [29824/50048]	Loss: 0.1015
Training Epoch: 91 [29952/50048]	Loss: 0.0604
Training Epoch: 91 [30080/50048]	Loss: 0.1403
Training Epoch: 91 [30208/50048]	Loss: 0.0805
Training Epoch: 91 [30336/50048]	Loss: 0.1090
Training Epoch: 91 [30464/50048]	Loss: 0.0738
Training Epoch: 91 [30592/50048]	Loss: 0.0554
Training Epoch: 91 [30720/50048]	Loss: 0.1258
Training Epoch: 91 [30848/50048]	Loss: 0.0693
Training Epoch: 91 [30976/50048]	Loss: 0.0600
Training Epoch: 91 [31104/50048]	Loss: 0.1748
Training Epoch: 91 [31232/50048]	Loss: 0.0666
Training Epoch: 91 [31360/50048]	Loss: 0.0707
Training Epoch: 91 [31488/50048]	Loss: 0.1539
Training Epoch: 91 [31616/50048]	Loss: 0.0591
Training Epoch: 91 [31744/50048]	Loss: 0.0820
Training Epoch: 91 [31872/50048]	Loss: 0.1155
Training Epoch: 91 [32000/50048]	Loss: 0.1329
Training Epoch: 91 [32128/50048]	Loss: 0.0683
Training Epoch: 91 [32256/50048]	Loss: 0.0638
Training Epoch: 91 [32384/50048]	Loss: 0.0829
Training Epoch: 91 [32512/50048]	Loss: 0.0794
Training Epoch: 91 [32640/50048]	Loss: 0.0717
Training Epoch: 91 [32768/50048]	Loss: 0.0665
Training Epoch: 91 [32896/50048]	Loss: 0.0959
Training Epoch: 91 [33024/50048]	Loss: 0.1121
Training Epoch: 91 [33152/50048]	Loss: 0.1042
Training Epoch: 91 [33280/50048]	Loss: 0.1181
Training Epoch: 91 [33408/50048]	Loss: 0.1165
Training Epoch: 91 [33536/50048]	Loss: 0.2278
Training Epoch: 91 [33664/50048]	Loss: 0.0759
Training Epoch: 91 [33792/50048]	Loss: 0.0759
Training Epoch: 91 [33920/50048]	Loss: 0.1121
Training Epoch: 91 [34048/50048]	Loss: 0.1139
Training Epoch: 91 [34176/50048]	Loss: 0.0791
Training Epoch: 91 [34304/50048]	Loss: 0.0604
Training Epoch: 91 [34432/50048]	Loss: 0.0999
Training Epoch: 91 [34560/50048]	Loss: 0.1270
Training Epoch: 91 [34688/50048]	Loss: 0.0741
Training Epoch: 91 [34816/50048]	Loss: 0.0926
Training Epoch: 91 [34944/50048]	Loss: 0.0767
Training Epoch: 91 [35072/50048]	Loss: 0.1222
Training Epoch: 91 [35200/50048]	Loss: 0.1103
Training Epoch: 91 [35328/50048]	Loss: 0.0663
Training Epoch: 91 [35456/50048]	Loss: 0.0462
Training Epoch: 91 [35584/50048]	Loss: 0.1027
Training Epoch: 91 [35712/50048]	Loss: 0.0324
Training Epoch: 91 [35840/50048]	Loss: 0.0488
Training Epoch: 91 [35968/50048]	Loss: 0.1327
Training Epoch: 91 [36096/50048]	Loss: 0.0423
Training Epoch: 91 [36224/50048]	Loss: 0.0738
Training Epoch: 91 [36352/50048]	Loss: 0.1347
Training Epoch: 91 [36480/50048]	Loss: 0.1193
Training Epoch: 91 [36608/50048]	Loss: 0.1301
Training Epoch: 91 [36736/50048]	Loss: 0.0918
Training Epoch: 91 [36864/50048]	Loss: 0.0496
Training Epoch: 91 [36992/50048]	Loss: 0.1410
Training Epoch: 91 [37120/50048]	Loss: 0.1433
Training Epoch: 91 [37248/50048]	Loss: 0.0253
Training Epoch: 91 [37376/50048]	Loss: 0.1178
Training Epoch: 91 [37504/50048]	Loss: 0.0428
Training Epoch: 91 [37632/50048]	Loss: 0.0752
Training Epoch: 91 [37760/50048]	Loss: 0.0825
Training Epoch: 91 [37888/50048]	Loss: 0.0659
Training Epoch: 91 [38016/50048]	Loss: 0.0741
Training Epoch: 91 [38144/50048]	Loss: 0.0869
Training Epoch: 91 [38272/50048]	Loss: 0.1618
Training Epoch: 91 [38400/50048]	Loss: 0.1535
Training Epoch: 91 [38528/50048]	Loss: 0.1153
Training Epoch: 91 [38656/50048]	Loss: 0.0782
Training Epoch: 91 [38784/50048]	Loss: 0.0492
Training Epoch: 91 [38912/50048]	Loss: 0.2055
Training Epoch: 91 [39040/50048]	Loss: 0.1020
Training Epoch: 91 [39168/50048]	Loss: 0.0570
Training Epoch: 91 [39296/50048]	Loss: 0.0371
Training Epoch: 91 [39424/50048]	Loss: 0.0721
Training Epoch: 91 [39552/50048]	Loss: 0.1101
Training Epoch: 91 [39680/50048]	Loss: 0.0923
Training Epoch: 91 [39808/50048]	Loss: 0.0891
Training Epoch: 91 [39936/50048]	Loss: 0.0623
Training Epoch: 91 [40064/50048]	Loss: 0.1135
Training Epoch: 91 [40192/50048]	Loss: 0.0751
Training Epoch: 91 [40320/50048]	Loss: 0.0845
Training Epoch: 91 [40448/50048]	Loss: 0.0691
Training Epoch: 91 [40576/50048]	Loss: 0.0550
Training Epoch: 91 [40704/50048]	Loss: 0.0812
Training Epoch: 91 [40832/50048]	Loss: 0.0599
Training Epoch: 91 [40960/50048]	Loss: 0.0952
Training Epoch: 91 [41088/50048]	Loss: 0.0871
Training Epoch: 91 [41216/50048]	Loss: 0.0991
Training Epoch: 91 [41344/50048]	Loss: 0.1161
Training Epoch: 91 [41472/50048]	Loss: 0.0667
Training Epoch: 91 [41600/50048]	Loss: 0.0482
Training Epoch: 91 [41728/50048]	Loss: 0.1697
Training Epoch: 91 [41856/50048]	Loss: 0.0983
Training Epoch: 91 [41984/50048]	Loss: 0.1599
Training Epoch: 91 [42112/50048]	Loss: 0.1848
Training Epoch: 91 [42240/50048]	Loss: 0.0824
Training Epoch: 91 [42368/50048]	Loss: 0.1150
Training Epoch: 91 [42496/50048]	Loss: 0.0842
Training Epoch: 91 [42624/50048]	Loss: 0.0890
Training Epoch: 91 [42752/50048]	Loss: 0.0964
Training Epoch: 91 [42880/50048]	Loss: 0.0837
Training Epoch: 91 [43008/50048]	Loss: 0.0716
Training Epoch: 91 [43136/50048]	Loss: 0.0360
Training Epoch: 91 [43264/50048]	Loss: 0.0710
Training Epoch: 91 [43392/50048]	Loss: 0.0712
Training Epoch: 91 [43520/50048]	Loss: 0.0708
Training Epoch: 91 [43648/50048]	Loss: 0.0461
Training Epoch: 91 [43776/50048]	Loss: 0.1030
Training Epoch: 91 [43904/50048]	Loss: 0.1258
Training Epoch: 91 [44032/50048]	Loss: 0.0902
Training Epoch: 91 [44160/50048]	Loss: 0.0893
Training Epoch: 91 [44288/50048]	Loss: 0.0975
Training Epoch: 91 [44416/50048]	Loss: 0.0870
Training Epoch: 91 [44544/50048]	Loss: 0.0548
Training Epoch: 91 [44672/50048]	Loss: 0.1027
Training Epoch: 91 [44800/50048]	Loss: 0.1004
Training Epoch: 91 [44928/50048]	Loss: 0.0958
Training Epoch: 91 [45056/50048]	Loss: 0.1583
Training Epoch: 91 [45184/50048]	Loss: 0.0607
Training Epoch: 91 [45312/50048]	Loss: 0.0935
Training Epoch: 91 [45440/50048]	Loss: 0.1258
Training Epoch: 91 [45568/50048]	Loss: 0.0910
Training Epoch: 91 [45696/50048]	Loss: 0.0836
2022-12-06 08:26:10,755 [ZeusDataLoader(train)] train epoch 92 done: time=86.53 energy=10498.48
2022-12-06 08:26:10,756 [ZeusDataLoader(eval)] Epoch 92 begin.
Training Epoch: 91 [45824/50048]	Loss: 0.0890
Training Epoch: 91 [45952/50048]	Loss: 0.0467
Training Epoch: 91 [46080/50048]	Loss: 0.0813
Training Epoch: 91 [46208/50048]	Loss: 0.1055
Training Epoch: 91 [46336/50048]	Loss: 0.0993
Training Epoch: 91 [46464/50048]	Loss: 0.0560
Training Epoch: 91 [46592/50048]	Loss: 0.1164
Training Epoch: 91 [46720/50048]	Loss: 0.0995
Training Epoch: 91 [46848/50048]	Loss: 0.0767
Training Epoch: 91 [46976/50048]	Loss: 0.0862
Training Epoch: 91 [47104/50048]	Loss: 0.0634
Training Epoch: 91 [47232/50048]	Loss: 0.0818
Training Epoch: 91 [47360/50048]	Loss: 0.0711
Training Epoch: 91 [47488/50048]	Loss: 0.0674
Training Epoch: 91 [47616/50048]	Loss: 0.0767
Training Epoch: 91 [47744/50048]	Loss: 0.1717
Training Epoch: 91 [47872/50048]	Loss: 0.1085
Training Epoch: 91 [48000/50048]	Loss: 0.0422
Training Epoch: 91 [48128/50048]	Loss: 0.1142
Training Epoch: 91 [48256/50048]	Loss: 0.0897
Training Epoch: 91 [48384/50048]	Loss: 0.0307
Training Epoch: 91 [48512/50048]	Loss: 0.0849
Training Epoch: 91 [48640/50048]	Loss: 0.0856
Training Epoch: 91 [48768/50048]	Loss: 0.1114
Training Epoch: 91 [48896/50048]	Loss: 0.1122
Training Epoch: 91 [49024/50048]	Loss: 0.0859
Training Epoch: 91 [49152/50048]	Loss: 0.1127
Training Epoch: 91 [49280/50048]	Loss: 0.0821
Training Epoch: 91 [49408/50048]	Loss: 0.0612
Training Epoch: 91 [49536/50048]	Loss: 0.1205
Training Epoch: 91 [49664/50048]	Loss: 0.0604
Training Epoch: 91 [49792/50048]	Loss: 0.0406
Training Epoch: 91 [49920/50048]	Loss: 0.1776
Training Epoch: 91 [50048/50048]	Loss: 0.1298
2022-12-06 13:26:14.487 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 08:26:14,516 [ZeusDataLoader(eval)] eval epoch 92 done: time=3.75 energy=452.86
2022-12-06 08:26:14,516 [ZeusDataLoader(train)] Up to epoch 92: time=8298.83, energy=1007351.78, cost=1229823.43
2022-12-06 08:26:14,516 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 08:26:14,516 [ZeusDataLoader(train)] Expected next epoch: time=8388.63, energy=1018149.79, cost=1243079.81
2022-12-06 08:26:14,517 [ZeusDataLoader(train)] Epoch 93 begin.
Validation Epoch: 91, Average loss: 0.0186, Accuracy: 0.6414
2022-12-06 08:26:14,705 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:26:14,706 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:26:14.708 [ZeusMonitor] Monitor started.
2022-12-06 13:26:14.708 [ZeusMonitor] Running indefinitely. 2022-12-06 13:26:14.708 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:26:14.708 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e93+gpu0.power.log
Training Epoch: 92 [128/50048]	Loss: 0.0775
Training Epoch: 92 [256/50048]	Loss: 0.1323
Training Epoch: 92 [384/50048]	Loss: 0.0892
Training Epoch: 92 [512/50048]	Loss: 0.0363
Training Epoch: 92 [640/50048]	Loss: 0.0466
Training Epoch: 92 [768/50048]	Loss: 0.0569
Training Epoch: 92 [896/50048]	Loss: 0.0798
Training Epoch: 92 [1024/50048]	Loss: 0.0938
Training Epoch: 92 [1152/50048]	Loss: 0.0760
Training Epoch: 92 [1280/50048]	Loss: 0.0620
Training Epoch: 92 [1408/50048]	Loss: 0.0783
Training Epoch: 92 [1536/50048]	Loss: 0.0990
Training Epoch: 92 [1664/50048]	Loss: 0.0813
Training Epoch: 92 [1792/50048]	Loss: 0.0561
Training Epoch: 92 [1920/50048]	Loss: 0.0470
Training Epoch: 92 [2048/50048]	Loss: 0.0753
Training Epoch: 92 [2176/50048]	Loss: 0.1051
Training Epoch: 92 [2304/50048]	Loss: 0.0546
Training Epoch: 92 [2432/50048]	Loss: 0.0727
Training Epoch: 92 [2560/50048]	Loss: 0.0573
Training Epoch: 92 [2688/50048]	Loss: 0.0533
Training Epoch: 92 [2816/50048]	Loss: 0.1126
Training Epoch: 92 [2944/50048]	Loss: 0.0211
Training Epoch: 92 [3072/50048]	Loss: 0.1286
Training Epoch: 92 [3200/50048]	Loss: 0.0450
Training Epoch: 92 [3328/50048]	Loss: 0.0256
Training Epoch: 92 [3456/50048]	Loss: 0.0530
Training Epoch: 92 [3584/50048]	Loss: 0.0957
Training Epoch: 92 [3712/50048]	Loss: 0.1057
Training Epoch: 92 [3840/50048]	Loss: 0.1141
Training Epoch: 92 [3968/50048]	Loss: 0.0858
Training Epoch: 92 [4096/50048]	Loss: 0.1283
Training Epoch: 92 [4224/50048]	Loss: 0.0405
Training Epoch: 92 [4352/50048]	Loss: 0.0700
Training Epoch: 92 [4480/50048]	Loss: 0.1058
Training Epoch: 92 [4608/50048]	Loss: 0.1513
Training Epoch: 92 [4736/50048]	Loss: 0.1684
Training Epoch: 92 [4864/50048]	Loss: 0.0516
Training Epoch: 92 [4992/50048]	Loss: 0.0958
Training Epoch: 92 [5120/50048]	Loss: 0.0372
Training Epoch: 92 [5248/50048]	Loss: 0.0867
Training Epoch: 92 [5376/50048]	Loss: 0.1023
Training Epoch: 92 [5504/50048]	Loss: 0.1549
Training Epoch: 92 [5632/50048]	Loss: 0.0351
Training Epoch: 92 [5760/50048]	Loss: 0.0429
Training Epoch: 92 [5888/50048]	Loss: 0.0516
Training Epoch: 92 [6016/50048]	Loss: 0.0704
Training Epoch: 92 [6144/50048]	Loss: 0.0423
Training Epoch: 92 [6272/50048]	Loss: 0.0717
Training Epoch: 92 [6400/50048]	Loss: 0.0443
Training Epoch: 92 [6528/50048]	Loss: 0.0241
Training Epoch: 92 [6656/50048]	Loss: 0.0621
Training Epoch: 92 [6784/50048]	Loss: 0.0381
Training Epoch: 92 [6912/50048]	Loss: 0.0682
Training Epoch: 92 [7040/50048]	Loss: 0.0799
Training Epoch: 92 [7168/50048]	Loss: 0.0770
Training Epoch: 92 [7296/50048]	Loss: 0.1770
Training Epoch: 92 [7424/50048]	Loss: 0.3131
Training Epoch: 92 [7552/50048]	Loss: 0.0822
Training Epoch: 92 [7680/50048]	Loss: 0.0906
Training Epoch: 92 [7808/50048]	Loss: 0.0939
Training Epoch: 92 [7936/50048]	Loss: 0.0832
Training Epoch: 92 [8064/50048]	Loss: 0.0771
Training Epoch: 92 [8192/50048]	Loss: 0.0725
Training Epoch: 92 [8320/50048]	Loss: 0.0734
Training Epoch: 92 [8448/50048]	Loss: 0.0707
Training Epoch: 92 [8576/50048]	Loss: 0.0962
Training Epoch: 92 [8704/50048]	Loss: 0.1977
Training Epoch: 92 [8832/50048]	Loss: 0.1588
Training Epoch: 92 [8960/50048]	Loss: 0.0580
Training Epoch: 92 [9088/50048]	Loss: 0.0509
Training Epoch: 92 [9216/50048]	Loss: 0.0457
Training Epoch: 92 [9344/50048]	Loss: 0.0538
Training Epoch: 92 [9472/50048]	Loss: 0.0988
Training Epoch: 92 [9600/50048]	Loss: 0.0508
Training Epoch: 92 [9728/50048]	Loss: 0.0993
Training Epoch: 92 [9856/50048]	Loss: 0.0771
Training Epoch: 92 [9984/50048]	Loss: 0.0290
Training Epoch: 92 [10112/50048]	Loss: 0.0595
Training Epoch: 92 [10240/50048]	Loss: 0.0795
Training Epoch: 92 [10368/50048]	Loss: 0.1109
Training Epoch: 92 [10496/50048]	Loss: 0.0828
Training Epoch: 92 [10624/50048]	Loss: 0.0809
Training Epoch: 92 [10752/50048]	Loss: 0.0607
Training Epoch: 92 [10880/50048]	Loss: 0.0593
Training Epoch: 92 [11008/50048]	Loss: 0.0833
Training Epoch: 92 [11136/50048]	Loss: 0.0973
Training Epoch: 92 [11264/50048]	Loss: 0.0912
Training Epoch: 92 [11392/50048]	Loss: 0.0921
Training Epoch: 92 [11520/50048]	Loss: 0.1197
Training Epoch: 92 [11648/50048]	Loss: 0.1195
Training Epoch: 92 [11776/50048]	Loss: 0.0738
Training Epoch: 92 [11904/50048]	Loss: 0.0810
Training Epoch: 92 [12032/50048]	Loss: 0.0904
Training Epoch: 92 [12160/50048]	Loss: 0.0797
Training Epoch: 92 [12288/50048]	Loss: 0.0721
Training Epoch: 92 [12416/50048]	Loss: 0.0721
Training Epoch: 92 [12544/50048]	Loss: 0.0695
Training Epoch: 92 [12672/50048]	Loss: 0.0493
Training Epoch: 92 [12800/50048]	Loss: 0.0476
Training Epoch: 92 [12928/50048]	Loss: 0.0711
Training Epoch: 92 [13056/50048]	Loss: 0.0708
Training Epoch: 92 [13184/50048]	Loss: 0.0791
Training Epoch: 92 [13312/50048]	Loss: 0.1248
Training Epoch: 92 [13440/50048]	Loss: 0.0487
Training Epoch: 92 [13568/50048]	Loss: 0.0441
Training Epoch: 92 [13696/50048]	Loss: 0.0934
Training Epoch: 92 [13824/50048]	Loss: 0.0553
Training Epoch: 92 [13952/50048]	Loss: 0.0683
Training Epoch: 92 [14080/50048]	Loss: 0.0969
Training Epoch: 92 [14208/50048]	Loss: 0.0546
Training Epoch: 92 [14336/50048]	Loss: 0.0496
Training Epoch: 92 [14464/50048]	Loss: 0.0828
Training Epoch: 92 [14592/50048]	Loss: 0.1264
Training Epoch: 92 [14720/50048]	Loss: 0.0937
Training Epoch: 92 [14848/50048]	Loss: 0.0976
Training Epoch: 92 [14976/50048]	Loss: 0.0784
Training Epoch: 92 [15104/50048]	Loss: 0.0854
Training Epoch: 92 [15232/50048]	Loss: 0.1235
Training Epoch: 92 [15360/50048]	Loss: 0.0268
Training Epoch: 92 [15488/50048]	Loss: 0.0477
Training Epoch: 92 [15616/50048]	Loss: 0.0751
Training Epoch: 92 [15744/50048]	Loss: 0.1215
Training Epoch: 92 [15872/50048]	Loss: 0.0830
Training Epoch: 92 [16000/50048]	Loss: 0.2299
Training Epoch: 92 [16128/50048]	Loss: 0.1333
Training Epoch: 92 [16256/50048]	Loss: 0.0478
Training Epoch: 92 [16384/50048]	Loss: 0.0548
Training Epoch: 92 [16512/50048]	Loss: 0.0975
Training Epoch: 92 [16640/50048]	Loss: 0.0342
Training Epoch: 92 [16768/50048]	Loss: 0.0273
Training Epoch: 92 [16896/50048]	Loss: 0.0357
Training Epoch: 92 [17024/50048]	Loss: 0.1037
Training Epoch: 92 [17152/50048]	Loss: 0.0508
Training Epoch: 92 [17280/50048]	Loss: 0.0563
Training Epoch: 92 [17408/50048]	Loss: 0.1269
Training Epoch: 92 [17536/50048]	Loss: 0.0642
Training Epoch: 92 [17664/50048]	Loss: 0.0566
Training Epoch: 92 [17792/50048]	Loss: 0.0966
Training Epoch: 92 [17920/50048]	Loss: 0.1639
Training Epoch: 92 [18048/50048]	Loss: 0.0549
Training Epoch: 92 [18176/50048]	Loss: 0.1031
Training Epoch: 92 [18304/50048]	Loss: 0.0949
Training Epoch: 92 [18432/50048]	Loss: 0.0698
Training Epoch: 92 [18560/50048]	Loss: 0.0727
Training Epoch: 92 [18688/50048]	Loss: 0.0759
Training Epoch: 92 [18816/50048]	Loss: 0.1158
Training Epoch: 92 [18944/50048]	Loss: 0.1204
Training Epoch: 92 [19072/50048]	Loss: 0.0945
Training Epoch: 92 [19200/50048]	Loss: 0.0491
Training Epoch: 92 [19328/50048]	Loss: 0.0694
Training Epoch: 92 [19456/50048]	Loss: 0.0989
Training Epoch: 92 [19584/50048]	Loss: 0.0631
Training Epoch: 92 [19712/50048]	Loss: 0.0514
Training Epoch: 92 [19840/50048]	Loss: 0.0831
Training Epoch: 92 [19968/50048]	Loss: 0.0470
Training Epoch: 92 [20096/50048]	Loss: 0.1425
Training Epoch: 92 [20224/50048]	Loss: 0.0662
Training Epoch: 92 [20352/50048]	Loss: 0.0667
Training Epoch: 92 [20480/50048]	Loss: 0.1122
Training Epoch: 92 [20608/50048]	Loss: 0.0618
Training Epoch: 92 [20736/50048]	Loss: 0.0639
Training Epoch: 92 [20864/50048]	Loss: 0.0633
Training Epoch: 92 [20992/50048]	Loss: 0.1799
Training Epoch: 92 [21120/50048]	Loss: 0.0893
Training Epoch: 92 [21248/50048]	Loss: 0.0307
Training Epoch: 92 [21376/50048]	Loss: 0.0525
Training Epoch: 92 [21504/50048]	Loss: 0.1134
Training Epoch: 92 [21632/50048]	Loss: 0.0746
Training Epoch: 92 [21760/50048]	Loss: 0.0960
Training Epoch: 92 [21888/50048]	Loss: 0.0929
Training Epoch: 92 [22016/50048]	Loss: 0.1076
Training Epoch: 92 [22144/50048]	Loss: 0.0842
Training Epoch: 92 [22272/50048]	Loss: 0.1670
Training Epoch: 92 [22400/50048]	Loss: 0.0707
Training Epoch: 92 [22528/50048]	Loss: 0.1341
Training Epoch: 92 [22656/50048]	Loss: 0.0699
Training Epoch: 92 [22784/50048]	Loss: 0.1453
Training Epoch: 92 [22912/50048]	Loss: 0.0303
Training Epoch: 92 [23040/50048]	Loss: 0.0653
Training Epoch: 92 [23168/50048]	Loss: 0.0605
Training Epoch: 92 [23296/50048]	Loss: 0.0639
Training Epoch: 92 [23424/50048]	Loss: 0.0342
Training Epoch: 92 [23552/50048]	Loss: 0.1026
Training Epoch: 92 [23680/50048]	Loss: 0.1187
Training Epoch: 92 [23808/50048]	Loss: 0.1073
Training Epoch: 92 [23936/50048]	Loss: 0.1062
Training Epoch: 92 [24064/50048]	Loss: 0.0748
Training Epoch: 92 [24192/50048]	Loss: 0.0950
Training Epoch: 92 [24320/50048]	Loss: 0.0778
Training Epoch: 92 [24448/50048]	Loss: 0.0381
Training Epoch: 92 [24576/50048]	Loss: 0.0900
Training Epoch: 92 [24704/50048]	Loss: 0.1015
Training Epoch: 92 [24832/50048]	Loss: 0.0752
Training Epoch: 92 [24960/50048]	Loss: 0.0904
Training Epoch: 92 [25088/50048]	Loss: 0.1224
Training Epoch: 92 [25216/50048]	Loss: 0.0618
Training Epoch: 92 [25344/50048]	Loss: 0.1005
Training Epoch: 92 [25472/50048]	Loss: 0.0720
Training Epoch: 92 [25600/50048]	Loss: 0.1181
Training Epoch: 92 [25728/50048]	Loss: 0.1480
Training Epoch: 92 [25856/50048]	Loss: 0.1162
Training Epoch: 92 [25984/50048]	Loss: 0.1401
Training Epoch: 92 [26112/50048]	Loss: 0.0580
Training Epoch: 92 [26240/50048]	Loss: 0.0366
Training Epoch: 92 [26368/50048]	Loss: 0.0865
Training Epoch: 92 [26496/50048]	Loss: 0.0689
Training Epoch: 92 [26624/50048]	Loss: 0.0620
Training Epoch: 92 [26752/50048]	Loss: 0.0832
Training Epoch: 92 [26880/50048]	Loss: 0.0937
Training Epoch: 92 [27008/50048]	Loss: 0.0901
Training Epoch: 92 [27136/50048]	Loss: 0.0977
Training Epoch: 92 [27264/50048]	Loss: 0.1121
Training Epoch: 92 [27392/50048]	Loss: 0.0475
Training Epoch: 92 [27520/50048]	Loss: 0.0658
Training Epoch: 92 [27648/50048]	Loss: 0.1494
Training Epoch: 92 [27776/50048]	Loss: 0.0715
Training Epoch: 92 [27904/50048]	Loss: 0.0710
Training Epoch: 92 [28032/50048]	Loss: 0.0700
Training Epoch: 92 [28160/50048]	Loss: 0.1427
Training Epoch: 92 [28288/50048]	Loss: 0.0541
Training Epoch: 92 [28416/50048]	Loss: 0.0943
Training Epoch: 92 [28544/50048]	Loss: 0.0554
Training Epoch: 92 [28672/50048]	Loss: 0.1313
Training Epoch: 92 [28800/50048]	Loss: 0.0833
Training Epoch: 92 [28928/50048]	Loss: 0.0915
Training Epoch: 92 [29056/50048]	Loss: 0.1185
Training Epoch: 92 [29184/50048]	Loss: 0.0552
Training Epoch: 92 [29312/50048]	Loss: 0.0697
Training Epoch: 92 [29440/50048]	Loss: 0.0689
Training Epoch: 92 [29568/50048]	Loss: 0.1435
Training Epoch: 92 [29696/50048]	Loss: 0.1308
Training Epoch: 92 [29824/50048]	Loss: 0.1105
Training Epoch: 92 [29952/50048]	Loss: 0.0942
Training Epoch: 92 [30080/50048]	Loss: 0.1113
Training Epoch: 92 [30208/50048]	Loss: 0.1324
Training Epoch: 92 [30336/50048]	Loss: 0.0632
Training Epoch: 92 [30464/50048]	Loss: 0.0737
Training Epoch: 92 [30592/50048]	Loss: 0.0697
Training Epoch: 92 [30720/50048]	Loss: 0.0859
Training Epoch: 92 [30848/50048]	Loss: 0.0546
Training Epoch: 92 [30976/50048]	Loss: 0.1128
Training Epoch: 92 [31104/50048]	Loss: 0.0930
Training Epoch: 92 [31232/50048]	Loss: 0.0947
Training Epoch: 92 [31360/50048]	Loss: 0.0648
Training Epoch: 92 [31488/50048]	Loss: 0.1299
Training Epoch: 92 [31616/50048]	Loss: 0.0583
Training Epoch: 92 [31744/50048]	Loss: 0.1498
Training Epoch: 92 [31872/50048]	Loss: 0.0830
Training Epoch: 92 [32000/50048]	Loss: 0.1249
Training Epoch: 92 [32128/50048]	Loss: 0.0426
Training Epoch: 92 [32256/50048]	Loss: 0.0894
Training Epoch: 92 [32384/50048]	Loss: 0.0724
Training Epoch: 92 [32512/50048]	Loss: 0.0867
Training Epoch: 92 [32640/50048]	Loss: 0.1155
Training Epoch: 92 [32768/50048]	Loss: 0.2808
Training Epoch: 92 [32896/50048]	Loss: 0.0609
Training Epoch: 92 [33024/50048]	Loss: 0.0956
Training Epoch: 92 [33152/50048]	Loss: 0.0589
Training Epoch: 92 [33280/50048]	Loss: 0.1514
Training Epoch: 92 [33408/50048]	Loss: 0.0695
Training Epoch: 92 [33536/50048]	Loss: 0.0768
Training Epoch: 92 [33664/50048]	Loss: 0.1768
Training Epoch: 92 [33792/50048]	Loss: 0.0847
Training Epoch: 92 [33920/50048]	Loss: 0.0739
Training Epoch: 92 [34048/50048]	Loss: 0.0333
Training Epoch: 92 [34176/50048]	Loss: 0.0574
Training Epoch: 92 [34304/50048]	Loss: 0.1475
Training Epoch: 92 [34432/50048]	Loss: 0.1048
Training Epoch: 92 [34560/50048]	Loss: 0.1062
Training Epoch: 92 [34688/50048]	Loss: 0.1672
Training Epoch: 92 [34816/50048]	Loss: 0.0650
Training Epoch: 92 [34944/50048]	Loss: 0.0433
Training Epoch: 92 [35072/50048]	Loss: 0.0533
Training Epoch: 92 [35200/50048]	Loss: 0.0279
Training Epoch: 92 [35328/50048]	Loss: 0.1194
Training Epoch: 92 [35456/50048]	Loss: 0.0657
Training Epoch: 92 [35584/50048]	Loss: 0.0434
Training Epoch: 92 [35712/50048]	Loss: 0.0644
Training Epoch: 92 [35840/50048]	Loss: 0.1254
Training Epoch: 92 [35968/50048]	Loss: 0.0326
Training Epoch: 92 [36096/50048]	Loss: 0.1501
Training Epoch: 92 [36224/50048]	Loss: 0.1154
Training Epoch: 92 [36352/50048]	Loss: 0.0802
Training Epoch: 92 [36480/50048]	Loss: 0.0972
Training Epoch: 92 [36608/50048]	Loss: 0.0775
Training Epoch: 92 [36736/50048]	Loss: 0.0973
Training Epoch: 92 [36864/50048]	Loss: 0.0880
Training Epoch: 92 [36992/50048]	Loss: 0.0860
Training Epoch: 92 [37120/50048]	Loss: 0.1681
Training Epoch: 92 [37248/50048]	Loss: 0.0716
Training Epoch: 92 [37376/50048]	Loss: 0.0746
Training Epoch: 92 [37504/50048]	Loss: 0.0732
Training Epoch: 92 [37632/50048]	Loss: 0.0646
Training Epoch: 92 [37760/50048]	Loss: 0.0696
Training Epoch: 92 [37888/50048]	Loss: 0.0287
Training Epoch: 92 [38016/50048]	Loss: 0.0482
Training Epoch: 92 [38144/50048]	Loss: 0.1775
Training Epoch: 92 [38272/50048]	Loss: 0.1506
Training Epoch: 92 [38400/50048]	Loss: 0.0907
Training Epoch: 92 [38528/50048]	Loss: 0.1222
Training Epoch: 92 [38656/50048]	Loss: 0.0904
Training Epoch: 92 [38784/50048]	Loss: 0.1515
Training Epoch: 92 [38912/50048]	Loss: 0.0777
Training Epoch: 92 [39040/50048]	Loss: 0.0671
Training Epoch: 92 [39168/50048]	Loss: 0.0549
Training Epoch: 92 [39296/50048]	Loss: 0.0860
Training Epoch: 92 [39424/50048]	Loss: 0.0230
Training Epoch: 92 [39552/50048]	Loss: 0.0980
Training Epoch: 92 [39680/50048]	Loss: 0.1078
Training Epoch: 92 [39808/50048]	Loss: 0.0872
Training Epoch: 92 [39936/50048]	Loss: 0.1604
Training Epoch: 92 [40064/50048]	Loss: 0.0876
Training Epoch: 92 [40192/50048]	Loss: 0.1413
Training Epoch: 92 [40320/50048]	Loss: 0.0378
Training Epoch: 92 [40448/50048]	Loss: 0.1336
Training Epoch: 92 [40576/50048]	Loss: 0.1797
Training Epoch: 92 [40704/50048]	Loss: 0.1099
Training Epoch: 92 [40832/50048]	Loss: 0.1647
Training Epoch: 92 [40960/50048]	Loss: 0.0516
Training Epoch: 92 [41088/50048]	Loss: 0.1092
Training Epoch: 92 [41216/50048]	Loss: 0.1580
Training Epoch: 92 [41344/50048]	Loss: 0.1006
Training Epoch: 92 [41472/50048]	Loss: 0.1052
Training Epoch: 92 [41600/50048]	Loss: 0.1119
Training Epoch: 92 [41728/50048]	Loss: 0.2884
Training Epoch: 92 [41856/50048]	Loss: 0.0731
Training Epoch: 92 [41984/50048]	Loss: 0.1312
Training Epoch: 92 [42112/50048]	Loss: 0.0701
Training Epoch: 92 [42240/50048]	Loss: 0.0312
Training Epoch: 92 [42368/50048]	Loss: 0.1823
Training Epoch: 92 [42496/50048]	Loss: 0.0575
Training Epoch: 92 [42624/50048]	Loss: 0.1167
Training Epoch: 92 [42752/50048]	Loss: 0.1190
Training Epoch: 92 [42880/50048]	Loss: 0.0454
Training Epoch: 92 [43008/50048]	Loss: 0.1000
Training Epoch: 92 [43136/50048]	Loss: 0.0844
Training Epoch: 92 [43264/50048]	Loss: 0.1056
Training Epoch: 92 [43392/50048]	Loss: 0.0942
Training Epoch: 92 [43520/50048]	Loss: 0.1341
Training Epoch: 92 [43648/50048]	Loss: 0.0679
Training Epoch: 92 [43776/50048]	Loss: 0.1234
Training Epoch: 92 [43904/50048]	Loss: 0.0761
Training Epoch: 92 [44032/50048]	Loss: 0.0561
Training Epoch: 92 [44160/50048]	Loss: 0.0951
Training Epoch: 92 [44288/50048]	Loss: 0.0713
Training Epoch: 92 [44416/50048]	Loss: 0.0789
Training Epoch: 92 [44544/50048]	Loss: 0.0836
Training Epoch: 92 [44672/50048]	Loss: 0.0710
Training Epoch: 92 [44800/50048]	Loss: 0.0645
Training Epoch: 92 [44928/50048]	Loss: 0.1056
Training Epoch: 92 [45056/50048]	Loss: 0.0922
Training Epoch: 92 [45184/50048]	Loss: 0.0781
Training Epoch: 92 [45312/50048]	Loss: 0.1153
Training Epoch: 92 [45440/50048]	Loss: 0.1417
Training Epoch: 92 [45568/50048]	Loss: 0.0556
Training Epoch: 92 [45696/50048]	Loss: 0.1239
2022-12-06 08:27:41,005 [ZeusDataLoader(train)] train epoch 93 done: time=86.48 energy=10490.16
2022-12-06 08:27:41,006 [ZeusDataLoader(eval)] Epoch 93 begin.
Training Epoch: 92 [45824/50048]	Loss: 0.0708
Training Epoch: 92 [45952/50048]	Loss: 0.0764
Training Epoch: 92 [46080/50048]	Loss: 0.1075
Training Epoch: 92 [46208/50048]	Loss: 0.1229
Training Epoch: 92 [46336/50048]	Loss: 0.0800
Training Epoch: 92 [46464/50048]	Loss: 0.0708
Training Epoch: 92 [46592/50048]	Loss: 0.0478
Training Epoch: 92 [46720/50048]	Loss: 0.0820
Training Epoch: 92 [46848/50048]	Loss: 0.1289
Training Epoch: 92 [46976/50048]	Loss: 0.1160
Training Epoch: 92 [47104/50048]	Loss: 0.1163
Training Epoch: 92 [47232/50048]	Loss: 0.0793
Training Epoch: 92 [47360/50048]	Loss: 0.1120
Training Epoch: 92 [47488/50048]	Loss: 0.1137
Training Epoch: 92 [47616/50048]	Loss: 0.0858
Training Epoch: 92 [47744/50048]	Loss: 0.0756
Training Epoch: 92 [47872/50048]	Loss: 0.1063
Training Epoch: 92 [48000/50048]	Loss: 0.0617
Training Epoch: 92 [48128/50048]	Loss: 0.1064
Training Epoch: 92 [48256/50048]	Loss: 0.1286
Training Epoch: 92 [48384/50048]	Loss: 0.0913
Training Epoch: 92 [48512/50048]	Loss: 0.1058
Training Epoch: 92 [48640/50048]	Loss: 0.0850
Training Epoch: 92 [48768/50048]	Loss: 0.0453
Training Epoch: 92 [48896/50048]	Loss: 0.0717
Training Epoch: 92 [49024/50048]	Loss: 0.0737
Training Epoch: 92 [49152/50048]	Loss: 0.0587
Training Epoch: 92 [49280/50048]	Loss: 0.0728
Training Epoch: 92 [49408/50048]	Loss: 0.0882
Training Epoch: 92 [49536/50048]	Loss: 0.0485
Training Epoch: 92 [49664/50048]	Loss: 0.0714
Training Epoch: 92 [49792/50048]	Loss: 0.1036
Training Epoch: 92 [49920/50048]	Loss: 0.1024
Training Epoch: 92 [50048/50048]	Loss: 0.0452
2022-12-06 13:27:44.696 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 08:27:44,728 [ZeusDataLoader(eval)] eval epoch 93 done: time=3.71 energy=450.23
2022-12-06 08:27:44,728 [ZeusDataLoader(train)] Up to epoch 93: time=8389.02, energy=1018292.16, cost=1243185.27
2022-12-06 08:27:44,728 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 08:27:44,728 [ZeusDataLoader(train)] Expected next epoch: time=8478.82, energy=1029090.18, cost=1256441.66
2022-12-06 08:27:44,729 [ZeusDataLoader(train)] Epoch 94 begin.
Validation Epoch: 92, Average loss: 0.0181, Accuracy: 0.6464
2022-12-06 08:27:44,919 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:27:44,920 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:27:44.922 [ZeusMonitor] Monitor started.
2022-12-06 13:27:44.922 [ZeusMonitor] Running indefinitely. 2022-12-06 13:27:44.922 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:27:44.922 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e94+gpu0.power.log
Training Epoch: 93 [128/50048]	Loss: 0.0821
Training Epoch: 93 [256/50048]	Loss: 0.0556
Training Epoch: 93 [384/50048]	Loss: 0.0212
Training Epoch: 93 [512/50048]	Loss: 0.1389
Training Epoch: 93 [640/50048]	Loss: 0.0793
Training Epoch: 93 [768/50048]	Loss: 0.1015
Training Epoch: 93 [896/50048]	Loss: 0.0386
Training Epoch: 93 [1024/50048]	Loss: 0.0747
Training Epoch: 93 [1152/50048]	Loss: 0.0793
Training Epoch: 93 [1280/50048]	Loss: 0.0562
Training Epoch: 93 [1408/50048]	Loss: 0.0905
Training Epoch: 93 [1536/50048]	Loss: 0.0593
Training Epoch: 93 [1664/50048]	Loss: 0.0976
Training Epoch: 93 [1792/50048]	Loss: 0.0921
Training Epoch: 93 [1920/50048]	Loss: 0.1657
Training Epoch: 93 [2048/50048]	Loss: 0.0403
Training Epoch: 93 [2176/50048]	Loss: 0.0890
Training Epoch: 93 [2304/50048]	Loss: 0.0681
Training Epoch: 93 [2432/50048]	Loss: 0.0862
Training Epoch: 93 [2560/50048]	Loss: 0.0789
Training Epoch: 93 [2688/50048]	Loss: 0.0525
Training Epoch: 93 [2816/50048]	Loss: 0.0909
Training Epoch: 93 [2944/50048]	Loss: 0.0439
Training Epoch: 93 [3072/50048]	Loss: 0.0925
Training Epoch: 93 [3200/50048]	Loss: 0.0777
Training Epoch: 93 [3328/50048]	Loss: 0.0307
Training Epoch: 93 [3456/50048]	Loss: 0.0529
Training Epoch: 93 [3584/50048]	Loss: 0.0974
Training Epoch: 93 [3712/50048]	Loss: 0.0257
Training Epoch: 93 [3840/50048]	Loss: 0.1133
Training Epoch: 93 [3968/50048]	Loss: 0.0361
Training Epoch: 93 [4096/50048]	Loss: 0.0240
Training Epoch: 93 [4224/50048]	Loss: 0.0698
Training Epoch: 93 [4352/50048]	Loss: 0.1063
Training Epoch: 93 [4480/50048]	Loss: 0.0866
Training Epoch: 93 [4608/50048]	Loss: 0.0751
Training Epoch: 93 [4736/50048]	Loss: 0.1088
Training Epoch: 93 [4864/50048]	Loss: 0.0782
Training Epoch: 93 [4992/50048]	Loss: 0.0744
Training Epoch: 93 [5120/50048]	Loss: 0.0798
Training Epoch: 93 [5248/50048]	Loss: 0.0777
Training Epoch: 93 [5376/50048]	Loss: 0.0836
Training Epoch: 93 [5504/50048]	Loss: 0.0960
Training Epoch: 93 [5632/50048]	Loss: 0.0616
Training Epoch: 93 [5760/50048]	Loss: 0.0723
Training Epoch: 93 [5888/50048]	Loss: 0.0387
Training Epoch: 93 [6016/50048]	Loss: 0.0303
Training Epoch: 93 [6144/50048]	Loss: 0.0197
Training Epoch: 93 [6272/50048]	Loss: 0.0444
Training Epoch: 93 [6400/50048]	Loss: 0.1168
Training Epoch: 93 [6528/50048]	Loss: 0.0846
Training Epoch: 93 [6656/50048]	Loss: 0.0912
Training Epoch: 93 [6784/50048]	Loss: 0.0973
Training Epoch: 93 [6912/50048]	Loss: 0.0620
Training Epoch: 93 [7040/50048]	Loss: 0.0714
Training Epoch: 93 [7168/50048]	Loss: 0.1218
Training Epoch: 93 [7296/50048]	Loss: 0.1369
Training Epoch: 93 [7424/50048]	Loss: 0.0326
Training Epoch: 93 [7552/50048]	Loss: 0.0766
Training Epoch: 93 [7680/50048]	Loss: 0.0600
Training Epoch: 93 [7808/50048]	Loss: 0.1281
Training Epoch: 93 [7936/50048]	Loss: 0.1023
Training Epoch: 93 [8064/50048]	Loss: 0.1795
Training Epoch: 93 [8192/50048]	Loss: 0.0568
Training Epoch: 93 [8320/50048]	Loss: 0.0951
Training Epoch: 93 [8448/50048]	Loss: 0.0750
Training Epoch: 93 [8576/50048]	Loss: 0.1515
Training Epoch: 93 [8704/50048]	Loss: 0.0630
Training Epoch: 93 [8832/50048]	Loss: 0.0781
Training Epoch: 93 [8960/50048]	Loss: 0.0709
Training Epoch: 93 [9088/50048]	Loss: 0.0843
Training Epoch: 93 [9216/50048]	Loss: 0.1159
Training Epoch: 93 [9344/50048]	Loss: 0.1113
Training Epoch: 93 [9472/50048]	Loss: 0.0724
Training Epoch: 93 [9600/50048]	Loss: 0.1345
Training Epoch: 93 [9728/50048]	Loss: 0.0770
Training Epoch: 93 [9856/50048]	Loss: 0.1007
Training Epoch: 93 [9984/50048]	Loss: 0.0509
Training Epoch: 93 [10112/50048]	Loss: 0.1027
Training Epoch: 93 [10240/50048]	Loss: 0.0524
Training Epoch: 93 [10368/50048]	Loss: 0.1237
Training Epoch: 93 [10496/50048]	Loss: 0.0828
Training Epoch: 93 [10624/50048]	Loss: 0.1294
Training Epoch: 93 [10752/50048]	Loss: 0.0559
Training Epoch: 93 [10880/50048]	Loss: 0.1163
Training Epoch: 93 [11008/50048]	Loss: 0.0965
Training Epoch: 93 [11136/50048]	Loss: 0.0729
Training Epoch: 93 [11264/50048]	Loss: 0.0488
Training Epoch: 93 [11392/50048]	Loss: 0.0805
Training Epoch: 93 [11520/50048]	Loss: 0.1103
Training Epoch: 93 [11648/50048]	Loss: 0.0849
Training Epoch: 93 [11776/50048]	Loss: 0.0860
Training Epoch: 93 [11904/50048]	Loss: 0.1718
Training Epoch: 93 [12032/50048]	Loss: 0.0880
Training Epoch: 93 [12160/50048]	Loss: 0.1000
Training Epoch: 93 [12288/50048]	Loss: 0.0661
Training Epoch: 93 [12416/50048]	Loss: 0.0525
Training Epoch: 93 [12544/50048]	Loss: 0.0368
Training Epoch: 93 [12672/50048]	Loss: 0.0452
Training Epoch: 93 [12800/50048]	Loss: 0.1082
Training Epoch: 93 [12928/50048]	Loss: 0.1282
Training Epoch: 93 [13056/50048]	Loss: 0.0583
Training Epoch: 93 [13184/50048]	Loss: 0.0959
Training Epoch: 93 [13312/50048]	Loss: 0.0426
Training Epoch: 93 [13440/50048]	Loss: 0.0733
Training Epoch: 93 [13568/50048]	Loss: 0.0647
Training Epoch: 93 [13696/50048]	Loss: 0.0923
Training Epoch: 93 [13824/50048]	Loss: 0.1244
Training Epoch: 93 [13952/50048]	Loss: 0.1227
Training Epoch: 93 [14080/50048]	Loss: 0.1201
Training Epoch: 93 [14208/50048]	Loss: 0.1518
Training Epoch: 93 [14336/50048]	Loss: 0.1113
Training Epoch: 93 [14464/50048]	Loss: 0.0644
Training Epoch: 93 [14592/50048]	Loss: 0.0561
Training Epoch: 93 [14720/50048]	Loss: 0.0846
Training Epoch: 93 [14848/50048]	Loss: 0.1012
Training Epoch: 93 [14976/50048]	Loss: 0.0744
Training Epoch: 93 [15104/50048]	Loss: 0.0757
Training Epoch: 93 [15232/50048]	Loss: 0.1018
Training Epoch: 93 [15360/50048]	Loss: 0.0587
Training Epoch: 93 [15488/50048]	Loss: 0.0548
Training Epoch: 93 [15616/50048]	Loss: 0.0948
Training Epoch: 93 [15744/50048]	Loss: 0.1022
Training Epoch: 93 [15872/50048]	Loss: 0.0585
Training Epoch: 93 [16000/50048]	Loss: 0.0819
Training Epoch: 93 [16128/50048]	Loss: 0.0769
Training Epoch: 93 [16256/50048]	Loss: 0.0885
Training Epoch: 93 [16384/50048]	Loss: 0.0950
Training Epoch: 93 [16512/50048]	Loss: 0.0744
Training Epoch: 93 [16640/50048]	Loss: 0.0652
Training Epoch: 93 [16768/50048]	Loss: 0.0738
Training Epoch: 93 [16896/50048]	Loss: 0.0619
Training Epoch: 93 [17024/50048]	Loss: 0.0416
Training Epoch: 93 [17152/50048]	Loss: 0.0583
Training Epoch: 93 [17280/50048]	Loss: 0.0419
Training Epoch: 93 [17408/50048]	Loss: 0.0710
Training Epoch: 93 [17536/50048]	Loss: 0.0742
Training Epoch: 93 [17664/50048]	Loss: 0.0723
Training Epoch: 93 [17792/50048]	Loss: 0.0743
Training Epoch: 93 [17920/50048]	Loss: 0.0421
Training Epoch: 93 [18048/50048]	Loss: 0.1766
Training Epoch: 93 [18176/50048]	Loss: 0.0704
Training Epoch: 93 [18304/50048]	Loss: 0.0803
Training Epoch: 93 [18432/50048]	Loss: 0.0823
Training Epoch: 93 [18560/50048]	Loss: 0.0687
Training Epoch: 93 [18688/50048]	Loss: 0.0867
Training Epoch: 93 [18816/50048]	Loss: 0.0497
Training Epoch: 93 [18944/50048]	Loss: 0.0942
Training Epoch: 93 [19072/50048]	Loss: 0.0929
Training Epoch: 93 [19200/50048]	Loss: 0.0495
Training Epoch: 93 [19328/50048]	Loss: 0.1761
Training Epoch: 93 [19456/50048]	Loss: 0.1399
Training Epoch: 93 [19584/50048]	Loss: 0.0343
Training Epoch: 93 [19712/50048]	Loss: 0.1263
Training Epoch: 93 [19840/50048]	Loss: 0.0574
Training Epoch: 93 [19968/50048]	Loss: 0.0357
Training Epoch: 93 [20096/50048]	Loss: 0.0615
Training Epoch: 93 [20224/50048]	Loss: 0.0938
Training Epoch: 93 [20352/50048]	Loss: 0.1273
Training Epoch: 93 [20480/50048]	Loss: 0.1221
Training Epoch: 93 [20608/50048]	Loss: 0.0659
Training Epoch: 93 [20736/50048]	Loss: 0.0558
Training Epoch: 93 [20864/50048]	Loss: 0.0474
Training Epoch: 93 [20992/50048]	Loss: 0.0924
Training Epoch: 93 [21120/50048]	Loss: 0.1347
Training Epoch: 93 [21248/50048]	Loss: 0.1497
Training Epoch: 93 [21376/50048]	Loss: 0.0860
Training Epoch: 93 [21504/50048]	Loss: 0.1554
Training Epoch: 93 [21632/50048]	Loss: 0.0537
Training Epoch: 93 [21760/50048]	Loss: 0.0848
Training Epoch: 93 [21888/50048]	Loss: 0.0471
Training Epoch: 93 [22016/50048]	Loss: 0.1280
Training Epoch: 93 [22144/50048]	Loss: 0.1652
Training Epoch: 93 [22272/50048]	Loss: 0.1696
Training Epoch: 93 [22400/50048]	Loss: 0.0501
Training Epoch: 93 [22528/50048]	Loss: 0.0563
Training Epoch: 93 [22656/50048]	Loss: 0.1006
Training Epoch: 93 [22784/50048]	Loss: 0.0632
Training Epoch: 93 [22912/50048]	Loss: 0.0766
Training Epoch: 93 [23040/50048]	Loss: 0.0513
Training Epoch: 93 [23168/50048]	Loss: 0.1906
Training Epoch: 93 [23296/50048]	Loss: 0.0308
Training Epoch: 93 [23424/50048]	Loss: 0.0680
Training Epoch: 93 [23552/50048]	Loss: 0.0626
Training Epoch: 93 [23680/50048]	Loss: 0.0360
Training Epoch: 93 [23808/50048]	Loss: 0.0993
Training Epoch: 93 [23936/50048]	Loss: 0.1182
Training Epoch: 93 [24064/50048]	Loss: 0.1683
Training Epoch: 93 [24192/50048]	Loss: 0.1111
Training Epoch: 93 [24320/50048]	Loss: 0.1330
Training Epoch: 93 [24448/50048]	Loss: 0.0874
Training Epoch: 93 [24576/50048]	Loss: 0.1499
Training Epoch: 93 [24704/50048]	Loss: 0.0966
Training Epoch: 93 [24832/50048]	Loss: 0.1576
Training Epoch: 93 [24960/50048]	Loss: 0.0913
Training Epoch: 93 [25088/50048]	Loss: 0.1255
Training Epoch: 93 [25216/50048]	Loss: 0.0868
Training Epoch: 93 [25344/50048]	Loss: 0.0620
Training Epoch: 93 [25472/50048]	Loss: 0.0634
Training Epoch: 93 [25600/50048]	Loss: 0.0972
Training Epoch: 93 [25728/50048]	Loss: 0.0862
Training Epoch: 93 [25856/50048]	Loss: 0.1404
Training Epoch: 93 [25984/50048]	Loss: 0.1240
Training Epoch: 93 [26112/50048]	Loss: 0.0573
Training Epoch: 93 [26240/50048]	Loss: 0.0950
Training Epoch: 93 [26368/50048]	Loss: 0.0697
Training Epoch: 93 [26496/50048]	Loss: 0.0866
Training Epoch: 93 [26624/50048]	Loss: 0.0798
Training Epoch: 93 [26752/50048]	Loss: 0.0976
Training Epoch: 93 [26880/50048]	Loss: 0.1317
Training Epoch: 93 [27008/50048]	Loss: 0.1967
Training Epoch: 93 [27136/50048]	Loss: 0.0982
Training Epoch: 93 [27264/50048]	Loss: 0.1710
Training Epoch: 93 [27392/50048]	Loss: 0.1200
Training Epoch: 93 [27520/50048]	Loss: 0.1756
Training Epoch: 93 [27648/50048]	Loss: 0.0328
Training Epoch: 93 [27776/50048]	Loss: 0.0496
Training Epoch: 93 [27904/50048]	Loss: 0.0871
Training Epoch: 93 [28032/50048]	Loss: 0.0947
Training Epoch: 93 [28160/50048]	Loss: 0.0798
Training Epoch: 93 [28288/50048]	Loss: 0.1120
Training Epoch: 93 [28416/50048]	Loss: 0.0510
Training Epoch: 93 [28544/50048]	Loss: 0.0522
Training Epoch: 93 [28672/50048]	Loss: 0.0875
Training Epoch: 93 [28800/50048]	Loss: 0.0813
Training Epoch: 93 [28928/50048]	Loss: 0.1547
Training Epoch: 93 [29056/50048]	Loss: 0.0827
Training Epoch: 93 [29184/50048]	Loss: 0.1597
Training Epoch: 93 [29312/50048]	Loss: 0.0320
Training Epoch: 93 [29440/50048]	Loss: 0.1005
Training Epoch: 93 [29568/50048]	Loss: 0.0905
Training Epoch: 93 [29696/50048]	Loss: 0.0717
Training Epoch: 93 [29824/50048]	Loss: 0.0990
Training Epoch: 93 [29952/50048]	Loss: 0.1208
Training Epoch: 93 [30080/50048]	Loss: 0.0801
Training Epoch: 93 [30208/50048]	Loss: 0.0532
Training Epoch: 93 [30336/50048]	Loss: 0.0966
Training Epoch: 93 [30464/50048]	Loss: 0.1249
Training Epoch: 93 [30592/50048]	Loss: 0.1003
Training Epoch: 93 [30720/50048]	Loss: 0.0324
Training Epoch: 93 [30848/50048]	Loss: 0.0936
Training Epoch: 93 [30976/50048]	Loss: 0.1082
Training Epoch: 93 [31104/50048]	Loss: 0.0515
Training Epoch: 93 [31232/50048]	Loss: 0.1335
Training Epoch: 93 [31360/50048]	Loss: 0.0869
Training Epoch: 93 [31488/50048]	Loss: 0.0779
Training Epoch: 93 [31616/50048]	Loss: 0.0780
Training Epoch: 93 [31744/50048]	Loss: 0.1096
Training Epoch: 93 [31872/50048]	Loss: 0.1165
Training Epoch: 93 [32000/50048]	Loss: 0.0718
Training Epoch: 93 [32128/50048]	Loss: 0.0571
Training Epoch: 93 [32256/50048]	Loss: 0.0857
Training Epoch: 93 [32384/50048]	Loss: 0.1241
Training Epoch: 93 [32512/50048]	Loss: 0.1501
Training Epoch: 93 [32640/50048]	Loss: 0.1427
Training Epoch: 93 [32768/50048]	Loss: 0.0261
Training Epoch: 93 [32896/50048]	Loss: 0.0727
Training Epoch: 93 [33024/50048]	Loss: 0.0835
Training Epoch: 93 [33152/50048]	Loss: 0.0615
Training Epoch: 93 [33280/50048]	Loss: 0.0719
Training Epoch: 93 [33408/50048]	Loss: 0.0721
Training Epoch: 93 [33536/50048]	Loss: 0.1537
Training Epoch: 93 [33664/50048]	Loss: 0.1345
Training Epoch: 93 [33792/50048]	Loss: 0.0683
Training Epoch: 93 [33920/50048]	Loss: 0.1291
Training Epoch: 93 [34048/50048]	Loss: 0.0499
Training Epoch: 93 [34176/50048]	Loss: 0.0567
Training Epoch: 93 [34304/50048]	Loss: 0.0771
Training Epoch: 93 [34432/50048]	Loss: 0.0461
Training Epoch: 93 [34560/50048]	Loss: 0.0473
Training Epoch: 93 [34688/50048]	Loss: 0.1408
Training Epoch: 93 [34816/50048]	Loss: 0.1240
Training Epoch: 93 [34944/50048]	Loss: 0.1385
Training Epoch: 93 [35072/50048]	Loss: 0.0727
Training Epoch: 93 [35200/50048]	Loss: 0.0766
Training Epoch: 93 [35328/50048]	Loss: 0.0919
Training Epoch: 93 [35456/50048]	Loss: 0.0970
Training Epoch: 93 [35584/50048]	Loss: 0.1029
Training Epoch: 93 [35712/50048]	Loss: 0.0973
Training Epoch: 93 [35840/50048]	Loss: 0.0734
Training Epoch: 93 [35968/50048]	Loss: 0.0279
Training Epoch: 93 [36096/50048]	Loss: 0.0624
Training Epoch: 93 [36224/50048]	Loss: 0.0660
Training Epoch: 93 [36352/50048]	Loss: 0.0472
Training Epoch: 93 [36480/50048]	Loss: 0.1174
Training Epoch: 93 [36608/50048]	Loss: 0.0602
Training Epoch: 93 [36736/50048]	Loss: 0.1071
Training Epoch: 93 [36864/50048]	Loss: 0.0953
Training Epoch: 93 [36992/50048]	Loss: 0.1192
Training Epoch: 93 [37120/50048]	Loss: 0.1167
Training Epoch: 93 [37248/50048]	Loss: 0.0790
Training Epoch: 93 [37376/50048]	Loss: 0.0876
Training Epoch: 93 [37504/50048]	Loss: 0.1250
Training Epoch: 93 [37632/50048]	Loss: 0.1556
Training Epoch: 93 [37760/50048]	Loss: 0.0599
Training Epoch: 93 [37888/50048]	Loss: 0.1674
Training Epoch: 93 [38016/50048]	Loss: 0.0755
Training Epoch: 93 [38144/50048]	Loss: 0.0877
Training Epoch: 93 [38272/50048]	Loss: 0.1075
Training Epoch: 93 [38400/50048]	Loss: 0.1419
Training Epoch: 93 [38528/50048]	Loss: 0.1034
Training Epoch: 93 [38656/50048]	Loss: 0.1006
Training Epoch: 93 [38784/50048]	Loss: 0.1299
Training Epoch: 93 [38912/50048]	Loss: 0.1386
Training Epoch: 93 [39040/50048]	Loss: 0.0847
Training Epoch: 93 [39168/50048]	Loss: 0.1240
Training Epoch: 93 [39296/50048]	Loss: 0.1065
Training Epoch: 93 [39424/50048]	Loss: 0.1510
Training Epoch: 93 [39552/50048]	Loss: 0.0892
Training Epoch: 93 [39680/50048]	Loss: 0.1047
Training Epoch: 93 [39808/50048]	Loss: 0.0892
Training Epoch: 93 [39936/50048]	Loss: 0.0934
Training Epoch: 93 [40064/50048]	Loss: 0.1112
Training Epoch: 93 [40192/50048]	Loss: 0.1110
Training Epoch: 93 [40320/50048]	Loss: 0.1246
Training Epoch: 93 [40448/50048]	Loss: 0.0859
Training Epoch: 93 [40576/50048]	Loss: 0.0828
Training Epoch: 93 [40704/50048]	Loss: 0.2066
Training Epoch: 93 [40832/50048]	Loss: 0.1436
Training Epoch: 93 [40960/50048]	Loss: 0.0699
Training Epoch: 93 [41088/50048]	Loss: 0.0535
Training Epoch: 93 [41216/50048]	Loss: 0.0695
Training Epoch: 93 [41344/50048]	Loss: 0.1174
Training Epoch: 93 [41472/50048]	Loss: 0.0956
Training Epoch: 93 [41600/50048]	Loss: 0.0386
Training Epoch: 93 [41728/50048]	Loss: 0.0910
Training Epoch: 93 [41856/50048]	Loss: 0.0797
Training Epoch: 93 [41984/50048]	Loss: 0.0732
Training Epoch: 93 [42112/50048]	Loss: 0.1280
Training Epoch: 93 [42240/50048]	Loss: 0.0591
Training Epoch: 93 [42368/50048]	Loss: 0.1269
Training Epoch: 93 [42496/50048]	Loss: 0.0721
Training Epoch: 93 [42624/50048]	Loss: 0.0519
Training Epoch: 93 [42752/50048]	Loss: 0.0884
Training Epoch: 93 [42880/50048]	Loss: 0.1205
Training Epoch: 93 [43008/50048]	Loss: 0.0629
Training Epoch: 93 [43136/50048]	Loss: 0.0592
Training Epoch: 93 [43264/50048]	Loss: 0.0558
Training Epoch: 93 [43392/50048]	Loss: 0.0811
Training Epoch: 93 [43520/50048]	Loss: 0.1266
Training Epoch: 93 [43648/50048]	Loss: 0.0370
Training Epoch: 93 [43776/50048]	Loss: 0.0767
Training Epoch: 93 [43904/50048]	Loss: 0.0557
Training Epoch: 93 [44032/50048]	Loss: 0.1680
Training Epoch: 93 [44160/50048]	Loss: 0.1113
Training Epoch: 93 [44288/50048]	Loss: 0.1056
Training Epoch: 93 [44416/50048]	Loss: 0.1452
Training Epoch: 93 [44544/50048]	Loss: 0.1090
Training Epoch: 93 [44672/50048]	Loss: 0.0804
Training Epoch: 93 [44800/50048]	Loss: 0.0959
Training Epoch: 93 [44928/50048]	Loss: 0.1232
Training Epoch: 93 [45056/50048]	Loss: 0.0296
Training Epoch: 93 [45184/50048]	Loss: 0.0468
Training Epoch: 93 [45312/50048]	Loss: 0.0777
Training Epoch: 93 [45440/50048]	Loss: 0.0680
Training Epoch: 93 [45568/50048]	Loss: 0.1144
Training Epoch: 93 [45696/50048]	Loss: 0.1255
2022-12-06 08:29:11,274 [ZeusDataLoader(train)] train epoch 94 done: time=86.54 energy=10505.91
2022-12-06 08:29:11,276 [ZeusDataLoader(eval)] Epoch 94 begin.
Training Epoch: 93 [45824/50048]	Loss: 0.0887
Training Epoch: 93 [45952/50048]	Loss: 0.0874
Training Epoch: 93 [46080/50048]	Loss: 0.0916
Training Epoch: 93 [46208/50048]	Loss: 0.0857
Training Epoch: 93 [46336/50048]	Loss: 0.1114
Training Epoch: 93 [46464/50048]	Loss: 0.1091
Training Epoch: 93 [46592/50048]	Loss: 0.0804
Training Epoch: 93 [46720/50048]	Loss: 0.1178
Training Epoch: 93 [46848/50048]	Loss: 0.1253
Training Epoch: 93 [46976/50048]	Loss: 0.1845
Training Epoch: 93 [47104/50048]	Loss: 0.0806
Training Epoch: 93 [47232/50048]	Loss: 0.0941
Training Epoch: 93 [47360/50048]	Loss: 0.0991
Training Epoch: 93 [47488/50048]	Loss: 0.1485
Training Epoch: 93 [47616/50048]	Loss: 0.1428
Training Epoch: 93 [47744/50048]	Loss: 0.0497
Training Epoch: 93 [47872/50048]	Loss: 0.0974
Training Epoch: 93 [48000/50048]	Loss: 0.0981
Training Epoch: 93 [48128/50048]	Loss: 0.0925
Training Epoch: 93 [48256/50048]	Loss: 0.0800
Training Epoch: 93 [48384/50048]	Loss: 0.0585
Training Epoch: 93 [48512/50048]	Loss: 0.0849
Training Epoch: 93 [48640/50048]	Loss: 0.0989
Training Epoch: 93 [48768/50048]	Loss: 0.0666
Training Epoch: 93 [48896/50048]	Loss: 0.0304
Training Epoch: 93 [49024/50048]	Loss: 0.0892
Training Epoch: 93 [49152/50048]	Loss: 0.0807
Training Epoch: 93 [49280/50048]	Loss: 0.0926
Training Epoch: 93 [49408/50048]	Loss: 0.0635
Training Epoch: 93 [49536/50048]	Loss: 0.0953
Training Epoch: 93 [49664/50048]	Loss: 0.1337
Training Epoch: 93 [49792/50048]	Loss: 0.0628
Training Epoch: 93 [49920/50048]	Loss: 0.1124
Training Epoch: 93 [50048/50048]	Loss: 0.1058
2022-12-06 13:29:14.940 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 08:29:14,960 [ZeusDataLoader(eval)] eval epoch 94 done: time=3.68 energy=440.90
2022-12-06 08:29:14,961 [ZeusDataLoader(train)] Up to epoch 94: time=8479.23, energy=1029238.97, cost=1256552.07
2022-12-06 08:29:14,961 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 08:29:14,961 [ZeusDataLoader(train)] Expected next epoch: time=8569.03, energy=1040036.98, cost=1269808.45
2022-12-06 08:29:14,962 [ZeusDataLoader(train)] Epoch 95 begin.
Validation Epoch: 93, Average loss: 0.0183, Accuracy: 0.6483
2022-12-06 08:29:15,154 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:29:15,155 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:29:15.157 [ZeusMonitor] Monitor started.
2022-12-06 13:29:15.157 [ZeusMonitor] Running indefinitely. 2022-12-06 13:29:15.157 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:29:15.157 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e95+gpu0.power.log
Training Epoch: 94 [128/50048]	Loss: 0.1287
Training Epoch: 94 [256/50048]	Loss: 0.0309
Training Epoch: 94 [384/50048]	Loss: 0.0623
Training Epoch: 94 [512/50048]	Loss: 0.1316
Training Epoch: 94 [640/50048]	Loss: 0.0713
Training Epoch: 94 [768/50048]	Loss: 0.0895
Training Epoch: 94 [896/50048]	Loss: 0.0358
Training Epoch: 94 [1024/50048]	Loss: 0.0722
Training Epoch: 94 [1152/50048]	Loss: 0.0765
Training Epoch: 94 [1280/50048]	Loss: 0.1646
Training Epoch: 94 [1408/50048]	Loss: 0.0778
Training Epoch: 94 [1536/50048]	Loss: 0.0570
Training Epoch: 94 [1664/50048]	Loss: 0.2020
Training Epoch: 94 [1792/50048]	Loss: 0.1019
Training Epoch: 94 [1920/50048]	Loss: 0.1591
Training Epoch: 94 [2048/50048]	Loss: 0.0787
Training Epoch: 94 [2176/50048]	Loss: 0.1190
Training Epoch: 94 [2304/50048]	Loss: 0.0596
Training Epoch: 94 [2432/50048]	Loss: 0.1412
Training Epoch: 94 [2560/50048]	Loss: 0.0789
Training Epoch: 94 [2688/50048]	Loss: 0.1735
Training Epoch: 94 [2816/50048]	Loss: 0.0519
Training Epoch: 94 [2944/50048]	Loss: 0.1115
Training Epoch: 94 [3072/50048]	Loss: 0.0818
Training Epoch: 94 [3200/50048]	Loss: 0.0910
Training Epoch: 94 [3328/50048]	Loss: 0.0471
Training Epoch: 94 [3456/50048]	Loss: 0.1199
Training Epoch: 94 [3584/50048]	Loss: 0.0303
Training Epoch: 94 [3712/50048]	Loss: 0.0629
Training Epoch: 94 [3840/50048]	Loss: 0.0715
Training Epoch: 94 [3968/50048]	Loss: 0.1041
Training Epoch: 94 [4096/50048]	Loss: 0.0686
Training Epoch: 94 [4224/50048]	Loss: 0.1453
Training Epoch: 94 [4352/50048]	Loss: 0.1149
Training Epoch: 94 [4480/50048]	Loss: 0.1088
Training Epoch: 94 [4608/50048]	Loss: 0.1081
Training Epoch: 94 [4736/50048]	Loss: 0.0759
Training Epoch: 94 [4864/50048]	Loss: 0.0587
Training Epoch: 94 [4992/50048]	Loss: 0.0754
Training Epoch: 94 [5120/50048]	Loss: 0.1104
Training Epoch: 94 [5248/50048]	Loss: 0.0747
Training Epoch: 94 [5376/50048]	Loss: 0.1035
Training Epoch: 94 [5504/50048]	Loss: 0.0651
Training Epoch: 94 [5632/50048]	Loss: 0.1943
Training Epoch: 94 [5760/50048]	Loss: 0.0915
Training Epoch: 94 [5888/50048]	Loss: 0.0859
Training Epoch: 94 [6016/50048]	Loss: 0.0493
Training Epoch: 94 [6144/50048]	Loss: 0.0965
Training Epoch: 94 [6272/50048]	Loss: 0.0886
Training Epoch: 94 [6400/50048]	Loss: 0.0672
Training Epoch: 94 [6528/50048]	Loss: 0.1010
Training Epoch: 94 [6656/50048]	Loss: 0.1312
Training Epoch: 94 [6784/50048]	Loss: 0.0289
Training Epoch: 94 [6912/50048]	Loss: 0.0580
Training Epoch: 94 [7040/50048]	Loss: 0.1146
Training Epoch: 94 [7168/50048]	Loss: 0.0843
Training Epoch: 94 [7296/50048]	Loss: 0.0608
Training Epoch: 94 [7424/50048]	Loss: 0.1006
Training Epoch: 94 [7552/50048]	Loss: 0.0636
Training Epoch: 94 [7680/50048]	Loss: 0.1114
Training Epoch: 94 [7808/50048]	Loss: 0.0761
Training Epoch: 94 [7936/50048]	Loss: 0.0481
Training Epoch: 94 [8064/50048]	Loss: 0.0939
Training Epoch: 94 [8192/50048]	Loss: 0.0867
Training Epoch: 94 [8320/50048]	Loss: 0.0724
Training Epoch: 94 [8448/50048]	Loss: 0.0616
Training Epoch: 94 [8576/50048]	Loss: 0.0678
Training Epoch: 94 [8704/50048]	Loss: 0.1144
Training Epoch: 94 [8832/50048]	Loss: 0.0969
Training Epoch: 94 [8960/50048]	Loss: 0.0256
Training Epoch: 94 [9088/50048]	Loss: 0.1185
Training Epoch: 94 [9216/50048]	Loss: 0.0402
Training Epoch: 94 [9344/50048]	Loss: 0.1011
Training Epoch: 94 [9472/50048]	Loss: 0.0690
Training Epoch: 94 [9600/50048]	Loss: 0.0426
Training Epoch: 94 [9728/50048]	Loss: 0.0488
Training Epoch: 94 [9856/50048]	Loss: 0.1204
Training Epoch: 94 [9984/50048]	Loss: 0.1916
Training Epoch: 94 [10112/50048]	Loss: 0.1560
Training Epoch: 94 [10240/50048]	Loss: 0.0353
Training Epoch: 94 [10368/50048]	Loss: 0.1148
Training Epoch: 94 [10496/50048]	Loss: 0.0519
Training Epoch: 94 [10624/50048]	Loss: 0.0916
Training Epoch: 94 [10752/50048]	Loss: 0.1595
Training Epoch: 94 [10880/50048]	Loss: 0.1132
Training Epoch: 94 [11008/50048]	Loss: 0.0560
Training Epoch: 94 [11136/50048]	Loss: 0.1232
Training Epoch: 94 [11264/50048]	Loss: 0.1240
Training Epoch: 94 [11392/50048]	Loss: 0.1546
Training Epoch: 94 [11520/50048]	Loss: 0.0719
Training Epoch: 94 [11648/50048]	Loss: 0.0499
Training Epoch: 94 [11776/50048]	Loss: 0.0844
Training Epoch: 94 [11904/50048]	Loss: 0.1337
Training Epoch: 94 [12032/50048]	Loss: 0.0875
Training Epoch: 94 [12160/50048]	Loss: 0.0722
Training Epoch: 94 [12288/50048]	Loss: 0.1183
Training Epoch: 94 [12416/50048]	Loss: 0.0798
Training Epoch: 94 [12544/50048]	Loss: 0.1142
Training Epoch: 94 [12672/50048]	Loss: 0.0527
Training Epoch: 94 [12800/50048]	Loss: 0.0606
Training Epoch: 94 [12928/50048]	Loss: 0.0525
Training Epoch: 94 [13056/50048]	Loss: 0.1306
Training Epoch: 94 [13184/50048]	Loss: 0.1407
Training Epoch: 94 [13312/50048]	Loss: 0.1230
Training Epoch: 94 [13440/50048]	Loss: 0.0751
Training Epoch: 94 [13568/50048]	Loss: 0.0607
Training Epoch: 94 [13696/50048]	Loss: 0.0300
Training Epoch: 94 [13824/50048]	Loss: 0.0843
Training Epoch: 94 [13952/50048]	Loss: 0.0570
Training Epoch: 94 [14080/50048]	Loss: 0.0755
Training Epoch: 94 [14208/50048]	Loss: 0.0515
Training Epoch: 94 [14336/50048]	Loss: 0.1181
Training Epoch: 94 [14464/50048]	Loss: 0.0966
Training Epoch: 94 [14592/50048]	Loss: 0.1837
Training Epoch: 94 [14720/50048]	Loss: 0.0994
Training Epoch: 94 [14848/50048]	Loss: 0.0871
Training Epoch: 94 [14976/50048]	Loss: 0.0491
Training Epoch: 94 [15104/50048]	Loss: 0.0804
Training Epoch: 94 [15232/50048]	Loss: 0.0652
Training Epoch: 94 [15360/50048]	Loss: 0.1025
Training Epoch: 94 [15488/50048]	Loss: 0.0742
Training Epoch: 94 [15616/50048]	Loss: 0.0928
Training Epoch: 94 [15744/50048]	Loss: 0.0441
Training Epoch: 94 [15872/50048]	Loss: 0.0329
Training Epoch: 94 [16000/50048]	Loss: 0.0526
Training Epoch: 94 [16128/50048]	Loss: 0.1070
Training Epoch: 94 [16256/50048]	Loss: 0.0762
Training Epoch: 94 [16384/50048]	Loss: 0.0718
Training Epoch: 94 [16512/50048]	Loss: 0.0743
Training Epoch: 94 [16640/50048]	Loss: 0.0244
Training Epoch: 94 [16768/50048]	Loss: 0.0866
Training Epoch: 94 [16896/50048]	Loss: 0.0966
Training Epoch: 94 [17024/50048]	Loss: 0.0919
Training Epoch: 94 [17152/50048]	Loss: 0.1477
Training Epoch: 94 [17280/50048]	Loss: 0.0615
Training Epoch: 94 [17408/50048]	Loss: 0.0715
Training Epoch: 94 [17536/50048]	Loss: 0.0596
Training Epoch: 94 [17664/50048]	Loss: 0.0980
Training Epoch: 94 [17792/50048]	Loss: 0.1474
Training Epoch: 94 [17920/50048]	Loss: 0.0649
Training Epoch: 94 [18048/50048]	Loss: 0.1457
Training Epoch: 94 [18176/50048]	Loss: 0.1365
Training Epoch: 94 [18304/50048]	Loss: 0.1130
Training Epoch: 94 [18432/50048]	Loss: 0.0710
Training Epoch: 94 [18560/50048]	Loss: 0.0520
Training Epoch: 94 [18688/50048]	Loss: 0.0631
Training Epoch: 94 [18816/50048]	Loss: 0.0909
Training Epoch: 94 [18944/50048]	Loss: 0.1650
Training Epoch: 94 [19072/50048]	Loss: 0.0377
Training Epoch: 94 [19200/50048]	Loss: 0.0512
Training Epoch: 94 [19328/50048]	Loss: 0.1036
Training Epoch: 94 [19456/50048]	Loss: 0.0715
Training Epoch: 94 [19584/50048]	Loss: 0.0958
Training Epoch: 94 [19712/50048]	Loss: 0.1210
Training Epoch: 94 [19840/50048]	Loss: 0.1270
Training Epoch: 94 [19968/50048]	Loss: 0.1091
Training Epoch: 94 [20096/50048]	Loss: 0.0483
Training Epoch: 94 [20224/50048]	Loss: 0.0784
Training Epoch: 94 [20352/50048]	Loss: 0.0768
Training Epoch: 94 [20480/50048]	Loss: 0.0754
Training Epoch: 94 [20608/50048]	Loss: 0.0854
Training Epoch: 94 [20736/50048]	Loss: 0.1103
Training Epoch: 94 [20864/50048]	Loss: 0.0606
Training Epoch: 94 [20992/50048]	Loss: 0.0849
Training Epoch: 94 [21120/50048]	Loss: 0.0350
Training Epoch: 94 [21248/50048]	Loss: 0.0512
Training Epoch: 94 [21376/50048]	Loss: 0.0822
Training Epoch: 94 [21504/50048]	Loss: 0.0389
Training Epoch: 94 [21632/50048]	Loss: 0.1092
Training Epoch: 94 [21760/50048]	Loss: 0.1775
Training Epoch: 94 [21888/50048]	Loss: 0.0950
Training Epoch: 94 [22016/50048]	Loss: 0.1114
Training Epoch: 94 [22144/50048]	Loss: 0.0710
Training Epoch: 94 [22272/50048]	Loss: 0.0730
Training Epoch: 94 [22400/50048]	Loss: 0.1510
Training Epoch: 94 [22528/50048]	Loss: 0.0747
Training Epoch: 94 [22656/50048]	Loss: 0.0353
Training Epoch: 94 [22784/50048]	Loss: 0.0485
Training Epoch: 94 [22912/50048]	Loss: 0.0902
Training Epoch: 94 [23040/50048]	Loss: 0.0685
Training Epoch: 94 [23168/50048]	Loss: 0.0840
Training Epoch: 94 [23296/50048]	Loss: 0.0351
Training Epoch: 94 [23424/50048]	Loss: 0.0695
Training Epoch: 94 [23552/50048]	Loss: 0.0798
Training Epoch: 94 [23680/50048]	Loss: 0.0972
Training Epoch: 94 [23808/50048]	Loss: 0.0572
Training Epoch: 94 [23936/50048]	Loss: 0.0733
Training Epoch: 94 [24064/50048]	Loss: 0.0654
Training Epoch: 94 [24192/50048]	Loss: 0.0630
Training Epoch: 94 [24320/50048]	Loss: 0.0749
Training Epoch: 94 [24448/50048]	Loss: 0.0763
Training Epoch: 94 [24576/50048]	Loss: 0.2067
Training Epoch: 94 [24704/50048]	Loss: 0.0639
Training Epoch: 94 [24832/50048]	Loss: 0.0988
Training Epoch: 94 [24960/50048]	Loss: 0.1092
Training Epoch: 94 [25088/50048]	Loss: 0.1240
Training Epoch: 94 [25216/50048]	Loss: 0.0895
Training Epoch: 94 [25344/50048]	Loss: 0.0761
Training Epoch: 94 [25472/50048]	Loss: 0.0496
Training Epoch: 94 [25600/50048]	Loss: 0.1362
Training Epoch: 94 [25728/50048]	Loss: 0.0723
Training Epoch: 94 [25856/50048]	Loss: 0.0877
Training Epoch: 94 [25984/50048]	Loss: 0.0875
Training Epoch: 94 [26112/50048]	Loss: 0.0409
Training Epoch: 94 [26240/50048]	Loss: 0.1773
Training Epoch: 94 [26368/50048]	Loss: 0.0848
Training Epoch: 94 [26496/50048]	Loss: 0.0912
Training Epoch: 94 [26624/50048]	Loss: 0.1214
Training Epoch: 94 [26752/50048]	Loss: 0.0768
Training Epoch: 94 [26880/50048]	Loss: 0.1059
Training Epoch: 94 [27008/50048]	Loss: 0.0970
Training Epoch: 94 [27136/50048]	Loss: 0.0891
Training Epoch: 94 [27264/50048]	Loss: 0.0535
Training Epoch: 94 [27392/50048]	Loss: 0.0665
Training Epoch: 94 [27520/50048]	Loss: 0.0834
Training Epoch: 94 [27648/50048]	Loss: 0.0451
Training Epoch: 94 [27776/50048]	Loss: 0.0796
Training Epoch: 94 [27904/50048]	Loss: 0.0595
Training Epoch: 94 [28032/50048]	Loss: 0.0508
Training Epoch: 94 [28160/50048]	Loss: 0.0826
Training Epoch: 94 [28288/50048]	Loss: 0.0999
Training Epoch: 94 [28416/50048]	Loss: 0.0489
Training Epoch: 94 [28544/50048]	Loss: 0.0594
Training Epoch: 94 [28672/50048]	Loss: 0.0736
Training Epoch: 94 [28800/50048]	Loss: 0.0884
Training Epoch: 94 [28928/50048]	Loss: 0.0357
Training Epoch: 94 [29056/50048]	Loss: 0.0570
Training Epoch: 94 [29184/50048]	Loss: 0.0866
Training Epoch: 94 [29312/50048]	Loss: 0.0856
Training Epoch: 94 [29440/50048]	Loss: 0.0954
Training Epoch: 94 [29568/50048]	Loss: 0.0295
Training Epoch: 94 [29696/50048]	Loss: 0.0606
Training Epoch: 94 [29824/50048]	Loss: 0.1765
Training Epoch: 94 [29952/50048]	Loss: 0.0630
Training Epoch: 94 [30080/50048]	Loss: 0.0974
Training Epoch: 94 [30208/50048]	Loss: 0.0827
Training Epoch: 94 [30336/50048]	Loss: 0.1087
Training Epoch: 94 [30464/50048]	Loss: 0.0978
Training Epoch: 94 [30592/50048]	Loss: 0.1106
Training Epoch: 94 [30720/50048]	Loss: 0.0902
Training Epoch: 94 [30848/50048]	Loss: 0.1434
Training Epoch: 94 [30976/50048]	Loss: 0.0248
Training Epoch: 94 [31104/50048]	Loss: 0.0607
Training Epoch: 94 [31232/50048]	Loss: 0.0682
Training Epoch: 94 [31360/50048]	Loss: 0.0643
Training Epoch: 94 [31488/50048]	Loss: 0.0905
Training Epoch: 94 [31616/50048]	Loss: 0.1325
Training Epoch: 94 [31744/50048]	Loss: 0.1436
Training Epoch: 94 [31872/50048]	Loss: 0.1241
Training Epoch: 94 [32000/50048]	Loss: 0.0345
Training Epoch: 94 [32128/50048]	Loss: 0.0598
Training Epoch: 94 [32256/50048]	Loss: 0.0888
Training Epoch: 94 [32384/50048]	Loss: 0.1158
Training Epoch: 94 [32512/50048]	Loss: 0.0667
Training Epoch: 94 [32640/50048]	Loss: 0.0706
Training Epoch: 94 [32768/50048]	Loss: 0.1158
Training Epoch: 94 [32896/50048]	Loss: 0.1742
Training Epoch: 94 [33024/50048]	Loss: 0.1185
Training Epoch: 94 [33152/50048]	Loss: 0.0672
Training Epoch: 94 [33280/50048]	Loss: 0.0711
Training Epoch: 94 [33408/50048]	Loss: 0.1309
Training Epoch: 94 [33536/50048]	Loss: 0.1534
Training Epoch: 94 [33664/50048]	Loss: 0.0861
Training Epoch: 94 [33792/50048]	Loss: 0.0531
Training Epoch: 94 [33920/50048]	Loss: 0.0995
Training Epoch: 94 [34048/50048]	Loss: 0.1598
Training Epoch: 94 [34176/50048]	Loss: 0.1416
Training Epoch: 94 [34304/50048]	Loss: 0.0659
Training Epoch: 94 [34432/50048]	Loss: 0.0712
Training Epoch: 94 [34560/50048]	Loss: 0.1445
Training Epoch: 94 [34688/50048]	Loss: 0.0698
Training Epoch: 94 [34816/50048]	Loss: 0.0536
Training Epoch: 94 [34944/50048]	Loss: 0.0876
Training Epoch: 94 [35072/50048]	Loss: 0.0696
Training Epoch: 94 [35200/50048]	Loss: 0.0918
Training Epoch: 94 [35328/50048]	Loss: 0.1160
Training Epoch: 94 [35456/50048]	Loss: 0.0566
Training Epoch: 94 [35584/50048]	Loss: 0.0842
Training Epoch: 94 [35712/50048]	Loss: 0.1042
Training Epoch: 94 [35840/50048]	Loss: 0.1484
Training Epoch: 94 [35968/50048]	Loss: 0.0800
Training Epoch: 94 [36096/50048]	Loss: 0.0997
Training Epoch: 94 [36224/50048]	Loss: 0.0719
Training Epoch: 94 [36352/50048]	Loss: 0.1734
Training Epoch: 94 [36480/50048]	Loss: 0.1343
Training Epoch: 94 [36608/50048]	Loss: 0.1178
Training Epoch: 94 [36736/50048]	Loss: 0.0902
Training Epoch: 94 [36864/50048]	Loss: 0.1136
Training Epoch: 94 [36992/50048]	Loss: 0.0855
Training Epoch: 94 [37120/50048]	Loss: 0.0329
Training Epoch: 94 [37248/50048]	Loss: 0.0607
Training Epoch: 94 [37376/50048]	Loss: 0.0863
Training Epoch: 94 [37504/50048]	Loss: 0.1306
Training Epoch: 94 [37632/50048]	Loss: 0.0822
Training Epoch: 94 [37760/50048]	Loss: 0.0818
Training Epoch: 94 [37888/50048]	Loss: 0.0550
Training Epoch: 94 [38016/50048]	Loss: 0.1011
Training Epoch: 94 [38144/50048]	Loss: 0.0905
Training Epoch: 94 [38272/50048]	Loss: 0.1087
Training Epoch: 94 [38400/50048]	Loss: 0.1446
Training Epoch: 94 [38528/50048]	Loss: 0.0738
Training Epoch: 94 [38656/50048]	Loss: 0.0622
Training Epoch: 94 [38784/50048]	Loss: 0.1369
Training Epoch: 94 [38912/50048]	Loss: 0.1435
Training Epoch: 94 [39040/50048]	Loss: 0.0618
Training Epoch: 94 [39168/50048]	Loss: 0.0792
Training Epoch: 94 [39296/50048]	Loss: 0.0592
Training Epoch: 94 [39424/50048]	Loss: 0.1040
Training Epoch: 94 [39552/50048]	Loss: 0.0853
Training Epoch: 94 [39680/50048]	Loss: 0.1347
Training Epoch: 94 [39808/50048]	Loss: 0.0675
Training Epoch: 94 [39936/50048]	Loss: 0.1613
Training Epoch: 94 [40064/50048]	Loss: 0.0915
Training Epoch: 94 [40192/50048]	Loss: 0.0882
Training Epoch: 94 [40320/50048]	Loss: 0.1152
Training Epoch: 94 [40448/50048]	Loss: 0.1191
Training Epoch: 94 [40576/50048]	Loss: 0.0777
Training Epoch: 94 [40704/50048]	Loss: 0.0908
Training Epoch: 94 [40832/50048]	Loss: 0.1074
Training Epoch: 94 [40960/50048]	Loss: 0.0983
Training Epoch: 94 [41088/50048]	Loss: 0.1321
Training Epoch: 94 [41216/50048]	Loss: 0.1139
Training Epoch: 94 [41344/50048]	Loss: 0.0767
Training Epoch: 94 [41472/50048]	Loss: 0.0648
Training Epoch: 94 [41600/50048]	Loss: 0.1552
Training Epoch: 94 [41728/50048]	Loss: 0.1061
Training Epoch: 94 [41856/50048]	Loss: 0.0978
Training Epoch: 94 [41984/50048]	Loss: 0.1514
Training Epoch: 94 [42112/50048]	Loss: 0.1314
Training Epoch: 94 [42240/50048]	Loss: 0.0346
Training Epoch: 94 [42368/50048]	Loss: 0.0757
Training Epoch: 94 [42496/50048]	Loss: 0.0865
Training Epoch: 94 [42624/50048]	Loss: 0.0534
Training Epoch: 94 [42752/50048]	Loss: 0.0904
Training Epoch: 94 [42880/50048]	Loss: 0.1345
Training Epoch: 94 [43008/50048]	Loss: 0.1275
Training Epoch: 94 [43136/50048]	Loss: 0.1355
Training Epoch: 94 [43264/50048]	Loss: 0.1604
Training Epoch: 94 [43392/50048]	Loss: 0.0843
Training Epoch: 94 [43520/50048]	Loss: 0.1328
Training Epoch: 94 [43648/50048]	Loss: 0.0461
Training Epoch: 94 [43776/50048]	Loss: 0.0676
Training Epoch: 94 [43904/50048]	Loss: 0.2129
Training Epoch: 94 [44032/50048]	Loss: 0.0876
Training Epoch: 94 [44160/50048]	Loss: 0.1036
Training Epoch: 94 [44288/50048]	Loss: 0.0660
Training Epoch: 94 [44416/50048]	Loss: 0.0548
Training Epoch: 94 [44544/50048]	Loss: 0.0553
Training Epoch: 94 [44672/50048]	Loss: 0.1861
Training Epoch: 94 [44800/50048]	Loss: 0.1249
Training Epoch: 94 [44928/50048]	Loss: 0.0511
Training Epoch: 94 [45056/50048]	Loss: 0.1303
Training Epoch: 94 [45184/50048]	Loss: 0.1152
Training Epoch: 94 [45312/50048]	Loss: 0.0839
Training Epoch: 94 [45440/50048]	Loss: 0.0875
Training Epoch: 94 [45568/50048]	Loss: 0.1123
Training Epoch: 94 [45696/50048]	Loss: 0.0425
2022-12-06 08:30:41,533 [ZeusDataLoader(train)] train epoch 95 done: time=86.56 energy=10509.92
2022-12-06 08:30:41,535 [ZeusDataLoader(eval)] Epoch 95 begin.
Training Epoch: 94 [45824/50048]	Loss: 0.0521
Training Epoch: 94 [45952/50048]	Loss: 0.1204
Training Epoch: 94 [46080/50048]	Loss: 0.1291
Training Epoch: 94 [46208/50048]	Loss: 0.0601
Training Epoch: 94 [46336/50048]	Loss: 0.0707
Training Epoch: 94 [46464/50048]	Loss: 0.1782
Training Epoch: 94 [46592/50048]	Loss: 0.1057
Training Epoch: 94 [46720/50048]	Loss: 0.0649
Training Epoch: 94 [46848/50048]	Loss: 0.0470
Training Epoch: 94 [46976/50048]	Loss: 0.1121
Training Epoch: 94 [47104/50048]	Loss: 0.0321
Training Epoch: 94 [47232/50048]	Loss: 0.0573
Training Epoch: 94 [47360/50048]	Loss: 0.0880
Training Epoch: 94 [47488/50048]	Loss: 0.0579
Training Epoch: 94 [47616/50048]	Loss: 0.0769
Training Epoch: 94 [47744/50048]	Loss: 0.0854
Training Epoch: 94 [47872/50048]	Loss: 0.0754
Training Epoch: 94 [48000/50048]	Loss: 0.0809
Training Epoch: 94 [48128/50048]	Loss: 0.0771
Training Epoch: 94 [48256/50048]	Loss: 0.1339
Training Epoch: 94 [48384/50048]	Loss: 0.0315
Training Epoch: 94 [48512/50048]	Loss: 0.0676
Training Epoch: 94 [48640/50048]	Loss: 0.1151
Training Epoch: 94 [48768/50048]	Loss: 0.0756
Training Epoch: 94 [48896/50048]	Loss: 0.0601
Training Epoch: 94 [49024/50048]	Loss: 0.0897
Training Epoch: 94 [49152/50048]	Loss: 0.1077
Training Epoch: 94 [49280/50048]	Loss: 0.2771
Training Epoch: 94 [49408/50048]	Loss: 0.0500
Training Epoch: 94 [49536/50048]	Loss: 0.0671
Training Epoch: 94 [49664/50048]	Loss: 0.1456
Training Epoch: 94 [49792/50048]	Loss: 0.1377
Training Epoch: 94 [49920/50048]	Loss: 0.1205
Training Epoch: 94 [50048/50048]	Loss: 0.0324
2022-12-06 13:30:45.252 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 08:30:45,298 [ZeusDataLoader(eval)] eval epoch 95 done: time=3.75 energy=459.12
2022-12-06 08:30:45,298 [ZeusDataLoader(train)] Up to epoch 95: time=8569.55, energy=1040208.01, cost=1269939.21
2022-12-06 08:30:45,298 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 08:30:45,298 [ZeusDataLoader(train)] Expected next epoch: time=8659.34, energy=1051006.02, cost=1283195.59
2022-12-06 08:30:45,299 [ZeusDataLoader(train)] Epoch 96 begin.
Validation Epoch: 94, Average loss: 0.0184, Accuracy: 0.6425
2022-12-06 08:30:45,481 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:30:45,482 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:30:45.496 [ZeusMonitor] Monitor started.
2022-12-06 13:30:45.496 [ZeusMonitor] Running indefinitely. 2022-12-06 13:30:45.496 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:30:45.496 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e96+gpu0.power.log
Training Epoch: 95 [128/50048]	Loss: 0.0722
Training Epoch: 95 [256/50048]	Loss: 0.0713
Training Epoch: 95 [384/50048]	Loss: 0.0399
Training Epoch: 95 [512/50048]	Loss: 0.0332
Training Epoch: 95 [640/50048]	Loss: 0.0634
Training Epoch: 95 [768/50048]	Loss: 0.1003
Training Epoch: 95 [896/50048]	Loss: 0.1472
Training Epoch: 95 [1024/50048]	Loss: 0.1104
Training Epoch: 95 [1152/50048]	Loss: 0.0859
Training Epoch: 95 [1280/50048]	Loss: 0.1297
Training Epoch: 95 [1408/50048]	Loss: 0.0484
Training Epoch: 95 [1536/50048]	Loss: 0.0432
Training Epoch: 95 [1664/50048]	Loss: 0.0655
Training Epoch: 95 [1792/50048]	Loss: 0.0791
Training Epoch: 95 [1920/50048]	Loss: 0.0457
Training Epoch: 95 [2048/50048]	Loss: 0.0471
Training Epoch: 95 [2176/50048]	Loss: 0.0702
Training Epoch: 95 [2304/50048]	Loss: 0.0559
Training Epoch: 95 [2432/50048]	Loss: 0.0478
Training Epoch: 95 [2560/50048]	Loss: 0.1215
Training Epoch: 95 [2688/50048]	Loss: 0.0511
Training Epoch: 95 [2816/50048]	Loss: 0.0828
Training Epoch: 95 [2944/50048]	Loss: 0.0469
Training Epoch: 95 [3072/50048]	Loss: 0.1133
Training Epoch: 95 [3200/50048]	Loss: 0.0388
Training Epoch: 95 [3328/50048]	Loss: 0.0405
Training Epoch: 95 [3456/50048]	Loss: 0.0304
Training Epoch: 95 [3584/50048]	Loss: 0.1396
Training Epoch: 95 [3712/50048]	Loss: 0.1124
Training Epoch: 95 [3840/50048]	Loss: 0.1028
Training Epoch: 95 [3968/50048]	Loss: 0.1198
Training Epoch: 95 [4096/50048]	Loss: 0.0523
Training Epoch: 95 [4224/50048]	Loss: 0.0876
Training Epoch: 95 [4352/50048]	Loss: 0.0894
Training Epoch: 95 [4480/50048]	Loss: 0.0722
Training Epoch: 95 [4608/50048]	Loss: 0.1029
Training Epoch: 95 [4736/50048]	Loss: 0.0689
Training Epoch: 95 [4864/50048]	Loss: 0.1093
Training Epoch: 95 [4992/50048]	Loss: 0.0997
Training Epoch: 95 [5120/50048]	Loss: 0.1049
Training Epoch: 95 [5248/50048]	Loss: 0.0702
Training Epoch: 95 [5376/50048]	Loss: 0.1022
Training Epoch: 95 [5504/50048]	Loss: 0.0780
Training Epoch: 95 [5632/50048]	Loss: 0.0562
Training Epoch: 95 [5760/50048]	Loss: 0.2409
Training Epoch: 95 [5888/50048]	Loss: 0.1136
Training Epoch: 95 [6016/50048]	Loss: 0.1121
Training Epoch: 95 [6144/50048]	Loss: 0.0599
Training Epoch: 95 [6272/50048]	Loss: 0.0566
Training Epoch: 95 [6400/50048]	Loss: 0.0289
Training Epoch: 95 [6528/50048]	Loss: 0.1739
Training Epoch: 95 [6656/50048]	Loss: 0.0376
Training Epoch: 95 [6784/50048]	Loss: 0.0847
Training Epoch: 95 [6912/50048]	Loss: 0.0999
Training Epoch: 95 [7040/50048]	Loss: 0.0885
Training Epoch: 95 [7168/50048]	Loss: 0.0665
Training Epoch: 95 [7296/50048]	Loss: 0.0853
Training Epoch: 95 [7424/50048]	Loss: 0.0960
Training Epoch: 95 [7552/50048]	Loss: 0.0556
Training Epoch: 95 [7680/50048]	Loss: 0.0722
Training Epoch: 95 [7808/50048]	Loss: 0.0940
Training Epoch: 95 [7936/50048]	Loss: 0.0934
Training Epoch: 95 [8064/50048]	Loss: 0.1174
Training Epoch: 95 [8192/50048]	Loss: 0.0745
Training Epoch: 95 [8320/50048]	Loss: 0.1148
Training Epoch: 95 [8448/50048]	Loss: 0.0728
Training Epoch: 95 [8576/50048]	Loss: 0.0555
Training Epoch: 95 [8704/50048]	Loss: 0.0968
Training Epoch: 95 [8832/50048]	Loss: 0.0436
Training Epoch: 95 [8960/50048]	Loss: 0.1007
Training Epoch: 95 [9088/50048]	Loss: 0.1334
Training Epoch: 95 [9216/50048]	Loss: 0.0609
Training Epoch: 95 [9344/50048]	Loss: 0.0516
Training Epoch: 95 [9472/50048]	Loss: 0.0543
Training Epoch: 95 [9600/50048]	Loss: 0.1044
Training Epoch: 95 [9728/50048]	Loss: 0.0385
Training Epoch: 95 [9856/50048]	Loss: 0.1028
Training Epoch: 95 [9984/50048]	Loss: 0.0428
Training Epoch: 95 [10112/50048]	Loss: 0.0729
Training Epoch: 95 [10240/50048]	Loss: 0.0693
Training Epoch: 95 [10368/50048]	Loss: 0.1012
Training Epoch: 95 [10496/50048]	Loss: 0.0842
Training Epoch: 95 [10624/50048]	Loss: 0.0931
Training Epoch: 95 [10752/50048]	Loss: 0.0687
Training Epoch: 95 [10880/50048]	Loss: 0.0955
Training Epoch: 95 [11008/50048]	Loss: 0.0907
Training Epoch: 95 [11136/50048]	Loss: 0.0884
Training Epoch: 95 [11264/50048]	Loss: 0.0610
Training Epoch: 95 [11392/50048]	Loss: 0.0735
Training Epoch: 95 [11520/50048]	Loss: 0.0255
Training Epoch: 95 [11648/50048]	Loss: 0.1015
Training Epoch: 95 [11776/50048]	Loss: 0.0503
Training Epoch: 95 [11904/50048]	Loss: 0.0573
Training Epoch: 95 [12032/50048]	Loss: 0.0651
Training Epoch: 95 [12160/50048]	Loss: 0.1459
Training Epoch: 95 [12288/50048]	Loss: 0.0674
Training Epoch: 95 [12416/50048]	Loss: 0.0255
Training Epoch: 95 [12544/50048]	Loss: 0.0518
Training Epoch: 95 [12672/50048]	Loss: 0.0522
Training Epoch: 95 [12800/50048]	Loss: 0.0497
Training Epoch: 95 [12928/50048]	Loss: 0.0761
Training Epoch: 95 [13056/50048]	Loss: 0.0984
Training Epoch: 95 [13184/50048]	Loss: 0.1316
Training Epoch: 95 [13312/50048]	Loss: 0.0561
Training Epoch: 95 [13440/50048]	Loss: 0.0525
Training Epoch: 95 [13568/50048]	Loss: 0.0886
Training Epoch: 95 [13696/50048]	Loss: 0.0729
Training Epoch: 95 [13824/50048]	Loss: 0.0511
Training Epoch: 95 [13952/50048]	Loss: 0.1246
Training Epoch: 95 [14080/50048]	Loss: 0.0820
Training Epoch: 95 [14208/50048]	Loss: 0.0790
Training Epoch: 95 [14336/50048]	Loss: 0.0736
Training Epoch: 95 [14464/50048]	Loss: 0.1264
Training Epoch: 95 [14592/50048]	Loss: 0.0309
Training Epoch: 95 [14720/50048]	Loss: 0.1190
Training Epoch: 95 [14848/50048]	Loss: 0.0394
Training Epoch: 95 [14976/50048]	Loss: 0.0658
Training Epoch: 95 [15104/50048]	Loss: 0.1009
Training Epoch: 95 [15232/50048]	Loss: 0.0762
Training Epoch: 95 [15360/50048]	Loss: 0.0792
Training Epoch: 95 [15488/50048]	Loss: 0.1011
Training Epoch: 95 [15616/50048]	Loss: 0.1129
Training Epoch: 95 [15744/50048]	Loss: 0.0773
Training Epoch: 95 [15872/50048]	Loss: 0.0458
Training Epoch: 95 [16000/50048]	Loss: 0.0611
Training Epoch: 95 [16128/50048]	Loss: 0.0529
Training Epoch: 95 [16256/50048]	Loss: 0.0866
Training Epoch: 95 [16384/50048]	Loss: 0.0514
Training Epoch: 95 [16512/50048]	Loss: 0.1406
Training Epoch: 95 [16640/50048]	Loss: 0.0421
Training Epoch: 95 [16768/50048]	Loss: 0.0681
Training Epoch: 95 [16896/50048]	Loss: 0.0880
Training Epoch: 95 [17024/50048]	Loss: 0.2061
Training Epoch: 95 [17152/50048]	Loss: 0.0998
Training Epoch: 95 [17280/50048]	Loss: 0.1333
Training Epoch: 95 [17408/50048]	Loss: 0.1044
Training Epoch: 95 [17536/50048]	Loss: 0.0981
Training Epoch: 95 [17664/50048]	Loss: 0.0476
Training Epoch: 95 [17792/50048]	Loss: 0.0869
Training Epoch: 95 [17920/50048]	Loss: 0.0518
Training Epoch: 95 [18048/50048]	Loss: 0.0511
Training Epoch: 95 [18176/50048]	Loss: 0.0876
Training Epoch: 95 [18304/50048]	Loss: 0.1921
Training Epoch: 95 [18432/50048]	Loss: 0.1081
Training Epoch: 95 [18560/50048]	Loss: 0.1127
Training Epoch: 95 [18688/50048]	Loss: 0.0864
Training Epoch: 95 [18816/50048]	Loss: 0.1213
Training Epoch: 95 [18944/50048]	Loss: 0.0895
Training Epoch: 95 [19072/50048]	Loss: 0.1615
Training Epoch: 95 [19200/50048]	Loss: 0.0820
Training Epoch: 95 [19328/50048]	Loss: 0.1395
Training Epoch: 95 [19456/50048]	Loss: 0.0568
Training Epoch: 95 [19584/50048]	Loss: 0.1052
Training Epoch: 95 [19712/50048]	Loss: 0.0698
Training Epoch: 95 [19840/50048]	Loss: 0.1394
Training Epoch: 95 [19968/50048]	Loss: 0.0631
Training Epoch: 95 [20096/50048]	Loss: 0.0526
Training Epoch: 95 [20224/50048]	Loss: 0.0534
Training Epoch: 95 [20352/50048]	Loss: 0.0851
Training Epoch: 95 [20480/50048]	Loss: 0.0974
Training Epoch: 95 [20608/50048]	Loss: 0.0985
Training Epoch: 95 [20736/50048]	Loss: 0.0725
Training Epoch: 95 [20864/50048]	Loss: 0.0567
Training Epoch: 95 [20992/50048]	Loss: 0.1041
Training Epoch: 95 [21120/50048]	Loss: 0.0262
Training Epoch: 95 [21248/50048]	Loss: 0.0975
Training Epoch: 95 [21376/50048]	Loss: 0.0522
Training Epoch: 95 [21504/50048]	Loss: 0.0638
Training Epoch: 95 [21632/50048]	Loss: 0.0667
Training Epoch: 95 [21760/50048]	Loss: 0.0588
Training Epoch: 95 [21888/50048]	Loss: 0.0344
Training Epoch: 95 [22016/50048]	Loss: 0.0859
Training Epoch: 95 [22144/50048]	Loss: 0.1009
Training Epoch: 95 [22272/50048]	Loss: 0.0528
Training Epoch: 95 [22400/50048]	Loss: 0.0612
Training Epoch: 95 [22528/50048]	Loss: 0.0946
Training Epoch: 95 [22656/50048]	Loss: 0.0483
Training Epoch: 95 [22784/50048]	Loss: 0.0667
Training Epoch: 95 [22912/50048]	Loss: 0.0822
Training Epoch: 95 [23040/50048]	Loss: 0.0985
Training Epoch: 95 [23168/50048]	Loss: 0.0229
Training Epoch: 95 [23296/50048]	Loss: 0.1308
Training Epoch: 95 [23424/50048]	Loss: 0.0741
Training Epoch: 95 [23552/50048]	Loss: 0.0378
Training Epoch: 95 [23680/50048]	Loss: 0.0347
Training Epoch: 95 [23808/50048]	Loss: 0.0525
Training Epoch: 95 [23936/50048]	Loss: 0.0578
Training Epoch: 95 [24064/50048]	Loss: 0.0918
Training Epoch: 95 [24192/50048]	Loss: 0.1027
Training Epoch: 95 [24320/50048]	Loss: 0.0920
Training Epoch: 95 [24448/50048]	Loss: 0.0587
Training Epoch: 95 [24576/50048]	Loss: 0.0847
Training Epoch: 95 [24704/50048]	Loss: 0.1212
Training Epoch: 95 [24832/50048]	Loss: 0.0847
Training Epoch: 95 [24960/50048]	Loss: 0.0802
Training Epoch: 95 [25088/50048]	Loss: 0.0803
Training Epoch: 95 [25216/50048]	Loss: 0.0392
Training Epoch: 95 [25344/50048]	Loss: 0.0470
Training Epoch: 95 [25472/50048]	Loss: 0.1029
Training Epoch: 95 [25600/50048]	Loss: 0.0837
Training Epoch: 95 [25728/50048]	Loss: 0.0351
Training Epoch: 95 [25856/50048]	Loss: 0.1171
Training Epoch: 95 [25984/50048]	Loss: 0.0758
Training Epoch: 95 [26112/50048]	Loss: 0.0530
Training Epoch: 95 [26240/50048]	Loss: 0.0514
Training Epoch: 95 [26368/50048]	Loss: 0.1592
Training Epoch: 95 [26496/50048]	Loss: 0.1197
Training Epoch: 95 [26624/50048]	Loss: 0.1096
Training Epoch: 95 [26752/50048]	Loss: 0.1027
Training Epoch: 95 [26880/50048]	Loss: 0.0996
Training Epoch: 95 [27008/50048]	Loss: 0.0696
Training Epoch: 95 [27136/50048]	Loss: 0.0922
Training Epoch: 95 [27264/50048]	Loss: 0.0390
Training Epoch: 95 [27392/50048]	Loss: 0.0424
Training Epoch: 95 [27520/50048]	Loss: 0.0407
Training Epoch: 95 [27648/50048]	Loss: 0.1070
Training Epoch: 95 [27776/50048]	Loss: 0.0653
Training Epoch: 95 [27904/50048]	Loss: 0.0958
Training Epoch: 95 [28032/50048]	Loss: 0.0739
Training Epoch: 95 [28160/50048]	Loss: 0.1042
Training Epoch: 95 [28288/50048]	Loss: 0.0828
Training Epoch: 95 [28416/50048]	Loss: 0.0965
Training Epoch: 95 [28544/50048]	Loss: 0.0390
Training Epoch: 95 [28672/50048]	Loss: 0.0740
Training Epoch: 95 [28800/50048]	Loss: 0.0973
Training Epoch: 95 [28928/50048]	Loss: 0.0974
Training Epoch: 95 [29056/50048]	Loss: 0.1222
Training Epoch: 95 [29184/50048]	Loss: 0.1114
Training Epoch: 95 [29312/50048]	Loss: 0.0997
Training Epoch: 95 [29440/50048]	Loss: 0.1094
Training Epoch: 95 [29568/50048]	Loss: 0.0724
Training Epoch: 95 [29696/50048]	Loss: 0.0388
Training Epoch: 95 [29824/50048]	Loss: 0.1753
Training Epoch: 95 [29952/50048]	Loss: 0.0787
Training Epoch: 95 [30080/50048]	Loss: 0.1100
Training Epoch: 95 [30208/50048]	Loss: 0.0675
Training Epoch: 95 [30336/50048]	Loss: 0.1154
Training Epoch: 95 [30464/50048]	Loss: 0.0870
Training Epoch: 95 [30592/50048]	Loss: 0.0476
Training Epoch: 95 [30720/50048]	Loss: 0.1289
Training Epoch: 95 [30848/50048]	Loss: 0.0441
Training Epoch: 95 [30976/50048]	Loss: 0.1423
Training Epoch: 95 [31104/50048]	Loss: 0.0787
Training Epoch: 95 [31232/50048]	Loss: 0.0664
Training Epoch: 95 [31360/50048]	Loss: 0.1009
Training Epoch: 95 [31488/50048]	Loss: 0.0839
Training Epoch: 95 [31616/50048]	Loss: 0.0447
Training Epoch: 95 [31744/50048]	Loss: 0.0776
Training Epoch: 95 [31872/50048]	Loss: 0.1379
Training Epoch: 95 [32000/50048]	Loss: 0.0695
Training Epoch: 95 [32128/50048]	Loss: 0.1215
Training Epoch: 95 [32256/50048]	Loss: 0.0857
Training Epoch: 95 [32384/50048]	Loss: 0.0476
Training Epoch: 95 [32512/50048]	Loss: 0.1092
Training Epoch: 95 [32640/50048]	Loss: 0.0474
Training Epoch: 95 [32768/50048]	Loss: 0.1134
Training Epoch: 95 [32896/50048]	Loss: 0.0659
Training Epoch: 95 [33024/50048]	Loss: 0.0858
Training Epoch: 95 [33152/50048]	Loss: 0.0535
Training Epoch: 95 [33280/50048]	Loss: 0.1140
Training Epoch: 95 [33408/50048]	Loss: 0.1260
Training Epoch: 95 [33536/50048]	Loss: 0.1376
Training Epoch: 95 [33664/50048]	Loss: 0.1525
Training Epoch: 95 [33792/50048]	Loss: 0.0609
Training Epoch: 95 [33920/50048]	Loss: 0.1006
Training Epoch: 95 [34048/50048]	Loss: 0.0607
Training Epoch: 95 [34176/50048]	Loss: 0.0497
Training Epoch: 95 [34304/50048]	Loss: 0.0738
Training Epoch: 95 [34432/50048]	Loss: 0.0511
Training Epoch: 95 [34560/50048]	Loss: 0.0770
Training Epoch: 95 [34688/50048]	Loss: 0.0883
Training Epoch: 95 [34816/50048]	Loss: 0.0779
Training Epoch: 95 [34944/50048]	Loss: 0.1418
Training Epoch: 95 [35072/50048]	Loss: 0.0780
Training Epoch: 95 [35200/50048]	Loss: 0.0943
Training Epoch: 95 [35328/50048]	Loss: 0.0596
Training Epoch: 95 [35456/50048]	Loss: 0.0755
Training Epoch: 95 [35584/50048]	Loss: 0.1036
Training Epoch: 95 [35712/50048]	Loss: 0.1102
Training Epoch: 95 [35840/50048]	Loss: 0.1459
Training Epoch: 95 [35968/50048]	Loss: 0.0788
Training Epoch: 95 [36096/50048]	Loss: 0.0369
Training Epoch: 95 [36224/50048]	Loss: 0.1156
Training Epoch: 95 [36352/50048]	Loss: 0.0459
Training Epoch: 95 [36480/50048]	Loss: 0.0329
Training Epoch: 95 [36608/50048]	Loss: 0.0545
Training Epoch: 95 [36736/50048]	Loss: 0.1438
Training Epoch: 95 [36864/50048]	Loss: 0.0236
Training Epoch: 95 [36992/50048]	Loss: 0.0918
Training Epoch: 95 [37120/50048]	Loss: 0.0700
Training Epoch: 95 [37248/50048]	Loss: 0.0622
Training Epoch: 95 [37376/50048]	Loss: 0.0340
Training Epoch: 95 [37504/50048]	Loss: 0.1475
Training Epoch: 95 [37632/50048]	Loss: 0.0795
Training Epoch: 95 [37760/50048]	Loss: 0.0735
Training Epoch: 95 [37888/50048]	Loss: 0.1240
Training Epoch: 95 [38016/50048]	Loss: 0.1201
Training Epoch: 95 [38144/50048]	Loss: 0.1471
Training Epoch: 95 [38272/50048]	Loss: 0.0953
Training Epoch: 95 [38400/50048]	Loss: 0.0445
Training Epoch: 95 [38528/50048]	Loss: 0.0826
Training Epoch: 95 [38656/50048]	Loss: 0.0308
Training Epoch: 95 [38784/50048]	Loss: 0.1827
Training Epoch: 95 [38912/50048]	Loss: 0.1013
Training Epoch: 95 [39040/50048]	Loss: 0.0727
Training Epoch: 95 [39168/50048]	Loss: 0.1205
Training Epoch: 95 [39296/50048]	Loss: 0.0333
Training Epoch: 95 [39424/50048]	Loss: 0.0491
Training Epoch: 95 [39552/50048]	Loss: 0.0816
Training Epoch: 95 [39680/50048]	Loss: 0.1005
Training Epoch: 95 [39808/50048]	Loss: 0.1203
Training Epoch: 95 [39936/50048]	Loss: 0.0877
Training Epoch: 95 [40064/50048]	Loss: 0.1629
Training Epoch: 95 [40192/50048]	Loss: 0.0607
Training Epoch: 95 [40320/50048]	Loss: 0.0927
Training Epoch: 95 [40448/50048]	Loss: 0.0734
Training Epoch: 95 [40576/50048]	Loss: 0.0244
Training Epoch: 95 [40704/50048]	Loss: 0.0861
Training Epoch: 95 [40832/50048]	Loss: 0.1129
Training Epoch: 95 [40960/50048]	Loss: 0.0726
Training Epoch: 95 [41088/50048]	Loss: 0.2018
Training Epoch: 95 [41216/50048]	Loss: 0.1792
Training Epoch: 95 [41344/50048]	Loss: 0.0939
Training Epoch: 95 [41472/50048]	Loss: 0.1083
Training Epoch: 95 [41600/50048]	Loss: 0.1012
Training Epoch: 95 [41728/50048]	Loss: 0.0764
Training Epoch: 95 [41856/50048]	Loss: 0.1873
Training Epoch: 95 [41984/50048]	Loss: 0.0912
Training Epoch: 95 [42112/50048]	Loss: 0.1211
Training Epoch: 95 [42240/50048]	Loss: 0.0720
Training Epoch: 95 [42368/50048]	Loss: 0.1263
Training Epoch: 95 [42496/50048]	Loss: 0.0702
Training Epoch: 95 [42624/50048]	Loss: 0.0662
Training Epoch: 95 [42752/50048]	Loss: 0.1440
Training Epoch: 95 [42880/50048]	Loss: 0.1123
Training Epoch: 95 [43008/50048]	Loss: 0.0613
Training Epoch: 95 [43136/50048]	Loss: 0.1892
Training Epoch: 95 [43264/50048]	Loss: 0.0895
Training Epoch: 95 [43392/50048]	Loss: 0.0816
Training Epoch: 95 [43520/50048]	Loss: 0.0969
Training Epoch: 95 [43648/50048]	Loss: 0.0557
Training Epoch: 95 [43776/50048]	Loss: 0.0563
Training Epoch: 95 [43904/50048]	Loss: 0.2451
Training Epoch: 95 [44032/50048]	Loss: 0.0488
Training Epoch: 95 [44160/50048]	Loss: 0.0582
Training Epoch: 95 [44288/50048]	Loss: 0.0669
Training Epoch: 95 [44416/50048]	Loss: 0.0390
Training Epoch: 95 [44544/50048]	Loss: 0.1277
Training Epoch: 95 [44672/50048]	Loss: 0.0753
Training Epoch: 95 [44800/50048]	Loss: 0.0920
Training Epoch: 95 [44928/50048]	Loss: 0.1532
Training Epoch: 95 [45056/50048]	Loss: 0.0769
Training Epoch: 95 [45184/50048]	Loss: 0.0422
Training Epoch: 95 [45312/50048]	Loss: 0.0429
Training Epoch: 95 [45440/50048]	Loss: 0.1203
Training Epoch: 95 [45568/50048]	Loss: 0.1950
Training Epoch: 95 [45696/50048]	Loss: 0.1814
2022-12-06 08:32:12,280 [ZeusDataLoader(train)] train epoch 96 done: time=86.97 energy=10525.25
2022-12-06 08:32:12,281 [ZeusDataLoader(eval)] Epoch 96 begin.
Training Epoch: 95 [45824/50048]	Loss: 0.1254
Training Epoch: 95 [45952/50048]	Loss: 0.0748
Training Epoch: 95 [46080/50048]	Loss: 0.1061
Training Epoch: 95 [46208/50048]	Loss: 0.0505
Training Epoch: 95 [46336/50048]	Loss: 0.0662
Training Epoch: 95 [46464/50048]	Loss: 0.0544
Training Epoch: 95 [46592/50048]	Loss: 0.0720
Training Epoch: 95 [46720/50048]	Loss: 0.0700
Training Epoch: 95 [46848/50048]	Loss: 0.1150
Training Epoch: 95 [46976/50048]	Loss: 0.1923
Training Epoch: 95 [47104/50048]	Loss: 0.1175
Training Epoch: 95 [47232/50048]	Loss: 0.0424
Training Epoch: 95 [47360/50048]	Loss: 0.0533
Training Epoch: 95 [47488/50048]	Loss: 0.0895
Training Epoch: 95 [47616/50048]	Loss: 0.0844
Training Epoch: 95 [47744/50048]	Loss: 0.0995
Training Epoch: 95 [47872/50048]	Loss: 0.0987
Training Epoch: 95 [48000/50048]	Loss: 0.1010
Training Epoch: 95 [48128/50048]	Loss: 0.2139
Training Epoch: 95 [48256/50048]	Loss: 0.0584
Training Epoch: 95 [48384/50048]	Loss: 0.0677
Training Epoch: 95 [48512/50048]	Loss: 0.0641
Training Epoch: 95 [48640/50048]	Loss: 0.0782
Training Epoch: 95 [48768/50048]	Loss: 0.0410
Training Epoch: 95 [48896/50048]	Loss: 0.0881
Training Epoch: 95 [49024/50048]	Loss: 0.0663
Training Epoch: 95 [49152/50048]	Loss: 0.0865
Training Epoch: 95 [49280/50048]	Loss: 0.1190
Training Epoch: 95 [49408/50048]	Loss: 0.0363
Training Epoch: 95 [49536/50048]	Loss: 0.0850
Training Epoch: 95 [49664/50048]	Loss: 0.0824
Training Epoch: 95 [49792/50048]	Loss: 0.0840
Training Epoch: 95 [49920/50048]	Loss: 0.0486
Training Epoch: 95 [50048/50048]	Loss: 0.0725
2022-12-06 13:32:15.958 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 08:32:15,986 [ZeusDataLoader(eval)] eval epoch 96 done: time=3.70 energy=453.73
2022-12-06 08:32:15,986 [ZeusDataLoader(train)] Up to epoch 96: time=8660.21, energy=1051186.99, cost=1283361.99
2022-12-06 08:32:15,986 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 08:32:15,987 [ZeusDataLoader(train)] Expected next epoch: time=8750.01, energy=1061985.00, cost=1296618.38
2022-12-06 08:32:15,988 [ZeusDataLoader(train)] Epoch 97 begin.
Validation Epoch: 95, Average loss: 0.0186, Accuracy: 0.6486
2022-12-06 08:32:16,174 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:32:16,175 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:32:16.179 [ZeusMonitor] Monitor started.
2022-12-06 13:32:16.179 [ZeusMonitor] Running indefinitely. 2022-12-06 13:32:16.179 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:32:16.179 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e97+gpu0.power.log
Training Epoch: 96 [128/50048]	Loss: 0.1018
Training Epoch: 96 [256/50048]	Loss: 0.1203
Training Epoch: 96 [384/50048]	Loss: 0.0939
Training Epoch: 96 [512/50048]	Loss: 0.0623
Training Epoch: 96 [640/50048]	Loss: 0.0848
Training Epoch: 96 [768/50048]	Loss: 0.0691
Training Epoch: 96 [896/50048]	Loss: 0.0870
Training Epoch: 96 [1024/50048]	Loss: 0.0909
Training Epoch: 96 [1152/50048]	Loss: 0.1016
Training Epoch: 96 [1280/50048]	Loss: 0.0859
Training Epoch: 96 [1408/50048]	Loss: 0.0606
Training Epoch: 96 [1536/50048]	Loss: 0.1614
Training Epoch: 96 [1664/50048]	Loss: 0.0697
Training Epoch: 96 [1792/50048]	Loss: 0.0672
Training Epoch: 96 [1920/50048]	Loss: 0.0488
Training Epoch: 96 [2048/50048]	Loss: 0.1067
Training Epoch: 96 [2176/50048]	Loss: 0.0755
Training Epoch: 96 [2304/50048]	Loss: 0.0381
Training Epoch: 96 [2432/50048]	Loss: 0.0643
Training Epoch: 96 [2560/50048]	Loss: 0.0661
Training Epoch: 96 [2688/50048]	Loss: 0.0744
Training Epoch: 96 [2816/50048]	Loss: 0.0407
Training Epoch: 96 [2944/50048]	Loss: 0.0989
Training Epoch: 96 [3072/50048]	Loss: 0.0519
Training Epoch: 96 [3200/50048]	Loss: 0.0978
Training Epoch: 96 [3328/50048]	Loss: 0.0934
Training Epoch: 96 [3456/50048]	Loss: 0.1292
Training Epoch: 96 [3584/50048]	Loss: 0.0342
Training Epoch: 96 [3712/50048]	Loss: 0.0923
Training Epoch: 96 [3840/50048]	Loss: 0.0608
Training Epoch: 96 [3968/50048]	Loss: 0.1234
Training Epoch: 96 [4096/50048]	Loss: 0.0864
Training Epoch: 96 [4224/50048]	Loss: 0.0918
Training Epoch: 96 [4352/50048]	Loss: 0.0705
Training Epoch: 96 [4480/50048]	Loss: 0.1024
Training Epoch: 96 [4608/50048]	Loss: 0.0577
Training Epoch: 96 [4736/50048]	Loss: 0.0704
Training Epoch: 96 [4864/50048]	Loss: 0.1146
Training Epoch: 96 [4992/50048]	Loss: 0.0653
Training Epoch: 96 [5120/50048]	Loss: 0.0718
Training Epoch: 96 [5248/50048]	Loss: 0.0735
Training Epoch: 96 [5376/50048]	Loss: 0.0430
Training Epoch: 96 [5504/50048]	Loss: 0.0830
Training Epoch: 96 [5632/50048]	Loss: 0.0752
Training Epoch: 96 [5760/50048]	Loss: 0.1127
Training Epoch: 96 [5888/50048]	Loss: 0.0857
Training Epoch: 96 [6016/50048]	Loss: 0.1123
Training Epoch: 96 [6144/50048]	Loss: 0.0645
Training Epoch: 96 [6272/50048]	Loss: 0.0791
Training Epoch: 96 [6400/50048]	Loss: 0.0284
Training Epoch: 96 [6528/50048]	Loss: 0.0438
Training Epoch: 96 [6656/50048]	Loss: 0.0907
Training Epoch: 96 [6784/50048]	Loss: 0.1109
Training Epoch: 96 [6912/50048]	Loss: 0.0266
Training Epoch: 96 [7040/50048]	Loss: 0.0671
Training Epoch: 96 [7168/50048]	Loss: 0.0945
Training Epoch: 96 [7296/50048]	Loss: 0.0570
Training Epoch: 96 [7424/50048]	Loss: 0.1282
Training Epoch: 96 [7552/50048]	Loss: 0.0502
Training Epoch: 96 [7680/50048]	Loss: 0.0990
Training Epoch: 96 [7808/50048]	Loss: 0.0632
Training Epoch: 96 [7936/50048]	Loss: 0.0721
Training Epoch: 96 [8064/50048]	Loss: 0.0588
Training Epoch: 96 [8192/50048]	Loss: 0.0898
Training Epoch: 96 [8320/50048]	Loss: 0.0348
Training Epoch: 96 [8448/50048]	Loss: 0.1038
Training Epoch: 96 [8576/50048]	Loss: 0.0515
Training Epoch: 96 [8704/50048]	Loss: 0.0897
Training Epoch: 96 [8832/50048]	Loss: 0.1789
Training Epoch: 96 [8960/50048]	Loss: 0.0406
Training Epoch: 96 [9088/50048]	Loss: 0.0194
Training Epoch: 96 [9216/50048]	Loss: 0.0718
Training Epoch: 96 [9344/50048]	Loss: 0.0769
Training Epoch: 96 [9472/50048]	Loss: 0.0639
Training Epoch: 96 [9600/50048]	Loss: 0.1560
Training Epoch: 96 [9728/50048]	Loss: 0.1303
Training Epoch: 96 [9856/50048]	Loss: 0.0575
Training Epoch: 96 [9984/50048]	Loss: 0.0554
Training Epoch: 96 [10112/50048]	Loss: 0.0890
Training Epoch: 96 [10240/50048]	Loss: 0.0818
Training Epoch: 96 [10368/50048]	Loss: 0.0986
Training Epoch: 96 [10496/50048]	Loss: 0.0723
Training Epoch: 96 [10624/50048]	Loss: 0.0320
Training Epoch: 96 [10752/50048]	Loss: 0.1229
Training Epoch: 96 [10880/50048]	Loss: 0.0620
Training Epoch: 96 [11008/50048]	Loss: 0.0506
Training Epoch: 96 [11136/50048]	Loss: 0.0845
Training Epoch: 96 [11264/50048]	Loss: 0.0792
Training Epoch: 96 [11392/50048]	Loss: 0.0769
Training Epoch: 96 [11520/50048]	Loss: 0.0438
Training Epoch: 96 [11648/50048]	Loss: 0.0959
Training Epoch: 96 [11776/50048]	Loss: 0.0660
Training Epoch: 96 [11904/50048]	Loss: 0.0948
Training Epoch: 96 [12032/50048]	Loss: 0.0697
Training Epoch: 96 [12160/50048]	Loss: 0.0601
Training Epoch: 96 [12288/50048]	Loss: 0.1097
Training Epoch: 96 [12416/50048]	Loss: 0.1147
Training Epoch: 96 [12544/50048]	Loss: 0.0479
Training Epoch: 96 [12672/50048]	Loss: 0.0314
Training Epoch: 96 [12800/50048]	Loss: 0.0790
Training Epoch: 96 [12928/50048]	Loss: 0.1716
Training Epoch: 96 [13056/50048]	Loss: 0.0961
Training Epoch: 96 [13184/50048]	Loss: 0.0658
Training Epoch: 96 [13312/50048]	Loss: 0.0938
Training Epoch: 96 [13440/50048]	Loss: 0.1362
Training Epoch: 96 [13568/50048]	Loss: 0.0335
Training Epoch: 96 [13696/50048]	Loss: 0.0862
Training Epoch: 96 [13824/50048]	Loss: 0.0943
Training Epoch: 96 [13952/50048]	Loss: 0.1211
Training Epoch: 96 [14080/50048]	Loss: 0.0489
Training Epoch: 96 [14208/50048]	Loss: 0.0649
Training Epoch: 96 [14336/50048]	Loss: 0.0674
Training Epoch: 96 [14464/50048]	Loss: 0.0721
Training Epoch: 96 [14592/50048]	Loss: 0.0935
Training Epoch: 96 [14720/50048]	Loss: 0.0284
Training Epoch: 96 [14848/50048]	Loss: 0.1038
Training Epoch: 96 [14976/50048]	Loss: 0.0890
Training Epoch: 96 [15104/50048]	Loss: 0.1120
Training Epoch: 96 [15232/50048]	Loss: 0.0669
Training Epoch: 96 [15360/50048]	Loss: 0.0798
Training Epoch: 96 [15488/50048]	Loss: 0.0547
Training Epoch: 96 [15616/50048]	Loss: 0.0506
Training Epoch: 96 [15744/50048]	Loss: 0.0890
Training Epoch: 96 [15872/50048]	Loss: 0.0518
Training Epoch: 96 [16000/50048]	Loss: 0.1894
Training Epoch: 96 [16128/50048]	Loss: 0.0580
Training Epoch: 96 [16256/50048]	Loss: 0.0984
Training Epoch: 96 [16384/50048]	Loss: 0.0648
Training Epoch: 96 [16512/50048]	Loss: 0.0966
Training Epoch: 96 [16640/50048]	Loss: 0.0467
Training Epoch: 96 [16768/50048]	Loss: 0.0422
Training Epoch: 96 [16896/50048]	Loss: 0.0817
Training Epoch: 96 [17024/50048]	Loss: 0.1272
Training Epoch: 96 [17152/50048]	Loss: 0.1131
Training Epoch: 96 [17280/50048]	Loss: 0.0638
Training Epoch: 96 [17408/50048]	Loss: 0.1660
Training Epoch: 96 [17536/50048]	Loss: 0.0451
Training Epoch: 96 [17664/50048]	Loss: 0.0653
Training Epoch: 96 [17792/50048]	Loss: 0.0759
Training Epoch: 96 [17920/50048]	Loss: 0.0611
Training Epoch: 96 [18048/50048]	Loss: 0.1078
Training Epoch: 96 [18176/50048]	Loss: 0.0334
Training Epoch: 96 [18304/50048]	Loss: 0.1197
Training Epoch: 96 [18432/50048]	Loss: 0.1451
Training Epoch: 96 [18560/50048]	Loss: 0.0404
Training Epoch: 96 [18688/50048]	Loss: 0.0832
Training Epoch: 96 [18816/50048]	Loss: 0.1247
Training Epoch: 96 [18944/50048]	Loss: 0.0797
Training Epoch: 96 [19072/50048]	Loss: 0.0826
Training Epoch: 96 [19200/50048]	Loss: 0.1012
Training Epoch: 96 [19328/50048]	Loss: 0.0582
Training Epoch: 96 [19456/50048]	Loss: 0.0796
Training Epoch: 96 [19584/50048]	Loss: 0.0430
Training Epoch: 96 [19712/50048]	Loss: 0.0475
Training Epoch: 96 [19840/50048]	Loss: 0.0539
Training Epoch: 96 [19968/50048]	Loss: 0.0753
Training Epoch: 96 [20096/50048]	Loss: 0.0684
Training Epoch: 96 [20224/50048]	Loss: 0.0532
Training Epoch: 96 [20352/50048]	Loss: 0.1080
Training Epoch: 96 [20480/50048]	Loss: 0.0677
Training Epoch: 96 [20608/50048]	Loss: 0.0411
Training Epoch: 96 [20736/50048]	Loss: 0.0500
Training Epoch: 96 [20864/50048]	Loss: 0.0982
Training Epoch: 96 [20992/50048]	Loss: 0.0600
Training Epoch: 96 [21120/50048]	Loss: 0.0848
Training Epoch: 96 [21248/50048]	Loss: 0.1181
Training Epoch: 96 [21376/50048]	Loss: 0.0469
Training Epoch: 96 [21504/50048]	Loss: 0.0973
Training Epoch: 96 [21632/50048]	Loss: 0.0687
Training Epoch: 96 [21760/50048]	Loss: 0.0597
Training Epoch: 96 [21888/50048]	Loss: 0.0160
Training Epoch: 96 [22016/50048]	Loss: 0.0941
Training Epoch: 96 [22144/50048]	Loss: 0.0325
Training Epoch: 96 [22272/50048]	Loss: 0.1083
Training Epoch: 96 [22400/50048]	Loss: 0.0839
Training Epoch: 96 [22528/50048]	Loss: 0.0317
Training Epoch: 96 [22656/50048]	Loss: 0.0825
Training Epoch: 96 [22784/50048]	Loss: 0.1041
Training Epoch: 96 [22912/50048]	Loss: 0.0366
Training Epoch: 96 [23040/50048]	Loss: 0.1155
Training Epoch: 96 [23168/50048]	Loss: 0.0469
Training Epoch: 96 [23296/50048]	Loss: 0.0669
Training Epoch: 96 [23424/50048]	Loss: 0.0588
Training Epoch: 96 [23552/50048]	Loss: 0.0946
Training Epoch: 96 [23680/50048]	Loss: 0.0409
Training Epoch: 96 [23808/50048]	Loss: 0.0328
Training Epoch: 96 [23936/50048]	Loss: 0.0293
Training Epoch: 96 [24064/50048]	Loss: 0.0627
Training Epoch: 96 [24192/50048]	Loss: 0.0690
Training Epoch: 96 [24320/50048]	Loss: 0.0500
Training Epoch: 96 [24448/50048]	Loss: 0.0495
Training Epoch: 96 [24576/50048]	Loss: 0.0556
Training Epoch: 96 [24704/50048]	Loss: 0.0944
Training Epoch: 96 [24832/50048]	Loss: 0.0991
Training Epoch: 96 [24960/50048]	Loss: 0.1361
Training Epoch: 96 [25088/50048]	Loss: 0.1086
Training Epoch: 96 [25216/50048]	Loss: 0.1214
Training Epoch: 96 [25344/50048]	Loss: 0.0556
Training Epoch: 96 [25472/50048]	Loss: 0.0933
Training Epoch: 96 [25600/50048]	Loss: 0.0707
Training Epoch: 96 [25728/50048]	Loss: 0.0526
Training Epoch: 96 [25856/50048]	Loss: 0.1566
Training Epoch: 96 [25984/50048]	Loss: 0.1014
Training Epoch: 96 [26112/50048]	Loss: 0.0640
Training Epoch: 96 [26240/50048]	Loss: 0.0713
Training Epoch: 96 [26368/50048]	Loss: 0.0665
Training Epoch: 96 [26496/50048]	Loss: 0.0323
Training Epoch: 96 [26624/50048]	Loss: 0.0712
Training Epoch: 96 [26752/50048]	Loss: 0.0880
Training Epoch: 96 [26880/50048]	Loss: 0.0928
Training Epoch: 96 [27008/50048]	Loss: 0.0932
Training Epoch: 96 [27136/50048]	Loss: 0.0694
Training Epoch: 96 [27264/50048]	Loss: 0.0676
Training Epoch: 96 [27392/50048]	Loss: 0.0623
Training Epoch: 96 [27520/50048]	Loss: 0.1308
Training Epoch: 96 [27648/50048]	Loss: 0.1810
Training Epoch: 96 [27776/50048]	Loss: 0.0644
Training Epoch: 96 [27904/50048]	Loss: 0.1133
Training Epoch: 96 [28032/50048]	Loss: 0.1003
Training Epoch: 96 [28160/50048]	Loss: 0.0703
Training Epoch: 96 [28288/50048]	Loss: 0.0626
Training Epoch: 96 [28416/50048]	Loss: 0.0808
Training Epoch: 96 [28544/50048]	Loss: 0.0785
Training Epoch: 96 [28672/50048]	Loss: 0.0476
Training Epoch: 96 [28800/50048]	Loss: 0.0533
Training Epoch: 96 [28928/50048]	Loss: 0.0940
Training Epoch: 96 [29056/50048]	Loss: 0.0868
Training Epoch: 96 [29184/50048]	Loss: 0.0913
Training Epoch: 96 [29312/50048]	Loss: 0.0688
Training Epoch: 96 [29440/50048]	Loss: 0.0318
Training Epoch: 96 [29568/50048]	Loss: 0.1172
Training Epoch: 96 [29696/50048]	Loss: 0.1223
Training Epoch: 96 [29824/50048]	Loss: 0.0363
Training Epoch: 96 [29952/50048]	Loss: 0.0313
Training Epoch: 96 [30080/50048]	Loss: 0.1078
Training Epoch: 96 [30208/50048]	Loss: 0.0748
Training Epoch: 96 [30336/50048]	Loss: 0.0571
Training Epoch: 96 [30464/50048]	Loss: 0.0479
Training Epoch: 96 [30592/50048]	Loss: 0.0861
Training Epoch: 96 [30720/50048]	Loss: 0.1363
Training Epoch: 96 [30848/50048]	Loss: 0.0475
Training Epoch: 96 [30976/50048]	Loss: 0.0466
Training Epoch: 96 [31104/50048]	Loss: 0.1100
Training Epoch: 96 [31232/50048]	Loss: 0.0888
Training Epoch: 96 [31360/50048]	Loss: 0.1174
Training Epoch: 96 [31488/50048]	Loss: 0.0848
Training Epoch: 96 [31616/50048]	Loss: 0.1502
Training Epoch: 96 [31744/50048]	Loss: 0.0779
Training Epoch: 96 [31872/50048]	Loss: 0.0776
Training Epoch: 96 [32000/50048]	Loss: 0.0724
Training Epoch: 96 [32128/50048]	Loss: 0.0830
Training Epoch: 96 [32256/50048]	Loss: 0.0862
Training Epoch: 96 [32384/50048]	Loss: 0.0694
Training Epoch: 96 [32512/50048]	Loss: 0.0785
Training Epoch: 96 [32640/50048]	Loss: 0.1795
Training Epoch: 96 [32768/50048]	Loss: 0.1003
Training Epoch: 96 [32896/50048]	Loss: 0.0632
Training Epoch: 96 [33024/50048]	Loss: 0.0952
Training Epoch: 96 [33152/50048]	Loss: 0.0994
Training Epoch: 96 [33280/50048]	Loss: 0.0880
Training Epoch: 96 [33408/50048]	Loss: 0.0980
Training Epoch: 96 [33536/50048]	Loss: 0.1207
Training Epoch: 96 [33664/50048]	Loss: 0.0505
Training Epoch: 96 [33792/50048]	Loss: 0.0449
Training Epoch: 96 [33920/50048]	Loss: 0.0664
Training Epoch: 96 [34048/50048]	Loss: 0.0418
Training Epoch: 96 [34176/50048]	Loss: 0.0791
Training Epoch: 96 [34304/50048]	Loss: 0.0586
Training Epoch: 96 [34432/50048]	Loss: 0.0981
Training Epoch: 96 [34560/50048]	Loss: 0.0562
Training Epoch: 96 [34688/50048]	Loss: 0.1046
Training Epoch: 96 [34816/50048]	Loss: 0.0848
Training Epoch: 96 [34944/50048]	Loss: 0.0987
Training Epoch: 96 [35072/50048]	Loss: 0.1313
Training Epoch: 96 [35200/50048]	Loss: 0.0939
Training Epoch: 96 [35328/50048]	Loss: 0.1457
Training Epoch: 96 [35456/50048]	Loss: 0.0434
Training Epoch: 96 [35584/50048]	Loss: 0.0900
Training Epoch: 96 [35712/50048]	Loss: 0.0863
Training Epoch: 96 [35840/50048]	Loss: 0.0995
Training Epoch: 96 [35968/50048]	Loss: 0.0340
Training Epoch: 96 [36096/50048]	Loss: 0.1174
Training Epoch: 96 [36224/50048]	Loss: 0.0479
Training Epoch: 96 [36352/50048]	Loss: 0.1034
Training Epoch: 96 [36480/50048]	Loss: 0.1234
Training Epoch: 96 [36608/50048]	Loss: 0.0946
Training Epoch: 96 [36736/50048]	Loss: 0.0680
Training Epoch: 96 [36864/50048]	Loss: 0.1003
Training Epoch: 96 [36992/50048]	Loss: 0.1251
Training Epoch: 96 [37120/50048]	Loss: 0.1155
Training Epoch: 96 [37248/50048]	Loss: 0.1221
Training Epoch: 96 [37376/50048]	Loss: 0.0956
Training Epoch: 96 [37504/50048]	Loss: 0.0283
Training Epoch: 96 [37632/50048]	Loss: 0.0770
Training Epoch: 96 [37760/50048]	Loss: 0.0330
Training Epoch: 96 [37888/50048]	Loss: 0.0287
Training Epoch: 96 [38016/50048]	Loss: 0.0634
Training Epoch: 96 [38144/50048]	Loss: 0.0773
Training Epoch: 96 [38272/50048]	Loss: 0.0443
Training Epoch: 96 [38400/50048]	Loss: 0.0981
Training Epoch: 96 [38528/50048]	Loss: 0.0963
Training Epoch: 96 [38656/50048]	Loss: 0.1122
Training Epoch: 96 [38784/50048]	Loss: 0.1182
Training Epoch: 96 [38912/50048]	Loss: 0.1238
Training Epoch: 96 [39040/50048]	Loss: 0.0378
Training Epoch: 96 [39168/50048]	Loss: 0.0167
Training Epoch: 96 [39296/50048]	Loss: 0.0770
Training Epoch: 96 [39424/50048]	Loss: 0.0965
Training Epoch: 96 [39552/50048]	Loss: 0.1088
Training Epoch: 96 [39680/50048]	Loss: 0.0495
Training Epoch: 96 [39808/50048]	Loss: 0.0602
Training Epoch: 96 [39936/50048]	Loss: 0.1094
Training Epoch: 96 [40064/50048]	Loss: 0.1536
Training Epoch: 96 [40192/50048]	Loss: 0.0419
Training Epoch: 96 [40320/50048]	Loss: 0.1642
Training Epoch: 96 [40448/50048]	Loss: 0.0661
Training Epoch: 96 [40576/50048]	Loss: 0.0582
Training Epoch: 96 [40704/50048]	Loss: 0.0800
Training Epoch: 96 [40832/50048]	Loss: 0.1168
Training Epoch: 96 [40960/50048]	Loss: 0.0230
Training Epoch: 96 [41088/50048]	Loss: 0.0645
Training Epoch: 96 [41216/50048]	Loss: 0.0650
Training Epoch: 96 [41344/50048]	Loss: 0.0697
Training Epoch: 96 [41472/50048]	Loss: 0.0433
Training Epoch: 96 [41600/50048]	Loss: 0.0591
Training Epoch: 96 [41728/50048]	Loss: 0.0985
Training Epoch: 96 [41856/50048]	Loss: 0.1209
Training Epoch: 96 [41984/50048]	Loss: 0.1076
Training Epoch: 96 [42112/50048]	Loss: 0.1649
Training Epoch: 96 [42240/50048]	Loss: 0.0335
Training Epoch: 96 [42368/50048]	Loss: 0.0923
Training Epoch: 96 [42496/50048]	Loss: 0.0426
Training Epoch: 96 [42624/50048]	Loss: 0.0494
Training Epoch: 96 [42752/50048]	Loss: 0.2087
Training Epoch: 96 [42880/50048]	Loss: 0.1233
Training Epoch: 96 [43008/50048]	Loss: 0.0660
Training Epoch: 96 [43136/50048]	Loss: 0.0923
Training Epoch: 96 [43264/50048]	Loss: 0.0495
Training Epoch: 96 [43392/50048]	Loss: 0.0728
Training Epoch: 96 [43520/50048]	Loss: 0.0559
Training Epoch: 96 [43648/50048]	Loss: 0.0238
Training Epoch: 96 [43776/50048]	Loss: 0.0816
Training Epoch: 96 [43904/50048]	Loss: 0.0522
Training Epoch: 96 [44032/50048]	Loss: 0.0727
Training Epoch: 96 [44160/50048]	Loss: 0.1061
Training Epoch: 96 [44288/50048]	Loss: 0.0558
Training Epoch: 96 [44416/50048]	Loss: 0.0753
Training Epoch: 96 [44544/50048]	Loss: 0.0966
Training Epoch: 96 [44672/50048]	Loss: 0.0378
Training Epoch: 96 [44800/50048]	Loss: 0.0570
Training Epoch: 96 [44928/50048]	Loss: 0.1144
Training Epoch: 96 [45056/50048]	Loss: 0.0725
Training Epoch: 96 [45184/50048]	Loss: 0.1242
Training Epoch: 96 [45312/50048]	Loss: 0.0656
Training Epoch: 96 [45440/50048]	Loss: 0.1207
Training Epoch: 96 [45568/50048]	Loss: 0.0493
Training Epoch: 96 [45696/50048]	Loss: 0.0778
2022-12-06 08:33:42,502 [ZeusDataLoader(train)] train epoch 97 done: time=86.50 energy=10495.30
2022-12-06 08:33:42,503 [ZeusDataLoader(eval)] Epoch 97 begin.
Training Epoch: 96 [45824/50048]	Loss: 0.0390
Training Epoch: 96 [45952/50048]	Loss: 0.1077
Training Epoch: 96 [46080/50048]	Loss: 0.2032
Training Epoch: 96 [46208/50048]	Loss: 0.1300
Training Epoch: 96 [46336/50048]	Loss: 0.0651
Training Epoch: 96 [46464/50048]	Loss: 0.0879
Training Epoch: 96 [46592/50048]	Loss: 0.0629
Training Epoch: 96 [46720/50048]	Loss: 0.1469
Training Epoch: 96 [46848/50048]	Loss: 0.0679
Training Epoch: 96 [46976/50048]	Loss: 0.1077
Training Epoch: 96 [47104/50048]	Loss: 0.0616
Training Epoch: 96 [47232/50048]	Loss: 0.0732
Training Epoch: 96 [47360/50048]	Loss: 0.0965
Training Epoch: 96 [47488/50048]	Loss: 0.0477
Training Epoch: 96 [47616/50048]	Loss: 0.0946
Training Epoch: 96 [47744/50048]	Loss: 0.0281
Training Epoch: 96 [47872/50048]	Loss: 0.1096
Training Epoch: 96 [48000/50048]	Loss: 0.1251
Training Epoch: 96 [48128/50048]	Loss: 0.0764
Training Epoch: 96 [48256/50048]	Loss: 0.1013
Training Epoch: 96 [48384/50048]	Loss: 0.0817
Training Epoch: 96 [48512/50048]	Loss: 0.1173
Training Epoch: 96 [48640/50048]	Loss: 0.0677
Training Epoch: 96 [48768/50048]	Loss: 0.0742
Training Epoch: 96 [48896/50048]	Loss: 0.0354
Training Epoch: 96 [49024/50048]	Loss: 0.0775
Training Epoch: 96 [49152/50048]	Loss: 0.0609
Training Epoch: 96 [49280/50048]	Loss: 0.0879
Training Epoch: 96 [49408/50048]	Loss: 0.0509
Training Epoch: 96 [49536/50048]	Loss: 0.1153
Training Epoch: 96 [49664/50048]	Loss: 0.1603
Training Epoch: 96 [49792/50048]	Loss: 0.0607
Training Epoch: 96 [49920/50048]	Loss: 0.0629
Training Epoch: 96 [50048/50048]	Loss: 0.1672
2022-12-06 13:33:46.208 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 08:33:46,226 [ZeusDataLoader(eval)] eval epoch 97 done: time=3.71 energy=453.40
2022-12-06 08:33:46,226 [ZeusDataLoader(train)] Up to epoch 97: time=8750.43, energy=1062135.68, cost=1296730.40
2022-12-06 08:33:46,226 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 08:33:46,226 [ZeusDataLoader(train)] Expected next epoch: time=8840.23, energy=1072933.70, cost=1309986.78
2022-12-06 08:33:46,227 [ZeusDataLoader(train)] Epoch 98 begin.
Validation Epoch: 96, Average loss: 0.0190, Accuracy: 0.6410
2022-12-06 08:33:46,414 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:33:46,415 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:33:46.417 [ZeusMonitor] Monitor started.
2022-12-06 13:33:46.417 [ZeusMonitor] Running indefinitely. 2022-12-06 13:33:46.417 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:33:46.417 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e98+gpu0.power.log
Training Epoch: 97 [128/50048]	Loss: 0.1666
Training Epoch: 97 [256/50048]	Loss: 0.0886
Training Epoch: 97 [384/50048]	Loss: 0.0843
Training Epoch: 97 [512/50048]	Loss: 0.0606
Training Epoch: 97 [640/50048]	Loss: 0.0581
Training Epoch: 97 [768/50048]	Loss: 0.0392
Training Epoch: 97 [896/50048]	Loss: 0.0494
Training Epoch: 97 [1024/50048]	Loss: 0.0572
Training Epoch: 97 [1152/50048]	Loss: 0.0470
Training Epoch: 97 [1280/50048]	Loss: 0.0792
Training Epoch: 97 [1408/50048]	Loss: 0.0517
Training Epoch: 97 [1536/50048]	Loss: 0.0402
Training Epoch: 97 [1664/50048]	Loss: 0.1830
Training Epoch: 97 [1792/50048]	Loss: 0.0554
Training Epoch: 97 [1920/50048]	Loss: 0.1237
Training Epoch: 97 [2048/50048]	Loss: 0.0620
Training Epoch: 97 [2176/50048]	Loss: 0.0665
Training Epoch: 97 [2304/50048]	Loss: 0.0811
Training Epoch: 97 [2432/50048]	Loss: 0.0251
Training Epoch: 97 [2560/50048]	Loss: 0.0437
Training Epoch: 97 [2688/50048]	Loss: 0.0784
Training Epoch: 97 [2816/50048]	Loss: 0.0473
Training Epoch: 97 [2944/50048]	Loss: 0.1051
Training Epoch: 97 [3072/50048]	Loss: 0.0275
Training Epoch: 97 [3200/50048]	Loss: 0.0277
Training Epoch: 97 [3328/50048]	Loss: 0.0508
Training Epoch: 97 [3456/50048]	Loss: 0.0520
Training Epoch: 97 [3584/50048]	Loss: 0.0649
Training Epoch: 97 [3712/50048]	Loss: 0.0550
Training Epoch: 97 [3840/50048]	Loss: 0.0578
Training Epoch: 97 [3968/50048]	Loss: 0.0955
Training Epoch: 97 [4096/50048]	Loss: 0.1417
Training Epoch: 97 [4224/50048]	Loss: 0.0698
Training Epoch: 97 [4352/50048]	Loss: 0.0470
Training Epoch: 97 [4480/50048]	Loss: 0.0915
Training Epoch: 97 [4608/50048]	Loss: 0.0423
Training Epoch: 97 [4736/50048]	Loss: 0.1825
Training Epoch: 97 [4864/50048]	Loss: 0.0514
Training Epoch: 97 [4992/50048]	Loss: 0.0734
Training Epoch: 97 [5120/50048]	Loss: 0.0553
Training Epoch: 97 [5248/50048]	Loss: 0.0460
Training Epoch: 97 [5376/50048]	Loss: 0.0551
Training Epoch: 97 [5504/50048]	Loss: 0.0462
Training Epoch: 97 [5632/50048]	Loss: 0.0889
Training Epoch: 97 [5760/50048]	Loss: 0.0911
Training Epoch: 97 [5888/50048]	Loss: 0.0979
Training Epoch: 97 [6016/50048]	Loss: 0.0816
Training Epoch: 97 [6144/50048]	Loss: 0.0397
Training Epoch: 97 [6272/50048]	Loss: 0.0866
Training Epoch: 97 [6400/50048]	Loss: 0.1621
Training Epoch: 97 [6528/50048]	Loss: 0.0776
Training Epoch: 97 [6656/50048]	Loss: 0.0731
Training Epoch: 97 [6784/50048]	Loss: 0.0494
Training Epoch: 97 [6912/50048]	Loss: 0.1174
Training Epoch: 97 [7040/50048]	Loss: 0.0988
Training Epoch: 97 [7168/50048]	Loss: 0.1308
Training Epoch: 97 [7296/50048]	Loss: 0.0705
Training Epoch: 97 [7424/50048]	Loss: 0.0413
Training Epoch: 97 [7552/50048]	Loss: 0.0347
Training Epoch: 97 [7680/50048]	Loss: 0.0686
Training Epoch: 97 [7808/50048]	Loss: 0.0837
Training Epoch: 97 [7936/50048]	Loss: 0.0390
Training Epoch: 97 [8064/50048]	Loss: 0.1256
Training Epoch: 97 [8192/50048]	Loss: 0.0380
Training Epoch: 97 [8320/50048]	Loss: 0.0630
Training Epoch: 97 [8448/50048]	Loss: 0.1064
Training Epoch: 97 [8576/50048]	Loss: 0.0678
Training Epoch: 97 [8704/50048]	Loss: 0.0427
Training Epoch: 97 [8832/50048]	Loss: 0.0931
Training Epoch: 97 [8960/50048]	Loss: 0.0613
Training Epoch: 97 [9088/50048]	Loss: 0.1579
Training Epoch: 97 [9216/50048]	Loss: 0.0462
Training Epoch: 97 [9344/50048]	Loss: 0.0328
Training Epoch: 97 [9472/50048]	Loss: 0.0872
Training Epoch: 97 [9600/50048]	Loss: 0.0991
Training Epoch: 97 [9728/50048]	Loss: 0.0965
Training Epoch: 97 [9856/50048]	Loss: 0.0682
Training Epoch: 97 [9984/50048]	Loss: 0.0358
Training Epoch: 97 [10112/50048]	Loss: 0.0367
Training Epoch: 97 [10240/50048]	Loss: 0.0811
Training Epoch: 97 [10368/50048]	Loss: 0.0588
Training Epoch: 97 [10496/50048]	Loss: 0.1474
Training Epoch: 97 [10624/50048]	Loss: 0.0856
Training Epoch: 97 [10752/50048]	Loss: 0.0350
Training Epoch: 97 [10880/50048]	Loss: 0.0404
Training Epoch: 97 [11008/50048]	Loss: 0.0665
Training Epoch: 97 [11136/50048]	Loss: 0.0631
Training Epoch: 97 [11264/50048]	Loss: 0.0848
Training Epoch: 97 [11392/50048]	Loss: 0.0391
Training Epoch: 97 [11520/50048]	Loss: 0.0551
Training Epoch: 97 [11648/50048]	Loss: 0.0671
Training Epoch: 97 [11776/50048]	Loss: 0.0966
Training Epoch: 97 [11904/50048]	Loss: 0.0337
Training Epoch: 97 [12032/50048]	Loss: 0.0344
Training Epoch: 97 [12160/50048]	Loss: 0.0406
Training Epoch: 97 [12288/50048]	Loss: 0.0718
Training Epoch: 97 [12416/50048]	Loss: 0.0715
Training Epoch: 97 [12544/50048]	Loss: 0.0617
Training Epoch: 97 [12672/50048]	Loss: 0.0660
Training Epoch: 97 [12800/50048]	Loss: 0.0572
Training Epoch: 97 [12928/50048]	Loss: 0.0321
Training Epoch: 97 [13056/50048]	Loss: 0.0414
Training Epoch: 97 [13184/50048]	Loss: 0.0996
Training Epoch: 97 [13312/50048]	Loss: 0.1094
Training Epoch: 97 [13440/50048]	Loss: 0.0967
Training Epoch: 97 [13568/50048]	Loss: 0.0905
Training Epoch: 97 [13696/50048]	Loss: 0.0477
Training Epoch: 97 [13824/50048]	Loss: 0.0374
Training Epoch: 97 [13952/50048]	Loss: 0.0655
Training Epoch: 97 [14080/50048]	Loss: 0.1270
Training Epoch: 97 [14208/50048]	Loss: 0.0896
Training Epoch: 97 [14336/50048]	Loss: 0.1113
Training Epoch: 97 [14464/50048]	Loss: 0.0445
Training Epoch: 97 [14592/50048]	Loss: 0.0781
Training Epoch: 97 [14720/50048]	Loss: 0.0756
Training Epoch: 97 [14848/50048]	Loss: 0.0978
Training Epoch: 97 [14976/50048]	Loss: 0.1106
Training Epoch: 97 [15104/50048]	Loss: 0.0476
Training Epoch: 97 [15232/50048]	Loss: 0.0256
Training Epoch: 97 [15360/50048]	Loss: 0.0622
Training Epoch: 97 [15488/50048]	Loss: 0.0643
Training Epoch: 97 [15616/50048]	Loss: 0.0488
Training Epoch: 97 [15744/50048]	Loss: 0.0414
Training Epoch: 97 [15872/50048]	Loss: 0.0789
Training Epoch: 97 [16000/50048]	Loss: 0.0793
Training Epoch: 97 [16128/50048]	Loss: 0.0658
Training Epoch: 97 [16256/50048]	Loss: 0.0570
Training Epoch: 97 [16384/50048]	Loss: 0.0465
Training Epoch: 97 [16512/50048]	Loss: 0.0772
Training Epoch: 97 [16640/50048]	Loss: 0.0561
Training Epoch: 97 [16768/50048]	Loss: 0.0656
Training Epoch: 97 [16896/50048]	Loss: 0.1119
Training Epoch: 97 [17024/50048]	Loss: 0.1002
Training Epoch: 97 [17152/50048]	Loss: 0.0619
Training Epoch: 97 [17280/50048]	Loss: 0.0314
Training Epoch: 97 [17408/50048]	Loss: 0.0536
Training Epoch: 97 [17536/50048]	Loss: 0.0767
Training Epoch: 97 [17664/50048]	Loss: 0.1277
Training Epoch: 97 [17792/50048]	Loss: 0.0805
Training Epoch: 97 [17920/50048]	Loss: 0.0802
Training Epoch: 97 [18048/50048]	Loss: 0.0424
Training Epoch: 97 [18176/50048]	Loss: 0.0877
Training Epoch: 97 [18304/50048]	Loss: 0.0352
Training Epoch: 97 [18432/50048]	Loss: 0.0829
Training Epoch: 97 [18560/50048]	Loss: 0.0872
Training Epoch: 97 [18688/50048]	Loss: 0.0845
Training Epoch: 97 [18816/50048]	Loss: 0.0327
Training Epoch: 97 [18944/50048]	Loss: 0.1051
Training Epoch: 97 [19072/50048]	Loss: 0.0511
Training Epoch: 97 [19200/50048]	Loss: 0.1280
Training Epoch: 97 [19328/50048]	Loss: 0.0581
Training Epoch: 97 [19456/50048]	Loss: 0.1022
Training Epoch: 97 [19584/50048]	Loss: 0.0615
Training Epoch: 97 [19712/50048]	Loss: 0.0575
Training Epoch: 97 [19840/50048]	Loss: 0.0819
Training Epoch: 97 [19968/50048]	Loss: 0.0546
Training Epoch: 97 [20096/50048]	Loss: 0.0881
Training Epoch: 97 [20224/50048]	Loss: 0.0739
Training Epoch: 97 [20352/50048]	Loss: 0.1187
Training Epoch: 97 [20480/50048]	Loss: 0.0625
Training Epoch: 97 [20608/50048]	Loss: 0.1424
Training Epoch: 97 [20736/50048]	Loss: 0.0543
Training Epoch: 97 [20864/50048]	Loss: 0.0986
Training Epoch: 97 [20992/50048]	Loss: 0.0731
Training Epoch: 97 [21120/50048]	Loss: 0.1380
Training Epoch: 97 [21248/50048]	Loss: 0.0766
Training Epoch: 97 [21376/50048]	Loss: 0.0692
Training Epoch: 97 [21504/50048]	Loss: 0.1255
Training Epoch: 97 [21632/50048]	Loss: 0.0799
Training Epoch: 97 [21760/50048]	Loss: 0.0560
Training Epoch: 97 [21888/50048]	Loss: 0.0655
Training Epoch: 97 [22016/50048]	Loss: 0.0667
Training Epoch: 97 [22144/50048]	Loss: 0.0458
Training Epoch: 97 [22272/50048]	Loss: 0.0525
Training Epoch: 97 [22400/50048]	Loss: 0.1116
Training Epoch: 97 [22528/50048]	Loss: 0.0710
Training Epoch: 97 [22656/50048]	Loss: 0.0528
Training Epoch: 97 [22784/50048]	Loss: 0.1085
Training Epoch: 97 [22912/50048]	Loss: 0.0854
Training Epoch: 97 [23040/50048]	Loss: 0.0265
Training Epoch: 97 [23168/50048]	Loss: 0.0242
Training Epoch: 97 [23296/50048]	Loss: 0.0891
Training Epoch: 97 [23424/50048]	Loss: 0.0827
Training Epoch: 97 [23552/50048]	Loss: 0.0855
Training Epoch: 97 [23680/50048]	Loss: 0.1103
Training Epoch: 97 [23808/50048]	Loss: 0.1144
Training Epoch: 97 [23936/50048]	Loss: 0.1163
Training Epoch: 97 [24064/50048]	Loss: 0.0974
Training Epoch: 97 [24192/50048]	Loss: 0.1588
Training Epoch: 97 [24320/50048]	Loss: 0.0619
Training Epoch: 97 [24448/50048]	Loss: 0.0369
Training Epoch: 97 [24576/50048]	Loss: 0.0524
Training Epoch: 97 [24704/50048]	Loss: 0.0484
Training Epoch: 97 [24832/50048]	Loss: 0.0816
Training Epoch: 97 [24960/50048]	Loss: 0.0849
Training Epoch: 97 [25088/50048]	Loss: 0.0293
Training Epoch: 97 [25216/50048]	Loss: 0.0839
Training Epoch: 97 [25344/50048]	Loss: 0.0876
Training Epoch: 97 [25472/50048]	Loss: 0.0690
Training Epoch: 97 [25600/50048]	Loss: 0.1078
Training Epoch: 97 [25728/50048]	Loss: 0.1298
Training Epoch: 97 [25856/50048]	Loss: 0.0909
Training Epoch: 97 [25984/50048]	Loss: 0.0728
Training Epoch: 97 [26112/50048]	Loss: 0.0945
Training Epoch: 97 [26240/50048]	Loss: 0.0544
Training Epoch: 97 [26368/50048]	Loss: 0.0650
Training Epoch: 97 [26496/50048]	Loss: 0.0721
Training Epoch: 97 [26624/50048]	Loss: 0.1040
Training Epoch: 97 [26752/50048]	Loss: 0.0719
Training Epoch: 97 [26880/50048]	Loss: 0.0503
Training Epoch: 97 [27008/50048]	Loss: 0.0296
Training Epoch: 97 [27136/50048]	Loss: 0.0371
Training Epoch: 97 [27264/50048]	Loss: 0.0671
Training Epoch: 97 [27392/50048]	Loss: 0.0374
Training Epoch: 97 [27520/50048]	Loss: 0.0874
Training Epoch: 97 [27648/50048]	Loss: 0.0849
Training Epoch: 97 [27776/50048]	Loss: 0.0894
Training Epoch: 97 [27904/50048]	Loss: 0.0744
Training Epoch: 97 [28032/50048]	Loss: 0.0669
Training Epoch: 97 [28160/50048]	Loss: 0.0958
Training Epoch: 97 [28288/50048]	Loss: 0.0569
Training Epoch: 97 [28416/50048]	Loss: 0.0756
Training Epoch: 97 [28544/50048]	Loss: 0.0955
Training Epoch: 97 [28672/50048]	Loss: 0.0771
Training Epoch: 97 [28800/50048]	Loss: 0.0632
Training Epoch: 97 [28928/50048]	Loss: 0.0741
Training Epoch: 97 [29056/50048]	Loss: 0.0639
Training Epoch: 97 [29184/50048]	Loss: 0.0493
Training Epoch: 97 [29312/50048]	Loss: 0.0275
Training Epoch: 97 [29440/50048]	Loss: 0.1008
Training Epoch: 97 [29568/50048]	Loss: 0.1033
Training Epoch: 97 [29696/50048]	Loss: 0.1061
Training Epoch: 97 [29824/50048]	Loss: 0.1116
Training Epoch: 97 [29952/50048]	Loss: 0.0194
Training Epoch: 97 [30080/50048]	Loss: 0.0862
Training Epoch: 97 [30208/50048]	Loss: 0.0471
Training Epoch: 97 [30336/50048]	Loss: 0.1020
Training Epoch: 97 [30464/50048]	Loss: 0.0859
Training Epoch: 97 [30592/50048]	Loss: 0.0851
Training Epoch: 97 [30720/50048]	Loss: 0.0610
Training Epoch: 97 [30848/50048]	Loss: 0.0591
Training Epoch: 97 [30976/50048]	Loss: 0.0438
Training Epoch: 97 [31104/50048]	Loss: 0.0782
Training Epoch: 97 [31232/50048]	Loss: 0.0396
Training Epoch: 97 [31360/50048]	Loss: 0.0537
Training Epoch: 97 [31488/50048]	Loss: 0.1133
Training Epoch: 97 [31616/50048]	Loss: 0.0302
Training Epoch: 97 [31744/50048]	Loss: 0.1309
Training Epoch: 97 [31872/50048]	Loss: 0.0583
Training Epoch: 97 [32000/50048]	Loss: 0.0989
Training Epoch: 97 [32128/50048]	Loss: 0.2037
Training Epoch: 97 [32256/50048]	Loss: 0.0652
Training Epoch: 97 [32384/50048]	Loss: 0.0821
Training Epoch: 97 [32512/50048]	Loss: 0.1591
Training Epoch: 97 [32640/50048]	Loss: 0.0603
Training Epoch: 97 [32768/50048]	Loss: 0.0788
Training Epoch: 97 [32896/50048]	Loss: 0.0424
Training Epoch: 97 [33024/50048]	Loss: 0.0776
Training Epoch: 97 [33152/50048]	Loss: 0.1235
Training Epoch: 97 [33280/50048]	Loss: 0.1027
Training Epoch: 97 [33408/50048]	Loss: 0.0243
Training Epoch: 97 [33536/50048]	Loss: 0.0517
Training Epoch: 97 [33664/50048]	Loss: 0.0622
Training Epoch: 97 [33792/50048]	Loss: 0.1526
Training Epoch: 97 [33920/50048]	Loss: 0.0742
Training Epoch: 97 [34048/50048]	Loss: 0.1070
Training Epoch: 97 [34176/50048]	Loss: 0.0819
Training Epoch: 97 [34304/50048]	Loss: 0.0659
Training Epoch: 97 [34432/50048]	Loss: 0.2026
Training Epoch: 97 [34560/50048]	Loss: 0.1360
Training Epoch: 97 [34688/50048]	Loss: 0.0904
Training Epoch: 97 [34816/50048]	Loss: 0.0623
Training Epoch: 97 [34944/50048]	Loss: 0.0553
Training Epoch: 97 [35072/50048]	Loss: 0.0601
Training Epoch: 97 [35200/50048]	Loss: 0.1031
Training Epoch: 97 [35328/50048]	Loss: 0.0572
Training Epoch: 97 [35456/50048]	Loss: 0.1381
Training Epoch: 97 [35584/50048]	Loss: 0.0857
Training Epoch: 97 [35712/50048]	Loss: 0.0381
Training Epoch: 97 [35840/50048]	Loss: 0.0846
Training Epoch: 97 [35968/50048]	Loss: 0.1165
Training Epoch: 97 [36096/50048]	Loss: 0.0904
Training Epoch: 97 [36224/50048]	Loss: 0.0393
Training Epoch: 97 [36352/50048]	Loss: 0.0634
Training Epoch: 97 [36480/50048]	Loss: 0.0501
Training Epoch: 97 [36608/50048]	Loss: 0.0782
Training Epoch: 97 [36736/50048]	Loss: 0.0728
Training Epoch: 97 [36864/50048]	Loss: 0.1061
Training Epoch: 97 [36992/50048]	Loss: 0.1513
Training Epoch: 97 [37120/50048]	Loss: 0.1053
Training Epoch: 97 [37248/50048]	Loss: 0.1004
Training Epoch: 97 [37376/50048]	Loss: 0.0752
Training Epoch: 97 [37504/50048]	Loss: 0.1007
Training Epoch: 97 [37632/50048]	Loss: 0.0477
Training Epoch: 97 [37760/50048]	Loss: 0.1166
Training Epoch: 97 [37888/50048]	Loss: 0.1030
Training Epoch: 97 [38016/50048]	Loss: 0.1367
Training Epoch: 97 [38144/50048]	Loss: 0.0650
Training Epoch: 97 [38272/50048]	Loss: 0.0518
Training Epoch: 97 [38400/50048]	Loss: 0.0617
Training Epoch: 97 [38528/50048]	Loss: 0.0559
Training Epoch: 97 [38656/50048]	Loss: 0.0851
Training Epoch: 97 [38784/50048]	Loss: 0.0859
Training Epoch: 97 [38912/50048]	Loss: 0.0637
Training Epoch: 97 [39040/50048]	Loss: 0.0638
Training Epoch: 97 [39168/50048]	Loss: 0.0702
Training Epoch: 97 [39296/50048]	Loss: 0.1095
Training Epoch: 97 [39424/50048]	Loss: 0.0618
Training Epoch: 97 [39552/50048]	Loss: 0.1004
Training Epoch: 97 [39680/50048]	Loss: 0.0418
Training Epoch: 97 [39808/50048]	Loss: 0.0406
Training Epoch: 97 [39936/50048]	Loss: 0.0993
Training Epoch: 97 [40064/50048]	Loss: 0.0708
Training Epoch: 97 [40192/50048]	Loss: 0.1337
Training Epoch: 97 [40320/50048]	Loss: 0.0690
Training Epoch: 97 [40448/50048]	Loss: 0.0905
Training Epoch: 97 [40576/50048]	Loss: 0.0971
Training Epoch: 97 [40704/50048]	Loss: 0.0939
Training Epoch: 97 [40832/50048]	Loss: 0.0832
Training Epoch: 97 [40960/50048]	Loss: 0.0702
Training Epoch: 97 [41088/50048]	Loss: 0.0507
Training Epoch: 97 [41216/50048]	Loss: 0.0440
Training Epoch: 97 [41344/50048]	Loss: 0.0267
Training Epoch: 97 [41472/50048]	Loss: 0.0911
Training Epoch: 97 [41600/50048]	Loss: 0.0653
Training Epoch: 97 [41728/50048]	Loss: 0.0958
Training Epoch: 97 [41856/50048]	Loss: 0.0515
Training Epoch: 97 [41984/50048]	Loss: 0.1437
Training Epoch: 97 [42112/50048]	Loss: 0.0878
Training Epoch: 97 [42240/50048]	Loss: 0.0951
Training Epoch: 97 [42368/50048]	Loss: 0.0483
Training Epoch: 97 [42496/50048]	Loss: 0.0695
Training Epoch: 97 [42624/50048]	Loss: 0.0747
Training Epoch: 97 [42752/50048]	Loss: 0.1416
Training Epoch: 97 [42880/50048]	Loss: 0.1349
Training Epoch: 97 [43008/50048]	Loss: 0.0486
Training Epoch: 97 [43136/50048]	Loss: 0.1616
Training Epoch: 97 [43264/50048]	Loss: 0.0505
Training Epoch: 97 [43392/50048]	Loss: 0.1222
Training Epoch: 97 [43520/50048]	Loss: 0.0397
Training Epoch: 97 [43648/50048]	Loss: 0.0394
Training Epoch: 97 [43776/50048]	Loss: 0.0960
Training Epoch: 97 [43904/50048]	Loss: 0.0274
Training Epoch: 97 [44032/50048]	Loss: 0.0353
Training Epoch: 97 [44160/50048]	Loss: 0.0812
Training Epoch: 97 [44288/50048]	Loss: 0.0846
Training Epoch: 97 [44416/50048]	Loss: 0.0574
Training Epoch: 97 [44544/50048]	Loss: 0.0562
Training Epoch: 97 [44672/50048]	Loss: 0.0642
Training Epoch: 97 [44800/50048]	Loss: 0.1317
Training Epoch: 97 [44928/50048]	Loss: 0.0579
Training Epoch: 97 [45056/50048]	Loss: 0.0892
Training Epoch: 97 [45184/50048]	Loss: 0.0650
Training Epoch: 97 [45312/50048]	Loss: 0.0503
Training Epoch: 97 [45440/50048]	Loss: 0.0834
Training Epoch: 97 [45568/50048]	Loss: 0.0808
Training Epoch: 97 [45696/50048]	Loss: 0.0669
2022-12-06 08:35:12,743 [ZeusDataLoader(train)] train epoch 98 done: time=86.51 energy=10498.52
2022-12-06 08:35:12,744 [ZeusDataLoader(eval)] Epoch 98 begin.
Training Epoch: 97 [45824/50048]	Loss: 0.0324
Training Epoch: 97 [45952/50048]	Loss: 0.0428
Training Epoch: 97 [46080/50048]	Loss: 0.1737
Training Epoch: 97 [46208/50048]	Loss: 0.0287
Training Epoch: 97 [46336/50048]	Loss: 0.1089
Training Epoch: 97 [46464/50048]	Loss: 0.1083
Training Epoch: 97 [46592/50048]	Loss: 0.0792
Training Epoch: 97 [46720/50048]	Loss: 0.2529
Training Epoch: 97 [46848/50048]	Loss: 0.0912
Training Epoch: 97 [46976/50048]	Loss: 0.0495
Training Epoch: 97 [47104/50048]	Loss: 0.0694
Training Epoch: 97 [47232/50048]	Loss: 0.1639
Training Epoch: 97 [47360/50048]	Loss: 0.1020
Training Epoch: 97 [47488/50048]	Loss: 0.0963
Training Epoch: 97 [47616/50048]	Loss: 0.0337
Training Epoch: 97 [47744/50048]	Loss: 0.0671
Training Epoch: 97 [47872/50048]	Loss: 0.0871
Training Epoch: 97 [48000/50048]	Loss: 0.0627
Training Epoch: 97 [48128/50048]	Loss: 0.0685
Training Epoch: 97 [48256/50048]	Loss: 0.1460
Training Epoch: 97 [48384/50048]	Loss: 0.0690
Training Epoch: 97 [48512/50048]	Loss: 0.0999
Training Epoch: 97 [48640/50048]	Loss: 0.0359
Training Epoch: 97 [48768/50048]	Loss: 0.1617
Training Epoch: 97 [48896/50048]	Loss: 0.0522
Training Epoch: 97 [49024/50048]	Loss: 0.0688
Training Epoch: 97 [49152/50048]	Loss: 0.0896
Training Epoch: 97 [49280/50048]	Loss: 0.0962
Training Epoch: 97 [49408/50048]	Loss: 0.1410
Training Epoch: 97 [49536/50048]	Loss: 0.1206
Training Epoch: 97 [49664/50048]	Loss: 0.0553
Training Epoch: 97 [49792/50048]	Loss: 0.1722
Training Epoch: 97 [49920/50048]	Loss: 0.0710
Training Epoch: 97 [50048/50048]	Loss: 0.0399
2022-12-06 13:35:16.473 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 08:35:16,499 [ZeusDataLoader(eval)] eval epoch 98 done: time=3.75 energy=455.35
2022-12-06 08:35:16,499 [ZeusDataLoader(train)] Up to epoch 98: time=8840.68, energy=1073089.55, cost=1310104.34
2022-12-06 08:35:16,500 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 08:35:16,500 [ZeusDataLoader(train)] Expected next epoch: time=8930.48, energy=1083887.56, cost=1323360.73
2022-12-06 08:35:16,501 [ZeusDataLoader(train)] Epoch 99 begin.
Validation Epoch: 97, Average loss: 0.0183, Accuracy: 0.6450
2022-12-06 08:35:16,689 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:35:16,690 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:35:16.692 [ZeusMonitor] Monitor started.
2022-12-06 13:35:16.692 [ZeusMonitor] Running indefinitely. 2022-12-06 13:35:16.692 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:35:16.692 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e99+gpu0.power.log
Training Epoch: 98 [128/50048]	Loss: 0.0758
Training Epoch: 98 [256/50048]	Loss: 0.1045
Training Epoch: 98 [384/50048]	Loss: 0.0346
Training Epoch: 98 [512/50048]	Loss: 0.0574
Training Epoch: 98 [640/50048]	Loss: 0.0756
Training Epoch: 98 [768/50048]	Loss: 0.0358
Training Epoch: 98 [896/50048]	Loss: 0.0764
Training Epoch: 98 [1024/50048]	Loss: 0.1068
Training Epoch: 98 [1152/50048]	Loss: 0.0326
Training Epoch: 98 [1280/50048]	Loss: 0.0859
Training Epoch: 98 [1408/50048]	Loss: 0.0596
Training Epoch: 98 [1536/50048]	Loss: 0.0706
Training Epoch: 98 [1664/50048]	Loss: 0.0838
Training Epoch: 98 [1792/50048]	Loss: 0.0567
Training Epoch: 98 [1920/50048]	Loss: 0.0332
Training Epoch: 98 [2048/50048]	Loss: 0.0498
Training Epoch: 98 [2176/50048]	Loss: 0.0552
Training Epoch: 98 [2304/50048]	Loss: 0.0546
Training Epoch: 98 [2432/50048]	Loss: 0.0561
Training Epoch: 98 [2560/50048]	Loss: 0.0618
Training Epoch: 98 [2688/50048]	Loss: 0.0665
Training Epoch: 98 [2816/50048]	Loss: 0.0490
Training Epoch: 98 [2944/50048]	Loss: 0.0680
Training Epoch: 98 [3072/50048]	Loss: 0.0986
Training Epoch: 98 [3200/50048]	Loss: 0.1188
Training Epoch: 98 [3328/50048]	Loss: 0.0690
Training Epoch: 98 [3456/50048]	Loss: 0.0426
Training Epoch: 98 [3584/50048]	Loss: 0.0666
Training Epoch: 98 [3712/50048]	Loss: 0.0782
Training Epoch: 98 [3840/50048]	Loss: 0.0784
Training Epoch: 98 [3968/50048]	Loss: 0.1465
Training Epoch: 98 [4096/50048]	Loss: 0.1022
Training Epoch: 98 [4224/50048]	Loss: 0.1393
Training Epoch: 98 [4352/50048]	Loss: 0.1201
Training Epoch: 98 [4480/50048]	Loss: 0.1224
Training Epoch: 98 [4608/50048]	Loss: 0.0304
Training Epoch: 98 [4736/50048]	Loss: 0.0767
Training Epoch: 98 [4864/50048]	Loss: 0.1052
Training Epoch: 98 [4992/50048]	Loss: 0.0609
Training Epoch: 98 [5120/50048]	Loss: 0.0729
Training Epoch: 98 [5248/50048]	Loss: 0.0526
Training Epoch: 98 [5376/50048]	Loss: 0.0888
Training Epoch: 98 [5504/50048]	Loss: 0.1314
Training Epoch: 98 [5632/50048]	Loss: 0.1048
Training Epoch: 98 [5760/50048]	Loss: 0.0795
Training Epoch: 98 [5888/50048]	Loss: 0.0458
Training Epoch: 98 [6016/50048]	Loss: 0.0938
Training Epoch: 98 [6144/50048]	Loss: 0.1016
Training Epoch: 98 [6272/50048]	Loss: 0.0670
Training Epoch: 98 [6400/50048]	Loss: 0.0490
Training Epoch: 98 [6528/50048]	Loss: 0.0752
Training Epoch: 98 [6656/50048]	Loss: 0.1590
Training Epoch: 98 [6784/50048]	Loss: 0.0774
Training Epoch: 98 [6912/50048]	Loss: 0.0789
Training Epoch: 98 [7040/50048]	Loss: 0.0479
Training Epoch: 98 [7168/50048]	Loss: 0.0880
Training Epoch: 98 [7296/50048]	Loss: 0.1100
Training Epoch: 98 [7424/50048]	Loss: 0.0726
Training Epoch: 98 [7552/50048]	Loss: 0.0351
Training Epoch: 98 [7680/50048]	Loss: 0.0694
Training Epoch: 98 [7808/50048]	Loss: 0.1378
Training Epoch: 98 [7936/50048]	Loss: 0.0814
Training Epoch: 98 [8064/50048]	Loss: 0.0668
Training Epoch: 98 [8192/50048]	Loss: 0.0399
Training Epoch: 98 [8320/50048]	Loss: 0.0988
Training Epoch: 98 [8448/50048]	Loss: 0.0743
Training Epoch: 98 [8576/50048]	Loss: 0.0613
Training Epoch: 98 [8704/50048]	Loss: 0.0474
Training Epoch: 98 [8832/50048]	Loss: 0.0505
Training Epoch: 98 [8960/50048]	Loss: 0.0561
Training Epoch: 98 [9088/50048]	Loss: 0.0574
Training Epoch: 98 [9216/50048]	Loss: 0.0250
Training Epoch: 98 [9344/50048]	Loss: 0.1132
Training Epoch: 98 [9472/50048]	Loss: 0.0607
Training Epoch: 98 [9600/50048]	Loss: 0.1252
Training Epoch: 98 [9728/50048]	Loss: 0.0399
Training Epoch: 98 [9856/50048]	Loss: 0.0996
Training Epoch: 98 [9984/50048]	Loss: 0.1317
Training Epoch: 98 [10112/50048]	Loss: 0.1221
Training Epoch: 98 [10240/50048]	Loss: 0.0922
Training Epoch: 98 [10368/50048]	Loss: 0.0344
Training Epoch: 98 [10496/50048]	Loss: 0.0551
Training Epoch: 98 [10624/50048]	Loss: 0.0439
Training Epoch: 98 [10752/50048]	Loss: 0.0203
Training Epoch: 98 [10880/50048]	Loss: 0.1064
Training Epoch: 98 [11008/50048]	Loss: 0.0594
Training Epoch: 98 [11136/50048]	Loss: 0.0607
Training Epoch: 98 [11264/50048]	Loss: 0.0837
Training Epoch: 98 [11392/50048]	Loss: 0.0532
Training Epoch: 98 [11520/50048]	Loss: 0.0918
Training Epoch: 98 [11648/50048]	Loss: 0.1165
Training Epoch: 98 [11776/50048]	Loss: 0.0436
Training Epoch: 98 [11904/50048]	Loss: 0.0310
Training Epoch: 98 [12032/50048]	Loss: 0.0418
Training Epoch: 98 [12160/50048]	Loss: 0.0789
Training Epoch: 98 [12288/50048]	Loss: 0.1423
Training Epoch: 98 [12416/50048]	Loss: 0.0497
Training Epoch: 98 [12544/50048]	Loss: 0.0391
Training Epoch: 98 [12672/50048]	Loss: 0.0206
Training Epoch: 98 [12800/50048]	Loss: 0.0674
Training Epoch: 98 [12928/50048]	Loss: 0.0534
Training Epoch: 98 [13056/50048]	Loss: 0.0569
Training Epoch: 98 [13184/50048]	Loss: 0.0402
Training Epoch: 98 [13312/50048]	Loss: 0.0456
Training Epoch: 98 [13440/50048]	Loss: 0.0856
Training Epoch: 98 [13568/50048]	Loss: 0.0616
Training Epoch: 98 [13696/50048]	Loss: 0.1123
Training Epoch: 98 [13824/50048]	Loss: 0.0549
Training Epoch: 98 [13952/50048]	Loss: 0.0732
Training Epoch: 98 [14080/50048]	Loss: 0.0474
Training Epoch: 98 [14208/50048]	Loss: 0.0707
Training Epoch: 98 [14336/50048]	Loss: 0.0723
Training Epoch: 98 [14464/50048]	Loss: 0.0402
Training Epoch: 98 [14592/50048]	Loss: 0.0715
Training Epoch: 98 [14720/50048]	Loss: 0.0387
Training Epoch: 98 [14848/50048]	Loss: 0.0526
Training Epoch: 98 [14976/50048]	Loss: 0.1043
Training Epoch: 98 [15104/50048]	Loss: 0.0427
Training Epoch: 98 [15232/50048]	Loss: 0.0523
Training Epoch: 98 [15360/50048]	Loss: 0.0270
Training Epoch: 98 [15488/50048]	Loss: 0.0614
Training Epoch: 98 [15616/50048]	Loss: 0.1298
Training Epoch: 98 [15744/50048]	Loss: 0.0646
Training Epoch: 98 [15872/50048]	Loss: 0.0595
Training Epoch: 98 [16000/50048]	Loss: 0.0731
Training Epoch: 98 [16128/50048]	Loss: 0.0243
Training Epoch: 98 [16256/50048]	Loss: 0.0482
Training Epoch: 98 [16384/50048]	Loss: 0.0594
Training Epoch: 98 [16512/50048]	Loss: 0.0945
Training Epoch: 98 [16640/50048]	Loss: 0.0773
Training Epoch: 98 [16768/50048]	Loss: 0.0229
Training Epoch: 98 [16896/50048]	Loss: 0.0937
Training Epoch: 98 [17024/50048]	Loss: 0.0498
Training Epoch: 98 [17152/50048]	Loss: 0.1001
Training Epoch: 98 [17280/50048]	Loss: 0.0538
Training Epoch: 98 [17408/50048]	Loss: 0.0898
Training Epoch: 98 [17536/50048]	Loss: 0.0480
Training Epoch: 98 [17664/50048]	Loss: 0.0555
Training Epoch: 98 [17792/50048]	Loss: 0.0434
Training Epoch: 98 [17920/50048]	Loss: 0.0546
Training Epoch: 98 [18048/50048]	Loss: 0.0731
Training Epoch: 98 [18176/50048]	Loss: 0.0749
Training Epoch: 98 [18304/50048]	Loss: 0.1034
Training Epoch: 98 [18432/50048]	Loss: 0.1406
Training Epoch: 98 [18560/50048]	Loss: 0.0836
Training Epoch: 98 [18688/50048]	Loss: 0.1783
Training Epoch: 98 [18816/50048]	Loss: 0.1058
Training Epoch: 98 [18944/50048]	Loss: 0.0708
Training Epoch: 98 [19072/50048]	Loss: 0.1005
Training Epoch: 98 [19200/50048]	Loss: 0.0345
Training Epoch: 98 [19328/50048]	Loss: 0.0629
Training Epoch: 98 [19456/50048]	Loss: 0.0622
Training Epoch: 98 [19584/50048]	Loss: 0.0804
Training Epoch: 98 [19712/50048]	Loss: 0.0556
Training Epoch: 98 [19840/50048]	Loss: 0.0699
Training Epoch: 98 [19968/50048]	Loss: 0.1437
Training Epoch: 98 [20096/50048]	Loss: 0.0611
Training Epoch: 98 [20224/50048]	Loss: 0.0671
Training Epoch: 98 [20352/50048]	Loss: 0.0792
Training Epoch: 98 [20480/50048]	Loss: 0.0898
Training Epoch: 98 [20608/50048]	Loss: 0.1516
Training Epoch: 98 [20736/50048]	Loss: 0.0554
Training Epoch: 98 [20864/50048]	Loss: 0.0958
Training Epoch: 98 [20992/50048]	Loss: 0.0710
Training Epoch: 98 [21120/50048]	Loss: 0.0745
Training Epoch: 98 [21248/50048]	Loss: 0.0851
Training Epoch: 98 [21376/50048]	Loss: 0.0978
Training Epoch: 98 [21504/50048]	Loss: 0.1129
Training Epoch: 98 [21632/50048]	Loss: 0.0706
Training Epoch: 98 [21760/50048]	Loss: 0.0896
Training Epoch: 98 [21888/50048]	Loss: 0.1083
Training Epoch: 98 [22016/50048]	Loss: 0.0547
Training Epoch: 98 [22144/50048]	Loss: 0.0909
Training Epoch: 98 [22272/50048]	Loss: 0.0769
Training Epoch: 98 [22400/50048]	Loss: 0.1565
Training Epoch: 98 [22528/50048]	Loss: 0.0900
Training Epoch: 98 [22656/50048]	Loss: 0.1114
Training Epoch: 98 [22784/50048]	Loss: 0.1067
Training Epoch: 98 [22912/50048]	Loss: 0.0828
Training Epoch: 98 [23040/50048]	Loss: 0.0475
Training Epoch: 98 [23168/50048]	Loss: 0.0831
Training Epoch: 98 [23296/50048]	Loss: 0.0786
Training Epoch: 98 [23424/50048]	Loss: 0.0699
Training Epoch: 98 [23552/50048]	Loss: 0.0970
Training Epoch: 98 [23680/50048]	Loss: 0.1436
Training Epoch: 98 [23808/50048]	Loss: 0.0645
Training Epoch: 98 [23936/50048]	Loss: 0.0637
Training Epoch: 98 [24064/50048]	Loss: 0.0380
Training Epoch: 98 [24192/50048]	Loss: 0.0671
Training Epoch: 98 [24320/50048]	Loss: 0.1466
Training Epoch: 98 [24448/50048]	Loss: 0.0450
Training Epoch: 98 [24576/50048]	Loss: 0.1187
Training Epoch: 98 [24704/50048]	Loss: 0.0926
Training Epoch: 98 [24832/50048]	Loss: 0.1142
Training Epoch: 98 [24960/50048]	Loss: 0.0830
Training Epoch: 98 [25088/50048]	Loss: 0.0876
Training Epoch: 98 [25216/50048]	Loss: 0.0403
Training Epoch: 98 [25344/50048]	Loss: 0.0675
Training Epoch: 98 [25472/50048]	Loss: 0.1068
Training Epoch: 98 [25600/50048]	Loss: 0.0663
Training Epoch: 98 [25728/50048]	Loss: 0.0482
Training Epoch: 98 [25856/50048]	Loss: 0.0735
Training Epoch: 98 [25984/50048]	Loss: 0.0272
Training Epoch: 98 [26112/50048]	Loss: 0.0663
Training Epoch: 98 [26240/50048]	Loss: 0.0966
Training Epoch: 98 [26368/50048]	Loss: 0.1145
Training Epoch: 98 [26496/50048]	Loss: 0.0370
Training Epoch: 98 [26624/50048]	Loss: 0.0634
Training Epoch: 98 [26752/50048]	Loss: 0.0672
Training Epoch: 98 [26880/50048]	Loss: 0.0864
Training Epoch: 98 [27008/50048]	Loss: 0.1035
Training Epoch: 98 [27136/50048]	Loss: 0.1008
Training Epoch: 98 [27264/50048]	Loss: 0.0726
Training Epoch: 98 [27392/50048]	Loss: 0.0695
Training Epoch: 98 [27520/50048]	Loss: 0.0492
Training Epoch: 98 [27648/50048]	Loss: 0.0786
Training Epoch: 98 [27776/50048]	Loss: 0.0627
Training Epoch: 98 [27904/50048]	Loss: 0.0843
Training Epoch: 98 [28032/50048]	Loss: 0.0759
Training Epoch: 98 [28160/50048]	Loss: 0.0378
Training Epoch: 98 [28288/50048]	Loss: 0.0721
Training Epoch: 98 [28416/50048]	Loss: 0.1871
Training Epoch: 98 [28544/50048]	Loss: 0.1366
Training Epoch: 98 [28672/50048]	Loss: 0.1043
Training Epoch: 98 [28800/50048]	Loss: 0.0618
Training Epoch: 98 [28928/50048]	Loss: 0.0583
Training Epoch: 98 [29056/50048]	Loss: 0.0847
Training Epoch: 98 [29184/50048]	Loss: 0.0909
Training Epoch: 98 [29312/50048]	Loss: 0.0464
Training Epoch: 98 [29440/50048]	Loss: 0.1372
Training Epoch: 98 [29568/50048]	Loss: 0.0468
Training Epoch: 98 [29696/50048]	Loss: 0.0594
Training Epoch: 98 [29824/50048]	Loss: 0.1159
Training Epoch: 98 [29952/50048]	Loss: 0.0863
Training Epoch: 98 [30080/50048]	Loss: 0.1179
Training Epoch: 98 [30208/50048]	Loss: 0.0756
Training Epoch: 98 [30336/50048]	Loss: 0.0685
Training Epoch: 98 [30464/50048]	Loss: 0.0966
Training Epoch: 98 [30592/50048]	Loss: 0.0894
Training Epoch: 98 [30720/50048]	Loss: 0.1538
Training Epoch: 98 [30848/50048]	Loss: 0.1569
Training Epoch: 98 [30976/50048]	Loss: 0.0373
Training Epoch: 98 [31104/50048]	Loss: 0.0325
Training Epoch: 98 [31232/50048]	Loss: 0.0934
Training Epoch: 98 [31360/50048]	Loss: 0.1304
Training Epoch: 98 [31488/50048]	Loss: 0.0643
Training Epoch: 98 [31616/50048]	Loss: 0.0478
Training Epoch: 98 [31744/50048]	Loss: 0.0506
Training Epoch: 98 [31872/50048]	Loss: 0.0699
Training Epoch: 98 [32000/50048]	Loss: 0.0543
Training Epoch: 98 [32128/50048]	Loss: 0.0693
Training Epoch: 98 [32256/50048]	Loss: 0.0268
Training Epoch: 98 [32384/50048]	Loss: 0.0475
Training Epoch: 98 [32512/50048]	Loss: 0.1712
Training Epoch: 98 [32640/50048]	Loss: 0.0150
Training Epoch: 98 [32768/50048]	Loss: 0.1017
Training Epoch: 98 [32896/50048]	Loss: 0.0563
Training Epoch: 98 [33024/50048]	Loss: 0.0809
Training Epoch: 98 [33152/50048]	Loss: 0.1281
Training Epoch: 98 [33280/50048]	Loss: 0.0517
Training Epoch: 98 [33408/50048]	Loss: 0.0558
Training Epoch: 98 [33536/50048]	Loss: 0.0845
Training Epoch: 98 [33664/50048]	Loss: 0.0678
Training Epoch: 98 [33792/50048]	Loss: 0.0309
Training Epoch: 98 [33920/50048]	Loss: 0.1160
Training Epoch: 98 [34048/50048]	Loss: 0.0462
Training Epoch: 98 [34176/50048]	Loss: 0.0773
Training Epoch: 98 [34304/50048]	Loss: 0.0316
Training Epoch: 98 [34432/50048]	Loss: 0.0526
Training Epoch: 98 [34560/50048]	Loss: 0.1352
Training Epoch: 98 [34688/50048]	Loss: 0.0819
Training Epoch: 98 [34816/50048]	Loss: 0.0843
Training Epoch: 98 [34944/50048]	Loss: 0.0820
Training Epoch: 98 [35072/50048]	Loss: 0.1005
Training Epoch: 98 [35200/50048]	Loss: 0.0360
Training Epoch: 98 [35328/50048]	Loss: 0.1155
Training Epoch: 98 [35456/50048]	Loss: 0.1233
Training Epoch: 98 [35584/50048]	Loss: 0.0988
Training Epoch: 98 [35712/50048]	Loss: 0.0664
Training Epoch: 98 [35840/50048]	Loss: 0.0530
Training Epoch: 98 [35968/50048]	Loss: 0.1552
Training Epoch: 98 [36096/50048]	Loss: 0.0931
Training Epoch: 98 [36224/50048]	Loss: 0.1120
Training Epoch: 98 [36352/50048]	Loss: 0.1365
Training Epoch: 98 [36480/50048]	Loss: 0.0236
Training Epoch: 98 [36608/50048]	Loss: 0.1128
Training Epoch: 98 [36736/50048]	Loss: 0.1803
Training Epoch: 98 [36864/50048]	Loss: 0.0470
Training Epoch: 98 [36992/50048]	Loss: 0.0586
Training Epoch: 98 [37120/50048]	Loss: 0.0483
Training Epoch: 98 [37248/50048]	Loss: 0.1047
Training Epoch: 98 [37376/50048]	Loss: 0.0680
Training Epoch: 98 [37504/50048]	Loss: 0.0501
Training Epoch: 98 [37632/50048]	Loss: 0.0333
Training Epoch: 98 [37760/50048]	Loss: 0.1038
Training Epoch: 98 [37888/50048]	Loss: 0.1029
Training Epoch: 98 [38016/50048]	Loss: 0.1362
Training Epoch: 98 [38144/50048]	Loss: 0.0719
Training Epoch: 98 [38272/50048]	Loss: 0.0666
Training Epoch: 98 [38400/50048]	Loss: 0.0496
Training Epoch: 98 [38528/50048]	Loss: 0.1381
Training Epoch: 98 [38656/50048]	Loss: 0.1192
Training Epoch: 98 [38784/50048]	Loss: 0.0645
Training Epoch: 98 [38912/50048]	Loss: 0.1140
Training Epoch: 98 [39040/50048]	Loss: 0.0728
Training Epoch: 98 [39168/50048]	Loss: 0.0703
Training Epoch: 98 [39296/50048]	Loss: 0.1003
Training Epoch: 98 [39424/50048]	Loss: 0.0680
Training Epoch: 98 [39552/50048]	Loss: 0.0955
Training Epoch: 98 [39680/50048]	Loss: 0.0621
Training Epoch: 98 [39808/50048]	Loss: 0.0806
Training Epoch: 98 [39936/50048]	Loss: 0.0644
Training Epoch: 98 [40064/50048]	Loss: 0.0479
Training Epoch: 98 [40192/50048]	Loss: 0.0308
Training Epoch: 98 [40320/50048]	Loss: 0.0901
Training Epoch: 98 [40448/50048]	Loss: 0.0972
Training Epoch: 98 [40576/50048]	Loss: 0.1427
Training Epoch: 98 [40704/50048]	Loss: 0.0777
Training Epoch: 98 [40832/50048]	Loss: 0.0377
Training Epoch: 98 [40960/50048]	Loss: 0.0371
Training Epoch: 98 [41088/50048]	Loss: 0.1135
Training Epoch: 98 [41216/50048]	Loss: 0.0656
Training Epoch: 98 [41344/50048]	Loss: 0.0786
Training Epoch: 98 [41472/50048]	Loss: 0.1344
Training Epoch: 98 [41600/50048]	Loss: 0.1001
Training Epoch: 98 [41728/50048]	Loss: 0.0795
Training Epoch: 98 [41856/50048]	Loss: 0.0455
Training Epoch: 98 [41984/50048]	Loss: 0.0826
Training Epoch: 98 [42112/50048]	Loss: 0.0645
Training Epoch: 98 [42240/50048]	Loss: 0.0529
Training Epoch: 98 [42368/50048]	Loss: 0.1067
Training Epoch: 98 [42496/50048]	Loss: 0.0945
Training Epoch: 98 [42624/50048]	Loss: 0.0558
Training Epoch: 98 [42752/50048]	Loss: 0.1757
Training Epoch: 98 [42880/50048]	Loss: 0.1083
Training Epoch: 98 [43008/50048]	Loss: 0.0966
Training Epoch: 98 [43136/50048]	Loss: 0.0529
Training Epoch: 98 [43264/50048]	Loss: 0.1127
Training Epoch: 98 [43392/50048]	Loss: 0.1451
Training Epoch: 98 [43520/50048]	Loss: 0.1183
Training Epoch: 98 [43648/50048]	Loss: 0.0451
Training Epoch: 98 [43776/50048]	Loss: 0.1156
Training Epoch: 98 [43904/50048]	Loss: 0.0432
Training Epoch: 98 [44032/50048]	Loss: 0.0712
Training Epoch: 98 [44160/50048]	Loss: 0.0968
Training Epoch: 98 [44288/50048]	Loss: 0.0617
Training Epoch: 98 [44416/50048]	Loss: 0.0909
Training Epoch: 98 [44544/50048]	Loss: 0.1370
Training Epoch: 98 [44672/50048]	Loss: 0.0730
Training Epoch: 98 [44800/50048]	Loss: 0.0857
Training Epoch: 98 [44928/50048]	Loss: 0.0750
Training Epoch: 98 [45056/50048]	Loss: 0.0763
Training Epoch: 98 [45184/50048]	Loss: 0.0498
Training Epoch: 98 [45312/50048]	Loss: 0.0766
Training Epoch: 98 [45440/50048]	Loss: 0.1192
Training Epoch: 98 [45568/50048]	Loss: 0.0360
Training Epoch: 98 [45696/50048]	Loss: 0.0893
2022-12-06 08:36:42,978 [ZeusDataLoader(train)] train epoch 99 done: time=86.47 energy=10491.45
2022-12-06 08:36:42,979 [ZeusDataLoader(eval)] Epoch 99 begin.
Training Epoch: 98 [45824/50048]	Loss: 0.0800
Training Epoch: 98 [45952/50048]	Loss: 0.0580
Training Epoch: 98 [46080/50048]	Loss: 0.0962
Training Epoch: 98 [46208/50048]	Loss: 0.1273
Training Epoch: 98 [46336/50048]	Loss: 0.0175
Training Epoch: 98 [46464/50048]	Loss: 0.0747
Training Epoch: 98 [46592/50048]	Loss: 0.1297
Training Epoch: 98 [46720/50048]	Loss: 0.1724
Training Epoch: 98 [46848/50048]	Loss: 0.0645
Training Epoch: 98 [46976/50048]	Loss: 0.1465
Training Epoch: 98 [47104/50048]	Loss: 0.0405
Training Epoch: 98 [47232/50048]	Loss: 0.1512
Training Epoch: 98 [47360/50048]	Loss: 0.0456
Training Epoch: 98 [47488/50048]	Loss: 0.1490
Training Epoch: 98 [47616/50048]	Loss: 0.0812
Training Epoch: 98 [47744/50048]	Loss: 0.0648
Training Epoch: 98 [47872/50048]	Loss: 0.0499
Training Epoch: 98 [48000/50048]	Loss: 0.0342
Training Epoch: 98 [48128/50048]	Loss: 0.0482
Training Epoch: 98 [48256/50048]	Loss: 0.1114
Training Epoch: 98 [48384/50048]	Loss: 0.0607
Training Epoch: 98 [48512/50048]	Loss: 0.0982
Training Epoch: 98 [48640/50048]	Loss: 0.0742
Training Epoch: 98 [48768/50048]	Loss: 0.0870
Training Epoch: 98 [48896/50048]	Loss: 0.0505
Training Epoch: 98 [49024/50048]	Loss: 0.0995
Training Epoch: 98 [49152/50048]	Loss: 0.0773
Training Epoch: 98 [49280/50048]	Loss: 0.0609
Training Epoch: 98 [49408/50048]	Loss: 0.0470
Training Epoch: 98 [49536/50048]	Loss: 0.0459
Training Epoch: 98 [49664/50048]	Loss: 0.0946
Training Epoch: 98 [49792/50048]	Loss: 0.0466
Training Epoch: 98 [49920/50048]	Loss: 0.0810
Training Epoch: 98 [50048/50048]	Loss: 0.0817
2022-12-06 13:36:46.666 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 08:36:46,685 [ZeusDataLoader(eval)] eval epoch 99 done: time=3.70 energy=454.72
2022-12-06 08:36:46,685 [ZeusDataLoader(train)] Up to epoch 99: time=8930.84, energy=1084035.73, cost=1323466.74
2022-12-06 08:36:46,685 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 08:36:46,685 [ZeusDataLoader(train)] Expected next epoch: time=9020.64, energy=1094833.74, cost=1336723.12
2022-12-06 08:36:46,686 [ZeusDataLoader(train)] Epoch 100 begin.
Validation Epoch: 98, Average loss: 0.0187, Accuracy: 0.6434
2022-12-06 08:36:46,845 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:36:46,845 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:36:46.849 [ZeusMonitor] Monitor started.
2022-12-06 13:36:46.849 [ZeusMonitor] Running indefinitely. 2022-12-06 13:36:46.849 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:36:46.849 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e100+gpu0.power.log
Training Epoch: 99 [128/50048]	Loss: 0.0431
Training Epoch: 99 [256/50048]	Loss: 0.0625
Training Epoch: 99 [384/50048]	Loss: 0.0582
Training Epoch: 99 [512/50048]	Loss: 0.0988
Training Epoch: 99 [640/50048]	Loss: 0.0643
Training Epoch: 99 [768/50048]	Loss: 0.0654
Training Epoch: 99 [896/50048]	Loss: 0.1284
Training Epoch: 99 [1024/50048]	Loss: 0.0967
Training Epoch: 99 [1152/50048]	Loss: 0.0879
Training Epoch: 99 [1280/50048]	Loss: 0.0347
Training Epoch: 99 [1408/50048]	Loss: 0.0881
Training Epoch: 99 [1536/50048]	Loss: 0.0400
Training Epoch: 99 [1664/50048]	Loss: 0.1454
Training Epoch: 99 [1792/50048]	Loss: 0.0948
Training Epoch: 99 [1920/50048]	Loss: 0.0842
Training Epoch: 99 [2048/50048]	Loss: 0.0521
Training Epoch: 99 [2176/50048]	Loss: 0.0744
Training Epoch: 99 [2304/50048]	Loss: 0.0622
Training Epoch: 99 [2432/50048]	Loss: 0.0629
Training Epoch: 99 [2560/50048]	Loss: 0.0842
Training Epoch: 99 [2688/50048]	Loss: 0.0423
Training Epoch: 99 [2816/50048]	Loss: 0.0738
Training Epoch: 99 [2944/50048]	Loss: 0.0385
Training Epoch: 99 [3072/50048]	Loss: 0.0885
Training Epoch: 99 [3200/50048]	Loss: 0.0998
Training Epoch: 99 [3328/50048]	Loss: 0.1376
Training Epoch: 99 [3456/50048]	Loss: 0.1446
Training Epoch: 99 [3584/50048]	Loss: 0.1392
Training Epoch: 99 [3712/50048]	Loss: 0.1541
Training Epoch: 99 [3840/50048]	Loss: 0.0807
Training Epoch: 99 [3968/50048]	Loss: 0.0755
Training Epoch: 99 [4096/50048]	Loss: 0.0507
Training Epoch: 99 [4224/50048]	Loss: 0.1190
Training Epoch: 99 [4352/50048]	Loss: 0.0528
Training Epoch: 99 [4480/50048]	Loss: 0.1121
Training Epoch: 99 [4608/50048]	Loss: 0.0893
Training Epoch: 99 [4736/50048]	Loss: 0.0689
Training Epoch: 99 [4864/50048]	Loss: 0.0930
Training Epoch: 99 [4992/50048]	Loss: 0.1459
Training Epoch: 99 [5120/50048]	Loss: 0.0560
Training Epoch: 99 [5248/50048]	Loss: 0.0573
Training Epoch: 99 [5376/50048]	Loss: 0.0432
Training Epoch: 99 [5504/50048]	Loss: 0.0389
Training Epoch: 99 [5632/50048]	Loss: 0.1303
Training Epoch: 99 [5760/50048]	Loss: 0.0382
Training Epoch: 99 [5888/50048]	Loss: 0.0474
Training Epoch: 99 [6016/50048]	Loss: 0.1072
Training Epoch: 99 [6144/50048]	Loss: 0.1116
Training Epoch: 99 [6272/50048]	Loss: 0.0821
Training Epoch: 99 [6400/50048]	Loss: 0.1122
Training Epoch: 99 [6528/50048]	Loss: 0.0346
Training Epoch: 99 [6656/50048]	Loss: 0.0366
Training Epoch: 99 [6784/50048]	Loss: 0.0810
Training Epoch: 99 [6912/50048]	Loss: 0.0598
Training Epoch: 99 [7040/50048]	Loss: 0.0813
Training Epoch: 99 [7168/50048]	Loss: 0.0641
Training Epoch: 99 [7296/50048]	Loss: 0.0528
Training Epoch: 99 [7424/50048]	Loss: 0.0526
Training Epoch: 99 [7552/50048]	Loss: 0.0380
Training Epoch: 99 [7680/50048]	Loss: 0.0631
Training Epoch: 99 [7808/50048]	Loss: 0.0602
Training Epoch: 99 [7936/50048]	Loss: 0.1017
Training Epoch: 99 [8064/50048]	Loss: 0.0630
Training Epoch: 99 [8192/50048]	Loss: 0.1131
Training Epoch: 99 [8320/50048]	Loss: 0.0687
Training Epoch: 99 [8448/50048]	Loss: 0.0306
Training Epoch: 99 [8576/50048]	Loss: 0.0553
Training Epoch: 99 [8704/50048]	Loss: 0.1153
Training Epoch: 99 [8832/50048]	Loss: 0.0627
Training Epoch: 99 [8960/50048]	Loss: 0.1211
Training Epoch: 99 [9088/50048]	Loss: 0.0429
Training Epoch: 99 [9216/50048]	Loss: 0.1580
Training Epoch: 99 [9344/50048]	Loss: 0.1064
Training Epoch: 99 [9472/50048]	Loss: 0.0881
Training Epoch: 99 [9600/50048]	Loss: 0.0347
Training Epoch: 99 [9728/50048]	Loss: 0.0913
Training Epoch: 99 [9856/50048]	Loss: 0.0852
Training Epoch: 99 [9984/50048]	Loss: 0.0604
Training Epoch: 99 [10112/50048]	Loss: 0.0445
Training Epoch: 99 [10240/50048]	Loss: 0.0623
Training Epoch: 99 [10368/50048]	Loss: 0.0488
Training Epoch: 99 [10496/50048]	Loss: 0.0314
Training Epoch: 99 [10624/50048]	Loss: 0.0598
Training Epoch: 99 [10752/50048]	Loss: 0.1272
Training Epoch: 99 [10880/50048]	Loss: 0.0820
Training Epoch: 99 [11008/50048]	Loss: 0.1277
Training Epoch: 99 [11136/50048]	Loss: 0.0799
Training Epoch: 99 [11264/50048]	Loss: 0.0433
Training Epoch: 99 [11392/50048]	Loss: 0.1064
Training Epoch: 99 [11520/50048]	Loss: 0.1696
Training Epoch: 99 [11648/50048]	Loss: 0.0892
Training Epoch: 99 [11776/50048]	Loss: 0.0721
Training Epoch: 99 [11904/50048]	Loss: 0.0638
Training Epoch: 99 [12032/50048]	Loss: 0.0686
Training Epoch: 99 [12160/50048]	Loss: 0.0605
Training Epoch: 99 [12288/50048]	Loss: 0.0628
Training Epoch: 99 [12416/50048]	Loss: 0.0543
Training Epoch: 99 [12544/50048]	Loss: 0.0689
Training Epoch: 99 [12672/50048]	Loss: 0.0966
Training Epoch: 99 [12800/50048]	Loss: 0.0531
Training Epoch: 99 [12928/50048]	Loss: 0.1105
Training Epoch: 99 [13056/50048]	Loss: 0.0416
Training Epoch: 99 [13184/50048]	Loss: 0.0706
Training Epoch: 99 [13312/50048]	Loss: 0.0980
Training Epoch: 99 [13440/50048]	Loss: 0.0244
Training Epoch: 99 [13568/50048]	Loss: 0.0547
Training Epoch: 99 [13696/50048]	Loss: 0.0717
Training Epoch: 99 [13824/50048]	Loss: 0.0995
Training Epoch: 99 [13952/50048]	Loss: 0.0643
Training Epoch: 99 [14080/50048]	Loss: 0.0333
Training Epoch: 99 [14208/50048]	Loss: 0.0895
Training Epoch: 99 [14336/50048]	Loss: 0.0895
Training Epoch: 99 [14464/50048]	Loss: 0.0642
Training Epoch: 99 [14592/50048]	Loss: 0.0823
Training Epoch: 99 [14720/50048]	Loss: 0.0563
Training Epoch: 99 [14848/50048]	Loss: 0.0456
Training Epoch: 99 [14976/50048]	Loss: 0.0835
Training Epoch: 99 [15104/50048]	Loss: 0.0508
Training Epoch: 99 [15232/50048]	Loss: 0.0611
Training Epoch: 99 [15360/50048]	Loss: 0.0296
Training Epoch: 99 [15488/50048]	Loss: 0.0605
Training Epoch: 99 [15616/50048]	Loss: 0.0349
Training Epoch: 99 [15744/50048]	Loss: 0.0377
Training Epoch: 99 [15872/50048]	Loss: 0.0546
Training Epoch: 99 [16000/50048]	Loss: 0.1034
Training Epoch: 99 [16128/50048]	Loss: 0.0493
Training Epoch: 99 [16256/50048]	Loss: 0.0643
Training Epoch: 99 [16384/50048]	Loss: 0.0651
Training Epoch: 99 [16512/50048]	Loss: 0.1033
Training Epoch: 99 [16640/50048]	Loss: 0.0674
Training Epoch: 99 [16768/50048]	Loss: 0.0692
Training Epoch: 99 [16896/50048]	Loss: 0.0322
Training Epoch: 99 [17024/50048]	Loss: 0.0369
Training Epoch: 99 [17152/50048]	Loss: 0.1127
Training Epoch: 99 [17280/50048]	Loss: 0.0905
Training Epoch: 99 [17408/50048]	Loss: 0.1226
Training Epoch: 99 [17536/50048]	Loss: 0.0982
Training Epoch: 99 [17664/50048]	Loss: 0.0287
Training Epoch: 99 [17792/50048]	Loss: 0.1028
Training Epoch: 99 [17920/50048]	Loss: 0.0973
Training Epoch: 99 [18048/50048]	Loss: 0.0482
Training Epoch: 99 [18176/50048]	Loss: 0.0618
Training Epoch: 99 [18304/50048]	Loss: 0.0844
Training Epoch: 99 [18432/50048]	Loss: 0.0665
Training Epoch: 99 [18560/50048]	Loss: 0.0725
Training Epoch: 99 [18688/50048]	Loss: 0.1106
Training Epoch: 99 [18816/50048]	Loss: 0.0556
Training Epoch: 99 [18944/50048]	Loss: 0.1071
Training Epoch: 99 [19072/50048]	Loss: 0.0496
Training Epoch: 99 [19200/50048]	Loss: 0.0365
Training Epoch: 99 [19328/50048]	Loss: 0.1130
Training Epoch: 99 [19456/50048]	Loss: 0.0425
Training Epoch: 99 [19584/50048]	Loss: 0.0370
Training Epoch: 99 [19712/50048]	Loss: 0.0289
Training Epoch: 99 [19840/50048]	Loss: 0.0294
Training Epoch: 99 [19968/50048]	Loss: 0.0996
Training Epoch: 99 [20096/50048]	Loss: 0.0816
Training Epoch: 99 [20224/50048]	Loss: 0.0838
Training Epoch: 99 [20352/50048]	Loss: 0.0116
Training Epoch: 99 [20480/50048]	Loss: 0.1019
Training Epoch: 99 [20608/50048]	Loss: 0.0347
Training Epoch: 99 [20736/50048]	Loss: 0.0717
Training Epoch: 99 [20864/50048]	Loss: 0.0799
Training Epoch: 99 [20992/50048]	Loss: 0.0582
Training Epoch: 99 [21120/50048]	Loss: 0.0406
Training Epoch: 99 [21248/50048]	Loss: 0.0401
Training Epoch: 99 [21376/50048]	Loss: 0.0721
Training Epoch: 99 [21504/50048]	Loss: 0.0563
Training Epoch: 99 [21632/50048]	Loss: 0.0456
Training Epoch: 99 [21760/50048]	Loss: 0.0548
Training Epoch: 99 [21888/50048]	Loss: 0.0792
Training Epoch: 99 [22016/50048]	Loss: 0.0970
Training Epoch: 99 [22144/50048]	Loss: 0.0592
Training Epoch: 99 [22272/50048]	Loss: 0.0582
Training Epoch: 99 [22400/50048]	Loss: 0.0364
Training Epoch: 99 [22528/50048]	Loss: 0.0529
Training Epoch: 99 [22656/50048]	Loss: 0.0229
Training Epoch: 99 [22784/50048]	Loss: 0.0329
Training Epoch: 99 [22912/50048]	Loss: 0.0778
Training Epoch: 99 [23040/50048]	Loss: 0.0903
Training Epoch: 99 [23168/50048]	Loss: 0.0745
Training Epoch: 99 [23296/50048]	Loss: 0.0470
Training Epoch: 99 [23424/50048]	Loss: 0.0736
Training Epoch: 99 [23552/50048]	Loss: 0.1616
Training Epoch: 99 [23680/50048]	Loss: 0.0733
Training Epoch: 99 [23808/50048]	Loss: 0.0517
Training Epoch: 99 [23936/50048]	Loss: 0.0332
Training Epoch: 99 [24064/50048]	Loss: 0.0946
Training Epoch: 99 [24192/50048]	Loss: 0.0273
Training Epoch: 99 [24320/50048]	Loss: 0.0719
Training Epoch: 99 [24448/50048]	Loss: 0.1217
Training Epoch: 99 [24576/50048]	Loss: 0.0945
Training Epoch: 99 [24704/50048]	Loss: 0.1655
Training Epoch: 99 [24832/50048]	Loss: 0.0591
Training Epoch: 99 [24960/50048]	Loss: 0.0969
Training Epoch: 99 [25088/50048]	Loss: 0.0893
Training Epoch: 99 [25216/50048]	Loss: 0.0891
Training Epoch: 99 [25344/50048]	Loss: 0.0984
Training Epoch: 99 [25472/50048]	Loss: 0.0423
Training Epoch: 99 [25600/50048]	Loss: 0.0597
Training Epoch: 99 [25728/50048]	Loss: 0.0741
Training Epoch: 99 [25856/50048]	Loss: 0.0955
Training Epoch: 99 [25984/50048]	Loss: 0.0804
Training Epoch: 99 [26112/50048]	Loss: 0.0661
Training Epoch: 99 [26240/50048]	Loss: 0.0648
Training Epoch: 99 [26368/50048]	Loss: 0.0903
Training Epoch: 99 [26496/50048]	Loss: 0.0805
Training Epoch: 99 [26624/50048]	Loss: 0.0540
Training Epoch: 99 [26752/50048]	Loss: 0.0271
Training Epoch: 99 [26880/50048]	Loss: 0.0665
Training Epoch: 99 [27008/50048]	Loss: 0.0633
Training Epoch: 99 [27136/50048]	Loss: 0.0540
Training Epoch: 99 [27264/50048]	Loss: 0.1012
Training Epoch: 99 [27392/50048]	Loss: 0.0933
Training Epoch: 99 [27520/50048]	Loss: 0.0560
Training Epoch: 99 [27648/50048]	Loss: 0.0351
Training Epoch: 99 [27776/50048]	Loss: 0.0462
Training Epoch: 99 [27904/50048]	Loss: 0.0892
Training Epoch: 99 [28032/50048]	Loss: 0.0910
Training Epoch: 99 [28160/50048]	Loss: 0.0558
Training Epoch: 99 [28288/50048]	Loss: 0.0520
Training Epoch: 99 [28416/50048]	Loss: 0.0675
Training Epoch: 99 [28544/50048]	Loss: 0.0803
Training Epoch: 99 [28672/50048]	Loss: 0.0761
Training Epoch: 99 [28800/50048]	Loss: 0.0229
Training Epoch: 99 [28928/50048]	Loss: 0.0423
Training Epoch: 99 [29056/50048]	Loss: 0.0956
Training Epoch: 99 [29184/50048]	Loss: 0.0708
Training Epoch: 99 [29312/50048]	Loss: 0.0690
Training Epoch: 99 [29440/50048]	Loss: 0.0772
Training Epoch: 99 [29568/50048]	Loss: 0.0740
Training Epoch: 99 [29696/50048]	Loss: 0.1102
Training Epoch: 99 [29824/50048]	Loss: 0.0283
Training Epoch: 99 [29952/50048]	Loss: 0.1127
Training Epoch: 99 [30080/50048]	Loss: 0.0438
Training Epoch: 99 [30208/50048]	Loss: 0.0803
Training Epoch: 99 [30336/50048]	Loss: 0.0447
Training Epoch: 99 [30464/50048]	Loss: 0.0484
Training Epoch: 99 [30592/50048]	Loss: 0.1000
Training Epoch: 99 [30720/50048]	Loss: 0.0703
Training Epoch: 99 [30848/50048]	Loss: 0.0920
Training Epoch: 99 [30976/50048]	Loss: 0.0925
Training Epoch: 99 [31104/50048]	Loss: 0.0760
Training Epoch: 99 [31232/50048]	Loss: 0.0846
Training Epoch: 99 [31360/50048]	Loss: 0.1049
Training Epoch: 99 [31488/50048]	Loss: 0.0794
Training Epoch: 99 [31616/50048]	Loss: 0.0743
Training Epoch: 99 [31744/50048]	Loss: 0.1038
Training Epoch: 99 [31872/50048]	Loss: 0.0715
Training Epoch: 99 [32000/50048]	Loss: 0.0598
Training Epoch: 99 [32128/50048]	Loss: 0.0356
Training Epoch: 99 [32256/50048]	Loss: 0.0666
Training Epoch: 99 [32384/50048]	Loss: 0.0105
Training Epoch: 99 [32512/50048]	Loss: 0.0715
Training Epoch: 99 [32640/50048]	Loss: 0.1143
Training Epoch: 99 [32768/50048]	Loss: 0.0589
Training Epoch: 99 [32896/50048]	Loss: 0.0587
Training Epoch: 99 [33024/50048]	Loss: 0.0746
Training Epoch: 99 [33152/50048]	Loss: 0.0530
Training Epoch: 99 [33280/50048]	Loss: 0.0452
Training Epoch: 99 [33408/50048]	Loss: 0.0880
Training Epoch: 99 [33536/50048]	Loss: 0.1239
Training Epoch: 99 [33664/50048]	Loss: 0.0424
Training Epoch: 99 [33792/50048]	Loss: 0.0660
Training Epoch: 99 [33920/50048]	Loss: 0.0388
Training Epoch: 99 [34048/50048]	Loss: 0.0950
Training Epoch: 99 [34176/50048]	Loss: 0.0744
Training Epoch: 99 [34304/50048]	Loss: 0.0900
Training Epoch: 99 [34432/50048]	Loss: 0.0792
Training Epoch: 99 [34560/50048]	Loss: 0.0605
Training Epoch: 99 [34688/50048]	Loss: 0.1067
Training Epoch: 99 [34816/50048]	Loss: 0.0912
Training Epoch: 99 [34944/50048]	Loss: 0.0482
Training Epoch: 99 [35072/50048]	Loss: 0.0316
Training Epoch: 99 [35200/50048]	Loss: 0.0519
Training Epoch: 99 [35328/50048]	Loss: 0.0668
Training Epoch: 99 [35456/50048]	Loss: 0.1304
Training Epoch: 99 [35584/50048]	Loss: 0.0447
Training Epoch: 99 [35712/50048]	Loss: 0.0900
Training Epoch: 99 [35840/50048]	Loss: 0.0393
Training Epoch: 99 [35968/50048]	Loss: 0.0258
Training Epoch: 99 [36096/50048]	Loss: 0.0927
Training Epoch: 99 [36224/50048]	Loss: 0.0673
Training Epoch: 99 [36352/50048]	Loss: 0.1180
Training Epoch: 99 [36480/50048]	Loss: 0.1012
Training Epoch: 99 [36608/50048]	Loss: 0.0979
Training Epoch: 99 [36736/50048]	Loss: 0.0681
Training Epoch: 99 [36864/50048]	Loss: 0.0378
Training Epoch: 99 [36992/50048]	Loss: 0.0398
Training Epoch: 99 [37120/50048]	Loss: 0.0211
Training Epoch: 99 [37248/50048]	Loss: 0.1306
Training Epoch: 99 [37376/50048]	Loss: 0.0650
Training Epoch: 99 [37504/50048]	Loss: 0.0624
Training Epoch: 99 [37632/50048]	Loss: 0.0818
Training Epoch: 99 [37760/50048]	Loss: 0.0415
Training Epoch: 99 [37888/50048]	Loss: 0.0361
Training Epoch: 99 [38016/50048]	Loss: 0.1374
Training Epoch: 99 [38144/50048]	Loss: 0.1188
Training Epoch: 99 [38272/50048]	Loss: 0.0622
Training Epoch: 99 [38400/50048]	Loss: 0.1958
Training Epoch: 99 [38528/50048]	Loss: 0.1050
Training Epoch: 99 [38656/50048]	Loss: 0.0809
Training Epoch: 99 [38784/50048]	Loss: 0.0911
Training Epoch: 99 [38912/50048]	Loss: 0.0654
Training Epoch: 99 [39040/50048]	Loss: 0.1566
Training Epoch: 99 [39168/50048]	Loss: 0.0941
Training Epoch: 99 [39296/50048]	Loss: 0.0364
Training Epoch: 99 [39424/50048]	Loss: 0.0883
Training Epoch: 99 [39552/50048]	Loss: 0.0983
Training Epoch: 99 [39680/50048]	Loss: 0.1426
Training Epoch: 99 [39808/50048]	Loss: 0.0543
Training Epoch: 99 [39936/50048]	Loss: 0.1183
Training Epoch: 99 [40064/50048]	Loss: 0.0447
Training Epoch: 99 [40192/50048]	Loss: 0.0955
Training Epoch: 99 [40320/50048]	Loss: 0.0433
Training Epoch: 99 [40448/50048]	Loss: 0.0786
Training Epoch: 99 [40576/50048]	Loss: 0.0899
Training Epoch: 99 [40704/50048]	Loss: 0.0332
Training Epoch: 99 [40832/50048]	Loss: 0.0518
Training Epoch: 99 [40960/50048]	Loss: 0.0613
Training Epoch: 99 [41088/50048]	Loss: 0.0221
Training Epoch: 99 [41216/50048]	Loss: 0.0796
Training Epoch: 99 [41344/50048]	Loss: 0.0619
Training Epoch: 99 [41472/50048]	Loss: 0.0463
Training Epoch: 99 [41600/50048]	Loss: 0.0326
Training Epoch: 99 [41728/50048]	Loss: 0.0719
Training Epoch: 99 [41856/50048]	Loss: 0.0683
Training Epoch: 99 [41984/50048]	Loss: 0.0348
Training Epoch: 99 [42112/50048]	Loss: 0.0767
Training Epoch: 99 [42240/50048]	Loss: 0.0394
Training Epoch: 99 [42368/50048]	Loss: 0.0718
Training Epoch: 99 [42496/50048]	Loss: 0.0808
Training Epoch: 99 [42624/50048]	Loss: 0.0719
Training Epoch: 99 [42752/50048]	Loss: 0.0545
Training Epoch: 99 [42880/50048]	Loss: 0.0636
Training Epoch: 99 [43008/50048]	Loss: 0.0556
Training Epoch: 99 [43136/50048]	Loss: 0.1128
Training Epoch: 99 [43264/50048]	Loss: 0.0714
Training Epoch: 99 [43392/50048]	Loss: 0.0829
Training Epoch: 99 [43520/50048]	Loss: 0.0764
Training Epoch: 99 [43648/50048]	Loss: 0.0523
Training Epoch: 99 [43776/50048]	Loss: 0.1248
Training Epoch: 99 [43904/50048]	Loss: 0.0820
Training Epoch: 99 [44032/50048]	Loss: 0.1252
Training Epoch: 99 [44160/50048]	Loss: 0.0577
Training Epoch: 99 [44288/50048]	Loss: 0.0862
Training Epoch: 99 [44416/50048]	Loss: 0.0802
Training Epoch: 99 [44544/50048]	Loss: 0.1521
Training Epoch: 99 [44672/50048]	Loss: 0.0691
Training Epoch: 99 [44800/50048]	Loss: 0.1331
Training Epoch: 99 [44928/50048]	Loss: 0.1029
Training Epoch: 99 [45056/50048]	Loss: 0.1517
Training Epoch: 99 [45184/50048]	Loss: 0.0422
Training Epoch: 99 [45312/50048]	Loss: 0.0915
Training Epoch: 99 [45440/50048]	Loss: 0.0843
Training Epoch: 99 [45568/50048]	Loss: 0.0496
Training Epoch: 99 [45696/50048]	Loss: 0.1392
2022-12-06 08:38:13,254 [ZeusDataLoader(train)] train epoch 100 done: time=86.56 energy=10513.48
2022-12-06 08:38:13,256 [ZeusDataLoader(eval)] Epoch 100 begin.
Training Epoch: 99 [45824/50048]	Loss: 0.0911
Training Epoch: 99 [45952/50048]	Loss: 0.0370
Training Epoch: 99 [46080/50048]	Loss: 0.0795
Training Epoch: 99 [46208/50048]	Loss: 0.1167
Training Epoch: 99 [46336/50048]	Loss: 0.0527
Training Epoch: 99 [46464/50048]	Loss: 0.0479
Training Epoch: 99 [46592/50048]	Loss: 0.0777
Training Epoch: 99 [46720/50048]	Loss: 0.1081
Training Epoch: 99 [46848/50048]	Loss: 0.0668
Training Epoch: 99 [46976/50048]	Loss: 0.0937
Training Epoch: 99 [47104/50048]	Loss: 0.0844
Training Epoch: 99 [47232/50048]	Loss: 0.0829
Training Epoch: 99 [47360/50048]	Loss: 0.0439
Training Epoch: 99 [47488/50048]	Loss: 0.0913
Training Epoch: 99 [47616/50048]	Loss: 0.0619
Training Epoch: 99 [47744/50048]	Loss: 0.0233
Training Epoch: 99 [47872/50048]	Loss: 0.0828
Training Epoch: 99 [48000/50048]	Loss: 0.1387
Training Epoch: 99 [48128/50048]	Loss: 0.1186
Training Epoch: 99 [48256/50048]	Loss: 0.0720
Training Epoch: 99 [48384/50048]	Loss: 0.0422
Training Epoch: 99 [48512/50048]	Loss: 0.0520
Training Epoch: 99 [48640/50048]	Loss: 0.0844
Training Epoch: 99 [48768/50048]	Loss: 0.1114
Training Epoch: 99 [48896/50048]	Loss: 0.1222
Training Epoch: 99 [49024/50048]	Loss: 0.0875
Training Epoch: 99 [49152/50048]	Loss: 0.0590
Training Epoch: 99 [49280/50048]	Loss: 0.0566
Training Epoch: 99 [49408/50048]	Loss: 0.0890
Training Epoch: 99 [49536/50048]	Loss: 0.1358
Training Epoch: 99 [49664/50048]	Loss: 0.0954
Training Epoch: 99 [49792/50048]	Loss: 0.0391
Training Epoch: 99 [49920/50048]	Loss: 0.0441
Training Epoch: 99 [50048/50048]	Loss: 0.0624
2022-12-06 13:38:16.904 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 08:38:16,914 [ZeusDataLoader(eval)] eval epoch 100 done: time=3.65 energy=440.00
2022-12-06 08:38:16,914 [ZeusDataLoader(train)] Up to epoch 100: time=9021.05, energy=1094989.21, cost=1336836.61
2022-12-06 08:38:16,914 [ZeusDataLoader(train)] Maximum number of epochs 100 reached. Stopping.
2022-12-06 08:38:16,915 [ZeusDataLoader(train)] Training done.
2022-12-06 08:38:16,915 [ZeusDataLoader(train)] Saved /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/rec00+try02+bs128+lr0.0100000.train.json: {"energy": 1094989.2070152608, "time": 9021.051456862002, "cost": 1336836.6059830557, "num_epochs": 100, "reached": false}
Validation Epoch: 99, Average loss: 0.0188, Accuracy: 0.6435

[run job] Job terminated with exit code 0.
[run job] stats={'energy': 1094989.2070152608, 'time': 9021.051456862002, 'cost': 1336836.6059830557, 'num_epochs': 100, 'reached': False}
[Zeus Master] cost=1336836.6059830557
[run job] Launching job with BS 128: and LR: 0.01
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835', 'ZEUS_JOB_ID': 'rec00+try03', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_TARGET_METRIC': '0.8', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_USE_OPTIMAL_PL': 'True'}
[run job] cwd=/workspace/zeus/examples/cifar100
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '0.01']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/rec00+try03.train.log'
2022-12-06 08:38:21,677 [ZeusDataLoader(train)] Distributed data parallel: OFF
2022-12-06 08:38:21,718 [ZeusDataLoader(train)] Power limit range: [175000, 150000, 125000, 100000]
2022-12-06 08:38:21,718 [ZeusDataLoader(train)] Power profiling: OFF
2022-12-06 08:38:21,719 [ZeusDataLoader(train)] Loaded /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+lr0.0100000.power.json: {'job_id': 'rec00+try01', 'train_power': {'175000': 120.25616099179643, '150000': 120.91003189146555, '125000': 121.11901311576284, '100000': 98.69303482927677}, 'train_throughput': {'175000': 4.544606647863609, '150000': 4.5365038969654385, '125000': 4.536710151601713, '100000': 3.9448251884904706}, 'eval_power': {'175000': 120.03848402041383}, 'eval_throughput': {'175000': 20.996480248389204}, 'optimal_pl': 175000}
2022-12-06 08:38:21,719 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 08:38:21,722 [ZeusDataLoader(train)] [GPU_0] Set GPU power limit to 175W.
2022-12-06 08:38:23,759 [ZeusDataLoader(train)] Up to epoch 0: time=0.00, energy=0.00, cost=0.00
2022-12-06 08:38:23,760 [ZeusDataLoader(train)] Epoch 1 begin.
Files already downloaded and verified
Files already downloaded and verified
2022-12-06 08:38:23,975 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:38:23,976 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:38:23.979 [ZeusMonitor] Monitor started.
2022-12-06 13:38:23.979 [ZeusMonitor] Running indefinitely. 2022-12-06 13:38:23.979 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:38:23.979 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e1+gpu0.power.log
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 4.8435
Training Epoch: 0 [384/50048]	Loss: 4.9286
Training Epoch: 0 [512/50048]	Loss: 4.7509
Training Epoch: 0 [640/50048]	Loss: 4.8855
Training Epoch: 0 [768/50048]	Loss: 4.8443
Training Epoch: 0 [896/50048]	Loss: 4.9051
Training Epoch: 0 [1024/50048]	Loss: 4.8013
Training Epoch: 0 [1152/50048]	Loss: 4.9079
Training Epoch: 0 [1280/50048]	Loss: 4.7993
Training Epoch: 0 [1408/50048]	Loss: 4.7607
Training Epoch: 0 [1536/50048]	Loss: 4.6563
Training Epoch: 0 [1664/50048]	Loss: 4.9268
Training Epoch: 0 [1792/50048]	Loss: 4.7641
Training Epoch: 0 [1920/50048]	Loss: 4.8524
Training Epoch: 0 [2048/50048]	Loss: 4.6732
Training Epoch: 0 [2176/50048]	Loss: 4.9451
Training Epoch: 0 [2304/50048]	Loss: 4.8089
Training Epoch: 0 [2432/50048]	Loss: 4.6640
Training Epoch: 0 [2560/50048]	Loss: 4.7273
Training Epoch: 0 [2688/50048]	Loss: 4.7301
Training Epoch: 0 [2816/50048]	Loss: 4.5149
Training Epoch: 0 [2944/50048]	Loss: 4.5849
Training Epoch: 0 [3072/50048]	Loss: 4.6186
Training Epoch: 0 [3200/50048]	Loss: 4.7666
Training Epoch: 0 [3328/50048]	Loss: 4.6405
Training Epoch: 0 [3456/50048]	Loss: 4.5039
Training Epoch: 0 [3584/50048]	Loss: 4.5375
Training Epoch: 0 [3712/50048]	Loss: 4.6822
Training Epoch: 0 [3840/50048]	Loss: 4.4813
Training Epoch: 0 [3968/50048]	Loss: 4.5911
Training Epoch: 0 [4096/50048]	Loss: 4.6608
Training Epoch: 0 [4224/50048]	Loss: 4.6271
Training Epoch: 0 [4352/50048]	Loss: 4.5941
Training Epoch: 0 [4480/50048]	Loss: 4.4548
Training Epoch: 0 [4608/50048]	Loss: 4.5075
Training Epoch: 0 [4736/50048]	Loss: 4.5760
Training Epoch: 0 [4864/50048]	Loss: 4.5045
Training Epoch: 0 [4992/50048]	Loss: 4.5593
Training Epoch: 0 [5120/50048]	Loss: 4.4718
Training Epoch: 0 [5248/50048]	Loss: 4.4502
Training Epoch: 0 [5376/50048]	Loss: 4.3189
Training Epoch: 0 [5504/50048]	Loss: 4.3760
Training Epoch: 0 [5632/50048]	Loss: 4.4780
Training Epoch: 0 [5760/50048]	Loss: 4.4930
Training Epoch: 0 [5888/50048]	Loss: 4.3727
Training Epoch: 0 [6016/50048]	Loss: 4.2317
Training Epoch: 0 [6144/50048]	Loss: 4.1406
Training Epoch: 0 [6272/50048]	Loss: 4.5206
Training Epoch: 0 [6400/50048]	Loss: 4.2355
Training Epoch: 0 [6528/50048]	Loss: 4.1903
Training Epoch: 0 [6656/50048]	Loss: 4.5160
Training Epoch: 0 [6784/50048]	Loss: 4.3627
Training Epoch: 0 [6912/50048]	Loss: 4.4057
Training Epoch: 0 [7040/50048]	Loss: 4.2524
Training Epoch: 0 [7168/50048]	Loss: 4.2214
Training Epoch: 0 [7296/50048]	Loss: 4.3147
Training Epoch: 0 [7424/50048]	Loss: 4.1537
Training Epoch: 0 [7552/50048]	Loss: 4.1621
Training Epoch: 0 [7680/50048]	Loss: 4.2802
Training Epoch: 0 [7808/50048]	Loss: 4.2957
Training Epoch: 0 [7936/50048]	Loss: 4.3204
Training Epoch: 0 [8064/50048]	Loss: 4.1257
Training Epoch: 0 [8192/50048]	Loss: 4.1397
Training Epoch: 0 [8320/50048]	Loss: 4.2549
Training Epoch: 0 [8448/50048]	Loss: 4.2127
Training Epoch: 0 [8576/50048]	Loss: 4.0805
Training Epoch: 0 [8704/50048]	Loss: 4.2099
Training Epoch: 0 [8832/50048]	Loss: 4.1510
Training Epoch: 0 [8960/50048]	Loss: 4.2591
Training Epoch: 0 [9088/50048]	Loss: 4.1692
Training Epoch: 0 [9216/50048]	Loss: 4.1973
Training Epoch: 0 [9344/50048]	Loss: 4.1161
Training Epoch: 0 [9472/50048]	Loss: 4.1738
Training Epoch: 0 [9600/50048]	Loss: 4.1186
Training Epoch: 0 [9728/50048]	Loss: 4.1971
Training Epoch: 0 [9856/50048]	Loss: 4.1241
Training Epoch: 0 [9984/50048]	Loss: 4.1607
Training Epoch: 0 [10112/50048]	Loss: 4.0933
Training Epoch: 0 [10240/50048]	Loss: 4.3335
Training Epoch: 0 [10368/50048]	Loss: 4.3118
Training Epoch: 0 [10496/50048]	Loss: 4.0878
Training Epoch: 0 [10624/50048]	Loss: 4.0793
Training Epoch: 0 [10752/50048]	Loss: 4.0503
Training Epoch: 0 [10880/50048]	Loss: 4.1787
Training Epoch: 0 [11008/50048]	Loss: 3.9544
Training Epoch: 0 [11136/50048]	Loss: 4.1452
Training Epoch: 0 [11264/50048]	Loss: 4.1281
Training Epoch: 0 [11392/50048]	Loss: 4.1394
Training Epoch: 0 [11520/50048]	Loss: 4.0079
Training Epoch: 0 [11648/50048]	Loss: 4.1274
Training Epoch: 0 [11776/50048]	Loss: 4.0774
Training Epoch: 0 [11904/50048]	Loss: 4.0958
Training Epoch: 0 [12032/50048]	Loss: 4.0371
Training Epoch: 0 [12160/50048]	Loss: 4.0908
Training Epoch: 0 [12288/50048]	Loss: 3.9455
Training Epoch: 0 [12416/50048]	Loss: 4.2139
Training Epoch: 0 [12544/50048]	Loss: 3.9468
Training Epoch: 0 [12672/50048]	Loss: 4.0982
Training Epoch: 0 [12800/50048]	Loss: 4.0104
Training Epoch: 0 [12928/50048]	Loss: 4.0028
Training Epoch: 0 [13056/50048]	Loss: 3.9418
Training Epoch: 0 [13184/50048]	Loss: 4.1189
Training Epoch: 0 [13312/50048]	Loss: 4.0737
Training Epoch: 0 [13440/50048]	Loss: 4.0605
Training Epoch: 0 [13568/50048]	Loss: 4.1159
Training Epoch: 0 [13696/50048]	Loss: 3.8546
Training Epoch: 0 [13824/50048]	Loss: 4.1333
Training Epoch: 0 [13952/50048]	Loss: 4.0633
Training Epoch: 0 [14080/50048]	Loss: 3.9000
Training Epoch: 0 [14208/50048]	Loss: 4.1803
Training Epoch: 0 [14336/50048]	Loss: 4.0880
Training Epoch: 0 [14464/50048]	Loss: 4.0351
Training Epoch: 0 [14592/50048]	Loss: 4.0483
Training Epoch: 0 [14720/50048]	Loss: 3.9050
Training Epoch: 0 [14848/50048]	Loss: 3.8163
Training Epoch: 0 [14976/50048]	Loss: 4.0054
Training Epoch: 0 [15104/50048]	Loss: 3.9512
Training Epoch: 0 [15232/50048]	Loss: 4.1210
Training Epoch: 0 [15360/50048]	Loss: 4.2034
Training Epoch: 0 [15488/50048]	Loss: 4.0089
Training Epoch: 0 [15616/50048]	Loss: 4.0494
Training Epoch: 0 [15744/50048]	Loss: 4.0069
Training Epoch: 0 [15872/50048]	Loss: 4.0996
Training Epoch: 0 [16000/50048]	Loss: 3.9917
Training Epoch: 0 [16128/50048]	Loss: 3.9410
Training Epoch: 0 [16256/50048]	Loss: 3.9751
Training Epoch: 0 [16384/50048]	Loss: 3.9150
Training Epoch: 0 [16512/50048]	Loss: 4.0055
Training Epoch: 0 [16640/50048]	Loss: 3.8883
Training Epoch: 0 [16768/50048]	Loss: 3.7915
Training Epoch: 0 [16896/50048]	Loss: 4.1723
Training Epoch: 0 [17024/50048]	Loss: 3.8510
Training Epoch: 0 [17152/50048]	Loss: 3.8628
Training Epoch: 0 [17280/50048]	Loss: 4.1701
Training Epoch: 0 [17408/50048]	Loss: 4.0480
Training Epoch: 0 [17536/50048]	Loss: 3.9855
Training Epoch: 0 [17664/50048]	Loss: 3.8617
Training Epoch: 0 [17792/50048]	Loss: 4.1022
Training Epoch: 0 [17920/50048]	Loss: 4.1321
Training Epoch: 0 [18048/50048]	Loss: 3.9105
Training Epoch: 0 [18176/50048]	Loss: 3.9505
Training Epoch: 0 [18304/50048]	Loss: 4.1899
Training Epoch: 0 [18432/50048]	Loss: 4.0084
Training Epoch: 0 [18560/50048]	Loss: 3.8740
Training Epoch: 0 [18688/50048]	Loss: 3.8338
Training Epoch: 0 [18816/50048]	Loss: 3.9639
Training Epoch: 0 [18944/50048]	Loss: 3.8782
Training Epoch: 0 [19072/50048]	Loss: 4.1162
Training Epoch: 0 [19200/50048]	Loss: 3.9550
Training Epoch: 0 [19328/50048]	Loss: 3.8640
Training Epoch: 0 [19456/50048]	Loss: 3.7703
Training Epoch: 0 [19584/50048]	Loss: 3.9039
Training Epoch: 0 [19712/50048]	Loss: 3.9235
Training Epoch: 0 [19840/50048]	Loss: 3.9912
Training Epoch: 0 [19968/50048]	Loss: 4.1137
Training Epoch: 0 [20096/50048]	Loss: 3.8634
Training Epoch: 0 [20224/50048]	Loss: 3.8384
Training Epoch: 0 [20352/50048]	Loss: 3.7017
Training Epoch: 0 [20480/50048]	Loss: 4.0654
Training Epoch: 0 [20608/50048]	Loss: 3.8269
Training Epoch: 0 [20736/50048]	Loss: 3.8362
Training Epoch: 0 [20864/50048]	Loss: 3.7400
Training Epoch: 0 [20992/50048]	Loss: 3.9055
Training Epoch: 0 [21120/50048]	Loss: 4.0616
Training Epoch: 0 [21248/50048]	Loss: 4.1875
Training Epoch: 0 [21376/50048]	Loss: 3.9449
Training Epoch: 0 [21504/50048]	Loss: 3.8133
Training Epoch: 0 [21632/50048]	Loss: 3.9018
Training Epoch: 0 [21760/50048]	Loss: 3.9698
Training Epoch: 0 [21888/50048]	Loss: 4.0008
Training Epoch: 0 [22016/50048]	Loss: 3.9593
Training Epoch: 0 [22144/50048]	Loss: 3.9891
Training Epoch: 0 [22272/50048]	Loss: 3.7324
Training Epoch: 0 [22400/50048]	Loss: 3.9677
Training Epoch: 0 [22528/50048]	Loss: 3.9396
Training Epoch: 0 [22656/50048]	Loss: 3.7728
Training Epoch: 0 [22784/50048]	Loss: 3.8333
Training Epoch: 0 [22912/50048]	Loss: 3.7433
Training Epoch: 0 [23040/50048]	Loss: 3.8733
Training Epoch: 0 [23168/50048]	Loss: 3.9173
Training Epoch: 0 [23296/50048]	Loss: 3.9343
Training Epoch: 0 [23424/50048]	Loss: 3.9176
Training Epoch: 0 [23552/50048]	Loss: 3.8112
Training Epoch: 0 [23680/50048]	Loss: 3.8833
Training Epoch: 0 [23808/50048]	Loss: 3.8359
Training Epoch: 0 [23936/50048]	Loss: 3.8943
Training Epoch: 0 [24064/50048]	Loss: 3.9410
Training Epoch: 0 [24192/50048]	Loss: 3.7903
Training Epoch: 0 [24320/50048]	Loss: 3.8305
Training Epoch: 0 [24448/50048]	Loss: 3.6527
Training Epoch: 0 [24576/50048]	Loss: 3.9585
Training Epoch: 0 [24704/50048]	Loss: 3.9631
Training Epoch: 0 [24832/50048]	Loss: 4.0625
Training Epoch: 0 [24960/50048]	Loss: 3.8803
Training Epoch: 0 [25088/50048]	Loss: 3.7320
Training Epoch: 0 [25216/50048]	Loss: 3.8110
Training Epoch: 0 [25344/50048]	Loss: 3.8493
Training Epoch: 0 [25472/50048]	Loss: 3.8035
Training Epoch: 0 [25600/50048]	Loss: 3.9448
Training Epoch: 0 [25728/50048]	Loss: 3.7735
Training Epoch: 0 [25856/50048]	Loss: 3.7621
Training Epoch: 0 [25984/50048]	Loss: 3.7637
Training Epoch: 0 [26112/50048]	Loss: 3.7857
Training Epoch: 0 [26240/50048]	Loss: 4.0532
Training Epoch: 0 [26368/50048]	Loss: 3.8215
Training Epoch: 0 [26496/50048]	Loss: 3.8255
Training Epoch: 0 [26624/50048]	Loss: 3.7762
Training Epoch: 0 [26752/50048]	Loss: 3.9225
Training Epoch: 0 [26880/50048]	Loss: 3.8223
Training Epoch: 0 [27008/50048]	Loss: 3.7100
Training Epoch: 0 [27136/50048]	Loss: 3.7244
Training Epoch: 0 [27264/50048]	Loss: 3.8625
Training Epoch: 0 [27392/50048]	Loss: 3.9144
Training Epoch: 0 [27520/50048]	Loss: 3.8306
Training Epoch: 0 [27648/50048]	Loss: 3.7571
Training Epoch: 0 [27776/50048]	Loss: 3.8236
Training Epoch: 0 [27904/50048]	Loss: 3.9855
Training Epoch: 0 [28032/50048]	Loss: 3.8091
Training Epoch: 0 [28160/50048]	Loss: 3.8198
Training Epoch: 0 [28288/50048]	Loss: 3.7200
Training Epoch: 0 [28416/50048]	Loss: 3.6278
Training Epoch: 0 [28544/50048]	Loss: 3.7787
Training Epoch: 0 [28672/50048]	Loss: 3.7546
Training Epoch: 0 [28800/50048]	Loss: 3.8202
Training Epoch: 0 [28928/50048]	Loss: 3.7754
Training Epoch: 0 [29056/50048]	Loss: 3.8308
Training Epoch: 0 [29184/50048]	Loss: 3.8851
Training Epoch: 0 [29312/50048]	Loss: 3.7088
Training Epoch: 0 [29440/50048]	Loss: 3.9265
Training Epoch: 0 [29568/50048]	Loss: 3.8045
Training Epoch: 0 [29696/50048]	Loss: 3.7515
Training Epoch: 0 [29824/50048]	Loss: 3.8619
Training Epoch: 0 [29952/50048]	Loss: 3.9230
Training Epoch: 0 [30080/50048]	Loss: 3.7921
Training Epoch: 0 [30208/50048]	Loss: 4.0114
Training Epoch: 0 [30336/50048]	Loss: 3.9236
Training Epoch: 0 [30464/50048]	Loss: 3.8932
Training Epoch: 0 [30592/50048]	Loss: 3.7973
Training Epoch: 0 [30720/50048]	Loss: 3.6995
Training Epoch: 0 [30848/50048]	Loss: 3.9018
Training Epoch: 0 [30976/50048]	Loss: 3.6537
Training Epoch: 0 [31104/50048]	Loss: 3.7895
Training Epoch: 0 [31232/50048]	Loss: 3.9621
Training Epoch: 0 [31360/50048]	Loss: 3.7220
Training Epoch: 0 [31488/50048]	Loss: 3.9880
Training Epoch: 0 [31616/50048]	Loss: 3.7938
Training Epoch: 0 [31744/50048]	Loss: 3.8243
Training Epoch: 0 [31872/50048]	Loss: 3.7520
Training Epoch: 0 [32000/50048]	Loss: 3.8578
Training Epoch: 0 [32128/50048]	Loss: 3.4868
Training Epoch: 0 [32256/50048]	Loss: 3.9056
Training Epoch: 0 [32384/50048]	Loss: 3.5954
Training Epoch: 0 [32512/50048]	Loss: 3.4482
Training Epoch: 0 [32640/50048]	Loss: 3.8161
Training Epoch: 0 [32768/50048]	Loss: 3.7330
Training Epoch: 0 [32896/50048]	Loss: 3.7965
Training Epoch: 0 [33024/50048]	Loss: 3.7669
Training Epoch: 0 [33152/50048]	Loss: 3.7754
Training Epoch: 0 [33280/50048]	Loss: 3.6146
Training Epoch: 0 [33408/50048]	Loss: 3.6718
Training Epoch: 0 [33536/50048]	Loss: 4.0047
Training Epoch: 0 [33664/50048]	Loss: 3.6337
Training Epoch: 0 [33792/50048]	Loss: 3.5578
Training Epoch: 0 [33920/50048]	Loss: 3.6555
Training Epoch: 0 [34048/50048]	Loss: 3.8300
Training Epoch: 0 [34176/50048]	Loss: 3.8891
Training Epoch: 0 [34304/50048]	Loss: 3.6713
Training Epoch: 0 [34432/50048]	Loss: 3.7775
Training Epoch: 0 [34560/50048]	Loss: 3.7965
Training Epoch: 0 [34688/50048]	Loss: 3.8608
Training Epoch: 0 [34816/50048]	Loss: 3.9565
Training Epoch: 0 [34944/50048]	Loss: 3.7768
Training Epoch: 0 [35072/50048]	Loss: 3.6871
Training Epoch: 0 [35200/50048]	Loss: 3.5579
Training Epoch: 0 [35328/50048]	Loss: 3.6942
Training Epoch: 0 [35456/50048]	Loss: 3.6639
Training Epoch: 0 [35584/50048]	Loss: 3.7456
Training Epoch: 0 [35712/50048]	Loss: 3.6538
Training Epoch: 0 [35840/50048]	Loss: 3.7356
Training Epoch: 0 [35968/50048]	Loss: 3.7553
Training Epoch: 0 [36096/50048]	Loss: 3.6186
Training Epoch: 0 [36224/50048]	Loss: 3.6304
Training Epoch: 0 [36352/50048]	Loss: 3.6131
Training Epoch: 0 [36480/50048]	Loss: 3.5909
Training Epoch: 0 [36608/50048]	Loss: 3.9389
Training Epoch: 0 [36736/50048]	Loss: 3.7507
Training Epoch: 0 [36864/50048]	Loss: 3.5812
Training Epoch: 0 [36992/50048]	Loss: 3.6241
Training Epoch: 0 [37120/50048]	Loss: 3.6753
Training Epoch: 0 [37248/50048]	Loss: 3.5266
Training Epoch: 0 [37376/50048]	Loss: 3.6717
Training Epoch: 0 [37504/50048]	Loss: 3.8097
Training Epoch: 0 [37632/50048]	Loss: 3.7440
Training Epoch: 0 [37760/50048]	Loss: 3.5509
Training Epoch: 0 [37888/50048]	Loss: 3.9216
Training Epoch: 0 [38016/50048]	Loss: 3.6774
Training Epoch: 0 [38144/50048]	Loss: 3.6891
Training Epoch: 0 [38272/50048]	Loss: 3.8611
Training Epoch: 0 [38400/50048]	Loss: 3.6701
Training Epoch: 0 [38528/50048]	Loss: 3.8370
Training Epoch: 0 [38656/50048]	Loss: 3.5310
Training Epoch: 0 [38784/50048]	Loss: 3.5021
Training Epoch: 0 [38912/50048]	Loss: 3.6852
Training Epoch: 0 [39040/50048]	Loss: 3.7330
Training Epoch: 0 [39168/50048]	Loss: 3.7800
Training Epoch: 0 [39296/50048]	Loss: 3.6242
Training Epoch: 0 [39424/50048]	Loss: 3.6364
Training Epoch: 0 [39552/50048]	Loss: 3.3914
Training Epoch: 0 [39680/50048]	Loss: 3.7045
Training Epoch: 0 [39808/50048]	Loss: 3.5825
Training Epoch: 0 [39936/50048]	Loss: 3.8473
Training Epoch: 0 [40064/50048]	Loss: 3.5682
Training Epoch: 0 [40192/50048]	Loss: 3.4905
Training Epoch: 0 [40320/50048]	Loss: 3.6172
Training Epoch: 0 [40448/50048]	Loss: 3.6606
Training Epoch: 0 [40576/50048]	Loss: 3.5653
Training Epoch: 0 [40704/50048]	Loss: 3.6526
Training Epoch: 0 [40832/50048]	Loss: 3.6414
Training Epoch: 0 [40960/50048]	Loss: 3.6722
Training Epoch: 0 [41088/50048]	Loss: 3.6269
Training Epoch: 0 [41216/50048]	Loss: 3.4335
Training Epoch: 0 [41344/50048]	Loss: 3.6982
Training Epoch: 0 [41472/50048]	Loss: 3.5990
Training Epoch: 0 [41600/50048]	Loss: 3.4379
Training Epoch: 0 [41728/50048]	Loss: 3.5061
Training Epoch: 0 [41856/50048]	Loss: 3.7298
Training Epoch: 0 [41984/50048]	Loss: 3.7049
Training Epoch: 0 [42112/50048]	Loss: 3.7080
Training Epoch: 0 [42240/50048]	Loss: 3.7594
Training Epoch: 0 [42368/50048]	Loss: 3.5412
Training Epoch: 0 [42496/50048]	Loss: 3.8703
Training Epoch: 0 [42624/50048]	Loss: 3.7356
Training Epoch: 0 [42752/50048]	Loss: 3.7328
Training Epoch: 0 [42880/50048]	Loss: 3.7187
Training Epoch: 0 [43008/50048]	Loss: 3.5079
Training Epoch: 0 [43136/50048]	Loss: 3.4856
Training Epoch: 0 [43264/50048]	Loss: 3.6480
Training Epoch: 0 [43392/50048]	Loss: 3.4769
Training Epoch: 0 [43520/50048]	Loss: 3.7325
Training Epoch: 0 [43648/50048]	Loss: 3.3317
Training Epoch: 0 [43776/50048]	Loss: 3.4004
Training Epoch: 0 [43904/50048]	Loss: 3.7368
Training Epoch: 0 [44032/50048]	Loss: 3.4352
Training Epoch: 0 [44160/50048]	Loss: 3.4945
Training Epoch: 0 [44288/50048]	Loss: 3.5247
Training Epoch: 0 [44416/50048]	Loss: 3.5779
Training Epoch: 0 [44544/50048]	Loss: 3.5755
Training Epoch: 0 [44672/50048]	Loss: 3.8242
Training Epoch: 0 [44800/50048]	Loss: 3.5564
Training Epoch: 0 [44928/50048]	Loss: 3.4554
Training Epoch: 0 [45056/50048]	Loss: 3.6136
Training Epoch: 0 [45184/50048]	Loss: 4.0295
Training Epoch: 0 [45312/50048]	Loss: 3.7408
Training Epoch: 0 [45440/50048]	Loss: 3.5739
Training Epoch: 0 [45568/50048]	Loss: 3.5139
Training Epoch: 0 [45696/50048]	Loss: 3.3889
Training Epoch: 0 [45824/50048]	Loss: 3.7794
Training Epoch: 0 [45952/50048]	Loss: 3.5802
Training Epoch: 0 [46080/50048]	Loss: 3.5753
Training Epoch: 0 [46208/50048]	Loss: 3.5404
Training Epoch: 0 [46336/50048]	Loss: 3.4742
Training Epoch: 0 [46464/50048]	Loss: 3.4845
Training Epoch: 0 [46592/50048]	Loss: 3.6185
Training Epoch: 0 [46720/50048]	Loss: 3.5509
2022-12-06 08:39:50,772 [ZeusDataLoader(train)] train epoch 1 done: time=87.00 energy=10512.48
2022-12-06 08:39:50,774 [ZeusDataLoader(eval)] Epoch 1 begin.
Training Epoch: 0 [46848/50048]	Loss: 3.5481
Training Epoch: 0 [46976/50048]	Loss: 3.8246
Training Epoch: 0 [47104/50048]	Loss: 3.6049
Training Epoch: 0 [47232/50048]	Loss: 3.4596
Training Epoch: 0 [47360/50048]	Loss: 3.2520
Training Epoch: 0 [47488/50048]	Loss: 3.4485
Training Epoch: 0 [47616/50048]	Loss: 3.5733
Training Epoch: 0 [47744/50048]	Loss: 3.8381
Training Epoch: 0 [47872/50048]	Loss: 3.5767
Training Epoch: 0 [48000/50048]	Loss: 3.6088
Training Epoch: 0 [48128/50048]	Loss: 3.8191
Training Epoch: 0 [48256/50048]	Loss: 3.5775
Training Epoch: 0 [48384/50048]	Loss: 3.5965
Training Epoch: 0 [48512/50048]	Loss: 3.6560
Training Epoch: 0 [48640/50048]	Loss: 3.5374
Training Epoch: 0 [48768/50048]	Loss: 3.8113
Training Epoch: 0 [48896/50048]	Loss: 3.5001
Training Epoch: 0 [49024/50048]	Loss: 3.6438
Training Epoch: 0 [49152/50048]	Loss: 3.5678
Training Epoch: 0 [49280/50048]	Loss: 3.4475
Training Epoch: 0 [49408/50048]	Loss: 3.2841
Training Epoch: 0 [49536/50048]	Loss: 3.5825
Training Epoch: 0 [49664/50048]	Loss: 3.5304
Training Epoch: 0 [49792/50048]	Loss: 3.4514
Training Epoch: 0 [49920/50048]	Loss: 3.6346
Training Epoch: 0 [50048/50048]	Loss: 3.6759
2022-12-06 13:39:54.502 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 08:39:54,519 [ZeusDataLoader(eval)] eval epoch 1 done: time=3.74 energy=451.11
2022-12-06 08:39:54,520 [ZeusDataLoader(train)] Up to epoch 1: time=90.74, energy=10963.58, cost=13421.27
2022-12-06 08:39:54,520 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 08:39:54,520 [ZeusDataLoader(train)] Expected next epoch: time=180.54, energy=21761.60, cost=26677.65
2022-12-06 08:39:54,521 [ZeusDataLoader(train)] Epoch 2 begin.
Validation Epoch: 0, Average loss: 0.0298, Accuracy: 0.1279
2022-12-06 08:39:54,711 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:39:54,712 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:39:54.714 [ZeusMonitor] Monitor started.
2022-12-06 13:39:54.714 [ZeusMonitor] Running indefinitely. 2022-12-06 13:39:54.714 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:39:54.714 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e2+gpu0.power.log
