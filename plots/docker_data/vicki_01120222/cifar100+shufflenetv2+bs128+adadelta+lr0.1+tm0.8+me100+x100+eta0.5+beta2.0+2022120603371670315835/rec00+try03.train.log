2022-12-06 08:38:21,677 [ZeusDataLoader(train)] Distributed data parallel: OFF
2022-12-06 08:38:21,718 [ZeusDataLoader(train)] Power limit range: [175000, 150000, 125000, 100000]
2022-12-06 08:38:21,718 [ZeusDataLoader(train)] Power profiling: OFF
2022-12-06 08:38:21,719 [ZeusDataLoader(train)] Loaded /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+lr0.0100000.power.json: {'job_id': 'rec00+try01', 'train_power': {'175000': 120.25616099179643, '150000': 120.91003189146555, '125000': 121.11901311576284, '100000': 98.69303482927677}, 'train_throughput': {'175000': 4.544606647863609, '150000': 4.5365038969654385, '125000': 4.536710151601713, '100000': 3.9448251884904706}, 'eval_power': {'175000': 120.03848402041383}, 'eval_throughput': {'175000': 20.996480248389204}, 'optimal_pl': 175000}
2022-12-06 08:38:21,719 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 08:38:21,722 [ZeusDataLoader(train)] [GPU_0] Set GPU power limit to 175W.
2022-12-06 08:38:23,759 [ZeusDataLoader(train)] Up to epoch 0: time=0.00, energy=0.00, cost=0.00
2022-12-06 08:38:23,760 [ZeusDataLoader(train)] Epoch 1 begin.
Files already downloaded and verified
Files already downloaded and verified
2022-12-06 08:38:23,975 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:38:23,976 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:38:23.979 [ZeusMonitor] Monitor started.
2022-12-06 13:38:23.979 [ZeusMonitor] Running indefinitely. 2022-12-06 13:38:23.979 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:38:23.979 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e1+gpu0.power.log
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 4.8435
Training Epoch: 0 [384/50048]	Loss: 4.9286
Training Epoch: 0 [512/50048]	Loss: 4.7509
Training Epoch: 0 [640/50048]	Loss: 4.8855
Training Epoch: 0 [768/50048]	Loss: 4.8443
Training Epoch: 0 [896/50048]	Loss: 4.9051
Training Epoch: 0 [1024/50048]	Loss: 4.8013
Training Epoch: 0 [1152/50048]	Loss: 4.9079
Training Epoch: 0 [1280/50048]	Loss: 4.7993
Training Epoch: 0 [1408/50048]	Loss: 4.7607
Training Epoch: 0 [1536/50048]	Loss: 4.6563
Training Epoch: 0 [1664/50048]	Loss: 4.9268
Training Epoch: 0 [1792/50048]	Loss: 4.7641
Training Epoch: 0 [1920/50048]	Loss: 4.8524
Training Epoch: 0 [2048/50048]	Loss: 4.6732
Training Epoch: 0 [2176/50048]	Loss: 4.9451
Training Epoch: 0 [2304/50048]	Loss: 4.8089
Training Epoch: 0 [2432/50048]	Loss: 4.6640
Training Epoch: 0 [2560/50048]	Loss: 4.7273
Training Epoch: 0 [2688/50048]	Loss: 4.7301
Training Epoch: 0 [2816/50048]	Loss: 4.5149
Training Epoch: 0 [2944/50048]	Loss: 4.5849
Training Epoch: 0 [3072/50048]	Loss: 4.6186
Training Epoch: 0 [3200/50048]	Loss: 4.7666
Training Epoch: 0 [3328/50048]	Loss: 4.6405
Training Epoch: 0 [3456/50048]	Loss: 4.5039
Training Epoch: 0 [3584/50048]	Loss: 4.5375
Training Epoch: 0 [3712/50048]	Loss: 4.6822
Training Epoch: 0 [3840/50048]	Loss: 4.4813
Training Epoch: 0 [3968/50048]	Loss: 4.5911
Training Epoch: 0 [4096/50048]	Loss: 4.6608
Training Epoch: 0 [4224/50048]	Loss: 4.6271
Training Epoch: 0 [4352/50048]	Loss: 4.5941
Training Epoch: 0 [4480/50048]	Loss: 4.4548
Training Epoch: 0 [4608/50048]	Loss: 4.5075
Training Epoch: 0 [4736/50048]	Loss: 4.5760
Training Epoch: 0 [4864/50048]	Loss: 4.5045
Training Epoch: 0 [4992/50048]	Loss: 4.5593
Training Epoch: 0 [5120/50048]	Loss: 4.4718
Training Epoch: 0 [5248/50048]	Loss: 4.4502
Training Epoch: 0 [5376/50048]	Loss: 4.3189
Training Epoch: 0 [5504/50048]	Loss: 4.3760
Training Epoch: 0 [5632/50048]	Loss: 4.4780
Training Epoch: 0 [5760/50048]	Loss: 4.4930
Training Epoch: 0 [5888/50048]	Loss: 4.3727
Training Epoch: 0 [6016/50048]	Loss: 4.2317
Training Epoch: 0 [6144/50048]	Loss: 4.1406
Training Epoch: 0 [6272/50048]	Loss: 4.5206
Training Epoch: 0 [6400/50048]	Loss: 4.2355
Training Epoch: 0 [6528/50048]	Loss: 4.1903
Training Epoch: 0 [6656/50048]	Loss: 4.5160
Training Epoch: 0 [6784/50048]	Loss: 4.3627
Training Epoch: 0 [6912/50048]	Loss: 4.4057
Training Epoch: 0 [7040/50048]	Loss: 4.2524
Training Epoch: 0 [7168/50048]	Loss: 4.2214
Training Epoch: 0 [7296/50048]	Loss: 4.3147
Training Epoch: 0 [7424/50048]	Loss: 4.1537
Training Epoch: 0 [7552/50048]	Loss: 4.1621
Training Epoch: 0 [7680/50048]	Loss: 4.2802
Training Epoch: 0 [7808/50048]	Loss: 4.2957
Training Epoch: 0 [7936/50048]	Loss: 4.3204
Training Epoch: 0 [8064/50048]	Loss: 4.1257
Training Epoch: 0 [8192/50048]	Loss: 4.1397
Training Epoch: 0 [8320/50048]	Loss: 4.2549
Training Epoch: 0 [8448/50048]	Loss: 4.2127
Training Epoch: 0 [8576/50048]	Loss: 4.0805
Training Epoch: 0 [8704/50048]	Loss: 4.2099
Training Epoch: 0 [8832/50048]	Loss: 4.1510
Training Epoch: 0 [8960/50048]	Loss: 4.2591
Training Epoch: 0 [9088/50048]	Loss: 4.1692
Training Epoch: 0 [9216/50048]	Loss: 4.1973
Training Epoch: 0 [9344/50048]	Loss: 4.1161
Training Epoch: 0 [9472/50048]	Loss: 4.1738
Training Epoch: 0 [9600/50048]	Loss: 4.1186
Training Epoch: 0 [9728/50048]	Loss: 4.1971
Training Epoch: 0 [9856/50048]	Loss: 4.1241
Training Epoch: 0 [9984/50048]	Loss: 4.1607
Training Epoch: 0 [10112/50048]	Loss: 4.0933
Training Epoch: 0 [10240/50048]	Loss: 4.3335
Training Epoch: 0 [10368/50048]	Loss: 4.3118
Training Epoch: 0 [10496/50048]	Loss: 4.0878
Training Epoch: 0 [10624/50048]	Loss: 4.0793
Training Epoch: 0 [10752/50048]	Loss: 4.0503
Training Epoch: 0 [10880/50048]	Loss: 4.1787
Training Epoch: 0 [11008/50048]	Loss: 3.9544
Training Epoch: 0 [11136/50048]	Loss: 4.1452
Training Epoch: 0 [11264/50048]	Loss: 4.1281
Training Epoch: 0 [11392/50048]	Loss: 4.1394
Training Epoch: 0 [11520/50048]	Loss: 4.0079
Training Epoch: 0 [11648/50048]	Loss: 4.1274
Training Epoch: 0 [11776/50048]	Loss: 4.0774
Training Epoch: 0 [11904/50048]	Loss: 4.0958
Training Epoch: 0 [12032/50048]	Loss: 4.0371
Training Epoch: 0 [12160/50048]	Loss: 4.0908
Training Epoch: 0 [12288/50048]	Loss: 3.9455
Training Epoch: 0 [12416/50048]	Loss: 4.2139
Training Epoch: 0 [12544/50048]	Loss: 3.9468
Training Epoch: 0 [12672/50048]	Loss: 4.0982
Training Epoch: 0 [12800/50048]	Loss: 4.0104
Training Epoch: 0 [12928/50048]	Loss: 4.0028
Training Epoch: 0 [13056/50048]	Loss: 3.9418
Training Epoch: 0 [13184/50048]	Loss: 4.1189
Training Epoch: 0 [13312/50048]	Loss: 4.0737
Training Epoch: 0 [13440/50048]	Loss: 4.0605
Training Epoch: 0 [13568/50048]	Loss: 4.1159
Training Epoch: 0 [13696/50048]	Loss: 3.8546
Training Epoch: 0 [13824/50048]	Loss: 4.1333
Training Epoch: 0 [13952/50048]	Loss: 4.0633
Training Epoch: 0 [14080/50048]	Loss: 3.9000
Training Epoch: 0 [14208/50048]	Loss: 4.1803
Training Epoch: 0 [14336/50048]	Loss: 4.0880
Training Epoch: 0 [14464/50048]	Loss: 4.0351
Training Epoch: 0 [14592/50048]	Loss: 4.0483
Training Epoch: 0 [14720/50048]	Loss: 3.9050
Training Epoch: 0 [14848/50048]	Loss: 3.8163
Training Epoch: 0 [14976/50048]	Loss: 4.0054
Training Epoch: 0 [15104/50048]	Loss: 3.9512
Training Epoch: 0 [15232/50048]	Loss: 4.1210
Training Epoch: 0 [15360/50048]	Loss: 4.2034
Training Epoch: 0 [15488/50048]	Loss: 4.0089
Training Epoch: 0 [15616/50048]	Loss: 4.0494
Training Epoch: 0 [15744/50048]	Loss: 4.0069
Training Epoch: 0 [15872/50048]	Loss: 4.0996
Training Epoch: 0 [16000/50048]	Loss: 3.9917
Training Epoch: 0 [16128/50048]	Loss: 3.9410
Training Epoch: 0 [16256/50048]	Loss: 3.9751
Training Epoch: 0 [16384/50048]	Loss: 3.9150
Training Epoch: 0 [16512/50048]	Loss: 4.0055
Training Epoch: 0 [16640/50048]	Loss: 3.8883
Training Epoch: 0 [16768/50048]	Loss: 3.7915
Training Epoch: 0 [16896/50048]	Loss: 4.1723
Training Epoch: 0 [17024/50048]	Loss: 3.8510
Training Epoch: 0 [17152/50048]	Loss: 3.8628
Training Epoch: 0 [17280/50048]	Loss: 4.1701
Training Epoch: 0 [17408/50048]	Loss: 4.0480
Training Epoch: 0 [17536/50048]	Loss: 3.9855
Training Epoch: 0 [17664/50048]	Loss: 3.8617
Training Epoch: 0 [17792/50048]	Loss: 4.1022
Training Epoch: 0 [17920/50048]	Loss: 4.1321
Training Epoch: 0 [18048/50048]	Loss: 3.9105
Training Epoch: 0 [18176/50048]	Loss: 3.9505
Training Epoch: 0 [18304/50048]	Loss: 4.1899
Training Epoch: 0 [18432/50048]	Loss: 4.0084
Training Epoch: 0 [18560/50048]	Loss: 3.8740
Training Epoch: 0 [18688/50048]	Loss: 3.8338
Training Epoch: 0 [18816/50048]	Loss: 3.9639
Training Epoch: 0 [18944/50048]	Loss: 3.8782
Training Epoch: 0 [19072/50048]	Loss: 4.1162
Training Epoch: 0 [19200/50048]	Loss: 3.9550
Training Epoch: 0 [19328/50048]	Loss: 3.8640
Training Epoch: 0 [19456/50048]	Loss: 3.7703
Training Epoch: 0 [19584/50048]	Loss: 3.9039
Training Epoch: 0 [19712/50048]	Loss: 3.9235
Training Epoch: 0 [19840/50048]	Loss: 3.9912
Training Epoch: 0 [19968/50048]	Loss: 4.1137
Training Epoch: 0 [20096/50048]	Loss: 3.8634
Training Epoch: 0 [20224/50048]	Loss: 3.8384
Training Epoch: 0 [20352/50048]	Loss: 3.7017
Training Epoch: 0 [20480/50048]	Loss: 4.0654
Training Epoch: 0 [20608/50048]	Loss: 3.8269
Training Epoch: 0 [20736/50048]	Loss: 3.8362
Training Epoch: 0 [20864/50048]	Loss: 3.7400
Training Epoch: 0 [20992/50048]	Loss: 3.9055
Training Epoch: 0 [21120/50048]	Loss: 4.0616
Training Epoch: 0 [21248/50048]	Loss: 4.1875
Training Epoch: 0 [21376/50048]	Loss: 3.9449
Training Epoch: 0 [21504/50048]	Loss: 3.8133
Training Epoch: 0 [21632/50048]	Loss: 3.9018
Training Epoch: 0 [21760/50048]	Loss: 3.9698
Training Epoch: 0 [21888/50048]	Loss: 4.0008
Training Epoch: 0 [22016/50048]	Loss: 3.9593
Training Epoch: 0 [22144/50048]	Loss: 3.9891
Training Epoch: 0 [22272/50048]	Loss: 3.7324
Training Epoch: 0 [22400/50048]	Loss: 3.9677
Training Epoch: 0 [22528/50048]	Loss: 3.9396
Training Epoch: 0 [22656/50048]	Loss: 3.7728
Training Epoch: 0 [22784/50048]	Loss: 3.8333
Training Epoch: 0 [22912/50048]	Loss: 3.7433
Training Epoch: 0 [23040/50048]	Loss: 3.8733
Training Epoch: 0 [23168/50048]	Loss: 3.9173
Training Epoch: 0 [23296/50048]	Loss: 3.9343
Training Epoch: 0 [23424/50048]	Loss: 3.9176
Training Epoch: 0 [23552/50048]	Loss: 3.8112
Training Epoch: 0 [23680/50048]	Loss: 3.8833
Training Epoch: 0 [23808/50048]	Loss: 3.8359
Training Epoch: 0 [23936/50048]	Loss: 3.8943
Training Epoch: 0 [24064/50048]	Loss: 3.9410
Training Epoch: 0 [24192/50048]	Loss: 3.7903
Training Epoch: 0 [24320/50048]	Loss: 3.8305
Training Epoch: 0 [24448/50048]	Loss: 3.6527
Training Epoch: 0 [24576/50048]	Loss: 3.9585
Training Epoch: 0 [24704/50048]	Loss: 3.9631
Training Epoch: 0 [24832/50048]	Loss: 4.0625
Training Epoch: 0 [24960/50048]	Loss: 3.8803
Training Epoch: 0 [25088/50048]	Loss: 3.7320
Training Epoch: 0 [25216/50048]	Loss: 3.8110
Training Epoch: 0 [25344/50048]	Loss: 3.8493
Training Epoch: 0 [25472/50048]	Loss: 3.8035
Training Epoch: 0 [25600/50048]	Loss: 3.9448
Training Epoch: 0 [25728/50048]	Loss: 3.7735
Training Epoch: 0 [25856/50048]	Loss: 3.7621
Training Epoch: 0 [25984/50048]	Loss: 3.7637
Training Epoch: 0 [26112/50048]	Loss: 3.7857
Training Epoch: 0 [26240/50048]	Loss: 4.0532
Training Epoch: 0 [26368/50048]	Loss: 3.8215
Training Epoch: 0 [26496/50048]	Loss: 3.8255
Training Epoch: 0 [26624/50048]	Loss: 3.7762
Training Epoch: 0 [26752/50048]	Loss: 3.9225
Training Epoch: 0 [26880/50048]	Loss: 3.8223
Training Epoch: 0 [27008/50048]	Loss: 3.7100
Training Epoch: 0 [27136/50048]	Loss: 3.7244
Training Epoch: 0 [27264/50048]	Loss: 3.8625
Training Epoch: 0 [27392/50048]	Loss: 3.9144
Training Epoch: 0 [27520/50048]	Loss: 3.8306
Training Epoch: 0 [27648/50048]	Loss: 3.7571
Training Epoch: 0 [27776/50048]	Loss: 3.8236
Training Epoch: 0 [27904/50048]	Loss: 3.9855
Training Epoch: 0 [28032/50048]	Loss: 3.8091
Training Epoch: 0 [28160/50048]	Loss: 3.8198
Training Epoch: 0 [28288/50048]	Loss: 3.7200
Training Epoch: 0 [28416/50048]	Loss: 3.6278
Training Epoch: 0 [28544/50048]	Loss: 3.7787
Training Epoch: 0 [28672/50048]	Loss: 3.7546
Training Epoch: 0 [28800/50048]	Loss: 3.8202
Training Epoch: 0 [28928/50048]	Loss: 3.7754
Training Epoch: 0 [29056/50048]	Loss: 3.8308
Training Epoch: 0 [29184/50048]	Loss: 3.8851
Training Epoch: 0 [29312/50048]	Loss: 3.7088
Training Epoch: 0 [29440/50048]	Loss: 3.9265
Training Epoch: 0 [29568/50048]	Loss: 3.8045
Training Epoch: 0 [29696/50048]	Loss: 3.7515
Training Epoch: 0 [29824/50048]	Loss: 3.8619
Training Epoch: 0 [29952/50048]	Loss: 3.9230
Training Epoch: 0 [30080/50048]	Loss: 3.7921
Training Epoch: 0 [30208/50048]	Loss: 4.0114
Training Epoch: 0 [30336/50048]	Loss: 3.9236
Training Epoch: 0 [30464/50048]	Loss: 3.8932
Training Epoch: 0 [30592/50048]	Loss: 3.7973
Training Epoch: 0 [30720/50048]	Loss: 3.6995
Training Epoch: 0 [30848/50048]	Loss: 3.9018
Training Epoch: 0 [30976/50048]	Loss: 3.6537
Training Epoch: 0 [31104/50048]	Loss: 3.7895
Training Epoch: 0 [31232/50048]	Loss: 3.9621
Training Epoch: 0 [31360/50048]	Loss: 3.7220
Training Epoch: 0 [31488/50048]	Loss: 3.9880
Training Epoch: 0 [31616/50048]	Loss: 3.7938
Training Epoch: 0 [31744/50048]	Loss: 3.8243
Training Epoch: 0 [31872/50048]	Loss: 3.7520
Training Epoch: 0 [32000/50048]	Loss: 3.8578
Training Epoch: 0 [32128/50048]	Loss: 3.4868
Training Epoch: 0 [32256/50048]	Loss: 3.9056
Training Epoch: 0 [32384/50048]	Loss: 3.5954
Training Epoch: 0 [32512/50048]	Loss: 3.4482
Training Epoch: 0 [32640/50048]	Loss: 3.8161
Training Epoch: 0 [32768/50048]	Loss: 3.7330
Training Epoch: 0 [32896/50048]	Loss: 3.7965
Training Epoch: 0 [33024/50048]	Loss: 3.7669
Training Epoch: 0 [33152/50048]	Loss: 3.7754
Training Epoch: 0 [33280/50048]	Loss: 3.6146
Training Epoch: 0 [33408/50048]	Loss: 3.6718
Training Epoch: 0 [33536/50048]	Loss: 4.0047
Training Epoch: 0 [33664/50048]	Loss: 3.6337
Training Epoch: 0 [33792/50048]	Loss: 3.5578
Training Epoch: 0 [33920/50048]	Loss: 3.6555
Training Epoch: 0 [34048/50048]	Loss: 3.8300
Training Epoch: 0 [34176/50048]	Loss: 3.8891
Training Epoch: 0 [34304/50048]	Loss: 3.6713
Training Epoch: 0 [34432/50048]	Loss: 3.7775
Training Epoch: 0 [34560/50048]	Loss: 3.7965
Training Epoch: 0 [34688/50048]	Loss: 3.8608
Training Epoch: 0 [34816/50048]	Loss: 3.9565
Training Epoch: 0 [34944/50048]	Loss: 3.7768
Training Epoch: 0 [35072/50048]	Loss: 3.6871
Training Epoch: 0 [35200/50048]	Loss: 3.5579
Training Epoch: 0 [35328/50048]	Loss: 3.6942
Training Epoch: 0 [35456/50048]	Loss: 3.6639
Training Epoch: 0 [35584/50048]	Loss: 3.7456
Training Epoch: 0 [35712/50048]	Loss: 3.6538
Training Epoch: 0 [35840/50048]	Loss: 3.7356
Training Epoch: 0 [35968/50048]	Loss: 3.7553
Training Epoch: 0 [36096/50048]	Loss: 3.6186
Training Epoch: 0 [36224/50048]	Loss: 3.6304
Training Epoch: 0 [36352/50048]	Loss: 3.6131
Training Epoch: 0 [36480/50048]	Loss: 3.5909
Training Epoch: 0 [36608/50048]	Loss: 3.9389
Training Epoch: 0 [36736/50048]	Loss: 3.7507
Training Epoch: 0 [36864/50048]	Loss: 3.5812
Training Epoch: 0 [36992/50048]	Loss: 3.6241
Training Epoch: 0 [37120/50048]	Loss: 3.6753
Training Epoch: 0 [37248/50048]	Loss: 3.5266
Training Epoch: 0 [37376/50048]	Loss: 3.6717
Training Epoch: 0 [37504/50048]	Loss: 3.8097
Training Epoch: 0 [37632/50048]	Loss: 3.7440
Training Epoch: 0 [37760/50048]	Loss: 3.5509
Training Epoch: 0 [37888/50048]	Loss: 3.9216
Training Epoch: 0 [38016/50048]	Loss: 3.6774
Training Epoch: 0 [38144/50048]	Loss: 3.6891
Training Epoch: 0 [38272/50048]	Loss: 3.8611
Training Epoch: 0 [38400/50048]	Loss: 3.6701
Training Epoch: 0 [38528/50048]	Loss: 3.8370
Training Epoch: 0 [38656/50048]	Loss: 3.5310
Training Epoch: 0 [38784/50048]	Loss: 3.5021
Training Epoch: 0 [38912/50048]	Loss: 3.6852
Training Epoch: 0 [39040/50048]	Loss: 3.7330
Training Epoch: 0 [39168/50048]	Loss: 3.7800
Training Epoch: 0 [39296/50048]	Loss: 3.6242
Training Epoch: 0 [39424/50048]	Loss: 3.6364
Training Epoch: 0 [39552/50048]	Loss: 3.3914
Training Epoch: 0 [39680/50048]	Loss: 3.7045
Training Epoch: 0 [39808/50048]	Loss: 3.5825
Training Epoch: 0 [39936/50048]	Loss: 3.8473
Training Epoch: 0 [40064/50048]	Loss: 3.5682
Training Epoch: 0 [40192/50048]	Loss: 3.4905
Training Epoch: 0 [40320/50048]	Loss: 3.6172
Training Epoch: 0 [40448/50048]	Loss: 3.6606
Training Epoch: 0 [40576/50048]	Loss: 3.5653
Training Epoch: 0 [40704/50048]	Loss: 3.6526
Training Epoch: 0 [40832/50048]	Loss: 3.6414
Training Epoch: 0 [40960/50048]	Loss: 3.6722
Training Epoch: 0 [41088/50048]	Loss: 3.6269
Training Epoch: 0 [41216/50048]	Loss: 3.4335
Training Epoch: 0 [41344/50048]	Loss: 3.6982
Training Epoch: 0 [41472/50048]	Loss: 3.5990
Training Epoch: 0 [41600/50048]	Loss: 3.4379
Training Epoch: 0 [41728/50048]	Loss: 3.5061
Training Epoch: 0 [41856/50048]	Loss: 3.7298
Training Epoch: 0 [41984/50048]	Loss: 3.7049
Training Epoch: 0 [42112/50048]	Loss: 3.7080
Training Epoch: 0 [42240/50048]	Loss: 3.7594
Training Epoch: 0 [42368/50048]	Loss: 3.5412
Training Epoch: 0 [42496/50048]	Loss: 3.8703
Training Epoch: 0 [42624/50048]	Loss: 3.7356
Training Epoch: 0 [42752/50048]	Loss: 3.7328
Training Epoch: 0 [42880/50048]	Loss: 3.7187
Training Epoch: 0 [43008/50048]	Loss: 3.5079
Training Epoch: 0 [43136/50048]	Loss: 3.4856
Training Epoch: 0 [43264/50048]	Loss: 3.6480
Training Epoch: 0 [43392/50048]	Loss: 3.4769
Training Epoch: 0 [43520/50048]	Loss: 3.7325
Training Epoch: 0 [43648/50048]	Loss: 3.3317
Training Epoch: 0 [43776/50048]	Loss: 3.4004
Training Epoch: 0 [43904/50048]	Loss: 3.7368
Training Epoch: 0 [44032/50048]	Loss: 3.4352
Training Epoch: 0 [44160/50048]	Loss: 3.4945
Training Epoch: 0 [44288/50048]	Loss: 3.5247
Training Epoch: 0 [44416/50048]	Loss: 3.5779
Training Epoch: 0 [44544/50048]	Loss: 3.5755
Training Epoch: 0 [44672/50048]	Loss: 3.8242
Training Epoch: 0 [44800/50048]	Loss: 3.5564
Training Epoch: 0 [44928/50048]	Loss: 3.4554
Training Epoch: 0 [45056/50048]	Loss: 3.6136
Training Epoch: 0 [45184/50048]	Loss: 4.0295
Training Epoch: 0 [45312/50048]	Loss: 3.7408
Training Epoch: 0 [45440/50048]	Loss: 3.5739
Training Epoch: 0 [45568/50048]	Loss: 3.5139
Training Epoch: 0 [45696/50048]	Loss: 3.3889
Training Epoch: 0 [45824/50048]	Loss: 3.7794
Training Epoch: 0 [45952/50048]	Loss: 3.5802
Training Epoch: 0 [46080/50048]	Loss: 3.5753
Training Epoch: 0 [46208/50048]	Loss: 3.5404
Training Epoch: 0 [46336/50048]	Loss: 3.4742
Training Epoch: 0 [46464/50048]	Loss: 3.4845
Training Epoch: 0 [46592/50048]	Loss: 3.6185
Training Epoch: 0 [46720/50048]	Loss: 3.5509
2022-12-06 08:39:50,772 [ZeusDataLoader(train)] train epoch 1 done: time=87.00 energy=10512.48
2022-12-06 08:39:50,774 [ZeusDataLoader(eval)] Epoch 1 begin.
Training Epoch: 0 [46848/50048]	Loss: 3.5481
Training Epoch: 0 [46976/50048]	Loss: 3.8246
Training Epoch: 0 [47104/50048]	Loss: 3.6049
Training Epoch: 0 [47232/50048]	Loss: 3.4596
Training Epoch: 0 [47360/50048]	Loss: 3.2520
Training Epoch: 0 [47488/50048]	Loss: 3.4485
Training Epoch: 0 [47616/50048]	Loss: 3.5733
Training Epoch: 0 [47744/50048]	Loss: 3.8381
Training Epoch: 0 [47872/50048]	Loss: 3.5767
Training Epoch: 0 [48000/50048]	Loss: 3.6088
Training Epoch: 0 [48128/50048]	Loss: 3.8191
Training Epoch: 0 [48256/50048]	Loss: 3.5775
Training Epoch: 0 [48384/50048]	Loss: 3.5965
Training Epoch: 0 [48512/50048]	Loss: 3.6560
Training Epoch: 0 [48640/50048]	Loss: 3.5374
Training Epoch: 0 [48768/50048]	Loss: 3.8113
Training Epoch: 0 [48896/50048]	Loss: 3.5001
Training Epoch: 0 [49024/50048]	Loss: 3.6438
Training Epoch: 0 [49152/50048]	Loss: 3.5678
Training Epoch: 0 [49280/50048]	Loss: 3.4475
Training Epoch: 0 [49408/50048]	Loss: 3.2841
Training Epoch: 0 [49536/50048]	Loss: 3.5825
Training Epoch: 0 [49664/50048]	Loss: 3.5304
Training Epoch: 0 [49792/50048]	Loss: 3.4514
Training Epoch: 0 [49920/50048]	Loss: 3.6346
Training Epoch: 0 [50048/50048]	Loss: 3.6759
2022-12-06 13:39:54.502 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-06 08:39:54,519 [ZeusDataLoader(eval)] eval epoch 1 done: time=3.74 energy=451.11
2022-12-06 08:39:54,520 [ZeusDataLoader(train)] Up to epoch 1: time=90.74, energy=10963.58, cost=13421.27
2022-12-06 08:39:54,520 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.80 energy=10798.01
2022-12-06 08:39:54,520 [ZeusDataLoader(train)] Expected next epoch: time=180.54, energy=21761.60, cost=26677.65
2022-12-06 08:39:54,521 [ZeusDataLoader(train)] Epoch 2 begin.
Validation Epoch: 0, Average loss: 0.0298, Accuracy: 0.1279
2022-12-06 08:39:54,711 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-06 08:39:54,712 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-06 13:39:54.714 [ZeusMonitor] Monitor started.
2022-12-06 13:39:54.714 [ZeusMonitor] Running indefinitely. 2022-12-06 13:39:54.714 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-06 13:39:54.714 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs128+adadelta+lr0.1+tm0.8+me100+x100+eta0.5+beta2.0+2022120603371670315835/bs128+e2+gpu0.power.log
Training Epoch: 1 [128/50048]	Loss: 3.6292
Training Epoch: 1 [256/50048]	Loss: 3.5163
Training Epoch: 1 [384/50048]	Loss: 3.3501
Training Epoch: 1 [512/50048]	Loss: 3.6837
Training Epoch: 1 [640/50048]	Loss: 3.3432
Training Epoch: 1 [768/50048]	Loss: 3.4934
Training Epoch: 1 [896/50048]	Loss: 3.4567
Training Epoch: 1 [1024/50048]	Loss: 3.4786
Training Epoch: 1 [1152/50048]	Loss: 3.4776
Training Epoch: 1 [1280/50048]	Loss: 3.6035
Training Epoch: 1 [1408/50048]	Loss: 3.6220
Training Epoch: 1 [1536/50048]	Loss: 3.7242
Training Epoch: 1 [1664/50048]	Loss: 3.3992
Training Epoch: 1 [1792/50048]	Loss: 3.2165
Training Epoch: 1 [1920/50048]	Loss: 3.3870
Training Epoch: 1 [2048/50048]	Loss: 3.4930
Training Epoch: 1 [2176/50048]	Loss: 3.8018
Training Epoch: 1 [2304/50048]	Loss: 3.5120
Training Epoch: 1 [2432/50048]	Loss: 3.6320
Training Epoch: 1 [2560/50048]	Loss: 3.3448
Training Epoch: 1 [2688/50048]	Loss: 3.4696
Training Epoch: 1 [2816/50048]	Loss: 3.3883
Training Epoch: 1 [2944/50048]	Loss: 3.4532
Training Epoch: 1 [3072/50048]	Loss: 3.3879
Training Epoch: 1 [3200/50048]	Loss: 3.5637
Training Epoch: 1 [3328/50048]	Loss: 3.4357
Training Epoch: 1 [3456/50048]	Loss: 3.3559
Training Epoch: 1 [3584/50048]	Loss: 3.5620
Training Epoch: 1 [3712/50048]	Loss: 3.5306
Training Epoch: 1 [3840/50048]	Loss: 3.5650
Training Epoch: 1 [3968/50048]	Loss: 3.4252
Training Epoch: 1 [4096/50048]	Loss: 3.4898
Training Epoch: 1 [4224/50048]	Loss: 3.5654
Training Epoch: 1 [4352/50048]	Loss: 3.4612
Training Epoch: 1 [4480/50048]	Loss: 3.1289
Training Epoch: 1 [4608/50048]	Loss: 3.3351
Training Epoch: 1 [4736/50048]	Loss: 3.5675
Training Epoch: 1 [4864/50048]	Loss: 3.4335
Training Epoch: 1 [4992/50048]	Loss: 3.6212
Training Epoch: 1 [5120/50048]	Loss: 3.3463
Training Epoch: 1 [5248/50048]	Loss: 3.5459
Training Epoch: 1 [5376/50048]	Loss: 3.4280
Training Epoch: 1 [5504/50048]	Loss: 3.3514
Training Epoch: 1 [5632/50048]	Loss: 3.3872
Training Epoch: 1 [5760/50048]	Loss: 3.5732
Training Epoch: 1 [5888/50048]	Loss: 3.4206
Training Epoch: 1 [6016/50048]	Loss: 3.5262
Training Epoch: 1 [6144/50048]	Loss: 3.5258
Training Epoch: 1 [6272/50048]	Loss: 3.2805
Training Epoch: 1 [6400/50048]	Loss: 3.5928
Training Epoch: 1 [6528/50048]	Loss: 3.4663
Training Epoch: 1 [6656/50048]	Loss: 3.3295
Training Epoch: 1 [6784/50048]	Loss: 3.4655
Training Epoch: 1 [6912/50048]	Loss: 3.2741
Training Epoch: 1 [7040/50048]	Loss: 3.4893
Training Epoch: 1 [7168/50048]	Loss: 3.4974
Training Epoch: 1 [7296/50048]	Loss: 3.4906
Training Epoch: 1 [7424/50048]	Loss: 3.4407
Training Epoch: 1 [7552/50048]	Loss: 3.4766
Training Epoch: 1 [7680/50048]	Loss: 3.4654
Training Epoch: 1 [7808/50048]	Loss: 3.4153
Training Epoch: 1 [7936/50048]	Loss: 3.4246
Training Epoch: 1 [8064/50048]	Loss: 3.2001
Training Epoch: 1 [8192/50048]	Loss: 3.3156
Training Epoch: 1 [8320/50048]	Loss: 3.5821
Training Epoch: 1 [8448/50048]	Loss: 3.4922
Training Epoch: 1 [8576/50048]	Loss: 3.3443
Training Epoch: 1 [8704/50048]	Loss: 3.5288
Training Epoch: 1 [8832/50048]	Loss: 3.5030
Training Epoch: 1 [8960/50048]	Loss: 3.4024
Training Epoch: 1 [9088/50048]	Loss: 3.5539
Training Epoch: 1 [9216/50048]	Loss: 3.8549
Training Epoch: 1 [9344/50048]	Loss: 3.4076
Training Epoch: 1 [9472/50048]	Loss: 3.5587
Training Epoch: 1 [9600/50048]	Loss: 3.7213
Training Epoch: 1 [9728/50048]	Loss: 3.3473
Training Epoch: 1 [9856/50048]	Loss: 3.3210
Training Epoch: 1 [9984/50048]	Loss: 3.3936
Training Epoch: 1 [10112/50048]	Loss: 3.3324
Training Epoch: 1 [10240/50048]	Loss: 3.6627
Training Epoch: 1 [10368/50048]	Loss: 3.4025
Training Epoch: 1 [10496/50048]	Loss: 3.5927
Training Epoch: 1 [10624/50048]	Loss: 3.4291
Training Epoch: 1 [10752/50048]	Loss: 3.4066
Traceback (most recent call last):
  File "/workspace/zeus/examples/cifar100/train_lr.py", line 256, in <module>
    main(parse_args())
  File "/workspace/zeus/examples/cifar100/train_lr.py", line 188, in main
    train(train_loader, model, criterion, optimizer, epoch, args)
  File "/workspace/zeus/examples/cifar100/train_lr.py", line 212, in train
    optimizer.step()
  File "/root/.local/miniconda3/lib/python3.9/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/root/.local/miniconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/root/.local/miniconda3/lib/python3.9/site-packages/torch/optim/adadelta.py", line 105, in step
    F.adadelta(params_with_grad,
  File "/root/.local/miniconda3/lib/python3.9/site-packages/torch/optim/_functional.py", line 203, in adadelta
    delta = acc_delta.add(eps).sqrt_().div_(std).mul_(grad)
KeyboardInterrupt
2022-12-06 08:40:13,472 [ZeusDataLoader(train)] [GPU_0] Stopped Zeus monitor.
