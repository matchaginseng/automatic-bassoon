[Power Profiler] Batch sizes: [8, 16, 32, 64, 128, 256, 512]
[Power Profiler] Learning rates factors: [0.8, 1.0, 1.2]

[Power Profiler] with batch size 8 and learning rate 0.0008
[run job] Launching job with BS 8: and LR: 0.0008 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs8+lr0.00080+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '0.0008', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0008', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs8+lr0.00080+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.0008
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.8076
Training Epoch: 0 [24/50000]	Loss: 4.7131
Training Epoch: 0 [32/50000]	Loss: 4.7865
Training Epoch: 0 [40/50000]	Loss: 4.7468
Training Epoch: 0 [48/50000]	Loss: 5.6070
Training Epoch: 0 [56/50000]	Loss: 5.4042
Training Epoch: 0 [64/50000]	Loss: 4.9400
Training Epoch: 0 [72/50000]	Loss: 5.2062
Training Epoch: 0 [80/50000]	Loss: 5.0867
Training Epoch: 0 [88/50000]	Loss: 4.8710
Training Epoch: 0 [96/50000]	Loss: 5.9382
Training Epoch: 0 [104/50000]	Loss: 5.8081
Training Epoch: 0 [112/50000]	Loss: 5.0300
Training Epoch: 0 [120/50000]	Loss: 5.6352
Training Epoch: 0 [128/50000]	Loss: 5.8511
Training Epoch: 0 [136/50000]	Loss: 5.6285
Training Epoch: 0 [144/50000]	Loss: 5.0816
Training Epoch: 0 [152/50000]	Loss: 5.1341
Training Epoch: 0 [160/50000]	Loss: 6.0506
Training Epoch: 0 [168/50000]	Loss: 5.4547
Training Epoch: 0 [176/50000]	Loss: 5.0467
Training Epoch: 0 [184/50000]	Loss: 5.3384
Training Epoch: 0 [192/50000]	Loss: 4.4943
Training Epoch: 0 [200/50000]	Loss: 5.7416
Training Epoch: 0 [208/50000]	Loss: 5.3566
Training Epoch: 0 [216/50000]	Loss: 4.8902
Training Epoch: 0 [224/50000]	Loss: 5.2683
Training Epoch: 0 [232/50000]	Loss: 4.7893
Training Epoch: 0 [240/50000]	Loss: 5.2127
Training Epoch: 0 [248/50000]	Loss: 5.0533
Training Epoch: 0 [256/50000]	Loss: 5.3468
Training Epoch: 0 [264/50000]	Loss: 4.7573
Training Epoch: 0 [272/50000]	Loss: 5.5712
Training Epoch: 0 [280/50000]	Loss: 5.0370
Training Epoch: 0 [288/50000]	Loss: 5.3492
Training Epoch: 0 [296/50000]	Loss: 5.4574
Training Epoch: 0 [304/50000]	Loss: 5.1923
Training Epoch: 0 [312/50000]	Loss: 4.6964
Training Epoch: 0 [320/50000]	Loss: 5.0591
Training Epoch: 0 [328/50000]	Loss: 5.2923
Training Epoch: 0 [336/50000]	Loss: 4.9941
Training Epoch: 0 [344/50000]	Loss: 4.4681
Training Epoch: 0 [352/50000]	Loss: 4.7942
Training Epoch: 0 [360/50000]	Loss: 5.5513
Training Epoch: 0 [368/50000]	Loss: 5.0041
Training Epoch: 0 [376/50000]	Loss: 4.9497
Training Epoch: 0 [384/50000]	Loss: 4.5779
Training Epoch: 0 [392/50000]	Loss: 4.7939
Training Epoch: 0 [400/50000]	Loss: 4.5351
Profile done with power limit 175W
epoch 1 train time consumed: 3.39s
Validation Epoch: 0, Average loss: 0.6282, Accuracy: 0.0102
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 175000, 'lr': 0.0008, 'energy': 97.16369342745973, 'time': 2.203665111999726, 'accuracy': 0.0102, 'total_cost': 5.879976823128256}
[run job] Launching job with BS 8: and LR: 0.0008 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs8+lr0.00080+pl150', 'ZEUS_COST_THRESH': '11.759953646256513', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '0.0008', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0008', '--power_limit', '150']
[run job] cost_ub=11.759953646256513
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs8+lr0.00080+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.0008
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.8076
Training Epoch: 0 [24/50000]	Loss: 4.7132
Training Epoch: 0 [32/50000]	Loss: 4.7872
Training Epoch: 0 [40/50000]	Loss: 4.7519
Training Epoch: 0 [48/50000]	Loss: 5.6007
Training Epoch: 0 [56/50000]	Loss: 5.3913
Training Epoch: 0 [64/50000]	Loss: 4.9996
Training Epoch: 0 [72/50000]	Loss: 5.2226
Training Epoch: 0 [80/50000]	Loss: 5.1411
Training Epoch: 0 [88/50000]	Loss: 4.8663
Training Epoch: 0 [96/50000]	Loss: 5.8905
Training Epoch: 0 [104/50000]	Loss: 5.7812
Training Epoch: 0 [112/50000]	Loss: 5.0241
Training Epoch: 0 [120/50000]	Loss: 5.5913
Training Epoch: 0 [128/50000]	Loss: 5.8400
Training Epoch: 0 [136/50000]	Loss: 5.6817
Training Epoch: 0 [144/50000]	Loss: 5.0989
Training Epoch: 0 [152/50000]	Loss: 5.0916
Training Epoch: 0 [160/50000]	Loss: 6.0494
Training Epoch: 0 [168/50000]	Loss: 5.5118
Training Epoch: 0 [176/50000]	Loss: 5.0247
Training Epoch: 0 [184/50000]	Loss: 5.3992
Training Epoch: 0 [192/50000]	Loss: 4.5605
Training Epoch: 0 [200/50000]	Loss: 5.6940
Training Epoch: 0 [208/50000]	Loss: 5.3809
Training Epoch: 0 [216/50000]	Loss: 4.8371
Training Epoch: 0 [224/50000]	Loss: 5.2518
Training Epoch: 0 [232/50000]	Loss: 4.7404
Training Epoch: 0 [240/50000]	Loss: 5.1986
Training Epoch: 0 [248/50000]	Loss: 5.0975
Training Epoch: 0 [256/50000]	Loss: 5.3149
Training Epoch: 0 [264/50000]	Loss: 4.8898
Training Epoch: 0 [272/50000]	Loss: 5.4845
Training Epoch: 0 [280/50000]	Loss: 5.1365
Training Epoch: 0 [288/50000]	Loss: 5.3126
Training Epoch: 0 [296/50000]	Loss: 5.4753
Training Epoch: 0 [304/50000]	Loss: 5.2339
Training Epoch: 0 [312/50000]	Loss: 4.6479
Training Epoch: 0 [320/50000]	Loss: 4.9944
Training Epoch: 0 [328/50000]	Loss: 5.2431
Training Epoch: 0 [336/50000]	Loss: 5.0412
Training Epoch: 0 [344/50000]	Loss: 4.4490
Training Epoch: 0 [352/50000]	Loss: 4.8363
Training Epoch: 0 [360/50000]	Loss: 5.5478
Training Epoch: 0 [368/50000]	Loss: 4.9853
Training Epoch: 0 [376/50000]	Loss: 4.8484
Training Epoch: 0 [384/50000]	Loss: 4.6242
Training Epoch: 0 [392/50000]	Loss: 4.8061
Training Epoch: 0 [400/50000]	Loss: 4.5616
Profile done with power limit 150W
epoch 1 train time consumed: 3.26s
Validation Epoch: 0, Average loss: 0.6187, Accuracy: 0.0103
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 150000, 'lr': 0.0008, 'energy': 94.38656865078978, 'time': 2.280025445999854, 'accuracy': 0.0103, 'total_cost': 5.963186711984343}
[run job] Launching job with BS 8: and LR: 0.0008 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs8+lr0.00080+pl125', 'ZEUS_COST_THRESH': '11.759953646256513', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '0.0008', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0008', '--power_limit', '125']
[run job] cost_ub=11.759953646256513
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs8+lr0.00080+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.0008
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.8076
Training Epoch: 0 [24/50000]	Loss: 4.7134
Training Epoch: 0 [32/50000]	Loss: 4.7920
Training Epoch: 0 [40/50000]	Loss: 4.7738
Training Epoch: 0 [48/50000]	Loss: 5.5896
Training Epoch: 0 [56/50000]	Loss: 5.4316
Training Epoch: 0 [64/50000]	Loss: 4.9473
Training Epoch: 0 [72/50000]	Loss: 5.1905
Training Epoch: 0 [80/50000]	Loss: 5.0875
Training Epoch: 0 [88/50000]	Loss: 4.8571
Training Epoch: 0 [96/50000]	Loss: 5.9538
Training Epoch: 0 [104/50000]	Loss: 5.7352
Training Epoch: 0 [112/50000]	Loss: 4.9831
Training Epoch: 0 [120/50000]	Loss: 5.5784
Training Epoch: 0 [128/50000]	Loss: 5.8743
Training Epoch: 0 [136/50000]	Loss: 5.6722
Training Epoch: 0 [144/50000]	Loss: 5.1121
Training Epoch: 0 [152/50000]	Loss: 5.1766
Training Epoch: 0 [160/50000]	Loss: 6.1012
Training Epoch: 0 [168/50000]	Loss: 5.5110
Training Epoch: 0 [176/50000]	Loss: 5.1533
Training Epoch: 0 [184/50000]	Loss: 5.4197
Training Epoch: 0 [192/50000]	Loss: 4.5904
Training Epoch: 0 [200/50000]	Loss: 5.7012
Training Epoch: 0 [208/50000]	Loss: 5.3310
Training Epoch: 0 [216/50000]	Loss: 4.8600
Training Epoch: 0 [224/50000]	Loss: 5.2326
Training Epoch: 0 [232/50000]	Loss: 4.8469
Training Epoch: 0 [240/50000]	Loss: 5.2585
Training Epoch: 0 [248/50000]	Loss: 5.1506
Training Epoch: 0 [256/50000]	Loss: 5.2330
Training Epoch: 0 [264/50000]	Loss: 4.7080
Training Epoch: 0 [272/50000]	Loss: 5.4704
Training Epoch: 0 [280/50000]	Loss: 5.0625
Training Epoch: 0 [288/50000]	Loss: 5.2978
Training Epoch: 0 [296/50000]	Loss: 5.5430
Training Epoch: 0 [304/50000]	Loss: 5.2596
Training Epoch: 0 [312/50000]	Loss: 4.6423
Training Epoch: 0 [320/50000]	Loss: 5.0313
Training Epoch: 0 [328/50000]	Loss: 5.3145
Training Epoch: 0 [336/50000]	Loss: 5.0786
Training Epoch: 0 [344/50000]	Loss: 4.5011
Training Epoch: 0 [352/50000]	Loss: 4.8532
Training Epoch: 0 [360/50000]	Loss: 5.5164
Training Epoch: 0 [368/50000]	Loss: 5.0648
Training Epoch: 0 [376/50000]	Loss: 4.9263
Training Epoch: 0 [384/50000]	Loss: 4.5967
Training Epoch: 0 [392/50000]	Loss: 4.8296
Training Epoch: 0 [400/50000]	Loss: 4.5642
Profile done with power limit 125W
epoch 1 train time consumed: 3.24s
Validation Epoch: 0, Average loss: 0.6212, Accuracy: 0.0123
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 125000, 'lr': 0.0008, 'energy': 95.80669652986165, 'time': 2.261584359000153, 'accuracy': 0.0123, 'total_cost': 4.979286090930375}
[run job] Launching job with BS 8: and LR: 0.0008 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs8+lr0.00080+pl100', 'ZEUS_COST_THRESH': '9.95857218186075', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '0.0008', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0008', '--power_limit', '100']
[run job] cost_ub=9.95857218186075
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs8+lr0.00080+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.0008
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.8076
Training Epoch: 0 [24/50000]	Loss: 4.7135
Training Epoch: 0 [32/50000]	Loss: 4.7898
Training Epoch: 0 [40/50000]	Loss: 4.7507
Training Epoch: 0 [48/50000]	Loss: 5.5799
Training Epoch: 0 [56/50000]	Loss: 5.3848
Training Epoch: 0 [64/50000]	Loss: 4.9524
Training Epoch: 0 [72/50000]	Loss: 5.2169
Training Epoch: 0 [80/50000]	Loss: 5.1398
Training Epoch: 0 [88/50000]	Loss: 4.8168
Training Epoch: 0 [96/50000]	Loss: 5.9021
Training Epoch: 0 [104/50000]	Loss: 5.8241
Training Epoch: 0 [112/50000]	Loss: 5.0447
Training Epoch: 0 [120/50000]	Loss: 5.5762
Training Epoch: 0 [128/50000]	Loss: 5.8255
Training Epoch: 0 [136/50000]	Loss: 5.6419
Training Epoch: 0 [144/50000]	Loss: 5.1030
Training Epoch: 0 [152/50000]	Loss: 5.1925
Training Epoch: 0 [160/50000]	Loss: 6.0223
Training Epoch: 0 [168/50000]	Loss: 5.4935
Training Epoch: 0 [176/50000]	Loss: 5.0833
Training Epoch: 0 [184/50000]	Loss: 5.3608
Training Epoch: 0 [192/50000]	Loss: 4.5491
Training Epoch: 0 [200/50000]	Loss: 5.7129
Training Epoch: 0 [208/50000]	Loss: 5.3186
Training Epoch: 0 [216/50000]	Loss: 4.8746
Training Epoch: 0 [224/50000]	Loss: 5.2254
Training Epoch: 0 [232/50000]	Loss: 4.8643
Training Epoch: 0 [240/50000]	Loss: 5.2540
Training Epoch: 0 [248/50000]	Loss: 5.1325
Training Epoch: 0 [256/50000]	Loss: 5.2393
Training Epoch: 0 [264/50000]	Loss: 4.6967
Training Epoch: 0 [272/50000]	Loss: 5.4903
Training Epoch: 0 [280/50000]	Loss: 5.0657
Training Epoch: 0 [288/50000]	Loss: 5.3090
Training Epoch: 0 [296/50000]	Loss: 5.4188
Training Epoch: 0 [304/50000]	Loss: 5.1872
Training Epoch: 0 [312/50000]	Loss: 4.7029
Training Epoch: 0 [320/50000]	Loss: 5.0680
Training Epoch: 0 [328/50000]	Loss: 5.3017
Training Epoch: 0 [336/50000]	Loss: 5.0940
Training Epoch: 0 [344/50000]	Loss: 4.4846
Training Epoch: 0 [352/50000]	Loss: 4.8182
Training Epoch: 0 [360/50000]	Loss: 5.5224
Training Epoch: 0 [368/50000]	Loss: 5.0457
Training Epoch: 0 [376/50000]	Loss: 4.9301
Training Epoch: 0 [384/50000]	Loss: 4.5968
Training Epoch: 0 [392/50000]	Loss: 4.8466
Training Epoch: 0 [400/50000]	Loss: 4.6071
Profile done with power limit 100W
epoch 1 train time consumed: 3.30s
Validation Epoch: 0, Average loss: 0.6262, Accuracy: 0.0105
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 100000, 'lr': 0.0008, 'energy': 93.66868254859544, 'time': 2.3210549550003634, 'accuracy': 0.0105, 'total_cost': 5.938997875074638}

[Power Profiler] with batch size 8 and learning rate 0.001
[run job] Launching job with BS 8: and LR: 0.001 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs8+lr0.00100+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '0.001', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '0.001', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs8+lr0.00100+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.001
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.8286
Training Epoch: 0 [24/50000]	Loss: 4.7004
Training Epoch: 0 [32/50000]	Loss: 4.8895
Training Epoch: 0 [40/50000]	Loss: 4.8662
Training Epoch: 0 [48/50000]	Loss: 5.8845
Training Epoch: 0 [56/50000]	Loss: 5.7070
Training Epoch: 0 [64/50000]	Loss: 5.1187
Training Epoch: 0 [72/50000]	Loss: 5.3735
Training Epoch: 0 [80/50000]	Loss: 5.3886
Training Epoch: 0 [88/50000]	Loss: 5.1512
Training Epoch: 0 [96/50000]	Loss: 6.2517
Training Epoch: 0 [104/50000]	Loss: 6.1482
Training Epoch: 0 [112/50000]	Loss: 5.2286
Training Epoch: 0 [120/50000]	Loss: 5.8723
Training Epoch: 0 [128/50000]	Loss: 6.0637
Training Epoch: 0 [136/50000]	Loss: 5.7322
Training Epoch: 0 [144/50000]	Loss: 5.1863
Training Epoch: 0 [152/50000]	Loss: 5.2971
Training Epoch: 0 [160/50000]	Loss: 6.1884
Training Epoch: 0 [168/50000]	Loss: 5.5259
Training Epoch: 0 [176/50000]	Loss: 5.1029
Training Epoch: 0 [184/50000]	Loss: 5.5603
Training Epoch: 0 [192/50000]	Loss: 4.5827
Training Epoch: 0 [200/50000]	Loss: 5.9575
Training Epoch: 0 [208/50000]	Loss: 5.5423
Training Epoch: 0 [216/50000]	Loss: 4.8438
Training Epoch: 0 [224/50000]	Loss: 5.3647
Training Epoch: 0 [232/50000]	Loss: 4.8047
Training Epoch: 0 [240/50000]	Loss: 5.2457
Training Epoch: 0 [248/50000]	Loss: 5.0415
Training Epoch: 0 [256/50000]	Loss: 5.3129
Training Epoch: 0 [264/50000]	Loss: 4.8079
Training Epoch: 0 [272/50000]	Loss: 5.7048
Training Epoch: 0 [280/50000]	Loss: 5.2345
Training Epoch: 0 [288/50000]	Loss: 5.4414
Training Epoch: 0 [296/50000]	Loss: 5.4840
Training Epoch: 0 [304/50000]	Loss: 5.1727
Training Epoch: 0 [312/50000]	Loss: 4.7674
Training Epoch: 0 [320/50000]	Loss: 5.1132
Training Epoch: 0 [328/50000]	Loss: 5.3770
Training Epoch: 0 [336/50000]	Loss: 5.0875
Training Epoch: 0 [344/50000]	Loss: 4.6215
Training Epoch: 0 [352/50000]	Loss: 4.9016
Training Epoch: 0 [360/50000]	Loss: 5.5819
Training Epoch: 0 [368/50000]	Loss: 5.0542
Training Epoch: 0 [376/50000]	Loss: 4.8099
Training Epoch: 0 [384/50000]	Loss: 4.5877
Training Epoch: 0 [392/50000]	Loss: 4.8073
Training Epoch: 0 [400/50000]	Loss: 4.6071
Profile done with power limit 175W
epoch 1 train time consumed: 3.21s
Validation Epoch: 0, Average loss: 0.6282, Accuracy: 0.0104
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 175000, 'lr': 0.001, 'energy': 95.65395459295121, 'time': 2.2429998729994622, 'accuracy': 0.0104, 'total_cost': 5.837276786334537}
[run job] Launching job with BS 8: and LR: 0.001 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs8+lr0.00100+pl150', 'ZEUS_COST_THRESH': '11.674553572669074', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '0.001', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '0.001', '--power_limit', '150']
[run job] cost_ub=11.674553572669074
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs8+lr0.00100+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.001
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.8286
Training Epoch: 0 [24/50000]	Loss: 4.7004
Training Epoch: 0 [32/50000]	Loss: 4.8893
Training Epoch: 0 [40/50000]	Loss: 4.8609
Training Epoch: 0 [48/50000]	Loss: 5.9018
Training Epoch: 0 [56/50000]	Loss: 5.7033
Training Epoch: 0 [64/50000]	Loss: 5.1233
Training Epoch: 0 [72/50000]	Loss: 5.3594
Training Epoch: 0 [80/50000]	Loss: 5.3791
Training Epoch: 0 [88/50000]	Loss: 5.1407
Training Epoch: 0 [96/50000]	Loss: 6.2433
Training Epoch: 0 [104/50000]	Loss: 6.1049
Training Epoch: 0 [112/50000]	Loss: 5.2521
Training Epoch: 0 [120/50000]	Loss: 5.8095
Training Epoch: 0 [128/50000]	Loss: 6.0795
Training Epoch: 0 [136/50000]	Loss: 5.8076
Training Epoch: 0 [144/50000]	Loss: 5.1420
Training Epoch: 0 [152/50000]	Loss: 5.3153
Training Epoch: 0 [160/50000]	Loss: 6.1562
Training Epoch: 0 [168/50000]	Loss: 5.5503
Training Epoch: 0 [176/50000]	Loss: 5.0518
Training Epoch: 0 [184/50000]	Loss: 5.6091
Training Epoch: 0 [192/50000]	Loss: 4.5361
Training Epoch: 0 [200/50000]	Loss: 5.9333
Training Epoch: 0 [208/50000]	Loss: 5.5156
Training Epoch: 0 [216/50000]	Loss: 4.8882
Training Epoch: 0 [224/50000]	Loss: 5.3096
Training Epoch: 0 [232/50000]	Loss: 4.8261
Training Epoch: 0 [240/50000]	Loss: 5.2888
Training Epoch: 0 [248/50000]	Loss: 5.0763
Training Epoch: 0 [256/50000]	Loss: 5.3264
Training Epoch: 0 [264/50000]	Loss: 4.8134
Training Epoch: 0 [272/50000]	Loss: 5.8023
Training Epoch: 0 [280/50000]	Loss: 5.2665
Training Epoch: 0 [288/50000]	Loss: 5.6363
Training Epoch: 0 [296/50000]	Loss: 5.7109
Training Epoch: 0 [304/50000]	Loss: 5.1758
Training Epoch: 0 [312/50000]	Loss: 4.7203
Training Epoch: 0 [320/50000]	Loss: 5.0011
Training Epoch: 0 [328/50000]	Loss: 5.3880
Training Epoch: 0 [336/50000]	Loss: 5.1359
Training Epoch: 0 [344/50000]	Loss: 4.5340
Training Epoch: 0 [352/50000]	Loss: 4.8513
Training Epoch: 0 [360/50000]	Loss: 5.4668
Training Epoch: 0 [368/50000]	Loss: 4.9504
Training Epoch: 0 [376/50000]	Loss: 4.9002
Training Epoch: 0 [384/50000]	Loss: 4.6152
Training Epoch: 0 [392/50000]	Loss: 4.9782
Training Epoch: 0 [400/50000]	Loss: 4.6078
Profile done with power limit 150W
epoch 1 train time consumed: 3.21s
Validation Epoch: 0, Average loss: 0.6272, Accuracy: 0.0105
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 150000, 'lr': 0.001, 'energy': 94.82412322079072, 'time': 2.2397586480001337, 'accuracy': 0.0105, 'total_cost': 5.755627746883997}
[run job] Launching job with BS 8: and LR: 0.001 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs8+lr0.00100+pl125', 'ZEUS_COST_THRESH': '11.511255493767994', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '0.001', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '0.001', '--power_limit', '125']
[run job] cost_ub=11.511255493767994
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs8+lr0.00100+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.001
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.8286
Training Epoch: 0 [24/50000]	Loss: 4.7004
Training Epoch: 0 [32/50000]	Loss: 4.8893
Training Epoch: 0 [40/50000]	Loss: 4.8635
Training Epoch: 0 [48/50000]	Loss: 5.9026
Training Epoch: 0 [56/50000]	Loss: 5.6946
Training Epoch: 0 [64/50000]	Loss: 5.1278
Training Epoch: 0 [72/50000]	Loss: 5.4489
Training Epoch: 0 [80/50000]	Loss: 5.3366
Training Epoch: 0 [88/50000]	Loss: 5.1812
Training Epoch: 0 [96/50000]	Loss: 6.2870
Training Epoch: 0 [104/50000]	Loss: 6.1034
Training Epoch: 0 [112/50000]	Loss: 5.2090
Training Epoch: 0 [120/50000]	Loss: 5.9342
Training Epoch: 0 [128/50000]	Loss: 6.1078
Training Epoch: 0 [136/50000]	Loss: 5.8452
Training Epoch: 0 [144/50000]	Loss: 5.1987
Training Epoch: 0 [152/50000]	Loss: 5.2974
Training Epoch: 0 [160/50000]	Loss: 6.1878
Training Epoch: 0 [168/50000]	Loss: 5.6110
Training Epoch: 0 [176/50000]	Loss: 5.0330
Training Epoch: 0 [184/50000]	Loss: 5.5077
Training Epoch: 0 [192/50000]	Loss: 4.5247
Training Epoch: 0 [200/50000]	Loss: 5.9819
Training Epoch: 0 [208/50000]	Loss: 5.4811
Training Epoch: 0 [216/50000]	Loss: 4.8113
Training Epoch: 0 [224/50000]	Loss: 5.3201
Training Epoch: 0 [232/50000]	Loss: 4.7415
Training Epoch: 0 [240/50000]	Loss: 5.2027
Training Epoch: 0 [248/50000]	Loss: 5.0759
Training Epoch: 0 [256/50000]	Loss: 5.4815
Training Epoch: 0 [264/50000]	Loss: 4.8960
Training Epoch: 0 [272/50000]	Loss: 5.7650
Training Epoch: 0 [280/50000]	Loss: 5.2503
Training Epoch: 0 [288/50000]	Loss: 5.4359
Training Epoch: 0 [296/50000]	Loss: 5.5718
Training Epoch: 0 [304/50000]	Loss: 5.2512
Training Epoch: 0 [312/50000]	Loss: 4.6915
Training Epoch: 0 [320/50000]	Loss: 5.0295
Training Epoch: 0 [328/50000]	Loss: 5.3085
Training Epoch: 0 [336/50000]	Loss: 5.0625
Training Epoch: 0 [344/50000]	Loss: 4.5393
Training Epoch: 0 [352/50000]	Loss: 4.7783
Training Epoch: 0 [360/50000]	Loss: 5.4347
Training Epoch: 0 [368/50000]	Loss: 4.9496
Training Epoch: 0 [376/50000]	Loss: 4.9465
Training Epoch: 0 [384/50000]	Loss: 4.6158
Training Epoch: 0 [392/50000]	Loss: 4.8801
Training Epoch: 0 [400/50000]	Loss: 4.5092
Profile done with power limit 125W
epoch 1 train time consumed: 3.21s
Validation Epoch: 0, Average loss: 0.6314, Accuracy: 0.0107
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 125000, 'lr': 0.001, 'energy': 97.24759466237109, 'time': 2.231993121999949, 'accuracy': 0.0107, 'total_cost': 5.6790164370789}
[run job] Launching job with BS 8: and LR: 0.001 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs8+lr0.00100+pl100', 'ZEUS_COST_THRESH': '11.3580328741578', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '0.001', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '0.001', '--power_limit', '100']
[run job] cost_ub=11.3580328741578
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs8+lr0.00100+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.001
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.8286
Training Epoch: 0 [24/50000]	Loss: 4.7004
Training Epoch: 0 [32/50000]	Loss: 4.8783
Training Epoch: 0 [40/50000]	Loss: 4.8772
Training Epoch: 0 [48/50000]	Loss: 5.8919
Training Epoch: 0 [56/50000]	Loss: 5.6687
Training Epoch: 0 [64/50000]	Loss: 5.1401
Training Epoch: 0 [72/50000]	Loss: 5.4925
Training Epoch: 0 [80/50000]	Loss: 5.2976
Training Epoch: 0 [88/50000]	Loss: 5.1078
Training Epoch: 0 [96/50000]	Loss: 6.2905
Training Epoch: 0 [104/50000]	Loss: 6.1417
Training Epoch: 0 [112/50000]	Loss: 5.1150
Training Epoch: 0 [120/50000]	Loss: 5.8505
Training Epoch: 0 [128/50000]	Loss: 6.1088
Training Epoch: 0 [136/50000]	Loss: 5.7727
Training Epoch: 0 [144/50000]	Loss: 5.2160
Training Epoch: 0 [152/50000]	Loss: 5.3299
Training Epoch: 0 [160/50000]	Loss: 6.1473
Training Epoch: 0 [168/50000]	Loss: 5.5817
Training Epoch: 0 [176/50000]	Loss: 5.0680
Training Epoch: 0 [184/50000]	Loss: 5.5364
Training Epoch: 0 [192/50000]	Loss: 4.5597
Training Epoch: 0 [200/50000]	Loss: 5.9266
Training Epoch: 0 [208/50000]	Loss: 5.5990
Training Epoch: 0 [216/50000]	Loss: 4.9101
Training Epoch: 0 [224/50000]	Loss: 5.3758
Training Epoch: 0 [232/50000]	Loss: 4.8085
Training Epoch: 0 [240/50000]	Loss: 5.1990
Training Epoch: 0 [248/50000]	Loss: 4.9856
Training Epoch: 0 [256/50000]	Loss: 5.3577
Training Epoch: 0 [264/50000]	Loss: 4.8749
Training Epoch: 0 [272/50000]	Loss: 5.6963
Training Epoch: 0 [280/50000]	Loss: 5.3029
Training Epoch: 0 [288/50000]	Loss: 5.4433
Training Epoch: 0 [296/50000]	Loss: 5.5650
Training Epoch: 0 [304/50000]	Loss: 5.1872
Training Epoch: 0 [312/50000]	Loss: 4.7451
Training Epoch: 0 [320/50000]	Loss: 5.1319
Training Epoch: 0 [328/50000]	Loss: 5.3607
Training Epoch: 0 [336/50000]	Loss: 5.1078
Training Epoch: 0 [344/50000]	Loss: 4.5798
Training Epoch: 0 [352/50000]	Loss: 4.8935
Training Epoch: 0 [360/50000]	Loss: 5.5065
Training Epoch: 0 [368/50000]	Loss: 4.9829
Training Epoch: 0 [376/50000]	Loss: 4.9602
Training Epoch: 0 [384/50000]	Loss: 4.6357
Training Epoch: 0 [392/50000]	Loss: 4.8655
Training Epoch: 0 [400/50000]	Loss: 4.6337
Profile done with power limit 100W
epoch 1 train time consumed: 3.22s
Validation Epoch: 0, Average loss: 0.6343, Accuracy: 0.0113
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 100000, 'lr': 0.001, 'energy': 95.82726394417486, 'time': 2.2400386730005266, 'accuracy': 0.0113, 'total_cost': 5.368703937503301}

[Power Profiler] with batch size 8 and learning rate 0.0012
[run job] Launching job with BS 8: and LR: 0.0012 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs8+lr0.00120+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '0.0012', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0012', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs8+lr0.00120+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.0012
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.8424
Training Epoch: 0 [24/50000]	Loss: 4.7715
Training Epoch: 0 [32/50000]	Loss: 4.9483
Training Epoch: 0 [40/50000]	Loss: 4.9718
Training Epoch: 0 [48/50000]	Loss: 6.2439
Training Epoch: 0 [56/50000]	Loss: 5.9327
Training Epoch: 0 [64/50000]	Loss: 5.2640
Training Epoch: 0 [72/50000]	Loss: 5.6710
Training Epoch: 0 [80/50000]	Loss: 5.5658
Training Epoch: 0 [88/50000]	Loss: 5.3087
Training Epoch: 0 [96/50000]	Loss: 6.5771
Training Epoch: 0 [104/50000]	Loss: 6.3495
Training Epoch: 0 [112/50000]	Loss: 5.3627
Training Epoch: 0 [120/50000]	Loss: 6.1438
Training Epoch: 0 [128/50000]	Loss: 6.3111
Training Epoch: 0 [136/50000]	Loss: 5.9832
Training Epoch: 0 [144/50000]	Loss: 5.2930
Training Epoch: 0 [152/50000]	Loss: 5.4282
Training Epoch: 0 [160/50000]	Loss: 6.2637
Training Epoch: 0 [168/50000]	Loss: 5.7227
Training Epoch: 0 [176/50000]	Loss: 5.0599
Training Epoch: 0 [184/50000]	Loss: 5.5976
Training Epoch: 0 [192/50000]	Loss: 4.6573
Training Epoch: 0 [200/50000]	Loss: 6.0166
Training Epoch: 0 [208/50000]	Loss: 5.6481
Training Epoch: 0 [216/50000]	Loss: 4.8535
Training Epoch: 0 [224/50000]	Loss: 5.4375
Training Epoch: 0 [232/50000]	Loss: 4.8805
Training Epoch: 0 [240/50000]	Loss: 5.2198
Training Epoch: 0 [248/50000]	Loss: 4.8789
Training Epoch: 0 [256/50000]	Loss: 5.5240
Training Epoch: 0 [264/50000]	Loss: 4.9895
Training Epoch: 0 [272/50000]	Loss: 5.8815
Training Epoch: 0 [280/50000]	Loss: 5.3182
Training Epoch: 0 [288/50000]	Loss: 5.6547
Training Epoch: 0 [296/50000]	Loss: 5.8600
Training Epoch: 0 [304/50000]	Loss: 5.2435
Training Epoch: 0 [312/50000]	Loss: 4.8219
Training Epoch: 0 [320/50000]	Loss: 5.1735
Training Epoch: 0 [328/50000]	Loss: 5.4554
Training Epoch: 0 [336/50000]	Loss: 5.0773
Training Epoch: 0 [344/50000]	Loss: 4.6876
Training Epoch: 0 [352/50000]	Loss: 4.8315
Training Epoch: 0 [360/50000]	Loss: 5.4995
Training Epoch: 0 [368/50000]	Loss: 4.8959
Training Epoch: 0 [376/50000]	Loss: 4.9686
Training Epoch: 0 [384/50000]	Loss: 4.7735
Training Epoch: 0 [392/50000]	Loss: 4.7561
Training Epoch: 0 [400/50000]	Loss: 4.5133
Profile done with power limit 175W
epoch 1 train time consumed: 3.33s
Validation Epoch: 0, Average loss: 0.6386, Accuracy: 0.0108
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 175000, 'lr': 0.0012, 'energy': 94.39402036616656, 'time': 2.3427121169997918, 'accuracy': 0.0108, 'total_cost': 5.843635516288028}
[run job] Launching job with BS 8: and LR: 0.0012 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs8+lr0.00120+pl150', 'ZEUS_COST_THRESH': '11.687271032576056', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '0.0012', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0012', '--power_limit', '150']
[run job] cost_ub=11.687271032576056
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs8+lr0.00120+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.0012
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.8424
Training Epoch: 0 [24/50000]	Loss: 4.7710
Training Epoch: 0 [32/50000]	Loss: 4.9690
Training Epoch: 0 [40/50000]	Loss: 4.9709
Training Epoch: 0 [48/50000]	Loss: 6.2002
Training Epoch: 0 [56/50000]	Loss: 5.9719
Training Epoch: 0 [64/50000]	Loss: 5.2877
Training Epoch: 0 [72/50000]	Loss: 5.6657
Training Epoch: 0 [80/50000]	Loss: 5.6359
Training Epoch: 0 [88/50000]	Loss: 5.3278
Training Epoch: 0 [96/50000]	Loss: 6.5532
Training Epoch: 0 [104/50000]	Loss: 6.4093
Training Epoch: 0 [112/50000]	Loss: 5.3744
Training Epoch: 0 [120/50000]	Loss: 6.2085
Training Epoch: 0 [128/50000]	Loss: 6.3241
Training Epoch: 0 [136/50000]	Loss: 5.9366
Training Epoch: 0 [144/50000]	Loss: 5.2206
Training Epoch: 0 [152/50000]	Loss: 5.3774
Training Epoch: 0 [160/50000]	Loss: 6.3487
Training Epoch: 0 [168/50000]	Loss: 5.6264
Training Epoch: 0 [176/50000]	Loss: 5.0951
Training Epoch: 0 [184/50000]	Loss: 5.6373
Training Epoch: 0 [192/50000]	Loss: 4.6886
Training Epoch: 0 [200/50000]	Loss: 6.0787
Training Epoch: 0 [208/50000]	Loss: 5.7167
Training Epoch: 0 [216/50000]	Loss: 4.9265
Training Epoch: 0 [224/50000]	Loss: 5.4167
Training Epoch: 0 [232/50000]	Loss: 4.9015
Training Epoch: 0 [240/50000]	Loss: 5.2410
Training Epoch: 0 [248/50000]	Loss: 4.9085
Training Epoch: 0 [256/50000]	Loss: 5.3634
Training Epoch: 0 [264/50000]	Loss: 5.0308
Training Epoch: 0 [272/50000]	Loss: 5.8927
Training Epoch: 0 [280/50000]	Loss: 5.5013
Training Epoch: 0 [288/50000]	Loss: 5.7060
Training Epoch: 0 [296/50000]	Loss: 5.7484
Training Epoch: 0 [304/50000]	Loss: 5.2906
Training Epoch: 0 [312/50000]	Loss: 4.8340
Training Epoch: 0 [320/50000]	Loss: 5.0372
Training Epoch: 0 [328/50000]	Loss: 5.4333
Training Epoch: 0 [336/50000]	Loss: 5.0540
Training Epoch: 0 [344/50000]	Loss: 4.6060
Training Epoch: 0 [352/50000]	Loss: 4.7453
Training Epoch: 0 [360/50000]	Loss: 5.5313
Training Epoch: 0 [368/50000]	Loss: 5.0676
Training Epoch: 0 [376/50000]	Loss: 4.8578
Training Epoch: 0 [384/50000]	Loss: 4.6417
Training Epoch: 0 [392/50000]	Loss: 4.8977
Training Epoch: 0 [400/50000]	Loss: 4.7270
Profile done with power limit 150W
epoch 1 train time consumed: 3.22s
Validation Epoch: 0, Average loss: 0.6400, Accuracy: 0.0100
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 150000, 'lr': 0.0012, 'energy': 97.91465162908382, 'time': 2.2472193980001975, 'accuracy': 0.01, 'total_cost': 6.132990991393434}
[run job] Launching job with BS 8: and LR: 0.0012 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs8+lr0.00120+pl125', 'ZEUS_COST_THRESH': '11.687271032576056', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '0.0012', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0012', '--power_limit', '125']
[run job] cost_ub=11.687271032576056
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs8+lr0.00120+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.0012
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.8424
Training Epoch: 0 [24/50000]	Loss: 4.7708
Training Epoch: 0 [32/50000]	Loss: 4.9528
Training Epoch: 0 [40/50000]	Loss: 4.9941
Training Epoch: 0 [48/50000]	Loss: 6.2023
Training Epoch: 0 [56/50000]	Loss: 5.9689
Training Epoch: 0 [64/50000]	Loss: 5.3243
Training Epoch: 0 [72/50000]	Loss: 5.6482
Training Epoch: 0 [80/50000]	Loss: 5.5305
Training Epoch: 0 [88/50000]	Loss: 5.3457
Training Epoch: 0 [96/50000]	Loss: 6.5492
Training Epoch: 0 [104/50000]	Loss: 6.3528
Training Epoch: 0 [112/50000]	Loss: 5.3896
Training Epoch: 0 [120/50000]	Loss: 6.1278
Training Epoch: 0 [128/50000]	Loss: 6.3220
Training Epoch: 0 [136/50000]	Loss: 5.9383
Training Epoch: 0 [144/50000]	Loss: 5.2795
Training Epoch: 0 [152/50000]	Loss: 5.3851
Training Epoch: 0 [160/50000]	Loss: 6.3303
Training Epoch: 0 [168/50000]	Loss: 5.7151
Training Epoch: 0 [176/50000]	Loss: 5.1541
Training Epoch: 0 [184/50000]	Loss: 5.5555
Training Epoch: 0 [192/50000]	Loss: 4.6303
Training Epoch: 0 [200/50000]	Loss: 5.9347
Training Epoch: 0 [208/50000]	Loss: 5.6270
Training Epoch: 0 [216/50000]	Loss: 4.7941
Training Epoch: 0 [224/50000]	Loss: 5.3775
Training Epoch: 0 [232/50000]	Loss: 4.8838
Training Epoch: 0 [240/50000]	Loss: 5.3058
Training Epoch: 0 [248/50000]	Loss: 5.0929
Training Epoch: 0 [256/50000]	Loss: 5.4011
Training Epoch: 0 [264/50000]	Loss: 4.9213
Training Epoch: 0 [272/50000]	Loss: 5.9883
Training Epoch: 0 [280/50000]	Loss: 5.3317
Training Epoch: 0 [288/50000]	Loss: 5.7284
Training Epoch: 0 [296/50000]	Loss: 5.7229
Training Epoch: 0 [304/50000]	Loss: 5.3472
Training Epoch: 0 [312/50000]	Loss: 4.7498
Training Epoch: 0 [320/50000]	Loss: 5.2150
Training Epoch: 0 [328/50000]	Loss: 5.3707
Training Epoch: 0 [336/50000]	Loss: 4.9676
Training Epoch: 0 [344/50000]	Loss: 4.6854
Training Epoch: 0 [352/50000]	Loss: 4.8651
Training Epoch: 0 [360/50000]	Loss: 5.5599
Training Epoch: 0 [368/50000]	Loss: 4.9630
Training Epoch: 0 [376/50000]	Loss: 4.9731
Training Epoch: 0 [384/50000]	Loss: 4.6046
Training Epoch: 0 [392/50000]	Loss: 4.7299
Training Epoch: 0 [400/50000]	Loss: 4.5609
Profile done with power limit 125W
epoch 1 train time consumed: 3.27s
Validation Epoch: 0, Average loss: 0.6415, Accuracy: 0.0154
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 125000, 'lr': 0.0012, 'energy': 94.34016101301853, 'time': 2.2912404210001114, 'accuracy': 0.0154, 'total_cost': 4.007292622803288}
[run job] Launching job with BS 8: and LR: 0.0012 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs8+lr0.00120+pl100', 'ZEUS_COST_THRESH': '8.014585245606575', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '0.0012', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0012', '--power_limit', '100']
[run job] cost_ub=8.014585245606575
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs8+lr0.00120+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.0012
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.8424
Training Epoch: 0 [24/50000]	Loss: 4.7710
Training Epoch: 0 [32/50000]	Loss: 4.9689
Training Epoch: 0 [40/50000]	Loss: 4.9764
Training Epoch: 0 [48/50000]	Loss: 6.1969
Training Epoch: 0 [56/50000]	Loss: 5.9815
Training Epoch: 0 [64/50000]	Loss: 5.2728
Training Epoch: 0 [72/50000]	Loss: 5.6572
Training Epoch: 0 [80/50000]	Loss: 5.5361
Training Epoch: 0 [88/50000]	Loss: 5.3574
Training Epoch: 0 [96/50000]	Loss: 6.5562
Training Epoch: 0 [104/50000]	Loss: 6.3685
Training Epoch: 0 [112/50000]	Loss: 5.3202
Training Epoch: 0 [120/50000]	Loss: 6.1815
Training Epoch: 0 [128/50000]	Loss: 6.2872
Training Epoch: 0 [136/50000]	Loss: 5.9018
Training Epoch: 0 [144/50000]	Loss: 5.3226
Training Epoch: 0 [152/50000]	Loss: 5.3285
Training Epoch: 0 [160/50000]	Loss: 6.2828
Training Epoch: 0 [168/50000]	Loss: 5.7300
Training Epoch: 0 [176/50000]	Loss: 5.0968
Training Epoch: 0 [184/50000]	Loss: 5.5969
Training Epoch: 0 [192/50000]	Loss: 4.6121
Training Epoch: 0 [200/50000]	Loss: 6.0658
Training Epoch: 0 [208/50000]	Loss: 5.7611
Training Epoch: 0 [216/50000]	Loss: 4.9035
Training Epoch: 0 [224/50000]	Loss: 5.4173
Training Epoch: 0 [232/50000]	Loss: 4.8756
Training Epoch: 0 [240/50000]	Loss: 5.2663
Training Epoch: 0 [248/50000]	Loss: 4.9199
Training Epoch: 0 [256/50000]	Loss: 5.5453
Training Epoch: 0 [264/50000]	Loss: 4.9624
Training Epoch: 0 [272/50000]	Loss: 5.8327
Training Epoch: 0 [280/50000]	Loss: 5.3479
Training Epoch: 0 [288/50000]	Loss: 5.6511
Training Epoch: 0 [296/50000]	Loss: 5.7778
Training Epoch: 0 [304/50000]	Loss: 5.2762
Training Epoch: 0 [312/50000]	Loss: 4.8310
Training Epoch: 0 [320/50000]	Loss: 5.2341
Training Epoch: 0 [328/50000]	Loss: 5.4357
Training Epoch: 0 [336/50000]	Loss: 5.1845
Training Epoch: 0 [344/50000]	Loss: 4.5593
Training Epoch: 0 [352/50000]	Loss: 4.7469
Training Epoch: 0 [360/50000]	Loss: 5.5129
Training Epoch: 0 [368/50000]	Loss: 5.0086
Training Epoch: 0 [376/50000]	Loss: 4.9684
Training Epoch: 0 [384/50000]	Loss: 4.5239
Training Epoch: 0 [392/50000]	Loss: 4.8772
Training Epoch: 0 [400/50000]	Loss: 4.6571
Profile done with power limit 100W
epoch 1 train time consumed: 3.22s
Validation Epoch: 0, Average loss: 0.6419, Accuracy: 0.0104
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 100000, 'lr': 0.0012, 'energy': 97.21342614671417, 'time': 2.2544562189996213, 'accuracy': 0.0104, 'total_cost': 5.900896648765903}

[Power Profiler] with batch size 16 and learning rate 0.0011313708498984763
[run job] Launching job with BS 16: and LR: 0.0011313708498984763 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs16+lr0.00113+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '0.0011313708498984763', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0011313708498984763', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs16+lr0.00113+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.0011313708498984763
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6490
Training Epoch: 0 [48/50000]	Loss: 4.9717
Training Epoch: 0 [64/50000]	Loss: 5.0852
Training Epoch: 0 [80/50000]	Loss: 5.0206
Training Epoch: 0 [96/50000]	Loss: 5.2088
Training Epoch: 0 [112/50000]	Loss: 5.3212
Training Epoch: 0 [128/50000]	Loss: 5.5504
Training Epoch: 0 [144/50000]	Loss: 5.3373
Training Epoch: 0 [160/50000]	Loss: 5.4288
Training Epoch: 0 [176/50000]	Loss: 5.2140
Training Epoch: 0 [192/50000]	Loss: 4.9645
Training Epoch: 0 [208/50000]	Loss: 5.5419
Training Epoch: 0 [224/50000]	Loss: 4.9978
Training Epoch: 0 [240/50000]	Loss: 5.0171
Training Epoch: 0 [256/50000]	Loss: 5.2847
Training Epoch: 0 [272/50000]	Loss: 5.0016
Training Epoch: 0 [288/50000]	Loss: 5.4159
Training Epoch: 0 [304/50000]	Loss: 5.4325
Training Epoch: 0 [320/50000]	Loss: 4.9352
Training Epoch: 0 [336/50000]	Loss: 5.2482
Training Epoch: 0 [352/50000]	Loss: 4.7878
Training Epoch: 0 [368/50000]	Loss: 5.5706
Training Epoch: 0 [384/50000]	Loss: 4.8238
Training Epoch: 0 [400/50000]	Loss: 4.9086
Training Epoch: 0 [416/50000]	Loss: 4.7747
Training Epoch: 0 [432/50000]	Loss: 4.5769
Training Epoch: 0 [448/50000]	Loss: 5.0154
Training Epoch: 0 [464/50000]	Loss: 4.9935
Training Epoch: 0 [480/50000]	Loss: 5.1611
Training Epoch: 0 [496/50000]	Loss: 4.9634
Training Epoch: 0 [512/50000]	Loss: 4.9514
Training Epoch: 0 [528/50000]	Loss: 4.6540
Training Epoch: 0 [544/50000]	Loss: 4.7989
Training Epoch: 0 [560/50000]	Loss: 4.5932
Training Epoch: 0 [576/50000]	Loss: 4.6042
Training Epoch: 0 [592/50000]	Loss: 5.0217
Training Epoch: 0 [608/50000]	Loss: 4.9799
Training Epoch: 0 [624/50000]	Loss: 5.7078
Training Epoch: 0 [640/50000]	Loss: 4.9714
Training Epoch: 0 [656/50000]	Loss: 5.1834
Training Epoch: 0 [672/50000]	Loss: 4.6510
Training Epoch: 0 [688/50000]	Loss: 5.0575
Training Epoch: 0 [704/50000]	Loss: 4.4357
Training Epoch: 0 [720/50000]	Loss: 4.9392
Training Epoch: 0 [736/50000]	Loss: 4.8612
Training Epoch: 0 [752/50000]	Loss: 4.9673
Training Epoch: 0 [768/50000]	Loss: 5.1814
Training Epoch: 0 [784/50000]	Loss: 4.7776
Training Epoch: 0 [800/50000]	Loss: 4.9276
Profile done with power limit 175W
epoch 1 train time consumed: 3.57s
Validation Epoch: 0, Average loss: 0.3020, Accuracy: 0.0133
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 175000, 'lr': 0.0011313708498984763, 'energy': 114.26823903237876, 'time': 2.4487637330003054, 'accuracy': 0.0133, 'total_cost': 10.651873275960186}
[run job] Launching job with BS 16: and LR: 0.0011313708498984763 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs16+lr0.00113+pl150', 'ZEUS_COST_THRESH': '21.303746551920373', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '0.0011313708498984763', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0011313708498984763', '--power_limit', '150']
[run job] cost_ub=21.303746551920373
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs16+lr0.00113+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.0011313708498984763
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6490
Training Epoch: 0 [48/50000]	Loss: 4.9715
Training Epoch: 0 [64/50000]	Loss: 5.0801
Training Epoch: 0 [80/50000]	Loss: 5.0135
Training Epoch: 0 [96/50000]	Loss: 5.2035
Training Epoch: 0 [112/50000]	Loss: 5.3148
Training Epoch: 0 [128/50000]	Loss: 5.5386
Training Epoch: 0 [144/50000]	Loss: 5.3327
Training Epoch: 0 [160/50000]	Loss: 5.4844
Training Epoch: 0 [176/50000]	Loss: 5.2450
Training Epoch: 0 [192/50000]	Loss: 4.9831
Training Epoch: 0 [208/50000]	Loss: 5.5242
Training Epoch: 0 [224/50000]	Loss: 5.0194
Training Epoch: 0 [240/50000]	Loss: 5.0532
Training Epoch: 0 [256/50000]	Loss: 5.2934
Training Epoch: 0 [272/50000]	Loss: 4.9754
Training Epoch: 0 [288/50000]	Loss: 5.3560
Training Epoch: 0 [304/50000]	Loss: 5.4619
Training Epoch: 0 [320/50000]	Loss: 4.9100
Training Epoch: 0 [336/50000]	Loss: 5.2758
Training Epoch: 0 [352/50000]	Loss: 4.8195
Training Epoch: 0 [368/50000]	Loss: 5.6190
Training Epoch: 0 [384/50000]	Loss: 4.9355
Training Epoch: 0 [400/50000]	Loss: 4.8991
Training Epoch: 0 [416/50000]	Loss: 4.7304
Training Epoch: 0 [432/50000]	Loss: 4.4877
Training Epoch: 0 [448/50000]	Loss: 5.0142
Training Epoch: 0 [464/50000]	Loss: 4.9202
Training Epoch: 0 [480/50000]	Loss: 5.1524
Training Epoch: 0 [496/50000]	Loss: 4.9548
Training Epoch: 0 [512/50000]	Loss: 4.9039
Training Epoch: 0 [528/50000]	Loss: 4.6981
Training Epoch: 0 [544/50000]	Loss: 4.8276
Training Epoch: 0 [560/50000]	Loss: 4.6426
Training Epoch: 0 [576/50000]	Loss: 4.6775
Training Epoch: 0 [592/50000]	Loss: 5.0226
Training Epoch: 0 [608/50000]	Loss: 4.9903
Training Epoch: 0 [624/50000]	Loss: 5.6510
Training Epoch: 0 [640/50000]	Loss: 4.9859
Training Epoch: 0 [656/50000]	Loss: 5.2198
Training Epoch: 0 [672/50000]	Loss: 4.7225
Training Epoch: 0 [688/50000]	Loss: 5.0398
Training Epoch: 0 [704/50000]	Loss: 4.5084
Training Epoch: 0 [720/50000]	Loss: 4.9442
Training Epoch: 0 [736/50000]	Loss: 4.8761
Training Epoch: 0 [752/50000]	Loss: 4.9129
Training Epoch: 0 [768/50000]	Loss: 5.2109
Training Epoch: 0 [784/50000]	Loss: 4.8765
Training Epoch: 0 [800/50000]	Loss: 4.9239
Profile done with power limit 150W
epoch 1 train time consumed: 3.52s
Validation Epoch: 0, Average loss: 0.3013, Accuracy: 0.0121
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 150000, 'lr': 0.0011313708498984763, 'energy': 116.46995814174365, 'time': 2.370886925999912, 'accuracy': 0.0121, 'total_cost': 11.422186993057876}
[run job] Launching job with BS 16: and LR: 0.0011313708498984763 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs16+lr0.00113+pl125', 'ZEUS_COST_THRESH': '21.303746551920373', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '0.0011313708498984763', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0011313708498984763', '--power_limit', '125']
[run job] cost_ub=21.303746551920373
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs16+lr0.00113+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.0011313708498984763
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6490
Training Epoch: 0 [48/50000]	Loss: 4.9717
Training Epoch: 0 [64/50000]	Loss: 5.0818
Training Epoch: 0 [80/50000]	Loss: 5.0211
Training Epoch: 0 [96/50000]	Loss: 5.2223
Training Epoch: 0 [112/50000]	Loss: 5.2866
Training Epoch: 0 [128/50000]	Loss: 5.5273
Training Epoch: 0 [144/50000]	Loss: 5.3069
Training Epoch: 0 [160/50000]	Loss: 5.5110
Training Epoch: 0 [176/50000]	Loss: 5.2241
Training Epoch: 0 [192/50000]	Loss: 4.8895
Training Epoch: 0 [208/50000]	Loss: 5.5968
Training Epoch: 0 [224/50000]	Loss: 5.1026
Training Epoch: 0 [240/50000]	Loss: 5.0477
Training Epoch: 0 [256/50000]	Loss: 5.3010
Training Epoch: 0 [272/50000]	Loss: 4.9888
Training Epoch: 0 [288/50000]	Loss: 5.4103
Training Epoch: 0 [304/50000]	Loss: 5.4522
Training Epoch: 0 [320/50000]	Loss: 4.9276
Training Epoch: 0 [336/50000]	Loss: 5.2475
Training Epoch: 0 [352/50000]	Loss: 4.7350
Training Epoch: 0 [368/50000]	Loss: 5.5804
Training Epoch: 0 [384/50000]	Loss: 4.9132
Training Epoch: 0 [400/50000]	Loss: 4.9280
Training Epoch: 0 [416/50000]	Loss: 4.7355
Training Epoch: 0 [432/50000]	Loss: 4.5559
Training Epoch: 0 [448/50000]	Loss: 5.1103
Training Epoch: 0 [464/50000]	Loss: 4.9454
Training Epoch: 0 [480/50000]	Loss: 5.1270
Training Epoch: 0 [496/50000]	Loss: 4.9413
Training Epoch: 0 [512/50000]	Loss: 4.9540
Training Epoch: 0 [528/50000]	Loss: 4.6987
Training Epoch: 0 [544/50000]	Loss: 4.7736
Training Epoch: 0 [560/50000]	Loss: 4.6576
Training Epoch: 0 [576/50000]	Loss: 4.6561
Training Epoch: 0 [592/50000]	Loss: 4.9921
Training Epoch: 0 [608/50000]	Loss: 4.9175
Training Epoch: 0 [624/50000]	Loss: 5.6740
Training Epoch: 0 [640/50000]	Loss: 4.9459
Training Epoch: 0 [656/50000]	Loss: 5.2262
Training Epoch: 0 [672/50000]	Loss: 4.6913
Training Epoch: 0 [688/50000]	Loss: 5.0819
Training Epoch: 0 [704/50000]	Loss: 4.4919
Training Epoch: 0 [720/50000]	Loss: 4.9832
Training Epoch: 0 [736/50000]	Loss: 4.8885
Training Epoch: 0 [752/50000]	Loss: 4.9620
Training Epoch: 0 [768/50000]	Loss: 5.2606
Training Epoch: 0 [784/50000]	Loss: 4.8220
Training Epoch: 0 [800/50000]	Loss: 5.0445
Profile done with power limit 125W
epoch 1 train time consumed: 3.57s
Validation Epoch: 0, Average loss: 0.3012, Accuracy: 0.0108
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 125000, 'lr': 0.0011313708498984763, 'energy': 115.3037123336963, 'time': 2.435076527000092, 'accuracy': 0.0108, 'total_cost': 13.090958437125389}
[run job] Launching job with BS 16: and LR: 0.0011313708498984763 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs16+lr0.00113+pl100', 'ZEUS_COST_THRESH': '21.303746551920373', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '0.0011313708498984763', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0011313708498984763', '--power_limit', '100']
[run job] cost_ub=21.303746551920373
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs16+lr0.00113+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.0011313708498984763
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6490
Training Epoch: 0 [48/50000]	Loss: 4.9717
Training Epoch: 0 [64/50000]	Loss: 5.0853
Training Epoch: 0 [80/50000]	Loss: 5.0017
Training Epoch: 0 [96/50000]	Loss: 5.2274
Training Epoch: 0 [112/50000]	Loss: 5.3202
Training Epoch: 0 [128/50000]	Loss: 5.5332
Training Epoch: 0 [144/50000]	Loss: 5.3216
Training Epoch: 0 [160/50000]	Loss: 5.5089
Training Epoch: 0 [176/50000]	Loss: 5.2459
Training Epoch: 0 [192/50000]	Loss: 4.9731
Training Epoch: 0 [208/50000]	Loss: 5.4933
Training Epoch: 0 [224/50000]	Loss: 5.0462
Training Epoch: 0 [240/50000]	Loss: 5.0463
Training Epoch: 0 [256/50000]	Loss: 5.2621
Training Epoch: 0 [272/50000]	Loss: 4.9700
Training Epoch: 0 [288/50000]	Loss: 5.3873
Training Epoch: 0 [304/50000]	Loss: 5.4520
Training Epoch: 0 [320/50000]	Loss: 4.9033
Training Epoch: 0 [336/50000]	Loss: 5.2595
Training Epoch: 0 [352/50000]	Loss: 4.7883
Training Epoch: 0 [368/50000]	Loss: 5.5892
Training Epoch: 0 [384/50000]	Loss: 4.8705
Training Epoch: 0 [400/50000]	Loss: 4.8980
Training Epoch: 0 [416/50000]	Loss: 4.6917
Training Epoch: 0 [432/50000]	Loss: 4.5438
Training Epoch: 0 [448/50000]	Loss: 5.0125
Training Epoch: 0 [464/50000]	Loss: 4.9482
Training Epoch: 0 [480/50000]	Loss: 5.1300
Training Epoch: 0 [496/50000]	Loss: 5.0159
Training Epoch: 0 [512/50000]	Loss: 4.9716
Training Epoch: 0 [528/50000]	Loss: 4.6646
Training Epoch: 0 [544/50000]	Loss: 4.8451
Training Epoch: 0 [560/50000]	Loss: 4.6628
Training Epoch: 0 [576/50000]	Loss: 4.5739
Training Epoch: 0 [592/50000]	Loss: 4.9820
Training Epoch: 0 [608/50000]	Loss: 4.9645
Training Epoch: 0 [624/50000]	Loss: 5.6894
Training Epoch: 0 [640/50000]	Loss: 5.0386
Training Epoch: 0 [656/50000]	Loss: 5.2126
Training Epoch: 0 [672/50000]	Loss: 4.6507
Training Epoch: 0 [688/50000]	Loss: 5.0731
Training Epoch: 0 [704/50000]	Loss: 4.4757
Training Epoch: 0 [720/50000]	Loss: 4.9803
Training Epoch: 0 [736/50000]	Loss: 4.9038
Training Epoch: 0 [752/50000]	Loss: 4.8784
Training Epoch: 0 [768/50000]	Loss: 5.2655
Training Epoch: 0 [784/50000]	Loss: 4.8083
Training Epoch: 0 [800/50000]	Loss: 4.9106
Profile done with power limit 100W
epoch 1 train time consumed: 3.87s
Validation Epoch: 0, Average loss: 0.2999, Accuracy: 0.0112
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 100000, 'lr': 0.0011313708498984763, 'energy': 98.99872835530581, 'time': 2.7405146650007737, 'accuracy': 0.0112, 'total_cost': 13.408884522308554}

[Power Profiler] with batch size 16 and learning rate 0.0014142135623730952
[run job] Launching job with BS 16: and LR: 0.0014142135623730952 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs16+lr0.00141+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '0.0014142135623730952', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0014142135623730952', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs16+lr0.00141+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.0014142135623730952
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6726
Training Epoch: 0 [48/50000]	Loss: 5.0944
Training Epoch: 0 [64/50000]	Loss: 5.2530
Training Epoch: 0 [80/50000]	Loss: 5.2395
Training Epoch: 0 [96/50000]	Loss: 5.4799
Training Epoch: 0 [112/50000]	Loss: 5.5860
Training Epoch: 0 [128/50000]	Loss: 5.7465
Training Epoch: 0 [144/50000]	Loss: 5.4980
Training Epoch: 0 [160/50000]	Loss: 5.6701
Training Epoch: 0 [176/50000]	Loss: 5.3127
Training Epoch: 0 [192/50000]	Loss: 5.0420
Training Epoch: 0 [208/50000]	Loss: 5.7162
Training Epoch: 0 [224/50000]	Loss: 5.0921
Training Epoch: 0 [240/50000]	Loss: 5.0569
Training Epoch: 0 [256/50000]	Loss: 5.2982
Training Epoch: 0 [272/50000]	Loss: 5.1405
Training Epoch: 0 [288/50000]	Loss: 5.5842
Training Epoch: 0 [304/50000]	Loss: 5.6504
Training Epoch: 0 [320/50000]	Loss: 5.0781
Training Epoch: 0 [336/50000]	Loss: 5.3831
Training Epoch: 0 [352/50000]	Loss: 4.8625
Training Epoch: 0 [368/50000]	Loss: 5.6814
Training Epoch: 0 [384/50000]	Loss: 5.0068
Training Epoch: 0 [400/50000]	Loss: 4.9385
Training Epoch: 0 [416/50000]	Loss: 4.8058
Training Epoch: 0 [432/50000]	Loss: 4.5705
Training Epoch: 0 [448/50000]	Loss: 5.1104
Training Epoch: 0 [464/50000]	Loss: 4.9711
Training Epoch: 0 [480/50000]	Loss: 5.3068
Training Epoch: 0 [496/50000]	Loss: 4.9515
Training Epoch: 0 [512/50000]	Loss: 4.9395
Training Epoch: 0 [528/50000]	Loss: 4.8494
Training Epoch: 0 [544/50000]	Loss: 4.8207
Training Epoch: 0 [560/50000]	Loss: 4.6967
Training Epoch: 0 [576/50000]	Loss: 4.6801
Training Epoch: 0 [592/50000]	Loss: 5.0176
Training Epoch: 0 [608/50000]	Loss: 4.9630
Training Epoch: 0 [624/50000]	Loss: 5.7308
Training Epoch: 0 [640/50000]	Loss: 4.9450
Training Epoch: 0 [656/50000]	Loss: 5.1860
Training Epoch: 0 [672/50000]	Loss: 4.7364
Training Epoch: 0 [688/50000]	Loss: 4.9840
Training Epoch: 0 [704/50000]	Loss: 4.5384
Training Epoch: 0 [720/50000]	Loss: 4.9850
Training Epoch: 0 [736/50000]	Loss: 5.0669
Training Epoch: 0 [752/50000]	Loss: 4.9255
Training Epoch: 0 [768/50000]	Loss: 5.3690
Training Epoch: 0 [784/50000]	Loss: 4.8667
Training Epoch: 0 [800/50000]	Loss: 4.9986
Profile done with power limit 175W
epoch 1 train time consumed: 3.53s
Validation Epoch: 0, Average loss: 0.3052, Accuracy: 0.0104
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 175000, 'lr': 0.0014142135623730952, 'energy': 115.54774945835459, 'time': 2.4450269750004736, 'accuracy': 0.0104, 'total_cost': 13.661482402910698}
[run job] Launching job with BS 16: and LR: 0.0014142135623730952 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs16+lr0.00141+pl150', 'ZEUS_COST_THRESH': '27.322964805821396', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '0.0014142135623730952', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0014142135623730952', '--power_limit', '150']
[run job] cost_ub=27.322964805821396
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs16+lr0.00141+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.0014142135623730952
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6726
Training Epoch: 0 [48/50000]	Loss: 5.0936
Training Epoch: 0 [64/50000]	Loss: 5.2507
Training Epoch: 0 [80/50000]	Loss: 5.2286
Training Epoch: 0 [96/50000]	Loss: 5.4589
Training Epoch: 0 [112/50000]	Loss: 5.5514
Training Epoch: 0 [128/50000]	Loss: 5.7562
Training Epoch: 0 [144/50000]	Loss: 5.4283
Training Epoch: 0 [160/50000]	Loss: 5.6590
Training Epoch: 0 [176/50000]	Loss: 5.3232
Training Epoch: 0 [192/50000]	Loss: 5.0173
Training Epoch: 0 [208/50000]	Loss: 5.7442
Training Epoch: 0 [224/50000]	Loss: 5.1134
Training Epoch: 0 [240/50000]	Loss: 5.0778
Training Epoch: 0 [256/50000]	Loss: 5.2546
Training Epoch: 0 [272/50000]	Loss: 5.1791
Training Epoch: 0 [288/50000]	Loss: 5.5703
Training Epoch: 0 [304/50000]	Loss: 5.6365
Training Epoch: 0 [320/50000]	Loss: 4.9745
Training Epoch: 0 [336/50000]	Loss: 5.3840
Training Epoch: 0 [352/50000]	Loss: 4.8607
Training Epoch: 0 [368/50000]	Loss: 5.6461
Training Epoch: 0 [384/50000]	Loss: 5.0189
Training Epoch: 0 [400/50000]	Loss: 4.9274
Training Epoch: 0 [416/50000]	Loss: 4.8238
Training Epoch: 0 [432/50000]	Loss: 4.6251
Training Epoch: 0 [448/50000]	Loss: 5.1528
Training Epoch: 0 [464/50000]	Loss: 4.9587
Training Epoch: 0 [480/50000]	Loss: 5.3127
Training Epoch: 0 [496/50000]	Loss: 4.9118
Training Epoch: 0 [512/50000]	Loss: 4.9573
Training Epoch: 0 [528/50000]	Loss: 4.7234
Training Epoch: 0 [544/50000]	Loss: 4.9195
Training Epoch: 0 [560/50000]	Loss: 4.6888
Training Epoch: 0 [576/50000]	Loss: 4.6766
Training Epoch: 0 [592/50000]	Loss: 5.0482
Training Epoch: 0 [608/50000]	Loss: 5.0099
Training Epoch: 0 [624/50000]	Loss: 5.7239
Training Epoch: 0 [640/50000]	Loss: 4.9498
Training Epoch: 0 [656/50000]	Loss: 5.2212
Training Epoch: 0 [672/50000]	Loss: 4.6747
Training Epoch: 0 [688/50000]	Loss: 5.0172
Training Epoch: 0 [704/50000]	Loss: 4.4991
Training Epoch: 0 [720/50000]	Loss: 4.9759
Training Epoch: 0 [736/50000]	Loss: 4.9391
Training Epoch: 0 [752/50000]	Loss: 4.9588
Training Epoch: 0 [768/50000]	Loss: 5.3542
Training Epoch: 0 [784/50000]	Loss: 4.8362
Training Epoch: 0 [800/50000]	Loss: 5.0087
Profile done with power limit 150W
epoch 1 train time consumed: 3.66s
Validation Epoch: 0, Average loss: 0.3034, Accuracy: 0.0117
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 150000, 'lr': 0.0014142135623730952, 'energy': 113.85318743516775, 'time': 2.5173135269997147, 'accuracy': 0.0117, 'total_cost': 12.42964164183815}
[run job] Launching job with BS 16: and LR: 0.0014142135623730952 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs16+lr0.00141+pl125', 'ZEUS_COST_THRESH': '24.8592832836763', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '0.0014142135623730952', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0014142135623730952', '--power_limit', '125']
[run job] cost_ub=24.8592832836763
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs16+lr0.00141+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.0014142135623730952
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6726
Training Epoch: 0 [48/50000]	Loss: 5.0937
Training Epoch: 0 [64/50000]	Loss: 5.2491
Training Epoch: 0 [80/50000]	Loss: 5.2490
Training Epoch: 0 [96/50000]	Loss: 5.4683
Training Epoch: 0 [112/50000]	Loss: 5.5407
Training Epoch: 0 [128/50000]	Loss: 5.7713
Training Epoch: 0 [144/50000]	Loss: 5.4212
Training Epoch: 0 [160/50000]	Loss: 5.6418
Training Epoch: 0 [176/50000]	Loss: 5.3357
Training Epoch: 0 [192/50000]	Loss: 5.0200
Training Epoch: 0 [208/50000]	Loss: 5.7513
Training Epoch: 0 [224/50000]	Loss: 5.1258
Training Epoch: 0 [240/50000]	Loss: 5.1202
Training Epoch: 0 [256/50000]	Loss: 5.2914
Training Epoch: 0 [272/50000]	Loss: 5.1251
Training Epoch: 0 [288/50000]	Loss: 5.6041
Training Epoch: 0 [304/50000]	Loss: 5.5937
Training Epoch: 0 [320/50000]	Loss: 4.9855
Training Epoch: 0 [336/50000]	Loss: 5.3512
Training Epoch: 0 [352/50000]	Loss: 4.8684
Training Epoch: 0 [368/50000]	Loss: 5.6867
Training Epoch: 0 [384/50000]	Loss: 5.0348
Training Epoch: 0 [400/50000]	Loss: 4.9567
Training Epoch: 0 [416/50000]	Loss: 4.8074
Training Epoch: 0 [432/50000]	Loss: 4.5780
Training Epoch: 0 [448/50000]	Loss: 5.2123
Training Epoch: 0 [464/50000]	Loss: 4.9616
Training Epoch: 0 [480/50000]	Loss: 5.3308
Training Epoch: 0 [496/50000]	Loss: 4.9286
Training Epoch: 0 [512/50000]	Loss: 4.9871
Training Epoch: 0 [528/50000]	Loss: 4.7602
Training Epoch: 0 [544/50000]	Loss: 4.9113
Training Epoch: 0 [560/50000]	Loss: 4.6613
Training Epoch: 0 [576/50000]	Loss: 4.6988
Training Epoch: 0 [592/50000]	Loss: 5.0089
Training Epoch: 0 [608/50000]	Loss: 4.9702
Training Epoch: 0 [624/50000]	Loss: 5.7283
Training Epoch: 0 [640/50000]	Loss: 4.9397
Training Epoch: 0 [656/50000]	Loss: 5.1819
Training Epoch: 0 [672/50000]	Loss: 4.6573
Training Epoch: 0 [688/50000]	Loss: 5.0382
Training Epoch: 0 [704/50000]	Loss: 4.5399
Training Epoch: 0 [720/50000]	Loss: 4.9448
Training Epoch: 0 [736/50000]	Loss: 5.0049
Training Epoch: 0 [752/50000]	Loss: 4.9812
Training Epoch: 0 [768/50000]	Loss: 5.2571
Training Epoch: 0 [784/50000]	Loss: 4.8692
Training Epoch: 0 [800/50000]	Loss: 4.9819
Profile done with power limit 125W
epoch 1 train time consumed: 3.49s
Validation Epoch: 0, Average loss: 0.3025, Accuracy: 0.0115
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 125000, 'lr': 0.0014142135623730952, 'energy': 116.51729784111846, 'time': 2.4022650550004983, 'accuracy': 0.0115, 'total_cost': 12.17916204403289}
[run job] Launching job with BS 16: and LR: 0.0014142135623730952 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs16+lr0.00141+pl100', 'ZEUS_COST_THRESH': '24.35832408806578', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '0.0014142135623730952', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0014142135623730952', '--power_limit', '100']
[run job] cost_ub=24.35832408806578
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs16+lr0.00141+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.0014142135623730952
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6726
Training Epoch: 0 [48/50000]	Loss: 5.0931
Training Epoch: 0 [64/50000]	Loss: 5.2535
Training Epoch: 0 [80/50000]	Loss: 5.2397
Training Epoch: 0 [96/50000]	Loss: 5.4470
Training Epoch: 0 [112/50000]	Loss: 5.5600
Training Epoch: 0 [128/50000]	Loss: 5.7951
Training Epoch: 0 [144/50000]	Loss: 5.4155
Training Epoch: 0 [160/50000]	Loss: 5.7066
Training Epoch: 0 [176/50000]	Loss: 5.2959
Training Epoch: 0 [192/50000]	Loss: 5.0629
Training Epoch: 0 [208/50000]	Loss: 5.8219
Training Epoch: 0 [224/50000]	Loss: 5.1138
Training Epoch: 0 [240/50000]	Loss: 5.1078
Training Epoch: 0 [256/50000]	Loss: 5.3037
Training Epoch: 0 [272/50000]	Loss: 5.1577
Training Epoch: 0 [288/50000]	Loss: 5.5513
Training Epoch: 0 [304/50000]	Loss: 5.6370
Training Epoch: 0 [320/50000]	Loss: 5.0840
Training Epoch: 0 [336/50000]	Loss: 5.4284
Training Epoch: 0 [352/50000]	Loss: 4.8215
Training Epoch: 0 [368/50000]	Loss: 5.6575
Training Epoch: 0 [384/50000]	Loss: 5.0428
Training Epoch: 0 [400/50000]	Loss: 4.8806
Training Epoch: 0 [416/50000]	Loss: 4.9257
Training Epoch: 0 [432/50000]	Loss: 4.5110
Training Epoch: 0 [448/50000]	Loss: 5.1288
Training Epoch: 0 [464/50000]	Loss: 5.0286
Training Epoch: 0 [480/50000]	Loss: 5.2366
Training Epoch: 0 [496/50000]	Loss: 4.9139
Training Epoch: 0 [512/50000]	Loss: 4.9934
Training Epoch: 0 [528/50000]	Loss: 4.7229
Training Epoch: 0 [544/50000]	Loss: 4.8499
Training Epoch: 0 [560/50000]	Loss: 4.6949
Training Epoch: 0 [576/50000]	Loss: 4.6846
Training Epoch: 0 [592/50000]	Loss: 5.0493
Training Epoch: 0 [608/50000]	Loss: 4.9931
Training Epoch: 0 [624/50000]	Loss: 5.7656
Training Epoch: 0 [640/50000]	Loss: 5.0263
Training Epoch: 0 [656/50000]	Loss: 5.2077
Training Epoch: 0 [672/50000]	Loss: 4.7132
Training Epoch: 0 [688/50000]	Loss: 5.0518
Training Epoch: 0 [704/50000]	Loss: 4.5402
Training Epoch: 0 [720/50000]	Loss: 4.9469
Training Epoch: 0 [736/50000]	Loss: 4.8966
Training Epoch: 0 [752/50000]	Loss: 4.8824
Training Epoch: 0 [768/50000]	Loss: 5.3133
Training Epoch: 0 [784/50000]	Loss: 4.7528
Training Epoch: 0 [800/50000]	Loss: 5.0246
Profile done with power limit 100W
epoch 1 train time consumed: 3.82s
Validation Epoch: 0, Average loss: 0.3052, Accuracy: 0.0120
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 100000, 'lr': 0.0014142135623730952, 'energy': 99.01141392081192, 'time': 2.6990052159999323, 'accuracy': 0.012, 'total_cost': 12.325970590263132}

[Power Profiler] with batch size 16 and learning rate 0.0016970562748477142
[run job] Launching job with BS 16: and LR: 0.0016970562748477142 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs16+lr0.00170+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '0.0016970562748477142', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0016970562748477142', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs16+lr0.00170+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.0016970562748477142
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6893
Training Epoch: 0 [48/50000]	Loss: 5.2448
Training Epoch: 0 [64/50000]	Loss: 5.4474
Training Epoch: 0 [80/50000]	Loss: 5.3662
Training Epoch: 0 [96/50000]	Loss: 5.7242
Training Epoch: 0 [112/50000]	Loss: 5.8045
Training Epoch: 0 [128/50000]	Loss: 5.9598
Training Epoch: 0 [144/50000]	Loss: 5.5603
Training Epoch: 0 [160/50000]	Loss: 5.7546
Training Epoch: 0 [176/50000]	Loss: 5.4002
Training Epoch: 0 [192/50000]	Loss: 5.0831
Training Epoch: 0 [208/50000]	Loss: 5.9262
Training Epoch: 0 [224/50000]	Loss: 5.2211
Training Epoch: 0 [240/50000]	Loss: 5.1327
Training Epoch: 0 [256/50000]	Loss: 5.4791
Training Epoch: 0 [272/50000]	Loss: 5.3913
Training Epoch: 0 [288/50000]	Loss: 5.8330
Training Epoch: 0 [304/50000]	Loss: 5.7585
Training Epoch: 0 [320/50000]	Loss: 5.1247
Training Epoch: 0 [336/50000]	Loss: 5.5062
Training Epoch: 0 [352/50000]	Loss: 4.8884
Training Epoch: 0 [368/50000]	Loss: 5.6696
Training Epoch: 0 [384/50000]	Loss: 5.0536
Training Epoch: 0 [400/50000]	Loss: 4.9460
Training Epoch: 0 [416/50000]	Loss: 4.9195
Training Epoch: 0 [432/50000]	Loss: 4.5789
Training Epoch: 0 [448/50000]	Loss: 5.2554
Training Epoch: 0 [464/50000]	Loss: 4.9947
Training Epoch: 0 [480/50000]	Loss: 5.4798
Training Epoch: 0 [496/50000]	Loss: 4.9111
Training Epoch: 0 [512/50000]	Loss: 5.0478
Training Epoch: 0 [528/50000]	Loss: 4.8541
Training Epoch: 0 [544/50000]	Loss: 4.9851
Training Epoch: 0 [560/50000]	Loss: 4.7296
Training Epoch: 0 [576/50000]	Loss: 4.6466
Training Epoch: 0 [592/50000]	Loss: 5.0853
Training Epoch: 0 [608/50000]	Loss: 5.0534
Training Epoch: 0 [624/50000]	Loss: 5.7551
Training Epoch: 0 [640/50000]	Loss: 5.0225
Training Epoch: 0 [656/50000]	Loss: 5.2795
Training Epoch: 0 [672/50000]	Loss: 4.6775
Training Epoch: 0 [688/50000]	Loss: 5.0661
Training Epoch: 0 [704/50000]	Loss: 4.6024
Training Epoch: 0 [720/50000]	Loss: 5.0697
Training Epoch: 0 [736/50000]	Loss: 5.0605
Training Epoch: 0 [752/50000]	Loss: 4.8574
Training Epoch: 0 [768/50000]	Loss: 5.2817
Training Epoch: 0 [784/50000]	Loss: 4.8507
Training Epoch: 0 [800/50000]	Loss: 5.1358
Profile done with power limit 175W
epoch 1 train time consumed: 3.55s
Validation Epoch: 0, Average loss: 0.3054, Accuracy: 0.0137
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 175000, 'lr': 0.0016970562748477142, 'energy': 114.13879590036645, 'time': 2.423691926000174, 'accuracy': 0.0137, 'total_cost': 10.230414089301174}
[run job] Launching job with BS 16: and LR: 0.0016970562748477142 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs16+lr0.00170+pl150', 'ZEUS_COST_THRESH': '20.46082817860235', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '0.0016970562748477142', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0016970562748477142', '--power_limit', '150']
[run job] cost_ub=20.46082817860235
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs16+lr0.00170+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.0016970562748477142
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6893
Training Epoch: 0 [48/50000]	Loss: 5.2466
Training Epoch: 0 [64/50000]	Loss: 5.4436
Training Epoch: 0 [80/50000]	Loss: 5.4013
Training Epoch: 0 [96/50000]	Loss: 5.7209
Training Epoch: 0 [112/50000]	Loss: 5.8070
Training Epoch: 0 [128/50000]	Loss: 6.0066
Training Epoch: 0 [144/50000]	Loss: 5.5268
Training Epoch: 0 [160/50000]	Loss: 5.7770
Training Epoch: 0 [176/50000]	Loss: 5.4171
Training Epoch: 0 [192/50000]	Loss: 5.1607
Training Epoch: 0 [208/50000]	Loss: 5.9056
Training Epoch: 0 [224/50000]	Loss: 5.2426
Training Epoch: 0 [240/50000]	Loss: 5.1394
Training Epoch: 0 [256/50000]	Loss: 5.4183
Training Epoch: 0 [272/50000]	Loss: 5.3285
Training Epoch: 0 [288/50000]	Loss: 5.8146
Training Epoch: 0 [304/50000]	Loss: 5.7461
Training Epoch: 0 [320/50000]	Loss: 5.0535
Training Epoch: 0 [336/50000]	Loss: 5.4848
Training Epoch: 0 [352/50000]	Loss: 4.9811
Training Epoch: 0 [368/50000]	Loss: 5.7237
Training Epoch: 0 [384/50000]	Loss: 4.9854
Training Epoch: 0 [400/50000]	Loss: 4.8799
Training Epoch: 0 [416/50000]	Loss: 4.9344
Training Epoch: 0 [432/50000]	Loss: 4.6059
Training Epoch: 0 [448/50000]	Loss: 5.3256
Training Epoch: 0 [464/50000]	Loss: 4.9785
Training Epoch: 0 [480/50000]	Loss: 5.3367
Training Epoch: 0 [496/50000]	Loss: 5.1053
Training Epoch: 0 [512/50000]	Loss: 5.0246
Training Epoch: 0 [528/50000]	Loss: 4.8650
Training Epoch: 0 [544/50000]	Loss: 4.9442
Training Epoch: 0 [560/50000]	Loss: 4.7475
Training Epoch: 0 [576/50000]	Loss: 4.7192
Training Epoch: 0 [592/50000]	Loss: 5.0716
Training Epoch: 0 [608/50000]	Loss: 4.9759
Training Epoch: 0 [624/50000]	Loss: 5.7726
Training Epoch: 0 [640/50000]	Loss: 4.9478
Training Epoch: 0 [656/50000]	Loss: 5.2394
Training Epoch: 0 [672/50000]	Loss: 4.6641
Training Epoch: 0 [688/50000]	Loss: 5.0901
Training Epoch: 0 [704/50000]	Loss: 4.5863
Training Epoch: 0 [720/50000]	Loss: 4.9956
Training Epoch: 0 [736/50000]	Loss: 5.0759
Training Epoch: 0 [752/50000]	Loss: 4.9818
Training Epoch: 0 [768/50000]	Loss: 5.4087
Training Epoch: 0 [784/50000]	Loss: 4.8960
Training Epoch: 0 [800/50000]	Loss: 5.0656
Profile done with power limit 150W
epoch 1 train time consumed: 3.56s
Validation Epoch: 0, Average loss: 0.3035, Accuracy: 0.0121
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 150000, 'lr': 0.0016970562748477142, 'energy': 114.89630125394172, 'time': 2.4563098240005274, 'accuracy': 0.0121, 'total_cost': 11.769836904321876}
[run job] Launching job with BS 16: and LR: 0.0016970562748477142 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs16+lr0.00170+pl125', 'ZEUS_COST_THRESH': '20.46082817860235', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '0.0016970562748477142', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0016970562748477142', '--power_limit', '125']
[run job] cost_ub=20.46082817860235
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs16+lr0.00170+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.0016970562748477142
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6893
Training Epoch: 0 [48/50000]	Loss: 5.2469
Training Epoch: 0 [64/50000]	Loss: 5.4449
Training Epoch: 0 [80/50000]	Loss: 5.3420
Training Epoch: 0 [96/50000]	Loss: 5.7144
Training Epoch: 0 [112/50000]	Loss: 5.7955
Training Epoch: 0 [128/50000]	Loss: 5.9687
Training Epoch: 0 [144/50000]	Loss: 5.5162
Training Epoch: 0 [160/50000]	Loss: 5.7616
Training Epoch: 0 [176/50000]	Loss: 5.4066
Training Epoch: 0 [192/50000]	Loss: 5.1440
Training Epoch: 0 [208/50000]	Loss: 5.9259
Training Epoch: 0 [224/50000]	Loss: 5.2161
Training Epoch: 0 [240/50000]	Loss: 5.1757
Training Epoch: 0 [256/50000]	Loss: 5.3520
Training Epoch: 0 [272/50000]	Loss: 5.3888
Training Epoch: 0 [288/50000]	Loss: 5.8552
Training Epoch: 0 [304/50000]	Loss: 5.7009
Training Epoch: 0 [320/50000]	Loss: 5.0434
Training Epoch: 0 [336/50000]	Loss: 5.4582
Training Epoch: 0 [352/50000]	Loss: 4.9839
Training Epoch: 0 [368/50000]	Loss: 5.8146
Training Epoch: 0 [384/50000]	Loss: 4.9671
Training Epoch: 0 [400/50000]	Loss: 4.9277
Training Epoch: 0 [416/50000]	Loss: 4.8482
Training Epoch: 0 [432/50000]	Loss: 4.5583
Training Epoch: 0 [448/50000]	Loss: 5.2987
Training Epoch: 0 [464/50000]	Loss: 4.9699
Training Epoch: 0 [480/50000]	Loss: 5.4618
Training Epoch: 0 [496/50000]	Loss: 4.9628
Training Epoch: 0 [512/50000]	Loss: 5.1178
Training Epoch: 0 [528/50000]	Loss: 4.8491
Training Epoch: 0 [544/50000]	Loss: 4.9428
Training Epoch: 0 [560/50000]	Loss: 4.6736
Training Epoch: 0 [576/50000]	Loss: 4.6576
Training Epoch: 0 [592/50000]	Loss: 5.1239
Training Epoch: 0 [608/50000]	Loss: 4.9397
Training Epoch: 0 [624/50000]	Loss: 5.7136
Training Epoch: 0 [640/50000]	Loss: 4.9375
Training Epoch: 0 [656/50000]	Loss: 5.2062
Training Epoch: 0 [672/50000]	Loss: 4.6153
Training Epoch: 0 [688/50000]	Loss: 5.0332
Training Epoch: 0 [704/50000]	Loss: 4.5851
Training Epoch: 0 [720/50000]	Loss: 4.9883
Training Epoch: 0 [736/50000]	Loss: 4.9272
Training Epoch: 0 [752/50000]	Loss: 5.0081
Training Epoch: 0 [768/50000]	Loss: 5.2889
Training Epoch: 0 [784/50000]	Loss: 4.8618
Training Epoch: 0 [800/50000]	Loss: 5.0857
Profile done with power limit 125W
epoch 1 train time consumed: 3.59s
Validation Epoch: 0, Average loss: 0.3082, Accuracy: 0.0133
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 125000, 'lr': 0.0016970562748477142, 'energy': 114.35799974466042, 'time': 2.4782037220002167, 'accuracy': 0.0133, 'total_cost': 10.783279277560233}
[run job] Launching job with BS 16: and LR: 0.0016970562748477142 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs16+lr0.00170+pl100', 'ZEUS_COST_THRESH': '20.46082817860235', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '0.0016970562748477142', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0016970562748477142', '--power_limit', '100']
[run job] cost_ub=20.46082817860235
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs16+lr0.00170+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.0016970562748477142
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6893
Training Epoch: 0 [48/50000]	Loss: 5.2459
Training Epoch: 0 [64/50000]	Loss: 5.4447
Training Epoch: 0 [80/50000]	Loss: 5.3851
Training Epoch: 0 [96/50000]	Loss: 5.7283
Training Epoch: 0 [112/50000]	Loss: 5.7428
Training Epoch: 0 [128/50000]	Loss: 5.9671
Training Epoch: 0 [144/50000]	Loss: 5.5101
Training Epoch: 0 [160/50000]	Loss: 5.7800
Training Epoch: 0 [176/50000]	Loss: 5.3859
Training Epoch: 0 [192/50000]	Loss: 5.1612
Training Epoch: 0 [208/50000]	Loss: 5.9757
Training Epoch: 0 [224/50000]	Loss: 5.2100
Training Epoch: 0 [240/50000]	Loss: 5.1014
Training Epoch: 0 [256/50000]	Loss: 5.3973
Training Epoch: 0 [272/50000]	Loss: 5.3213
Training Epoch: 0 [288/50000]	Loss: 5.8412
Training Epoch: 0 [304/50000]	Loss: 5.7479
Training Epoch: 0 [320/50000]	Loss: 5.1506
Training Epoch: 0 [336/50000]	Loss: 5.3985
Training Epoch: 0 [352/50000]	Loss: 4.9266
Training Epoch: 0 [368/50000]	Loss: 5.7085
Training Epoch: 0 [384/50000]	Loss: 4.9000
Training Epoch: 0 [400/50000]	Loss: 4.9248
Training Epoch: 0 [416/50000]	Loss: 4.8888
Training Epoch: 0 [432/50000]	Loss: 4.4501
Training Epoch: 0 [448/50000]	Loss: 5.3612
Training Epoch: 0 [464/50000]	Loss: 4.8275
Training Epoch: 0 [480/50000]	Loss: 5.5612
Training Epoch: 0 [496/50000]	Loss: 4.6979
Training Epoch: 0 [512/50000]	Loss: 5.2143
Training Epoch: 0 [528/50000]	Loss: 4.9676
Training Epoch: 0 [544/50000]	Loss: 5.1600
Training Epoch: 0 [560/50000]	Loss: 4.7912
Training Epoch: 0 [576/50000]	Loss: 4.6673
Training Epoch: 0 [592/50000]	Loss: 5.1497
Training Epoch: 0 [608/50000]	Loss: 5.0440
Training Epoch: 0 [624/50000]	Loss: 5.7997
Training Epoch: 0 [640/50000]	Loss: 5.0462
Training Epoch: 0 [656/50000]	Loss: 5.2169
Training Epoch: 0 [672/50000]	Loss: 4.7419
Training Epoch: 0 [688/50000]	Loss: 4.9747
Training Epoch: 0 [704/50000]	Loss: 4.6474
Training Epoch: 0 [720/50000]	Loss: 5.0015
Training Epoch: 0 [736/50000]	Loss: 5.0212
Training Epoch: 0 [752/50000]	Loss: 5.0150
Training Epoch: 0 [768/50000]	Loss: 5.3296
Training Epoch: 0 [784/50000]	Loss: 4.9192
Training Epoch: 0 [800/50000]	Loss: 5.0901
Profile done with power limit 100W
epoch 1 train time consumed: 3.88s
Validation Epoch: 0, Average loss: 0.3045, Accuracy: 0.0139
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 100000, 'lr': 0.0016970562748477142, 'energy': 98.8875421840345, 'time': 2.746671421999963, 'accuracy': 0.0139, 'total_cost': 10.824159495808589}

[Power Profiler] with batch size 32 and learning rate 0.0016
[run job] Launching job with BS 32: and LR: 0.0016 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs32+lr0.00160+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '0.0016', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0016', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs32+lr0.00160+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.0016
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 4.8917
Training Epoch: 0 [96/50016]	Loss: 4.9880
Training Epoch: 0 [128/50016]	Loss: 5.1738
Training Epoch: 0 [160/50016]	Loss: 5.3077
Training Epoch: 0 [192/50016]	Loss: 5.1259
Training Epoch: 0 [224/50016]	Loss: 5.1353
Training Epoch: 0 [256/50016]	Loss: 5.1311
Training Epoch: 0 [288/50016]	Loss: 5.0903
Training Epoch: 0 [320/50016]	Loss: 5.1525
Training Epoch: 0 [352/50016]	Loss: 4.9311
Training Epoch: 0 [384/50016]	Loss: 5.2795
Training Epoch: 0 [416/50016]	Loss: 4.9597
Training Epoch: 0 [448/50016]	Loss: 4.8161
Training Epoch: 0 [480/50016]	Loss: 5.1000
Training Epoch: 0 [512/50016]	Loss: 4.9635
Training Epoch: 0 [544/50016]	Loss: 4.7472
Training Epoch: 0 [576/50016]	Loss: 4.6095
Training Epoch: 0 [608/50016]	Loss: 5.0759
Training Epoch: 0 [640/50016]	Loss: 5.4931
Training Epoch: 0 [672/50016]	Loss: 4.9326
Training Epoch: 0 [704/50016]	Loss: 4.6125
Training Epoch: 0 [736/50016]	Loss: 5.0413
Training Epoch: 0 [768/50016]	Loss: 5.3698
Training Epoch: 0 [800/50016]	Loss: 4.9187
Training Epoch: 0 [832/50016]	Loss: 5.0847
Training Epoch: 0 [864/50016]	Loss: 5.2528
Training Epoch: 0 [896/50016]	Loss: 4.6525
Training Epoch: 0 [928/50016]	Loss: 4.8168
Training Epoch: 0 [960/50016]	Loss: 4.7434
Training Epoch: 0 [992/50016]	Loss: 4.8024
Training Epoch: 0 [1024/50016]	Loss: 4.7348
Training Epoch: 0 [1056/50016]	Loss: 4.9234
Training Epoch: 0 [1088/50016]	Loss: 4.6771
Training Epoch: 0 [1120/50016]	Loss: 4.9002
Training Epoch: 0 [1152/50016]	Loss: 4.6287
Training Epoch: 0 [1184/50016]	Loss: 4.7022
Training Epoch: 0 [1216/50016]	Loss: 4.5417
Training Epoch: 0 [1248/50016]	Loss: 4.7196
Training Epoch: 0 [1280/50016]	Loss: 4.6667
Training Epoch: 0 [1312/50016]	Loss: 4.5991
Training Epoch: 0 [1344/50016]	Loss: 4.6966
Training Epoch: 0 [1376/50016]	Loss: 4.5745
Training Epoch: 0 [1408/50016]	Loss: 4.6341
Training Epoch: 0 [1440/50016]	Loss: 4.8326
Training Epoch: 0 [1472/50016]	Loss: 4.3861
Training Epoch: 0 [1504/50016]	Loss: 4.9167
Training Epoch: 0 [1536/50016]	Loss: 4.5671
Training Epoch: 0 [1568/50016]	Loss: 4.7522
Training Epoch: 0 [1600/50016]	Loss: 4.9289
Profile done with power limit 175W
epoch 1 train time consumed: 4.69s
Validation Epoch: 0, Average loss: 0.1900, Accuracy: 0.0222
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 175000, 'lr': 0.0016, 'energy': 128.95859008873185, 'time': 3.322503302999394, 'accuracy': 0.022164536741214057, 'total_cost': 18.219742025184775}
[run job] Launching job with BS 32: and LR: 0.0016 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs32+lr0.00160+pl150', 'ZEUS_COST_THRESH': '36.43948405036955', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '0.0016', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0016', '--power_limit', '150']
[run job] cost_ub=36.43948405036955
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs32+lr0.00160+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.0016
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 4.8917
Training Epoch: 0 [96/50016]	Loss: 4.9879
Training Epoch: 0 [128/50016]	Loss: 5.1861
Training Epoch: 0 [160/50016]	Loss: 5.3024
Training Epoch: 0 [192/50016]	Loss: 5.1275
Training Epoch: 0 [224/50016]	Loss: 5.1131
Training Epoch: 0 [256/50016]	Loss: 5.1645
Training Epoch: 0 [288/50016]	Loss: 5.0545
Training Epoch: 0 [320/50016]	Loss: 5.1100
Training Epoch: 0 [352/50016]	Loss: 4.9613
Training Epoch: 0 [384/50016]	Loss: 5.3266
Training Epoch: 0 [416/50016]	Loss: 4.9812
Training Epoch: 0 [448/50016]	Loss: 4.8380
Training Epoch: 0 [480/50016]	Loss: 5.0600
Training Epoch: 0 [512/50016]	Loss: 4.9477
Training Epoch: 0 [544/50016]	Loss: 4.7940
Training Epoch: 0 [576/50016]	Loss: 4.6264
Training Epoch: 0 [608/50016]	Loss: 5.1184
Training Epoch: 0 [640/50016]	Loss: 5.4176
Training Epoch: 0 [672/50016]	Loss: 4.9372
Training Epoch: 0 [704/50016]	Loss: 4.6426
Training Epoch: 0 [736/50016]	Loss: 5.0287
Training Epoch: 0 [768/50016]	Loss: 5.3782
Training Epoch: 0 [800/50016]	Loss: 4.9279
Training Epoch: 0 [832/50016]	Loss: 5.1239
Training Epoch: 0 [864/50016]	Loss: 5.2303
Training Epoch: 0 [896/50016]	Loss: 4.5318
Training Epoch: 0 [928/50016]	Loss: 4.7641
Training Epoch: 0 [960/50016]	Loss: 4.7989
Training Epoch: 0 [992/50016]	Loss: 4.8150
Training Epoch: 0 [1024/50016]	Loss: 4.7231
Training Epoch: 0 [1056/50016]	Loss: 4.9644
Training Epoch: 0 [1088/50016]	Loss: 4.7038
Training Epoch: 0 [1120/50016]	Loss: 4.8773
Training Epoch: 0 [1152/50016]	Loss: 4.6390
Training Epoch: 0 [1184/50016]	Loss: 4.7127
Training Epoch: 0 [1216/50016]	Loss: 4.5304
Training Epoch: 0 [1248/50016]	Loss: 4.7331
Training Epoch: 0 [1280/50016]	Loss: 4.6526
Training Epoch: 0 [1312/50016]	Loss: 4.5712
Training Epoch: 0 [1344/50016]	Loss: 4.7709
Training Epoch: 0 [1376/50016]	Loss: 4.5717
Training Epoch: 0 [1408/50016]	Loss: 4.6176
Training Epoch: 0 [1440/50016]	Loss: 4.9607
Training Epoch: 0 [1472/50016]	Loss: 4.4772
Training Epoch: 0 [1504/50016]	Loss: 4.7802
Training Epoch: 0 [1536/50016]	Loss: 4.6085
Training Epoch: 0 [1568/50016]	Loss: 4.5540
Training Epoch: 0 [1600/50016]	Loss: 4.8662
Profile done with power limit 150W
epoch 1 train time consumed: 4.67s
Validation Epoch: 0, Average loss: 0.1949, Accuracy: 0.0239
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 150000, 'lr': 0.0016, 'energy': 129.79704682607593, 'time': 3.287513893999858, 'accuracy': 0.023861821086261982, 'total_cost': 16.791744056678684}
[run job] Launching job with BS 32: and LR: 0.0016 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs32+lr0.00160+pl125', 'ZEUS_COST_THRESH': '33.58348811335737', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '0.0016', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0016', '--power_limit', '125']
[run job] cost_ub=33.58348811335737
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs32+lr0.00160+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.0016
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 4.8917
Training Epoch: 0 [96/50016]	Loss: 4.9889
Training Epoch: 0 [128/50016]	Loss: 5.1697
Training Epoch: 0 [160/50016]	Loss: 5.3279
Training Epoch: 0 [192/50016]	Loss: 5.1143
Training Epoch: 0 [224/50016]	Loss: 5.1203
Training Epoch: 0 [256/50016]	Loss: 5.1385
Training Epoch: 0 [288/50016]	Loss: 5.0592
Training Epoch: 0 [320/50016]	Loss: 5.1574
Training Epoch: 0 [352/50016]	Loss: 4.9467
Training Epoch: 0 [384/50016]	Loss: 5.3172
Training Epoch: 0 [416/50016]	Loss: 4.9595
Training Epoch: 0 [448/50016]	Loss: 4.8470
Training Epoch: 0 [480/50016]	Loss: 5.0981
Training Epoch: 0 [512/50016]	Loss: 4.9728
Training Epoch: 0 [544/50016]	Loss: 4.7651
Training Epoch: 0 [576/50016]	Loss: 4.6143
Training Epoch: 0 [608/50016]	Loss: 5.0967
Training Epoch: 0 [640/50016]	Loss: 5.3725
Training Epoch: 0 [672/50016]	Loss: 4.9251
Training Epoch: 0 [704/50016]	Loss: 4.7000
Training Epoch: 0 [736/50016]	Loss: 5.0280
Training Epoch: 0 [768/50016]	Loss: 5.3068
Training Epoch: 0 [800/50016]	Loss: 4.8938
Training Epoch: 0 [832/50016]	Loss: 5.0418
Training Epoch: 0 [864/50016]	Loss: 5.1477
Training Epoch: 0 [896/50016]	Loss: 4.5270
Training Epoch: 0 [928/50016]	Loss: 4.7845
Training Epoch: 0 [960/50016]	Loss: 4.6838
Training Epoch: 0 [992/50016]	Loss: 4.8076
Training Epoch: 0 [1024/50016]	Loss: 4.6460
Training Epoch: 0 [1056/50016]	Loss: 4.9797
Training Epoch: 0 [1088/50016]	Loss: 4.6413
Training Epoch: 0 [1120/50016]	Loss: 4.9258
Training Epoch: 0 [1152/50016]	Loss: 4.6780
Training Epoch: 0 [1184/50016]	Loss: 4.7129
Training Epoch: 0 [1216/50016]	Loss: 4.7284
Training Epoch: 0 [1248/50016]	Loss: 4.8378
Training Epoch: 0 [1280/50016]	Loss: 4.6782
Training Epoch: 0 [1312/50016]	Loss: 4.6916
Training Epoch: 0 [1344/50016]	Loss: 4.6966
Training Epoch: 0 [1376/50016]	Loss: 4.8327
Training Epoch: 0 [1408/50016]	Loss: 4.6981
Training Epoch: 0 [1440/50016]	Loss: 4.8592
Training Epoch: 0 [1472/50016]	Loss: 4.3312
Training Epoch: 0 [1504/50016]	Loss: 4.7869
Training Epoch: 0 [1536/50016]	Loss: 4.7371
Training Epoch: 0 [1568/50016]	Loss: 4.8639
Training Epoch: 0 [1600/50016]	Loss: 5.1692
Profile done with power limit 125W
epoch 1 train time consumed: 4.69s
Validation Epoch: 0, Average loss: 0.1531, Accuracy: 0.0221
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 125000, 'lr': 0.0016, 'energy': 122.96939844968732, 'time': 3.342067035000582, 'accuracy': 0.022064696485623002, 'total_cost': 18.047202957027505}
[run job] Launching job with BS 32: and LR: 0.0016 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs32+lr0.00160+pl100', 'ZEUS_COST_THRESH': '33.58348811335737', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '0.0016', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0016', '--power_limit', '100']
[run job] cost_ub=33.58348811335737
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs32+lr0.00160+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.0016
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 4.8917
Training Epoch: 0 [96/50016]	Loss: 4.9883
Training Epoch: 0 [128/50016]	Loss: 5.1825
Training Epoch: 0 [160/50016]	Loss: 5.3074
Training Epoch: 0 [192/50016]	Loss: 5.1138
Training Epoch: 0 [224/50016]	Loss: 5.1110
Training Epoch: 0 [256/50016]	Loss: 5.1606
Training Epoch: 0 [288/50016]	Loss: 5.0873
Training Epoch: 0 [320/50016]	Loss: 5.1312
Training Epoch: 0 [352/50016]	Loss: 4.9229
Training Epoch: 0 [384/50016]	Loss: 5.3299
Training Epoch: 0 [416/50016]	Loss: 4.9306
Training Epoch: 0 [448/50016]	Loss: 4.8467
Training Epoch: 0 [480/50016]	Loss: 5.0725
Training Epoch: 0 [512/50016]	Loss: 4.9034
Training Epoch: 0 [544/50016]	Loss: 4.8485
Training Epoch: 0 [576/50016]	Loss: 4.6233
Training Epoch: 0 [608/50016]	Loss: 5.1112
Training Epoch: 0 [640/50016]	Loss: 5.4215
Training Epoch: 0 [672/50016]	Loss: 4.9016
Training Epoch: 0 [704/50016]	Loss: 4.6441
Training Epoch: 0 [736/50016]	Loss: 5.0327
Training Epoch: 0 [768/50016]	Loss: 5.3815
Training Epoch: 0 [800/50016]	Loss: 4.9671
Training Epoch: 0 [832/50016]	Loss: 5.0615
Training Epoch: 0 [864/50016]	Loss: 5.1963
Training Epoch: 0 [896/50016]	Loss: 4.6135
Training Epoch: 0 [928/50016]	Loss: 4.8291
Training Epoch: 0 [960/50016]	Loss: 4.7122
Training Epoch: 0 [992/50016]	Loss: 4.7682
Training Epoch: 0 [1024/50016]	Loss: 4.7703
Training Epoch: 0 [1056/50016]	Loss: 4.9266
Training Epoch: 0 [1088/50016]	Loss: 4.6925
Training Epoch: 0 [1120/50016]	Loss: 4.8700
Training Epoch: 0 [1152/50016]	Loss: 4.6608
Training Epoch: 0 [1184/50016]	Loss: 4.7276
Training Epoch: 0 [1216/50016]	Loss: 4.6592
Training Epoch: 0 [1248/50016]	Loss: 4.7056
Training Epoch: 0 [1280/50016]	Loss: 4.7461
Training Epoch: 0 [1312/50016]	Loss: 4.6490
Training Epoch: 0 [1344/50016]	Loss: 4.7326
Training Epoch: 0 [1376/50016]	Loss: 4.6212
Training Epoch: 0 [1408/50016]	Loss: 4.6123
Training Epoch: 0 [1440/50016]	Loss: 4.7961
Training Epoch: 0 [1472/50016]	Loss: 4.3862
Training Epoch: 0 [1504/50016]	Loss: 5.0018
Training Epoch: 0 [1536/50016]	Loss: 4.6599
Training Epoch: 0 [1568/50016]	Loss: 4.8611
Training Epoch: 0 [1600/50016]	Loss: 4.9724
Profile done with power limit 100W
epoch 1 train time consumed: 5.52s
Validation Epoch: 0, Average loss: 0.2061, Accuracy: 0.0237
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 100000, 'lr': 0.0016, 'energy': 99.19686766948467, 'time': 4.022859483000502, 'accuracy': 0.02366214057507987, 'total_cost': 18.640792044674335}

[Power Profiler] with batch size 32 and learning rate 0.002
[run job] Launching job with BS 32: and LR: 0.002 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs32+lr0.00200+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '0.002', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '0.002', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs32+lr0.00200+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.002
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 5.0068
Training Epoch: 0 [96/50016]	Loss: 5.1678
Training Epoch: 0 [128/50016]	Loss: 5.4362
Training Epoch: 0 [160/50016]	Loss: 5.5058
Training Epoch: 0 [192/50016]	Loss: 5.2286
Training Epoch: 0 [224/50016]	Loss: 5.2952
Training Epoch: 0 [256/50016]	Loss: 5.1743
Training Epoch: 0 [288/50016]	Loss: 5.2160
Training Epoch: 0 [320/50016]	Loss: 5.2252
Training Epoch: 0 [352/50016]	Loss: 5.0735
Training Epoch: 0 [384/50016]	Loss: 5.5653
Training Epoch: 0 [416/50016]	Loss: 5.0582
Training Epoch: 0 [448/50016]	Loss: 4.8972
Training Epoch: 0 [480/50016]	Loss: 5.2000
Training Epoch: 0 [512/50016]	Loss: 5.0390
Training Epoch: 0 [544/50016]	Loss: 4.8846
Training Epoch: 0 [576/50016]	Loss: 4.6579
Training Epoch: 0 [608/50016]	Loss: 5.1718
Training Epoch: 0 [640/50016]	Loss: 5.5351
Training Epoch: 0 [672/50016]	Loss: 4.9763
Training Epoch: 0 [704/50016]	Loss: 4.7526
Training Epoch: 0 [736/50016]	Loss: 5.0685
Training Epoch: 0 [768/50016]	Loss: 5.3263
Training Epoch: 0 [800/50016]	Loss: 4.9380
Training Epoch: 0 [832/50016]	Loss: 5.1359
Training Epoch: 0 [864/50016]	Loss: 5.3015
Training Epoch: 0 [896/50016]	Loss: 4.6712
Training Epoch: 0 [928/50016]	Loss: 4.8130
Training Epoch: 0 [960/50016]	Loss: 4.7279
Training Epoch: 0 [992/50016]	Loss: 4.8470
Training Epoch: 0 [1024/50016]	Loss: 4.6444
Training Epoch: 0 [1056/50016]	Loss: 4.9945
Training Epoch: 0 [1088/50016]	Loss: 4.7180
Training Epoch: 0 [1120/50016]	Loss: 4.9280
Training Epoch: 0 [1152/50016]	Loss: 4.6847
Training Epoch: 0 [1184/50016]	Loss: 4.4991
Training Epoch: 0 [1216/50016]	Loss: 4.5940
Training Epoch: 0 [1248/50016]	Loss: 4.8095
Training Epoch: 0 [1280/50016]	Loss: 4.7584
Training Epoch: 0 [1312/50016]	Loss: 4.6319
Training Epoch: 0 [1344/50016]	Loss: 4.6291
Training Epoch: 0 [1376/50016]	Loss: 4.7044
Training Epoch: 0 [1408/50016]	Loss: 4.5775
Training Epoch: 0 [1440/50016]	Loss: 4.8360
Training Epoch: 0 [1472/50016]	Loss: 4.3474
Training Epoch: 0 [1504/50016]	Loss: 4.7480
Training Epoch: 0 [1536/50016]	Loss: 4.6034
Training Epoch: 0 [1568/50016]	Loss: 4.7503
Training Epoch: 0 [1600/50016]	Loss: 5.0191
Profile done with power limit 175W
epoch 1 train time consumed: 4.53s
Validation Epoch: 0, Average loss: 0.1943, Accuracy: 0.0205
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 175000, 'lr': 0.002, 'energy': 130.23799443548376, 'time': 3.2146951790000458, 'accuracy': 0.020467252396166133, 'total_cost': 19.17078446183168}
[run job] Launching job with BS 32: and LR: 0.002 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs32+lr0.00200+pl150', 'ZEUS_COST_THRESH': '38.34156892366336', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '0.002', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '0.002', '--power_limit', '150']
[run job] cost_ub=38.34156892366336
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs32+lr0.00200+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.002
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 5.0068
Training Epoch: 0 [96/50016]	Loss: 5.1684
Training Epoch: 0 [128/50016]	Loss: 5.4285
Training Epoch: 0 [160/50016]	Loss: 5.5106
Training Epoch: 0 [192/50016]	Loss: 5.2673
Training Epoch: 0 [224/50016]	Loss: 5.2875
Training Epoch: 0 [256/50016]	Loss: 5.2025
Training Epoch: 0 [288/50016]	Loss: 5.2078
Training Epoch: 0 [320/50016]	Loss: 5.2285
Training Epoch: 0 [352/50016]	Loss: 5.0832
Training Epoch: 0 [384/50016]	Loss: 5.5718
Training Epoch: 0 [416/50016]	Loss: 5.0162
Training Epoch: 0 [448/50016]	Loss: 4.8676
Training Epoch: 0 [480/50016]	Loss: 5.1445
Training Epoch: 0 [512/50016]	Loss: 4.9859
Training Epoch: 0 [544/50016]	Loss: 4.8978
Training Epoch: 0 [576/50016]	Loss: 4.6700
Training Epoch: 0 [608/50016]	Loss: 5.1439
Training Epoch: 0 [640/50016]	Loss: 5.5614
Training Epoch: 0 [672/50016]	Loss: 4.9911
Training Epoch: 0 [704/50016]	Loss: 4.7298
Training Epoch: 0 [736/50016]	Loss: 5.0409
Training Epoch: 0 [768/50016]	Loss: 5.2344
Training Epoch: 0 [800/50016]	Loss: 4.9892
Training Epoch: 0 [832/50016]	Loss: 5.1016
Training Epoch: 0 [864/50016]	Loss: 5.3027
Training Epoch: 0 [896/50016]	Loss: 4.6786
Training Epoch: 0 [928/50016]	Loss: 4.8010
Training Epoch: 0 [960/50016]	Loss: 4.7215
Training Epoch: 0 [992/50016]	Loss: 4.7391
Training Epoch: 0 [1024/50016]	Loss: 4.7141
Training Epoch: 0 [1056/50016]	Loss: 4.8925
Training Epoch: 0 [1088/50016]	Loss: 4.6471
Training Epoch: 0 [1120/50016]	Loss: 4.8006
Training Epoch: 0 [1152/50016]	Loss: 4.5778
Training Epoch: 0 [1184/50016]	Loss: 4.5940
Training Epoch: 0 [1216/50016]	Loss: 4.4729
Training Epoch: 0 [1248/50016]	Loss: 4.6818
Training Epoch: 0 [1280/50016]	Loss: 4.6117
Training Epoch: 0 [1312/50016]	Loss: 4.4831
Training Epoch: 0 [1344/50016]	Loss: 4.6478
Training Epoch: 0 [1376/50016]	Loss: 4.6204
Training Epoch: 0 [1408/50016]	Loss: 4.5411
Training Epoch: 0 [1440/50016]	Loss: 4.7473
Training Epoch: 0 [1472/50016]	Loss: 4.2749
Training Epoch: 0 [1504/50016]	Loss: 4.6287
Training Epoch: 0 [1536/50016]	Loss: 4.5884
Training Epoch: 0 [1568/50016]	Loss: 4.6714
Training Epoch: 0 [1600/50016]	Loss: 4.8532
Profile done with power limit 150W
epoch 1 train time consumed: 4.63s
Validation Epoch: 0, Average loss: 0.2061, Accuracy: 0.0279
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 150000, 'lr': 0.002, 'energy': 129.41047573334137, 'time': 3.2989510980005434, 'accuracy': 0.027855431309904154, 'total_cost': 14.416061406592698}
[run job] Launching job with BS 32: and LR: 0.002 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs32+lr0.00200+pl125', 'ZEUS_COST_THRESH': '28.832122813185396', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '0.002', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '0.002', '--power_limit', '125']
[run job] cost_ub=28.832122813185396
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs32+lr0.00200+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.002
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 5.0068
Training Epoch: 0 [96/50016]	Loss: 5.1678
Training Epoch: 0 [128/50016]	Loss: 5.4304
Training Epoch: 0 [160/50016]	Loss: 5.5396
Training Epoch: 0 [192/50016]	Loss: 5.2242
Training Epoch: 0 [224/50016]	Loss: 5.2420
Training Epoch: 0 [256/50016]	Loss: 5.2083
Training Epoch: 0 [288/50016]	Loss: 5.2197
Training Epoch: 0 [320/50016]	Loss: 5.2605
Training Epoch: 0 [352/50016]	Loss: 5.0554
Training Epoch: 0 [384/50016]	Loss: 5.5070
Training Epoch: 0 [416/50016]	Loss: 5.0382
Training Epoch: 0 [448/50016]	Loss: 4.9195
Training Epoch: 0 [480/50016]	Loss: 5.1180
Training Epoch: 0 [512/50016]	Loss: 5.0147
Training Epoch: 0 [544/50016]	Loss: 4.8915
Training Epoch: 0 [576/50016]	Loss: 4.5334
Training Epoch: 0 [608/50016]	Loss: 5.1303
Training Epoch: 0 [640/50016]	Loss: 5.5593
Training Epoch: 0 [672/50016]	Loss: 5.1190
Training Epoch: 0 [704/50016]	Loss: 4.6937
Training Epoch: 0 [736/50016]	Loss: 5.1695
Training Epoch: 0 [768/50016]	Loss: 5.1625
Training Epoch: 0 [800/50016]	Loss: 5.0533
Training Epoch: 0 [832/50016]	Loss: 5.0312
Training Epoch: 0 [864/50016]	Loss: 5.2983
Training Epoch: 0 [896/50016]	Loss: 4.7456
Training Epoch: 0 [928/50016]	Loss: 4.7136
Training Epoch: 0 [960/50016]	Loss: 4.6934
Training Epoch: 0 [992/50016]	Loss: 4.7018
Training Epoch: 0 [1024/50016]	Loss: 4.7073
Training Epoch: 0 [1056/50016]	Loss: 4.8081
Training Epoch: 0 [1088/50016]	Loss: 4.6215
Training Epoch: 0 [1120/50016]	Loss: 4.7132
Training Epoch: 0 [1152/50016]	Loss: 4.5133
Training Epoch: 0 [1184/50016]	Loss: 4.6187
Training Epoch: 0 [1216/50016]	Loss: 4.5747
Training Epoch: 0 [1248/50016]	Loss: 4.4413
Training Epoch: 0 [1280/50016]	Loss: 4.5943
Training Epoch: 0 [1312/50016]	Loss: 4.4753
Training Epoch: 0 [1344/50016]	Loss: 4.7204
Training Epoch: 0 [1376/50016]	Loss: 4.6315
Training Epoch: 0 [1408/50016]	Loss: 4.6586
Training Epoch: 0 [1440/50016]	Loss: 4.7425
Training Epoch: 0 [1472/50016]	Loss: 4.4129
Training Epoch: 0 [1504/50016]	Loss: 4.7171
Training Epoch: 0 [1536/50016]	Loss: 4.5899
Training Epoch: 0 [1568/50016]	Loss: 4.7696
Training Epoch: 0 [1600/50016]	Loss: 4.8029
Profile done with power limit 125W
epoch 1 train time consumed: 4.72s
Validation Epoch: 0, Average loss: 0.1685, Accuracy: 0.0260
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 125000, 'lr': 0.002, 'energy': 122.51216623274738, 'time': 3.3738826640001207, 'accuracy': 0.025958466453674122, 'total_cost': 15.462393169658931}
[run job] Launching job with BS 32: and LR: 0.002 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs32+lr0.00200+pl100', 'ZEUS_COST_THRESH': '28.832122813185396', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '0.002', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '0.002', '--power_limit', '100']
[run job] cost_ub=28.832122813185396
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs32+lr0.00200+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.002
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 5.0068
Training Epoch: 0 [96/50016]	Loss: 5.1681
Training Epoch: 0 [128/50016]	Loss: 5.4384
Training Epoch: 0 [160/50016]	Loss: 5.5147
Training Epoch: 0 [192/50016]	Loss: 5.2494
Training Epoch: 0 [224/50016]	Loss: 5.2806
Training Epoch: 0 [256/50016]	Loss: 5.1687
Training Epoch: 0 [288/50016]	Loss: 5.2093
Training Epoch: 0 [320/50016]	Loss: 5.2761
Training Epoch: 0 [352/50016]	Loss: 5.0397
Training Epoch: 0 [384/50016]	Loss: 5.5635
Training Epoch: 0 [416/50016]	Loss: 4.9783
Training Epoch: 0 [448/50016]	Loss: 4.9020
Training Epoch: 0 [480/50016]	Loss: 5.1617
Training Epoch: 0 [512/50016]	Loss: 4.9950
Training Epoch: 0 [544/50016]	Loss: 4.9224
Training Epoch: 0 [576/50016]	Loss: 4.6430
Training Epoch: 0 [608/50016]	Loss: 5.1793
Training Epoch: 0 [640/50016]	Loss: 5.5313
Training Epoch: 0 [672/50016]	Loss: 4.9206
Training Epoch: 0 [704/50016]	Loss: 4.6900
Training Epoch: 0 [736/50016]	Loss: 5.1071
Training Epoch: 0 [768/50016]	Loss: 5.3536
Training Epoch: 0 [800/50016]	Loss: 4.9630
Training Epoch: 0 [832/50016]	Loss: 5.0752
Training Epoch: 0 [864/50016]	Loss: 5.2931
Training Epoch: 0 [896/50016]	Loss: 4.7280
Training Epoch: 0 [928/50016]	Loss: 4.8758
Training Epoch: 0 [960/50016]	Loss: 4.7381
Training Epoch: 0 [992/50016]	Loss: 4.7982
Training Epoch: 0 [1024/50016]	Loss: 4.7307
Training Epoch: 0 [1056/50016]	Loss: 4.9008
Training Epoch: 0 [1088/50016]	Loss: 4.7582
Training Epoch: 0 [1120/50016]	Loss: 4.8425
Training Epoch: 0 [1152/50016]	Loss: 4.6515
Training Epoch: 0 [1184/50016]	Loss: 4.6588
Training Epoch: 0 [1216/50016]	Loss: 4.4531
Training Epoch: 0 [1248/50016]	Loss: 4.6071
Training Epoch: 0 [1280/50016]	Loss: 4.7165
Training Epoch: 0 [1312/50016]	Loss: 4.7157
Training Epoch: 0 [1344/50016]	Loss: 4.6041
Training Epoch: 0 [1376/50016]	Loss: 4.6137
Training Epoch: 0 [1408/50016]	Loss: 4.6450
Training Epoch: 0 [1440/50016]	Loss: 4.6426
Training Epoch: 0 [1472/50016]	Loss: 4.2958
Training Epoch: 0 [1504/50016]	Loss: 4.9285
Training Epoch: 0 [1536/50016]	Loss: 4.6188
Training Epoch: 0 [1568/50016]	Loss: 4.9276
Training Epoch: 0 [1600/50016]	Loss: 4.7556
Profile done with power limit 100W
epoch 1 train time consumed: 5.55s
Validation Epoch: 0, Average loss: 0.1756, Accuracy: 0.0255
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 100000, 'lr': 0.002, 'energy': 99.13777517854797, 'time': 4.089466127999913, 'accuracy': 0.02545926517571885, 'total_cost': 17.608026128603925}

[Power Profiler] with batch size 32 and learning rate 0.0024
[run job] Launching job with BS 32: and LR: 0.0024 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs32+lr0.00240+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '0.0024', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0024', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs32+lr0.00240+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.0024
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 5.1427
Training Epoch: 0 [96/50016]	Loss: 5.3798
Training Epoch: 0 [128/50016]	Loss: 5.6608
Training Epoch: 0 [160/50016]	Loss: 5.6614
Training Epoch: 0 [192/50016]	Loss: 5.3485
Training Epoch: 0 [224/50016]	Loss: 5.3971
Training Epoch: 0 [256/50016]	Loss: 5.2512
Training Epoch: 0 [288/50016]	Loss: 5.3839
Training Epoch: 0 [320/50016]	Loss: 5.3805
Training Epoch: 0 [352/50016]	Loss: 5.1619
Training Epoch: 0 [384/50016]	Loss: 5.5779
Training Epoch: 0 [416/50016]	Loss: 5.0307
Training Epoch: 0 [448/50016]	Loss: 4.9509
Training Epoch: 0 [480/50016]	Loss: 5.3431
Training Epoch: 0 [512/50016]	Loss: 4.9809
Training Epoch: 0 [544/50016]	Loss: 4.9969
Training Epoch: 0 [576/50016]	Loss: 4.5701
Training Epoch: 0 [608/50016]	Loss: 5.1638
Training Epoch: 0 [640/50016]	Loss: 5.6280
Training Epoch: 0 [672/50016]	Loss: 5.1528
Training Epoch: 0 [704/50016]	Loss: 4.6038
Training Epoch: 0 [736/50016]	Loss: 5.0616
Training Epoch: 0 [768/50016]	Loss: 5.2067
Training Epoch: 0 [800/50016]	Loss: 4.9911
Training Epoch: 0 [832/50016]	Loss: 4.7511
Training Epoch: 0 [864/50016]	Loss: 5.2448
Training Epoch: 0 [896/50016]	Loss: 4.7793
Training Epoch: 0 [928/50016]	Loss: 4.8663
Training Epoch: 0 [960/50016]	Loss: 4.7158
Training Epoch: 0 [992/50016]	Loss: 4.7031
Training Epoch: 0 [1024/50016]	Loss: 4.8090
Training Epoch: 0 [1056/50016]	Loss: 4.8358
Training Epoch: 0 [1088/50016]	Loss: 4.7130
Training Epoch: 0 [1120/50016]	Loss: 4.6611
Training Epoch: 0 [1152/50016]	Loss: 4.6641
Training Epoch: 0 [1184/50016]	Loss: 4.7214
Training Epoch: 0 [1216/50016]	Loss: 4.6161
Training Epoch: 0 [1248/50016]	Loss: 4.5039
Training Epoch: 0 [1280/50016]	Loss: 4.6588
Training Epoch: 0 [1312/50016]	Loss: 4.5655
Training Epoch: 0 [1344/50016]	Loss: 4.7692
Training Epoch: 0 [1376/50016]	Loss: 4.6063
Training Epoch: 0 [1408/50016]	Loss: 4.6356
Training Epoch: 0 [1440/50016]	Loss: 4.6403
Training Epoch: 0 [1472/50016]	Loss: 4.5852
Training Epoch: 0 [1504/50016]	Loss: 4.9070
Training Epoch: 0 [1536/50016]	Loss: 4.5709
Training Epoch: 0 [1568/50016]	Loss: 4.8178
Training Epoch: 0 [1600/50016]	Loss: 4.6757
Profile done with power limit 175W
epoch 1 train time consumed: 4.54s
Validation Epoch: 0, Average loss: 0.1547, Accuracy: 0.0205
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 175000, 'lr': 0.0024, 'energy': 130.42613822443087, 'time': 3.196039735999875, 'accuracy': 0.020467252396166133, 'total_cost': 19.071281008554468}
[run job] Launching job with BS 32: and LR: 0.0024 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs32+lr0.00240+pl150', 'ZEUS_COST_THRESH': '38.142562017108936', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '0.0024', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0024', '--power_limit', '150']
[run job] cost_ub=38.142562017108936
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs32+lr0.00240+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.0024
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 5.1427
Training Epoch: 0 [96/50016]	Loss: 5.3798
Training Epoch: 0 [128/50016]	Loss: 5.6587
Training Epoch: 0 [160/50016]	Loss: 5.6880
Training Epoch: 0 [192/50016]	Loss: 5.3733
Training Epoch: 0 [224/50016]	Loss: 5.3702
Training Epoch: 0 [256/50016]	Loss: 5.3016
Training Epoch: 0 [288/50016]	Loss: 5.4035
Training Epoch: 0 [320/50016]	Loss: 5.3682
Training Epoch: 0 [352/50016]	Loss: 5.1605
Training Epoch: 0 [384/50016]	Loss: 5.6026
Training Epoch: 0 [416/50016]	Loss: 5.0300
Training Epoch: 0 [448/50016]	Loss: 4.9308
Training Epoch: 0 [480/50016]	Loss: 5.2869
Training Epoch: 0 [512/50016]	Loss: 5.0258
Training Epoch: 0 [544/50016]	Loss: 4.9953
Training Epoch: 0 [576/50016]	Loss: 4.6112
Training Epoch: 0 [608/50016]	Loss: 5.2682
Training Epoch: 0 [640/50016]	Loss: 5.6328
Training Epoch: 0 [672/50016]	Loss: 4.9772
Training Epoch: 0 [704/50016]	Loss: 4.7007
Training Epoch: 0 [736/50016]	Loss: 5.1184
Training Epoch: 0 [768/50016]	Loss: 5.1266
Training Epoch: 0 [800/50016]	Loss: 4.7847
Training Epoch: 0 [832/50016]	Loss: 4.9419
Training Epoch: 0 [864/50016]	Loss: 5.2187
Training Epoch: 0 [896/50016]	Loss: 4.7531
Training Epoch: 0 [928/50016]	Loss: 4.8767
Training Epoch: 0 [960/50016]	Loss: 4.6835
Training Epoch: 0 [992/50016]	Loss: 4.6999
Training Epoch: 0 [1024/50016]	Loss: 4.7437
Training Epoch: 0 [1056/50016]	Loss: 4.7784
Training Epoch: 0 [1088/50016]	Loss: 4.6907
Training Epoch: 0 [1120/50016]	Loss: 4.7093
Training Epoch: 0 [1152/50016]	Loss: 4.6648
Training Epoch: 0 [1184/50016]	Loss: 4.6712
Training Epoch: 0 [1216/50016]	Loss: 4.6230
Training Epoch: 0 [1248/50016]	Loss: 4.4466
Training Epoch: 0 [1280/50016]	Loss: 4.6255
Training Epoch: 0 [1312/50016]	Loss: 4.5237
Training Epoch: 0 [1344/50016]	Loss: 4.7211
Training Epoch: 0 [1376/50016]	Loss: 4.6316
Training Epoch: 0 [1408/50016]	Loss: 4.5908
Training Epoch: 0 [1440/50016]	Loss: 4.6656
Training Epoch: 0 [1472/50016]	Loss: 4.5145
Training Epoch: 0 [1504/50016]	Loss: 4.9505
Training Epoch: 0 [1536/50016]	Loss: 4.5158
Training Epoch: 0 [1568/50016]	Loss: 4.8313
Training Epoch: 0 [1600/50016]	Loss: 4.6631
Profile done with power limit 150W
epoch 1 train time consumed: 4.61s
Validation Epoch: 0, Average loss: 0.1582, Accuracy: 0.0244
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 150000, 'lr': 0.0024, 'energy': 130.290134568869, 'time': 3.2563502370003334, 'accuracy': 0.024361022364217253, 'total_cost': 16.318093171176894}
[run job] Launching job with BS 32: and LR: 0.0024 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs32+lr0.00240+pl125', 'ZEUS_COST_THRESH': '32.63618634235379', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '0.0024', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0024', '--power_limit', '125']
[run job] cost_ub=32.63618634235379
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs32+lr0.00240+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.0024
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 5.1427
Training Epoch: 0 [96/50016]	Loss: 5.3809
Training Epoch: 0 [128/50016]	Loss: 5.6584
Training Epoch: 0 [160/50016]	Loss: 5.6680
Training Epoch: 0 [192/50016]	Loss: 5.3268
Training Epoch: 0 [224/50016]	Loss: 5.3899
Training Epoch: 0 [256/50016]	Loss: 5.2508
Training Epoch: 0 [288/50016]	Loss: 5.4364
Training Epoch: 0 [320/50016]	Loss: 5.3771
Training Epoch: 0 [352/50016]	Loss: 5.1167
Training Epoch: 0 [384/50016]	Loss: 5.6146
Training Epoch: 0 [416/50016]	Loss: 4.9959
Training Epoch: 0 [448/50016]	Loss: 4.9806
Training Epoch: 0 [480/50016]	Loss: 5.3538
Training Epoch: 0 [512/50016]	Loss: 5.0638
Training Epoch: 0 [544/50016]	Loss: 5.0290
Training Epoch: 0 [576/50016]	Loss: 4.6990
Training Epoch: 0 [608/50016]	Loss: 5.2134
Training Epoch: 0 [640/50016]	Loss: 5.6497
Training Epoch: 0 [672/50016]	Loss: 5.0252
Training Epoch: 0 [704/50016]	Loss: 4.6866
Training Epoch: 0 [736/50016]	Loss: 5.1573
Training Epoch: 0 [768/50016]	Loss: 5.3289
Training Epoch: 0 [800/50016]	Loss: 4.8667
Training Epoch: 0 [832/50016]	Loss: 5.0171
Training Epoch: 0 [864/50016]	Loss: 5.2097
Training Epoch: 0 [896/50016]	Loss: 4.7235
Training Epoch: 0 [928/50016]	Loss: 4.7152
Training Epoch: 0 [960/50016]	Loss: 4.6828
Training Epoch: 0 [992/50016]	Loss: 4.6187
Training Epoch: 0 [1024/50016]	Loss: 4.7075
Training Epoch: 0 [1056/50016]	Loss: 4.8713
Training Epoch: 0 [1088/50016]	Loss: 4.6629
Training Epoch: 0 [1120/50016]	Loss: 4.6927
Training Epoch: 0 [1152/50016]	Loss: 4.6402
Training Epoch: 0 [1184/50016]	Loss: 4.7147
Training Epoch: 0 [1216/50016]	Loss: 4.4866
Training Epoch: 0 [1248/50016]	Loss: 4.5071
Training Epoch: 0 [1280/50016]	Loss: 4.6384
Training Epoch: 0 [1312/50016]	Loss: 4.5429
Training Epoch: 0 [1344/50016]	Loss: 4.7333
Training Epoch: 0 [1376/50016]	Loss: 4.5388
Training Epoch: 0 [1408/50016]	Loss: 4.5902
Training Epoch: 0 [1440/50016]	Loss: 4.6571
Training Epoch: 0 [1472/50016]	Loss: 4.5004
Training Epoch: 0 [1504/50016]	Loss: 4.9003
Training Epoch: 0 [1536/50016]	Loss: 4.5190
Training Epoch: 0 [1568/50016]	Loss: 4.8509
Training Epoch: 0 [1600/50016]	Loss: 4.7650
Profile done with power limit 125W
epoch 1 train time consumed: 4.70s
Validation Epoch: 0, Average loss: 0.1672, Accuracy: 0.0243
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 125000, 'lr': 0.0024, 'energy': 123.30341314673954, 'time': 3.3397537470000316, 'accuracy': 0.024261182108626198, 'total_cost': 16.420325046711}
[run job] Launching job with BS 32: and LR: 0.0024 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs32+lr0.00240+pl100', 'ZEUS_COST_THRESH': '32.63618634235379', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '0.0024', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0024', '--power_limit', '100']
[run job] cost_ub=32.63618634235379
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs32+lr0.00240+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.0024
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 5.1427
Training Epoch: 0 [96/50016]	Loss: 5.3800
Training Epoch: 0 [128/50016]	Loss: 5.6518
Training Epoch: 0 [160/50016]	Loss: 5.6481
Training Epoch: 0 [192/50016]	Loss: 5.3424
Training Epoch: 0 [224/50016]	Loss: 5.3664
Training Epoch: 0 [256/50016]	Loss: 5.2987
Training Epoch: 0 [288/50016]	Loss: 5.4139
Training Epoch: 0 [320/50016]	Loss: 5.3928
Training Epoch: 0 [352/50016]	Loss: 5.0567
Training Epoch: 0 [384/50016]	Loss: 5.5346
Training Epoch: 0 [416/50016]	Loss: 4.9861
Training Epoch: 0 [448/50016]	Loss: 4.9562
Training Epoch: 0 [480/50016]	Loss: 5.1431
Training Epoch: 0 [512/50016]	Loss: 4.9229
Training Epoch: 0 [544/50016]	Loss: 4.9118
Training Epoch: 0 [576/50016]	Loss: 4.4986
Training Epoch: 0 [608/50016]	Loss: 5.2312
Training Epoch: 0 [640/50016]	Loss: 5.6200
Training Epoch: 0 [672/50016]	Loss: 5.2600
Training Epoch: 0 [704/50016]	Loss: 4.7274
Training Epoch: 0 [736/50016]	Loss: 4.9825
Training Epoch: 0 [768/50016]	Loss: 4.9340
Training Epoch: 0 [800/50016]	Loss: 4.7612
Training Epoch: 0 [832/50016]	Loss: 4.9622
Training Epoch: 0 [864/50016]	Loss: 5.2450
Training Epoch: 0 [896/50016]	Loss: 4.8321
Training Epoch: 0 [928/50016]	Loss: 4.8669
Training Epoch: 0 [960/50016]	Loss: 4.6796
Training Epoch: 0 [992/50016]	Loss: 4.7378
Training Epoch: 0 [1024/50016]	Loss: 4.8450
Training Epoch: 0 [1056/50016]	Loss: 4.7845
Training Epoch: 0 [1088/50016]	Loss: 4.6579
Training Epoch: 0 [1120/50016]	Loss: 4.6820
Training Epoch: 0 [1152/50016]	Loss: 4.6717
Training Epoch: 0 [1184/50016]	Loss: 4.7597
Training Epoch: 0 [1216/50016]	Loss: 4.5697
Training Epoch: 0 [1248/50016]	Loss: 4.4856
Training Epoch: 0 [1280/50016]	Loss: 4.7244
Training Epoch: 0 [1312/50016]	Loss: 4.5426
Training Epoch: 0 [1344/50016]	Loss: 4.7153
Training Epoch: 0 [1376/50016]	Loss: 4.5674
Training Epoch: 0 [1408/50016]	Loss: 4.6463
Training Epoch: 0 [1440/50016]	Loss: 4.6569
Training Epoch: 0 [1472/50016]	Loss: 4.5464
Training Epoch: 0 [1504/50016]	Loss: 4.9593
Training Epoch: 0 [1536/50016]	Loss: 4.5149
Training Epoch: 0 [1568/50016]	Loss: 4.7577
Training Epoch: 0 [1600/50016]	Loss: 4.6671
Profile done with power limit 100W
epoch 1 train time consumed: 5.50s
Validation Epoch: 0, Average loss: 0.1568, Accuracy: 0.0199
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 100000, 'lr': 0.0024, 'energy': 99.34368358435245, 'time': 4.001007664000099, 'accuracy': 0.01986821086261981, 'total_cost': 22.09157235473508}

[Power Profiler] with batch size 64 and learning rate 0.0022627416997969526
[run job] Launching job with BS 64: and LR: 0.0022627416997969526 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs64+lr0.00226+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '0.0022627416997969526', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0022627416997969526', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs64+lr0.00226+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.0022627416997969526
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 4.8843
Training Epoch: 0 [192/50048]	Loss: 5.0517
Training Epoch: 0 [256/50048]	Loss: 5.1981
Training Epoch: 0 [320/50048]	Loss: 5.1215
Training Epoch: 0 [384/50048]	Loss: 5.1551
Training Epoch: 0 [448/50048]	Loss: 4.9210
Training Epoch: 0 [512/50048]	Loss: 5.1429
Training Epoch: 0 [576/50048]	Loss: 4.6965
Training Epoch: 0 [640/50048]	Loss: 5.3001
Training Epoch: 0 [704/50048]	Loss: 4.8394
Training Epoch: 0 [768/50048]	Loss: 5.2487
Training Epoch: 0 [832/50048]	Loss: 5.1348
Training Epoch: 0 [896/50048]	Loss: 4.9567
Training Epoch: 0 [960/50048]	Loss: 4.8902
Training Epoch: 0 [1024/50048]	Loss: 5.0399
Training Epoch: 0 [1088/50048]	Loss: 5.0564
Training Epoch: 0 [1152/50048]	Loss: 4.8244
Training Epoch: 0 [1216/50048]	Loss: 4.8546
Training Epoch: 0 [1280/50048]	Loss: 4.9177
Training Epoch: 0 [1344/50048]	Loss: 4.7863
Training Epoch: 0 [1408/50048]	Loss: 4.7357
Training Epoch: 0 [1472/50048]	Loss: 4.7154
Training Epoch: 0 [1536/50048]	Loss: 4.8202
Training Epoch: 0 [1600/50048]	Loss: 4.9481
Training Epoch: 0 [1664/50048]	Loss: 4.7797
Training Epoch: 0 [1728/50048]	Loss: 4.7288
Training Epoch: 0 [1792/50048]	Loss: 4.7198
Training Epoch: 0 [1856/50048]	Loss: 4.7752
Training Epoch: 0 [1920/50048]	Loss: 4.7509
Training Epoch: 0 [1984/50048]	Loss: 4.6328
Training Epoch: 0 [2048/50048]	Loss: 4.5360
Training Epoch: 0 [2112/50048]	Loss: 4.7902
Training Epoch: 0 [2176/50048]	Loss: 4.6417
Training Epoch: 0 [2240/50048]	Loss: 4.6981
Training Epoch: 0 [2304/50048]	Loss: 4.7207
Training Epoch: 0 [2368/50048]	Loss: 4.6116
Training Epoch: 0 [2432/50048]	Loss: 4.6524
Training Epoch: 0 [2496/50048]	Loss: 4.4736
Training Epoch: 0 [2560/50048]	Loss: 4.6703
Training Epoch: 0 [2624/50048]	Loss: 4.4353
Training Epoch: 0 [2688/50048]	Loss: 4.5280
Training Epoch: 0 [2752/50048]	Loss: 4.3951
Training Epoch: 0 [2816/50048]	Loss: 4.3184
Training Epoch: 0 [2880/50048]	Loss: 4.3893
Training Epoch: 0 [2944/50048]	Loss: 4.4956
Training Epoch: 0 [3008/50048]	Loss: 4.3736
Training Epoch: 0 [3072/50048]	Loss: 4.6431
Training Epoch: 0 [3136/50048]	Loss: 4.5360
Training Epoch: 0 [3200/50048]	Loss: 4.3173
Profile done with power limit 175W
epoch 1 train time consumed: 7.22s
Validation Epoch: 0, Average loss: 0.0729, Accuracy: 0.0252
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 175000, 'lr': 0.0022627416997969526, 'energy': 140.97252710311687, 'time': 5.275773026000024, 'accuracy': 0.025179140127388536, 'total_cost': 52.913659243105585}
[run job] Launching job with BS 64: and LR: 0.0022627416997969526 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs64+lr0.00226+pl150', 'ZEUS_COST_THRESH': '105.82731848621117', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '0.0022627416997969526', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0022627416997969526', '--power_limit', '150']
[run job] cost_ub=105.82731848621117
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs64+lr0.00226+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.0022627416997969526
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 4.8843
Training Epoch: 0 [192/50048]	Loss: 5.0515
Training Epoch: 0 [256/50048]	Loss: 5.2072
Training Epoch: 0 [320/50048]	Loss: 5.1331
Training Epoch: 0 [384/50048]	Loss: 5.1386
Training Epoch: 0 [448/50048]	Loss: 4.9046
Training Epoch: 0 [512/50048]	Loss: 5.0923
Training Epoch: 0 [576/50048]	Loss: 4.6994
Training Epoch: 0 [640/50048]	Loss: 5.3028
Training Epoch: 0 [704/50048]	Loss: 4.8417
Training Epoch: 0 [768/50048]	Loss: 5.1770
Training Epoch: 0 [832/50048]	Loss: 5.0532
Training Epoch: 0 [896/50048]	Loss: 5.0014
Training Epoch: 0 [960/50048]	Loss: 4.8674
Training Epoch: 0 [1024/50048]	Loss: 4.9638
Training Epoch: 0 [1088/50048]	Loss: 4.9865
Training Epoch: 0 [1152/50048]	Loss: 4.7818
Training Epoch: 0 [1216/50048]	Loss: 4.8472
Training Epoch: 0 [1280/50048]	Loss: 4.8002
Training Epoch: 0 [1344/50048]	Loss: 4.7496
Training Epoch: 0 [1408/50048]	Loss: 4.5980
Training Epoch: 0 [1472/50048]	Loss: 4.6211
Training Epoch: 0 [1536/50048]	Loss: 4.7651
Training Epoch: 0 [1600/50048]	Loss: 4.8984
Training Epoch: 0 [1664/50048]	Loss: 4.8235
Training Epoch: 0 [1728/50048]	Loss: 4.7379
Training Epoch: 0 [1792/50048]	Loss: 4.7316
Training Epoch: 0 [1856/50048]	Loss: 4.9343
Training Epoch: 0 [1920/50048]	Loss: 4.6255
Training Epoch: 0 [1984/50048]	Loss: 4.6675
Training Epoch: 0 [2048/50048]	Loss: 4.5256
Training Epoch: 0 [2112/50048]	Loss: 4.7918
Training Epoch: 0 [2176/50048]	Loss: 4.6561
Training Epoch: 0 [2240/50048]	Loss: 4.6067
Training Epoch: 0 [2304/50048]	Loss: 4.6955
Training Epoch: 0 [2368/50048]	Loss: 4.6108
Training Epoch: 0 [2432/50048]	Loss: 4.6067
Training Epoch: 0 [2496/50048]	Loss: 4.6729
Training Epoch: 0 [2560/50048]	Loss: 4.5891
Training Epoch: 0 [2624/50048]	Loss: 4.5727
Training Epoch: 0 [2688/50048]	Loss: 4.5369
Training Epoch: 0 [2752/50048]	Loss: 4.4612
Training Epoch: 0 [2816/50048]	Loss: 4.4383
Training Epoch: 0 [2880/50048]	Loss: 4.3527
Training Epoch: 0 [2944/50048]	Loss: 4.5325
Training Epoch: 0 [3008/50048]	Loss: 4.4271
Training Epoch: 0 [3072/50048]	Loss: 4.5935
Training Epoch: 0 [3136/50048]	Loss: 4.5360
Training Epoch: 0 [3200/50048]	Loss: 4.4533
Profile done with power limit 150W
epoch 1 train time consumed: 7.20s
Validation Epoch: 0, Average loss: 0.0733, Accuracy: 0.0233
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 150000, 'lr': 0.0022627416997969526, 'energy': 140.92292866023027, 'time': 5.303515381999205, 'accuracy': 0.023288216560509552, 'total_cost': 57.50187586794667}
[run job] Launching job with BS 64: and LR: 0.0022627416997969526 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs64+lr0.00226+pl125', 'ZEUS_COST_THRESH': '105.82731848621117', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '0.0022627416997969526', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0022627416997969526', '--power_limit', '125']
[run job] cost_ub=105.82731848621117
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs64+lr0.00226+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.0022627416997969526
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 4.8843
Training Epoch: 0 [192/50048]	Loss: 5.0520
Training Epoch: 0 [256/50048]	Loss: 5.2054
Training Epoch: 0 [320/50048]	Loss: 5.1193
Training Epoch: 0 [384/50048]	Loss: 5.1713
Training Epoch: 0 [448/50048]	Loss: 4.8958
Training Epoch: 0 [512/50048]	Loss: 5.1080
Training Epoch: 0 [576/50048]	Loss: 4.6844
Training Epoch: 0 [640/50048]	Loss: 5.3262
Training Epoch: 0 [704/50048]	Loss: 4.8527
Training Epoch: 0 [768/50048]	Loss: 5.1890
Training Epoch: 0 [832/50048]	Loss: 5.0344
Training Epoch: 0 [896/50048]	Loss: 5.0251
Training Epoch: 0 [960/50048]	Loss: 4.8778
Training Epoch: 0 [1024/50048]	Loss: 4.9638
Training Epoch: 0 [1088/50048]	Loss: 4.9616
Training Epoch: 0 [1152/50048]	Loss: 4.9136
Training Epoch: 0 [1216/50048]	Loss: 4.8331
Training Epoch: 0 [1280/50048]	Loss: 4.7518
Training Epoch: 0 [1344/50048]	Loss: 4.8060
Training Epoch: 0 [1408/50048]	Loss: 4.6213
Training Epoch: 0 [1472/50048]	Loss: 4.6458
Training Epoch: 0 [1536/50048]	Loss: 4.7535
Training Epoch: 0 [1600/50048]	Loss: 4.9109
Training Epoch: 0 [1664/50048]	Loss: 4.8596
Training Epoch: 0 [1728/50048]	Loss: 4.6253
Training Epoch: 0 [1792/50048]	Loss: 4.6186
Training Epoch: 0 [1856/50048]	Loss: 4.8674
Training Epoch: 0 [1920/50048]	Loss: 4.6111
Training Epoch: 0 [1984/50048]	Loss: 4.6159
Training Epoch: 0 [2048/50048]	Loss: 4.5331
Training Epoch: 0 [2112/50048]	Loss: 4.7540
Training Epoch: 0 [2176/50048]	Loss: 4.7110
Training Epoch: 0 [2240/50048]	Loss: 4.6207
Training Epoch: 0 [2304/50048]	Loss: 4.6279
Training Epoch: 0 [2368/50048]	Loss: 4.6576
Training Epoch: 0 [2432/50048]	Loss: 4.5652
Training Epoch: 0 [2496/50048]	Loss: 4.5367
Training Epoch: 0 [2560/50048]	Loss: 4.5593
Training Epoch: 0 [2624/50048]	Loss: 4.4334
Training Epoch: 0 [2688/50048]	Loss: 4.4999
Training Epoch: 0 [2752/50048]	Loss: 4.4341
Training Epoch: 0 [2816/50048]	Loss: 4.4663
Training Epoch: 0 [2880/50048]	Loss: 4.3290
Training Epoch: 0 [2944/50048]	Loss: 4.4161
Training Epoch: 0 [3008/50048]	Loss: 4.3011
Training Epoch: 0 [3072/50048]	Loss: 4.4493
Training Epoch: 0 [3136/50048]	Loss: 4.6516
Training Epoch: 0 [3200/50048]	Loss: 4.4969
Profile done with power limit 125W
epoch 1 train time consumed: 7.77s
Validation Epoch: 0, Average loss: 0.0746, Accuracy: 0.0186
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 125000, 'lr': 0.0022627416997969526, 'energy': 123.50823660687274, 'time': 5.776365122000243, 'accuracy': 0.018610668789808917, 'total_cost': 74.04951870312529}
[run job] Launching job with BS 64: and LR: 0.0022627416997969526 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs64+lr0.00226+pl100', 'ZEUS_COST_THRESH': '105.82731848621117', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '0.0022627416997969526', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0022627416997969526', '--power_limit', '100']
[run job] cost_ub=105.82731848621117
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs64+lr0.00226+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.0022627416997969526
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 4.8843
Training Epoch: 0 [192/50048]	Loss: 5.0519
Training Epoch: 0 [256/50048]	Loss: 5.1981
Training Epoch: 0 [320/50048]	Loss: 5.1178
Training Epoch: 0 [384/50048]	Loss: 5.1595
Training Epoch: 0 [448/50048]	Loss: 4.8978
Training Epoch: 0 [512/50048]	Loss: 5.1309
Training Epoch: 0 [576/50048]	Loss: 4.6806
Training Epoch: 0 [640/50048]	Loss: 5.2857
Training Epoch: 0 [704/50048]	Loss: 4.8700
Training Epoch: 0 [768/50048]	Loss: 5.1462
Training Epoch: 0 [832/50048]	Loss: 5.0377
Training Epoch: 0 [896/50048]	Loss: 5.0147
Training Epoch: 0 [960/50048]	Loss: 4.8354
Training Epoch: 0 [1024/50048]	Loss: 4.9683
Training Epoch: 0 [1088/50048]	Loss: 5.0338
Training Epoch: 0 [1152/50048]	Loss: 4.8485
Training Epoch: 0 [1216/50048]	Loss: 4.8527
Training Epoch: 0 [1280/50048]	Loss: 4.8433
Training Epoch: 0 [1344/50048]	Loss: 4.8048
Training Epoch: 0 [1408/50048]	Loss: 4.6792
Training Epoch: 0 [1472/50048]	Loss: 4.6705
Training Epoch: 0 [1536/50048]	Loss: 4.8052
Training Epoch: 0 [1600/50048]	Loss: 4.8469
Training Epoch: 0 [1664/50048]	Loss: 4.7700
Training Epoch: 0 [1728/50048]	Loss: 4.6988
Training Epoch: 0 [1792/50048]	Loss: 4.6210
Training Epoch: 0 [1856/50048]	Loss: 4.8054
Training Epoch: 0 [1920/50048]	Loss: 4.6947
Training Epoch: 0 [1984/50048]	Loss: 4.6369
Training Epoch: 0 [2048/50048]	Loss: 4.5425
Training Epoch: 0 [2112/50048]	Loss: 4.8160
Training Epoch: 0 [2176/50048]	Loss: 4.7175
Training Epoch: 0 [2240/50048]	Loss: 4.6645
Training Epoch: 0 [2304/50048]	Loss: 4.6906
Training Epoch: 0 [2368/50048]	Loss: 4.6577
Training Epoch: 0 [2432/50048]	Loss: 4.5904
Training Epoch: 0 [2496/50048]	Loss: 4.5557
Training Epoch: 0 [2560/50048]	Loss: 4.6426
Training Epoch: 0 [2624/50048]	Loss: 4.4282
Training Epoch: 0 [2688/50048]	Loss: 4.5560
Training Epoch: 0 [2752/50048]	Loss: 4.4222
Training Epoch: 0 [2816/50048]	Loss: 4.3080
Training Epoch: 0 [2880/50048]	Loss: 4.2982
Training Epoch: 0 [2944/50048]	Loss: 4.4484
Training Epoch: 0 [3008/50048]	Loss: 4.3885
Training Epoch: 0 [3072/50048]	Loss: 4.5901
Training Epoch: 0 [3136/50048]	Loss: 4.5344
Training Epoch: 0 [3200/50048]	Loss: 4.4567
Profile done with power limit 100W
epoch 1 train time consumed: 15.46s
Validation Epoch: 0, Average loss: 0.0738, Accuracy: 0.0220
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 100000, 'lr': 0.0022627416997969526, 'energy': 97.41921568021128, 'time': 11.99165712499962, 'accuracy': 0.021994426751592355, 'total_cost': 118.70733566459153}

[Power Profiler] with batch size 64 and learning rate 0.0028284271247461905
[run job] Launching job with BS 64: and LR: 0.0028284271247461905 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs64+lr0.00283+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '0.0028284271247461905', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0028284271247461905', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs64+lr0.00283+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.0028284271247461905
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 5.0397
Training Epoch: 0 [192/50048]	Loss: 5.2123
Training Epoch: 0 [256/50048]	Loss: 5.3961
Training Epoch: 0 [320/50048]	Loss: 5.2590
Training Epoch: 0 [384/50048]	Loss: 5.2519
Training Epoch: 0 [448/50048]	Loss: 4.9705
Training Epoch: 0 [512/50048]	Loss: 5.1363
Training Epoch: 0 [576/50048]	Loss: 4.6707
Training Epoch: 0 [640/50048]	Loss: 5.5235
Training Epoch: 0 [704/50048]	Loss: 4.7851
Training Epoch: 0 [768/50048]	Loss: 5.0825
Training Epoch: 0 [832/50048]	Loss: 4.8427
Training Epoch: 0 [896/50048]	Loss: 4.9713
Training Epoch: 0 [960/50048]	Loss: 4.9046
Training Epoch: 0 [1024/50048]	Loss: 4.8382
Training Epoch: 0 [1088/50048]	Loss: 4.8529
Training Epoch: 0 [1152/50048]	Loss: 4.8185
Training Epoch: 0 [1216/50048]	Loss: 4.7392
Training Epoch: 0 [1280/50048]	Loss: 4.6437
Training Epoch: 0 [1344/50048]	Loss: 4.8446
Training Epoch: 0 [1408/50048]	Loss: 4.5608
Training Epoch: 0 [1472/50048]	Loss: 4.5819
Training Epoch: 0 [1536/50048]	Loss: 4.6690
Training Epoch: 0 [1600/50048]	Loss: 4.8892
Training Epoch: 0 [1664/50048]	Loss: 4.9288
Training Epoch: 0 [1728/50048]	Loss: 4.7947
Training Epoch: 0 [1792/50048]	Loss: 4.7309
Training Epoch: 0 [1856/50048]	Loss: 4.7946
Training Epoch: 0 [1920/50048]	Loss: 4.5712
Training Epoch: 0 [1984/50048]	Loss: 4.5306
Training Epoch: 0 [2048/50048]	Loss: 4.5634
Training Epoch: 0 [2112/50048]	Loss: 4.7745
Training Epoch: 0 [2176/50048]	Loss: 4.6449
Training Epoch: 0 [2240/50048]	Loss: 4.5978
Training Epoch: 0 [2304/50048]	Loss: 4.6292
Training Epoch: 0 [2368/50048]	Loss: 4.5836
Training Epoch: 0 [2432/50048]	Loss: 4.5799
Training Epoch: 0 [2496/50048]	Loss: 4.7127
Training Epoch: 0 [2560/50048]	Loss: 4.5033
Training Epoch: 0 [2624/50048]	Loss: 4.5375
Training Epoch: 0 [2688/50048]	Loss: 4.5319
Training Epoch: 0 [2752/50048]	Loss: 4.5172
Training Epoch: 0 [2816/50048]	Loss: 4.5104
Training Epoch: 0 [2880/50048]	Loss: 4.4272
Training Epoch: 0 [2944/50048]	Loss: 4.5973
Training Epoch: 0 [3008/50048]	Loss: 4.5465
Training Epoch: 0 [3072/50048]	Loss: 4.6170
Training Epoch: 0 [3136/50048]	Loss: 4.6160
Training Epoch: 0 [3200/50048]	Loss: 4.5237
Profile done with power limit 175W
epoch 1 train time consumed: 7.18s
Validation Epoch: 0, Average loss: 0.0729, Accuracy: 0.0198
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 175000, 'lr': 0.0028284271247461905, 'energy': 140.98157212485637, 'time': 5.263480868999977, 'accuracy': 0.019804936305732483, 'total_cost': 67.11732182503509}
[run job] Launching job with BS 64: and LR: 0.0028284271247461905 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs64+lr0.00283+pl150', 'ZEUS_COST_THRESH': '134.23464365007018', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '0.0028284271247461905', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0028284271247461905', '--power_limit', '150']
[run job] cost_ub=134.23464365007018
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs64+lr0.00283+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.0028284271247461905
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 5.0397
Training Epoch: 0 [192/50048]	Loss: 5.2115
Training Epoch: 0 [256/50048]	Loss: 5.3992
Training Epoch: 0 [320/50048]	Loss: 5.2456
Training Epoch: 0 [384/50048]	Loss: 5.2645
Training Epoch: 0 [448/50048]	Loss: 4.9837
Training Epoch: 0 [512/50048]	Loss: 5.1717
Training Epoch: 0 [576/50048]	Loss: 4.6929
Training Epoch: 0 [640/50048]	Loss: 5.4899
Training Epoch: 0 [704/50048]	Loss: 4.8340
Training Epoch: 0 [768/50048]	Loss: 5.1245
Training Epoch: 0 [832/50048]	Loss: 4.8847
Training Epoch: 0 [896/50048]	Loss: 4.9550
Training Epoch: 0 [960/50048]	Loss: 4.9678
Training Epoch: 0 [1024/50048]	Loss: 4.8075
Training Epoch: 0 [1088/50048]	Loss: 4.8221
Training Epoch: 0 [1152/50048]	Loss: 4.8105
Training Epoch: 0 [1216/50048]	Loss: 4.6743
Training Epoch: 0 [1280/50048]	Loss: 4.6334
Training Epoch: 0 [1344/50048]	Loss: 4.7970
Training Epoch: 0 [1408/50048]	Loss: 4.5584
Training Epoch: 0 [1472/50048]	Loss: 4.6269
Training Epoch: 0 [1536/50048]	Loss: 4.6857
Training Epoch: 0 [1600/50048]	Loss: 4.9506
Training Epoch: 0 [1664/50048]	Loss: 4.9645
Training Epoch: 0 [1728/50048]	Loss: 4.7246
Training Epoch: 0 [1792/50048]	Loss: 4.6673
Training Epoch: 0 [1856/50048]	Loss: 4.7355
Training Epoch: 0 [1920/50048]	Loss: 4.5935
Training Epoch: 0 [1984/50048]	Loss: 4.5565
Training Epoch: 0 [2048/50048]	Loss: 4.5523
Training Epoch: 0 [2112/50048]	Loss: 4.6554
Training Epoch: 0 [2176/50048]	Loss: 4.6446
Training Epoch: 0 [2240/50048]	Loss: 4.5721
Training Epoch: 0 [2304/50048]	Loss: 4.6298
Training Epoch: 0 [2368/50048]	Loss: 4.6062
Training Epoch: 0 [2432/50048]	Loss: 4.5392
Training Epoch: 0 [2496/50048]	Loss: 4.5819
Training Epoch: 0 [2560/50048]	Loss: 4.5630
Training Epoch: 0 [2624/50048]	Loss: 4.4412
Training Epoch: 0 [2688/50048]	Loss: 4.4748
Training Epoch: 0 [2752/50048]	Loss: 4.5280
Training Epoch: 0 [2816/50048]	Loss: 4.4620
Training Epoch: 0 [2880/50048]	Loss: 4.4488
Training Epoch: 0 [2944/50048]	Loss: 4.5016
Training Epoch: 0 [3008/50048]	Loss: 4.3511
Training Epoch: 0 [3072/50048]	Loss: 4.5318
Training Epoch: 0 [3136/50048]	Loss: 4.8109
Training Epoch: 0 [3200/50048]	Loss: 4.5952
Profile done with power limit 150W
epoch 1 train time consumed: 7.15s
Validation Epoch: 0, Average loss: 0.0744, Accuracy: 0.0181
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 150000, 'lr': 0.0028284271247461905, 'energy': 141.45725251390022, 'time': 5.262336994999714, 'accuracy': 0.018113057324840764, 'total_cost': 73.48102611974329}
[run job] Launching job with BS 64: and LR: 0.0028284271247461905 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs64+lr0.00283+pl125', 'ZEUS_COST_THRESH': '134.23464365007018', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '0.0028284271247461905', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0028284271247461905', '--power_limit', '125']
[run job] cost_ub=134.23464365007018
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs64+lr0.00283+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.0028284271247461905
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 5.0397
Training Epoch: 0 [192/50048]	Loss: 5.2115
Training Epoch: 0 [256/50048]	Loss: 5.3894
Training Epoch: 0 [320/50048]	Loss: 5.2660
Training Epoch: 0 [384/50048]	Loss: 5.2472
Training Epoch: 0 [448/50048]	Loss: 4.9984
Training Epoch: 0 [512/50048]	Loss: 5.1995
Training Epoch: 0 [576/50048]	Loss: 4.6958
Training Epoch: 0 [640/50048]	Loss: 5.4784
Training Epoch: 0 [704/50048]	Loss: 4.7735
Training Epoch: 0 [768/50048]	Loss: 5.1758
Training Epoch: 0 [832/50048]	Loss: 4.9016
Training Epoch: 0 [896/50048]	Loss: 4.9306
Training Epoch: 0 [960/50048]	Loss: 4.8751
Training Epoch: 0 [1024/50048]	Loss: 4.7881
Training Epoch: 0 [1088/50048]	Loss: 4.7845
Training Epoch: 0 [1152/50048]	Loss: 4.7964
Training Epoch: 0 [1216/50048]	Loss: 4.7308
Training Epoch: 0 [1280/50048]	Loss: 4.6426
Training Epoch: 0 [1344/50048]	Loss: 4.8136
Training Epoch: 0 [1408/50048]	Loss: 4.5606
Training Epoch: 0 [1472/50048]	Loss: 4.6204
Training Epoch: 0 [1536/50048]	Loss: 4.6991
Training Epoch: 0 [1600/50048]	Loss: 4.8813
Training Epoch: 0 [1664/50048]	Loss: 4.9528
Training Epoch: 0 [1728/50048]	Loss: 4.8338
Training Epoch: 0 [1792/50048]	Loss: 4.7619
Training Epoch: 0 [1856/50048]	Loss: 4.8373
Training Epoch: 0 [1920/50048]	Loss: 4.7346
Training Epoch: 0 [1984/50048]	Loss: 4.5474
Training Epoch: 0 [2048/50048]	Loss: 4.6006
Training Epoch: 0 [2112/50048]	Loss: 4.7698
Training Epoch: 0 [2176/50048]	Loss: 4.6042
Training Epoch: 0 [2240/50048]	Loss: 4.5796
Training Epoch: 0 [2304/50048]	Loss: 4.6042
Training Epoch: 0 [2368/50048]	Loss: 4.6788
Training Epoch: 0 [2432/50048]	Loss: 4.6782
Training Epoch: 0 [2496/50048]	Loss: 4.7449
Training Epoch: 0 [2560/50048]	Loss: 4.4279
Training Epoch: 0 [2624/50048]	Loss: 4.5071
Training Epoch: 0 [2688/50048]	Loss: 4.5388
Training Epoch: 0 [2752/50048]	Loss: 4.5883
Training Epoch: 0 [2816/50048]	Loss: 4.5462
Training Epoch: 0 [2880/50048]	Loss: 4.5047
Training Epoch: 0 [2944/50048]	Loss: 4.5729
Training Epoch: 0 [3008/50048]	Loss: 4.5241
Training Epoch: 0 [3072/50048]	Loss: 4.5819
Training Epoch: 0 [3136/50048]	Loss: 4.6616
Training Epoch: 0 [3200/50048]	Loss: 4.4860
Profile done with power limit 125W
epoch 1 train time consumed: 7.77s
Validation Epoch: 0, Average loss: 0.0722, Accuracy: 0.0228
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 125000, 'lr': 0.0028284271247461905, 'energy': 123.55583863974817, 'time': 5.781693342999461, 'accuracy': 0.0227906050955414, 'total_cost': 60.53381294184599}
[run job] Launching job with BS 64: and LR: 0.0028284271247461905 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs64+lr0.00283+pl100', 'ZEUS_COST_THRESH': '121.06762588369197', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '0.0028284271247461905', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0028284271247461905', '--power_limit', '100']
[run job] cost_ub=121.06762588369197
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs64+lr0.00283+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.0028284271247461905
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 5.0397
Training Epoch: 0 [192/50048]	Loss: 5.2124
Training Epoch: 0 [256/50048]	Loss: 5.3850
Training Epoch: 0 [320/50048]	Loss: 5.2374
Training Epoch: 0 [384/50048]	Loss: 5.2512
Training Epoch: 0 [448/50048]	Loss: 4.9777
Training Epoch: 0 [512/50048]	Loss: 5.1775
Training Epoch: 0 [576/50048]	Loss: 4.6980
Training Epoch: 0 [640/50048]	Loss: 5.4626
Training Epoch: 0 [704/50048]	Loss: 4.8588
Training Epoch: 0 [768/50048]	Loss: 5.1479
Training Epoch: 0 [832/50048]	Loss: 4.9325
Training Epoch: 0 [896/50048]	Loss: 4.9265
Training Epoch: 0 [960/50048]	Loss: 4.9197
Training Epoch: 0 [1024/50048]	Loss: 4.7553
Training Epoch: 0 [1088/50048]	Loss: 4.8341
Training Epoch: 0 [1152/50048]	Loss: 4.7835
Training Epoch: 0 [1216/50048]	Loss: 4.7030
Training Epoch: 0 [1280/50048]	Loss: 4.6260
Training Epoch: 0 [1344/50048]	Loss: 4.7887
Training Epoch: 0 [1408/50048]	Loss: 4.5555
Training Epoch: 0 [1472/50048]	Loss: 4.5947
Training Epoch: 0 [1536/50048]	Loss: 4.7251
Training Epoch: 0 [1600/50048]	Loss: 4.9116
Training Epoch: 0 [1664/50048]	Loss: 4.9952
Training Epoch: 0 [1728/50048]	Loss: 4.8020
Training Epoch: 0 [1792/50048]	Loss: 4.7441
Training Epoch: 0 [1856/50048]	Loss: 4.9511
Training Epoch: 0 [1920/50048]	Loss: 4.6473
Training Epoch: 0 [1984/50048]	Loss: 4.5562
Training Epoch: 0 [2048/50048]	Loss: 4.5655
Training Epoch: 0 [2112/50048]	Loss: 4.8247
Training Epoch: 0 [2176/50048]	Loss: 4.5812
Training Epoch: 0 [2240/50048]	Loss: 4.5518
Training Epoch: 0 [2304/50048]	Loss: 4.5933
Training Epoch: 0 [2368/50048]	Loss: 4.5745
Training Epoch: 0 [2432/50048]	Loss: 4.6949
Training Epoch: 0 [2496/50048]	Loss: 4.7569
Training Epoch: 0 [2560/50048]	Loss: 4.4380
Training Epoch: 0 [2624/50048]	Loss: 4.5189
Training Epoch: 0 [2688/50048]	Loss: 4.6337
Training Epoch: 0 [2752/50048]	Loss: 4.5202
Training Epoch: 0 [2816/50048]	Loss: 4.4311
Training Epoch: 0 [2880/50048]	Loss: 4.3664
Training Epoch: 0 [2944/50048]	Loss: 4.6227
Training Epoch: 0 [3008/50048]	Loss: 4.3839
Training Epoch: 0 [3072/50048]	Loss: 4.4639
Training Epoch: 0 [3136/50048]	Loss: 4.7316
Training Epoch: 0 [3200/50048]	Loss: 4.5254
Profile done with power limit 100W
epoch 1 train time consumed: 15.50s
Validation Epoch: 0, Average loss: 0.0902, Accuracy: 0.0232
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 100000, 'lr': 0.0028284271247461905, 'energy': 97.76864428468683, 'time': 12.041186259000824, 'accuracy': 0.023188694267515922, 'total_cost': 113.20371756741001}

[Power Profiler] with batch size 64 and learning rate 0.0033941125496954284
[run job] Launching job with BS 64: and LR: 0.0033941125496954284 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs64+lr0.00339+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '0.0033941125496954284', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0033941125496954284', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs64+lr0.00339+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.0033941125496954284
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 5.2218
Training Epoch: 0 [192/50048]	Loss: 5.4194
Training Epoch: 0 [256/50048]	Loss: 5.5732
Training Epoch: 0 [320/50048]	Loss: 5.3696
Training Epoch: 0 [384/50048]	Loss: 5.3685
Training Epoch: 0 [448/50048]	Loss: 5.1101
Training Epoch: 0 [512/50048]	Loss: 5.3007
Training Epoch: 0 [576/50048]	Loss: 4.8189
Training Epoch: 0 [640/50048]	Loss: 5.5890
Training Epoch: 0 [704/50048]	Loss: 4.9062
Training Epoch: 0 [768/50048]	Loss: 5.0422
Training Epoch: 0 [832/50048]	Loss: 4.8356
Training Epoch: 0 [896/50048]	Loss: 4.9237
Training Epoch: 0 [960/50048]	Loss: 4.8804
Training Epoch: 0 [1024/50048]	Loss: 4.7413
Training Epoch: 0 [1088/50048]	Loss: 4.8422
Training Epoch: 0 [1152/50048]	Loss: 4.8351
Training Epoch: 0 [1216/50048]	Loss: 4.8206
Training Epoch: 0 [1280/50048]	Loss: 4.6796
Training Epoch: 0 [1344/50048]	Loss: 4.7561
Training Epoch: 0 [1408/50048]	Loss: 4.5650
Training Epoch: 0 [1472/50048]	Loss: 4.5792
Training Epoch: 0 [1536/50048]	Loss: 4.6800
Training Epoch: 0 [1600/50048]	Loss: 4.7318
Training Epoch: 0 [1664/50048]	Loss: 4.8552
Training Epoch: 0 [1728/50048]	Loss: 4.6684
Training Epoch: 0 [1792/50048]	Loss: 4.6570
Training Epoch: 0 [1856/50048]	Loss: 4.7743
Training Epoch: 0 [1920/50048]	Loss: 4.5891
Training Epoch: 0 [1984/50048]	Loss: 4.5316
Training Epoch: 0 [2048/50048]	Loss: 4.5307
Training Epoch: 0 [2112/50048]	Loss: 4.6713
Training Epoch: 0 [2176/50048]	Loss: 4.6114
Training Epoch: 0 [2240/50048]	Loss: 4.5727
Training Epoch: 0 [2304/50048]	Loss: 4.6352
Training Epoch: 0 [2368/50048]	Loss: 4.5862
Training Epoch: 0 [2432/50048]	Loss: 4.5229
Training Epoch: 0 [2496/50048]	Loss: 4.5883
Training Epoch: 0 [2560/50048]	Loss: 4.5325
Training Epoch: 0 [2624/50048]	Loss: 4.4700
Training Epoch: 0 [2688/50048]	Loss: 4.4811
Training Epoch: 0 [2752/50048]	Loss: 4.4928
Training Epoch: 0 [2816/50048]	Loss: 4.4848
Training Epoch: 0 [2880/50048]	Loss: 4.4809
Training Epoch: 0 [2944/50048]	Loss: 4.5305
Training Epoch: 0 [3008/50048]	Loss: 4.5113
Training Epoch: 0 [3072/50048]	Loss: 4.6341
Training Epoch: 0 [3136/50048]	Loss: 4.6254
Training Epoch: 0 [3200/50048]	Loss: 4.4893
Profile done with power limit 175W
epoch 1 train time consumed: 7.19s
Validation Epoch: 0, Average loss: 0.0752, Accuracy: 0.0204
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 175000, 'lr': 0.0033941125496954284, 'energy': 140.85165413137014, 'time': 5.2863207230002445, 'accuracy': 0.020402070063694266, 'total_cost': 65.40872651904066}
[run job] Launching job with BS 64: and LR: 0.0033941125496954284 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs64+lr0.00339+pl150', 'ZEUS_COST_THRESH': '130.8174530380813', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '0.0033941125496954284', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0033941125496954284', '--power_limit', '150']
[run job] cost_ub=130.8174530380813
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs64+lr0.00339+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.0033941125496954284
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 5.2218
Training Epoch: 0 [192/50048]	Loss: 5.4177
Training Epoch: 0 [256/50048]	Loss: 5.5588
Training Epoch: 0 [320/50048]	Loss: 5.3660
Training Epoch: 0 [384/50048]	Loss: 5.3740
Training Epoch: 0 [448/50048]	Loss: 5.0658
Training Epoch: 0 [512/50048]	Loss: 5.2397
Training Epoch: 0 [576/50048]	Loss: 4.7921
Training Epoch: 0 [640/50048]	Loss: 5.5557
Training Epoch: 0 [704/50048]	Loss: 4.7966
Training Epoch: 0 [768/50048]	Loss: 4.9339
Training Epoch: 0 [832/50048]	Loss: 4.7319
Training Epoch: 0 [896/50048]	Loss: 4.8825
Training Epoch: 0 [960/50048]	Loss: 4.9041
Training Epoch: 0 [1024/50048]	Loss: 4.7786
Training Epoch: 0 [1088/50048]	Loss: 4.7876
Training Epoch: 0 [1152/50048]	Loss: 4.8946
Training Epoch: 0 [1216/50048]	Loss: 4.7822
Training Epoch: 0 [1280/50048]	Loss: 4.6844
Training Epoch: 0 [1344/50048]	Loss: 4.7875
Training Epoch: 0 [1408/50048]	Loss: 4.5601
Training Epoch: 0 [1472/50048]	Loss: 4.5806
Training Epoch: 0 [1536/50048]	Loss: 4.7183
Training Epoch: 0 [1600/50048]	Loss: 4.7829
Training Epoch: 0 [1664/50048]	Loss: 4.8960
Training Epoch: 0 [1728/50048]	Loss: 4.7160
Training Epoch: 0 [1792/50048]	Loss: 4.6811
Training Epoch: 0 [1856/50048]	Loss: 4.7584
Training Epoch: 0 [1920/50048]	Loss: 4.6698
Training Epoch: 0 [1984/50048]	Loss: 4.5381
Training Epoch: 0 [2048/50048]	Loss: 4.5434
Training Epoch: 0 [2112/50048]	Loss: 4.7257
Training Epoch: 0 [2176/50048]	Loss: 4.6318
Training Epoch: 0 [2240/50048]	Loss: 4.5685
Training Epoch: 0 [2304/50048]	Loss: 4.6275
Training Epoch: 0 [2368/50048]	Loss: 4.5997
Training Epoch: 0 [2432/50048]	Loss: 4.5279
Training Epoch: 0 [2496/50048]	Loss: 4.5423
Training Epoch: 0 [2560/50048]	Loss: 4.5715
Training Epoch: 0 [2624/50048]	Loss: 4.4579
Training Epoch: 0 [2688/50048]	Loss: 4.5135
Training Epoch: 0 [2752/50048]	Loss: 4.4829
Training Epoch: 0 [2816/50048]	Loss: 4.4902
Training Epoch: 0 [2880/50048]	Loss: 4.4975
Training Epoch: 0 [2944/50048]	Loss: 4.5218
Training Epoch: 0 [3008/50048]	Loss: 4.4976
Training Epoch: 0 [3072/50048]	Loss: 4.6700
Training Epoch: 0 [3136/50048]	Loss: 4.6709
Training Epoch: 0 [3200/50048]	Loss: 4.4748
Profile done with power limit 150W
epoch 1 train time consumed: 7.22s
Validation Epoch: 0, Average loss: 0.0788, Accuracy: 0.0196
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 150000, 'lr': 0.0033941125496954284, 'energy': 141.13091758683527, 'time': 5.285094322000077, 'accuracy': 0.019605891719745222, 'total_cost': 68.10929405399577}
[run job] Launching job with BS 64: and LR: 0.0033941125496954284 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs64+lr0.00339+pl125', 'ZEUS_COST_THRESH': '130.8174530380813', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '0.0033941125496954284', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0033941125496954284', '--power_limit', '125']
[run job] cost_ub=130.8174530380813
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs64+lr0.00339+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.0033941125496954284
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 5.2218
Training Epoch: 0 [192/50048]	Loss: 5.4200
Training Epoch: 0 [256/50048]	Loss: 5.5639
Training Epoch: 0 [320/50048]	Loss: 5.3643
Training Epoch: 0 [384/50048]	Loss: 5.3820
Training Epoch: 0 [448/50048]	Loss: 5.0898
Training Epoch: 0 [512/50048]	Loss: 5.1582
Training Epoch: 0 [576/50048]	Loss: 4.7865
Training Epoch: 0 [640/50048]	Loss: 5.4860
Training Epoch: 0 [704/50048]	Loss: 4.7798
Training Epoch: 0 [768/50048]	Loss: 4.8925
Training Epoch: 0 [832/50048]	Loss: 4.7316
Training Epoch: 0 [896/50048]	Loss: 4.8803
Training Epoch: 0 [960/50048]	Loss: 4.9794
Training Epoch: 0 [1024/50048]	Loss: 4.7961
Training Epoch: 0 [1088/50048]	Loss: 4.7945
Training Epoch: 0 [1152/50048]	Loss: 4.8797
Training Epoch: 0 [1216/50048]	Loss: 4.9394
Training Epoch: 0 [1280/50048]	Loss: 4.7483
Training Epoch: 0 [1344/50048]	Loss: 4.7020
Training Epoch: 0 [1408/50048]	Loss: 4.5826
Training Epoch: 0 [1472/50048]	Loss: 4.6138
Training Epoch: 0 [1536/50048]	Loss: 4.7973
Training Epoch: 0 [1600/50048]	Loss: 4.6330
Training Epoch: 0 [1664/50048]	Loss: 4.9160
Training Epoch: 0 [1728/50048]	Loss: 4.6532
Training Epoch: 0 [1792/50048]	Loss: 4.8733
Training Epoch: 0 [1856/50048]	Loss: 4.6746
Training Epoch: 0 [1920/50048]	Loss: 4.6698
Training Epoch: 0 [1984/50048]	Loss: 4.5923
Training Epoch: 0 [2048/50048]	Loss: 4.5751
Training Epoch: 0 [2112/50048]	Loss: 4.7086
Training Epoch: 0 [2176/50048]	Loss: 4.5337
Training Epoch: 0 [2240/50048]	Loss: 4.6080
Training Epoch: 0 [2304/50048]	Loss: 4.6056
Training Epoch: 0 [2368/50048]	Loss: 4.5567
Training Epoch: 0 [2432/50048]	Loss: 4.6369
Training Epoch: 0 [2496/50048]	Loss: 4.4815
Training Epoch: 0 [2560/50048]	Loss: 4.4901
Training Epoch: 0 [2624/50048]	Loss: 4.3864
Training Epoch: 0 [2688/50048]	Loss: 4.4313
Training Epoch: 0 [2752/50048]	Loss: 4.4316
Training Epoch: 0 [2816/50048]	Loss: 4.4233
Training Epoch: 0 [2880/50048]	Loss: 4.3224
Training Epoch: 0 [2944/50048]	Loss: 4.5755
Training Epoch: 0 [3008/50048]	Loss: 4.5128
Training Epoch: 0 [3072/50048]	Loss: 4.5942
Training Epoch: 0 [3136/50048]	Loss: 4.5144
Training Epoch: 0 [3200/50048]	Loss: 4.5017
Profile done with power limit 125W
epoch 1 train time consumed: 7.74s
Validation Epoch: 0, Average loss: 0.0842, Accuracy: 0.0230
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 125000, 'lr': 0.0033941125496954284, 'energy': 123.66890939217338, 'time': 5.767239947999769, 'accuracy': 0.02298964968152866, 'total_cost': 59.882365491771424}
[run job] Launching job with BS 64: and LR: 0.0033941125496954284 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs64+lr0.00339+pl100', 'ZEUS_COST_THRESH': '119.76473098354285', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '0.0033941125496954284', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0033941125496954284', '--power_limit', '100']
[run job] cost_ub=119.76473098354285
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs64+lr0.00339+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.0033941125496954284
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 5.2218
Training Epoch: 0 [192/50048]	Loss: 5.4187
Training Epoch: 0 [256/50048]	Loss: 5.5707
Training Epoch: 0 [320/50048]	Loss: 5.3673
Training Epoch: 0 [384/50048]	Loss: 5.3631
Training Epoch: 0 [448/50048]	Loss: 5.0894
Training Epoch: 0 [512/50048]	Loss: 5.1794
Training Epoch: 0 [576/50048]	Loss: 4.7747
Training Epoch: 0 [640/50048]	Loss: 5.5178
Training Epoch: 0 [704/50048]	Loss: 4.7411
Training Epoch: 0 [768/50048]	Loss: 4.9169
Training Epoch: 0 [832/50048]	Loss: 4.7737
Training Epoch: 0 [896/50048]	Loss: 4.8654
Training Epoch: 0 [960/50048]	Loss: 4.9667
Training Epoch: 0 [1024/50048]	Loss: 4.7963
Training Epoch: 0 [1088/50048]	Loss: 4.8306
Training Epoch: 0 [1152/50048]	Loss: 4.8099
Training Epoch: 0 [1216/50048]	Loss: 4.7474
Training Epoch: 0 [1280/50048]	Loss: 4.6854
Training Epoch: 0 [1344/50048]	Loss: 4.8053
Training Epoch: 0 [1408/50048]	Loss: 4.5579
Training Epoch: 0 [1472/50048]	Loss: 4.5707
Training Epoch: 0 [1536/50048]	Loss: 4.6703
Training Epoch: 0 [1600/50048]	Loss: 4.7973
Training Epoch: 0 [1664/50048]	Loss: 4.8182
Training Epoch: 0 [1728/50048]	Loss: 4.7821
Training Epoch: 0 [1792/50048]	Loss: 4.6800
Training Epoch: 0 [1856/50048]	Loss: 4.7258
Training Epoch: 0 [1920/50048]	Loss: 4.5712
Training Epoch: 0 [1984/50048]	Loss: 4.5426
Training Epoch: 0 [2048/50048]	Loss: 4.5580
Training Epoch: 0 [2112/50048]	Loss: 4.7411
Training Epoch: 0 [2176/50048]	Loss: 4.5419
Training Epoch: 0 [2240/50048]	Loss: 4.5695
Training Epoch: 0 [2304/50048]	Loss: 4.5796
Training Epoch: 0 [2368/50048]	Loss: 4.6663
Training Epoch: 0 [2432/50048]	Loss: 4.6092
Training Epoch: 0 [2496/50048]	Loss: 4.5925
Training Epoch: 0 [2560/50048]	Loss: 4.4906
Training Epoch: 0 [2624/50048]	Loss: 4.4520
Training Epoch: 0 [2688/50048]	Loss: 4.4903
Training Epoch: 0 [2752/50048]	Loss: 4.4651
Training Epoch: 0 [2816/50048]	Loss: 4.4797
Training Epoch: 0 [2880/50048]	Loss: 4.4695
Training Epoch: 0 [2944/50048]	Loss: 4.5422
Training Epoch: 0 [3008/50048]	Loss: 4.5208
Training Epoch: 0 [3072/50048]	Loss: 4.6127
Training Epoch: 0 [3136/50048]	Loss: 4.5458
Training Epoch: 0 [3200/50048]	Loss: 4.5005
Profile done with power limit 100W
epoch 1 train time consumed: 15.35s
Validation Epoch: 0, Average loss: 0.0760, Accuracy: 0.0218
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 100000, 'lr': 0.0033941125496954284, 'energy': 98.9903751133419, 'time': 11.893013675000475, 'accuracy': 0.021795382165605094, 'total_cost': 119.49122158439621}

[Power Profiler] with batch size 128 and learning rate 0.0032
[run job] Launching job with BS 128: and LR: 0.0032 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs128+lr0.00320+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '0.0032', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0032', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs128+lr0.00320+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.0032
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 5.1136
Training Epoch: 0 [384/50048]	Loss: 5.3865
Training Epoch: 0 [512/50048]	Loss: 5.1020
Training Epoch: 0 [640/50048]	Loss: 5.0020
Training Epoch: 0 [768/50048]	Loss: 5.0185
Training Epoch: 0 [896/50048]	Loss: 5.0168
Training Epoch: 0 [1024/50048]	Loss: 5.0425
Training Epoch: 0 [1152/50048]	Loss: 4.9617
Training Epoch: 0 [1280/50048]	Loss: 4.9392
Training Epoch: 0 [1408/50048]	Loss: 4.6797
Training Epoch: 0 [1536/50048]	Loss: 4.8298
Training Epoch: 0 [1664/50048]	Loss: 4.8802
Training Epoch: 0 [1792/50048]	Loss: 4.5468
Training Epoch: 0 [1920/50048]	Loss: 4.6491
Training Epoch: 0 [2048/50048]	Loss: 4.5151
Training Epoch: 0 [2176/50048]	Loss: 4.7289
Training Epoch: 0 [2304/50048]	Loss: 4.6571
Training Epoch: 0 [2432/50048]	Loss: 4.5762
Training Epoch: 0 [2560/50048]	Loss: 4.5987
Training Epoch: 0 [2688/50048]	Loss: 4.5911
Training Epoch: 0 [2816/50048]	Loss: 4.6648
Training Epoch: 0 [2944/50048]	Loss: 4.4892
Training Epoch: 0 [3072/50048]	Loss: 4.5659
Training Epoch: 0 [3200/50048]	Loss: 4.6226
Training Epoch: 0 [3328/50048]	Loss: 4.5754
Training Epoch: 0 [3456/50048]	Loss: 4.5065
Training Epoch: 0 [3584/50048]	Loss: 4.4795
Training Epoch: 0 [3712/50048]	Loss: 4.7581
Training Epoch: 0 [3840/50048]	Loss: 4.5612
Training Epoch: 0 [3968/50048]	Loss: 4.5477
Training Epoch: 0 [4096/50048]	Loss: 4.7017
Training Epoch: 0 [4224/50048]	Loss: 4.5216
Training Epoch: 0 [4352/50048]	Loss: 4.4525
Training Epoch: 0 [4480/50048]	Loss: 4.4580
Training Epoch: 0 [4608/50048]	Loss: 4.4998
Training Epoch: 0 [4736/50048]	Loss: 4.3840
Training Epoch: 0 [4864/50048]	Loss: 4.4837
Training Epoch: 0 [4992/50048]	Loss: 4.3812
Training Epoch: 0 [5120/50048]	Loss: 4.4423
Training Epoch: 0 [5248/50048]	Loss: 4.4239
Training Epoch: 0 [5376/50048]	Loss: 4.3610
Training Epoch: 0 [5504/50048]	Loss: 4.4480
Training Epoch: 0 [5632/50048]	Loss: 4.4470
Training Epoch: 0 [5760/50048]	Loss: 4.3784
Training Epoch: 0 [5888/50048]	Loss: 4.2796
Training Epoch: 0 [6016/50048]	Loss: 4.3001
Training Epoch: 0 [6144/50048]	Loss: 4.1457
Training Epoch: 0 [6272/50048]	Loss: 4.4649
Training Epoch: 0 [6400/50048]	Loss: 4.2932
Profile done with power limit 175W
epoch 1 train time consumed: 11.48s
Validation Epoch: 0, Average loss: 0.0363, Accuracy: 0.0350
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 175000, 'lr': 0.0032, 'energy': 144.32286471894184, 'time': 8.642025066000315, 'accuracy': 0.03500791139240506, 'total_cost': 126.00349668837067}
[run job] Launching job with BS 128: and LR: 0.0032 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs128+lr0.00320+pl150', 'ZEUS_COST_THRESH': '252.00699337674135', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '0.0032', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0032', '--power_limit', '150']
[run job] cost_ub=252.00699337674135
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs128+lr0.00320+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.0032
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 5.1136
Training Epoch: 0 [384/50048]	Loss: 5.3871
Training Epoch: 0 [512/50048]	Loss: 5.1036
Training Epoch: 0 [640/50048]	Loss: 5.0037
Training Epoch: 0 [768/50048]	Loss: 4.9951
Training Epoch: 0 [896/50048]	Loss: 5.0329
Training Epoch: 0 [1024/50048]	Loss: 5.0099
Training Epoch: 0 [1152/50048]	Loss: 4.9349
Training Epoch: 0 [1280/50048]	Loss: 4.8864
Training Epoch: 0 [1408/50048]	Loss: 4.6674
Training Epoch: 0 [1536/50048]	Loss: 4.8352
Training Epoch: 0 [1664/50048]	Loss: 4.8066
Training Epoch: 0 [1792/50048]	Loss: 4.5573
Training Epoch: 0 [1920/50048]	Loss: 4.7185
Training Epoch: 0 [2048/50048]	Loss: 4.5146
Training Epoch: 0 [2176/50048]	Loss: 4.7480
Training Epoch: 0 [2304/50048]	Loss: 4.6546
Training Epoch: 0 [2432/50048]	Loss: 4.6026
Training Epoch: 0 [2560/50048]	Loss: 4.6579
Training Epoch: 0 [2688/50048]	Loss: 4.6156
Training Epoch: 0 [2816/50048]	Loss: 4.6536
Training Epoch: 0 [2944/50048]	Loss: 4.4897
Training Epoch: 0 [3072/50048]	Loss: 4.6618
Training Epoch: 0 [3200/50048]	Loss: 4.5920
Training Epoch: 0 [3328/50048]	Loss: 4.6099
Training Epoch: 0 [3456/50048]	Loss: 4.5105
Training Epoch: 0 [3584/50048]	Loss: 4.5421
Training Epoch: 0 [3712/50048]	Loss: 4.7759
Training Epoch: 0 [3840/50048]	Loss: 4.5358
Training Epoch: 0 [3968/50048]	Loss: 4.5408
Training Epoch: 0 [4096/50048]	Loss: 4.6882
Training Epoch: 0 [4224/50048]	Loss: 4.5450
Training Epoch: 0 [4352/50048]	Loss: 4.4555
Training Epoch: 0 [4480/50048]	Loss: 4.4604
Training Epoch: 0 [4608/50048]	Loss: 4.5655
Training Epoch: 0 [4736/50048]	Loss: 4.4445
Training Epoch: 0 [4864/50048]	Loss: 4.5166
Training Epoch: 0 [4992/50048]	Loss: 4.4884
Training Epoch: 0 [5120/50048]	Loss: 4.4273
Training Epoch: 0 [5248/50048]	Loss: 4.4267
Training Epoch: 0 [5376/50048]	Loss: 4.3797
Training Epoch: 0 [5504/50048]	Loss: 4.4049
Training Epoch: 0 [5632/50048]	Loss: 4.4874
Training Epoch: 0 [5760/50048]	Loss: 4.3816
Training Epoch: 0 [5888/50048]	Loss: 4.2917
Training Epoch: 0 [6016/50048]	Loss: 4.3384
Training Epoch: 0 [6144/50048]	Loss: 4.1306
Training Epoch: 0 [6272/50048]	Loss: 4.5559
Training Epoch: 0 [6400/50048]	Loss: 4.3189
Profile done with power limit 150W
epoch 1 train time consumed: 11.44s
Validation Epoch: 0, Average loss: 0.0378, Accuracy: 0.0298
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 150000, 'lr': 0.0032, 'energy': 144.60684146270575, 'time': 8.628159099000186, 'accuracy': 0.029766613924050632, 'total_cost': 148.08396598160488}
[run job] Launching job with BS 128: and LR: 0.0032 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs128+lr0.00320+pl125', 'ZEUS_COST_THRESH': '252.00699337674135', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '0.0032', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0032', '--power_limit', '125']
[run job] cost_ub=252.00699337674135
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs128+lr0.00320+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.0032
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 5.1136
Training Epoch: 0 [384/50048]	Loss: 5.3871
Training Epoch: 0 [512/50048]	Loss: 5.1016
Training Epoch: 0 [640/50048]	Loss: 5.0142
Training Epoch: 0 [768/50048]	Loss: 5.0060
Training Epoch: 0 [896/50048]	Loss: 5.0057
Training Epoch: 0 [1024/50048]	Loss: 5.0522
Training Epoch: 0 [1152/50048]	Loss: 4.9587
Training Epoch: 0 [1280/50048]	Loss: 4.9166
Training Epoch: 0 [1408/50048]	Loss: 4.6564
Training Epoch: 0 [1536/50048]	Loss: 4.8080
Training Epoch: 0 [1664/50048]	Loss: 4.8869
Training Epoch: 0 [1792/50048]	Loss: 4.5657
Training Epoch: 0 [1920/50048]	Loss: 4.6483
Training Epoch: 0 [2048/50048]	Loss: 4.5716
Training Epoch: 0 [2176/50048]	Loss: 4.7463
Training Epoch: 0 [2304/50048]	Loss: 4.6663
Training Epoch: 0 [2432/50048]	Loss: 4.5821
Training Epoch: 0 [2560/50048]	Loss: 4.6368
Training Epoch: 0 [2688/50048]	Loss: 4.5988
Training Epoch: 0 [2816/50048]	Loss: 4.6976
Training Epoch: 0 [2944/50048]	Loss: 4.5006
Training Epoch: 0 [3072/50048]	Loss: 4.5563
Training Epoch: 0 [3200/50048]	Loss: 4.6430
Training Epoch: 0 [3328/50048]	Loss: 4.5630
Training Epoch: 0 [3456/50048]	Loss: 4.5443
Training Epoch: 0 [3584/50048]	Loss: 4.5354
Training Epoch: 0 [3712/50048]	Loss: 4.8539
Training Epoch: 0 [3840/50048]	Loss: 4.5389
Training Epoch: 0 [3968/50048]	Loss: 4.5528
Training Epoch: 0 [4096/50048]	Loss: 4.7866
Training Epoch: 0 [4224/50048]	Loss: 4.5440
Training Epoch: 0 [4352/50048]	Loss: 4.4558
Training Epoch: 0 [4480/50048]	Loss: 4.4852
Training Epoch: 0 [4608/50048]	Loss: 4.5447
Training Epoch: 0 [4736/50048]	Loss: 4.3964
Training Epoch: 0 [4864/50048]	Loss: 4.4691
Training Epoch: 0 [4992/50048]	Loss: 4.4080
Training Epoch: 0 [5120/50048]	Loss: 4.4404
Training Epoch: 0 [5248/50048]	Loss: 4.4060
Training Epoch: 0 [5376/50048]	Loss: 4.3886
Training Epoch: 0 [5504/50048]	Loss: 4.4761
Training Epoch: 0 [5632/50048]	Loss: 4.4745
Training Epoch: 0 [5760/50048]	Loss: 4.3795
Training Epoch: 0 [5888/50048]	Loss: 4.2790
Training Epoch: 0 [6016/50048]	Loss: 4.3295
Training Epoch: 0 [6144/50048]	Loss: 4.1481
Training Epoch: 0 [6272/50048]	Loss: 4.4788
Training Epoch: 0 [6400/50048]	Loss: 4.2898
Profile done with power limit 125W
epoch 1 train time consumed: 12.74s
Validation Epoch: 0, Average loss: 0.0362, Accuracy: 0.0346
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 125000, 'lr': 0.0032, 'energy': 123.17760556517919, 'time': 9.722833820999767, 'accuracy': 0.034612341772151896, 'total_cost': 133.8875401308027}
[run job] Launching job with BS 128: and LR: 0.0032 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs128+lr0.00320+pl100', 'ZEUS_COST_THRESH': '252.00699337674135', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '0.0032', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0032', '--power_limit', '100']
[run job] cost_ub=252.00699337674135
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs128+lr0.00320+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.0032
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 5.1136
Training Epoch: 0 [384/50048]	Loss: 5.3873
Training Epoch: 0 [512/50048]	Loss: 5.0929
Training Epoch: 0 [640/50048]	Loss: 4.9896
Training Epoch: 0 [768/50048]	Loss: 5.0132
Training Epoch: 0 [896/50048]	Loss: 5.0249
Training Epoch: 0 [1024/50048]	Loss: 5.0684
Training Epoch: 0 [1152/50048]	Loss: 4.9766
Training Epoch: 0 [1280/50048]	Loss: 4.9667
Training Epoch: 0 [1408/50048]	Loss: 4.6979
Training Epoch: 0 [1536/50048]	Loss: 4.8437
Training Epoch: 0 [1664/50048]	Loss: 4.7905
Training Epoch: 0 [1792/50048]	Loss: 4.5495
Training Epoch: 0 [1920/50048]	Loss: 4.6572
Training Epoch: 0 [2048/50048]	Loss: 4.5655
Training Epoch: 0 [2176/50048]	Loss: 4.7091
Training Epoch: 0 [2304/50048]	Loss: 4.6387
Training Epoch: 0 [2432/50048]	Loss: 4.5771
Training Epoch: 0 [2560/50048]	Loss: 4.6132
Training Epoch: 0 [2688/50048]	Loss: 4.6024
Training Epoch: 0 [2816/50048]	Loss: 4.6816
Training Epoch: 0 [2944/50048]	Loss: 4.4885
Training Epoch: 0 [3072/50048]	Loss: 4.5354
Training Epoch: 0 [3200/50048]	Loss: 4.5808
Training Epoch: 0 [3328/50048]	Loss: 4.5247
Training Epoch: 0 [3456/50048]	Loss: 4.5451
Training Epoch: 0 [3584/50048]	Loss: 4.5122
Training Epoch: 0 [3712/50048]	Loss: 4.8517
Training Epoch: 0 [3840/50048]	Loss: 4.5712
Training Epoch: 0 [3968/50048]	Loss: 4.5297
Training Epoch: 0 [4096/50048]	Loss: 4.7401
Training Epoch: 0 [4224/50048]	Loss: 4.5381
Training Epoch: 0 [4352/50048]	Loss: 4.4719
Training Epoch: 0 [4480/50048]	Loss: 4.4500
Training Epoch: 0 [4608/50048]	Loss: 4.5455
Training Epoch: 0 [4736/50048]	Loss: 4.4348
Training Epoch: 0 [4864/50048]	Loss: 4.5076
Training Epoch: 0 [4992/50048]	Loss: 4.4479
Training Epoch: 0 [5120/50048]	Loss: 4.4650
Training Epoch: 0 [5248/50048]	Loss: 4.4057
Training Epoch: 0 [5376/50048]	Loss: 4.4483
Training Epoch: 0 [5504/50048]	Loss: 4.4971
Training Epoch: 0 [5632/50048]	Loss: 4.4485
Training Epoch: 0 [5760/50048]	Loss: 4.4515
Training Epoch: 0 [5888/50048]	Loss: 4.3329
Training Epoch: 0 [6016/50048]	Loss: 4.3504
Training Epoch: 0 [6144/50048]	Loss: 4.2199
Training Epoch: 0 [6272/50048]	Loss: 4.5322
Training Epoch: 0 [6400/50048]	Loss: 4.3395
Profile done with power limit 100W
epoch 1 train time consumed: 26.55s
Validation Epoch: 0, Average loss: 0.0366, Accuracy: 0.0286
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 100000, 'lr': 0.0032, 'energy': 98.73342946860437, 'time': 20.617449428000327, 'accuracy': 0.02857990506329114, 'total_cost': 315.6496082032171}

[Power Profiler] with batch size 128 and learning rate 0.004
[run job] Launching job with BS 128: and LR: 0.004 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs128+lr0.00400+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '0.004', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '0.004', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs128+lr0.00400+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.004
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 5.3519
Training Epoch: 0 [384/50048]	Loss: 5.6631
Training Epoch: 0 [512/50048]	Loss: 5.2282
Training Epoch: 0 [640/50048]	Loss: 5.0278
Training Epoch: 0 [768/50048]	Loss: 4.9885
Training Epoch: 0 [896/50048]	Loss: 5.0208
Training Epoch: 0 [1024/50048]	Loss: 4.9303
Training Epoch: 0 [1152/50048]	Loss: 4.8055
Training Epoch: 0 [1280/50048]	Loss: 4.8090
Training Epoch: 0 [1408/50048]	Loss: 4.6093
Training Epoch: 0 [1536/50048]	Loss: 4.8887
Training Epoch: 0 [1664/50048]	Loss: 4.9806
Training Epoch: 0 [1792/50048]	Loss: 4.5738
Training Epoch: 0 [1920/50048]	Loss: 4.6990
Training Epoch: 0 [2048/50048]	Loss: 4.5389
Training Epoch: 0 [2176/50048]	Loss: 4.6918
Training Epoch: 0 [2304/50048]	Loss: 4.6190
Training Epoch: 0 [2432/50048]	Loss: 4.5426
Training Epoch: 0 [2560/50048]	Loss: 4.5961
Training Epoch: 0 [2688/50048]	Loss: 4.5715
Training Epoch: 0 [2816/50048]	Loss: 4.6319
Training Epoch: 0 [2944/50048]	Loss: 4.5647
Training Epoch: 0 [3072/50048]	Loss: 4.5902
Training Epoch: 0 [3200/50048]	Loss: 4.6162
Training Epoch: 0 [3328/50048]	Loss: 4.5378
Training Epoch: 0 [3456/50048]	Loss: 4.5020
Training Epoch: 0 [3584/50048]	Loss: 4.5355
Training Epoch: 0 [3712/50048]	Loss: 4.7655
Training Epoch: 0 [3840/50048]	Loss: 4.5932
Training Epoch: 0 [3968/50048]	Loss: 4.5530
Training Epoch: 0 [4096/50048]	Loss: 4.6126
Training Epoch: 0 [4224/50048]	Loss: 4.5550
Training Epoch: 0 [4352/50048]	Loss: 4.5244
Training Epoch: 0 [4480/50048]	Loss: 4.4650
Training Epoch: 0 [4608/50048]	Loss: 4.4699
Training Epoch: 0 [4736/50048]	Loss: 4.4087
Training Epoch: 0 [4864/50048]	Loss: 4.4763
Training Epoch: 0 [4992/50048]	Loss: 4.4362
Training Epoch: 0 [5120/50048]	Loss: 4.4639
Training Epoch: 0 [5248/50048]	Loss: 4.4144
Training Epoch: 0 [5376/50048]	Loss: 4.3865
Training Epoch: 0 [5504/50048]	Loss: 4.4650
Training Epoch: 0 [5632/50048]	Loss: 4.4925
Training Epoch: 0 [5760/50048]	Loss: 4.3510
Training Epoch: 0 [5888/50048]	Loss: 4.2805
Training Epoch: 0 [6016/50048]	Loss: 4.2907
Training Epoch: 0 [6144/50048]	Loss: 4.1749
Training Epoch: 0 [6272/50048]	Loss: 4.4595
Training Epoch: 0 [6400/50048]	Loss: 4.2818
Profile done with power limit 175W
epoch 1 train time consumed: 11.48s
Validation Epoch: 0, Average loss: 0.0357, Accuracy: 0.0328
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 175000, 'lr': 0.004, 'energy': 143.8220655880103, 'time': 8.65234766499998, 'accuracy': 0.03283227848101266, 'total_cost': 134.30264630359622}
[run job] Launching job with BS 128: and LR: 0.004 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs128+lr0.00400+pl150', 'ZEUS_COST_THRESH': '268.60529260719244', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '0.004', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '0.004', '--power_limit', '150']
[run job] cost_ub=268.60529260719244
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs128+lr0.00400+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.004
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 5.3519
Training Epoch: 0 [384/50048]	Loss: 5.6637
Training Epoch: 0 [512/50048]	Loss: 5.2101
Training Epoch: 0 [640/50048]	Loss: 4.9898
Training Epoch: 0 [768/50048]	Loss: 4.9562
Training Epoch: 0 [896/50048]	Loss: 5.0383
Training Epoch: 0 [1024/50048]	Loss: 4.9217
Training Epoch: 0 [1152/50048]	Loss: 4.8577
Training Epoch: 0 [1280/50048]	Loss: 4.7981
Training Epoch: 0 [1408/50048]	Loss: 4.6345
Training Epoch: 0 [1536/50048]	Loss: 4.9586
Training Epoch: 0 [1664/50048]	Loss: 5.0581
Training Epoch: 0 [1792/50048]	Loss: 4.5963
Training Epoch: 0 [1920/50048]	Loss: 4.8060
Training Epoch: 0 [2048/50048]	Loss: 4.5199
Training Epoch: 0 [2176/50048]	Loss: 4.7035
Training Epoch: 0 [2304/50048]	Loss: 4.6030
Training Epoch: 0 [2432/50048]	Loss: 4.5463
Training Epoch: 0 [2560/50048]	Loss: 4.5817
Training Epoch: 0 [2688/50048]	Loss: 4.6247
Training Epoch: 0 [2816/50048]	Loss: 4.5470
Training Epoch: 0 [2944/50048]	Loss: 4.5324
Training Epoch: 0 [3072/50048]	Loss: 4.6820
Training Epoch: 0 [3200/50048]	Loss: 4.5997
Training Epoch: 0 [3328/50048]	Loss: 4.5922
Training Epoch: 0 [3456/50048]	Loss: 4.5212
Training Epoch: 0 [3584/50048]	Loss: 4.5669
Training Epoch: 0 [3712/50048]	Loss: 4.8283
Training Epoch: 0 [3840/50048]	Loss: 4.6027
Training Epoch: 0 [3968/50048]	Loss: 4.5425
Training Epoch: 0 [4096/50048]	Loss: 4.7016
Training Epoch: 0 [4224/50048]	Loss: 4.6665
Training Epoch: 0 [4352/50048]	Loss: 4.5749
Training Epoch: 0 [4480/50048]	Loss: 4.5139
Training Epoch: 0 [4608/50048]	Loss: 4.5796
Training Epoch: 0 [4736/50048]	Loss: 4.4660
Training Epoch: 0 [4864/50048]	Loss: 4.5092
Training Epoch: 0 [4992/50048]	Loss: 4.4330
Training Epoch: 0 [5120/50048]	Loss: 4.4601
Training Epoch: 0 [5248/50048]	Loss: 4.4535
Training Epoch: 0 [5376/50048]	Loss: 4.4427
Training Epoch: 0 [5504/50048]	Loss: 4.3814
Training Epoch: 0 [5632/50048]	Loss: 4.4896
Training Epoch: 0 [5760/50048]	Loss: 4.4419
Training Epoch: 0 [5888/50048]	Loss: 4.3635
Training Epoch: 0 [6016/50048]	Loss: 4.3753
Training Epoch: 0 [6144/50048]	Loss: 4.1886
Training Epoch: 0 [6272/50048]	Loss: 4.5380
Training Epoch: 0 [6400/50048]	Loss: 4.3548
Profile done with power limit 150W
epoch 1 train time consumed: 11.41s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0289
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 150000, 'lr': 0.004, 'energy': 144.4714266806383, 'time': 8.644126246999804, 'accuracy': 0.028876582278481014, 'total_cost': 152.86589091433257}
[run job] Launching job with BS 128: and LR: 0.004 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs128+lr0.00400+pl125', 'ZEUS_COST_THRESH': '268.60529260719244', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '0.004', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '0.004', '--power_limit', '125']
[run job] cost_ub=268.60529260719244
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs128+lr0.00400+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.004
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 5.3519
Training Epoch: 0 [384/50048]	Loss: 5.6641
Training Epoch: 0 [512/50048]	Loss: 5.2354
Training Epoch: 0 [640/50048]	Loss: 5.0129
Training Epoch: 0 [768/50048]	Loss: 4.9914
Training Epoch: 0 [896/50048]	Loss: 5.0242
Training Epoch: 0 [1024/50048]	Loss: 4.9273
Training Epoch: 0 [1152/50048]	Loss: 4.8294
Training Epoch: 0 [1280/50048]	Loss: 4.7949
Training Epoch: 0 [1408/50048]	Loss: 4.6198
Training Epoch: 0 [1536/50048]	Loss: 4.7952
Training Epoch: 0 [1664/50048]	Loss: 4.9998
Training Epoch: 0 [1792/50048]	Loss: 4.5858
Training Epoch: 0 [1920/50048]	Loss: 4.7614
Training Epoch: 0 [2048/50048]	Loss: 4.5122
Training Epoch: 0 [2176/50048]	Loss: 4.6813
Training Epoch: 0 [2304/50048]	Loss: 4.5955
Training Epoch: 0 [2432/50048]	Loss: 4.5385
Training Epoch: 0 [2560/50048]	Loss: 4.5674
Training Epoch: 0 [2688/50048]	Loss: 4.5321
Training Epoch: 0 [2816/50048]	Loss: 4.5240
Training Epoch: 0 [2944/50048]	Loss: 4.5244
Training Epoch: 0 [3072/50048]	Loss: 4.7498
Training Epoch: 0 [3200/50048]	Loss: 4.7553
Training Epoch: 0 [3328/50048]	Loss: 4.5651
Training Epoch: 0 [3456/50048]	Loss: 4.5033
Training Epoch: 0 [3584/50048]	Loss: 4.5511
Training Epoch: 0 [3712/50048]	Loss: 4.7311
Training Epoch: 0 [3840/50048]	Loss: 4.6296
Training Epoch: 0 [3968/50048]	Loss: 4.5969
Training Epoch: 0 [4096/50048]	Loss: 4.6350
Training Epoch: 0 [4224/50048]	Loss: 4.6008
Training Epoch: 0 [4352/50048]	Loss: 4.5131
Training Epoch: 0 [4480/50048]	Loss: 4.4774
Training Epoch: 0 [4608/50048]	Loss: 4.4300
Training Epoch: 0 [4736/50048]	Loss: 4.4288
Training Epoch: 0 [4864/50048]	Loss: 4.5140
Training Epoch: 0 [4992/50048]	Loss: 4.3432
Training Epoch: 0 [5120/50048]	Loss: 4.4138
Training Epoch: 0 [5248/50048]	Loss: 4.4335
Training Epoch: 0 [5376/50048]	Loss: 4.3413
Training Epoch: 0 [5504/50048]	Loss: 4.3660
Training Epoch: 0 [5632/50048]	Loss: 4.4931
Training Epoch: 0 [5760/50048]	Loss: 4.4674
Training Epoch: 0 [5888/50048]	Loss: 4.3105
Training Epoch: 0 [6016/50048]	Loss: 4.3677
Training Epoch: 0 [6144/50048]	Loss: 4.2017
Training Epoch: 0 [6272/50048]	Loss: 4.5117
Training Epoch: 0 [6400/50048]	Loss: 4.3596
Profile done with power limit 125W
epoch 1 train time consumed: 12.65s
Validation Epoch: 0, Average loss: 0.0381, Accuracy: 0.0290
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 125000, 'lr': 0.004, 'energy': 123.31830187846056, 'time': 9.649225109000326, 'accuracy': 0.028975474683544302, 'total_cost': 158.79800317229018}
[run job] Launching job with BS 128: and LR: 0.004 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs128+lr0.00400+pl100', 'ZEUS_COST_THRESH': '268.60529260719244', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '0.004', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '0.004', '--power_limit', '100']
[run job] cost_ub=268.60529260719244
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs128+lr0.00400+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.004
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 5.3519
Training Epoch: 0 [384/50048]	Loss: 5.6623
Training Epoch: 0 [512/50048]	Loss: 5.2352
Training Epoch: 0 [640/50048]	Loss: 5.0425
Training Epoch: 0 [768/50048]	Loss: 4.9974
Training Epoch: 0 [896/50048]	Loss: 5.0194
Training Epoch: 0 [1024/50048]	Loss: 4.9321
Training Epoch: 0 [1152/50048]	Loss: 4.8776
Training Epoch: 0 [1280/50048]	Loss: 4.8570
Training Epoch: 0 [1408/50048]	Loss: 4.6347
Training Epoch: 0 [1536/50048]	Loss: 4.6601
Training Epoch: 0 [1664/50048]	Loss: 4.9431
Training Epoch: 0 [1792/50048]	Loss: 4.7773
Training Epoch: 0 [1920/50048]	Loss: 4.8062
Training Epoch: 0 [2048/50048]	Loss: 4.5390
Training Epoch: 0 [2176/50048]	Loss: 4.6853
Training Epoch: 0 [2304/50048]	Loss: 4.6002
Training Epoch: 0 [2432/50048]	Loss: 4.7328
Training Epoch: 0 [2560/50048]	Loss: 4.7704
Training Epoch: 0 [2688/50048]	Loss: 4.6488
Training Epoch: 0 [2816/50048]	Loss: 4.5977
Training Epoch: 0 [2944/50048]	Loss: 4.5378
Training Epoch: 0 [3072/50048]	Loss: 4.6517
Training Epoch: 0 [3200/50048]	Loss: 4.5449
Training Epoch: 0 [3328/50048]	Loss: 4.5738
Training Epoch: 0 [3456/50048]	Loss: 4.4773
Training Epoch: 0 [3584/50048]	Loss: 4.5552
Training Epoch: 0 [3712/50048]	Loss: 4.7228
Training Epoch: 0 [3840/50048]	Loss: 4.5179
Training Epoch: 0 [3968/50048]	Loss: 4.5827
Training Epoch: 0 [4096/50048]	Loss: 4.6110
Training Epoch: 0 [4224/50048]	Loss: 4.5613
Training Epoch: 0 [4352/50048]	Loss: 4.5066
Training Epoch: 0 [4480/50048]	Loss: 4.4963
Training Epoch: 0 [4608/50048]	Loss: 4.4922
Training Epoch: 0 [4736/50048]	Loss: 4.4878
Training Epoch: 0 [4864/50048]	Loss: 4.5147
Training Epoch: 0 [4992/50048]	Loss: 4.3283
Training Epoch: 0 [5120/50048]	Loss: 4.4171
Training Epoch: 0 [5248/50048]	Loss: 4.4663
Training Epoch: 0 [5376/50048]	Loss: 4.4106
Training Epoch: 0 [5504/50048]	Loss: 4.3945
Training Epoch: 0 [5632/50048]	Loss: 4.4600
Training Epoch: 0 [5760/50048]	Loss: 4.4089
Training Epoch: 0 [5888/50048]	Loss: 4.3139
Training Epoch: 0 [6016/50048]	Loss: 4.3493
Training Epoch: 0 [6144/50048]	Loss: 4.2205
Training Epoch: 0 [6272/50048]	Loss: 4.4618
Training Epoch: 0 [6400/50048]	Loss: 4.3518
Profile done with power limit 100W
epoch 1 train time consumed: 26.46s
Validation Epoch: 0, Average loss: 0.0371, Accuracy: 0.0293
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 100000, 'lr': 0.004, 'energy': 99.3152647134903, 'time': 20.880086557999675, 'accuracy': 0.029272151898734177, 'total_cost': 312.77416965539743}

[Power Profiler] with batch size 128 and learning rate 0.0048
[run job] Launching job with BS 128: and LR: 0.0048 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs128+lr0.00480+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '0.0048', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0048', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs128+lr0.00480+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.0048
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 5.6144
Training Epoch: 0 [384/50048]	Loss: 5.9811
Training Epoch: 0 [512/50048]	Loss: 5.3826
Training Epoch: 0 [640/50048]	Loss: 5.0985
Training Epoch: 0 [768/50048]	Loss: 5.1259
Training Epoch: 0 [896/50048]	Loss: 5.0382
Training Epoch: 0 [1024/50048]	Loss: 4.9377
Training Epoch: 0 [1152/50048]	Loss: 4.8204
Training Epoch: 0 [1280/50048]	Loss: 4.7642
Training Epoch: 0 [1408/50048]	Loss: 4.5547
Training Epoch: 0 [1536/50048]	Loss: 4.7097
Training Epoch: 0 [1664/50048]	Loss: 4.8994
Training Epoch: 0 [1792/50048]	Loss: 4.6601
Training Epoch: 0 [1920/50048]	Loss: 4.5764
Training Epoch: 0 [2048/50048]	Loss: 4.5366
Training Epoch: 0 [2176/50048]	Loss: 4.6759
Training Epoch: 0 [2304/50048]	Loss: 4.6261
Training Epoch: 0 [2432/50048]	Loss: 4.6333
Training Epoch: 0 [2560/50048]	Loss: 4.5664
Training Epoch: 0 [2688/50048]	Loss: 4.5128
Training Epoch: 0 [2816/50048]	Loss: 4.7689
Training Epoch: 0 [2944/50048]	Loss: 4.6729
Training Epoch: 0 [3072/50048]	Loss: 4.6421
Training Epoch: 0 [3200/50048]	Loss: 4.7256
Training Epoch: 0 [3328/50048]	Loss: 4.6204
Training Epoch: 0 [3456/50048]	Loss: 4.5214
Training Epoch: 0 [3584/50048]	Loss: 4.6003
Training Epoch: 0 [3712/50048]	Loss: 4.8199
Training Epoch: 0 [3840/50048]	Loss: 4.6769
Training Epoch: 0 [3968/50048]	Loss: 4.5537
Training Epoch: 0 [4096/50048]	Loss: 4.6466
Training Epoch: 0 [4224/50048]	Loss: 4.6028
Training Epoch: 0 [4352/50048]	Loss: 4.6012
Training Epoch: 0 [4480/50048]	Loss: 4.5272
Training Epoch: 0 [4608/50048]	Loss: 4.5786
Training Epoch: 0 [4736/50048]	Loss: 4.4823
Training Epoch: 0 [4864/50048]	Loss: 4.5134
Training Epoch: 0 [4992/50048]	Loss: 4.5544
Training Epoch: 0 [5120/50048]	Loss: 4.5689
Training Epoch: 0 [5248/50048]	Loss: 4.4746
Training Epoch: 0 [5376/50048]	Loss: 4.5520
Training Epoch: 0 [5504/50048]	Loss: 4.5608
Training Epoch: 0 [5632/50048]	Loss: 4.6005
Training Epoch: 0 [5760/50048]	Loss: 4.5007
Training Epoch: 0 [5888/50048]	Loss: 4.4782
Training Epoch: 0 [6016/50048]	Loss: 4.4951
Training Epoch: 0 [6144/50048]	Loss: 4.4549
Training Epoch: 0 [6272/50048]	Loss: 4.5528
Training Epoch: 0 [6400/50048]	Loss: 4.4701
Profile done with power limit 175W
epoch 1 train time consumed: 11.44s
Validation Epoch: 0, Average loss: 0.0358, Accuracy: 0.0209
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 175000, 'lr': 0.0048, 'energy': 143.7565025614159, 'time': 8.634199962000821, 'accuracy': 0.02086629746835443, 'total_cost': 210.83320997511314}
[run job] Launching job with BS 128: and LR: 0.0048 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs128+lr0.00480+pl150', 'ZEUS_COST_THRESH': '421.66641995022627', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '0.0048', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0048', '--power_limit', '150']
[run job] cost_ub=421.66641995022627
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs128+lr0.00480+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.0048
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 5.6144
Training Epoch: 0 [384/50048]	Loss: 5.9818
Training Epoch: 0 [512/50048]	Loss: 5.3641
Training Epoch: 0 [640/50048]	Loss: 5.0585
Training Epoch: 0 [768/50048]	Loss: 5.0045
Training Epoch: 0 [896/50048]	Loss: 5.0513
Training Epoch: 0 [1024/50048]	Loss: 4.9454
Training Epoch: 0 [1152/50048]	Loss: 4.8338
Training Epoch: 0 [1280/50048]	Loss: 4.7528
Training Epoch: 0 [1408/50048]	Loss: 4.5552
Training Epoch: 0 [1536/50048]	Loss: 4.8142
Training Epoch: 0 [1664/50048]	Loss: 4.9792
Training Epoch: 0 [1792/50048]	Loss: 4.6172
Training Epoch: 0 [1920/50048]	Loss: 4.8503
Training Epoch: 0 [2048/50048]	Loss: 4.5577
Training Epoch: 0 [2176/50048]	Loss: 4.7206
Training Epoch: 0 [2304/50048]	Loss: 4.6009
Training Epoch: 0 [2432/50048]	Loss: 4.5736
Training Epoch: 0 [2560/50048]	Loss: 4.8437
Training Epoch: 0 [2688/50048]	Loss: 4.6856
Training Epoch: 0 [2816/50048]	Loss: 4.6525
Training Epoch: 0 [2944/50048]	Loss: 4.6250
Training Epoch: 0 [3072/50048]	Loss: 4.5972
Training Epoch: 0 [3200/50048]	Loss: 4.5961
Training Epoch: 0 [3328/50048]	Loss: 4.5435
Training Epoch: 0 [3456/50048]	Loss: 4.5459
Training Epoch: 0 [3584/50048]	Loss: 4.5640
Training Epoch: 0 [3712/50048]	Loss: 4.8097
Training Epoch: 0 [3840/50048]	Loss: 4.5994
Training Epoch: 0 [3968/50048]	Loss: 4.5212
Training Epoch: 0 [4096/50048]	Loss: 4.6606
Training Epoch: 0 [4224/50048]	Loss: 4.6173
Training Epoch: 0 [4352/50048]	Loss: 4.5911
Training Epoch: 0 [4480/50048]	Loss: 4.5065
Training Epoch: 0 [4608/50048]	Loss: 4.5563
Training Epoch: 0 [4736/50048]	Loss: 4.4987
Training Epoch: 0 [4864/50048]	Loss: 4.5447
Training Epoch: 0 [4992/50048]	Loss: 4.5268
Training Epoch: 0 [5120/50048]	Loss: 4.5002
Training Epoch: 0 [5248/50048]	Loss: 4.4821
Training Epoch: 0 [5376/50048]	Loss: 4.5156
Training Epoch: 0 [5504/50048]	Loss: 4.5144
Training Epoch: 0 [5632/50048]	Loss: 4.5316
Training Epoch: 0 [5760/50048]	Loss: 4.4923
Training Epoch: 0 [5888/50048]	Loss: 4.4411
Training Epoch: 0 [6016/50048]	Loss: 4.4515
Training Epoch: 0 [6144/50048]	Loss: 4.3923
Training Epoch: 0 [6272/50048]	Loss: 4.6145
Training Epoch: 0 [6400/50048]	Loss: 4.3919
Profile done with power limit 150W
epoch 1 train time consumed: 11.48s
Validation Epoch: 0, Average loss: 0.0374, Accuracy: 0.0260
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 150000, 'lr': 0.0048, 'energy': 144.36667985148583, 'time': 8.640819667999494, 'accuracy': 0.02600870253164557, 'total_cost': 169.60127678592585}
[run job] Launching job with BS 128: and LR: 0.0048 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs128+lr0.00480+pl125', 'ZEUS_COST_THRESH': '339.2025535718517', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '0.0048', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0048', '--power_limit', '125']
[run job] cost_ub=339.2025535718517
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs128+lr0.00480+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.0048
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 5.6144
Training Epoch: 0 [384/50048]	Loss: 5.9809
Training Epoch: 0 [512/50048]	Loss: 5.3611
Training Epoch: 0 [640/50048]	Loss: 5.1063
Training Epoch: 0 [768/50048]	Loss: 5.0825
Training Epoch: 0 [896/50048]	Loss: 5.0519
Training Epoch: 0 [1024/50048]	Loss: 4.9296
Training Epoch: 0 [1152/50048]	Loss: 4.8044
Training Epoch: 0 [1280/50048]	Loss: 4.7956
Training Epoch: 0 [1408/50048]	Loss: 4.5469
Training Epoch: 0 [1536/50048]	Loss: 4.6979
Training Epoch: 0 [1664/50048]	Loss: 4.9324
Training Epoch: 0 [1792/50048]	Loss: 4.6415
Training Epoch: 0 [1920/50048]	Loss: 4.5806
Training Epoch: 0 [2048/50048]	Loss: 4.5453
Training Epoch: 0 [2176/50048]	Loss: 4.6111
Training Epoch: 0 [2304/50048]	Loss: 4.6082
Training Epoch: 0 [2432/50048]	Loss: 4.6226
Training Epoch: 0 [2560/50048]	Loss: 4.5517
Training Epoch: 0 [2688/50048]	Loss: 4.5124
Training Epoch: 0 [2816/50048]	Loss: 4.8934
Training Epoch: 0 [2944/50048]	Loss: 4.6786
Training Epoch: 0 [3072/50048]	Loss: 4.6233
Training Epoch: 0 [3200/50048]	Loss: 4.7225
Training Epoch: 0 [3328/50048]	Loss: 4.6085
Training Epoch: 0 [3456/50048]	Loss: 4.5251
Training Epoch: 0 [3584/50048]	Loss: 4.5716
Training Epoch: 0 [3712/50048]	Loss: 4.7429
Training Epoch: 0 [3840/50048]	Loss: 4.6229
Training Epoch: 0 [3968/50048]	Loss: 4.6366
Training Epoch: 0 [4096/50048]	Loss: 4.6272
Training Epoch: 0 [4224/50048]	Loss: 4.5358
Training Epoch: 0 [4352/50048]	Loss: 4.5815
Training Epoch: 0 [4480/50048]	Loss: 4.5219
Training Epoch: 0 [4608/50048]	Loss: 4.5622
Training Epoch: 0 [4736/50048]	Loss: 4.4550
Training Epoch: 0 [4864/50048]	Loss: 4.5357
Training Epoch: 0 [4992/50048]	Loss: 4.4886
Training Epoch: 0 [5120/50048]	Loss: 4.5282
Training Epoch: 0 [5248/50048]	Loss: 4.4768
Training Epoch: 0 [5376/50048]	Loss: 4.5266
Training Epoch: 0 [5504/50048]	Loss: 4.5753
Training Epoch: 0 [5632/50048]	Loss: 4.5461
Training Epoch: 0 [5760/50048]	Loss: 4.4869
Training Epoch: 0 [5888/50048]	Loss: 4.4745
Training Epoch: 0 [6016/50048]	Loss: 4.5196
Training Epoch: 0 [6144/50048]	Loss: 4.4152
Training Epoch: 0 [6272/50048]	Loss: 4.5895
Training Epoch: 0 [6400/50048]	Loss: 4.4165
Profile done with power limit 125W
epoch 1 train time consumed: 12.72s
Validation Epoch: 0, Average loss: 0.0352, Accuracy: 0.0233
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 125000, 'lr': 0.0048, 'energy': 123.0974409774381, 'time': 9.683135105000474, 'accuracy': 0.02333860759493671, 'total_cost': 197.6981280850627}
[run job] Launching job with BS 128: and LR: 0.0048 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs128+lr0.00480+pl100', 'ZEUS_COST_THRESH': '339.2025535718517', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '0.0048', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0048', '--power_limit', '100']
[run job] cost_ub=339.2025535718517
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs128+lr0.00480+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.0048
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 5.6144
Training Epoch: 0 [384/50048]	Loss: 5.9802
Training Epoch: 0 [512/50048]	Loss: 5.3841
Training Epoch: 0 [640/50048]	Loss: 5.0933
Training Epoch: 0 [768/50048]	Loss: 5.1031
Training Epoch: 0 [896/50048]	Loss: 5.0483
Training Epoch: 0 [1024/50048]	Loss: 4.9221
Training Epoch: 0 [1152/50048]	Loss: 4.7828
Training Epoch: 0 [1280/50048]	Loss: 4.7534
Training Epoch: 0 [1408/50048]	Loss: 4.5455
Training Epoch: 0 [1536/50048]	Loss: 4.7706
Training Epoch: 0 [1664/50048]	Loss: 5.0231
Training Epoch: 0 [1792/50048]	Loss: 4.6187
Training Epoch: 0 [1920/50048]	Loss: 4.8486
Training Epoch: 0 [2048/50048]	Loss: 4.6008
Training Epoch: 0 [2176/50048]	Loss: 4.7352
Training Epoch: 0 [2304/50048]	Loss: 4.5830
Training Epoch: 0 [2432/50048]	Loss: 4.6153
Training Epoch: 0 [2560/50048]	Loss: 4.6338
Training Epoch: 0 [2688/50048]	Loss: 4.5657
Training Epoch: 0 [2816/50048]	Loss: 4.6413
Training Epoch: 0 [2944/50048]	Loss: 4.5859
Training Epoch: 0 [3072/50048]	Loss: 4.6482
Training Epoch: 0 [3200/50048]	Loss: 4.5892
Training Epoch: 0 [3328/50048]	Loss: 4.5651
Training Epoch: 0 [3456/50048]	Loss: 4.5127
Training Epoch: 0 [3584/50048]	Loss: 4.5538
Training Epoch: 0 [3712/50048]	Loss: 4.7854
Training Epoch: 0 [3840/50048]	Loss: 4.5774
Training Epoch: 0 [3968/50048]	Loss: 4.5347
Training Epoch: 0 [4096/50048]	Loss: 4.6505
Training Epoch: 0 [4224/50048]	Loss: 4.6078
Training Epoch: 0 [4352/50048]	Loss: 4.5424
Training Epoch: 0 [4480/50048]	Loss: 4.4877
Training Epoch: 0 [4608/50048]	Loss: 4.5399
Training Epoch: 0 [4736/50048]	Loss: 4.4813
Training Epoch: 0 [4864/50048]	Loss: 4.5432
Training Epoch: 0 [4992/50048]	Loss: 4.4858
Training Epoch: 0 [5120/50048]	Loss: 4.4598
Training Epoch: 0 [5248/50048]	Loss: 4.4658
Training Epoch: 0 [5376/50048]	Loss: 4.4954
Training Epoch: 0 [5504/50048]	Loss: 4.4742
Training Epoch: 0 [5632/50048]	Loss: 4.5117
Training Epoch: 0 [5760/50048]	Loss: 4.4520
Training Epoch: 0 [5888/50048]	Loss: 4.4336
Training Epoch: 0 [6016/50048]	Loss: 4.4218
Training Epoch: 0 [6144/50048]	Loss: 4.3249
Training Epoch: 0 [6272/50048]	Loss: 4.5444
Training Epoch: 0 [6400/50048]	Loss: 4.4089
Profile done with power limit 100W
epoch 1 train time consumed: 26.57s
Validation Epoch: 0, Average loss: 0.0377, Accuracy: 0.0266
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 100000, 'lr': 0.0048, 'energy': 99.05878700207634, 'time': 21.18148697200013, 'accuracy': 0.026602056962025316, 'total_cost': 348.809429630846}

[Power Profiler] with batch size 256 and learning rate 0.004525483399593905
[run job] Launching job with BS 256: and LR: 0.004525483399593905 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs256+lr0.00453+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '0.004525483399593905', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '0.004525483399593905', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs256+lr0.00453+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.004525483399593905
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 5.5787
Training Epoch: 0 [768/50176]	Loss: 5.3856
Training Epoch: 0 [1024/50176]	Loss: 5.1413
Training Epoch: 0 [1280/50176]	Loss: 4.9698
Training Epoch: 0 [1536/50176]	Loss: 4.9615
Training Epoch: 0 [1792/50176]	Loss: 4.7118
Training Epoch: 0 [2048/50176]	Loss: 4.6054
Training Epoch: 0 [2304/50176]	Loss: 4.7788
Training Epoch: 0 [2560/50176]	Loss: 4.6851
Training Epoch: 0 [2816/50176]	Loss: 4.6520
Training Epoch: 0 [3072/50176]	Loss: 4.6292
Training Epoch: 0 [3328/50176]	Loss: 4.6126
Training Epoch: 0 [3584/50176]	Loss: 4.5852
Training Epoch: 0 [3840/50176]	Loss: 4.7144
Training Epoch: 0 [4096/50176]	Loss: 4.6968
Training Epoch: 0 [4352/50176]	Loss: 4.6221
Training Epoch: 0 [4608/50176]	Loss: 4.5050
Training Epoch: 0 [4864/50176]	Loss: 4.5539
Training Epoch: 0 [5120/50176]	Loss: 4.5773
Training Epoch: 0 [5376/50176]	Loss: 4.5173
Training Epoch: 0 [5632/50176]	Loss: 4.6701
Training Epoch: 0 [5888/50176]	Loss: 4.4990
Training Epoch: 0 [6144/50176]	Loss: 4.4643
Training Epoch: 0 [6400/50176]	Loss: 4.5804
Training Epoch: 0 [6656/50176]	Loss: 4.5689
Training Epoch: 0 [6912/50176]	Loss: 4.4785
Training Epoch: 0 [7168/50176]	Loss: 4.4914
Training Epoch: 0 [7424/50176]	Loss: 4.4168
Training Epoch: 0 [7680/50176]	Loss: 4.4015
Training Epoch: 0 [7936/50176]	Loss: 4.3895
Training Epoch: 0 [8192/50176]	Loss: 4.3680
Training Epoch: 0 [8448/50176]	Loss: 4.3965
Training Epoch: 0 [8704/50176]	Loss: 4.3455
Training Epoch: 0 [8960/50176]	Loss: 4.3374
Training Epoch: 0 [9216/50176]	Loss: 4.3454
Training Epoch: 0 [9472/50176]	Loss: 4.3854
Training Epoch: 0 [9728/50176]	Loss: 4.3218
Training Epoch: 0 [9984/50176]	Loss: 4.2809
Training Epoch: 0 [10240/50176]	Loss: 4.3937
Training Epoch: 0 [10496/50176]	Loss: 4.2790
Training Epoch: 0 [10752/50176]	Loss: 4.2102
Training Epoch: 0 [11008/50176]	Loss: 4.1978
Training Epoch: 0 [11264/50176]	Loss: 4.2983
Training Epoch: 0 [11520/50176]	Loss: 4.2288
Training Epoch: 0 [11776/50176]	Loss: 4.2613
Training Epoch: 0 [12032/50176]	Loss: 4.2606
Training Epoch: 0 [12288/50176]	Loss: 4.1495
Training Epoch: 0 [12544/50176]	Loss: 4.1683
Training Epoch: 0 [12800/50176]	Loss: 4.1570
Profile done with power limit 175W
epoch 1 train time consumed: 12.70s
Validation Epoch: 0, Average loss: 0.0170, Accuracy: 0.0430
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 175000, 'lr': 0.004525483399593905, 'energy': 154.37487671563272, 'time': 9.516267467999569, 'accuracy': 0.04296875, 'total_cost': 232.60997581189937}
[run job] Launching job with BS 256: and LR: 0.004525483399593905 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs256+lr0.00453+pl150', 'ZEUS_COST_THRESH': '465.21995162379875', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '0.004525483399593905', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '0.004525483399593905', '--power_limit', '150']
[run job] cost_ub=465.21995162379875
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs256+lr0.00453+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.004525483399593905
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 5.5787
Training Epoch: 0 [768/50176]	Loss: 5.3849
Training Epoch: 0 [1024/50176]	Loss: 5.1388
Training Epoch: 0 [1280/50176]	Loss: 4.9672
Training Epoch: 0 [1536/50176]	Loss: 4.9468
Training Epoch: 0 [1792/50176]	Loss: 4.7149
Training Epoch: 0 [2048/50176]	Loss: 4.6179
Training Epoch: 0 [2304/50176]	Loss: 4.7885
Training Epoch: 0 [2560/50176]	Loss: 4.6885
Training Epoch: 0 [2816/50176]	Loss: 4.6631
Training Epoch: 0 [3072/50176]	Loss: 4.6384
Training Epoch: 0 [3328/50176]	Loss: 4.6149
Training Epoch: 0 [3584/50176]	Loss: 4.5398
Training Epoch: 0 [3840/50176]	Loss: 4.7338
Training Epoch: 0 [4096/50176]	Loss: 4.7020
Training Epoch: 0 [4352/50176]	Loss: 4.6161
Training Epoch: 0 [4608/50176]	Loss: 4.4897
Training Epoch: 0 [4864/50176]	Loss: 4.5259
Training Epoch: 0 [5120/50176]	Loss: 4.5328
Training Epoch: 0 [5376/50176]	Loss: 4.4948
Training Epoch: 0 [5632/50176]	Loss: 4.6436
Training Epoch: 0 [5888/50176]	Loss: 4.4484
Training Epoch: 0 [6144/50176]	Loss: 4.3684
Training Epoch: 0 [6400/50176]	Loss: 4.5349
Training Epoch: 0 [6656/50176]	Loss: 4.4994
Training Epoch: 0 [6912/50176]	Loss: 4.4006
Training Epoch: 0 [7168/50176]	Loss: 4.4912
Training Epoch: 0 [7424/50176]	Loss: 4.4313
Training Epoch: 0 [7680/50176]	Loss: 4.3347
Training Epoch: 0 [7936/50176]	Loss: 4.4035
Training Epoch: 0 [8192/50176]	Loss: 4.2903
Training Epoch: 0 [8448/50176]	Loss: 4.3796
Training Epoch: 0 [8704/50176]	Loss: 4.3276
Training Epoch: 0 [8960/50176]	Loss: 4.3350
Training Epoch: 0 [9216/50176]	Loss: 4.3161
Training Epoch: 0 [9472/50176]	Loss: 4.2794
Training Epoch: 0 [9728/50176]	Loss: 4.2815
Training Epoch: 0 [9984/50176]	Loss: 4.2570
Training Epoch: 0 [10240/50176]	Loss: 4.3389
Training Epoch: 0 [10496/50176]	Loss: 4.2595
Training Epoch: 0 [10752/50176]	Loss: 4.1977
Training Epoch: 0 [11008/50176]	Loss: 4.1784
Training Epoch: 0 [11264/50176]	Loss: 4.2891
Training Epoch: 0 [11520/50176]	Loss: 4.1745
Training Epoch: 0 [11776/50176]	Loss: 4.2218
Training Epoch: 0 [12032/50176]	Loss: 4.2611
Training Epoch: 0 [12288/50176]	Loss: 4.1975
Training Epoch: 0 [12544/50176]	Loss: 4.1429
Training Epoch: 0 [12800/50176]	Loss: 4.1288
Profile done with power limit 150W
epoch 1 train time consumed: 13.00s
Validation Epoch: 0, Average loss: 0.0176, Accuracy: 0.0431
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 150000, 'lr': 0.004525483399593905, 'energy': 147.45303682476367, 'time': 9.749285360000613, 'accuracy': 0.04306640625, 'total_cost': 232.76869239323668}
[run job] Launching job with BS 256: and LR: 0.004525483399593905 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs256+lr0.00453+pl125', 'ZEUS_COST_THRESH': '465.21995162379875', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '0.004525483399593905', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '0.004525483399593905', '--power_limit', '125']
[run job] cost_ub=465.21995162379875
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs256+lr0.00453+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.004525483399593905
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 5.5787
Training Epoch: 0 [768/50176]	Loss: 5.3850
Training Epoch: 0 [1024/50176]	Loss: 5.1326
Training Epoch: 0 [1280/50176]	Loss: 4.9817
Training Epoch: 0 [1536/50176]	Loss: 4.9569
Training Epoch: 0 [1792/50176]	Loss: 4.7229
Training Epoch: 0 [2048/50176]	Loss: 4.6389
Training Epoch: 0 [2304/50176]	Loss: 4.7930
Training Epoch: 0 [2560/50176]	Loss: 4.6951
Training Epoch: 0 [2816/50176]	Loss: 4.6977
Training Epoch: 0 [3072/50176]	Loss: 4.6054
Training Epoch: 0 [3328/50176]	Loss: 4.6063
Training Epoch: 0 [3584/50176]	Loss: 4.5578
Training Epoch: 0 [3840/50176]	Loss: 4.7161
Training Epoch: 0 [4096/50176]	Loss: 4.7066
Training Epoch: 0 [4352/50176]	Loss: 4.6088
Training Epoch: 0 [4608/50176]	Loss: 4.5023
Training Epoch: 0 [4864/50176]	Loss: 4.5483
Training Epoch: 0 [5120/50176]	Loss: 4.5259
Training Epoch: 0 [5376/50176]	Loss: 4.4931
Training Epoch: 0 [5632/50176]	Loss: 4.6773
Training Epoch: 0 [5888/50176]	Loss: 4.4676
Training Epoch: 0 [6144/50176]	Loss: 4.3764
Training Epoch: 0 [6400/50176]	Loss: 4.5015
Training Epoch: 0 [6656/50176]	Loss: 4.5037
Training Epoch: 0 [6912/50176]	Loss: 4.4364
Training Epoch: 0 [7168/50176]	Loss: 4.4871
Training Epoch: 0 [7424/50176]	Loss: 4.4149
Training Epoch: 0 [7680/50176]	Loss: 4.3501
Training Epoch: 0 [7936/50176]	Loss: 4.4901
Training Epoch: 0 [8192/50176]	Loss: 4.3213
Training Epoch: 0 [8448/50176]	Loss: 4.3950
Training Epoch: 0 [8704/50176]	Loss: 4.3341
Training Epoch: 0 [8960/50176]	Loss: 4.3296
Training Epoch: 0 [9216/50176]	Loss: 4.2903
Training Epoch: 0 [9472/50176]	Loss: 4.3175
Training Epoch: 0 [9728/50176]	Loss: 4.3033
Training Epoch: 0 [9984/50176]	Loss: 4.2710
Training Epoch: 0 [10240/50176]	Loss: 4.3473
Training Epoch: 0 [10496/50176]	Loss: 4.2121
Training Epoch: 0 [10752/50176]	Loss: 4.1905
Training Epoch: 0 [11008/50176]	Loss: 4.1630
Training Epoch: 0 [11264/50176]	Loss: 4.2556
Training Epoch: 0 [11520/50176]	Loss: 4.2045
Training Epoch: 0 [11776/50176]	Loss: 4.2796
Training Epoch: 0 [12032/50176]	Loss: 4.2693
Training Epoch: 0 [12288/50176]	Loss: 4.1690
Training Epoch: 0 [12544/50176]	Loss: 4.1708
Training Epoch: 0 [12800/50176]	Loss: 4.1406
Profile done with power limit 125W
epoch 1 train time consumed: 14.62s
Validation Epoch: 0, Average loss: 0.0199, Accuracy: 0.0389
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 125000, 'lr': 0.004525483399593905, 'energy': 123.09533211501682, 'time': 11.124153409000428, 'accuracy': 0.0388671875, 'total_cost': 272.0589235938369}
[run job] Launching job with BS 256: and LR: 0.004525483399593905 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs256+lr0.00453+pl100', 'ZEUS_COST_THRESH': '465.21995162379875', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '0.004525483399593905', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '0.004525483399593905', '--power_limit', '100']
[run job] cost_ub=465.21995162379875
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs256+lr0.00453+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.004525483399593905
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 5.5787
Training Epoch: 0 [768/50176]	Loss: 5.3852
Training Epoch: 0 [1024/50176]	Loss: 5.1411
Training Epoch: 0 [1280/50176]	Loss: 4.9822
Training Epoch: 0 [1536/50176]	Loss: 4.9765
Training Epoch: 0 [1792/50176]	Loss: 4.7111
Training Epoch: 0 [2048/50176]	Loss: 4.6293
Training Epoch: 0 [2304/50176]	Loss: 4.7865
Training Epoch: 0 [2560/50176]	Loss: 4.6880
Training Epoch: 0 [2816/50176]	Loss: 4.6866
Training Epoch: 0 [3072/50176]	Loss: 4.6387
Training Epoch: 0 [3328/50176]	Loss: 4.6218
Training Epoch: 0 [3584/50176]	Loss: 4.5857
Training Epoch: 0 [3840/50176]	Loss: 4.6860
Training Epoch: 0 [4096/50176]	Loss: 4.7109
Training Epoch: 0 [4352/50176]	Loss: 4.6457
Training Epoch: 0 [4608/50176]	Loss: 4.5184
Training Epoch: 0 [4864/50176]	Loss: 4.5796
Training Epoch: 0 [5120/50176]	Loss: 4.5576
Training Epoch: 0 [5376/50176]	Loss: 4.5096
Training Epoch: 0 [5632/50176]	Loss: 4.6413
Training Epoch: 0 [5888/50176]	Loss: 4.4749
Training Epoch: 0 [6144/50176]	Loss: 4.4772
Training Epoch: 0 [6400/50176]	Loss: 4.5534
Training Epoch: 0 [6656/50176]	Loss: 4.5645
Training Epoch: 0 [6912/50176]	Loss: 4.4556
Training Epoch: 0 [7168/50176]	Loss: 4.4854
Training Epoch: 0 [7424/50176]	Loss: 4.4413
Training Epoch: 0 [7680/50176]	Loss: 4.3921
Training Epoch: 0 [7936/50176]	Loss: 4.3732
Training Epoch: 0 [8192/50176]	Loss: 4.3437
Training Epoch: 0 [8448/50176]	Loss: 4.4069
Training Epoch: 0 [8704/50176]	Loss: 4.3383
Training Epoch: 0 [8960/50176]	Loss: 4.3361
Training Epoch: 0 [9216/50176]	Loss: 4.3422
Training Epoch: 0 [9472/50176]	Loss: 4.3206
Training Epoch: 0 [9728/50176]	Loss: 4.3322
Training Epoch: 0 [9984/50176]	Loss: 4.2712
Training Epoch: 0 [10240/50176]	Loss: 4.3735
Training Epoch: 0 [10496/50176]	Loss: 4.2681
Training Epoch: 0 [10752/50176]	Loss: 4.1774
Training Epoch: 0 [11008/50176]	Loss: 4.1480
Training Epoch: 0 [11264/50176]	Loss: 4.3074
Training Epoch: 0 [11520/50176]	Loss: 4.1806
Training Epoch: 0 [11776/50176]	Loss: 4.2377
Training Epoch: 0 [12032/50176]	Loss: 4.2333
Training Epoch: 0 [12288/50176]	Loss: 4.1304
Training Epoch: 0 [12544/50176]	Loss: 4.1304
Training Epoch: 0 [12800/50176]	Loss: 4.1266
Profile done with power limit 100W
epoch 1 train time consumed: 36.89s
Validation Epoch: 0, Average loss: 0.0170, Accuracy: 0.0474
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 100000, 'lr': 0.004525483399593905, 'energy': 103.46993867140552, 'time': 28.75322704700011, 'accuracy': 0.04736328125, 'total_cost': 539.0723751657055}

[Power Profiler] with batch size 256 and learning rate 0.005656854249492381
[run job] Launching job with BS 256: and LR: 0.005656854249492381 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs256+lr0.00566+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '0.005656854249492381', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '0.005656854249492381', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs256+lr0.00566+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.005656854249492381
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 5.9997
Training Epoch: 0 [768/50176]	Loss: 5.7264
Training Epoch: 0 [1024/50176]	Loss: 5.1647
Training Epoch: 0 [1280/50176]	Loss: 4.8545
Training Epoch: 0 [1536/50176]	Loss: 4.7933
Training Epoch: 0 [1792/50176]	Loss: 4.6431
Training Epoch: 0 [2048/50176]	Loss: 4.6547
Training Epoch: 0 [2304/50176]	Loss: 4.7654
Training Epoch: 0 [2560/50176]	Loss: 4.6374
Training Epoch: 0 [2816/50176]	Loss: 4.6370
Training Epoch: 0 [3072/50176]	Loss: 4.6453
Training Epoch: 0 [3328/50176]	Loss: 4.5709
Training Epoch: 0 [3584/50176]	Loss: 4.5928
Training Epoch: 0 [3840/50176]	Loss: 4.7323
Training Epoch: 0 [4096/50176]	Loss: 4.6391
Training Epoch: 0 [4352/50176]	Loss: 4.5985
Training Epoch: 0 [4608/50176]	Loss: 4.6131
Training Epoch: 0 [4864/50176]	Loss: 4.5494
Training Epoch: 0 [5120/50176]	Loss: 4.5746
Training Epoch: 0 [5376/50176]	Loss: 4.5680
Training Epoch: 0 [5632/50176]	Loss: 4.7675
Training Epoch: 0 [5888/50176]	Loss: 4.6034
Training Epoch: 0 [6144/50176]	Loss: 4.5291
Training Epoch: 0 [6400/50176]	Loss: 4.7023
Training Epoch: 0 [6656/50176]	Loss: 4.5930
Training Epoch: 0 [6912/50176]	Loss: 4.4788
Training Epoch: 0 [7168/50176]	Loss: 4.5983
Training Epoch: 0 [7424/50176]	Loss: 4.5039
Training Epoch: 0 [7680/50176]	Loss: 4.4828
Training Epoch: 0 [7936/50176]	Loss: 4.5049
Training Epoch: 0 [8192/50176]	Loss: 4.4449
Training Epoch: 0 [8448/50176]	Loss: 4.4735
Training Epoch: 0 [8704/50176]	Loss: 4.4396
Training Epoch: 0 [8960/50176]	Loss: 4.4159
Training Epoch: 0 [9216/50176]	Loss: 4.4102
Training Epoch: 0 [9472/50176]	Loss: 4.4295
Training Epoch: 0 [9728/50176]	Loss: 4.3978
Training Epoch: 0 [9984/50176]	Loss: 4.4378
Training Epoch: 0 [10240/50176]	Loss: 4.3926
Training Epoch: 0 [10496/50176]	Loss: 4.3699
Training Epoch: 0 [10752/50176]	Loss: 4.3895
Training Epoch: 0 [11008/50176]	Loss: 4.3495
Training Epoch: 0 [11264/50176]	Loss: 4.4527
Training Epoch: 0 [11520/50176]	Loss: 4.3274
Training Epoch: 0 [11776/50176]	Loss: 4.3888
Training Epoch: 0 [12032/50176]	Loss: 4.3273
Training Epoch: 0 [12288/50176]	Loss: 4.3646
Training Epoch: 0 [12544/50176]	Loss: 4.2701
Training Epoch: 0 [12800/50176]	Loss: 4.3154
Profile done with power limit 175W
epoch 1 train time consumed: 12.72s
Validation Epoch: 0, Average loss: 0.0189, Accuracy: 0.0311
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 175000, 'lr': 0.005656854249492381, 'energy': 154.17320633387823, 'time': 9.520845784999437, 'accuracy': 0.0310546875, 'total_cost': 321.8079665315009}
[run job] Launching job with BS 256: and LR: 0.005656854249492381 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs256+lr0.00566+pl150', 'ZEUS_COST_THRESH': '643.6159330630018', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '0.005656854249492381', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '0.005656854249492381', '--power_limit', '150']
[run job] cost_ub=643.6159330630018
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs256+lr0.00566+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.005656854249492381
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 5.9997
Training Epoch: 0 [768/50176]	Loss: 5.7271
Training Epoch: 0 [1024/50176]	Loss: 5.1783
Training Epoch: 0 [1280/50176]	Loss: 4.8533
Training Epoch: 0 [1536/50176]	Loss: 4.7866
Training Epoch: 0 [1792/50176]	Loss: 4.6450
Training Epoch: 0 [2048/50176]	Loss: 4.6450
Training Epoch: 0 [2304/50176]	Loss: 4.7713
Training Epoch: 0 [2560/50176]	Loss: 4.6564
Training Epoch: 0 [2816/50176]	Loss: 4.6321
Training Epoch: 0 [3072/50176]	Loss: 4.6299
Training Epoch: 0 [3328/50176]	Loss: 4.5770
Training Epoch: 0 [3584/50176]	Loss: 4.5797
Training Epoch: 0 [3840/50176]	Loss: 4.6577
Training Epoch: 0 [4096/50176]	Loss: 4.6446
Training Epoch: 0 [4352/50176]	Loss: 4.5810
Training Epoch: 0 [4608/50176]	Loss: 4.5948
Training Epoch: 0 [4864/50176]	Loss: 4.5281
Training Epoch: 0 [5120/50176]	Loss: 4.5554
Training Epoch: 0 [5376/50176]	Loss: 4.5433
Training Epoch: 0 [5632/50176]	Loss: 4.7334
Training Epoch: 0 [5888/50176]	Loss: 4.6139
Training Epoch: 0 [6144/50176]	Loss: 4.4966
Training Epoch: 0 [6400/50176]	Loss: 4.6301
Training Epoch: 0 [6656/50176]	Loss: 4.5663
Training Epoch: 0 [6912/50176]	Loss: 4.4853
Training Epoch: 0 [7168/50176]	Loss: 4.5230
Training Epoch: 0 [7424/50176]	Loss: 4.5224
Training Epoch: 0 [7680/50176]	Loss: 4.5271
Training Epoch: 0 [7936/50176]	Loss: 4.4796
Training Epoch: 0 [8192/50176]	Loss: 4.4647
Training Epoch: 0 [8448/50176]	Loss: 4.4528
Training Epoch: 0 [8704/50176]	Loss: 4.4112
Training Epoch: 0 [8960/50176]	Loss: 4.4263
Training Epoch: 0 [9216/50176]	Loss: 4.4046
Training Epoch: 0 [9472/50176]	Loss: 4.4094
Training Epoch: 0 [9728/50176]	Loss: 4.3726
Training Epoch: 0 [9984/50176]	Loss: 4.4008
Training Epoch: 0 [10240/50176]	Loss: 4.4470
Training Epoch: 0 [10496/50176]	Loss: 4.3489
Training Epoch: 0 [10752/50176]	Loss: 4.3354
Training Epoch: 0 [11008/50176]	Loss: 4.3219
Training Epoch: 0 [11264/50176]	Loss: 4.3679
Training Epoch: 0 [11520/50176]	Loss: 4.2877
Training Epoch: 0 [11776/50176]	Loss: 4.3651
Training Epoch: 0 [12032/50176]	Loss: 4.2880
Training Epoch: 0 [12288/50176]	Loss: 4.3823
Training Epoch: 0 [12544/50176]	Loss: 4.2756
Training Epoch: 0 [12800/50176]	Loss: 4.3205
Profile done with power limit 150W
epoch 1 train time consumed: 12.94s
Validation Epoch: 0, Average loss: 0.0202, Accuracy: 0.0271
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 150000, 'lr': 0.005656854249492381, 'energy': 147.12315597290782, 'time': 9.7458393349998, 'accuracy': 0.0271484375, 'total_cost': 368.740041015421}
[run job] Launching job with BS 256: and LR: 0.005656854249492381 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs256+lr0.00566+pl125', 'ZEUS_COST_THRESH': '643.6159330630018', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '0.005656854249492381', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '0.005656854249492381', '--power_limit', '125']
[run job] cost_ub=643.6159330630018
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs256+lr0.00566+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.005656854249492381
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 5.9997
Training Epoch: 0 [768/50176]	Loss: 5.7260
Training Epoch: 0 [1024/50176]	Loss: 5.1612
Training Epoch: 0 [1280/50176]	Loss: 4.8536
Training Epoch: 0 [1536/50176]	Loss: 4.7880
Training Epoch: 0 [1792/50176]	Loss: 4.6464
Training Epoch: 0 [2048/50176]	Loss: 4.6476
Training Epoch: 0 [2304/50176]	Loss: 4.7666
Training Epoch: 0 [2560/50176]	Loss: 4.6436
Training Epoch: 0 [2816/50176]	Loss: 4.6370
Training Epoch: 0 [3072/50176]	Loss: 4.6278
Training Epoch: 0 [3328/50176]	Loss: 4.5739
Training Epoch: 0 [3584/50176]	Loss: 4.5942
Training Epoch: 0 [3840/50176]	Loss: 4.7199
Training Epoch: 0 [4096/50176]	Loss: 4.6230
Training Epoch: 0 [4352/50176]	Loss: 4.6355
Training Epoch: 0 [4608/50176]	Loss: 4.5687
Training Epoch: 0 [4864/50176]	Loss: 4.5317
Training Epoch: 0 [5120/50176]	Loss: 4.5571
Training Epoch: 0 [5376/50176]	Loss: 4.5522
Training Epoch: 0 [5632/50176]	Loss: 4.6481
Training Epoch: 0 [5888/50176]	Loss: 4.5139
Training Epoch: 0 [6144/50176]	Loss: 4.5220
Training Epoch: 0 [6400/50176]	Loss: 4.7274
Training Epoch: 0 [6656/50176]	Loss: 4.5678
Training Epoch: 0 [6912/50176]	Loss: 4.4736
Training Epoch: 0 [7168/50176]	Loss: 4.5142
Training Epoch: 0 [7424/50176]	Loss: 4.5072
Training Epoch: 0 [7680/50176]	Loss: 4.4826
Training Epoch: 0 [7936/50176]	Loss: 4.4953
Training Epoch: 0 [8192/50176]	Loss: 4.4546
Training Epoch: 0 [8448/50176]	Loss: 4.4659
Training Epoch: 0 [8704/50176]	Loss: 4.4344
Training Epoch: 0 [8960/50176]	Loss: 4.4305
Training Epoch: 0 [9216/50176]	Loss: 4.4137
Training Epoch: 0 [9472/50176]	Loss: 4.4745
Training Epoch: 0 [9728/50176]	Loss: 4.3819
Training Epoch: 0 [9984/50176]	Loss: 4.4132
Training Epoch: 0 [10240/50176]	Loss: 4.4142
Training Epoch: 0 [10496/50176]	Loss: 4.3546
Training Epoch: 0 [10752/50176]	Loss: 4.3814
Training Epoch: 0 [11008/50176]	Loss: 4.3230
Training Epoch: 0 [11264/50176]	Loss: 4.4418
Training Epoch: 0 [11520/50176]	Loss: 4.3414
Training Epoch: 0 [11776/50176]	Loss: 4.4377
Training Epoch: 0 [12032/50176]	Loss: 4.3570
Training Epoch: 0 [12288/50176]	Loss: 4.3726
Training Epoch: 0 [12544/50176]	Loss: 4.2982
Training Epoch: 0 [12800/50176]	Loss: 4.3394
Profile done with power limit 125W
epoch 1 train time consumed: 14.63s
Validation Epoch: 0, Average loss: 0.0209, Accuracy: 0.0234
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 125000, 'lr': 0.005656854249492381, 'energy': 122.97429905406376, 'time': 11.11174159300026, 'accuracy': 0.0234375, 'total_cost': 450.4780152985222}
[run job] Launching job with BS 256: and LR: 0.005656854249492381 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs256+lr0.00566+pl100', 'ZEUS_COST_THRESH': '643.6159330630018', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '0.005656854249492381', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '0.005656854249492381', '--power_limit', '100']
[run job] cost_ub=643.6159330630018
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs256+lr0.00566+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.005656854249492381
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 5.9997
Training Epoch: 0 [768/50176]	Loss: 5.7264
Training Epoch: 0 [1024/50176]	Loss: 5.1777
Training Epoch: 0 [1280/50176]	Loss: 4.8550
Training Epoch: 0 [1536/50176]	Loss: 4.7880
Training Epoch: 0 [1792/50176]	Loss: 4.6421
Training Epoch: 0 [2048/50176]	Loss: 4.6420
Training Epoch: 0 [2304/50176]	Loss: 4.7652
Training Epoch: 0 [2560/50176]	Loss: 4.6459
Training Epoch: 0 [2816/50176]	Loss: 4.6345
Training Epoch: 0 [3072/50176]	Loss: 4.6448
Training Epoch: 0 [3328/50176]	Loss: 4.5676
Training Epoch: 0 [3584/50176]	Loss: 4.6016
Training Epoch: 0 [3840/50176]	Loss: 4.6877
Training Epoch: 0 [4096/50176]	Loss: 4.6398
Training Epoch: 0 [4352/50176]	Loss: 4.6232
Training Epoch: 0 [4608/50176]	Loss: 4.6066
Training Epoch: 0 [4864/50176]	Loss: 4.5167
Training Epoch: 0 [5120/50176]	Loss: 4.5585
Training Epoch: 0 [5376/50176]	Loss: 4.5240
Training Epoch: 0 [5632/50176]	Loss: 4.6352
Training Epoch: 0 [5888/50176]	Loss: 4.7594
Training Epoch: 0 [6144/50176]	Loss: 4.5113
Training Epoch: 0 [6400/50176]	Loss: 4.6595
Training Epoch: 0 [6656/50176]	Loss: 4.6049
Training Epoch: 0 [6912/50176]	Loss: 4.4754
Training Epoch: 0 [7168/50176]	Loss: 4.5135
Training Epoch: 0 [7424/50176]	Loss: 4.5003
Training Epoch: 0 [7680/50176]	Loss: 4.4863
Training Epoch: 0 [7936/50176]	Loss: 4.4658
Training Epoch: 0 [8192/50176]	Loss: 4.4675
Training Epoch: 0 [8448/50176]	Loss: 4.4677
Training Epoch: 0 [8704/50176]	Loss: 4.4385
Training Epoch: 0 [8960/50176]	Loss: 4.4276
Training Epoch: 0 [9216/50176]	Loss: 4.3780
Training Epoch: 0 [9472/50176]	Loss: 4.4033
Training Epoch: 0 [9728/50176]	Loss: 4.3596
Training Epoch: 0 [9984/50176]	Loss: 4.3903
Training Epoch: 0 [10240/50176]	Loss: 4.4269
Training Epoch: 0 [10496/50176]	Loss: 4.3315
Training Epoch: 0 [10752/50176]	Loss: 4.3533
Training Epoch: 0 [11008/50176]	Loss: 4.2981
Training Epoch: 0 [11264/50176]	Loss: 4.3486
Training Epoch: 0 [11520/50176]	Loss: 4.3133
Training Epoch: 0 [11776/50176]	Loss: 4.3864
Training Epoch: 0 [12032/50176]	Loss: 4.3498
Training Epoch: 0 [12288/50176]	Loss: 4.3205
Training Epoch: 0 [12544/50176]	Loss: 4.2503
Training Epoch: 0 [12800/50176]	Loss: 4.2802
Profile done with power limit 100W
epoch 1 train time consumed: 37.28s
Validation Epoch: 0, Average loss: 0.0173, Accuracy: 0.0312
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 100000, 'lr': 0.005656854249492381, 'energy': 102.86839930869162, 'time': 29.11806512699968, 'accuracy': 0.03115234375, 'total_cost': 828.1993625800663}

[Power Profiler] with batch size 256 and learning rate 0.006788225099390857
[run job] Launching job with BS 256: and LR: 0.006788225099390857 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs256+lr0.00679+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '0.006788225099390857', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '0.006788225099390857', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs256+lr0.00679+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.006788225099390857
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 6.4519
Training Epoch: 0 [768/50176]	Loss: 6.0637
Training Epoch: 0 [1024/50176]	Loss: 5.2338
Training Epoch: 0 [1280/50176]	Loss: 4.8748
Training Epoch: 0 [1536/50176]	Loss: 4.8120
Training Epoch: 0 [1792/50176]	Loss: 4.6226
Training Epoch: 0 [2048/50176]	Loss: 4.6206
Training Epoch: 0 [2304/50176]	Loss: 4.7230
Training Epoch: 0 [2560/50176]	Loss: 4.6376
Training Epoch: 0 [2816/50176]	Loss: 4.5885
Training Epoch: 0 [3072/50176]	Loss: 4.6931
Training Epoch: 0 [3328/50176]	Loss: 4.5460
Training Epoch: 0 [3584/50176]	Loss: 4.6481
Training Epoch: 0 [3840/50176]	Loss: 4.8462
Training Epoch: 0 [4096/50176]	Loss: 4.6060
Training Epoch: 0 [4352/50176]	Loss: 4.5737
Training Epoch: 0 [4608/50176]	Loss: 4.5870
Training Epoch: 0 [4864/50176]	Loss: 4.5223
Training Epoch: 0 [5120/50176]	Loss: 4.5173
Training Epoch: 0 [5376/50176]	Loss: 4.5261
Training Epoch: 0 [5632/50176]	Loss: 4.5676
Training Epoch: 0 [5888/50176]	Loss: 4.5134
Training Epoch: 0 [6144/50176]	Loss: 4.5019
Training Epoch: 0 [6400/50176]	Loss: 4.8026
Training Epoch: 0 [6656/50176]	Loss: 4.5579
Training Epoch: 0 [6912/50176]	Loss: 4.4442
Training Epoch: 0 [7168/50176]	Loss: 4.4909
Training Epoch: 0 [7424/50176]	Loss: 4.5122
Training Epoch: 0 [7680/50176]	Loss: 4.4624
Training Epoch: 0 [7936/50176]	Loss: 4.5487
Training Epoch: 0 [8192/50176]	Loss: 4.4372
Training Epoch: 0 [8448/50176]	Loss: 4.4804
Training Epoch: 0 [8704/50176]	Loss: 4.4620
Training Epoch: 0 [8960/50176]	Loss: 4.3838
Training Epoch: 0 [9216/50176]	Loss: 4.3790
Training Epoch: 0 [9472/50176]	Loss: 4.4030
Training Epoch: 0 [9728/50176]	Loss: 4.3687
Training Epoch: 0 [9984/50176]	Loss: 4.3924
Training Epoch: 0 [10240/50176]	Loss: 4.4005
Training Epoch: 0 [10496/50176]	Loss: 4.3366
Training Epoch: 0 [10752/50176]	Loss: 4.3252
Training Epoch: 0 [11008/50176]	Loss: 4.2963
Training Epoch: 0 [11264/50176]	Loss: 4.4368
Training Epoch: 0 [11520/50176]	Loss: 4.2986
Training Epoch: 0 [11776/50176]	Loss: 4.3552
Training Epoch: 0 [12032/50176]	Loss: 4.4147
Training Epoch: 0 [12288/50176]	Loss: 4.3365
Training Epoch: 0 [12544/50176]	Loss: 4.2641
Training Epoch: 0 [12800/50176]	Loss: 4.2941
Profile done with power limit 175W
epoch 1 train time consumed: 12.68s
Validation Epoch: 0, Average loss: 0.0207, Accuracy: 0.0321
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 175000, 'lr': 0.006788225099390857, 'energy': 154.15570083165568, 'time': 9.50454029499997, 'accuracy': 0.03212890625, 'total_cost': 310.4992118983613}
[run job] Launching job with BS 256: and LR: 0.006788225099390857 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs256+lr0.00679+pl150', 'ZEUS_COST_THRESH': '620.9984237967226', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '0.006788225099390857', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '0.006788225099390857', '--power_limit', '150']
[run job] cost_ub=620.9984237967226
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs256+lr0.00679+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.006788225099390857
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 6.4519
Training Epoch: 0 [768/50176]	Loss: 6.0629
Training Epoch: 0 [1024/50176]	Loss: 5.2286
Training Epoch: 0 [1280/50176]	Loss: 4.8698
Training Epoch: 0 [1536/50176]	Loss: 4.8170
Training Epoch: 0 [1792/50176]	Loss: 4.6246
Training Epoch: 0 [2048/50176]	Loss: 4.6132
Training Epoch: 0 [2304/50176]	Loss: 4.7257
Training Epoch: 0 [2560/50176]	Loss: 4.6351
Training Epoch: 0 [2816/50176]	Loss: 4.5888
Training Epoch: 0 [3072/50176]	Loss: 4.6969
Training Epoch: 0 [3328/50176]	Loss: 4.5474
Training Epoch: 0 [3584/50176]	Loss: 4.6390
Training Epoch: 0 [3840/50176]	Loss: 4.7472
Training Epoch: 0 [4096/50176]	Loss: 4.6023
Training Epoch: 0 [4352/50176]	Loss: 4.5882
Training Epoch: 0 [4608/50176]	Loss: 4.6064
Training Epoch: 0 [4864/50176]	Loss: 4.5299
Training Epoch: 0 [5120/50176]	Loss: 4.4940
Training Epoch: 0 [5376/50176]	Loss: 4.5083
Training Epoch: 0 [5632/50176]	Loss: 4.5848
Training Epoch: 0 [5888/50176]	Loss: 4.5111
Training Epoch: 0 [6144/50176]	Loss: 4.5443
Training Epoch: 0 [6400/50176]	Loss: 4.6778
Training Epoch: 0 [6656/50176]	Loss: 4.6987
Training Epoch: 0 [6912/50176]	Loss: 4.8929
Training Epoch: 0 [7168/50176]	Loss: 4.5562
Training Epoch: 0 [7424/50176]	Loss: 4.6976
Training Epoch: 0 [7680/50176]	Loss: 4.6231
Training Epoch: 0 [7936/50176]	Loss: 4.4986
Training Epoch: 0 [8192/50176]	Loss: 4.5009
Training Epoch: 0 [8448/50176]	Loss: 4.5316
Training Epoch: 0 [8704/50176]	Loss: 4.4612
Training Epoch: 0 [8960/50176]	Loss: 4.5276
Training Epoch: 0 [9216/50176]	Loss: 4.4451
Training Epoch: 0 [9472/50176]	Loss: 4.4995
Training Epoch: 0 [9728/50176]	Loss: 4.3991
Training Epoch: 0 [9984/50176]	Loss: 4.4532
Training Epoch: 0 [10240/50176]	Loss: 4.4880
Training Epoch: 0 [10496/50176]	Loss: 4.3659
Training Epoch: 0 [10752/50176]	Loss: 4.3875
Training Epoch: 0 [11008/50176]	Loss: 4.3478
Training Epoch: 0 [11264/50176]	Loss: 4.4238
Training Epoch: 0 [11520/50176]	Loss: 4.3787
Training Epoch: 0 [11776/50176]	Loss: 4.4250
Training Epoch: 0 [12032/50176]	Loss: 4.3671
Training Epoch: 0 [12288/50176]	Loss: 4.3496
Training Epoch: 0 [12544/50176]	Loss: 4.3559
Training Epoch: 0 [12800/50176]	Loss: 4.3747
Profile done with power limit 150W
epoch 1 train time consumed: 13.02s
Validation Epoch: 0, Average loss: 0.0494, Accuracy: 0.0193
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 150000, 'lr': 0.006788225099390857, 'energy': 147.03873889108394, 'time': 9.774259951000204, 'accuracy': 0.0193359375, 'total_cost': 519.0996245250437}
[run job] Launching job with BS 256: and LR: 0.006788225099390857 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs256+lr0.00679+pl125', 'ZEUS_COST_THRESH': '620.9984237967226', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '0.006788225099390857', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '0.006788225099390857', '--power_limit', '125']
[run job] cost_ub=620.9984237967226
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs256+lr0.00679+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.006788225099390857
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 6.4519
Training Epoch: 0 [768/50176]	Loss: 6.0627
Training Epoch: 0 [1024/50176]	Loss: 5.2459
Training Epoch: 0 [1280/50176]	Loss: 4.8693
Training Epoch: 0 [1536/50176]	Loss: 4.8112
Training Epoch: 0 [1792/50176]	Loss: 4.6232
Training Epoch: 0 [2048/50176]	Loss: 4.6079
Training Epoch: 0 [2304/50176]	Loss: 4.7264
Training Epoch: 0 [2560/50176]	Loss: 4.6406
Training Epoch: 0 [2816/50176]	Loss: 4.5853
Training Epoch: 0 [3072/50176]	Loss: 4.7319
Training Epoch: 0 [3328/50176]	Loss: 4.5531
Training Epoch: 0 [3584/50176]	Loss: 4.6477
Training Epoch: 0 [3840/50176]	Loss: 4.6986
Training Epoch: 0 [4096/50176]	Loss: 4.6043
Training Epoch: 0 [4352/50176]	Loss: 4.5863
Training Epoch: 0 [4608/50176]	Loss: 4.6610
Training Epoch: 0 [4864/50176]	Loss: 4.5182
Training Epoch: 0 [5120/50176]	Loss: 4.5173
Training Epoch: 0 [5376/50176]	Loss: 4.5245
Training Epoch: 0 [5632/50176]	Loss: 4.5786
Training Epoch: 0 [5888/50176]	Loss: 4.5195
Training Epoch: 0 [6144/50176]	Loss: 4.5426
Training Epoch: 0 [6400/50176]	Loss: 4.5912
Training Epoch: 0 [6656/50176]	Loss: 4.7903
Training Epoch: 0 [6912/50176]	Loss: 4.8527
Training Epoch: 0 [7168/50176]	Loss: 4.6566
Training Epoch: 0 [7424/50176]	Loss: 4.6004
Training Epoch: 0 [7680/50176]	Loss: 4.6832
Training Epoch: 0 [7936/50176]	Loss: 4.5570
Training Epoch: 0 [8192/50176]	Loss: 4.5621
Training Epoch: 0 [8448/50176]	Loss: 4.4826
Training Epoch: 0 [8704/50176]	Loss: 4.5866
Training Epoch: 0 [8960/50176]	Loss: 4.4247
Training Epoch: 0 [9216/50176]	Loss: 4.4375
Training Epoch: 0 [9472/50176]	Loss: 4.4998
Training Epoch: 0 [9728/50176]	Loss: 4.4137
Training Epoch: 0 [9984/50176]	Loss: 4.4284
Training Epoch: 0 [10240/50176]	Loss: 4.4151
Training Epoch: 0 [10496/50176]	Loss: 4.3703
Training Epoch: 0 [10752/50176]	Loss: 4.3808
Training Epoch: 0 [11008/50176]	Loss: 4.3234
Training Epoch: 0 [11264/50176]	Loss: 4.4289
Training Epoch: 0 [11520/50176]	Loss: 4.3639
Training Epoch: 0 [11776/50176]	Loss: 4.4216
Training Epoch: 0 [12032/50176]	Loss: 4.4236
Training Epoch: 0 [12288/50176]	Loss: 4.4275
Training Epoch: 0 [12544/50176]	Loss: 4.3241
Training Epoch: 0 [12800/50176]	Loss: 4.3707
Profile done with power limit 125W
epoch 1 train time consumed: 14.64s
Validation Epoch: 0, Average loss: 0.0279, Accuracy: 0.0165
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 125000, 'lr': 0.006788225099390857, 'energy': 122.99029449118201, 'time': 11.105981093000082, 'accuracy': 0.01650390625, 'total_cost': 639.4347690407621}
[run job] Launching job with BS 256: and LR: 0.006788225099390857 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs256+lr0.00679+pl100', 'ZEUS_COST_THRESH': '620.9984237967226', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '0.006788225099390857', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '0.006788225099390857', '--power_limit', '100']
[run job] cost_ub=620.9984237967226
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs256+lr0.00679+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.006788225099390857
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 6.4519
Training Epoch: 0 [768/50176]	Loss: 6.0655
Training Epoch: 0 [1024/50176]	Loss: 5.2343
Training Epoch: 0 [1280/50176]	Loss: 4.8762
Training Epoch: 0 [1536/50176]	Loss: 4.8129
Training Epoch: 0 [1792/50176]	Loss: 4.6250
Training Epoch: 0 [2048/50176]	Loss: 4.6137
Training Epoch: 0 [2304/50176]	Loss: 4.7222
Training Epoch: 0 [2560/50176]	Loss: 4.6295
Training Epoch: 0 [2816/50176]	Loss: 4.5895
Training Epoch: 0 [3072/50176]	Loss: 4.7036
Training Epoch: 0 [3328/50176]	Loss: 4.5514
Training Epoch: 0 [3584/50176]	Loss: 4.6598
Training Epoch: 0 [3840/50176]	Loss: 4.7354
Training Epoch: 0 [4096/50176]	Loss: 4.6260
Training Epoch: 0 [4352/50176]	Loss: 4.6203
Training Epoch: 0 [4608/50176]	Loss: 4.5947
Training Epoch: 0 [4864/50176]	Loss: 4.5208
Training Epoch: 0 [5120/50176]	Loss: 4.5186
Training Epoch: 0 [5376/50176]	Loss: 4.5116
Training Epoch: 0 [5632/50176]	Loss: 4.5812
Training Epoch: 0 [5888/50176]	Loss: 4.5155
Training Epoch: 0 [6144/50176]	Loss: 4.5536
Training Epoch: 0 [6400/50176]	Loss: 4.8075
Training Epoch: 0 [6656/50176]	Loss: 4.6515
Training Epoch: 0 [6912/50176]	Loss: 4.4951
Training Epoch: 0 [7168/50176]	Loss: 4.7146
Training Epoch: 0 [7424/50176]	Loss: 4.6042
Training Epoch: 0 [7680/50176]	Loss: 4.6087
Training Epoch: 0 [7936/50176]	Loss: 4.5476
Training Epoch: 0 [8192/50176]	Loss: 4.5027
Training Epoch: 0 [8448/50176]	Loss: 4.5111
Training Epoch: 0 [8704/50176]	Loss: 4.4730
Training Epoch: 0 [8960/50176]	Loss: 4.4646
Training Epoch: 0 [9216/50176]	Loss: 4.4360
Training Epoch: 0 [9472/50176]	Loss: 4.4771
Training Epoch: 0 [9728/50176]	Loss: 4.4275
Training Epoch: 0 [9984/50176]	Loss: 4.4109
Training Epoch: 0 [10240/50176]	Loss: 4.4185
Training Epoch: 0 [10496/50176]	Loss: 4.3732
Training Epoch: 0 [10752/50176]	Loss: 4.3637
Training Epoch: 0 [11008/50176]	Loss: 4.3360
Training Epoch: 0 [11264/50176]	Loss: 4.3837
Training Epoch: 0 [11520/50176]	Loss: 4.3350
Training Epoch: 0 [11776/50176]	Loss: 4.3959
Training Epoch: 0 [12032/50176]	Loss: 4.4046
Training Epoch: 0 [12288/50176]	Loss: 4.3536
Training Epoch: 0 [12544/50176]	Loss: 4.3201
Training Epoch: 0 [12800/50176]	Loss: 4.3632
Profile done with power limit 100W
epoch 1 train time consumed: 36.60s
Validation Epoch: 0, Average loss: 0.0246, Accuracy: 0.0237
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 100000, 'lr': 0.006788225099390857, 'energy': 102.71251431880901, 'time': 28.962961090000135, 'accuracy': 0.02373046875, 'total_cost': 1080.8266393108954}

[Power Profiler] with batch size 512 and learning rate 0.0064
[run job] Launching job with BS 512: and LR: 0.0064 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs512+lr0.00640+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '0.0064', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0064', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs512+lr0.00640+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.0064
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 5.9978
Training Epoch: 0 [1536/50176]	Loss: 5.4432
Training Epoch: 0 [2048/50176]	Loss: 4.6748
Training Epoch: 0 [2560/50176]	Loss: 4.8490
Training Epoch: 0 [3072/50176]	Loss: 4.7482
Training Epoch: 0 [3584/50176]	Loss: 4.6743
Training Epoch: 0 [4096/50176]	Loss: 4.7389
Training Epoch: 0 [4608/50176]	Loss: 4.5910
Training Epoch: 0 [5120/50176]	Loss: 4.8920
Training Epoch: 0 [5632/50176]	Loss: 4.6778
Training Epoch: 0 [6144/50176]	Loss: 4.6155
Training Epoch: 0 [6656/50176]	Loss: 4.6635
Training Epoch: 0 [7168/50176]	Loss: 4.6018
Training Epoch: 0 [7680/50176]	Loss: 4.5794
Training Epoch: 0 [8192/50176]	Loss: 4.5943
Training Epoch: 0 [8704/50176]	Loss: 4.6070
Training Epoch: 0 [9216/50176]	Loss: 4.5596
Training Epoch: 0 [9728/50176]	Loss: 4.5843
Training Epoch: 0 [10240/50176]	Loss: 4.5245
Training Epoch: 0 [10752/50176]	Loss: 4.5332
Training Epoch: 0 [11264/50176]	Loss: 4.4947
Training Epoch: 0 [11776/50176]	Loss: 4.4962
Training Epoch: 0 [12288/50176]	Loss: 4.5115
Training Epoch: 0 [12800/50176]	Loss: 4.4518
Training Epoch: 0 [13312/50176]	Loss: 4.4268
Training Epoch: 0 [13824/50176]	Loss: 4.3828
Training Epoch: 0 [14336/50176]	Loss: 4.3567
Training Epoch: 0 [14848/50176]	Loss: 4.2864
Training Epoch: 0 [15360/50176]	Loss: 4.3476
Training Epoch: 0 [15872/50176]	Loss: 4.3335
Training Epoch: 0 [16384/50176]	Loss: 4.2534
Training Epoch: 0 [16896/50176]	Loss: 4.2888
Training Epoch: 0 [17408/50176]	Loss: 4.3468
Training Epoch: 0 [17920/50176]	Loss: 4.3140
Training Epoch: 0 [18432/50176]	Loss: 4.2542
Training Epoch: 0 [18944/50176]	Loss: 4.2613
Training Epoch: 0 [19456/50176]	Loss: 4.2508
Training Epoch: 0 [19968/50176]	Loss: 4.2450
Training Epoch: 0 [20480/50176]	Loss: 4.2158
Training Epoch: 0 [20992/50176]	Loss: 4.2879
Training Epoch: 0 [21504/50176]	Loss: 4.2277
Training Epoch: 0 [22016/50176]	Loss: 4.2785
Training Epoch: 0 [22528/50176]	Loss: 4.2528
Training Epoch: 0 [23040/50176]	Loss: 4.1786
Training Epoch: 0 [23552/50176]	Loss: 4.2471
Training Epoch: 0 [24064/50176]	Loss: 4.2294
Training Epoch: 0 [24576/50176]	Loss: 4.1988
Training Epoch: 0 [25088/50176]	Loss: 4.2348
Training Epoch: 0 [25600/50176]	Loss: 4.1459
Profile done with power limit 175W
epoch 1 train time consumed: 23.84s
Validation Epoch: 0, Average loss: 0.0086, Accuracy: 0.0442
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 175000, 'lr': 0.0064, 'energy': 155.35838363431816, 'time': 18.13963056800003, 'accuracy': 0.04423828125, 'total_cost': 863.9119209504936}
[run job] Launching job with BS 512: and LR: 0.0064 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs512+lr0.00640+pl150', 'ZEUS_COST_THRESH': '1727.8238419009872', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '0.0064', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0064', '--power_limit', '150']
[run job] cost_ub=1727.8238419009872
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs512+lr0.00640+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.0064
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 5.9978
Training Epoch: 0 [1536/50176]	Loss: 5.4432
Training Epoch: 0 [2048/50176]	Loss: 4.6761
Training Epoch: 0 [2560/50176]	Loss: 4.8451
Training Epoch: 0 [3072/50176]	Loss: 4.7415
Training Epoch: 0 [3584/50176]	Loss: 4.6567
Training Epoch: 0 [4096/50176]	Loss: 4.7265
Training Epoch: 0 [4608/50176]	Loss: 4.7140
Training Epoch: 0 [5120/50176]	Loss: 4.5784
Training Epoch: 0 [5632/50176]	Loss: 4.6841
Training Epoch: 0 [6144/50176]	Loss: 4.6373
Training Epoch: 0 [6656/50176]	Loss: 4.6413
Training Epoch: 0 [7168/50176]	Loss: 4.6113
Training Epoch: 0 [7680/50176]	Loss: 4.5974
Training Epoch: 0 [8192/50176]	Loss: 4.5953
Training Epoch: 0 [8704/50176]	Loss: 4.5960
Training Epoch: 0 [9216/50176]	Loss: 4.6233
Training Epoch: 0 [9728/50176]	Loss: 4.6666
Training Epoch: 0 [10240/50176]	Loss: 4.5465
Training Epoch: 0 [10752/50176]	Loss: 4.5266
Training Epoch: 0 [11264/50176]	Loss: 4.5155
Training Epoch: 0 [11776/50176]	Loss: 4.5423
Training Epoch: 0 [12288/50176]	Loss: 4.5681
Training Epoch: 0 [12800/50176]	Loss: 4.5185
Training Epoch: 0 [13312/50176]	Loss: 4.4874
Training Epoch: 0 [13824/50176]	Loss: 4.4220
Training Epoch: 0 [14336/50176]	Loss: 4.4114
Training Epoch: 0 [14848/50176]	Loss: 4.3548
Training Epoch: 0 [15360/50176]	Loss: 4.3612
Training Epoch: 0 [15872/50176]	Loss: 4.3585
Training Epoch: 0 [16384/50176]	Loss: 4.2675
Training Epoch: 0 [16896/50176]	Loss: 4.3132
Training Epoch: 0 [17408/50176]	Loss: 4.3543
Training Epoch: 0 [17920/50176]	Loss: 4.3401
Training Epoch: 0 [18432/50176]	Loss: 4.2724
Training Epoch: 0 [18944/50176]	Loss: 4.2291
Training Epoch: 0 [19456/50176]	Loss: 4.2812
Training Epoch: 0 [19968/50176]	Loss: 4.2385
Training Epoch: 0 [20480/50176]	Loss: 4.2244
Training Epoch: 0 [20992/50176]	Loss: 4.1578
Training Epoch: 0 [21504/50176]	Loss: 4.2158
Training Epoch: 0 [22016/50176]	Loss: 4.2624
Training Epoch: 0 [22528/50176]	Loss: 4.2462
Training Epoch: 0 [23040/50176]	Loss: 4.2111
Training Epoch: 0 [23552/50176]	Loss: 4.2301
Training Epoch: 0 [24064/50176]	Loss: 4.3186
Training Epoch: 0 [24576/50176]	Loss: 4.2523
Training Epoch: 0 [25088/50176]	Loss: 4.2731
Training Epoch: 0 [25600/50176]	Loss: 4.2006
Profile done with power limit 150W
epoch 1 train time consumed: 24.43s
Validation Epoch: 0, Average loss: 0.0282, Accuracy: 0.0214
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 150000, 'lr': 0.0064, 'energy': 146.8717465874235, 'time': 18.690804988999844, 'accuracy': 0.02138671875, 'total_cost': 1793.9925962342547}
[run job] Launching job with BS 512: and LR: 0.0064 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs512+lr0.00640+pl125', 'ZEUS_COST_THRESH': '1727.8238419009872', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '0.0064', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0064', '--power_limit', '125']
[run job] cost_ub=1727.8238419009872
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs512+lr0.00640+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.0064
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 5.9978
Training Epoch: 0 [1536/50176]	Loss: 5.4433
Training Epoch: 0 [2048/50176]	Loss: 4.6769
Training Epoch: 0 [2560/50176]	Loss: 4.8382
Training Epoch: 0 [3072/50176]	Loss: 4.7410
Training Epoch: 0 [3584/50176]	Loss: 4.6614
Training Epoch: 0 [4096/50176]	Loss: 4.7389
Training Epoch: 0 [4608/50176]	Loss: 4.5846
Training Epoch: 0 [5120/50176]	Loss: 4.5847
Training Epoch: 0 [5632/50176]	Loss: 4.6717
Training Epoch: 0 [6144/50176]	Loss: 4.6314
Training Epoch: 0 [6656/50176]	Loss: 4.8193
Training Epoch: 0 [7168/50176]	Loss: 4.7611
Training Epoch: 0 [7680/50176]	Loss: 4.6596
Training Epoch: 0 [8192/50176]	Loss: 4.7003
Training Epoch: 0 [8704/50176]	Loss: 4.6212
Training Epoch: 0 [9216/50176]	Loss: 4.7050
Training Epoch: 0 [9728/50176]	Loss: 4.6083
Training Epoch: 0 [10240/50176]	Loss: 4.6147
Training Epoch: 0 [10752/50176]	Loss: 4.5435
Training Epoch: 0 [11264/50176]	Loss: 4.5695
Training Epoch: 0 [11776/50176]	Loss: 4.5657
Training Epoch: 0 [12288/50176]	Loss: 4.5322
Training Epoch: 0 [12800/50176]	Loss: 4.4971
Training Epoch: 0 [13312/50176]	Loss: 4.5245
Training Epoch: 0 [13824/50176]	Loss: 4.5224
Training Epoch: 0 [14336/50176]	Loss: 4.4998
Training Epoch: 0 [14848/50176]	Loss: 4.3883
Training Epoch: 0 [15360/50176]	Loss: 4.4126
Training Epoch: 0 [15872/50176]	Loss: 4.3735
Training Epoch: 0 [16384/50176]	Loss: 4.2746
Training Epoch: 0 [16896/50176]	Loss: 4.3252
Training Epoch: 0 [17408/50176]	Loss: 4.4070
Training Epoch: 0 [17920/50176]	Loss: 4.3614
Training Epoch: 0 [18432/50176]	Loss: 4.2734
Training Epoch: 0 [18944/50176]	Loss: 4.2742
Training Epoch: 0 [19456/50176]	Loss: 4.2958
Training Epoch: 0 [19968/50176]	Loss: 4.2675
Training Epoch: 0 [20480/50176]	Loss: 4.2630
Training Epoch: 0 [20992/50176]	Loss: 4.2235
Training Epoch: 0 [21504/50176]	Loss: 4.2730
Training Epoch: 0 [22016/50176]	Loss: 4.2700
Training Epoch: 0 [22528/50176]	Loss: 4.1827
Training Epoch: 0 [23040/50176]	Loss: 4.1427
Training Epoch: 0 [23552/50176]	Loss: 4.2778
Training Epoch: 0 [24064/50176]	Loss: 4.1797
Training Epoch: 0 [24576/50176]	Loss: 4.2153
Training Epoch: 0 [25088/50176]	Loss: 4.1992
Training Epoch: 0 [25600/50176]	Loss: 4.1981
Profile done with power limit 125W
epoch 1 train time consumed: 27.76s
Validation Epoch: 0, Average loss: 0.0086, Accuracy: 0.0443
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 125000, 'lr': 0.0064, 'energy': 123.08875334249106, 'time': 21.376406217000294, 'accuracy': 0.0443359375, 'total_cost': 916.5967857834457}
[run job] Launching job with BS 512: and LR: 0.0064 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs512+lr0.00640+pl100', 'ZEUS_COST_THRESH': '1727.8238419009872', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '0.0064', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0064', '--power_limit', '100']
[run job] cost_ub=1727.8238419009872
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs512+lr0.00640+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.0064
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 5.9978
Training Epoch: 0 [1536/50176]	Loss: 5.4429
Training Epoch: 0 [2048/50176]	Loss: 4.6759
Training Epoch: 0 [2560/50176]	Loss: 4.8468
Training Epoch: 0 [3072/50176]	Loss: 4.7451
Training Epoch: 0 [3584/50176]	Loss: 4.6731
Training Epoch: 0 [4096/50176]	Loss: 4.7385
Training Epoch: 0 [4608/50176]	Loss: 4.5969
Training Epoch: 0 [5120/50176]	Loss: 4.6105
Training Epoch: 0 [5632/50176]	Loss: 4.7371
Training Epoch: 0 [6144/50176]	Loss: 4.6263
Training Epoch: 0 [6656/50176]	Loss: 4.6882
Training Epoch: 0 [7168/50176]	Loss: 4.6119
Training Epoch: 0 [7680/50176]	Loss: 4.5867
Training Epoch: 0 [8192/50176]	Loss: 4.6453
Training Epoch: 0 [8704/50176]	Loss: 4.6589
Training Epoch: 0 [9216/50176]	Loss: 4.6237
Training Epoch: 0 [9728/50176]	Loss: 4.5707
Training Epoch: 0 [10240/50176]	Loss: 4.5286
Training Epoch: 0 [10752/50176]	Loss: 4.5179
Training Epoch: 0 [11264/50176]	Loss: 4.5353
Training Epoch: 0 [11776/50176]	Loss: 4.5035
Training Epoch: 0 [12288/50176]	Loss: 4.5308
Training Epoch: 0 [12800/50176]	Loss: 4.4930
Training Epoch: 0 [13312/50176]	Loss: 4.4623
Training Epoch: 0 [13824/50176]	Loss: 4.4247
Training Epoch: 0 [14336/50176]	Loss: 4.4170
Training Epoch: 0 [14848/50176]	Loss: 4.3756
Training Epoch: 0 [15360/50176]	Loss: 4.3677
Training Epoch: 0 [15872/50176]	Loss: 4.3401
Training Epoch: 0 [16384/50176]	Loss: 4.2763
Training Epoch: 0 [16896/50176]	Loss: 4.3083
Training Epoch: 0 [17408/50176]	Loss: 4.3750
Training Epoch: 0 [17920/50176]	Loss: 4.3379
Training Epoch: 0 [18432/50176]	Loss: 4.2557
Training Epoch: 0 [18944/50176]	Loss: 4.2443
Training Epoch: 0 [19456/50176]	Loss: 4.2650
Training Epoch: 0 [19968/50176]	Loss: 4.2769
Training Epoch: 0 [20480/50176]	Loss: 4.2535
Training Epoch: 0 [20992/50176]	Loss: 4.1845
Training Epoch: 0 [21504/50176]	Loss: 4.2329
Training Epoch: 0 [22016/50176]	Loss: 4.2648
Training Epoch: 0 [22528/50176]	Loss: 4.2391
Training Epoch: 0 [23040/50176]	Loss: 4.2034
Training Epoch: 0 [23552/50176]	Loss: 4.2401
Training Epoch: 0 [24064/50176]	Loss: 4.2471
Training Epoch: 0 [24576/50176]	Loss: 4.2015
Training Epoch: 0 [25088/50176]	Loss: 4.2448
Training Epoch: 0 [25600/50176]	Loss: 4.1625
Profile done with power limit 100W
epoch 1 train time consumed: 66.10s
Validation Epoch: 0, Average loss: 0.0085, Accuracy: 0.0402
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 100000, 'lr': 0.0064, 'energy': 103.68025803948781, 'time': 52.0712412339999, 'accuracy': 0.040234375, 'total_cost': 2300.174669075095}

[Power Profiler] with batch size 512 and learning rate 0.008
[run job] Launching job with BS 512: and LR: 0.008 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs512+lr0.00800+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '0.008', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '0.008', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs512+lr0.00800+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.008
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 6.5227
Training Epoch: 0 [1536/50176]	Loss: 5.3003
Training Epoch: 0 [2048/50176]	Loss: 4.6758
Training Epoch: 0 [2560/50176]	Loss: 4.8434
Training Epoch: 0 [3072/50176]	Loss: 4.6607
Training Epoch: 0 [3584/50176]	Loss: 4.7297
Training Epoch: 0 [4096/50176]	Loss: 4.6804
Training Epoch: 0 [4608/50176]	Loss: 4.6262
Training Epoch: 0 [5120/50176]	Loss: 4.6973
Training Epoch: 0 [5632/50176]	Loss: 4.7410
Training Epoch: 0 [6144/50176]	Loss: 4.6401
Training Epoch: 0 [6656/50176]	Loss: 4.6765
Training Epoch: 0 [7168/50176]	Loss: 4.6874
Training Epoch: 0 [7680/50176]	Loss: 4.6179
Training Epoch: 0 [8192/50176]	Loss: 4.7245
Training Epoch: 0 [8704/50176]	Loss: 4.6873
Training Epoch: 0 [9216/50176]	Loss: 4.7007
Training Epoch: 0 [9728/50176]	Loss: 4.6013
Training Epoch: 0 [10240/50176]	Loss: 4.5814
Training Epoch: 0 [10752/50176]	Loss: 4.5566
Training Epoch: 0 [11264/50176]	Loss: 4.5556
Training Epoch: 0 [11776/50176]	Loss: 4.5813
Training Epoch: 0 [12288/50176]	Loss: 4.5828
Training Epoch: 0 [12800/50176]	Loss: 4.5928
Training Epoch: 0 [13312/50176]	Loss: 4.5474
Training Epoch: 0 [13824/50176]	Loss: 4.5659
Training Epoch: 0 [14336/50176]	Loss: 4.5141
Training Epoch: 0 [14848/50176]	Loss: 4.4971
Training Epoch: 0 [15360/50176]	Loss: 4.4935
Training Epoch: 0 [15872/50176]	Loss: 4.4725
Training Epoch: 0 [16384/50176]	Loss: 4.5016
Training Epoch: 0 [16896/50176]	Loss: 4.4774
Training Epoch: 0 [17408/50176]	Loss: 4.5497
Training Epoch: 0 [17920/50176]	Loss: 4.4680
Training Epoch: 0 [18432/50176]	Loss: 4.4311
Training Epoch: 0 [18944/50176]	Loss: 4.4470
Training Epoch: 0 [19456/50176]	Loss: 4.4598
Training Epoch: 0 [19968/50176]	Loss: 4.4134
Training Epoch: 0 [20480/50176]	Loss: 4.4297
Training Epoch: 0 [20992/50176]	Loss: 4.3635
Training Epoch: 0 [21504/50176]	Loss: 4.4139
Training Epoch: 0 [22016/50176]	Loss: 4.4276
Training Epoch: 0 [22528/50176]	Loss: 4.3444
Training Epoch: 0 [23040/50176]	Loss: 4.3557
Training Epoch: 0 [23552/50176]	Loss: 4.4407
Training Epoch: 0 [24064/50176]	Loss: 4.3852
Training Epoch: 0 [24576/50176]	Loss: 4.3361
Training Epoch: 0 [25088/50176]	Loss: 4.3538
Training Epoch: 0 [25600/50176]	Loss: 4.3902
Profile done with power limit 175W
epoch 1 train time consumed: 23.80s
Validation Epoch: 0, Average loss: 0.0141, Accuracy: 0.0229
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 175000, 'lr': 0.008, 'energy': 155.29671626071058, 'time': 18.133260544000223, 'accuracy': 0.02294921875, 'total_cost': 1664.4325246119129}
[run job] Launching job with BS 512: and LR: 0.008 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs512+lr0.00800+pl150', 'ZEUS_COST_THRESH': '3328.8650492238257', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '0.008', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '0.008', '--power_limit', '150']
[run job] cost_ub=3328.8650492238257
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs512+lr0.00800+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.008
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 6.5227
Training Epoch: 0 [1536/50176]	Loss: 5.2994
Training Epoch: 0 [2048/50176]	Loss: 4.6786
Training Epoch: 0 [2560/50176]	Loss: 4.8271
Training Epoch: 0 [3072/50176]	Loss: 4.6796
Training Epoch: 0 [3584/50176]	Loss: 4.5858
Training Epoch: 0 [4096/50176]	Loss: 4.6517
Training Epoch: 0 [4608/50176]	Loss: 4.6345
Training Epoch: 0 [5120/50176]	Loss: 4.9192
Training Epoch: 0 [5632/50176]	Loss: 4.8194
Training Epoch: 0 [6144/50176]	Loss: 4.6472
Training Epoch: 0 [6656/50176]	Loss: 4.7368
Training Epoch: 0 [7168/50176]	Loss: 4.6586
Training Epoch: 0 [7680/50176]	Loss: 4.6181
Training Epoch: 0 [8192/50176]	Loss: 4.6647
Training Epoch: 0 [8704/50176]	Loss: 4.6363
Training Epoch: 0 [9216/50176]	Loss: 4.7310
Training Epoch: 0 [9728/50176]	Loss: 4.7525
Training Epoch: 0 [10240/50176]	Loss: 4.5753
Training Epoch: 0 [10752/50176]	Loss: 4.5446
Training Epoch: 0 [11264/50176]	Loss: 4.5951
Training Epoch: 0 [11776/50176]	Loss: 4.6227
Training Epoch: 0 [12288/50176]	Loss: 4.5294
Training Epoch: 0 [12800/50176]	Loss: 4.5669
Training Epoch: 0 [13312/50176]	Loss: 4.5116
Training Epoch: 0 [13824/50176]	Loss: 4.6102
Training Epoch: 0 [14336/50176]	Loss: 4.5438
Training Epoch: 0 [14848/50176]	Loss: 4.5094
Training Epoch: 0 [15360/50176]	Loss: 4.4654
Training Epoch: 0 [15872/50176]	Loss: 4.4644
Training Epoch: 0 [16384/50176]	Loss: 4.4755
Training Epoch: 0 [16896/50176]	Loss: 4.4607
Training Epoch: 0 [17408/50176]	Loss: 4.5334
Training Epoch: 0 [17920/50176]	Loss: 4.5096
Training Epoch: 0 [18432/50176]	Loss: 4.4328
Training Epoch: 0 [18944/50176]	Loss: 4.4429
Training Epoch: 0 [19456/50176]	Loss: 4.4331
Training Epoch: 0 [19968/50176]	Loss: 4.4489
Training Epoch: 0 [20480/50176]	Loss: 4.4352
Training Epoch: 0 [20992/50176]	Loss: 4.4008
Training Epoch: 0 [21504/50176]	Loss: 4.4187
Training Epoch: 0 [22016/50176]	Loss: 4.4357
Training Epoch: 0 [22528/50176]	Loss: 4.3872
Training Epoch: 0 [23040/50176]	Loss: 4.4102
Training Epoch: 0 [23552/50176]	Loss: 4.5220
Training Epoch: 0 [24064/50176]	Loss: 4.4347
Training Epoch: 0 [24576/50176]	Loss: 4.3958
Training Epoch: 0 [25088/50176]	Loss: 4.3462
Training Epoch: 0 [25600/50176]	Loss: 4.3809
Profile done with power limit 150W
epoch 1 train time consumed: 24.48s
Validation Epoch: 0, Average loss: 0.0087, Accuracy: 0.0268
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 150000, 'lr': 0.008, 'energy': 147.05802709879453, 'time': 18.716894057999525, 'accuracy': 0.0267578125, 'total_cost': 1436.7170501978792}
[run job] Launching job with BS 512: and LR: 0.008 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs512+lr0.00800+pl125', 'ZEUS_COST_THRESH': '2873.4341003957584', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '0.008', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '0.008', '--power_limit', '125']
[run job] cost_ub=2873.4341003957584
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs512+lr0.00800+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.008
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 6.5227
Training Epoch: 0 [1536/50176]	Loss: 5.2991
Training Epoch: 0 [2048/50176]	Loss: 4.6766
Training Epoch: 0 [2560/50176]	Loss: 4.8374
Training Epoch: 0 [3072/50176]	Loss: 4.6703
Training Epoch: 0 [3584/50176]	Loss: 4.6330
Training Epoch: 0 [4096/50176]	Loss: 4.7424
Training Epoch: 0 [4608/50176]	Loss: 4.6518
Training Epoch: 0 [5120/50176]	Loss: 4.8694
Training Epoch: 0 [5632/50176]	Loss: 4.6871
Training Epoch: 0 [6144/50176]	Loss: 4.6519
Training Epoch: 0 [6656/50176]	Loss: 4.7297
Training Epoch: 0 [7168/50176]	Loss: 4.6333
Training Epoch: 0 [7680/50176]	Loss: 4.6072
Training Epoch: 0 [8192/50176]	Loss: 4.6920
Training Epoch: 0 [8704/50176]	Loss: 4.6679
Training Epoch: 0 [9216/50176]	Loss: 4.6521
Training Epoch: 0 [9728/50176]	Loss: 4.5873
Training Epoch: 0 [10240/50176]	Loss: 4.6018
Training Epoch: 0 [10752/50176]	Loss: 4.5708
Training Epoch: 0 [11264/50176]	Loss: 4.5695
Training Epoch: 0 [11776/50176]	Loss: 4.5980
Training Epoch: 0 [12288/50176]	Loss: 4.6661
Training Epoch: 0 [12800/50176]	Loss: 4.5903
Training Epoch: 0 [13312/50176]	Loss: 4.5439
Training Epoch: 0 [13824/50176]	Loss: 4.5577
Training Epoch: 0 [14336/50176]	Loss: 4.5095
Training Epoch: 0 [14848/50176]	Loss: 4.5194
Training Epoch: 0 [15360/50176]	Loss: 4.5293
Training Epoch: 0 [15872/50176]	Loss: 4.4504
Training Epoch: 0 [16384/50176]	Loss: 4.4900
Training Epoch: 0 [16896/50176]	Loss: 4.4642
Training Epoch: 0 [17408/50176]	Loss: 4.5542
Training Epoch: 0 [17920/50176]	Loss: 4.5182
Training Epoch: 0 [18432/50176]	Loss: 4.4363
Training Epoch: 0 [18944/50176]	Loss: 4.4419
Training Epoch: 0 [19456/50176]	Loss: 4.4896
Training Epoch: 0 [19968/50176]	Loss: 4.3905
Training Epoch: 0 [20480/50176]	Loss: 4.3869
Training Epoch: 0 [20992/50176]	Loss: 4.3531
Training Epoch: 0 [21504/50176]	Loss: 4.4039
Training Epoch: 0 [22016/50176]	Loss: 4.4112
Training Epoch: 0 [22528/50176]	Loss: 4.3911
Training Epoch: 0 [23040/50176]	Loss: 4.3755
Training Epoch: 0 [23552/50176]	Loss: 4.4153
Training Epoch: 0 [24064/50176]	Loss: 4.3775
Training Epoch: 0 [24576/50176]	Loss: 4.3241
Training Epoch: 0 [25088/50176]	Loss: 4.3147
Training Epoch: 0 [25600/50176]	Loss: 4.3063
Profile done with power limit 125W
epoch 1 train time consumed: 27.78s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0322
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 125000, 'lr': 0.008, 'energy': 123.00045702410289, 'time': 21.40397927899994, 'accuracy': 0.0322265625, 'total_cost': 1262.2675289600813}
[run job] Launching job with BS 512: and LR: 0.008 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs512+lr0.00800+pl100', 'ZEUS_COST_THRESH': '2524.5350579201627', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '0.008', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '0.008', '--power_limit', '100']
[run job] cost_ub=2524.5350579201627
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs512+lr0.00800+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.008
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 6.5227
Training Epoch: 0 [1536/50176]	Loss: 5.3002
Training Epoch: 0 [2048/50176]	Loss: 4.6766
Training Epoch: 0 [2560/50176]	Loss: 4.8349
Training Epoch: 0 [3072/50176]	Loss: 4.6663
Training Epoch: 0 [3584/50176]	Loss: 4.6121
Training Epoch: 0 [4096/50176]	Loss: 4.6370
Training Epoch: 0 [4608/50176]	Loss: 4.6051
Training Epoch: 0 [5120/50176]	Loss: 4.6877
Training Epoch: 0 [5632/50176]	Loss: 4.7375
Training Epoch: 0 [6144/50176]	Loss: 4.6840
Training Epoch: 0 [6656/50176]	Loss: 4.7281
Training Epoch: 0 [7168/50176]	Loss: 4.6792
Training Epoch: 0 [7680/50176]	Loss: 4.6312
Training Epoch: 0 [8192/50176]	Loss: 4.5797
Training Epoch: 0 [8704/50176]	Loss: 4.6118
Training Epoch: 0 [9216/50176]	Loss: 4.7062
Training Epoch: 0 [9728/50176]	Loss: 4.7601
Training Epoch: 0 [10240/50176]	Loss: 4.5686
Training Epoch: 0 [10752/50176]	Loss: 4.5620
Training Epoch: 0 [11264/50176]	Loss: 4.5814
Training Epoch: 0 [11776/50176]	Loss: 4.6006
Training Epoch: 0 [12288/50176]	Loss: 4.5620
Training Epoch: 0 [12800/50176]	Loss: 4.6280
Training Epoch: 0 [13312/50176]	Loss: 4.5729
Training Epoch: 0 [13824/50176]	Loss: 4.6687
Training Epoch: 0 [14336/50176]	Loss: 4.5718
Training Epoch: 0 [14848/50176]	Loss: 4.4914
Training Epoch: 0 [15360/50176]	Loss: 4.5325
Training Epoch: 0 [15872/50176]	Loss: 4.4649
Training Epoch: 0 [16384/50176]	Loss: 4.6511
Training Epoch: 0 [16896/50176]	Loss: 4.5111
Training Epoch: 0 [17408/50176]	Loss: 4.7018
Training Epoch: 0 [17920/50176]	Loss: 4.4914
Training Epoch: 0 [18432/50176]	Loss: 4.5316
Training Epoch: 0 [18944/50176]	Loss: 4.5862
Training Epoch: 0 [19456/50176]	Loss: 4.5782
Training Epoch: 0 [19968/50176]	Loss: 4.4405
Training Epoch: 0 [20480/50176]	Loss: 4.5178
Training Epoch: 0 [20992/50176]	Loss: 4.3749
Training Epoch: 0 [21504/50176]	Loss: 4.4455
Training Epoch: 0 [22016/50176]	Loss: 4.4028
Training Epoch: 0 [22528/50176]	Loss: 4.3872
Training Epoch: 0 [23040/50176]	Loss: 4.3818
Training Epoch: 0 [23552/50176]	Loss: 4.4399
Training Epoch: 0 [24064/50176]	Loss: 4.4556
Training Epoch: 0 [24576/50176]	Loss: 4.3556
Training Epoch: 0 [25088/50176]	Loss: 4.3543
Training Epoch: 0 [25600/50176]	Loss: 4.4387
Profile done with power limit 100W
epoch 1 train time consumed: 66.12s
Validation Epoch: 0, Average loss: 0.0128, Accuracy: 0.0189
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 100000, 'lr': 0.008, 'energy': 104.01211348092335, 'time': 52.08029924699986, 'accuracy': 0.0189453125, 'total_cost': 4891.574791037027}

[Power Profiler] with batch size 512 and learning rate 0.0096
[run job] Launching job with BS 512: and LR: 0.0096 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs512+lr0.00960+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '0.0096', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0096', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs512+lr0.00960+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.0096
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 7.0369
Training Epoch: 0 [1536/50176]	Loss: 5.2667
Training Epoch: 0 [2048/50176]	Loss: 4.6953
Training Epoch: 0 [2560/50176]	Loss: 4.8525
Training Epoch: 0 [3072/50176]	Loss: 4.7155
Training Epoch: 0 [3584/50176]	Loss: 4.5834
Training Epoch: 0 [4096/50176]	Loss: 4.7488
Training Epoch: 0 [4608/50176]	Loss: 4.6907
Training Epoch: 0 [5120/50176]	Loss: 4.7972
Training Epoch: 0 [5632/50176]	Loss: 4.6553
Training Epoch: 0 [6144/50176]	Loss: 4.5960
Training Epoch: 0 [6656/50176]	Loss: 4.7608
Training Epoch: 0 [7168/50176]	Loss: 4.5923
Training Epoch: 0 [7680/50176]	Loss: 4.6062
Training Epoch: 0 [8192/50176]	Loss: 4.6132
Training Epoch: 0 [8704/50176]	Loss: 4.6267
Training Epoch: 0 [9216/50176]	Loss: 4.5889
Training Epoch: 0 [9728/50176]	Loss: 4.6070
Training Epoch: 0 [10240/50176]	Loss: 4.5929
Training Epoch: 0 [10752/50176]	Loss: 4.5653
Training Epoch: 0 [11264/50176]	Loss: 4.5711
Training Epoch: 0 [11776/50176]	Loss: 4.5742
Training Epoch: 0 [12288/50176]	Loss: 4.6540
Training Epoch: 0 [12800/50176]	Loss: 4.6458
Training Epoch: 0 [13312/50176]	Loss: 4.5808
Training Epoch: 0 [13824/50176]	Loss: 4.6116
Training Epoch: 0 [14336/50176]	Loss: 4.5553
Training Epoch: 0 [14848/50176]	Loss: 4.5516
Training Epoch: 0 [15360/50176]	Loss: 4.5748
Training Epoch: 0 [15872/50176]	Loss: 4.5298
Training Epoch: 0 [16384/50176]	Loss: 4.5140
Training Epoch: 0 [16896/50176]	Loss: 4.4925
Training Epoch: 0 [17408/50176]	Loss: 4.5345
Training Epoch: 0 [17920/50176]	Loss: 4.6141
Training Epoch: 0 [18432/50176]	Loss: 4.4754
Training Epoch: 0 [18944/50176]	Loss: 4.4624
Training Epoch: 0 [19456/50176]	Loss: 4.4582
Training Epoch: 0 [19968/50176]	Loss: 4.4202
Training Epoch: 0 [20480/50176]	Loss: 4.4144
Training Epoch: 0 [20992/50176]	Loss: 4.3833
Training Epoch: 0 [21504/50176]	Loss: 4.4089
Training Epoch: 0 [22016/50176]	Loss: 4.4650
Training Epoch: 0 [22528/50176]	Loss: 4.4074
Training Epoch: 0 [23040/50176]	Loss: 4.3880
Training Epoch: 0 [23552/50176]	Loss: 4.5123
Training Epoch: 0 [24064/50176]	Loss: 4.4075
Training Epoch: 0 [24576/50176]	Loss: 4.3401
Training Epoch: 0 [25088/50176]	Loss: 4.3911
Training Epoch: 0 [25600/50176]	Loss: 4.3397
Profile done with power limit 175W
epoch 1 train time consumed: 23.78s
Validation Epoch: 0, Average loss: 0.0089, Accuracy: 0.0184
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 175000, 'lr': 0.0096, 'energy': 155.39406891761777, 'time': 18.126648850000493, 'accuracy': 0.018359375, 'total_cost': 2080.395056671305}
[run job] Launching job with BS 512: and LR: 0.0096 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs512+lr0.00960+pl150', 'ZEUS_COST_THRESH': '4160.79011334261', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '0.0096', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0096', '--power_limit', '150']
[run job] cost_ub=4160.79011334261
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs512+lr0.00960+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.0096
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 7.0369
Training Epoch: 0 [1536/50176]	Loss: 5.2665
Training Epoch: 0 [2048/50176]	Loss: 4.6956
Training Epoch: 0 [2560/50176]	Loss: 4.8658
Training Epoch: 0 [3072/50176]	Loss: 4.6868
Training Epoch: 0 [3584/50176]	Loss: 4.5831
Training Epoch: 0 [4096/50176]	Loss: 4.7077
Training Epoch: 0 [4608/50176]	Loss: 4.7157
Training Epoch: 0 [5120/50176]	Loss: 4.6022
Training Epoch: 0 [5632/50176]	Loss: 4.6355
Training Epoch: 0 [6144/50176]	Loss: 4.5972
Training Epoch: 0 [6656/50176]	Loss: 4.6760
Training Epoch: 0 [7168/50176]	Loss: 4.5818
Training Epoch: 0 [7680/50176]	Loss: 4.6121
Training Epoch: 0 [8192/50176]	Loss: 4.5939
Training Epoch: 0 [8704/50176]	Loss: 4.6124
Training Epoch: 0 [9216/50176]	Loss: 4.6243
Training Epoch: 0 [9728/50176]	Loss: 4.6638
Training Epoch: 0 [10240/50176]	Loss: 4.5453
Training Epoch: 0 [10752/50176]	Loss: 4.5155
Training Epoch: 0 [11264/50176]	Loss: 4.5228
Training Epoch: 0 [11776/50176]	Loss: 4.5417
Training Epoch: 0 [12288/50176]	Loss: 4.6422
Training Epoch: 0 [12800/50176]	Loss: 4.6392
Training Epoch: 0 [13312/50176]	Loss: 4.5283
Training Epoch: 0 [13824/50176]	Loss: 4.5221
Training Epoch: 0 [14336/50176]	Loss: 4.5027
Training Epoch: 0 [14848/50176]	Loss: 4.4948
Training Epoch: 0 [15360/50176]	Loss: 4.4728
Training Epoch: 0 [15872/50176]	Loss: 4.4782
Training Epoch: 0 [16384/50176]	Loss: 4.4605
Training Epoch: 0 [16896/50176]	Loss: 4.4449
Training Epoch: 0 [17408/50176]	Loss: 4.4792
Training Epoch: 0 [17920/50176]	Loss: 4.5507
Training Epoch: 0 [18432/50176]	Loss: 4.4177
Training Epoch: 0 [18944/50176]	Loss: 4.4260
Training Epoch: 0 [19456/50176]	Loss: 4.4028
Training Epoch: 0 [19968/50176]	Loss: 4.3547
Training Epoch: 0 [20480/50176]	Loss: 4.4111
Training Epoch: 0 [20992/50176]	Loss: 4.2993
Training Epoch: 0 [21504/50176]	Loss: 4.3764
Training Epoch: 0 [22016/50176]	Loss: 4.3848
Training Epoch: 0 [22528/50176]	Loss: 4.3924
Training Epoch: 0 [23040/50176]	Loss: 4.3857
Training Epoch: 0 [23552/50176]	Loss: 4.4476
Training Epoch: 0 [24064/50176]	Loss: 4.4142
Training Epoch: 0 [24576/50176]	Loss: 4.3310
Training Epoch: 0 [25088/50176]	Loss: 4.3914
Training Epoch: 0 [25600/50176]	Loss: 4.4324
Profile done with power limit 150W
epoch 1 train time consumed: 24.53s
Validation Epoch: 0, Average loss: 0.0087, Accuracy: 0.0229
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 150000, 'lr': 0.0096, 'energy': 146.9008731988174, 'time': 18.724064208000527, 'accuracy': 0.0228515625, 'total_cost': 1682.132947744244}
[run job] Launching job with BS 512: and LR: 0.0096 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs512+lr0.00960+pl125', 'ZEUS_COST_THRESH': '3364.265895488488', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '0.0096', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0096', '--power_limit', '125']
[run job] cost_ub=3364.265895488488
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs512+lr0.00960+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.0096
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 7.0369
Training Epoch: 0 [1536/50176]	Loss: 5.2667
Training Epoch: 0 [2048/50176]	Loss: 4.6940
Training Epoch: 0 [2560/50176]	Loss: 4.8683
Training Epoch: 0 [3072/50176]	Loss: 4.6880
Training Epoch: 0 [3584/50176]	Loss: 4.5804
Training Epoch: 0 [4096/50176]	Loss: 4.6592
Training Epoch: 0 [4608/50176]	Loss: 4.7471
Training Epoch: 0 [5120/50176]	Loss: 4.6456
Training Epoch: 0 [5632/50176]	Loss: 4.6812
Training Epoch: 0 [6144/50176]	Loss: 4.6147
Training Epoch: 0 [6656/50176]	Loss: 4.6781
Training Epoch: 0 [7168/50176]	Loss: 4.7146
Training Epoch: 0 [7680/50176]	Loss: 4.6014
Training Epoch: 0 [8192/50176]	Loss: 4.6022
Training Epoch: 0 [8704/50176]	Loss: 4.6339
Training Epoch: 0 [9216/50176]	Loss: 4.5998
Training Epoch: 0 [9728/50176]	Loss: 4.6858
Training Epoch: 0 [10240/50176]	Loss: 4.5613
Training Epoch: 0 [10752/50176]	Loss: 4.5737
Training Epoch: 0 [11264/50176]	Loss: 4.5411
Training Epoch: 0 [11776/50176]	Loss: 4.5628
Training Epoch: 0 [12288/50176]	Loss: 4.6186
Training Epoch: 0 [12800/50176]	Loss: 4.5945
Training Epoch: 0 [13312/50176]	Loss: 4.5812
Training Epoch: 0 [13824/50176]	Loss: 4.5634
Training Epoch: 0 [14336/50176]	Loss: 4.5337
Training Epoch: 0 [14848/50176]	Loss: 4.5284
Training Epoch: 0 [15360/50176]	Loss: 4.5305
Training Epoch: 0 [15872/50176]	Loss: 4.5263
Training Epoch: 0 [16384/50176]	Loss: 4.4811
Training Epoch: 0 [16896/50176]	Loss: 4.5386
Training Epoch: 0 [17408/50176]	Loss: 4.5550
Training Epoch: 0 [17920/50176]	Loss: 4.6461
Training Epoch: 0 [18432/50176]	Loss: 4.5011
Training Epoch: 0 [18944/50176]	Loss: 4.5138
Training Epoch: 0 [19456/50176]	Loss: 4.4775
Training Epoch: 0 [19968/50176]	Loss: 4.4699
Training Epoch: 0 [20480/50176]	Loss: 4.4570
Training Epoch: 0 [20992/50176]	Loss: 4.4074
Training Epoch: 0 [21504/50176]	Loss: 4.4025
Training Epoch: 0 [22016/50176]	Loss: 4.4424
Training Epoch: 0 [22528/50176]	Loss: 4.4312
Training Epoch: 0 [23040/50176]	Loss: 4.4398
Training Epoch: 0 [23552/50176]	Loss: 4.4964
Training Epoch: 0 [24064/50176]	Loss: 4.4492
Training Epoch: 0 [24576/50176]	Loss: 4.4335
Training Epoch: 0 [25088/50176]	Loss: 4.4514
Training Epoch: 0 [25600/50176]	Loss: 4.4095
Profile done with power limit 125W
epoch 1 train time consumed: 27.75s
Validation Epoch: 0, Average loss: 0.0089, Accuracy: 0.0201
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 125000, 'lr': 0.0096, 'energy': 123.01529578948703, 'time': 21.373666524999862, 'accuracy': 0.0201171875, 'total_cost': 2019.315887157897}
[run job] Launching job with BS 512: and LR: 0.0096 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315', 'ZEUS_JOB_ID': 'bs512+lr0.00960+pl100', 'ZEUS_COST_THRESH': '3364.265895488488', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '0.0096', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0096', '--power_limit', '100']
[run job] cost_ub=3364.265895488488
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112713051669572315/bs512+lr0.00960+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.0096
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 7.0369
Training Epoch: 0 [1536/50176]	Loss: 5.2657
Training Epoch: 0 [2048/50176]	Loss: 4.6939
Training Epoch: 0 [2560/50176]	Loss: 4.8874
Training Epoch: 0 [3072/50176]	Loss: 4.6836
Training Epoch: 0 [3584/50176]	Loss: 4.5847
Training Epoch: 0 [4096/50176]	Loss: 4.6753
Training Epoch: 0 [4608/50176]	Loss: 4.7091
Training Epoch: 0 [5120/50176]	Loss: 4.7042
Training Epoch: 0 [5632/50176]	Loss: 4.6640
Training Epoch: 0 [6144/50176]	Loss: 4.6045
Training Epoch: 0 [6656/50176]	Loss: 4.7454
Training Epoch: 0 [7168/50176]	Loss: 4.5982
Training Epoch: 0 [7680/50176]	Loss: 4.5890
Training Epoch: 0 [8192/50176]	Loss: 4.5670
Training Epoch: 0 [8704/50176]	Loss: 4.5994
Training Epoch: 0 [9216/50176]	Loss: 4.5986
Training Epoch: 0 [9728/50176]	Loss: 4.5694
Training Epoch: 0 [10240/50176]	Loss: 4.5625
Training Epoch: 0 [10752/50176]	Loss: 4.5294
Training Epoch: 0 [11264/50176]	Loss: 4.5294
Training Epoch: 0 [11776/50176]	Loss: 4.5411
Training Epoch: 0 [12288/50176]	Loss: 4.6545
Training Epoch: 0 [12800/50176]	Loss: 4.6017
Training Epoch: 0 [13312/50176]	Loss: 4.5480
Training Epoch: 0 [13824/50176]	Loss: 4.5671
Training Epoch: 0 [14336/50176]	Loss: 4.5123
Training Epoch: 0 [14848/50176]	Loss: 4.4972
Training Epoch: 0 [15360/50176]	Loss: 4.4817
Training Epoch: 0 [15872/50176]	Loss: 4.5357
Training Epoch: 0 [16384/50176]	Loss: 4.4684
Training Epoch: 0 [16896/50176]	Loss: 4.5046
Training Epoch: 0 [17408/50176]	Loss: 4.5301
Training Epoch: 0 [17920/50176]	Loss: 4.5926
Training Epoch: 0 [18432/50176]	Loss: 4.4597
Training Epoch: 0 [18944/50176]	Loss: 4.4579
Training Epoch: 0 [19456/50176]	Loss: 4.4164
Training Epoch: 0 [19968/50176]	Loss: 4.4204
Training Epoch: 0 [20480/50176]	Loss: 4.4249
Training Epoch: 0 [20992/50176]	Loss: 4.3359
Training Epoch: 0 [21504/50176]	Loss: 4.3865
Training Epoch: 0 [22016/50176]	Loss: 4.4438
Training Epoch: 0 [22528/50176]	Loss: 4.3778
Training Epoch: 0 [23040/50176]	Loss: 4.4085
Training Epoch: 0 [23552/50176]	Loss: 4.5307
Training Epoch: 0 [24064/50176]	Loss: 4.4238
Training Epoch: 0 [24576/50176]	Loss: 4.3506
Training Epoch: 0 [25088/50176]	Loss: 4.4229
Training Epoch: 0 [25600/50176]	Loss: 4.3699
Profile done with power limit 100W
epoch 1 train time consumed: 66.07s
Validation Epoch: 0, Average loss: 0.0091, Accuracy: 0.0212
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 100000, 'lr': 0.0096, 'energy': 103.74357066633573, 'time': 52.0414078470003, 'accuracy': 0.02119140625, 'total_cost': 4365.641409431296}
[Power Profiler]
[HistoryEntry(bs=8, pl=175, lr=0.0008, energy=97.16369342745973, time=2.203665111999726, accuracy=0.0102, total_cost=5.879976823128256), HistoryEntry(bs=8, pl=150, lr=0.0008, energy=94.38656865078978, time=2.280025445999854, accuracy=0.0103, total_cost=5.963186711984343), HistoryEntry(bs=8, pl=125, lr=0.0008, energy=95.80669652986165, time=2.261584359000153, accuracy=0.0123, total_cost=4.979286090930375), HistoryEntry(bs=8, pl=100, lr=0.0008, energy=93.66868254859544, time=2.3210549550003634, accuracy=0.0105, total_cost=5.938997875074638), HistoryEntry(bs=8, pl=175, lr=0.001, energy=95.65395459295121, time=2.2429998729994622, accuracy=0.0104, total_cost=5.837276786334537), HistoryEntry(bs=8, pl=150, lr=0.001, energy=94.82412322079072, time=2.2397586480001337, accuracy=0.0105, total_cost=5.755627746883997), HistoryEntry(bs=8, pl=125, lr=0.001, energy=97.24759466237109, time=2.231993121999949, accuracy=0.0107, total_cost=5.6790164370789), HistoryEntry(bs=8, pl=100, lr=0.001, energy=95.82726394417486, time=2.2400386730005266, accuracy=0.0113, total_cost=5.368703937503301), HistoryEntry(bs=8, pl=175, lr=0.0012, energy=94.39402036616656, time=2.3427121169997918, accuracy=0.0108, total_cost=5.843635516288028), HistoryEntry(bs=8, pl=150, lr=0.0012, energy=97.91465162908382, time=2.2472193980001975, accuracy=0.01, total_cost=6.132990991393434), HistoryEntry(bs=8, pl=125, lr=0.0012, energy=94.34016101301853, time=2.2912404210001114, accuracy=0.0154, total_cost=4.007292622803288), HistoryEntry(bs=8, pl=100, lr=0.0012, energy=97.21342614671417, time=2.2544562189996213, accuracy=0.0104, total_cost=5.900896648765903), HistoryEntry(bs=16, pl=175, lr=0.0011313708498984763, energy=114.26823903237876, time=2.4487637330003054, accuracy=0.0133, total_cost=10.651873275960186), HistoryEntry(bs=16, pl=150, lr=0.0011313708498984763, energy=116.46995814174365, time=2.370886925999912, accuracy=0.0121, total_cost=11.422186993057876), HistoryEntry(bs=16, pl=125, lr=0.0011313708498984763, energy=115.3037123336963, time=2.435076527000092, accuracy=0.0108, total_cost=13.090958437125389), HistoryEntry(bs=16, pl=100, lr=0.0011313708498984763, energy=98.99872835530581, time=2.7405146650007737, accuracy=0.0112, total_cost=13.408884522308554), HistoryEntry(bs=16, pl=175, lr=0.0014142135623730952, energy=115.54774945835459, time=2.4450269750004736, accuracy=0.0104, total_cost=13.661482402910698), HistoryEntry(bs=16, pl=150, lr=0.0014142135623730952, energy=113.85318743516775, time=2.5173135269997147, accuracy=0.0117, total_cost=12.42964164183815), HistoryEntry(bs=16, pl=125, lr=0.0014142135623730952, energy=116.51729784111846, time=2.4022650550004983, accuracy=0.0115, total_cost=12.17916204403289), HistoryEntry(bs=16, pl=100, lr=0.0014142135623730952, energy=99.01141392081192, time=2.6990052159999323, accuracy=0.012, total_cost=12.325970590263132), HistoryEntry(bs=16, pl=175, lr=0.0016970562748477142, energy=114.13879590036645, time=2.423691926000174, accuracy=0.0137, total_cost=10.230414089301174), HistoryEntry(bs=16, pl=150, lr=0.0016970562748477142, energy=114.89630125394172, time=2.4563098240005274, accuracy=0.0121, total_cost=11.769836904321876), HistoryEntry(bs=16, pl=125, lr=0.0016970562748477142, energy=114.35799974466042, time=2.4782037220002167, accuracy=0.0133, total_cost=10.783279277560233), HistoryEntry(bs=16, pl=100, lr=0.0016970562748477142, energy=98.8875421840345, time=2.746671421999963, accuracy=0.0139, total_cost=10.824159495808589), HistoryEntry(bs=32, pl=175, lr=0.0016, energy=128.95859008873185, time=3.322503302999394, accuracy=0.022164536741214057, total_cost=18.219742025184775), HistoryEntry(bs=32, pl=150, lr=0.0016, energy=129.79704682607593, time=3.287513893999858, accuracy=0.023861821086261982, total_cost=16.791744056678684), HistoryEntry(bs=32, pl=125, lr=0.0016, energy=122.96939844968732, time=3.342067035000582, accuracy=0.022064696485623002, total_cost=18.047202957027505), HistoryEntry(bs=32, pl=100, lr=0.0016, energy=99.19686766948467, time=4.022859483000502, accuracy=0.02366214057507987, total_cost=18.640792044674335), HistoryEntry(bs=32, pl=175, lr=0.002, energy=130.23799443548376, time=3.2146951790000458, accuracy=0.020467252396166133, total_cost=19.17078446183168), HistoryEntry(bs=32, pl=150, lr=0.002, energy=129.41047573334137, time=3.2989510980005434, accuracy=0.027855431309904154, total_cost=14.416061406592698), HistoryEntry(bs=32, pl=125, lr=0.002, energy=122.51216623274738, time=3.3738826640001207, accuracy=0.025958466453674122, total_cost=15.462393169658931), HistoryEntry(bs=32, pl=100, lr=0.002, energy=99.13777517854797, time=4.089466127999913, accuracy=0.02545926517571885, total_cost=17.608026128603925), HistoryEntry(bs=32, pl=175, lr=0.0024, energy=130.42613822443087, time=3.196039735999875, accuracy=0.020467252396166133, total_cost=19.071281008554468), HistoryEntry(bs=32, pl=150, lr=0.0024, energy=130.290134568869, time=3.2563502370003334, accuracy=0.024361022364217253, total_cost=16.318093171176894), HistoryEntry(bs=32, pl=125, lr=0.0024, energy=123.30341314673954, time=3.3397537470000316, accuracy=0.024261182108626198, total_cost=16.420325046711), HistoryEntry(bs=32, pl=100, lr=0.0024, energy=99.34368358435245, time=4.001007664000099, accuracy=0.01986821086261981, total_cost=22.09157235473508), HistoryEntry(bs=64, pl=175, lr=0.0022627416997969526, energy=140.97252710311687, time=5.275773026000024, accuracy=0.025179140127388536, total_cost=52.913659243105585), HistoryEntry(bs=64, pl=150, lr=0.0022627416997969526, energy=140.92292866023027, time=5.303515381999205, accuracy=0.023288216560509552, total_cost=57.50187586794667), HistoryEntry(bs=64, pl=125, lr=0.0022627416997969526, energy=123.50823660687274, time=5.776365122000243, accuracy=0.018610668789808917, total_cost=74.04951870312529), HistoryEntry(bs=64, pl=100, lr=0.0022627416997969526, energy=97.41921568021128, time=11.99165712499962, accuracy=0.021994426751592355, total_cost=118.70733566459153), HistoryEntry(bs=64, pl=175, lr=0.0028284271247461905, energy=140.98157212485637, time=5.263480868999977, accuracy=0.019804936305732483, total_cost=67.11732182503509), HistoryEntry(bs=64, pl=150, lr=0.0028284271247461905, energy=141.45725251390022, time=5.262336994999714, accuracy=0.018113057324840764, total_cost=73.48102611974329), HistoryEntry(bs=64, pl=125, lr=0.0028284271247461905, energy=123.55583863974817, time=5.781693342999461, accuracy=0.0227906050955414, total_cost=60.53381294184599), HistoryEntry(bs=64, pl=100, lr=0.0028284271247461905, energy=97.76864428468683, time=12.041186259000824, accuracy=0.023188694267515922, total_cost=113.20371756741001), HistoryEntry(bs=64, pl=175, lr=0.0033941125496954284, energy=140.85165413137014, time=5.2863207230002445, accuracy=0.020402070063694266, total_cost=65.40872651904066), HistoryEntry(bs=64, pl=150, lr=0.0033941125496954284, energy=141.13091758683527, time=5.285094322000077, accuracy=0.019605891719745222, total_cost=68.10929405399577), HistoryEntry(bs=64, pl=125, lr=0.0033941125496954284, energy=123.66890939217338, time=5.767239947999769, accuracy=0.02298964968152866, total_cost=59.882365491771424), HistoryEntry(bs=64, pl=100, lr=0.0033941125496954284, energy=98.9903751133419, time=11.893013675000475, accuracy=0.021795382165605094, total_cost=119.49122158439621), HistoryEntry(bs=128, pl=175, lr=0.0032, energy=144.32286471894184, time=8.642025066000315, accuracy=0.03500791139240506, total_cost=126.00349668837067), HistoryEntry(bs=128, pl=150, lr=0.0032, energy=144.60684146270575, time=8.628159099000186, accuracy=0.029766613924050632, total_cost=148.08396598160488), HistoryEntry(bs=128, pl=125, lr=0.0032, energy=123.17760556517919, time=9.722833820999767, accuracy=0.034612341772151896, total_cost=133.8875401308027), HistoryEntry(bs=128, pl=100, lr=0.0032, energy=98.73342946860437, time=20.617449428000327, accuracy=0.02857990506329114, total_cost=315.6496082032171), HistoryEntry(bs=128, pl=175, lr=0.004, energy=143.8220655880103, time=8.65234766499998, accuracy=0.03283227848101266, total_cost=134.30264630359622), HistoryEntry(bs=128, pl=150, lr=0.004, energy=144.4714266806383, time=8.644126246999804, accuracy=0.028876582278481014, total_cost=152.86589091433257), HistoryEntry(bs=128, pl=125, lr=0.004, energy=123.31830187846056, time=9.649225109000326, accuracy=0.028975474683544302, total_cost=158.79800317229018), HistoryEntry(bs=128, pl=100, lr=0.004, energy=99.3152647134903, time=20.880086557999675, accuracy=0.029272151898734177, total_cost=312.77416965539743), HistoryEntry(bs=128, pl=175, lr=0.0048, energy=143.7565025614159, time=8.634199962000821, accuracy=0.02086629746835443, total_cost=210.83320997511314), HistoryEntry(bs=128, pl=150, lr=0.0048, energy=144.36667985148583, time=8.640819667999494, accuracy=0.02600870253164557, total_cost=169.60127678592585), HistoryEntry(bs=128, pl=125, lr=0.0048, energy=123.0974409774381, time=9.683135105000474, accuracy=0.02333860759493671, total_cost=197.6981280850627), HistoryEntry(bs=128, pl=100, lr=0.0048, energy=99.05878700207634, time=21.18148697200013, accuracy=0.026602056962025316, total_cost=348.809429630846), HistoryEntry(bs=256, pl=175, lr=0.004525483399593905, energy=154.37487671563272, time=9.516267467999569, accuracy=0.04296875, total_cost=232.60997581189937), HistoryEntry(bs=256, pl=150, lr=0.004525483399593905, energy=147.45303682476367, time=9.749285360000613, accuracy=0.04306640625, total_cost=232.76869239323668), HistoryEntry(bs=256, pl=125, lr=0.004525483399593905, energy=123.09533211501682, time=11.124153409000428, accuracy=0.0388671875, total_cost=272.0589235938369), HistoryEntry(bs=256, pl=100, lr=0.004525483399593905, energy=103.46993867140552, time=28.75322704700011, accuracy=0.04736328125, total_cost=539.0723751657055), HistoryEntry(bs=256, pl=175, lr=0.005656854249492381, energy=154.17320633387823, time=9.520845784999437, accuracy=0.0310546875, total_cost=321.8079665315009), HistoryEntry(bs=256, pl=150, lr=0.005656854249492381, energy=147.12315597290782, time=9.7458393349998, accuracy=0.0271484375, total_cost=368.740041015421), HistoryEntry(bs=256, pl=125, lr=0.005656854249492381, energy=122.97429905406376, time=11.11174159300026, accuracy=0.0234375, total_cost=450.4780152985222), HistoryEntry(bs=256, pl=100, lr=0.005656854249492381, energy=102.86839930869162, time=29.11806512699968, accuracy=0.03115234375, total_cost=828.1993625800663), HistoryEntry(bs=256, pl=175, lr=0.006788225099390857, energy=154.15570083165568, time=9.50454029499997, accuracy=0.03212890625, total_cost=310.4992118983613), HistoryEntry(bs=256, pl=150, lr=0.006788225099390857, energy=147.03873889108394, time=9.774259951000204, accuracy=0.0193359375, total_cost=519.0996245250437), HistoryEntry(bs=256, pl=125, lr=0.006788225099390857, energy=122.99029449118201, time=11.105981093000082, accuracy=0.01650390625, total_cost=639.4347690407621), HistoryEntry(bs=256, pl=100, lr=0.006788225099390857, energy=102.71251431880901, time=28.962961090000135, accuracy=0.02373046875, total_cost=1080.8266393108954), HistoryEntry(bs=512, pl=175, lr=0.0064, energy=155.35838363431816, time=18.13963056800003, accuracy=0.04423828125, total_cost=863.9119209504936), HistoryEntry(bs=512, pl=150, lr=0.0064, energy=146.8717465874235, time=18.690804988999844, accuracy=0.02138671875, total_cost=1793.9925962342547), HistoryEntry(bs=512, pl=125, lr=0.0064, energy=123.08875334249106, time=21.376406217000294, accuracy=0.0443359375, total_cost=916.5967857834457), HistoryEntry(bs=512, pl=100, lr=0.0064, energy=103.68025803948781, time=52.0712412339999, accuracy=0.040234375, total_cost=2300.174669075095), HistoryEntry(bs=512, pl=175, lr=0.008, energy=155.29671626071058, time=18.133260544000223, accuracy=0.02294921875, total_cost=1664.4325246119129), HistoryEntry(bs=512, pl=150, lr=0.008, energy=147.05802709879453, time=18.716894057999525, accuracy=0.0267578125, total_cost=1436.7170501978792), HistoryEntry(bs=512, pl=125, lr=0.008, energy=123.00045702410289, time=21.40397927899994, accuracy=0.0322265625, total_cost=1262.2675289600813), HistoryEntry(bs=512, pl=100, lr=0.008, energy=104.01211348092335, time=52.08029924699986, accuracy=0.0189453125, total_cost=4891.574791037027), HistoryEntry(bs=512, pl=175, lr=0.0096, energy=155.39406891761777, time=18.126648850000493, accuracy=0.018359375, total_cost=2080.395056671305), HistoryEntry(bs=512, pl=150, lr=0.0096, energy=146.9008731988174, time=18.724064208000527, accuracy=0.0228515625, total_cost=1682.132947744244), HistoryEntry(bs=512, pl=125, lr=0.0096, energy=123.01529578948703, time=21.373666524999862, accuracy=0.0201171875, total_cost=2019.315887157897), HistoryEntry(bs=512, pl=100, lr=0.0096, energy=103.74357066633573, time=52.0414078470003, accuracy=0.02119140625, total_cost=4365.641409431296)]
optimized batch size: 8, learning rate factor: 0.001, power limit: 100
