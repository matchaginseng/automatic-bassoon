Files already downloaded and verified
Files already downloaded and verified
Job(cifar100,shufflenetv2,adam,0.6,bs1024~100)
[Training Loop] Testing batch sizes: [128, 256, 512, 1024]
[Training Loop] Testing power limits: [175, 150, 125, 100]
[Training Loop] Testing learning rates: [0.001, 0.005, 0.01]
[Training Loop] Testing dropout rates: [0.0, 0.25, 0.5]
[Training Loop] Reprofiling at accuracy thresholds [0.5, 0.4, 0.3]
[Training Loop] Model's accuracy 0.0 surpasses threshold 0.0! Reprofiling...
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
Launching Zeus monitor 0...
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6240
Profiling... [256/50048]	Loss: 4.6427
Profiling... [384/50048]	Loss: 4.6745
Profiling... [512/50048]	Loss: 4.6849
Profiling... [640/50048]	Loss: 4.7564
Profiling... [768/50048]	Loss: 4.7847
Profiling... [896/50048]	Loss: 4.7521
Profiling... [1024/50048]	Loss: 4.7050
Profiling... [1152/50048]	Loss: 4.7635
Profiling... [1280/50048]	Loss: 4.6168
Profiling... [1408/50048]	Loss: 4.5929
Profiling... [1536/50048]	Loss: 4.6580
Profiling... [1664/50048]	Loss: 4.5593
Profile done
epoch 1 train time consumed: 6.18s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 128.6470025285053,
                        "time": 2.228481837000004,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19732102.21420677
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6169
Profiling... [256/50048]	Loss: 4.6937
Profiling... [384/50048]	Loss: 4.6901
Profiling... [512/50048]	Loss: 4.7441
Profiling... [640/50048]	Loss: 4.6903
Profiling... [768/50048]	Loss: 4.6913
Profiling... [896/50048]	Loss: 4.5950
Profiling... [1024/50048]	Loss: 4.7235
Profiling... [1152/50048]	Loss: 4.6584
Profiling... [1280/50048]	Loss: 4.5947
Profiling... [1408/50048]	Loss: 4.6428
Profiling... [1536/50048]	Loss: 4.6985
Profiling... [1664/50048]	Loss: 4.6609
Profile done
epoch 1 train time consumed: 3.21s
Validation Epoch: 0, Average loss: 0.0365, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 109.71449376536087,
                        "time": 2.2371905819999256,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19807072.334393386
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6603
Profiling... [256/50048]	Loss: 4.6948
Profiling... [384/50048]	Loss: 4.6477
Profiling... [512/50048]	Loss: 4.6952
Profiling... [640/50048]	Loss: 4.7148
Profiling... [768/50048]	Loss: 4.7643
Profiling... [896/50048]	Loss: 4.6819
Profiling... [1024/50048]	Loss: 4.6165
Profiling... [1152/50048]	Loss: 4.6599
Profiling... [1280/50048]	Loss: 4.6410
Profiling... [1408/50048]	Loss: 4.6340
Profiling... [1536/50048]	Loss: 4.5945
Profiling... [1664/50048]	Loss: 4.6833
Profile done
epoch 1 train time consumed: 3.26s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 114.22505232092703,
                        "time": 2.3051010389999647,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 20408846.455571525
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6433
Profiling... [256/50048]	Loss: 4.5930
Profiling... [384/50048]	Loss: 4.7069
Profiling... [512/50048]	Loss: 4.7209
Profiling... [640/50048]	Loss: 4.7353
Profiling... [768/50048]	Loss: 4.7956
Profiling... [896/50048]	Loss: 4.7128
Profiling... [1024/50048]	Loss: 4.6269
Profiling... [1152/50048]	Loss: 4.6652
Profiling... [1280/50048]	Loss: 4.6493
Profiling... [1408/50048]	Loss: 4.6795
Profiling... [1536/50048]	Loss: 4.6705
Profiling... [1664/50048]	Loss: 4.7003
Profile done
epoch 1 train time consumed: 3.69s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 112.56210819485032,
                        "time": 2.6815356189999875,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 23741488.152044985
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6404
Profiling... [256/50048]	Loss: 4.6759
Profiling... [384/50048]	Loss: 4.6636
Profiling... [512/50048]	Loss: 4.6556
Profiling... [640/50048]	Loss: 4.6397
Profiling... [768/50048]	Loss: 4.7589
Profiling... [896/50048]	Loss: 4.7376
Profiling... [1024/50048]	Loss: 4.8230
Profiling... [1152/50048]	Loss: 4.5873
Profiling... [1280/50048]	Loss: 4.5828
Profiling... [1408/50048]	Loss: 4.6262
Profiling... [1536/50048]	Loss: 4.5755
Profiling... [1664/50048]	Loss: 4.7910
Profile done
epoch 1 train time consumed: 3.19s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 110.77692360846407,
                        "time": 2.239382338999917,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19826597.450042468
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6447
Profiling... [256/50048]	Loss: 4.6627
Profiling... [384/50048]	Loss: 4.7263
Profiling... [512/50048]	Loss: 4.7211
Profiling... [640/50048]	Loss: 4.7860
Profiling... [768/50048]	Loss: 4.7233
Profiling... [896/50048]	Loss: 4.7296
Profiling... [1024/50048]	Loss: 4.7275
Profiling... [1152/50048]	Loss: 4.7223
Profiling... [1280/50048]	Loss: 4.6232
Profiling... [1408/50048]	Loss: 4.6134
Profiling... [1536/50048]	Loss: 4.6515
Profiling... [1664/50048]	Loss: 4.6079
Profile done
epoch 1 train time consumed: 3.24s
Validation Epoch: 0, Average loss: 0.0363, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.13101438557452,
                        "time": 2.233327333000034,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19773254.66052468
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6164
Profiling... [256/50048]	Loss: 4.6924
Profiling... [384/50048]	Loss: 4.6937
Profiling... [512/50048]	Loss: 4.7279
Profiling... [640/50048]	Loss: 4.7147
Profiling... [768/50048]	Loss: 4.7531
Profiling... [896/50048]	Loss: 4.6626
Profiling... [1024/50048]	Loss: 4.6233
Profiling... [1152/50048]	Loss: 4.7964
Profiling... [1280/50048]	Loss: 4.7081
Profiling... [1408/50048]	Loss: 4.6313
Profiling... [1536/50048]	Loss: 4.5411
Profiling... [1664/50048]	Loss: 4.6073
Profile done
epoch 1 train time consumed: 3.24s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 114.52213071275638,
                        "time": 2.271443346999945,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 20110882.933947954
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6285
Profiling... [256/50048]	Loss: 4.6547
Profiling... [384/50048]	Loss: 4.6995
Profiling... [512/50048]	Loss: 4.7405
Profiling... [640/50048]	Loss: 4.7604
Profiling... [768/50048]	Loss: 4.7139
Profiling... [896/50048]	Loss: 4.7270
Profiling... [1024/50048]	Loss: 4.6198
Profiling... [1152/50048]	Loss: 4.6548
Profiling... [1280/50048]	Loss: 4.6646
Profiling... [1408/50048]	Loss: 4.6558
Profiling... [1536/50048]	Loss: 4.6298
Profiling... [1664/50048]	Loss: 4.5922
Profile done
epoch 1 train time consumed: 3.73s
Validation Epoch: 0, Average loss: 0.0365, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.81027969747186,
                        "time": 2.6704820749999953,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 23643792.015048753
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6170
Profiling... [256/50048]	Loss: 4.6408
Profiling... [384/50048]	Loss: 4.6932
Profiling... [512/50048]	Loss: 4.7398
Profiling... [640/50048]	Loss: 4.6814
Profiling... [768/50048]	Loss: 4.8199
Profiling... [896/50048]	Loss: 4.6560
Profiling... [1024/50048]	Loss: 4.6942
Profiling... [1152/50048]	Loss: 4.6726
Profiling... [1280/50048]	Loss: 4.6497
Profiling... [1408/50048]	Loss: 4.5591
Profiling... [1536/50048]	Loss: 4.6022
Profiling... [1664/50048]	Loss: 4.5812
Profile done
epoch 1 train time consumed: 3.11s
Validation Epoch: 0, Average loss: 0.0363, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 112.62053831123488,
                        "time": 2.17068301300003,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19218563.373049695
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.5946
Profiling... [256/50048]	Loss: 4.7007
Profiling... [384/50048]	Loss: 4.6448
Profiling... [512/50048]	Loss: 4.7084
Profiling... [640/50048]	Loss: 4.7680
Profiling... [768/50048]	Loss: 4.7857
Profiling... [896/50048]	Loss: 4.6803
Profiling... [1024/50048]	Loss: 4.6916
Profiling... [1152/50048]	Loss: 4.6346
Profiling... [1280/50048]	Loss: 4.7449
Profiling... [1408/50048]	Loss: 4.6581
Profiling... [1536/50048]	Loss: 4.5813
Profiling... [1664/50048]	Loss: 4.7110
Profile done
epoch 1 train time consumed: 3.14s
Validation Epoch: 0, Average loss: 0.0365, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 114.0112708234743,
                        "time": 2.175478130999977,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19261170.85066076
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6499
Profiling... [256/50048]	Loss: 4.7260
Profiling... [384/50048]	Loss: 4.7012
Profiling... [512/50048]	Loss: 4.6998
Profiling... [640/50048]	Loss: 4.6505
Profiling... [768/50048]	Loss: 4.6284
Profiling... [896/50048]	Loss: 4.7036
Profiling... [1024/50048]	Loss: 4.7604
Profiling... [1152/50048]	Loss: 4.6922
Profiling... [1280/50048]	Loss: 4.6908
Profiling... [1408/50048]	Loss: 4.6295
Profiling... [1536/50048]	Loss: 4.6280
Profiling... [1664/50048]	Loss: 4.6717
Profile done
epoch 1 train time consumed: 3.23s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 114.95265286741963,
                        "time": 2.275699111999984,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 20148612.120162506
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6649
Profiling... [256/50048]	Loss: 4.6693
Profiling... [384/50048]	Loss: 4.6747
Profiling... [512/50048]	Loss: 4.7825
Profiling... [640/50048]	Loss: 4.6532
Profiling... [768/50048]	Loss: 4.7050
Profiling... [896/50048]	Loss: 4.6877
Profiling... [1024/50048]	Loss: 4.7391
Profiling... [1152/50048]	Loss: 4.7010
Profiling... [1280/50048]	Loss: 4.5718
Profiling... [1408/50048]	Loss: 4.6896
Profiling... [1536/50048]	Loss: 4.6169
Profiling... [1664/50048]	Loss: 4.6713
Profile done
epoch 1 train time consumed: 3.67s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 114.38092949339148,
                        "time": 2.687758702999986,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 23796832.580948703
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6477
Profiling... [256/50048]	Loss: 5.8417
Profiling... [384/50048]	Loss: 5.5352
Profiling... [512/50048]	Loss: 5.2249
Profiling... [640/50048]	Loss: 4.8912
Profiling... [768/50048]	Loss: 4.8901
Profiling... [896/50048]	Loss: 4.9596
Profiling... [1024/50048]	Loss: 4.6591
Profiling... [1152/50048]	Loss: 4.7880
Profiling... [1280/50048]	Loss: 4.6892
Profiling... [1408/50048]	Loss: 4.5968
Profiling... [1536/50048]	Loss: 4.7869
Profiling... [1664/50048]	Loss: 4.6901
Profile done
epoch 1 train time consumed: 3.12s
Validation Epoch: 0, Average loss: 0.0370, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.66166206484985,
                        "time": 2.1800477680000085,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19301590.805096164
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6401
Profiling... [256/50048]	Loss: 5.4750
Profiling... [384/50048]	Loss: 6.0885
Profiling... [512/50048]	Loss: 5.8013
Profiling... [640/50048]	Loss: 5.0399
Profiling... [768/50048]	Loss: 4.6815
Profiling... [896/50048]	Loss: 4.9040
Profiling... [1024/50048]	Loss: 5.0427
Profiling... [1152/50048]	Loss: 4.7866
Profiling... [1280/50048]	Loss: 4.6660
Profiling... [1408/50048]	Loss: 4.5863
Profiling... [1536/50048]	Loss: 4.7910
Profiling... [1664/50048]	Loss: 4.6838
Profile done
epoch 1 train time consumed: 3.12s
Validation Epoch: 0, Average loss: 0.0373, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.66408817595406,
                        "time": 2.1753988259999915,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19260540.50505681
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6061
Profiling... [256/50048]	Loss: 5.7748
Profiling... [384/50048]	Loss: 6.0709
Profiling... [512/50048]	Loss: 5.0413
Profiling... [640/50048]	Loss: 4.8567
Profiling... [768/50048]	Loss: 4.6499
Profiling... [896/50048]	Loss: 4.5769
Profiling... [1024/50048]	Loss: 4.6339
Profiling... [1152/50048]	Loss: 4.7604
Profiling... [1280/50048]	Loss: 4.9054
Profiling... [1408/50048]	Loss: 4.6699
Profiling... [1536/50048]	Loss: 4.7810
Profiling... [1664/50048]	Loss: 4.8037
Profile done
epoch 1 train time consumed: 3.26s
Validation Epoch: 0, Average loss: 0.0371, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.35665270431504,
                        "time": 2.2859082639999997,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 20239048.725204613
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6401
Profiling... [256/50048]	Loss: 5.3391
Profiling... [384/50048]	Loss: 5.8315
Profiling... [512/50048]	Loss: 5.4296
Profiling... [640/50048]	Loss: 5.4644
Profiling... [768/50048]	Loss: 5.0147
Profiling... [896/50048]	Loss: 4.9973
Profiling... [1024/50048]	Loss: 4.7577
Profiling... [1152/50048]	Loss: 4.7017
Profiling... [1280/50048]	Loss: 4.6989
Profiling... [1408/50048]	Loss: 4.7317
Profiling... [1536/50048]	Loss: 5.0102
Profiling... [1664/50048]	Loss: 4.6476
Profile done
epoch 1 train time consumed: 3.72s
Validation Epoch: 0, Average loss: 0.0372, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.87911806397553,
                        "time": 2.7225475039999765,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 24104913.65595857
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6283
Profiling... [256/50048]	Loss: 5.8690
Profiling... [384/50048]	Loss: 5.6312
Profiling... [512/50048]	Loss: 5.3514
Profiling... [640/50048]	Loss: 5.0139
Profiling... [768/50048]	Loss: 5.2796
Profiling... [896/50048]	Loss: 4.9683
Profiling... [1024/50048]	Loss: 5.1225
Profiling... [1152/50048]	Loss: 4.6958
Profiling... [1280/50048]	Loss: 5.0279
Profiling... [1408/50048]	Loss: 4.5986
Profiling... [1536/50048]	Loss: 4.6191
Profiling... [1664/50048]	Loss: 4.6769
Profile done
epoch 1 train time consumed: 3.23s
Validation Epoch: 0, Average loss: 0.0367, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 114.27422401928453,
                        "time": 2.176046035000013,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19266227.869228475
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6259
Profiling... [256/50048]	Loss: 5.7005
Profiling... [384/50048]	Loss: 5.8178
Profiling... [512/50048]	Loss: 5.5832
Profiling... [640/50048]	Loss: 5.1903
Profiling... [768/50048]	Loss: 4.8893
Profiling... [896/50048]	Loss: 4.6130
Profiling... [1024/50048]	Loss: 4.7514
Profiling... [1152/50048]	Loss: 4.8957
Profiling... [1280/50048]	Loss: 4.7346
Profiling... [1408/50048]	Loss: 4.6818
Profiling... [1536/50048]	Loss: 4.6279
Profiling... [1664/50048]	Loss: 4.7703
Profile done
epoch 1 train time consumed: 3.11s
Validation Epoch: 0, Average loss: 0.0370, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 114.99664955407471,
                        "time": 2.1741801719999785,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19249787.346744925
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6637
Profiling... [256/50048]	Loss: 5.7793
Profiling... [384/50048]	Loss: 5.9834
Profiling... [512/50048]	Loss: 5.1583
Profiling... [640/50048]	Loss: 5.1907
Profiling... [768/50048]	Loss: 4.9570
Profiling... [896/50048]	Loss: 4.8138
Profiling... [1024/50048]	Loss: 4.6884
Profiling... [1152/50048]	Loss: 4.8301
Profiling... [1280/50048]	Loss: 5.0234
Profiling... [1408/50048]	Loss: 4.7178
Profiling... [1536/50048]	Loss: 4.8190
Profiling... [1664/50048]	Loss: 4.6538
Profile done
epoch 1 train time consumed: 3.23s
Validation Epoch: 0, Average loss: 0.0367, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.53673085738919,
                        "time": 2.297336636999944,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 20340254.54139964
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6131
Profiling... [256/50048]	Loss: 5.9593
Profiling... [384/50048]	Loss: 5.9958
Profiling... [512/50048]	Loss: 5.3237
Profiling... [640/50048]	Loss: 5.0384
Profiling... [768/50048]	Loss: 4.6767
Profiling... [896/50048]	Loss: 4.6029
Profiling... [1024/50048]	Loss: 4.9463
Profiling... [1152/50048]	Loss: 4.7878
Profiling... [1280/50048]	Loss: 4.5129
Profiling... [1408/50048]	Loss: 4.7075
Profiling... [1536/50048]	Loss: 5.0070
Profiling... [1664/50048]	Loss: 4.6010
Profile done
epoch 1 train time consumed: 3.72s
Validation Epoch: 0, Average loss: 0.0370, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.20311638747569,
                        "time": 2.7121512979999807,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 24012912.069823746
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6230
Profiling... [256/50048]	Loss: 5.9791
Profiling... [384/50048]	Loss: 5.7316
Profiling... [512/50048]	Loss: 5.5460
Profiling... [640/50048]	Loss: 5.0994
Profiling... [768/50048]	Loss: 5.2819
Profiling... [896/50048]	Loss: 4.9249
Profiling... [1024/50048]	Loss: 4.9133
Profiling... [1152/50048]	Loss: 4.7105
Profiling... [1280/50048]	Loss: 4.8005
Profiling... [1408/50048]	Loss: 4.6056
Profiling... [1536/50048]	Loss: 4.5822
Profiling... [1664/50048]	Loss: 4.5595
Profile done
epoch 1 train time consumed: 3.24s
Validation Epoch: 0, Average loss: 0.0378, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.69728468992889,
                        "time": 2.185761568999851,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19352293.800871365
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6198
Profiling... [256/50048]	Loss: 5.6557
Profiling... [384/50048]	Loss: 6.0637
Profiling... [512/50048]	Loss: 5.3683
Profiling... [640/50048]	Loss: 5.0481
Profiling... [768/50048]	Loss: 5.1295
Profiling... [896/50048]	Loss: 4.9873
Profiling... [1024/50048]	Loss: 4.7569
Profiling... [1152/50048]	Loss: 4.9769
Profiling... [1280/50048]	Loss: 4.7005
Profiling... [1408/50048]	Loss: 4.7470
Profiling... [1536/50048]	Loss: 4.7881
Profiling... [1664/50048]	Loss: 4.7373
Profile done
epoch 1 train time consumed: 3.12s
Validation Epoch: 0, Average loss: 0.0367, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.27817909461444,
                        "time": 2.178845117000037,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19291120.91714384
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6236
Profiling... [256/50048]	Loss: 5.5796
Profiling... [384/50048]	Loss: 6.2532
Profiling... [512/50048]	Loss: 5.3283
Profiling... [640/50048]	Loss: 5.1109
Profiling... [768/50048]	Loss: 5.3056
Profiling... [896/50048]	Loss: 4.6493
Profiling... [1024/50048]	Loss: 4.9466
Profiling... [1152/50048]	Loss: 4.8940
Profiling... [1280/50048]	Loss: 4.9232
Profiling... [1408/50048]	Loss: 4.7127
Profiling... [1536/50048]	Loss: 4.6904
Profiling... [1664/50048]	Loss: 4.8535
Profile done
epoch 1 train time consumed: 3.32s
Validation Epoch: 0, Average loss: 0.0373, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.71073518433484,
                        "time": 2.2927429879998726,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 20299603.27184635
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6211
Profiling... [256/50048]	Loss: 5.4268
Profiling... [384/50048]	Loss: 5.7387
Profiling... [512/50048]	Loss: 5.5541
Profiling... [640/50048]	Loss: 5.5091
Profiling... [768/50048]	Loss: 5.1414
Profiling... [896/50048]	Loss: 4.8661
Profiling... [1024/50048]	Loss: 4.8408
Profiling... [1152/50048]	Loss: 4.7612
Profiling... [1280/50048]	Loss: 4.7562
Profiling... [1408/50048]	Loss: 4.7870
Profiling... [1536/50048]	Loss: 4.7461
Profiling... [1664/50048]	Loss: 4.6689
Profile done
epoch 1 train time consumed: 3.72s
Validation Epoch: 0, Average loss: 0.0367, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.39603538911855,
                        "time": 2.727329004000012,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 24147319.41995966
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6146
Profiling... [256/50048]	Loss: 7.6339
Profiling... [384/50048]	Loss: 7.6149
Profiling... [512/50048]	Loss: 5.1719
Profiling... [640/50048]	Loss: 5.5766
Profiling... [768/50048]	Loss: 5.8772
Profiling... [896/50048]	Loss: 5.6202
Profiling... [1024/50048]	Loss: 5.2377
Profiling... [1152/50048]	Loss: 4.9317
Profiling... [1280/50048]	Loss: 4.6967
Profiling... [1408/50048]	Loss: 4.9174
Profiling... [1536/50048]	Loss: 4.6758
Profiling... [1664/50048]	Loss: 4.7021
Profile done
epoch 1 train time consumed: 3.13s
Validation Epoch: 0, Average loss: 0.0384, Accuracy: 0.0126
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.02629333523277,
                        "time": 2.170495919000132,
                        "accuracy": 0.012559335443037974,
                        "total_cost": 15131630.636395594
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6325
Profiling... [256/50048]	Loss: 7.8121
Profiling... [384/50048]	Loss: 6.3571
Profiling... [512/50048]	Loss: 6.3465
Profiling... [640/50048]	Loss: 5.9330
Profiling... [768/50048]	Loss: 5.1344
Profiling... [896/50048]	Loss: 5.4165
Profiling... [1024/50048]	Loss: 5.2345
Profiling... [1152/50048]	Loss: 5.0677
Profiling... [1280/50048]	Loss: 4.7146
Profiling... [1408/50048]	Loss: 4.7991
Profiling... [1536/50048]	Loss: 4.7696
Profiling... [1664/50048]	Loss: 4.6153
Profile done
epoch 1 train time consumed: 3.38s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.49359022447815,
                        "time": 2.313173001999985,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 20480462.162163127
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6130
Profiling... [256/50048]	Loss: 8.4966
Profiling... [384/50048]	Loss: 7.3404
Profiling... [512/50048]	Loss: 6.6811
Profiling... [640/50048]	Loss: 5.0845
Profiling... [768/50048]	Loss: 5.0928
Profiling... [896/50048]	Loss: 4.8869
Profiling... [1024/50048]	Loss: 5.0594
Profiling... [1152/50048]	Loss: 4.7640
Profiling... [1280/50048]	Loss: 4.8696
Profiling... [1408/50048]	Loss: 4.6154
Profiling... [1536/50048]	Loss: 4.6766
Profiling... [1664/50048]	Loss: 4.9319
Profile done
epoch 1 train time consumed: 3.32s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.82488588569096,
                        "time": 2.2954113879998204,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 20323242.133952964
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6104
Profiling... [256/50048]	Loss: 8.0119
Profiling... [384/50048]	Loss: 6.5593
Profiling... [512/50048]	Loss: 5.9935
Profiling... [640/50048]	Loss: 5.1850
Profiling... [768/50048]	Loss: 4.8211
Profiling... [896/50048]	Loss: 4.7998
Profiling... [1024/50048]	Loss: 5.2734
Profiling... [1152/50048]	Loss: 5.3577
Profiling... [1280/50048]	Loss: 5.3711
Profiling... [1408/50048]	Loss: 4.7981
Profiling... [1536/50048]	Loss: 4.9838
Profiling... [1664/50048]	Loss: 4.6193
Profile done
epoch 1 train time consumed: 3.82s
Validation Epoch: 0, Average loss: 0.0404, Accuracy: 0.0097
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.58227725089175,
                        "time": 2.720762286000081,
                        "accuracy": 0.009691455696202531,
                        "total_cost": 24580820.82228257
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6172
Profiling... [256/50048]	Loss: 7.7577
Profiling... [384/50048]	Loss: 6.2960
Profiling... [512/50048]	Loss: 5.2639
Profiling... [640/50048]	Loss: 5.7504
Profiling... [768/50048]	Loss: 5.2078
Profiling... [896/50048]	Loss: 5.1153
Profiling... [1024/50048]	Loss: 5.0428
Profiling... [1152/50048]	Loss: 5.0980
Profiling... [1280/50048]	Loss: 4.8169
Profiling... [1408/50048]	Loss: 4.7776
Profiling... [1536/50048]	Loss: 4.5822
Profiling... [1664/50048]	Loss: 4.6159
Profile done
epoch 1 train time consumed: 3.17s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.21100424763182,
                        "time": 2.178619742000137,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19289118.086176965
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6803
Profiling... [256/50048]	Loss: 7.7124
Profiling... [384/50048]	Loss: 7.2853
Profiling... [512/50048]	Loss: 6.0406
Profiling... [640/50048]	Loss: 5.0970
Profiling... [768/50048]	Loss: 5.0827
Profiling... [896/50048]	Loss: 4.7887
Profiling... [1024/50048]	Loss: 4.7346
Profiling... [1152/50048]	Loss: 4.6114
Profiling... [1280/50048]	Loss: 4.7960
Profiling... [1408/50048]	Loss: 4.6989
Profiling... [1536/50048]	Loss: 4.7474
Profiling... [1664/50048]	Loss: 4.5616
Profile done
epoch 1 train time consumed: 3.22s
Validation Epoch: 0, Average loss: 0.0384, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.64791706404488,
                        "time": 2.178192564000028,
                        "accuracy": 0.009790348101265823,
                        "total_cost": 19480185.902873006
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6059
Profiling... [256/50048]	Loss: 7.2573
Profiling... [384/50048]	Loss: 6.5392
Profiling... [512/50048]	Loss: 4.8500
Profiling... [640/50048]	Loss: 5.1654
Profiling... [768/50048]	Loss: 5.1107
Profiling... [896/50048]	Loss: 5.7231
Profiling... [1024/50048]	Loss: 5.6133
Profiling... [1152/50048]	Loss: 4.8613
Profiling... [1280/50048]	Loss: 4.8472
Profiling... [1408/50048]	Loss: 5.0629
Profiling... [1536/50048]	Loss: 5.1892
Profiling... [1664/50048]	Loss: 4.9889
Profile done
epoch 1 train time consumed: 3.27s
Validation Epoch: 0, Average loss: 0.0827, Accuracy: 0.0165
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.93485830696288,
                        "time": 2.3068366269999387,
                        "accuracy": 0.01651503164556962,
                        "total_cost": 12230187.055404358
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6221
Profiling... [256/50048]	Loss: 7.7120
Profiling... [384/50048]	Loss: 7.0102
Profiling... [512/50048]	Loss: 5.8928
Profiling... [640/50048]	Loss: 5.2239
Profiling... [768/50048]	Loss: 4.7230
Profiling... [896/50048]	Loss: 5.2222
Profiling... [1024/50048]	Loss: 4.8273
Profiling... [1152/50048]	Loss: 4.5960
Profiling... [1280/50048]	Loss: 5.1969
Profiling... [1408/50048]	Loss: 5.3427
Profiling... [1536/50048]	Loss: 4.5846
Profiling... [1664/50048]	Loss: 4.5658
Profile done
epoch 1 train time consumed: 3.75s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0108
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.6972551721628,
                        "time": 2.721122710999907,
                        "accuracy": 0.010779272151898734,
                        "total_cost": 22103129.69831164
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6225
Profiling... [256/50048]	Loss: 7.6547
Profiling... [384/50048]	Loss: 6.9335
Profiling... [512/50048]	Loss: 5.7445
Profiling... [640/50048]	Loss: 5.5521
Profiling... [768/50048]	Loss: 4.8904
Profiling... [896/50048]	Loss: 4.9996
Profiling... [1024/50048]	Loss: 4.9803
Profiling... [1152/50048]	Loss: 5.3135
Profiling... [1280/50048]	Loss: 4.7626
Profiling... [1408/50048]	Loss: 4.8549
Profiling... [1536/50048]	Loss: 5.1666
Profiling... [1664/50048]	Loss: 4.6957
Profile done
epoch 1 train time consumed: 3.22s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.3521937542424,
                        "time": 2.1844929179999326,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19341133.753168184
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6155
Profiling... [256/50048]	Loss: 7.1110
Profiling... [384/50048]	Loss: 6.5431
Profiling... [512/50048]	Loss: 4.9373
Profiling... [640/50048]	Loss: 5.0867
Profiling... [768/50048]	Loss: 5.9146
Profiling... [896/50048]	Loss: 5.1700
Profiling... [1024/50048]	Loss: 4.7810
Profiling... [1152/50048]	Loss: 4.9582
Profiling... [1280/50048]	Loss: 4.9404
Profiling... [1408/50048]	Loss: 4.7434
Profiling... [1536/50048]	Loss: 5.1530
Profiling... [1664/50048]	Loss: 4.6447
Profile done
epoch 1 train time consumed: 3.15s
Validation Epoch: 0, Average loss: 0.1171, Accuracy: 0.0076
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.76797352631034,
                        "time": 2.1876251819999197,
                        "accuracy": 0.0076147151898734175,
                        "total_cost": 25154431.533670366
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6059
Profiling... [256/50048]	Loss: 7.5950
Profiling... [384/50048]	Loss: 6.7195
Profiling... [512/50048]	Loss: 6.4226
Profiling... [640/50048]	Loss: 4.6791
Profiling... [768/50048]	Loss: 4.7008
Profiling... [896/50048]	Loss: 4.7106
Profiling... [1024/50048]	Loss: 4.6462
Profiling... [1152/50048]	Loss: 4.6490
Profiling... [1280/50048]	Loss: 4.5608
Profiling... [1408/50048]	Loss: 4.6551
Profiling... [1536/50048]	Loss: 4.5056
Profiling... [1664/50048]	Loss: 5.5576
Profile done
epoch 1 train time consumed: 3.27s
Validation Epoch: 0, Average loss: 0.0378, Accuracy: 0.0103
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 116.04834103501432,
                        "time": 2.306322909000073,
                        "accuracy": 0.010284810126582278,
                        "total_cost": 19634497.334016602
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6235
Profiling... [256/50048]	Loss: 6.7997
Profiling... [384/50048]	Loss: 6.2095
Profiling... [512/50048]	Loss: 5.4471
Profiling... [640/50048]	Loss: 5.7036
Profiling... [768/50048]	Loss: 6.7546
Profiling... [896/50048]	Loss: 5.8333
Profiling... [1024/50048]	Loss: 7.3941
Profiling... [1152/50048]	Loss: 5.8989
Profiling... [1280/50048]	Loss: 5.1545
Profiling... [1408/50048]	Loss: 5.3425
Profiling... [1536/50048]	Loss: 4.8244
Profiling... [1664/50048]	Loss: 4.6644
Profile done
epoch 1 train time consumed: 3.75s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.82804421184525,
                        "time": 2.729754465000042,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 24168853.674351025
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6538
Profiling... [512/50176]	Loss: 4.6428
Profiling... [768/50176]	Loss: 4.6525
Profiling... [1024/50176]	Loss: 4.6487
Profiling... [1280/50176]	Loss: 4.6733
Profiling... [1536/50176]	Loss: 4.6434
Profiling... [1792/50176]	Loss: 4.6760
Profiling... [2048/50176]	Loss: 4.6352
Profiling... [2304/50176]	Loss: 4.6887
Profiling... [2560/50176]	Loss: 4.6053
Profiling... [2816/50176]	Loss: 4.6124
Profiling... [3072/50176]	Loss: 4.5423
Profiling... [3328/50176]	Loss: 4.5875
Profile done
epoch 1 train time consumed: 3.54s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.66792304250578,
                        "time": 2.4109826740000244,
                        "accuracy": 0.009765625,
                        "total_cost": 21616683.07498994
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6208
Profiling... [512/50176]	Loss: 4.6498
Profiling... [768/50176]	Loss: 4.7312
Profiling... [1024/50176]	Loss: 4.6994
Profiling... [1280/50176]	Loss: 4.7650
Profiling... [1536/50176]	Loss: 4.7178
Profiling... [1792/50176]	Loss: 4.6475
Profiling... [2048/50176]	Loss: 4.6000
Profiling... [2304/50176]	Loss: 4.5883
Profiling... [2560/50176]	Loss: 4.5497
Profiling... [2816/50176]	Loss: 4.5949
Profiling... [3072/50176]	Loss: 4.5677
Profiling... [3328/50176]	Loss: 4.5487
Profile done
epoch 1 train time consumed: 3.55s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 116.13565417493237,
                        "time": 2.423176395999917,
                        "accuracy": 0.009765625,
                        "total_cost": 21726069.06756691
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6486
Profiling... [512/50176]	Loss: 4.6522
Profiling... [768/50176]	Loss: 4.6631
Profiling... [1024/50176]	Loss: 4.6719
Profiling... [1280/50176]	Loss: 4.6306
Profiling... [1536/50176]	Loss: 4.7546
Profiling... [1792/50176]	Loss: 4.6309
Profiling... [2048/50176]	Loss: 4.6255
Profiling... [2304/50176]	Loss: 4.5897
Profiling... [2560/50176]	Loss: 4.5964
Profiling... [2816/50176]	Loss: 4.5704
Profiling... [3072/50176]	Loss: 4.5617
Profiling... [3328/50176]	Loss: 4.5387
Profile done
epoch 1 train time consumed: 3.71s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 116.41464737527144,
                        "time": 2.631235671000013,
                        "accuracy": 0.009765625,
                        "total_cost": 23591554.908047512
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6285
Profiling... [512/50176]	Loss: 4.6341
Profiling... [768/50176]	Loss: 4.6719
Profiling... [1024/50176]	Loss: 4.6777
Profiling... [1280/50176]	Loss: 4.6778
Profiling... [1536/50176]	Loss: 4.6325
Profiling... [1792/50176]	Loss: 4.5500
Profiling... [2048/50176]	Loss: 4.6066
Profiling... [2304/50176]	Loss: 4.6299
Profiling... [2560/50176]	Loss: 4.5432
Profiling... [2816/50176]	Loss: 4.5522
Profiling... [3072/50176]	Loss: 4.6877
Profiling... [3328/50176]	Loss: 4.4901
Profile done
epoch 1 train time consumed: 4.44s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 116.15271505447477,
                        "time": 3.2085714749998715,
                        "accuracy": 0.009765625,
                        "total_cost": 28767881.85155815
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6344
Profiling... [512/50176]	Loss: 4.6702
Profiling... [768/50176]	Loss: 4.7022
Profiling... [1024/50176]	Loss: 4.6745
Profiling... [1280/50176]	Loss: 4.6367
Profiling... [1536/50176]	Loss: 4.6227
Profiling... [1792/50176]	Loss: 4.6685
Profiling... [2048/50176]	Loss: 4.5490
Profiling... [2304/50176]	Loss: 4.6035
Profiling... [2560/50176]	Loss: 4.6761
Profiling... [2816/50176]	Loss: 4.5103
Profiling... [3072/50176]	Loss: 4.6187
Profiling... [3328/50176]	Loss: 4.5387
Profile done
epoch 1 train time consumed: 3.60s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.93944211283113,
                        "time": 2.4190870879999693,
                        "accuracy": 0.009765625,
                        "total_cost": 21689380.249978863
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6409
Profiling... [512/50176]	Loss: 4.6409
Profiling... [768/50176]	Loss: 4.6459
Profiling... [1024/50176]	Loss: 4.6776
Profiling... [1280/50176]	Loss: 4.7347
Profiling... [1536/50176]	Loss: 4.6271
Profiling... [1792/50176]	Loss: 4.5837
Profiling... [2048/50176]	Loss: 4.5978
Profiling... [2304/50176]	Loss: 4.5877
Profiling... [2560/50176]	Loss: 4.6290
Profiling... [2816/50176]	Loss: 4.6492
Profiling... [3072/50176]	Loss: 4.4932
Profiling... [3328/50176]	Loss: 4.6412
Profile done
epoch 1 train time consumed: 3.54s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 116.33389080525642,
                        "time": 2.436243486999956,
                        "accuracy": 0.009765625,
                        "total_cost": 21843252.62892974
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6489
Profiling... [512/50176]	Loss: 4.6750
Profiling... [768/50176]	Loss: 4.7172
Profiling... [1024/50176]	Loss: 4.6274
Profiling... [1280/50176]	Loss: 4.6118
Profiling... [1536/50176]	Loss: 4.5844
Profiling... [1792/50176]	Loss: 4.6102
Profiling... [2048/50176]	Loss: 4.6208
Profiling... [2304/50176]	Loss: 4.6587
Profiling... [2560/50176]	Loss: 4.6449
Profiling... [2816/50176]	Loss: 4.5801
Profiling... [3072/50176]	Loss: 4.6459
Profiling... [3328/50176]	Loss: 4.5459
Profile done
epoch 1 train time consumed: 3.78s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 116.56004764722815,
                        "time": 2.6545272900000327,
                        "accuracy": 0.009765625,
                        "total_cost": 23800406.40396334
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6061
Profiling... [512/50176]	Loss: 4.6621
Profiling... [768/50176]	Loss: 4.7200
Profiling... [1024/50176]	Loss: 4.7050
Profiling... [1280/50176]	Loss: 4.7001
Profiling... [1536/50176]	Loss: 4.6521
Profiling... [1792/50176]	Loss: 4.6778
Profiling... [2048/50176]	Loss: 4.6184
Profiling... [2304/50176]	Loss: 4.5986
Profiling... [2560/50176]	Loss: 4.5684
Profiling... [2816/50176]	Loss: 4.4657
Profiling... [3072/50176]	Loss: 4.4881
Profiling... [3328/50176]	Loss: 4.5438
Profile done
epoch 1 train time consumed: 4.50s
Validation Epoch: 0, Average loss: 0.0182, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 116.34316554082471,
                        "time": 3.2062607280001885,
                        "accuracy": 0.009765625,
                        "total_cost": 28747195.0808411
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6209
Profiling... [512/50176]	Loss: 4.6710
Profiling... [768/50176]	Loss: 4.6788
Profiling... [1024/50176]	Loss: 4.6677
Profiling... [1280/50176]	Loss: 4.6740
Profiling... [1536/50176]	Loss: 4.6499
Profiling... [1792/50176]	Loss: 4.5990
Profiling... [2048/50176]	Loss: 4.5659
Profiling... [2304/50176]	Loss: 4.5597
Profiling... [2560/50176]	Loss: 4.5069
Profiling... [2816/50176]	Loss: 4.5884
Profiling... [3072/50176]	Loss: 4.4921
Profiling... [3328/50176]	Loss: 4.5577
Profile done
epoch 1 train time consumed: 3.54s
Validation Epoch: 0, Average loss: 0.0182, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 116.12020629927885,
                        "time": 2.4110220260001824,
                        "accuracy": 0.009765625,
                        "total_cost": 21617091.733764257
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6432
Profiling... [512/50176]	Loss: 4.6868
Profiling... [768/50176]	Loss: 4.6522
Profiling... [1024/50176]	Loss: 4.6580
Profiling... [1280/50176]	Loss: 4.6555
Profiling... [1536/50176]	Loss: 4.6441
Profiling... [1792/50176]	Loss: 4.5984
Profiling... [2048/50176]	Loss: 4.6764
Profiling... [2304/50176]	Loss: 4.6831
Profiling... [2560/50176]	Loss: 4.6260
Profiling... [2816/50176]	Loss: 4.5588
Profiling... [3072/50176]	Loss: 4.4995
Profiling... [3328/50176]	Loss: 4.5440
Profile done
epoch 1 train time consumed: 3.54s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 116.53266772947471,
                        "time": 2.4196349730000293,
                        "accuracy": 0.009765625,
                        "total_cost": 21694366.04381903
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6262
Profiling... [512/50176]	Loss: 4.6459
Profiling... [768/50176]	Loss: 4.6013
Profiling... [1024/50176]	Loss: 4.7038
Profiling... [1280/50176]	Loss: 4.6459
Profiling... [1536/50176]	Loss: 4.6272
Profiling... [1792/50176]	Loss: 4.7095
Profiling... [2048/50176]	Loss: 4.6294
Profiling... [2304/50176]	Loss: 4.5630
Profiling... [2560/50176]	Loss: 4.6004
Profiling... [2816/50176]	Loss: 4.5399
Profiling... [3072/50176]	Loss: 4.5022
Profiling... [3328/50176]	Loss: 4.5103
Profile done
epoch 1 train time consumed: 3.80s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 116.72966411427632,
                        "time": 2.65269073200011,
                        "accuracy": 0.009765625,
                        "total_cost": 23783962.92086603
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6242
Profiling... [512/50176]	Loss: 4.6566
Profiling... [768/50176]	Loss: 4.6667
Profiling... [1024/50176]	Loss: 4.6917
Profiling... [1280/50176]	Loss: 4.6849
Profiling... [1536/50176]	Loss: 4.6787
Profiling... [1792/50176]	Loss: 4.6767
Profiling... [2048/50176]	Loss: 4.5670
Profiling... [2304/50176]	Loss: 4.5568
Profiling... [2560/50176]	Loss: 4.6464
Profiling... [2816/50176]	Loss: 4.5991
Profiling... [3072/50176]	Loss: 4.5814
Profiling... [3328/50176]	Loss: 4.5192
Profile done
epoch 1 train time consumed: 4.51s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 116.5185380833571,
                        "time": 3.2067450499998813,
                        "accuracy": 0.009765625,
                        "total_cost": 28751566.284554817
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6344
Profiling... [512/50176]	Loss: 5.6441
Profiling... [768/50176]	Loss: 5.8547
Profiling... [1024/50176]	Loss: 4.9777
Profiling... [1280/50176]	Loss: 4.7531
Profiling... [1536/50176]	Loss: 4.9192
Profiling... [1792/50176]	Loss: 4.8167
Profiling... [2048/50176]	Loss: 4.6115
Profiling... [2304/50176]	Loss: 4.6540
Profiling... [2560/50176]	Loss: 4.6609
Profiling... [2816/50176]	Loss: 4.6477
Profiling... [3072/50176]	Loss: 4.6789
Profiling... [3328/50176]	Loss: 4.6282
Profile done
epoch 1 train time consumed: 3.55s
Validation Epoch: 0, Average loss: 0.0184, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 116.3052739755562,
                        "time": 2.416361745999893,
                        "accuracy": 0.009765625,
                        "total_cost": 21664990.267641544
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6395
Profiling... [512/50176]	Loss: 5.6239
Profiling... [768/50176]	Loss: 5.2808
Profiling... [1024/50176]	Loss: 4.8740
Profiling... [1280/50176]	Loss: 5.1048
Profiling... [1536/50176]	Loss: 4.6718
Profiling... [1792/50176]	Loss: 5.0930
Profiling... [2048/50176]	Loss: 4.7127
Profiling... [2304/50176]	Loss: 4.8806
Profiling... [2560/50176]	Loss: 4.6877
Profiling... [2816/50176]	Loss: 4.5892
Profiling... [3072/50176]	Loss: 4.6099
Profiling... [3328/50176]	Loss: 4.6014
Profile done
epoch 1 train time consumed: 3.53s
Validation Epoch: 0, Average loss: 0.0184, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 116.62396155055225,
                        "time": 2.4293314389999523,
                        "accuracy": 0.009765625,
                        "total_cost": 21781315.58816395
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6557
Profiling... [512/50176]	Loss: 5.6832
Profiling... [768/50176]	Loss: 5.4436
Profiling... [1024/50176]	Loss: 4.9574
Profiling... [1280/50176]	Loss: 4.9311
Profiling... [1536/50176]	Loss: 4.9136
Profiling... [1792/50176]	Loss: 4.8684
Profiling... [2048/50176]	Loss: 4.7076
Profiling... [2304/50176]	Loss: 4.6157
Profiling... [2560/50176]	Loss: 4.6810
Profiling... [2816/50176]	Loss: 4.5875
Profiling... [3072/50176]	Loss: 4.6904
Profiling... [3328/50176]	Loss: 4.5312
Profile done
epoch 1 train time consumed: 3.74s
Validation Epoch: 0, Average loss: 0.0186, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 116.8542605362101,
                        "time": 2.6310132220000924,
                        "accuracy": 0.009765625,
                        "total_cost": 23589619.65847214
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6379
Profiling... [512/50176]	Loss: 5.7427
Profiling... [768/50176]	Loss: 5.2587
Profiling... [1024/50176]	Loss: 4.7371
Profiling... [1280/50176]	Loss: 4.9173
Profiling... [1536/50176]	Loss: 4.7819
Profiling... [1792/50176]	Loss: 4.7603
Profiling... [2048/50176]	Loss: 5.1952
Profiling... [2304/50176]	Loss: 4.8177
Profiling... [2560/50176]	Loss: 4.6847
Profiling... [2816/50176]	Loss: 4.6707
Profiling... [3072/50176]	Loss: 4.7276
Profiling... [3328/50176]	Loss: 4.6290
Profile done
epoch 1 train time consumed: 4.43s
Validation Epoch: 0, Average loss: 0.0184, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 116.65069244057672,
                        "time": 3.200295459000017,
                        "accuracy": 0.009765625,
                        "total_cost": 28693761.126723062
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6217
Profiling... [512/50176]	Loss: 5.6244
Profiling... [768/50176]	Loss: 5.5201
Profiling... [1024/50176]	Loss: 5.0389
Profiling... [1280/50176]	Loss: 4.9013
Profiling... [1536/50176]	Loss: 4.7303
Profiling... [1792/50176]	Loss: 4.9100
Profiling... [2048/50176]	Loss: 4.9794
Profiling... [2304/50176]	Loss: 5.0321
Profiling... [2560/50176]	Loss: 4.8085
Profiling... [2816/50176]	Loss: 4.6816
Profiling... [3072/50176]	Loss: 4.7619
Profiling... [3328/50176]	Loss: 4.7031
Profile done
epoch 1 train time consumed: 3.59s
Validation Epoch: 0, Average loss: 0.0184, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 116.48593722265082,
                        "time": 2.4256580619999113,
                        "accuracy": 0.009765625,
                        "total_cost": 21748363.054219175
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6472
Profiling... [512/50176]	Loss: 5.5423
Profiling... [768/50176]	Loss: 5.5411
Profiling... [1024/50176]	Loss: 4.9984
Profiling... [1280/50176]	Loss: 4.8992
Profiling... [1536/50176]	Loss: 4.9070
Profiling... [1792/50176]	Loss: 4.8724
Profiling... [2048/50176]	Loss: 4.6479
Profiling... [2304/50176]	Loss: 4.9078
Profiling... [2560/50176]	Loss: 4.7219
Profiling... [2816/50176]	Loss: 4.6324
Profiling... [3072/50176]	Loss: 4.6732
Profiling... [3328/50176]	Loss: 4.5337
Profile done
epoch 1 train time consumed: 3.58s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 116.76338256344046,
                        "time": 2.4435108039999704,
                        "accuracy": 0.009765625,
                        "total_cost": 21908464.80828417
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6251
Profiling... [512/50176]	Loss: 5.4058
Profiling... [768/50176]	Loss: 5.5614
Profiling... [1024/50176]	Loss: 5.1222
Profiling... [1280/50176]	Loss: 4.9031
Profiling... [1536/50176]	Loss: 4.8961
Profiling... [1792/50176]	Loss: 4.6710
Profiling... [2048/50176]	Loss: 4.7445
Profiling... [2304/50176]	Loss: 4.8691
Profiling... [2560/50176]	Loss: 4.6416
Profiling... [2816/50176]	Loss: 4.8264
Profiling... [3072/50176]	Loss: 4.7027
Profiling... [3328/50176]	Loss: 4.6843
Profile done
epoch 1 train time consumed: 3.77s
Validation Epoch: 0, Average loss: 0.0186, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 116.93835386829218,
                        "time": 2.6466733140000542,
                        "accuracy": 0.009765625,
                        "total_cost": 23730039.17161348
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6398
Profiling... [512/50176]	Loss: 5.5955
Profiling... [768/50176]	Loss: 5.4758
Profiling... [1024/50176]	Loss: 4.9852
Profiling... [1280/50176]	Loss: 4.9386
Profiling... [1536/50176]	Loss: 4.8820
Profiling... [1792/50176]	Loss: 4.7993
Profiling... [2048/50176]	Loss: 4.7821
Profiling... [2304/50176]	Loss: 4.6087
Profiling... [2560/50176]	Loss: 4.7208
Profiling... [2816/50176]	Loss: 4.6405
Profiling... [3072/50176]	Loss: 4.6161
Profiling... [3328/50176]	Loss: 4.6919
Profile done
epoch 1 train time consumed: 4.50s
Validation Epoch: 0, Average loss: 0.0185, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 116.76908483124488,
                        "time": 3.2287585809999655,
                        "accuracy": 0.009765625,
                        "total_cost": 28948980.268013485
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6237
Profiling... [512/50176]	Loss: 5.8267
Profiling... [768/50176]	Loss: 5.7021
Profiling... [1024/50176]	Loss: 4.9736
Profiling... [1280/50176]	Loss: 4.9417
Profiling... [1536/50176]	Loss: 4.6907
Profiling... [1792/50176]	Loss: 4.7889
Profiling... [2048/50176]	Loss: 4.6442
Profiling... [2304/50176]	Loss: 5.0353
Profiling... [2560/50176]	Loss: 4.7441
Profiling... [2816/50176]	Loss: 4.7484
Profiling... [3072/50176]	Loss: 5.0286
Profiling... [3328/50176]	Loss: 4.6097
Profile done
epoch 1 train time consumed: 3.52s
Validation Epoch: 0, Average loss: 0.0184, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 116.61039844068081,
                        "time": 2.4198077620001186,
                        "accuracy": 0.009765625,
                        "total_cost": 21695924.894581627
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6293
Profiling... [512/50176]	Loss: 5.7352
Profiling... [768/50176]	Loss: 5.5365
Profiling... [1024/50176]	Loss: 4.9338
Profiling... [1280/50176]	Loss: 4.6703
Profiling... [1536/50176]	Loss: 4.7222
Profiling... [1792/50176]	Loss: 4.8128
Profiling... [2048/50176]	Loss: 4.7254
Profiling... [2304/50176]	Loss: 4.6120
Profiling... [2560/50176]	Loss: 4.6362
Profiling... [2816/50176]	Loss: 4.6329
Profiling... [3072/50176]	Loss: 4.5423
Profiling... [3328/50176]	Loss: 4.5444
Profile done
epoch 1 train time consumed: 3.55s
Validation Epoch: 0, Average loss: 0.0186, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 116.86365074475482,
                        "time": 2.4307130269999107,
                        "accuracy": 0.009765625,
                        "total_cost": 21793732.6962295
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6207
Profiling... [512/50176]	Loss: 5.6910
Profiling... [768/50176]	Loss: 5.2341
Profiling... [1024/50176]	Loss: 5.1449
Profiling... [1280/50176]	Loss: 4.9949
Profiling... [1536/50176]	Loss: 4.8707
Profiling... [1792/50176]	Loss: 4.7078
Profiling... [2048/50176]	Loss: 4.7344
Profiling... [2304/50176]	Loss: 4.5967
Profiling... [2560/50176]	Loss: 4.6488
Profiling... [2816/50176]	Loss: 4.8423
Profiling... [3072/50176]	Loss: 4.8740
Profiling... [3328/50176]	Loss: 4.5891
Profile done
epoch 1 train time consumed: 3.79s
Validation Epoch: 0, Average loss: 0.0185, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 117.02844765105716,
                        "time": 2.6448059219999323,
                        "accuracy": 0.009765625,
                        "total_cost": 23713308.35872656
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6598
Profiling... [512/50176]	Loss: 5.4886
Profiling... [768/50176]	Loss: 5.5905
Profiling... [1024/50176]	Loss: 4.9433
Profiling... [1280/50176]	Loss: 4.8007
Profiling... [1536/50176]	Loss: 4.6138
Profiling... [1792/50176]	Loss: 4.8440
Profiling... [2048/50176]	Loss: 4.7260
Profiling... [2304/50176]	Loss: 4.7690
Profiling... [2560/50176]	Loss: 4.8093
Profiling... [2816/50176]	Loss: 4.9103
Profiling... [3072/50176]	Loss: 4.7242
Profiling... [3328/50176]	Loss: 4.8163
Profile done
epoch 1 train time consumed: 5.22s
Validation Epoch: 0, Average loss: 0.0185, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 116.91132892632557,
                        "time": 3.193735080000124,
                        "accuracy": 0.009765625,
                        "total_cost": 28634983.56799812
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6328
Profiling... [512/50176]	Loss: 7.1665
Profiling... [768/50176]	Loss: 5.3523
Profiling... [1024/50176]	Loss: 5.3787
Profiling... [1280/50176]	Loss: 5.1982
Profiling... [1536/50176]	Loss: 5.0699
Profiling... [1792/50176]	Loss: 5.3026
Profiling... [2048/50176]	Loss: 5.3477
Profiling... [2304/50176]	Loss: 5.1110
Profiling... [2560/50176]	Loss: 5.0916
Profiling... [2816/50176]	Loss: 4.7337
Profiling... [3072/50176]	Loss: 4.9781
Profiling... [3328/50176]	Loss: 4.9601
Profile done
epoch 1 train time consumed: 3.53s
Validation Epoch: 0, Average loss: 0.0669, Accuracy: 0.0110
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 116.65905029377456,
                        "time": 2.41722047799999,
                        "accuracy": 0.01103515625,
                        "total_cost": 19179410.091964595
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6289
Profiling... [512/50176]	Loss: 7.6302
Profiling... [768/50176]	Loss: 5.8001
Profiling... [1024/50176]	Loss: 4.7954
Profiling... [1280/50176]	Loss: 4.8415
Profiling... [1536/50176]	Loss: 4.6690
Profiling... [1792/50176]	Loss: 5.4318
Profiling... [2048/50176]	Loss: 4.9327
Profiling... [2304/50176]	Loss: 4.8702
Profiling... [2560/50176]	Loss: 4.7812
Profiling... [2816/50176]	Loss: 4.8226
Profiling... [3072/50176]	Loss: 4.8174
Profiling... [3328/50176]	Loss: 4.7590
Profile done
epoch 1 train time consumed: 3.59s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 116.91975526686727,
                        "time": 2.4474856970000474,
                        "accuracy": 0.009765625,
                        "total_cost": 21944123.2078705
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6376
Profiling... [512/50176]	Loss: 7.6013
Profiling... [768/50176]	Loss: 6.0317
Profiling... [1024/50176]	Loss: 4.8337
Profiling... [1280/50176]	Loss: 4.9331
Profiling... [1536/50176]	Loss: 4.6515
Profiling... [1792/50176]	Loss: 4.8607
Profiling... [2048/50176]	Loss: 4.7028
Profiling... [2304/50176]	Loss: 4.8362
Profiling... [2560/50176]	Loss: 4.6725
Profiling... [2816/50176]	Loss: 4.9111
Profiling... [3072/50176]	Loss: 4.7440
Profiling... [3328/50176]	Loss: 4.6507
Profile done
epoch 1 train time consumed: 3.80s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 117.06273167195057,
                        "time": 2.646874665000041,
                        "accuracy": 0.009765625,
                        "total_cost": 23731861.337788694
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6103
Profiling... [512/50176]	Loss: 7.6526
Profiling... [768/50176]	Loss: 6.4639
Profiling... [1024/50176]	Loss: 5.5132
Profiling... [1280/50176]	Loss: 5.5716
Profiling... [1536/50176]	Loss: 5.0037
Profiling... [1792/50176]	Loss: 4.7529
Profiling... [2048/50176]	Loss: 4.8794
Profiling... [2304/50176]	Loss: 4.9169
Profiling... [2560/50176]	Loss: 4.9000
Profiling... [2816/50176]	Loss: 4.7387
Profiling... [3072/50176]	Loss: 4.8937
Profiling... [3328/50176]	Loss: 4.6609
Profile done
epoch 1 train time consumed: 4.52s
Validation Epoch: 0, Average loss: 0.0206, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 116.89589368777031,
                        "time": 3.227878310000051,
                        "accuracy": 0.009765625,
                        "total_cost": 28941108.734452315
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6443
Profiling... [512/50176]	Loss: 8.3023
Profiling... [768/50176]	Loss: 5.7613
Profiling... [1024/50176]	Loss: 4.8365
Profiling... [1280/50176]	Loss: 4.7195
Profiling... [1536/50176]	Loss: 5.3916
Profiling... [1792/50176]	Loss: 4.9962
Profiling... [2048/50176]	Loss: 4.7957
Profiling... [2304/50176]	Loss: 4.7952
Profiling... [2560/50176]	Loss: 4.6073
Profiling... [2816/50176]	Loss: 4.8985
Profiling... [3072/50176]	Loss: 4.7412
Profiling... [3328/50176]	Loss: 4.5934
Profile done
epoch 1 train time consumed: 3.58s
Validation Epoch: 0, Average loss: 0.0503, Accuracy: 0.0114
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 116.7486134674083,
                        "time": 2.408042010999907,
                        "accuracy": 0.01142578125,
                        "total_cost": 18453376.546612043
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6510
Profiling... [512/50176]	Loss: 7.2140
Profiling... [768/50176]	Loss: 5.7365
Profiling... [1024/50176]	Loss: 5.1857
Profiling... [1280/50176]	Loss: 5.4456
Profiling... [1536/50176]	Loss: 5.0679
Profiling... [1792/50176]	Loss: 4.8084
Profiling... [2048/50176]	Loss: 4.9094
Profiling... [2304/50176]	Loss: 5.8723
Profiling... [2560/50176]	Loss: 4.9902
Profiling... [2816/50176]	Loss: 4.8733
Profiling... [3072/50176]	Loss: 4.7140
Profiling... [3328/50176]	Loss: 4.6962
Profile done
epoch 1 train time consumed: 3.59s
Validation Epoch: 0, Average loss: 0.0186, Accuracy: 0.0182
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 116.9785993428839,
                        "time": 2.430605602000014,
                        "accuracy": 0.0181640625,
                        "total_cost": 11716550.446489584
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6135
Profiling... [512/50176]	Loss: 7.6581
Profiling... [768/50176]	Loss: 5.6601
Profiling... [1024/50176]	Loss: 5.0968
Profiling... [1280/50176]	Loss: 5.1175
Profiling... [1536/50176]	Loss: 4.6731
Profiling... [1792/50176]	Loss: 5.5116
Profiling... [2048/50176]	Loss: 5.1921
Profiling... [2304/50176]	Loss: 4.7836
Profiling... [2560/50176]	Loss: 4.6473
Profiling... [2816/50176]	Loss: 4.9046
Profiling... [3072/50176]	Loss: 5.0628
Profiling... [3328/50176]	Loss: 4.8397
Profile done
epoch 1 train time consumed: 3.78s
Validation Epoch: 0, Average loss: 0.0184, Accuracy: 0.0128
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 117.11946197514281,
                        "time": 2.6548579810000774,
                        "accuracy": 0.01279296875,
                        "total_cost": 18170570.541469067
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6313
Profiling... [512/50176]	Loss: 7.5683
Profiling... [768/50176]	Loss: 5.9313
Profiling... [1024/50176]	Loss: 5.6976
Profiling... [1280/50176]	Loss: 5.4006
Profiling... [1536/50176]	Loss: 4.7997
Profiling... [1792/50176]	Loss: 4.6592
Profiling... [2048/50176]	Loss: 4.6350
Profiling... [2304/50176]	Loss: 4.6350
Profiling... [2560/50176]	Loss: 4.5840
Profiling... [2816/50176]	Loss: 4.5480
Profiling... [3072/50176]	Loss: 4.6233
Profiling... [3328/50176]	Loss: 4.7202
Profile done
epoch 1 train time consumed: 4.48s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 116.9607314638007,
                        "time": 3.2183251460000974,
                        "accuracy": 0.00986328125,
                        "total_cost": 28569768.21041079
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6529
Profiling... [512/50176]	Loss: 8.1570
Profiling... [768/50176]	Loss: 5.9173
Profiling... [1024/50176]	Loss: 5.3836
Profiling... [1280/50176]	Loss: 6.0138
Profiling... [1536/50176]	Loss: 5.0709
Profiling... [1792/50176]	Loss: 5.2126
Profiling... [2048/50176]	Loss: 4.9383
Profiling... [2304/50176]	Loss: 4.8367
Profiling... [2560/50176]	Loss: 4.7355
Profiling... [2816/50176]	Loss: 4.7057
Profiling... [3072/50176]	Loss: 4.7384
Profiling... [3328/50176]	Loss: 4.7234
Profile done
epoch 1 train time consumed: 3.52s
Validation Epoch: 0, Average loss: 0.0185, Accuracy: 0.0105
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 116.82033557444726,
                        "time": 2.4207932899998923,
                        "accuracy": 0.010546875,
                        "total_cost": 20097025.120449197
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6392
Profiling... [512/50176]	Loss: 7.4229
Profiling... [768/50176]	Loss: 5.8360
Profiling... [1024/50176]	Loss: 5.8259
Profiling... [1280/50176]	Loss: 5.1630
Profiling... [1536/50176]	Loss: 4.9746
Profiling... [1792/50176]	Loss: 4.8032
Profiling... [2048/50176]	Loss: 5.0345
Profiling... [2304/50176]	Loss: 4.9771
Profiling... [2560/50176]	Loss: 4.7811
Profiling... [2816/50176]	Loss: 5.0547
Profiling... [3072/50176]	Loss: 4.7494
Profiling... [3328/50176]	Loss: 4.9535
Profile done
epoch 1 train time consumed: 3.53s
Validation Epoch: 0, Average loss: 0.0188, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 117.05765176115224,
                        "time": 2.421480927999937,
                        "accuracy": 0.009765625,
                        "total_cost": 21710981.901885703
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6310
Profiling... [512/50176]	Loss: 7.4369
Profiling... [768/50176]	Loss: 5.7847
Profiling... [1024/50176]	Loss: 5.5649
Profiling... [1280/50176]	Loss: 5.1548
Profiling... [1536/50176]	Loss: 4.8893
Profiling... [1792/50176]	Loss: 4.8956
Profiling... [2048/50176]	Loss: 4.7694
Profiling... [2304/50176]	Loss: 4.7114
Profiling... [2560/50176]	Loss: 4.8777
Profiling... [2816/50176]	Loss: 4.5970
Profiling... [3072/50176]	Loss: 4.6627
Profiling... [3328/50176]	Loss: 4.8286
Profile done
epoch 1 train time consumed: 3.90s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0101
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 117.18686597949738,
                        "time": 2.6457471470000655,
                        "accuracy": 0.01005859375,
                        "total_cost": 23030843.528268676
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6598
Profiling... [512/50176]	Loss: 7.4310
Profiling... [768/50176]	Loss: 6.8655
Profiling... [1024/50176]	Loss: 5.4654
Profiling... [1280/50176]	Loss: 5.4724
Profiling... [1536/50176]	Loss: 4.8901
Profiling... [1792/50176]	Loss: 4.8351
Profiling... [2048/50176]	Loss: 4.8580
Profiling... [2304/50176]	Loss: 4.8657
Profiling... [2560/50176]	Loss: 4.7531
Profiling... [2816/50176]	Loss: 4.6862
Profiling... [3072/50176]	Loss: 4.7406
Profiling... [3328/50176]	Loss: 4.6245
Profile done
epoch 1 train time consumed: 4.48s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 117.03016595627902,
                        "time": 3.2300503690000824,
                        "accuracy": 0.009765625,
                        "total_cost": 28960605.588774174
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6374
Profiling... [1024/50176]	Loss: 4.6377
Profiling... [1536/50176]	Loss: 4.6610
Profiling... [2048/50176]	Loss: 4.6512
Profiling... [2560/50176]	Loss: 4.6272
Profiling... [3072/50176]	Loss: 4.6042
Profiling... [3584/50176]	Loss: 4.5788
Profiling... [4096/50176]	Loss: 4.5517
Profiling... [4608/50176]	Loss: 4.5235
Profiling... [5120/50176]	Loss: 4.5079
Profiling... [5632/50176]	Loss: 4.5237
Profiling... [6144/50176]	Loss: 4.4779
Profiling... [6656/50176]	Loss: 4.4952
Profile done
epoch 1 train time consumed: 6.55s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 117.12630856757725,
                        "time": 4.588137630000119,
                        "accuracy": 0.009765625,
                        "total_cost": 41137227.61593973
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6439
Profiling... [1024/50176]	Loss: 4.6541
Profiling... [1536/50176]	Loss: 4.6480
Profiling... [2048/50176]	Loss: 4.6199
Profiling... [2560/50176]	Loss: 4.6105
Profiling... [3072/50176]	Loss: 4.5720
Profiling... [3584/50176]	Loss: 4.5958
Profiling... [4096/50176]	Loss: 4.5381
Profiling... [4608/50176]	Loss: 4.5236
Profiling... [5120/50176]	Loss: 4.5458
Profiling... [5632/50176]	Loss: 4.4948
Profiling... [6144/50176]	Loss: 4.4705
Profiling... [6656/50176]	Loss: 4.5351
Profile done
epoch 1 train time consumed: 6.58s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 117.48241339830541,
                        "time": 4.609582381000109,
                        "accuracy": 0.009765625,
                        "total_cost": 41329585.230740346
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6387
Profiling... [1024/50176]	Loss: 4.6467
Profiling... [1536/50176]	Loss: 4.6650
Profiling... [2048/50176]	Loss: 4.6194
Profiling... [2560/50176]	Loss: 4.6656
Profiling... [3072/50176]	Loss: 4.6363
Profiling... [3584/50176]	Loss: 4.5745
Profiling... [4096/50176]	Loss: 4.5657
Profiling... [4608/50176]	Loss: 4.5477
Profiling... [5120/50176]	Loss: 4.5583
Profiling... [5632/50176]	Loss: 4.5142
Profiling... [6144/50176]	Loss: 4.4878
Profiling... [6656/50176]	Loss: 4.4597
Profile done
epoch 1 train time consumed: 7.18s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 117.6164346964197,
                        "time": 5.077927804999945,
                        "accuracy": 0.009765625,
                        "total_cost": 45528812.218319885
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6165
Profiling... [1024/50176]	Loss: 4.6402
Profiling... [1536/50176]	Loss: 4.6305
Profiling... [2048/50176]	Loss: 4.6868
Profiling... [2560/50176]	Loss: 4.6036
Profiling... [3072/50176]	Loss: 4.5770
Profiling... [3584/50176]	Loss: 4.5547
Profiling... [4096/50176]	Loss: 4.5666
Profiling... [4608/50176]	Loss: 4.5517
Profiling... [5120/50176]	Loss: 4.5445
Profiling... [5632/50176]	Loss: 4.5795
Profiling... [6144/50176]	Loss: 4.5155
Profiling... [6656/50176]	Loss: 4.5157
Profile done
epoch 1 train time consumed: 13.67s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 117.26547486038343,
                        "time": 9.911809435000123,
                        "accuracy": 0.009765625,
                        "total_cost": 88869322.96525529
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6409
Profiling... [1024/50176]	Loss: 4.6119
Profiling... [1536/50176]	Loss: 4.6279
Profiling... [2048/50176]	Loss: 4.6637
Profiling... [2560/50176]	Loss: 4.6231
Profiling... [3072/50176]	Loss: 4.5988
Profiling... [3584/50176]	Loss: 4.5589
Profiling... [4096/50176]	Loss: 4.5337
Profiling... [4608/50176]	Loss: 4.5227
Profiling... [5120/50176]	Loss: 4.5730
Profiling... [5632/50176]	Loss: 4.5793
Profiling... [6144/50176]	Loss: 4.5348
Profiling... [6656/50176]	Loss: 4.5137
Profile done
epoch 1 train time consumed: 6.65s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 117.12835637622752,
                        "time": 4.586686894999957,
                        "accuracy": 0.009765625,
                        "total_cost": 41124220.81137747
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6257
Profiling... [1024/50176]	Loss: 4.6333
Profiling... [1536/50176]	Loss: 4.6527
Profiling... [2048/50176]	Loss: 4.6578
Profiling... [2560/50176]	Loss: 4.6140
Profiling... [3072/50176]	Loss: 4.5791
Profiling... [3584/50176]	Loss: 4.6008
Profiling... [4096/50176]	Loss: 4.5478
Profiling... [4608/50176]	Loss: 4.5210
Profiling... [5120/50176]	Loss: 4.5438
Profiling... [5632/50176]	Loss: 4.5018
Profiling... [6144/50176]	Loss: 4.5084
Profiling... [6656/50176]	Loss: 4.4727
Profile done
epoch 1 train time consumed: 6.63s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 117.43542621609434,
                        "time": 4.623961538000003,
                        "accuracy": 0.009765625,
                        "total_cost": 41458497.84545395
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6437
Profiling... [1024/50176]	Loss: 4.6481
Profiling... [1536/50176]	Loss: 4.6919
Profiling... [2048/50176]	Loss: 4.6394
Profiling... [2560/50176]	Loss: 4.6279
Profiling... [3072/50176]	Loss: 4.5731
Profiling... [3584/50176]	Loss: 4.5660
Profiling... [4096/50176]	Loss: 4.5554
Profiling... [4608/50176]	Loss: 4.5345
Profiling... [5120/50176]	Loss: 4.5285
Profiling... [5632/50176]	Loss: 4.4640
Profiling... [6144/50176]	Loss: 4.5052
Profiling... [6656/50176]	Loss: 4.4325
Profile done
epoch 1 train time consumed: 7.08s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 117.54856300494193,
                        "time": 5.03078768599994,
                        "accuracy": 0.009765625,
                        "total_cost": 45106135.393959
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6246
Profiling... [1024/50176]	Loss: 4.6600
Profiling... [1536/50176]	Loss: 4.6565
Profiling... [2048/50176]	Loss: 4.6183
Profiling... [2560/50176]	Loss: 4.6046
Profiling... [3072/50176]	Loss: 4.5687
Profiling... [3584/50176]	Loss: 4.6126
Profiling... [4096/50176]	Loss: 4.5747
Profiling... [4608/50176]	Loss: 4.5839
Profiling... [5120/50176]	Loss: 4.5055
Profiling... [5632/50176]	Loss: 4.4972
Profiling... [6144/50176]	Loss: 4.5028
Profiling... [6656/50176]	Loss: 4.4879
Profile done
epoch 1 train time consumed: 14.24s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 117.20249034597362,
                        "time": 10.44408786300005,
                        "accuracy": 0.009765625,
                        "total_cost": 93641699.79555556
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6292
Profiling... [1024/50176]	Loss: 4.6330
Profiling... [1536/50176]	Loss: 4.6650
Profiling... [2048/50176]	Loss: 4.6963
Profiling... [2560/50176]	Loss: 4.6155
Profiling... [3072/50176]	Loss: 4.6015
Profiling... [3584/50176]	Loss: 4.5636
Profiling... [4096/50176]	Loss: 4.5559
Profiling... [4608/50176]	Loss: 4.4911
Profiling... [5120/50176]	Loss: 4.5597
Profiling... [5632/50176]	Loss: 4.5203
Profiling... [6144/50176]	Loss: 4.4778
Profiling... [6656/50176]	Loss: 4.5427
Profile done
epoch 1 train time consumed: 6.68s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 117.0741020994749,
                        "time": 4.5806406209999295,
                        "accuracy": 0.009765625,
                        "total_cost": 41069997.21281186
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6457
Profiling... [1024/50176]	Loss: 4.6554
Profiling... [1536/50176]	Loss: 4.6348
Profiling... [2048/50176]	Loss: 4.5875
Profiling... [2560/50176]	Loss: 4.5936
Profiling... [3072/50176]	Loss: 4.6132
Profiling... [3584/50176]	Loss: 4.5804
Profiling... [4096/50176]	Loss: 4.5860
Profiling... [4608/50176]	Loss: 4.5552
Profiling... [5120/50176]	Loss: 4.5420
Profiling... [5632/50176]	Loss: 4.5318
Profiling... [6144/50176]	Loss: 4.5069
Profiling... [6656/50176]	Loss: 4.4456
Profile done
epoch 1 train time consumed: 6.60s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 117.3588192627802,
                        "time": 4.6182016299999304,
                        "accuracy": 0.009765625,
                        "total_cost": 41406836.32334859
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6349
Profiling... [1024/50176]	Loss: 4.6529
Profiling... [1536/50176]	Loss: 4.6742
Profiling... [2048/50176]	Loss: 4.6441
Profiling... [2560/50176]	Loss: 4.6029
Profiling... [3072/50176]	Loss: 4.5930
Profiling... [3584/50176]	Loss: 4.5490
Profiling... [4096/50176]	Loss: 4.5504
Profiling... [4608/50176]	Loss: 4.4988
Profiling... [5120/50176]	Loss: 4.5184
Profiling... [5632/50176]	Loss: 4.5657
Profiling... [6144/50176]	Loss: 4.5072
Profiling... [6656/50176]	Loss: 4.4947
Profile done
epoch 1 train time consumed: 7.20s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 117.46643079620027,
                        "time": 5.038344135999978,
                        "accuracy": 0.009765625,
                        "total_cost": 45173865.47726208
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6363
Profiling... [1024/50176]	Loss: 4.6116
Profiling... [1536/50176]	Loss: 4.6201
Profiling... [2048/50176]	Loss: 4.6598
Profiling... [2560/50176]	Loss: 4.6566
Profiling... [3072/50176]	Loss: 4.5772
Profiling... [3584/50176]	Loss: 4.6371
Profiling... [4096/50176]	Loss: 4.5425
Profiling... [4608/50176]	Loss: 4.5337
Profiling... [5120/50176]	Loss: 4.5490
Profiling... [5632/50176]	Loss: 4.5474
Profiling... [6144/50176]	Loss: 4.4938
Profiling... [6656/50176]	Loss: 4.4943
Profile done
epoch 1 train time consumed: 13.48s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 117.16413171438037,
                        "time": 9.710533207000026,
                        "accuracy": 0.009765625,
                        "total_cost": 87064629.11573434
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6555
Profiling... [1024/50176]	Loss: 5.6460
Profiling... [1536/50176]	Loss: 5.0404
Profiling... [2048/50176]	Loss: 4.8336
Profiling... [2560/50176]	Loss: 4.6655
Profiling... [3072/50176]	Loss: 4.7622
Profiling... [3584/50176]	Loss: 4.7103
Profiling... [4096/50176]	Loss: 4.6804
Profiling... [4608/50176]	Loss: 4.6375
Profiling... [5120/50176]	Loss: 4.6031
Profiling... [5632/50176]	Loss: 4.6595
Profiling... [6144/50176]	Loss: 4.6169
Profiling... [6656/50176]	Loss: 4.7080
Profile done
epoch 1 train time consumed: 6.53s
Validation Epoch: 0, Average loss: 0.0094, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 117.03865280720053,
                        "time": 4.582299982999984,
                        "accuracy": 0.009765625,
                        "total_cost": 41084866.72597842
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6411
Profiling... [1024/50176]	Loss: 5.5863
Profiling... [1536/50176]	Loss: 5.3681
Profiling... [2048/50176]	Loss: 4.7041
Profiling... [2560/50176]	Loss: 4.6823
Profiling... [3072/50176]	Loss: 4.7741
Profiling... [3584/50176]	Loss: 4.6768
Profiling... [4096/50176]	Loss: 4.6918
Profiling... [4608/50176]	Loss: 4.7415
Profiling... [5120/50176]	Loss: 4.6677
Profiling... [5632/50176]	Loss: 4.6414
Profiling... [6144/50176]	Loss: 4.5962
Profiling... [6656/50176]	Loss: 4.6495
Profile done
epoch 1 train time consumed: 6.85s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 117.30113301984754,
                        "time": 4.6984107089999725,
                        "accuracy": 0.009765625,
                        "total_cost": 42125977.75229714
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6455
Profiling... [1024/50176]	Loss: 5.7528
Profiling... [1536/50176]	Loss: 5.0247
Profiling... [2048/50176]	Loss: 4.7262
Profiling... [2560/50176]	Loss: 4.7904
Profiling... [3072/50176]	Loss: 4.6193
Profiling... [3584/50176]	Loss: 4.6697
Profiling... [4096/50176]	Loss: 4.7523
Profiling... [4608/50176]	Loss: 4.6519
Profiling... [5120/50176]	Loss: 5.1045
Profiling... [5632/50176]	Loss: 4.7105
Profiling... [6144/50176]	Loss: 4.6097
Profiling... [6656/50176]	Loss: 4.7232
Profile done
epoch 1 train time consumed: 7.12s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 117.37787744810684,
                        "time": 5.075886214000093,
                        "accuracy": 0.009765625,
                        "total_cost": 45510445.271039166
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6271
Profiling... [1024/50176]	Loss: 5.5450
Profiling... [1536/50176]	Loss: 5.2470
Profiling... [2048/50176]	Loss: 4.8209
Profiling... [2560/50176]	Loss: 4.6968
Profiling... [3072/50176]	Loss: 4.6844
Profiling... [3584/50176]	Loss: 4.7779
Profiling... [4096/50176]	Loss: 4.7958
Profiling... [4608/50176]	Loss: 4.6913
Profiling... [5120/50176]	Loss: 4.6389
Profiling... [5632/50176]	Loss: 4.6241
Profiling... [6144/50176]	Loss: 4.5636
Profiling... [6656/50176]	Loss: 4.7536
Profile done
epoch 1 train time consumed: 13.58s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 117.1210889697644,
                        "time": 9.7292278970001,
                        "accuracy": 0.009765625,
                        "total_cost": 87232224.24274684
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6316
Profiling... [1024/50176]	Loss: 5.6072
Profiling... [1536/50176]	Loss: 5.0811
Profiling... [2048/50176]	Loss: 4.7653
Profiling... [2560/50176]	Loss: 4.7183
Profiling... [3072/50176]	Loss: 4.8188
Profiling... [3584/50176]	Loss: 4.9085
Profiling... [4096/50176]	Loss: 4.7966
Profiling... [4608/50176]	Loss: 4.7954
Profiling... [5120/50176]	Loss: 4.6122
Profiling... [5632/50176]	Loss: 4.5682
Profiling... [6144/50176]	Loss: 4.6152
Profiling... [6656/50176]	Loss: 4.5500
Profile done
epoch 1 train time consumed: 6.54s
Validation Epoch: 0, Average loss: 0.0094, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 116.9896021694215,
                        "time": 4.582594181000104,
                        "accuracy": 0.009765625,
                        "total_cost": 41087492.994312055
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6199
Profiling... [1024/50176]	Loss: 5.5393
Profiling... [1536/50176]	Loss: 5.2441
Profiling... [2048/50176]	Loss: 4.7616
Profiling... [2560/50176]	Loss: 4.7355
Profiling... [3072/50176]	Loss: 4.8355
Profiling... [3584/50176]	Loss: 4.7866
Profiling... [4096/50176]	Loss: 4.7983
Profiling... [4608/50176]	Loss: 4.7662
Profiling... [5120/50176]	Loss: 4.6719
Profiling... [5632/50176]	Loss: 4.6083
Profiling... [6144/50176]	Loss: 4.5749
Profiling... [6656/50176]	Loss: 4.5666
Profile done
epoch 1 train time consumed: 6.58s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 117.2443570822529,
                        "time": 4.6235104040001715,
                        "accuracy": 0.009765625,
                        "total_cost": 41454407.74168628
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6281
Profiling... [1024/50176]	Loss: 5.7219
Profiling... [1536/50176]	Loss: 5.2244
Profiling... [2048/50176]	Loss: 4.7525
Profiling... [2560/50176]	Loss: 4.6873
Profiling... [3072/50176]	Loss: 4.7847
Profiling... [3584/50176]	Loss: 4.6876
Profiling... [4096/50176]	Loss: 4.7354
Profiling... [4608/50176]	Loss: 4.6200
Profiling... [5120/50176]	Loss: 4.6177
Profiling... [5632/50176]	Loss: 4.6523
Profiling... [6144/50176]	Loss: 4.6347
Profiling... [6656/50176]	Loss: 4.6195
Profile done
epoch 1 train time consumed: 7.21s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 117.35528066593656,
                        "time": 5.055171933000111,
                        "accuracy": 0.009765625,
                        "total_cost": 45324714.9770768
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6401
Profiling... [1024/50176]	Loss: 5.4257
Profiling... [1536/50176]	Loss: 5.1377
Profiling... [2048/50176]	Loss: 4.8044
Profiling... [2560/50176]	Loss: 4.8757
Profiling... [3072/50176]	Loss: 4.7463
Profiling... [3584/50176]	Loss: 4.8400
Profiling... [4096/50176]	Loss: 4.6665
Profiling... [4608/50176]	Loss: 4.7389
Profiling... [5120/50176]	Loss: 4.6643
Profiling... [5632/50176]	Loss: 4.8563
Profiling... [6144/50176]	Loss: 4.6732
Profiling... [6656/50176]	Loss: 4.7348
Profile done
epoch 1 train time consumed: 14.17s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 117.0907253436414,
                        "time": 10.336570393999864,
                        "accuracy": 0.009765625,
                        "total_cost": 92677638.93631873
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6292
Profiling... [1024/50176]	Loss: 5.6357
Profiling... [1536/50176]	Loss: 5.3336
Profiling... [2048/50176]	Loss: 4.8428
Profiling... [2560/50176]	Loss: 4.7144
Profiling... [3072/50176]	Loss: 4.6714
Profiling... [3584/50176]	Loss: 4.7657
Profiling... [4096/50176]	Loss: 4.5787
Profiling... [4608/50176]	Loss: 4.5681
Profiling... [5120/50176]	Loss: 4.6738
Profiling... [5632/50176]	Loss: 4.6437
Profiling... [6144/50176]	Loss: 4.8740
Profiling... [6656/50176]	Loss: 4.7009
Profile done
epoch 1 train time consumed: 6.73s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 116.97968749818176,
                        "time": 4.579444832000036,
                        "accuracy": 0.009765625,
                        "total_cost": 41059253.63841888
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6225
Profiling... [1024/50176]	Loss: 5.5605
Profiling... [1536/50176]	Loss: 5.1491
Profiling... [2048/50176]	Loss: 4.9827
Profiling... [2560/50176]	Loss: 4.8132
Profiling... [3072/50176]	Loss: 4.8449
Profiling... [3584/50176]	Loss: 4.6352
Profiling... [4096/50176]	Loss: 4.6426
Profiling... [4608/50176]	Loss: 4.5685
Profiling... [5120/50176]	Loss: 4.6366
Profiling... [5632/50176]	Loss: 4.5033
Profiling... [6144/50176]	Loss: 4.6664
Profiling... [6656/50176]	Loss: 4.5336
Profile done
epoch 1 train time consumed: 6.66s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 117.20841427019106,
                        "time": 4.6155873139998675,
                        "accuracy": 0.009765625,
                        "total_cost": 41383360.79974279
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6147
Profiling... [1024/50176]	Loss: 5.6230
Profiling... [1536/50176]	Loss: 5.0246
Profiling... [2048/50176]	Loss: 4.7404
Profiling... [2560/50176]	Loss: 4.7836
Profiling... [3072/50176]	Loss: 4.7514
Profiling... [3584/50176]	Loss: 4.8011
Profiling... [4096/50176]	Loss: 4.6970
Profiling... [4608/50176]	Loss: 4.5718
Profiling... [5120/50176]	Loss: 4.5989
Profiling... [5632/50176]	Loss: 4.6630
Profiling... [6144/50176]	Loss: 4.6449
Profiling... [6656/50176]	Loss: 4.6294
Profile done
epoch 1 train time consumed: 7.12s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 117.30464179601016,
                        "time": 5.050286971000105,
                        "accuracy": 0.009765625,
                        "total_cost": 45280903.27189087
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6460
Profiling... [1024/50176]	Loss: 5.4698
Profiling... [1536/50176]	Loss: 5.1223
Profiling... [2048/50176]	Loss: 4.8652
Profiling... [2560/50176]	Loss: 4.9004
Profiling... [3072/50176]	Loss: 4.7612
Profiling... [3584/50176]	Loss: 4.7613
Profiling... [4096/50176]	Loss: 4.6940
Profiling... [4608/50176]	Loss: 4.6425
Profiling... [5120/50176]	Loss: 4.6296
Profiling... [5632/50176]	Loss: 4.7061
Profiling... [6144/50176]	Loss: 4.9349
Profiling... [6656/50176]	Loss: 4.7959
Profile done
epoch 1 train time consumed: 13.14s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 117.06717602599798,
                        "time": 9.368183572000135,
                        "accuracy": 0.009765625,
                        "total_cost": 83995076.1930389
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6326
Profiling... [1024/50176]	Loss: 7.7232
Profiling... [1536/50176]	Loss: 5.8279
Profiling... [2048/50176]	Loss: 4.8156
Profiling... [2560/50176]	Loss: 4.9777
Profiling... [3072/50176]	Loss: 4.6367
Profiling... [3584/50176]	Loss: 4.7040
Profiling... [4096/50176]	Loss: 5.0645
Profiling... [4608/50176]	Loss: 4.7468
Profiling... [5120/50176]	Loss: 4.7754
Profiling... [5632/50176]	Loss: 4.8450
Profiling... [6144/50176]	Loss: 4.7759
Profiling... [6656/50176]	Loss: 4.6783
Profile done
epoch 1 train time consumed: 6.57s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 116.9757459228963,
                        "time": 4.578432310999915,
                        "accuracy": 0.009765625,
                        "total_cost": 41050174.461937755
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6277
Profiling... [1024/50176]	Loss: 7.5253
Profiling... [1536/50176]	Loss: 5.5211
Profiling... [2048/50176]	Loss: 4.8848
Profiling... [2560/50176]	Loss: 5.0696
Profiling... [3072/50176]	Loss: 4.8404
Profiling... [3584/50176]	Loss: 4.7062
Profiling... [4096/50176]	Loss: 4.9948
Profiling... [4608/50176]	Loss: 4.7519
Profiling... [5120/50176]	Loss: 4.7942
Profiling... [5632/50176]	Loss: 5.1513
Profiling... [6144/50176]	Loss: 4.7509
Profiling... [6656/50176]	Loss: 4.7823
Profile done
epoch 1 train time consumed: 6.72s
Validation Epoch: 0, Average loss: 0.0236, Accuracy: 0.0100
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 117.20593494333076,
                        "time": 4.607290958000021,
                        "accuracy": 0.0099609375,
                        "total_cost": 40498995.17461545
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6323
Profiling... [1024/50176]	Loss: 7.4704
Profiling... [1536/50176]	Loss: 5.8987
Profiling... [2048/50176]	Loss: 5.1635
Profiling... [2560/50176]	Loss: 4.9620
Profiling... [3072/50176]	Loss: 4.7223
Profiling... [3584/50176]	Loss: 5.2515
Profiling... [4096/50176]	Loss: 4.7179
Profiling... [4608/50176]	Loss: 4.8194
Profiling... [5120/50176]	Loss: 5.0602
Profiling... [5632/50176]	Loss: 4.7613
Profiling... [6144/50176]	Loss: 4.8087
Profiling... [6656/50176]	Loss: 4.5949
Profile done
epoch 1 train time consumed: 7.16s
Validation Epoch: 0, Average loss: 0.0150, Accuracy: 0.0150
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 117.28569841384908,
                        "time": 5.02275140200004,
                        "accuracy": 0.0150390625,
                        "total_cost": 29242866.44384082
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6320
Profiling... [1024/50176]	Loss: 7.5434
Profiling... [1536/50176]	Loss: 5.5727
Profiling... [2048/50176]	Loss: 5.0450
Profiling... [2560/50176]	Loss: 4.8509
Profiling... [3072/50176]	Loss: 4.6809
Profiling... [3584/50176]	Loss: 4.8488
Profiling... [4096/50176]	Loss: 4.5730
Profiling... [4608/50176]	Loss: 4.9529
Profiling... [5120/50176]	Loss: 4.5499
Profiling... [5632/50176]	Loss: 4.6999
Profiling... [6144/50176]	Loss: 4.6265
Profiling... [6656/50176]	Loss: 4.6930
Profile done
epoch 1 train time consumed: 12.87s
Validation Epoch: 0, Average loss: 0.0261, Accuracy: 0.0161
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 117.05892362319183,
                        "time": 9.078478866000069,
                        "accuracy": 0.01611328125,
                        "total_cost": 49331867.72601645
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6362
Profiling... [1024/50176]	Loss: 7.3544
Profiling... [1536/50176]	Loss: 6.3390
Profiling... [2048/50176]	Loss: 5.1217
Profiling... [2560/50176]	Loss: 4.8986
Profiling... [3072/50176]	Loss: 4.6800
Profiling... [3584/50176]	Loss: 4.8160
Profiling... [4096/50176]	Loss: 5.0518
Profiling... [4608/50176]	Loss: 4.7395
Profiling... [5120/50176]	Loss: 4.8083
Profiling... [5632/50176]	Loss: 4.9574
Profiling... [6144/50176]	Loss: 4.8648
Profiling... [6656/50176]	Loss: 4.7690
Profile done
epoch 1 train time consumed: 6.63s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 116.95938084443753,
                        "time": 4.5660415230001945,
                        "accuracy": 0.009765625,
                        "total_cost": 40939074.96522108
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6266
Profiling... [1024/50176]	Loss: 7.2045
Profiling... [1536/50176]	Loss: 5.6594
Profiling... [2048/50176]	Loss: 4.7920
Profiling... [2560/50176]	Loss: 5.0841
Profiling... [3072/50176]	Loss: 5.0103
Profiling... [3584/50176]	Loss: 5.3196
Profiling... [4096/50176]	Loss: 4.8490
Profiling... [4608/50176]	Loss: 4.8929
Profiling... [5120/50176]	Loss: 4.6685
Profiling... [5632/50176]	Loss: 4.5992
Profiling... [6144/50176]	Loss: 4.7914
Profiling... [6656/50176]	Loss: 4.7154
Profile done
epoch 1 train time consumed: 6.71s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0100
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 117.18023098859246,
                        "time": 4.58728712900006,
                        "accuracy": 0.0099609375,
                        "total_cost": 40323151.65818468
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6455
Profiling... [1024/50176]	Loss: 7.4557
Profiling... [1536/50176]	Loss: 5.8627
Profiling... [2048/50176]	Loss: 5.1727
Profiling... [2560/50176]	Loss: 4.8051
Profiling... [3072/50176]	Loss: 4.7814
Profiling... [3584/50176]	Loss: 4.8566
Profiling... [4096/50176]	Loss: 4.8110
Profiling... [4608/50176]	Loss: 5.0122
Profiling... [5120/50176]	Loss: 4.7523
Profiling... [5632/50176]	Loss: 4.7311
Profiling... [6144/50176]	Loss: 4.7034
Profiling... [6656/50176]	Loss: 4.6136
Profile done
epoch 1 train time consumed: 7.24s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0100
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 117.2448543487374,
                        "time": 5.060865727999953,
                        "accuracy": 0.0099609375,
                        "total_cost": 44486016.64578007
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6316
Profiling... [1024/50176]	Loss: 7.3442
Profiling... [1536/50176]	Loss: 5.7745
Profiling... [2048/50176]	Loss: 5.0559
Profiling... [2560/50176]	Loss: 4.7719
Profiling... [3072/50176]	Loss: 4.8797
Profiling... [3584/50176]	Loss: 5.3910
Profiling... [4096/50176]	Loss: 5.0626
Profiling... [4608/50176]	Loss: 4.7004
Profiling... [5120/50176]	Loss: 5.6845
Profiling... [5632/50176]	Loss: 4.7793
Profiling... [6144/50176]	Loss: 4.6921
Profiling... [6656/50176]	Loss: 4.7739
Profile done
epoch 1 train time consumed: 13.58s
Validation Epoch: 0, Average loss: 0.1858, Accuracy: 0.0109
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 117.0323669607827,
                        "time": 9.472137126999996,
                        "accuracy": 0.0109375,
                        "total_cost": 75827773.43328683
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6636
Profiling... [1024/50176]	Loss: 7.5784
Profiling... [1536/50176]	Loss: 5.6351
Profiling... [2048/50176]	Loss: 4.9542
Profiling... [2560/50176]	Loss: 5.0171
Profiling... [3072/50176]	Loss: 4.8232
Profiling... [3584/50176]	Loss: 4.7090
Profiling... [4096/50176]	Loss: 4.9747
Profiling... [4608/50176]	Loss: 4.8956
Profiling... [5120/50176]	Loss: 4.6898
Profiling... [5632/50176]	Loss: 4.9655
Profiling... [6144/50176]	Loss: 5.0974
Profiling... [6656/50176]	Loss: 4.9486
Profile done
epoch 1 train time consumed: 6.53s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 116.93700741118741,
                        "time": 4.566906034000112,
                        "accuracy": 0.009765625,
                        "total_cost": 40946820.9292679
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6434
Profiling... [1024/50176]	Loss: 7.0750
Profiling... [1536/50176]	Loss: 6.2279
Profiling... [2048/50176]	Loss: 5.4557
Profiling... [2560/50176]	Loss: 5.0429
Profiling... [3072/50176]	Loss: 4.7657
Profiling... [3584/50176]	Loss: 4.7843
Profiling... [4096/50176]	Loss: 4.7249
Profiling... [4608/50176]	Loss: 4.6996
Profiling... [5120/50176]	Loss: 4.7338
Profiling... [5632/50176]	Loss: 4.8278
Profiling... [6144/50176]	Loss: 4.7378
Profiling... [6656/50176]	Loss: 4.6501
Profile done
epoch 1 train time consumed: 6.74s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 117.14712294334915,
                        "time": 4.600456011000006,
                        "accuracy": 0.009765625,
                        "total_cost": 41247679.08407895
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6552
Profiling... [1024/50176]	Loss: 7.3558
Profiling... [1536/50176]	Loss: 6.1654
Profiling... [2048/50176]	Loss: 5.0270
Profiling... [2560/50176]	Loss: 4.7744
Profiling... [3072/50176]	Loss: 4.7256
Profiling... [3584/50176]	Loss: 4.6785
Profiling... [4096/50176]	Loss: 4.7792
Profiling... [4608/50176]	Loss: 4.5996
Profiling... [5120/50176]	Loss: 4.7260
Profiling... [5632/50176]	Loss: 4.6352
Profiling... [6144/50176]	Loss: 4.5820
Profiling... [6656/50176]	Loss: 4.6778
Profile done
epoch 1 train time consumed: 7.14s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0103
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 117.22498767604846,
                        "time": 5.034372270999938,
                        "accuracy": 0.01025390625,
                        "total_cost": 42988753.75676532
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6459
Profiling... [1024/50176]	Loss: 7.5681
Profiling... [1536/50176]	Loss: 5.5079
Profiling... [2048/50176]	Loss: 4.9037
Profiling... [2560/50176]	Loss: 4.8574
Profiling... [3072/50176]	Loss: 4.8226
Profiling... [3584/50176]	Loss: 4.6093
Profiling... [4096/50176]	Loss: 4.6389
Profiling... [4608/50176]	Loss: 4.6625
Profiling... [5120/50176]	Loss: 4.6641
Profiling... [5632/50176]	Loss: 4.6848
Profiling... [6144/50176]	Loss: 4.6974
Profiling... [6656/50176]	Loss: 4.6034
Profile done
epoch 1 train time consumed: 13.58s
Validation Epoch: 0, Average loss: 0.0233, Accuracy: 0.0130
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 117.00663761928806,
                        "time": 9.774596710999958,
                        "accuracy": 0.01298828125,
                        "total_cost": 65893942.55380138
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6390
Profiling... [2048/50176]	Loss: 4.6333
Profiling... [3072/50176]	Loss: 4.6060
Profiling... [4096/50176]	Loss: 4.5986
Profiling... [5120/50176]	Loss: 4.5797
Profiling... [6144/50176]	Loss: 4.5567
Profiling... [7168/50176]	Loss: 4.5421
Profiling... [8192/50176]	Loss: 4.5298
Profiling... [9216/50176]	Loss: 4.4980
Profiling... [10240/50176]	Loss: 4.4626
Profiling... [11264/50176]	Loss: 4.4873
Profiling... [12288/50176]	Loss: 4.4865
Profiling... [13312/50176]	Loss: 4.4642
Profile done
epoch 1 train time consumed: 12.66s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 117.17453404661774,
                        "time": 8.93694186200014,
                        "accuracy": 0.009765625,
                        "total_cost": 80128614.80184351
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6332
Profiling... [2048/50176]	Loss: 4.6205
Profiling... [3072/50176]	Loss: 4.5986
Profiling... [4096/50176]	Loss: 4.5991
Profiling... [5120/50176]	Loss: 4.5791
Profiling... [6144/50176]	Loss: 4.5109
Profiling... [7168/50176]	Loss: 4.5385
Profiling... [8192/50176]	Loss: 4.5189
Profiling... [9216/50176]	Loss: 4.4815
Profiling... [10240/50176]	Loss: 4.4846
Profiling... [11264/50176]	Loss: 4.4626
Profiling... [12288/50176]	Loss: 4.4731
Profiling... [13312/50176]	Loss: 4.4727
Profile done
epoch 1 train time consumed: 12.83s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 117.51025099563604,
                        "time": 9.030063230999986,
                        "accuracy": 0.009765625,
                        "total_cost": 80963696.14959507
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6515
Profiling... [2048/50176]	Loss: 4.6212
Profiling... [3072/50176]	Loss: 4.6353
Profiling... [4096/50176]	Loss: 4.5870
Profiling... [5120/50176]	Loss: 4.5532
Profiling... [6144/50176]	Loss: 4.5617
Profiling... [7168/50176]	Loss: 4.5482
Profiling... [8192/50176]	Loss: 4.4953
Profiling... [9216/50176]	Loss: 4.4844
Profiling... [10240/50176]	Loss: 4.4511
Profiling... [11264/50176]	Loss: 4.4751
Profiling... [12288/50176]	Loss: 4.4638
Profiling... [13312/50176]	Loss: 4.4790
Profile done
epoch 1 train time consumed: 14.00s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 117.5758517953665,
                        "time": 9.957676128000003,
                        "accuracy": 0.009765625,
                        "total_cost": 89280722.1582158
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6233
Profiling... [2048/50176]	Loss: 4.6243
Profiling... [3072/50176]	Loss: 4.5967
Profiling... [4096/50176]	Loss: 4.5812
Profiling... [5120/50176]	Loss: 4.5952
Profiling... [6144/50176]	Loss: 4.5468
Profiling... [7168/50176]	Loss: 4.5456
Profiling... [8192/50176]	Loss: 4.5422
Profiling... [9216/50176]	Loss: 4.5194
Profiling... [10240/50176]	Loss: 4.4756
Profiling... [11264/50176]	Loss: 4.4531
Profiling... [12288/50176]	Loss: 4.4529
Profiling... [13312/50176]	Loss: 4.4573
Profile done
epoch 1 train time consumed: 25.11s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 117.02535857051436,
                        "time": 18.634341359000018,
                        "accuracy": 0.009765625,
                        "total_cost": 167075349.9291784
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6369
Profiling... [2048/50176]	Loss: 4.6173
Profiling... [3072/50176]	Loss: 4.6445
Profiling... [4096/50176]	Loss: 4.5878
Profiling... [5120/50176]	Loss: 4.5362
Profiling... [6144/50176]	Loss: 4.5582
Profiling... [7168/50176]	Loss: 4.4912
Profiling... [8192/50176]	Loss: 4.4963
Profiling... [9216/50176]	Loss: 4.4946
Profiling... [10240/50176]	Loss: 4.5516
Profiling... [11264/50176]	Loss: 4.4890
Profiling... [12288/50176]	Loss: 4.4912
Profiling... [13312/50176]	Loss: 4.4423
Profile done
epoch 1 train time consumed: 12.59s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 117.18256491841984,
                        "time": 8.907614372999888,
                        "accuracy": 0.009765625,
                        "total_cost": 79865668.21757504
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6416
Profiling... [2048/50176]	Loss: 4.6192
Profiling... [3072/50176]	Loss: 4.6357
Profiling... [4096/50176]	Loss: 4.5790
Profiling... [5120/50176]	Loss: 4.5255
Profiling... [6144/50176]	Loss: 4.5639
Profiling... [7168/50176]	Loss: 4.5214
Profiling... [8192/50176]	Loss: 4.5158
Profiling... [9216/50176]	Loss: 4.5120
Profiling... [10240/50176]	Loss: 4.4739
Profiling... [11264/50176]	Loss: 4.4765
Profiling... [12288/50176]	Loss: 4.4492
Profiling... [13312/50176]	Loss: 4.4335
Profile done
epoch 1 train time consumed: 12.75s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 117.47034542318302,
                        "time": 9.028196454999943,
                        "accuracy": 0.009765625,
                        "total_cost": 80946940.15903269
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6338
Profiling... [2048/50176]	Loss: 4.6254
Profiling... [3072/50176]	Loss: 4.6240
Profiling... [4096/50176]	Loss: 4.5960
Profiling... [5120/50176]	Loss: 4.5821
Profiling... [6144/50176]	Loss: 4.5283
Profiling... [7168/50176]	Loss: 4.4906
Profiling... [8192/50176]	Loss: 4.5121
Profiling... [9216/50176]	Loss: 4.5162
Profiling... [10240/50176]	Loss: 4.4823
Profiling... [11264/50176]	Loss: 4.4841
Profiling... [12288/50176]	Loss: 4.4949
Profiling... [13312/50176]	Loss: 4.4478
Profile done
epoch 1 train time consumed: 13.86s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 117.53169805398062,
                        "time": 9.983611689999861,
                        "accuracy": 0.009765625,
                        "total_cost": 89513238.35313219
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6465
Profiling... [2048/50176]	Loss: 4.6064
Profiling... [3072/50176]	Loss: 4.6430
Profiling... [4096/50176]	Loss: 4.6215
Profiling... [5120/50176]	Loss: 4.5695
Profiling... [6144/50176]	Loss: 4.5597
Profiling... [7168/50176]	Loss: 4.4914
Profiling... [8192/50176]	Loss: 4.5193
Profiling... [9216/50176]	Loss: 4.4753
Profiling... [10240/50176]	Loss: 4.5308
Profiling... [11264/50176]	Loss: 4.4650
Profiling... [12288/50176]	Loss: 4.4492
Profiling... [13312/50176]	Loss: 4.4655
Profile done
epoch 1 train time consumed: 25.13s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 117.02157528736258,
                        "time": 18.632844446000036,
                        "accuracy": 0.009765625,
                        "total_cost": 167061925.01038906
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6338
Profiling... [2048/50176]	Loss: 4.6348
Profiling... [3072/50176]	Loss: 4.5892
Profiling... [4096/50176]	Loss: 4.5993
Profiling... [5120/50176]	Loss: 4.6215
Profiling... [6144/50176]	Loss: 4.5661
Profiling... [7168/50176]	Loss: 4.5468
Profiling... [8192/50176]	Loss: 4.5085
Profiling... [9216/50176]	Loss: 4.4966
Profiling... [10240/50176]	Loss: 4.4792
Profiling... [11264/50176]	Loss: 4.4665
Profiling... [12288/50176]	Loss: 4.4916
Profiling... [13312/50176]	Loss: 4.4543
Profile done
epoch 1 train time consumed: 12.80s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 117.16981284231223,
                        "time": 8.929519302999779,
                        "accuracy": 0.009765625,
                        "total_cost": 80062061.98427984
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6388
Profiling... [2048/50176]	Loss: 4.6155
Profiling... [3072/50176]	Loss: 4.6336
Profiling... [4096/50176]	Loss: 4.5829
Profiling... [5120/50176]	Loss: 4.5727
Profiling... [6144/50176]	Loss: 4.5285
Profiling... [7168/50176]	Loss: 4.5436
Profiling... [8192/50176]	Loss: 4.5122
Profiling... [9216/50176]	Loss: 4.4995
Profiling... [10240/50176]	Loss: 4.4651
Profiling... [11264/50176]	Loss: 4.4767
Profiling... [12288/50176]	Loss: 4.4906
Profiling... [13312/50176]	Loss: 4.4500
Profile done
epoch 1 train time consumed: 12.75s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 117.43527530306905,
                        "time": 9.028866995999579,
                        "accuracy": 0.009765625,
                        "total_cost": 80952936.02720135
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6359
Profiling... [2048/50176]	Loss: 4.6260
Profiling... [3072/50176]	Loss: 4.6180
Profiling... [4096/50176]	Loss: 4.5982
Profiling... [5120/50176]	Loss: 4.5571
Profiling... [6144/50176]	Loss: 4.5515
Profiling... [7168/50176]	Loss: 4.5129
Profiling... [8192/50176]	Loss: 4.4971
Profiling... [9216/50176]	Loss: 4.4871
Profiling... [10240/50176]	Loss: 4.4860
Profiling... [11264/50176]	Loss: 4.4775
Profiling... [12288/50176]	Loss: 4.4725
Profiling... [13312/50176]	Loss: 4.4525
Profile done
epoch 1 train time consumed: 13.86s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 117.49604651793747,
                        "time": 9.959290620000047,
                        "accuracy": 0.009765625,
                        "total_cost": 89295157.03162786
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6403
Profiling... [2048/50176]	Loss: 4.6279
Profiling... [3072/50176]	Loss: 4.6007
Profiling... [4096/50176]	Loss: 4.6127
Profiling... [5120/50176]	Loss: 4.5763
Profiling... [6144/50176]	Loss: 4.5731
Profiling... [7168/50176]	Loss: 4.5300
Profiling... [8192/50176]	Loss: 4.5124
Profiling... [9216/50176]	Loss: 4.4772
Profiling... [10240/50176]	Loss: 4.4697
Profiling... [11264/50176]	Loss: 4.4620
Profiling... [12288/50176]	Loss: 4.4601
Profiling... [13312/50176]	Loss: 4.3940
Profile done
epoch 1 train time consumed: 25.11s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 117.02563211195987,
                        "time": 18.57970644099987,
                        "accuracy": 0.009765625,
                        "total_cost": 166585493.96816334
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6339
Profiling... [2048/50176]	Loss: 5.6037
Profiling... [3072/50176]	Loss: 5.1039
Profiling... [4096/50176]	Loss: 4.7830
Profiling... [5120/50176]	Loss: 4.7055
Profiling... [6144/50176]	Loss: 4.7553
Profiling... [7168/50176]	Loss: 4.7306
Profiling... [8192/50176]	Loss: 4.6607
Profiling... [9216/50176]	Loss: 4.6412
Profiling... [10240/50176]	Loss: 4.5563
Profiling... [11264/50176]	Loss: 4.5560
Profiling... [12288/50176]	Loss: 4.5231
Profiling... [13312/50176]	Loss: 4.4734
Profile done
epoch 1 train time consumed: 12.64s
Validation Epoch: 0, Average loss: 0.0047, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 117.14814888152104,
                        "time": 8.908989510000083,
                        "accuracy": 0.009765625,
                        "total_cost": 79877981.99703123
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6380
Profiling... [2048/50176]	Loss: 5.4980
Profiling... [3072/50176]	Loss: 5.0480
Profiling... [4096/50176]	Loss: 4.8240
Profiling... [5120/50176]	Loss: 4.7641
Profiling... [6144/50176]	Loss: 4.8142
Profiling... [7168/50176]	Loss: 4.6914
Profiling... [8192/50176]	Loss: 4.7060
Profiling... [9216/50176]	Loss: 4.6079
Profiling... [10240/50176]	Loss: 4.6262
Profiling... [11264/50176]	Loss: 4.5862
Profiling... [12288/50176]	Loss: 4.6431
Profiling... [13312/50176]	Loss: 4.6739
Profile done
epoch 1 train time consumed: 12.72s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 117.40436741687809,
                        "time": 9.025242572000025,
                        "accuracy": 0.009765625,
                        "total_cost": 80920425.11334163
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6373
Profiling... [2048/50176]	Loss: 5.5260
Profiling... [3072/50176]	Loss: 4.9418
Profiling... [4096/50176]	Loss: 4.7589
Profiling... [5120/50176]	Loss: 4.7536
Profiling... [6144/50176]	Loss: 4.6915
Profiling... [7168/50176]	Loss: 4.7419
Profiling... [8192/50176]	Loss: 4.7335
Profiling... [9216/50176]	Loss: 4.6677
Profiling... [10240/50176]	Loss: 4.7826
Profiling... [11264/50176]	Loss: 4.6336
Profiling... [12288/50176]	Loss: 4.6032
Profiling... [13312/50176]	Loss: 4.5694
Profile done
epoch 1 train time consumed: 13.88s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 117.45819100472224,
                        "time": 9.939532305000284,
                        "accuracy": 0.009765625,
                        "total_cost": 89117984.40238224
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6260
Profiling... [2048/50176]	Loss: 5.6661
Profiling... [3072/50176]	Loss: 5.1073
Profiling... [4096/50176]	Loss: 4.8385
Profiling... [5120/50176]	Loss: 4.7995
Profiling... [6144/50176]	Loss: 4.6449
Profiling... [7168/50176]	Loss: 4.6074
Profiling... [8192/50176]	Loss: 4.6287
Profiling... [9216/50176]	Loss: 4.5816
Profiling... [10240/50176]	Loss: 4.5620
Profiling... [11264/50176]	Loss: 4.5032
Profiling... [12288/50176]	Loss: 4.5143
Profiling... [13312/50176]	Loss: 4.5180
Profile done
epoch 1 train time consumed: 25.40s
Validation Epoch: 0, Average loss: 0.0047, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 116.99869757039379,
                        "time": 18.64019964299996,
                        "accuracy": 0.009765625,
                        "total_cost": 167127849.81021062
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6508
Profiling... [2048/50176]	Loss: 5.5595
Profiling... [3072/50176]	Loss: 4.9817
Profiling... [4096/50176]	Loss: 4.6929
Profiling... [5120/50176]	Loss: 4.7367
Profiling... [6144/50176]	Loss: 4.6988
Profiling... [7168/50176]	Loss: 4.6985
Profiling... [8192/50176]	Loss: 4.6160
Profiling... [9216/50176]	Loss: 4.6953
Profiling... [10240/50176]	Loss: 4.5736
Profiling... [11264/50176]	Loss: 4.6216
Profiling... [12288/50176]	Loss: 4.6037
Profiling... [13312/50176]	Loss: 4.4959
Profile done
epoch 1 train time consumed: 12.60s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 117.1197456871762,
                        "time": 8.900188788999913,
                        "accuracy": 0.009765625,
                        "total_cost": 79799061.80723304
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6326
Profiling... [2048/50176]	Loss: 5.5429
Profiling... [3072/50176]	Loss: 5.0409
Profiling... [4096/50176]	Loss: 4.7377
Profiling... [5120/50176]	Loss: 4.8805
Profiling... [6144/50176]	Loss: 4.8343
Profiling... [7168/50176]	Loss: 4.7058
Profiling... [8192/50176]	Loss: 4.6596
Profiling... [9216/50176]	Loss: 4.6007
Profiling... [10240/50176]	Loss: 4.5933
Profiling... [11264/50176]	Loss: 4.6483
Profiling... [12288/50176]	Loss: 4.7023
Profiling... [13312/50176]	Loss: 4.5714
Profile done
epoch 1 train time consumed: 12.75s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 117.35640855333807,
                        "time": 9.01676122400022,
                        "accuracy": 0.009765625,
                        "total_cost": 80844359.11240038
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6285
Profiling... [2048/50176]	Loss: 5.6456
Profiling... [3072/50176]	Loss: 4.9731
Profiling... [4096/50176]	Loss: 4.6717
Profiling... [5120/50176]	Loss: 4.8267
Profiling... [6144/50176]	Loss: 4.5906
Profiling... [7168/50176]	Loss: 4.6240
Profiling... [8192/50176]	Loss: 4.6576
Profiling... [9216/50176]	Loss: 4.5768
Profiling... [10240/50176]	Loss: 4.5698
Profiling... [11264/50176]	Loss: 4.4954
Profiling... [12288/50176]	Loss: 4.5739
Profiling... [13312/50176]	Loss: 4.5201
Profile done
epoch 1 train time consumed: 14.11s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 117.41295971281365,
                        "time": 9.939315910999994,
                        "accuracy": 0.009765625,
                        "total_cost": 89116021.19288987
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6287
Profiling... [2048/50176]	Loss: 5.5085
Profiling... [3072/50176]	Loss: 5.1020
Profiling... [4096/50176]	Loss: 4.8153
Profiling... [5120/50176]	Loss: 4.6959
Profiling... [6144/50176]	Loss: 4.6600
Profiling... [7168/50176]	Loss: 4.8233
Profiling... [8192/50176]	Loss: 4.6863
Profiling... [9216/50176]	Loss: 4.6844
Profiling... [10240/50176]	Loss: 4.6671
Profiling... [11264/50176]	Loss: 4.6670
Profiling... [12288/50176]	Loss: 4.6695
Profiling... [13312/50176]	Loss: 4.5575
Profile done
epoch 1 train time consumed: 25.18s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 116.9906654071476,
                        "time": 18.563122757999736,
                        "accuracy": 0.009765625,
                        "total_cost": 166436771.5703525
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6328
Profiling... [2048/50176]	Loss: 5.5104
Profiling... [3072/50176]	Loss: 4.9954
Profiling... [4096/50176]	Loss: 4.7251
Profiling... [5120/50176]	Loss: 4.6987
Profiling... [6144/50176]	Loss: 4.6861
Profiling... [7168/50176]	Loss: 4.6849
Profiling... [8192/50176]	Loss: 4.6936
Profiling... [9216/50176]	Loss: 4.6251
Profiling... [10240/50176]	Loss: 4.5877
Profiling... [11264/50176]	Loss: 4.5301
Profiling... [12288/50176]	Loss: 4.5021
Profiling... [13312/50176]	Loss: 4.4485
Profile done
epoch 1 train time consumed: 12.66s
Validation Epoch: 0, Average loss: 0.0047, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 117.09886847771703,
                        "time": 8.912038009999833,
                        "accuracy": 0.009765625,
                        "total_cost": 79905292.35541873
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6273
Profiling... [2048/50176]	Loss: 5.4920
Profiling... [3072/50176]	Loss: 4.9803
Profiling... [4096/50176]	Loss: 4.7350
Profiling... [5120/50176]	Loss: 4.6283
Profiling... [6144/50176]	Loss: 4.6733
Profiling... [7168/50176]	Loss: 4.5909
Profiling... [8192/50176]	Loss: 4.6380
Profiling... [9216/50176]	Loss: 4.5608
Profiling... [10240/50176]	Loss: 4.6022
Profiling... [11264/50176]	Loss: 4.5666
Profiling... [12288/50176]	Loss: 4.6079
Profiling... [13312/50176]	Loss: 4.5716
Profile done
epoch 1 train time consumed: 12.88s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 117.3276077674519,
                        "time": 9.014229796000109,
                        "accuracy": 0.009765625,
                        "total_cost": 80821649.01467392
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6441
Profiling... [2048/50176]	Loss: 5.5233
Profiling... [3072/50176]	Loss: 4.9711
Profiling... [4096/50176]	Loss: 4.6297
Profiling... [5120/50176]	Loss: 4.6241
Profiling... [6144/50176]	Loss: 4.8043
Profiling... [7168/50176]	Loss: 4.7379
Profiling... [8192/50176]	Loss: 4.6500
Profiling... [9216/50176]	Loss: 4.6174
Profiling... [10240/50176]	Loss: 4.5839
Profiling... [11264/50176]	Loss: 4.5755
Profiling... [12288/50176]	Loss: 4.5084
Profiling... [13312/50176]	Loss: 4.5084
Profile done
epoch 1 train time consumed: 13.93s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 117.37688946281754,
                        "time": 9.975623964000079,
                        "accuracy": 0.009765625,
                        "total_cost": 89441541.19226156
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6327
Profiling... [2048/50176]	Loss: 5.4251
Profiling... [3072/50176]	Loss: 5.1536
Profiling... [4096/50176]	Loss: 4.8450
Profiling... [5120/50176]	Loss: 4.7179
Profiling... [6144/50176]	Loss: 4.6703
Profiling... [7168/50176]	Loss: 4.6318
Profiling... [8192/50176]	Loss: 4.6664
Profiling... [9216/50176]	Loss: 4.5923
Profiling... [10240/50176]	Loss: 4.6187
Profiling... [11264/50176]	Loss: 4.4934
Profiling... [12288/50176]	Loss: 4.6072
Profiling... [13312/50176]	Loss: 4.4412
Profile done
epoch 1 train time consumed: 25.17s
Validation Epoch: 0, Average loss: 0.0047, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 116.9831543710259,
                        "time": 18.583291358000224,
                        "accuracy": 0.009765625,
                        "total_cost": 166617595.88821474
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6231
Profiling... [2048/50176]	Loss: 7.0478
Profiling... [3072/50176]	Loss: 5.2139
Profiling... [4096/50176]	Loss: 4.7202
Profiling... [5120/50176]	Loss: 5.7676
Profiling... [6144/50176]	Loss: 4.8991
Profiling... [7168/50176]	Loss: 5.5166
Profiling... [8192/50176]	Loss: 5.5377
Profiling... [9216/50176]	Loss: 5.0102
Profiling... [10240/50176]	Loss: 5.1196
Profiling... [11264/50176]	Loss: 4.7850
Profiling... [12288/50176]	Loss: 4.8154
Profiling... [13312/50176]	Loss: 4.7832
Profile done
epoch 1 train time consumed: 12.62s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 117.09181748287688,
                        "time": 8.90595975999986,
                        "accuracy": 0.009765625,
                        "total_cost": 79850791.5783528
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6270
Profiling... [2048/50176]	Loss: 7.0502
Profiling... [3072/50176]	Loss: 5.2939
Profiling... [4096/50176]	Loss: 4.8987
Profiling... [5120/50176]	Loss: 4.9529
Profiling... [6144/50176]	Loss: 4.8364
Profiling... [7168/50176]	Loss: 4.8465
Profiling... [8192/50176]	Loss: 6.1022
Profiling... [9216/50176]	Loss: 5.1048
Profiling... [10240/50176]	Loss: 5.1390
Profiling... [11264/50176]	Loss: 4.8173
Profiling... [12288/50176]	Loss: 4.7503
Profiling... [13312/50176]	Loss: 4.7949
Profile done
epoch 1 train time consumed: 12.73s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 117.3005829833474,
                        "time": 8.995931428000404,
                        "accuracy": 0.009765625,
                        "total_cost": 80657573.26853393
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6410
Profiling... [2048/50176]	Loss: 7.1537
Profiling... [3072/50176]	Loss: 5.6215
Profiling... [4096/50176]	Loss: 5.0807
Profiling... [5120/50176]	Loss: 4.6256
Profiling... [6144/50176]	Loss: 4.7410
Profiling... [7168/50176]	Loss: 4.8437
Profiling... [8192/50176]	Loss: 4.6275
Profiling... [9216/50176]	Loss: 4.6099
Profiling... [10240/50176]	Loss: 4.8481
Profiling... [11264/50176]	Loss: 4.7033
Profiling... [12288/50176]	Loss: 4.7532
Profiling... [13312/50176]	Loss: 4.6805
Profile done
epoch 1 train time consumed: 14.02s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 117.34587932046921,
                        "time": 9.92220599500024,
                        "accuracy": 0.009765625,
                        "total_cost": 88962579.410551
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6444
Profiling... [2048/50176]	Loss: 7.4732
Profiling... [3072/50176]	Loss: 5.7887
Profiling... [4096/50176]	Loss: 5.0112
Profiling... [5120/50176]	Loss: 4.8471
Profiling... [6144/50176]	Loss: 4.6540
Profiling... [7168/50176]	Loss: 5.7983
Profiling... [8192/50176]	Loss: 4.6556
Profiling... [9216/50176]	Loss: 4.6495
Profiling... [10240/50176]	Loss: 4.7711
Profiling... [11264/50176]	Loss: 4.7797
Profiling... [12288/50176]	Loss: 4.6297
Profiling... [13312/50176]	Loss: 4.6797
Profile done
epoch 1 train time consumed: 25.21s
Validation Epoch: 0, Average loss: 0.0050, Accuracy: 0.0102
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 116.97122157104563,
                        "time": 18.724083362999863,
                        "accuracy": 0.01015625,
                        "total_cost": 161423003.93495354
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6296
Profiling... [2048/50176]	Loss: 7.1556
Profiling... [3072/50176]	Loss: 5.3696
Profiling... [4096/50176]	Loss: 5.0430
Profiling... [5120/50176]	Loss: 4.7332
Profiling... [6144/50176]	Loss: 4.6611
Profiling... [7168/50176]	Loss: 4.7556
Profiling... [8192/50176]	Loss: 4.6170
Profiling... [9216/50176]	Loss: 5.2853
Profiling... [10240/50176]	Loss: 4.5866
Profiling... [11264/50176]	Loss: 4.6698
Profiling... [12288/50176]	Loss: 4.6146
Profiling... [13312/50176]	Loss: 4.6671
Profile done
epoch 1 train time consumed: 12.61s
Validation Epoch: 0, Average loss: 0.0174, Accuracy: 0.0112
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 117.07446904811013,
                        "time": 8.91127928800006,
                        "accuracy": 0.01123046875,
                        "total_cost": 69476937.84781657
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6317
Profiling... [2048/50176]	Loss: 7.3624
Profiling... [3072/50176]	Loss: 5.2780
Profiling... [4096/50176]	Loss: 4.8126
Profiling... [5120/50176]	Loss: 4.8551
Profiling... [6144/50176]	Loss: 4.7716
Profiling... [7168/50176]	Loss: 4.7871
Profiling... [8192/50176]	Loss: 4.7669
Profiling... [9216/50176]	Loss: 4.6268
Profiling... [10240/50176]	Loss: 4.8428
Profiling... [11264/50176]	Loss: 4.6368
Profiling... [12288/50176]	Loss: 4.6874
Profiling... [13312/50176]	Loss: 4.6520
Profile done
epoch 1 train time consumed: 12.77s
Validation Epoch: 0, Average loss: 0.0064, Accuracy: 0.0104
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 117.27964829943367,
                        "time": 9.001194783000301,
                        "accuracy": 0.0103515625,
                        "total_cost": 76136561.2188246
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6371
Profiling... [2048/50176]	Loss: 7.1198
Profiling... [3072/50176]	Loss: 5.5888
Profiling... [4096/50176]	Loss: 4.9856
Profiling... [5120/50176]	Loss: 5.0290
Profiling... [6144/50176]	Loss: 4.8508
Profiling... [7168/50176]	Loss: 4.6047
Profiling... [8192/50176]	Loss: 4.8495
Profiling... [9216/50176]	Loss: 4.6473
Profiling... [10240/50176]	Loss: 4.6353
Profiling... [11264/50176]	Loss: 4.7304
Profiling... [12288/50176]	Loss: 4.6727
Profiling... [13312/50176]	Loss: 4.6199
Profile done
epoch 1 train time consumed: 15.06s
Validation Epoch: 0, Average loss: 0.0075, Accuracy: 0.0109
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 117.26836040077251,
                        "time": 9.924169698000242,
                        "accuracy": 0.0109375,
                        "total_cost": 79446559.46326244
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6312
Profiling... [2048/50176]	Loss: 7.0510
Profiling... [3072/50176]	Loss: 5.2141
Profiling... [4096/50176]	Loss: 4.8627
Profiling... [5120/50176]	Loss: 4.6985
Profiling... [6144/50176]	Loss: 5.2541
Profiling... [7168/50176]	Loss: 5.3318
Profiling... [8192/50176]	Loss: 4.9602
Profiling... [9216/50176]	Loss: 4.7551
Profiling... [10240/50176]	Loss: 4.8116
Profiling... [11264/50176]	Loss: 4.6521
Profiling... [12288/50176]	Loss: 4.7174
Profiling... [13312/50176]	Loss: 4.7172
Profile done
epoch 1 train time consumed: 25.32s
Validation Epoch: 0, Average loss: 0.0136, Accuracy: 0.0100
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 116.91945579111635,
                        "time": 18.700742247000107,
                        "accuracy": 0.0099609375,
                        "total_cost": 164382939.54918543
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6385
Profiling... [2048/50176]	Loss: 7.2192
Profiling... [3072/50176]	Loss: 5.4789
Profiling... [4096/50176]	Loss: 5.1344
Profiling... [5120/50176]	Loss: 4.6918
Profiling... [6144/50176]	Loss: 5.0004
Profiling... [7168/50176]	Loss: 4.7528
Profiling... [8192/50176]	Loss: 4.6963
Profiling... [9216/50176]	Loss: 4.6774
Profiling... [10240/50176]	Loss: 4.7172
Profiling... [11264/50176]	Loss: 4.7665
Profiling... [12288/50176]	Loss: 4.6607
Profiling... [13312/50176]	Loss: 4.7108
Profile done
epoch 1 train time consumed: 12.57s
Validation Epoch: 0, Average loss: 0.0280, Accuracy: 0.0141
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 117.01115534683676,
                        "time": 8.897032649999801,
                        "accuracy": 0.0140625,
                        "total_cost": 55396329.45136
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6391
Profiling... [2048/50176]	Loss: 7.1992
Profiling... [3072/50176]	Loss: 5.3681
Profiling... [4096/50176]	Loss: 4.8427
Profiling... [5120/50176]	Loss: 5.2812
Profiling... [6144/50176]	Loss: 5.0642
Profiling... [7168/50176]	Loss: 4.8421
Profiling... [8192/50176]	Loss: 4.8785
Profiling... [9216/50176]	Loss: 4.6315
Profiling... [10240/50176]	Loss: 4.7269
Profiling... [11264/50176]	Loss: 4.6460
Profiling... [12288/50176]	Loss: 4.6901
Profiling... [13312/50176]	Loss: 4.6158
Profile done
epoch 1 train time consumed: 12.80s
Validation Epoch: 0, Average loss: 0.0047, Accuracy: 0.0115
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 117.19557973810154,
                        "time": 9.041072212000017,
                        "accuracy": 0.0115234375,
                        "total_cost": 68696828.12959857
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6157
Profiling... [2048/50176]	Loss: 6.9120
Profiling... [3072/50176]	Loss: 5.6119
Profiling... [4096/50176]	Loss: 4.8890
Profiling... [5120/50176]	Loss: 4.8597
Profiling... [6144/50176]	Loss: 4.6358
Profiling... [7168/50176]	Loss: 4.8889
Profiling... [8192/50176]	Loss: 4.6312
Profiling... [9216/50176]	Loss: 4.7874
Profiling... [10240/50176]	Loss: 4.6453
Profiling... [11264/50176]	Loss: 4.7751
Profiling... [12288/50176]	Loss: 4.7211
Profiling... [13312/50176]	Loss: 4.6687
Profile done
epoch 1 train time consumed: 13.99s
Validation Epoch: 0, Average loss: 0.0053, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 117.23647578012546,
                        "time": 9.929740704999858,
                        "accuracy": 0.00986328125,
                        "total_cost": 88148594.1192562
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6247
Profiling... [2048/50176]	Loss: 7.3132
Profiling... [3072/50176]	Loss: 5.2791
Profiling... [4096/50176]	Loss: 4.8164
Profiling... [5120/50176]	Loss: 5.0866
Profiling... [6144/50176]	Loss: 4.8950
Profiling... [7168/50176]	Loss: 4.6552
Profiling... [8192/50176]	Loss: 4.6470
Profiling... [9216/50176]	Loss: 4.6629
Profiling... [10240/50176]	Loss: 4.6021
Profiling... [11264/50176]	Loss: 4.7151
Profiling... [12288/50176]	Loss: 4.7331
Profiling... [13312/50176]	Loss: 4.7134
Profile done
epoch 1 train time consumed: 25.15s
Validation Epoch: 0, Average loss: 0.0049, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 116.89358393175179,
                        "time": 18.62774540999999,
                        "accuracy": 0.009765625,
                        "total_cost": 167016085.0263829
                    },
                    
[Training Loop] The optimal parameters are lr: 0.01 dr: 0.25 bs: 256 pl: 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[GPU_0] Set GPU power limit to 150W.
[GPU_0] Set GPU power limit to 150W.
Training Epoch: 0 [256/50176]	Loss: 4.6358
Training Epoch: 0 [512/50176]	Loss: 7.6507
Training Epoch: 0 [768/50176]	Loss: 5.8548
Training Epoch: 0 [1024/50176]	Loss: 5.1230
Training Epoch: 0 [1280/50176]	Loss: 4.9551
Training Epoch: 0 [1536/50176]	Loss: 4.8140
Training Epoch: 0 [1792/50176]	Loss: 4.7210
Training Epoch: 0 [2048/50176]	Loss: 4.9448
Training Epoch: 0 [2304/50176]	Loss: 5.3924
Training Epoch: 0 [2560/50176]	Loss: 4.6547
Training Epoch: 0 [2816/50176]	Loss: 4.7414
Training Epoch: 0 [3072/50176]	Loss: 4.8321
Training Epoch: 0 [3328/50176]	Loss: 4.8078
Training Epoch: 0 [3584/50176]	Loss: 4.9081
Training Epoch: 0 [3840/50176]	Loss: 4.6435
Training Epoch: 0 [4096/50176]	Loss: 4.7787
Training Epoch: 0 [4352/50176]	Loss: 4.6789
Training Epoch: 0 [4608/50176]	Loss: 4.7519
Training Epoch: 0 [4864/50176]	Loss: 4.6714
Training Epoch: 0 [5120/50176]	Loss: 4.6153
Training Epoch: 0 [5376/50176]	Loss: 4.6691
Training Epoch: 0 [5632/50176]	Loss: 4.6151
Training Epoch: 0 [5888/50176]	Loss: 4.5461
Training Epoch: 0 [6144/50176]	Loss: 4.6370
Training Epoch: 0 [6400/50176]	Loss: 4.5650
Training Epoch: 0 [6656/50176]	Loss: 4.5839
Training Epoch: 0 [6912/50176]	Loss: 4.5503
Training Epoch: 0 [7168/50176]	Loss: 4.5255
Training Epoch: 0 [7424/50176]	Loss: 4.5424
Training Epoch: 0 [7680/50176]	Loss: 4.5368
Training Epoch: 0 [7936/50176]	Loss: 4.4619
Training Epoch: 0 [8192/50176]	Loss: 4.5514
Training Epoch: 0 [8448/50176]	Loss: 4.4363
Training Epoch: 0 [8704/50176]	Loss: 4.5064
Training Epoch: 0 [8960/50176]	Loss: 4.5199
Training Epoch: 0 [9216/50176]	Loss: 4.4483
Training Epoch: 0 [9472/50176]	Loss: 4.4329
Training Epoch: 0 [9728/50176]	Loss: 4.4100
Training Epoch: 0 [9984/50176]	Loss: 4.3992
Training Epoch: 0 [10240/50176]	Loss: 4.4482
Training Epoch: 0 [10496/50176]	Loss: 4.4080
Training Epoch: 0 [10752/50176]	Loss: 4.4167
Training Epoch: 0 [11008/50176]	Loss: 4.4713
Training Epoch: 0 [11264/50176]	Loss: 4.3208
Training Epoch: 0 [11520/50176]	Loss: 4.3365
Training Epoch: 0 [11776/50176]	Loss: 4.4213
Training Epoch: 0 [12032/50176]	Loss: 4.3684
Training Epoch: 0 [12288/50176]	Loss: 4.3607
Training Epoch: 0 [12544/50176]	Loss: 4.3021
Training Epoch: 0 [12800/50176]	Loss: 4.3343
Training Epoch: 0 [13056/50176]	Loss: 4.3527
Training Epoch: 0 [13312/50176]	Loss: 4.4781
Training Epoch: 0 [13568/50176]	Loss: 4.3681
Training Epoch: 0 [13824/50176]	Loss: 4.3775
Training Epoch: 0 [14080/50176]	Loss: 4.3909
Training Epoch: 0 [14336/50176]	Loss: 4.3473
Training Epoch: 0 [14592/50176]	Loss: 4.3806
Training Epoch: 0 [14848/50176]	Loss: 4.3941
Training Epoch: 0 [15104/50176]	Loss: 4.3128
Training Epoch: 0 [15360/50176]	Loss: 4.4220
Training Epoch: 0 [15616/50176]	Loss: 4.3877
Training Epoch: 0 [15872/50176]	Loss: 4.3439
Training Epoch: 0 [16128/50176]	Loss: 4.3415
Training Epoch: 0 [16384/50176]	Loss: 4.2601
Training Epoch: 0 [16640/50176]	Loss: 4.3219
Training Epoch: 0 [16896/50176]	Loss: 4.3480
Training Epoch: 0 [17152/50176]	Loss: 4.3317
Training Epoch: 0 [17408/50176]	Loss: 4.2768
Training Epoch: 0 [17664/50176]	Loss: 4.3142
Training Epoch: 0 [17920/50176]	Loss: 4.2966
Training Epoch: 0 [18176/50176]	Loss: 4.2705
Training Epoch: 0 [18432/50176]	Loss: 4.2761
Training Epoch: 0 [18688/50176]	Loss: 4.2215
Training Epoch: 0 [18944/50176]	Loss: 4.2789
Training Epoch: 0 [19200/50176]	Loss: 4.2908
Training Epoch: 0 [19456/50176]	Loss: 4.3315
Training Epoch: 0 [19712/50176]	Loss: 4.3365
Training Epoch: 0 [19968/50176]	Loss: 4.2995
Training Epoch: 0 [20224/50176]	Loss: 4.2289
Training Epoch: 0 [20480/50176]	Loss: 4.1839
Training Epoch: 0 [20736/50176]	Loss: 4.2751
Training Epoch: 0 [20992/50176]	Loss: 4.3178
Training Epoch: 0 [21248/50176]	Loss: 4.3183
Training Epoch: 0 [21504/50176]	Loss: 4.2416
Training Epoch: 0 [21760/50176]	Loss: 4.2617
Training Epoch: 0 [22016/50176]	Loss: 4.1734
Training Epoch: 0 [22272/50176]	Loss: 4.2127
Training Epoch: 0 [22528/50176]	Loss: 4.2410
Training Epoch: 0 [22784/50176]	Loss: 4.2465
Training Epoch: 0 [23040/50176]	Loss: 4.2298
Training Epoch: 0 [23296/50176]	Loss: 4.2056
Training Epoch: 0 [23552/50176]	Loss: 4.2357
Training Epoch: 0 [23808/50176]	Loss: 4.1926
Training Epoch: 0 [24064/50176]	Loss: 4.2005
Training Epoch: 0 [24320/50176]	Loss: 4.1749
Training Epoch: 0 [24576/50176]	Loss: 4.1266
Training Epoch: 0 [24832/50176]	Loss: 4.2587
Training Epoch: 0 [25088/50176]	Loss: 4.1438
Training Epoch: 0 [25344/50176]	Loss: 4.2073
Training Epoch: 0 [25600/50176]	Loss: 4.1612
Training Epoch: 0 [25856/50176]	Loss: 4.2331
Training Epoch: 0 [26112/50176]	Loss: 4.1872
Training Epoch: 0 [26368/50176]	Loss: 4.2785
Training Epoch: 0 [26624/50176]	Loss: 4.1412
Training Epoch: 0 [26880/50176]	Loss: 4.1849
Training Epoch: 0 [27136/50176]	Loss: 4.1991
Training Epoch: 0 [27392/50176]	Loss: 4.2072
Training Epoch: 0 [27648/50176]	Loss: 4.1612
Training Epoch: 0 [27904/50176]	Loss: 4.1693
Training Epoch: 0 [28160/50176]	Loss: 4.1757
Training Epoch: 0 [28416/50176]	Loss: 4.1481
Training Epoch: 0 [28672/50176]	Loss: 4.1523
Training Epoch: 0 [28928/50176]	Loss: 4.1799
Training Epoch: 0 [29184/50176]	Loss: 4.1342
Training Epoch: 0 [29440/50176]	Loss: 4.2496
Training Epoch: 0 [29696/50176]	Loss: 4.2046
Training Epoch: 0 [29952/50176]	Loss: 4.2603
Training Epoch: 0 [30208/50176]	Loss: 4.1180
Training Epoch: 0 [30464/50176]	Loss: 4.1937
Training Epoch: 0 [30720/50176]	Loss: 4.2055
Training Epoch: 0 [30976/50176]	Loss: 4.1529
Training Epoch: 0 [31232/50176]	Loss: 4.0981
Training Epoch: 0 [31488/50176]	Loss: 4.1504
Training Epoch: 0 [31744/50176]	Loss: 4.0972
Training Epoch: 0 [32000/50176]	Loss: 4.1100
Training Epoch: 0 [32256/50176]	Loss: 4.0300
Training Epoch: 0 [32512/50176]	Loss: 4.1993
Training Epoch: 0 [32768/50176]	Loss: 3.9188
Training Epoch: 0 [33024/50176]	Loss: 4.0620
Training Epoch: 0 [33280/50176]	Loss: 4.1051
Training Epoch: 0 [33536/50176]	Loss: 4.0950
Training Epoch: 0 [33792/50176]	Loss: 4.0760
Training Epoch: 0 [34048/50176]	Loss: 4.0393
Training Epoch: 0 [34304/50176]	Loss: 4.1330
Training Epoch: 0 [34560/50176]	Loss: 4.1607
Training Epoch: 0 [34816/50176]	Loss: 4.0763
Training Epoch: 0 [35072/50176]	Loss: 4.0689
Training Epoch: 0 [35328/50176]	Loss: 4.0639
Training Epoch: 0 [35584/50176]	Loss: 4.1537
Training Epoch: 0 [35840/50176]	Loss: 4.2876
Training Epoch: 0 [36096/50176]	Loss: 3.9505
Training Epoch: 0 [36352/50176]	Loss: 4.0371
Training Epoch: 0 [36608/50176]	Loss: 4.0976
Training Epoch: 0 [36864/50176]	Loss: 4.0347
Training Epoch: 0 [37120/50176]	Loss: 4.0802
Training Epoch: 0 [37376/50176]	Loss: 4.0952
Training Epoch: 0 [37632/50176]	Loss: 4.0141
Training Epoch: 0 [37888/50176]	Loss: 4.0939
Training Epoch: 0 [38144/50176]	Loss: 3.9971
Training Epoch: 0 [38400/50176]	Loss: 4.1171
Training Epoch: 0 [38656/50176]	Loss: 3.9900
Training Epoch: 0 [38912/50176]	Loss: 4.0882
Training Epoch: 0 [39168/50176]	Loss: 4.0980
Training Epoch: 0 [39424/50176]	Loss: 4.0961
Training Epoch: 0 [39680/50176]	Loss: 4.0689
Training Epoch: 0 [39936/50176]	Loss: 4.0664
Training Epoch: 0 [40192/50176]	Loss: 4.1970
Training Epoch: 0 [40448/50176]	Loss: 3.9331
Training Epoch: 0 [40704/50176]	Loss: 3.9495
Training Epoch: 0 [40960/50176]	Loss: 4.0220
Training Epoch: 0 [41216/50176]	Loss: 4.0861
Training Epoch: 0 [41472/50176]	Loss: 4.0256
Training Epoch: 0 [41728/50176]	Loss: 3.9402
Training Epoch: 0 [41984/50176]	Loss: 4.0685
Training Epoch: 0 [42240/50176]	Loss: 3.9577
Training Epoch: 0 [42496/50176]	Loss: 3.9501
Training Epoch: 0 [42752/50176]	Loss: 4.0693
Training Epoch: 0 [43008/50176]	Loss: 3.9665
Training Epoch: 0 [43264/50176]	Loss: 3.8975
Training Epoch: 0 [43520/50176]	Loss: 3.9464
Training Epoch: 0 [43776/50176]	Loss: 4.0918
Training Epoch: 0 [44032/50176]	Loss: 3.9848
Training Epoch: 0 [44288/50176]	Loss: 3.9992
Training Epoch: 0 [44544/50176]	Loss: 3.9024
Training Epoch: 0 [44800/50176]	Loss: 4.0085
Training Epoch: 0 [45056/50176]	Loss: 3.9589
Training Epoch: 0 [45312/50176]	Loss: 3.9170
Training Epoch: 0 [45568/50176]	Loss: 3.9169
Training Epoch: 0 [45824/50176]	Loss: 4.0148
Training Epoch: 0 [46080/50176]	Loss: 3.9860
Training Epoch: 0 [46336/50176]	Loss: 3.8944
Training Epoch: 0 [46592/50176]	Loss: 3.9317
Training Epoch: 0 [46848/50176]	Loss: 4.0029
Training Epoch: 0 [47104/50176]	Loss: 4.0626
Training Epoch: 0 [47360/50176]	Loss: 3.8001
Training Epoch: 0 [47616/50176]	Loss: 3.9062
Training Epoch: 0 [47872/50176]	Loss: 3.9695
Training Epoch: 0 [48128/50176]	Loss: 3.9305
Training Epoch: 0 [48384/50176]	Loss: 3.9319
Training Epoch: 0 [48640/50176]	Loss: 4.0377
Training Epoch: 0 [48896/50176]	Loss: 3.9422
Training Epoch: 0 [49152/50176]	Loss: 3.9286
Training Epoch: 0 [49408/50176]	Loss: 3.8081
Training Epoch: 0 [49664/50176]	Loss: 3.8896
Training Epoch: 0 [49920/50176]	Loss: 3.8940
Training Epoch: 0 [50176/50176]	Loss: 3.9724
Validation Epoch: 0, Average loss: 0.0159, Accuracy: 0.0790
Training Epoch: 1 [256/50176]	Loss: 4.0182
Training Epoch: 1 [512/50176]	Loss: 4.0048
Training Epoch: 1 [768/50176]	Loss: 3.8330
Training Epoch: 1 [1024/50176]	Loss: 3.9359
Training Epoch: 1 [1280/50176]	Loss: 3.8486
Training Epoch: 1 [1536/50176]	Loss: 4.0207
Training Epoch: 1 [1792/50176]	Loss: 3.9295
Training Epoch: 1 [2048/50176]	Loss: 3.8488
Training Epoch: 1 [2304/50176]	Loss: 3.9350
Training Epoch: 1 [2560/50176]	Loss: 3.8535
Training Epoch: 1 [2816/50176]	Loss: 3.8926
Training Epoch: 1 [3072/50176]	Loss: 3.9150
Training Epoch: 1 [3328/50176]	Loss: 3.8528
Training Epoch: 1 [3584/50176]	Loss: 3.9582
Training Epoch: 1 [3840/50176]	Loss: 3.8257
Training Epoch: 1 [4096/50176]	Loss: 3.8249
Training Epoch: 1 [4352/50176]	Loss: 3.8927
Training Epoch: 1 [4608/50176]	Loss: 3.7344
Training Epoch: 1 [4864/50176]	Loss: 3.9126
Training Epoch: 1 [5120/50176]	Loss: 3.9180
Training Epoch: 1 [5376/50176]	Loss: 3.8296
Training Epoch: 1 [5632/50176]	Loss: 3.9190
Training Epoch: 1 [5888/50176]	Loss: 3.8587
Training Epoch: 1 [6144/50176]	Loss: 3.8465
Training Epoch: 1 [6400/50176]	Loss: 3.9027
Training Epoch: 1 [6656/50176]	Loss: 3.8005
Training Epoch: 1 [6912/50176]	Loss: 3.8857
Training Epoch: 1 [7168/50176]	Loss: 3.8243
Training Epoch: 1 [7424/50176]	Loss: 4.0117
Training Epoch: 1 [7680/50176]	Loss: 3.9394
Training Epoch: 1 [7936/50176]	Loss: 3.8057
Training Epoch: 1 [8192/50176]	Loss: 3.8700
Training Epoch: 1 [8448/50176]	Loss: 3.9553
Training Epoch: 1 [8704/50176]	Loss: 3.8652
Training Epoch: 1 [8960/50176]	Loss: 3.9330
Training Epoch: 1 [9216/50176]	Loss: 3.7727
Training Epoch: 1 [9472/50176]	Loss: 3.8500
Training Epoch: 1 [9728/50176]	Loss: 3.8198
Training Epoch: 1 [9984/50176]	Loss: 3.8452
Training Epoch: 1 [10240/50176]	Loss: 3.7624
Training Epoch: 1 [10496/50176]	Loss: 3.8603
Training Epoch: 1 [10752/50176]	Loss: 3.7742
Training Epoch: 1 [11008/50176]	Loss: 3.7921
Training Epoch: 1 [11264/50176]	Loss: 3.8186
Training Epoch: 1 [11520/50176]	Loss: 3.8446
Training Epoch: 1 [11776/50176]	Loss: 3.8519
Training Epoch: 1 [12032/50176]	Loss: 3.7508
Training Epoch: 1 [12288/50176]	Loss: 3.8656
Training Epoch: 1 [12544/50176]	Loss: 3.7747
Training Epoch: 1 [12800/50176]	Loss: 3.7827
Training Epoch: 1 [13056/50176]	Loss: 3.7865
Training Epoch: 1 [13312/50176]	Loss: 3.9310
Training Epoch: 1 [13568/50176]	Loss: 3.7128
Training Epoch: 1 [13824/50176]	Loss: 3.7865
Training Epoch: 1 [14080/50176]	Loss: 3.7620
Training Epoch: 1 [14336/50176]	Loss: 3.6822
Training Epoch: 1 [14592/50176]	Loss: 3.7530
Training Epoch: 1 [14848/50176]	Loss: 3.9362
Training Epoch: 1 [15104/50176]	Loss: 3.7754
Training Epoch: 1 [15360/50176]	Loss: 3.8161
Training Epoch: 1 [15616/50176]	Loss: 3.6870
Training Epoch: 1 [15872/50176]	Loss: 3.8939
Training Epoch: 1 [16128/50176]	Loss: 3.7750
Training Epoch: 1 [16384/50176]	Loss: 3.7204
Training Epoch: 1 [16640/50176]	Loss: 3.6849
Training Epoch: 1 [16896/50176]	Loss: 3.6146
Training Epoch: 1 [17152/50176]	Loss: 3.8046
Training Epoch: 1 [17408/50176]	Loss: 3.7166
Training Epoch: 1 [17664/50176]	Loss: 3.7840
Training Epoch: 1 [17920/50176]	Loss: 3.7154
Training Epoch: 1 [18176/50176]	Loss: 3.7490
Training Epoch: 1 [18432/50176]	Loss: 3.6285
Training Epoch: 1 [18688/50176]	Loss: 3.6876
Training Epoch: 1 [18944/50176]	Loss: 3.8474
Training Epoch: 1 [19200/50176]	Loss: 3.6752
Training Epoch: 1 [19456/50176]	Loss: 3.8303
Training Epoch: 1 [19712/50176]	Loss: 3.6777
Training Epoch: 1 [19968/50176]	Loss: 3.7403
Training Epoch: 1 [20224/50176]	Loss: 3.6605
Training Epoch: 1 [20480/50176]	Loss: 3.7745
Training Epoch: 1 [20736/50176]	Loss: 3.6359
Training Epoch: 1 [20992/50176]	Loss: 3.7208
Training Epoch: 1 [21248/50176]	Loss: 3.6662
Training Epoch: 1 [21504/50176]	Loss: 3.8909
Training Epoch: 1 [21760/50176]	Loss: 3.6572
Training Epoch: 1 [22016/50176]	Loss: 3.7687
Training Epoch: 1 [22272/50176]	Loss: 3.7264
Training Epoch: 1 [22528/50176]	Loss: 3.7094
Training Epoch: 1 [22784/50176]	Loss: 3.8405
Training Epoch: 1 [23040/50176]	Loss: 3.7985
Training Epoch: 1 [23296/50176]	Loss: 3.6375
Training Epoch: 1 [23552/50176]	Loss: 3.7039
Training Epoch: 1 [23808/50176]	Loss: 3.6209
Training Epoch: 1 [24064/50176]	Loss: 3.7107
Training Epoch: 1 [24320/50176]	Loss: 3.8503
Training Epoch: 1 [24576/50176]	Loss: 3.6699
Training Epoch: 1 [24832/50176]	Loss: 3.5508
Training Epoch: 1 [25088/50176]	Loss: 3.7077
Training Epoch: 1 [25344/50176]	Loss: 3.6805
Training Epoch: 1 [25600/50176]	Loss: 3.5778
Training Epoch: 1 [25856/50176]	Loss: 3.7633
Training Epoch: 1 [26112/50176]	Loss: 3.6141
Training Epoch: 1 [26368/50176]	Loss: 3.8816
Training Epoch: 1 [26624/50176]	Loss: 3.7704
Training Epoch: 1 [26880/50176]	Loss: 3.6855
Training Epoch: 1 [27136/50176]	Loss: 3.6994
Training Epoch: 1 [27392/50176]	Loss: 3.8131
Training Epoch: 1 [27648/50176]	Loss: 3.7255
Training Epoch: 1 [27904/50176]	Loss: 3.5691
Training Epoch: 1 [28160/50176]	Loss: 3.8829
Training Epoch: 1 [28416/50176]	Loss: 3.6296
Training Epoch: 1 [28672/50176]	Loss: 3.7262
Training Epoch: 1 [28928/50176]	Loss: 3.6709
Training Epoch: 1 [29184/50176]	Loss: 3.6796
Training Epoch: 1 [29440/50176]	Loss: 3.6614
Training Epoch: 1 [29696/50176]	Loss: 3.5878
Training Epoch: 1 [29952/50176]	Loss: 3.5941
Training Epoch: 1 [30208/50176]	Loss: 3.6246
Training Epoch: 1 [30464/50176]	Loss: 3.7146
Training Epoch: 1 [30720/50176]	Loss: 3.6852
Training Epoch: 1 [30976/50176]	Loss: 3.7644
Training Epoch: 1 [31232/50176]	Loss: 3.7022
Training Epoch: 1 [31488/50176]	Loss: 3.6712
Training Epoch: 1 [31744/50176]	Loss: 3.8009
Training Epoch: 1 [32000/50176]	Loss: 3.7166
Training Epoch: 1 [32256/50176]	Loss: 3.7238
Training Epoch: 1 [32512/50176]	Loss: 3.6569
Training Epoch: 1 [32768/50176]	Loss: 3.6651
Training Epoch: 1 [33024/50176]	Loss: 3.6172
Training Epoch: 1 [33280/50176]	Loss: 3.6767
Training Epoch: 1 [33536/50176]	Loss: 3.7148
Training Epoch: 1 [33792/50176]	Loss: 3.5964
Training Epoch: 1 [34048/50176]	Loss: 3.5712
Training Epoch: 1 [34304/50176]	Loss: 3.4656
Training Epoch: 1 [34560/50176]	Loss: 3.6269
Training Epoch: 1 [34816/50176]	Loss: 3.7461
Training Epoch: 1 [35072/50176]	Loss: 3.6966
Training Epoch: 1 [35328/50176]	Loss: 3.7737
Training Epoch: 1 [35584/50176]	Loss: 3.6037
Training Epoch: 1 [35840/50176]	Loss: 3.6729
Training Epoch: 1 [36096/50176]	Loss: 3.5985
Training Epoch: 1 [36352/50176]	Loss: 3.6935
Training Epoch: 1 [36608/50176]	Loss: 3.5426
Training Epoch: 1 [36864/50176]	Loss: 3.6430
Training Epoch: 1 [37120/50176]	Loss: 3.5574
Training Epoch: 1 [37376/50176]	Loss: 3.5699
Training Epoch: 1 [37632/50176]	Loss: 3.5119
Training Epoch: 1 [37888/50176]	Loss: 3.6294
Training Epoch: 1 [38144/50176]	Loss: 3.5123
Training Epoch: 1 [38400/50176]	Loss: 3.5163
Training Epoch: 1 [38656/50176]	Loss: 3.5161
Training Epoch: 1 [38912/50176]	Loss: 3.5320
Training Epoch: 1 [39168/50176]	Loss: 3.5121
Training Epoch: 1 [39424/50176]	Loss: 3.5527
Training Epoch: 1 [39680/50176]	Loss: 3.7041
Training Epoch: 1 [39936/50176]	Loss: 3.6206
Training Epoch: 1 [40192/50176]	Loss: 3.5871
Training Epoch: 1 [40448/50176]	Loss: 3.7540
Training Epoch: 1 [40704/50176]	Loss: 3.5754
Training Epoch: 1 [40960/50176]	Loss: 3.6026
Training Epoch: 1 [41216/50176]	Loss: 3.7882
Training Epoch: 1 [41472/50176]	Loss: 3.5734
Training Epoch: 1 [41728/50176]	Loss: 3.6618
Training Epoch: 1 [41984/50176]	Loss: 3.6107
Training Epoch: 1 [42240/50176]	Loss: 3.5689
Training Epoch: 1 [42496/50176]	Loss: 3.7434
Training Epoch: 1 [42752/50176]	Loss: 3.5398
Training Epoch: 1 [43008/50176]	Loss: 3.5204
Training Epoch: 1 [43264/50176]	Loss: 3.4973
Training Epoch: 1 [43520/50176]	Loss: 3.4675
Training Epoch: 1 [43776/50176]	Loss: 3.5230
Training Epoch: 1 [44032/50176]	Loss: 3.6461
Training Epoch: 1 [44288/50176]	Loss: 3.5874
Training Epoch: 1 [44544/50176]	Loss: 3.5690
Training Epoch: 1 [44800/50176]	Loss: 3.4828
Training Epoch: 1 [45056/50176]	Loss: 3.4891
Training Epoch: 1 [45312/50176]	Loss: 3.4807
Training Epoch: 1 [45568/50176]	Loss: 3.5665
Training Epoch: 1 [45824/50176]	Loss: 3.5387
Training Epoch: 1 [46080/50176]	Loss: 3.4862
Training Epoch: 1 [46336/50176]	Loss: 3.4577
Training Epoch: 1 [46592/50176]	Loss: 3.5572
Training Epoch: 1 [46848/50176]	Loss: 3.5091
Training Epoch: 1 [47104/50176]	Loss: 3.4752
Training Epoch: 1 [47360/50176]	Loss: 3.4460
Training Epoch: 1 [47616/50176]	Loss: 3.5672
Training Epoch: 1 [47872/50176]	Loss: 3.4442
Training Epoch: 1 [48128/50176]	Loss: 3.4941
Training Epoch: 1 [48384/50176]	Loss: 3.4760
Training Epoch: 1 [48640/50176]	Loss: 3.4028
Training Epoch: 1 [48896/50176]	Loss: 3.5422
Training Epoch: 1 [49152/50176]	Loss: 3.4010
Training Epoch: 1 [49408/50176]	Loss: 3.4413
Training Epoch: 1 [49664/50176]	Loss: 3.5164
Training Epoch: 1 [49920/50176]	Loss: 3.4040
Training Epoch: 1 [50176/50176]	Loss: 3.4178
Validation Epoch: 1, Average loss: 0.0135, Accuracy: 0.1628
Training Epoch: 2 [256/50176]	Loss: 3.3881
Training Epoch: 2 [512/50176]	Loss: 3.5920
Training Epoch: 2 [768/50176]	Loss: 3.4003
Training Epoch: 2 [1024/50176]	Loss: 3.4767
Training Epoch: 2 [1280/50176]	Loss: 3.4864
Training Epoch: 2 [1536/50176]	Loss: 3.4625
Training Epoch: 2 [1792/50176]	Loss: 3.4432
Training Epoch: 2 [2048/50176]	Loss: 3.3662
Training Epoch: 2 [2304/50176]	Loss: 3.4632
Training Epoch: 2 [2560/50176]	Loss: 3.3967
Training Epoch: 2 [2816/50176]	Loss: 3.2944
Training Epoch: 2 [3072/50176]	Loss: 3.3332
Training Epoch: 2 [3328/50176]	Loss: 3.6290
Training Epoch: 2 [3584/50176]	Loss: 3.5396
Training Epoch: 2 [3840/50176]	Loss: 3.5409
Training Epoch: 2 [4096/50176]	Loss: 3.4311
Training Epoch: 2 [4352/50176]	Loss: 3.4138
Training Epoch: 2 [4608/50176]	Loss: 3.3940
Training Epoch: 2 [4864/50176]	Loss: 3.3392
Training Epoch: 2 [5120/50176]	Loss: 3.3425
Training Epoch: 2 [5376/50176]	Loss: 3.6053
Training Epoch: 2 [5632/50176]	Loss: 3.4892
Training Epoch: 2 [5888/50176]	Loss: 3.4372
Training Epoch: 2 [6144/50176]	Loss: 3.4236
Training Epoch: 2 [6400/50176]	Loss: 3.4741
Training Epoch: 2 [6656/50176]	Loss: 3.6139
Training Epoch: 2 [6912/50176]	Loss: 3.3748
Training Epoch: 2 [7168/50176]	Loss: 3.5344
Training Epoch: 2 [7424/50176]	Loss: 3.5534
Training Epoch: 2 [7680/50176]	Loss: 3.4642
Training Epoch: 2 [7936/50176]	Loss: 3.4487
Training Epoch: 2 [8192/50176]	Loss: 3.4552
Training Epoch: 2 [8448/50176]	Loss: 3.3727
Training Epoch: 2 [8704/50176]	Loss: 3.4536
Training Epoch: 2 [8960/50176]	Loss: 3.4476
Training Epoch: 2 [9216/50176]	Loss: 3.5288
Training Epoch: 2 [9472/50176]	Loss: 3.4752
Training Epoch: 2 [9728/50176]	Loss: 3.3382
Training Epoch: 2 [9984/50176]	Loss: 3.4300
Training Epoch: 2 [10240/50176]	Loss: 3.2903
Training Epoch: 2 [10496/50176]	Loss: 3.5013
Training Epoch: 2 [10752/50176]	Loss: 3.4543
Training Epoch: 2 [11008/50176]	Loss: 3.5024
Training Epoch: 2 [11264/50176]	Loss: 3.3504
Training Epoch: 2 [11520/50176]	Loss: 3.7067
Training Epoch: 2 [11776/50176]	Loss: 3.3905
Training Epoch: 2 [12032/50176]	Loss: 3.2962
Training Epoch: 2 [12288/50176]	Loss: 3.1585
Training Epoch: 2 [12544/50176]	Loss: 3.3893
Training Epoch: 2 [12800/50176]	Loss: 3.5117
Training Epoch: 2 [13056/50176]	Loss: 3.4937
Training Epoch: 2 [13312/50176]	Loss: 3.4068
Training Epoch: 2 [13568/50176]	Loss: 3.5033
Training Epoch: 2 [13824/50176]	Loss: 3.3450
Training Epoch: 2 [14080/50176]	Loss: 3.4047
Training Epoch: 2 [14336/50176]	Loss: 3.3064
Training Epoch: 2 [14592/50176]	Loss: 3.4493
Training Epoch: 2 [14848/50176]	Loss: 3.2680
Training Epoch: 2 [15104/50176]	Loss: 3.3069
Training Epoch: 2 [15360/50176]	Loss: 3.2600
Training Epoch: 2 [15616/50176]	Loss: 3.3956
Training Epoch: 2 [15872/50176]	Loss: 3.3607
Training Epoch: 2 [16128/50176]	Loss: 3.3056
Training Epoch: 2 [16384/50176]	Loss: 3.3580
Training Epoch: 2 [16640/50176]	Loss: 3.3329
Training Epoch: 2 [16896/50176]	Loss: 3.3055
Training Epoch: 2 [17152/50176]	Loss: 3.5512
Training Epoch: 2 [17408/50176]	Loss: 3.1960
Training Epoch: 2 [17664/50176]	Loss: 3.3669
Training Epoch: 2 [17920/50176]	Loss: 3.1981
Training Epoch: 2 [18176/50176]	Loss: 3.3666
Training Epoch: 2 [18432/50176]	Loss: 3.4723
Training Epoch: 2 [18688/50176]	Loss: 3.1773
Training Epoch: 2 [18944/50176]	Loss: 3.1843
Training Epoch: 2 [19200/50176]	Loss: 3.1514
Training Epoch: 2 [19456/50176]	Loss: 3.3213
Training Epoch: 2 [19712/50176]	Loss: 3.2398
Training Epoch: 2 [19968/50176]	Loss: 3.3012
Training Epoch: 2 [20224/50176]	Loss: 3.4892
Training Epoch: 2 [20480/50176]	Loss: 3.3401
Training Epoch: 2 [20736/50176]	Loss: 3.2419
Training Epoch: 2 [20992/50176]	Loss: 3.3386
Training Epoch: 2 [21248/50176]	Loss: 3.4580
Training Epoch: 2 [21504/50176]	Loss: 3.1998
Training Epoch: 2 [21760/50176]	Loss: 3.0833
Training Epoch: 2 [22016/50176]	Loss: 3.3734
Training Epoch: 2 [22272/50176]	Loss: 3.3222
Training Epoch: 2 [22528/50176]	Loss: 3.3466
Training Epoch: 2 [22784/50176]	Loss: 3.2826
Training Epoch: 2 [23040/50176]	Loss: 3.1889
Training Epoch: 2 [23296/50176]	Loss: 3.2841
Training Epoch: 2 [23552/50176]	Loss: 3.1499
Training Epoch: 2 [23808/50176]	Loss: 3.2664
Training Epoch: 2 [24064/50176]	Loss: 3.2372
Training Epoch: 2 [24320/50176]	Loss: 3.2874
Training Epoch: 2 [24576/50176]	Loss: 3.1776
Training Epoch: 2 [24832/50176]	Loss: 3.3456
Training Epoch: 2 [25088/50176]	Loss: 3.1678
Training Epoch: 2 [25344/50176]	Loss: 3.3348
Training Epoch: 2 [25600/50176]	Loss: 3.3482
Training Epoch: 2 [25856/50176]	Loss: 3.2355
Training Epoch: 2 [26112/50176]	Loss: 3.3269
Training Epoch: 2 [26368/50176]	Loss: 3.2713
Training Epoch: 2 [26624/50176]	Loss: 3.2458
Training Epoch: 2 [26880/50176]	Loss: 3.1705
Training Epoch: 2 [27136/50176]	Loss: 3.1603
Training Epoch: 2 [27392/50176]	Loss: 3.1564
Training Epoch: 2 [27648/50176]	Loss: 3.3442
Training Epoch: 2 [27904/50176]	Loss: 3.3317
Training Epoch: 2 [28160/50176]	Loss: 2.9849
Training Epoch: 2 [28416/50176]	Loss: 3.2445
Training Epoch: 2 [28672/50176]	Loss: 3.0877
Training Epoch: 2 [28928/50176]	Loss: 3.0694
Training Epoch: 2 [29184/50176]	Loss: 3.2727
Training Epoch: 2 [29440/50176]	Loss: 3.3198
Training Epoch: 2 [29696/50176]	Loss: 3.1941
Training Epoch: 2 [29952/50176]	Loss: 3.1965
Training Epoch: 2 [30208/50176]	Loss: 3.3158
Training Epoch: 2 [30464/50176]	Loss: 3.3481
Training Epoch: 2 [30720/50176]	Loss: 3.0847
Training Epoch: 2 [30976/50176]	Loss: 3.0983
Training Epoch: 2 [31232/50176]	Loss: 3.1418
Training Epoch: 2 [31488/50176]	Loss: 3.1647
Training Epoch: 2 [31744/50176]	Loss: 3.2434
Training Epoch: 2 [32000/50176]	Loss: 3.2402
Training Epoch: 2 [32256/50176]	Loss: 3.0279
Training Epoch: 2 [32512/50176]	Loss: 3.3777
Training Epoch: 2 [32768/50176]	Loss: 3.2346
Training Epoch: 2 [33024/50176]	Loss: 3.1759
Training Epoch: 2 [33280/50176]	Loss: 3.1929
Training Epoch: 2 [33536/50176]	Loss: 3.0456
Training Epoch: 2 [33792/50176]	Loss: 3.1314
Training Epoch: 2 [34048/50176]	Loss: 3.1468
Training Epoch: 2 [34304/50176]	Loss: 3.1107
Training Epoch: 2 [34560/50176]	Loss: 3.2659
Training Epoch: 2 [34816/50176]	Loss: 3.2132
Training Epoch: 2 [35072/50176]	Loss: 3.1981
Training Epoch: 2 [35328/50176]	Loss: 3.0402
Training Epoch: 2 [35584/50176]	Loss: 2.9584
Training Epoch: 2 [35840/50176]	Loss: 3.3671
Training Epoch: 2 [36096/50176]	Loss: 3.0988
Training Epoch: 2 [36352/50176]	Loss: 3.0060
Training Epoch: 2 [36608/50176]	Loss: 3.0973
Training Epoch: 2 [36864/50176]	Loss: 3.1986
Training Epoch: 2 [37120/50176]	Loss: 3.1329
Training Epoch: 2 [37376/50176]	Loss: 3.1733
Training Epoch: 2 [37632/50176]	Loss: 3.3327
Training Epoch: 2 [37888/50176]	Loss: 3.0296
Training Epoch: 2 [38144/50176]	Loss: 3.2118
Training Epoch: 2 [38400/50176]	Loss: 3.0457
Training Epoch: 2 [38656/50176]	Loss: 3.1621
Training Epoch: 2 [38912/50176]	Loss: 3.2038
Training Epoch: 2 [39168/50176]	Loss: 3.2804
Training Epoch: 2 [39424/50176]	Loss: 3.3026
Training Epoch: 2 [39680/50176]	Loss: 3.0817
Training Epoch: 2 [39936/50176]	Loss: 3.4161
Training Epoch: 2 [40192/50176]	Loss: 3.1403
Training Epoch: 2 [40448/50176]	Loss: 3.2141
Training Epoch: 2 [40704/50176]	Loss: 3.0591
Training Epoch: 2 [40960/50176]	Loss: 3.2368
Training Epoch: 2 [41216/50176]	Loss: 3.1664
Training Epoch: 2 [41472/50176]	Loss: 3.1992
Training Epoch: 2 [41728/50176]	Loss: 3.1349
Training Epoch: 2 [41984/50176]	Loss: 3.2569
Training Epoch: 2 [42240/50176]	Loss: 3.1279
Training Epoch: 2 [42496/50176]	Loss: 3.0315
Training Epoch: 2 [42752/50176]	Loss: 3.1341
Training Epoch: 2 [43008/50176]	Loss: 3.0484
Training Epoch: 2 [43264/50176]	Loss: 2.9932
Training Epoch: 2 [43520/50176]	Loss: 3.1106
Training Epoch: 2 [43776/50176]	Loss: 3.1304
Training Epoch: 2 [44032/50176]	Loss: 3.3284
Training Epoch: 2 [44288/50176]	Loss: 3.1802
Training Epoch: 2 [44544/50176]	Loss: 3.1287
Training Epoch: 2 [44800/50176]	Loss: 3.1058
Training Epoch: 2 [45056/50176]	Loss: 3.1645
Training Epoch: 2 [45312/50176]	Loss: 3.0366
Training Epoch: 2 [45568/50176]	Loss: 3.1536
Training Epoch: 2 [45824/50176]	Loss: 3.1661
Training Epoch: 2 [46080/50176]	Loss: 3.1173
Training Epoch: 2 [46336/50176]	Loss: 3.1004
Training Epoch: 2 [46592/50176]	Loss: 3.1767
Training Epoch: 2 [46848/50176]	Loss: 3.1290
Training Epoch: 2 [47104/50176]	Loss: 3.0152
Training Epoch: 2 [47360/50176]	Loss: 2.9334
Training Epoch: 2 [47616/50176]	Loss: 3.0900
Training Epoch: 2 [47872/50176]	Loss: 3.1010
Training Epoch: 2 [48128/50176]	Loss: 2.9777
Training Epoch: 2 [48384/50176]	Loss: 3.1432
Training Epoch: 2 [48640/50176]	Loss: 3.1250
Training Epoch: 2 [48896/50176]	Loss: 3.2177
Training Epoch: 2 [49152/50176]	Loss: 3.0560
Training Epoch: 2 [49408/50176]	Loss: 3.2137
Training Epoch: 2 [49664/50176]	Loss: 3.1945
Training Epoch: 2 [49920/50176]	Loss: 3.3657
Training Epoch: 2 [50176/50176]	Loss: 3.1577
Validation Epoch: 2, Average loss: 0.0122, Accuracy: 0.2195
Training Epoch: 3 [256/50176]	Loss: 2.8622
Training Epoch: 3 [512/50176]	Loss: 2.9094
Training Epoch: 3 [768/50176]	Loss: 3.1000
Training Epoch: 3 [1024/50176]	Loss: 3.0201
Training Epoch: 3 [1280/50176]	Loss: 2.9378
Training Epoch: 3 [1536/50176]	Loss: 3.1291
Training Epoch: 3 [1792/50176]	Loss: 3.0255
Training Epoch: 3 [2048/50176]	Loss: 3.2425
Training Epoch: 3 [2304/50176]	Loss: 3.0581
Training Epoch: 3 [2560/50176]	Loss: 3.1966
Training Epoch: 3 [2816/50176]	Loss: 2.9914
Training Epoch: 3 [3072/50176]	Loss: 3.1326
Training Epoch: 3 [3328/50176]	Loss: 2.9243
Training Epoch: 3 [3584/50176]	Loss: 3.0086
Training Epoch: 3 [3840/50176]	Loss: 3.0492
Training Epoch: 3 [4096/50176]	Loss: 2.9704
Training Epoch: 3 [4352/50176]	Loss: 3.1692
Training Epoch: 3 [4608/50176]	Loss: 3.0655
Training Epoch: 3 [4864/50176]	Loss: 3.1036
Training Epoch: 3 [5120/50176]	Loss: 2.9093
Training Epoch: 3 [5376/50176]	Loss: 3.0505
Training Epoch: 3 [5632/50176]	Loss: 3.0170
Training Epoch: 3 [5888/50176]	Loss: 3.1011
Training Epoch: 3 [6144/50176]	Loss: 3.2350
Training Epoch: 3 [6400/50176]	Loss: 3.0414
Training Epoch: 3 [6656/50176]	Loss: 2.9740
Training Epoch: 3 [6912/50176]	Loss: 3.2088
Training Epoch: 3 [7168/50176]	Loss: 2.9942
Training Epoch: 3 [7424/50176]	Loss: 3.0945
Training Epoch: 3 [7680/50176]	Loss: 2.9409
Training Epoch: 3 [7936/50176]	Loss: 3.0013
Training Epoch: 3 [8192/50176]	Loss: 2.9827
Training Epoch: 3 [8448/50176]	Loss: 2.8962
Training Epoch: 3 [8704/50176]	Loss: 2.9724
Training Epoch: 3 [8960/50176]	Loss: 2.9490
Training Epoch: 3 [9216/50176]	Loss: 2.9268
Training Epoch: 3 [9472/50176]	Loss: 2.8307
Training Epoch: 3 [9728/50176]	Loss: 3.0334
Training Epoch: 3 [9984/50176]	Loss: 2.8882
Training Epoch: 3 [10240/50176]	Loss: 2.8214
Training Epoch: 3 [10496/50176]	Loss: 3.1358
Training Epoch: 3 [10752/50176]	Loss: 2.9500
Training Epoch: 3 [11008/50176]	Loss: 2.8478
Training Epoch: 3 [11264/50176]	Loss: 2.9533
Training Epoch: 3 [11520/50176]	Loss: 2.9931
Training Epoch: 3 [11776/50176]	Loss: 3.0323
Training Epoch: 3 [12032/50176]	Loss: 2.9266
Training Epoch: 3 [12288/50176]	Loss: 3.0713
Training Epoch: 3 [12544/50176]	Loss: 2.8820
Training Epoch: 3 [12800/50176]	Loss: 2.8013
Training Epoch: 3 [13056/50176]	Loss: 2.9223
Training Epoch: 3 [13312/50176]	Loss: 2.8110
Training Epoch: 3 [13568/50176]	Loss: 2.8236
Training Epoch: 3 [13824/50176]	Loss: 2.8716
Training Epoch: 3 [14080/50176]	Loss: 2.9951
Training Epoch: 3 [14336/50176]	Loss: 3.0951
Training Epoch: 3 [14592/50176]	Loss: 3.1736
Training Epoch: 3 [14848/50176]	Loss: 2.9156
Training Epoch: 3 [15104/50176]	Loss: 2.9040
Training Epoch: 3 [15360/50176]	Loss: 2.8429
Training Epoch: 3 [15616/50176]	Loss: 3.1108
Training Epoch: 3 [15872/50176]	Loss: 2.9675
Training Epoch: 3 [16128/50176]	Loss: 2.8959
Training Epoch: 3 [16384/50176]	Loss: 2.8162
Training Epoch: 3 [16640/50176]	Loss: 2.7449
Training Epoch: 3 [16896/50176]	Loss: 2.6961
Training Epoch: 3 [17152/50176]	Loss: 3.0713
Training Epoch: 3 [17408/50176]	Loss: 2.8994
Training Epoch: 3 [17664/50176]	Loss: 2.9943
Training Epoch: 3 [17920/50176]	Loss: 2.7781
Training Epoch: 3 [18176/50176]	Loss: 2.9482
Training Epoch: 3 [18432/50176]	Loss: 3.1959
Training Epoch: 3 [18688/50176]	Loss: 2.9055
Training Epoch: 3 [18944/50176]	Loss: 3.0068
Training Epoch: 3 [19200/50176]	Loss: 2.9851
Training Epoch: 3 [19456/50176]	Loss: 2.6497
Training Epoch: 3 [19712/50176]	Loss: 2.8740
Training Epoch: 3 [19968/50176]	Loss: 2.8848
Training Epoch: 3 [20224/50176]	Loss: 2.9337
Training Epoch: 3 [20480/50176]	Loss: 2.9785
Training Epoch: 3 [20736/50176]	Loss: 2.7735
Training Epoch: 3 [20992/50176]	Loss: 2.8249
Training Epoch: 3 [21248/50176]	Loss: 3.0850
Training Epoch: 3 [21504/50176]	Loss: 2.9312
Training Epoch: 3 [21760/50176]	Loss: 2.9740
Training Epoch: 3 [22016/50176]	Loss: 2.6527
Training Epoch: 3 [22272/50176]	Loss: 2.8983
Training Epoch: 3 [22528/50176]	Loss: 2.8129
Training Epoch: 3 [22784/50176]	Loss: 2.8873
Training Epoch: 3 [23040/50176]	Loss: 2.9910
Training Epoch: 3 [23296/50176]	Loss: 2.8848
Training Epoch: 3 [23552/50176]	Loss: 2.9143
Training Epoch: 3 [23808/50176]	Loss: 2.8305
Training Epoch: 3 [24064/50176]	Loss: 2.8028
Training Epoch: 3 [24320/50176]	Loss: 2.8863
Training Epoch: 3 [24576/50176]	Loss: 3.0294
Training Epoch: 3 [24832/50176]	Loss: 3.1151
Training Epoch: 3 [25088/50176]	Loss: 2.8313
Training Epoch: 3 [25344/50176]	Loss: 2.8239
Training Epoch: 3 [25600/50176]	Loss: 2.7839
Training Epoch: 3 [25856/50176]	Loss: 3.0847
Training Epoch: 3 [26112/50176]	Loss: 2.9289
Training Epoch: 3 [26368/50176]	Loss: 2.8769
Training Epoch: 3 [26624/50176]	Loss: 3.0288
Training Epoch: 3 [26880/50176]	Loss: 2.7560
Training Epoch: 3 [27136/50176]	Loss: 2.8093
Training Epoch: 3 [27392/50176]	Loss: 3.0954
Training Epoch: 3 [27648/50176]	Loss: 2.9134
Training Epoch: 3 [27904/50176]	Loss: 2.7765
Training Epoch: 3 [28160/50176]	Loss: 2.8319
Training Epoch: 3 [28416/50176]	Loss: 2.6757
Training Epoch: 3 [28672/50176]	Loss: 2.8694
Training Epoch: 3 [28928/50176]	Loss: 2.8925
Training Epoch: 3 [29184/50176]	Loss: 2.7277
Training Epoch: 3 [29440/50176]	Loss: 2.9033
Training Epoch: 3 [29696/50176]	Loss: 2.8050
Training Epoch: 3 [29952/50176]	Loss: 2.8281
Training Epoch: 3 [30208/50176]	Loss: 2.7887
Training Epoch: 3 [30464/50176]	Loss: 2.8511
Training Epoch: 3 [30720/50176]	Loss: 2.8520
Training Epoch: 3 [30976/50176]	Loss: 2.7934
Training Epoch: 3 [31232/50176]	Loss: 2.8589
Training Epoch: 3 [31488/50176]	Loss: 3.0032
Training Epoch: 3 [31744/50176]	Loss: 2.7363
Training Epoch: 3 [32000/50176]	Loss: 2.7160
Training Epoch: 3 [32256/50176]	Loss: 3.0277
Training Epoch: 3 [32512/50176]	Loss: 2.6892
Training Epoch: 3 [32768/50176]	Loss: 3.0565
Training Epoch: 3 [33024/50176]	Loss: 2.8053
Training Epoch: 3 [33280/50176]	Loss: 2.7105
Training Epoch: 3 [33536/50176]	Loss: 2.7422
Training Epoch: 3 [33792/50176]	Loss: 2.9595
Training Epoch: 3 [34048/50176]	Loss: 2.6784
Training Epoch: 3 [34304/50176]	Loss: 2.9348
Training Epoch: 3 [34560/50176]	Loss: 2.8331
Training Epoch: 3 [34816/50176]	Loss: 2.9767
Training Epoch: 3 [35072/50176]	Loss: 2.9015
Training Epoch: 3 [35328/50176]	Loss: 2.8413
Training Epoch: 3 [35584/50176]	Loss: 2.7452
Training Epoch: 3 [35840/50176]	Loss: 2.9183
Training Epoch: 3 [36096/50176]	Loss: 2.6467
Training Epoch: 3 [36352/50176]	Loss: 2.7916
Training Epoch: 3 [36608/50176]	Loss: 2.6758
Training Epoch: 3 [36864/50176]	Loss: 2.6871
Training Epoch: 3 [37120/50176]	Loss: 2.9820
Training Epoch: 3 [37376/50176]	Loss: 2.8509
Training Epoch: 3 [37632/50176]	Loss: 2.7319
Training Epoch: 3 [37888/50176]	Loss: 2.8824
Training Epoch: 3 [38144/50176]	Loss: 2.8817
Training Epoch: 3 [38400/50176]	Loss: 2.7042
Training Epoch: 3 [38656/50176]	Loss: 2.8236
Training Epoch: 3 [38912/50176]	Loss: 2.9430
Training Epoch: 3 [39168/50176]	Loss: 2.9319
Training Epoch: 3 [39424/50176]	Loss: 2.7730
Training Epoch: 3 [39680/50176]	Loss: 2.8304
Training Epoch: 3 [39936/50176]	Loss: 2.8707
Training Epoch: 3 [40192/50176]	Loss: 2.8990
Training Epoch: 3 [40448/50176]	Loss: 2.8033
Training Epoch: 3 [40704/50176]	Loss: 2.7649
Training Epoch: 3 [40960/50176]	Loss: 2.8915
Training Epoch: 3 [41216/50176]	Loss: 2.6428
Training Epoch: 3 [41472/50176]	Loss: 2.6868
Training Epoch: 3 [41728/50176]	Loss: 2.7010
Training Epoch: 3 [41984/50176]	Loss: 2.8469
Training Epoch: 3 [42240/50176]	Loss: 2.8055
Training Epoch: 3 [42496/50176]	Loss: 2.8452
Training Epoch: 3 [42752/50176]	Loss: 2.7459
Training Epoch: 3 [43008/50176]	Loss: 2.7233
Training Epoch: 3 [43264/50176]	Loss: 2.7970
Training Epoch: 3 [43520/50176]	Loss: 2.7623
Training Epoch: 3 [43776/50176]	Loss: 2.6355
Training Epoch: 3 [44032/50176]	Loss: 2.8581
Training Epoch: 3 [44288/50176]	Loss: 2.6362
Training Epoch: 3 [44544/50176]	Loss: 2.5136
Training Epoch: 3 [44800/50176]	Loss: 2.7959
Training Epoch: 3 [45056/50176]	Loss: 2.7941
Training Epoch: 3 [45312/50176]	Loss: 2.7537
Training Epoch: 3 [45568/50176]	Loss: 2.6726
Training Epoch: 3 [45824/50176]	Loss: 2.9089
Training Epoch: 3 [46080/50176]	Loss: 2.7901
Training Epoch: 3 [46336/50176]	Loss: 2.6879
Training Epoch: 3 [46592/50176]	Loss: 2.8235
Training Epoch: 3 [46848/50176]	Loss: 2.8179
Training Epoch: 3 [47104/50176]	Loss: 2.8621
Training Epoch: 3 [47360/50176]	Loss: 2.7086
Training Epoch: 3 [47616/50176]	Loss: 2.8437
Training Epoch: 3 [47872/50176]	Loss: 2.9763
Training Epoch: 3 [48128/50176]	Loss: 2.8510
Training Epoch: 3 [48384/50176]	Loss: 2.8767
Training Epoch: 3 [48640/50176]	Loss: 2.8348
Training Epoch: 3 [48896/50176]	Loss: 2.7170
Training Epoch: 3 [49152/50176]	Loss: 2.8049
Training Epoch: 3 [49408/50176]	Loss: 2.5140
Training Epoch: 3 [49664/50176]	Loss: 2.7900
Training Epoch: 3 [49920/50176]	Loss: 2.7104
Training Epoch: 3 [50176/50176]	Loss: 3.2225
Validation Epoch: 3, Average loss: 0.0113, Accuracy: 0.2706
Training Epoch: 4 [256/50176]	Loss: 2.6656
Training Epoch: 4 [512/50176]	Loss: 2.7887
Training Epoch: 4 [768/50176]	Loss: 2.7782
Training Epoch: 4 [1024/50176]	Loss: 2.7824
Training Epoch: 4 [1280/50176]	Loss: 2.8783
Training Epoch: 4 [1536/50176]	Loss: 2.7337
Training Epoch: 4 [1792/50176]	Loss: 2.5896
Training Epoch: 4 [2048/50176]	Loss: 2.5389
Training Epoch: 4 [2304/50176]	Loss: 2.6281
Training Epoch: 4 [2560/50176]	Loss: 2.8146
Training Epoch: 4 [2816/50176]	Loss: 2.7518
Training Epoch: 4 [3072/50176]	Loss: 2.7347
Training Epoch: 4 [3328/50176]	Loss: 2.7491
Training Epoch: 4 [3584/50176]	Loss: 2.6761
Training Epoch: 4 [3840/50176]	Loss: 2.7073
Training Epoch: 4 [4096/50176]	Loss: 2.6914
Training Epoch: 4 [4352/50176]	Loss: 2.7344
Training Epoch: 4 [4608/50176]	Loss: 2.6910
Training Epoch: 4 [4864/50176]	Loss: 2.8278
Training Epoch: 4 [5120/50176]	Loss: 2.6730
Training Epoch: 4 [5376/50176]	Loss: 2.8183
Training Epoch: 4 [5632/50176]	Loss: 2.9032
Training Epoch: 4 [5888/50176]	Loss: 2.6407
Training Epoch: 4 [6144/50176]	Loss: 2.5974
Training Epoch: 4 [6400/50176]	Loss: 2.7387
Training Epoch: 4 [6656/50176]	Loss: 2.6104
Training Epoch: 4 [6912/50176]	Loss: 2.5113
Training Epoch: 4 [7168/50176]	Loss: 2.8693
Training Epoch: 4 [7424/50176]	Loss: 2.4116
Training Epoch: 4 [7680/50176]	Loss: 2.6832
Training Epoch: 4 [7936/50176]	Loss: 2.4233
Training Epoch: 4 [8192/50176]	Loss: 2.7827
Training Epoch: 4 [8448/50176]	Loss: 2.7301
Training Epoch: 4 [8704/50176]	Loss: 2.6923
Training Epoch: 4 [8960/50176]	Loss: 2.5822
Training Epoch: 4 [9216/50176]	Loss: 2.6845
Training Epoch: 4 [9472/50176]	Loss: 2.5164
Training Epoch: 4 [9728/50176]	Loss: 2.8106
Training Epoch: 4 [9984/50176]	Loss: 2.6392
Training Epoch: 4 [10240/50176]	Loss: 2.6749
Training Epoch: 4 [10496/50176]	Loss: 2.5836
Training Epoch: 4 [10752/50176]	Loss: 2.4853
Training Epoch: 4 [11008/50176]	Loss: 2.7139
Training Epoch: 4 [11264/50176]	Loss: 2.7210
Training Epoch: 4 [11520/50176]	Loss: 2.5816
Training Epoch: 4 [11776/50176]	Loss: 2.8920
Training Epoch: 4 [12032/50176]	Loss: 2.6948
Training Epoch: 4 [12288/50176]	Loss: 2.7484
Training Epoch: 4 [12544/50176]	Loss: 2.4655
Training Epoch: 4 [12800/50176]	Loss: 2.5312
Training Epoch: 4 [13056/50176]	Loss: 2.6162
Training Epoch: 4 [13312/50176]	Loss: 2.5955
Training Epoch: 4 [13568/50176]	Loss: 2.8853
Training Epoch: 4 [13824/50176]	Loss: 2.6532
Training Epoch: 4 [14080/50176]	Loss: 2.6990
Training Epoch: 4 [14336/50176]	Loss: 2.6574
Training Epoch: 4 [14592/50176]	Loss: 2.8329
Training Epoch: 4 [14848/50176]	Loss: 2.5592
Training Epoch: 4 [15104/50176]	Loss: 2.7059
Training Epoch: 4 [15360/50176]	Loss: 2.7278
Training Epoch: 4 [15616/50176]	Loss: 2.5614
Training Epoch: 4 [15872/50176]	Loss: 2.6125
Training Epoch: 4 [16128/50176]	Loss: 2.6759
Training Epoch: 4 [16384/50176]	Loss: 2.6677
Training Epoch: 4 [16640/50176]	Loss: 2.5189
Training Epoch: 4 [16896/50176]	Loss: 2.5755
Training Epoch: 4 [17152/50176]	Loss: 2.7260
Training Epoch: 4 [17408/50176]	Loss: 2.5300
Training Epoch: 4 [17664/50176]	Loss: 2.7110
Training Epoch: 4 [17920/50176]	Loss: 2.5407
Training Epoch: 4 [18176/50176]	Loss: 2.5436
Training Epoch: 4 [18432/50176]	Loss: 2.6340
Training Epoch: 4 [18688/50176]	Loss: 2.3627
Training Epoch: 4 [18944/50176]	Loss: 2.6152
Training Epoch: 4 [19200/50176]	Loss: 2.7828
Training Epoch: 4 [19456/50176]	Loss: 2.6796
Training Epoch: 4 [19712/50176]	Loss: 2.7822
Training Epoch: 4 [19968/50176]	Loss: 2.7760
Training Epoch: 4 [20224/50176]	Loss: 2.6097
Training Epoch: 4 [20480/50176]	Loss: 2.5584
Training Epoch: 4 [20736/50176]	Loss: 2.5326
Training Epoch: 4 [20992/50176]	Loss: 2.5977
Training Epoch: 4 [21248/50176]	Loss: 2.4932
Training Epoch: 4 [21504/50176]	Loss: 2.5386
Training Epoch: 4 [21760/50176]	Loss: 2.6241
Training Epoch: 4 [22016/50176]	Loss: 2.5518
Training Epoch: 4 [22272/50176]	Loss: 2.5642
Training Epoch: 4 [22528/50176]	Loss: 2.5857
Training Epoch: 4 [22784/50176]	Loss: 2.4335
Training Epoch: 4 [23040/50176]	Loss: 2.5816
Training Epoch: 4 [23296/50176]	Loss: 2.5842
Training Epoch: 4 [23552/50176]	Loss: 2.4804
Training Epoch: 4 [23808/50176]	Loss: 2.6166
Training Epoch: 4 [24064/50176]	Loss: 2.6618
Training Epoch: 4 [24320/50176]	Loss: 2.4381
Training Epoch: 4 [24576/50176]	Loss: 2.6052
Training Epoch: 4 [24832/50176]	Loss: 2.4613
Training Epoch: 4 [25088/50176]	Loss: 2.4473
Training Epoch: 4 [25344/50176]	Loss: 2.6658
Training Epoch: 4 [25600/50176]	Loss: 2.6315
Training Epoch: 4 [25856/50176]	Loss: 2.7397
Training Epoch: 4 [26112/50176]	Loss: 2.5193
Training Epoch: 4 [26368/50176]	Loss: 2.3630
Training Epoch: 4 [26624/50176]	Loss: 2.9175
Training Epoch: 4 [26880/50176]	Loss: 2.4953
Training Epoch: 4 [27136/50176]	Loss: 2.6896
Training Epoch: 4 [27392/50176]	Loss: 2.5602
Training Epoch: 4 [27648/50176]	Loss: 2.6700
Training Epoch: 4 [27904/50176]	Loss: 2.6968
Training Epoch: 4 [28160/50176]	Loss: 2.5379
Training Epoch: 4 [28416/50176]	Loss: 2.5999
Training Epoch: 4 [28672/50176]	Loss: 2.7606
Training Epoch: 4 [28928/50176]	Loss: 2.5788
Training Epoch: 4 [29184/50176]	Loss: 2.5626
Training Epoch: 4 [29440/50176]	Loss: 2.5615
Training Epoch: 4 [29696/50176]	Loss: 2.7503
Training Epoch: 4 [29952/50176]	Loss: 2.5731
Training Epoch: 4 [30208/50176]	Loss: 2.4263
Training Epoch: 4 [30464/50176]	Loss: 2.5480
Training Epoch: 4 [30720/50176]	Loss: 2.5016
Training Epoch: 4 [30976/50176]	Loss: 2.7205
Training Epoch: 4 [31232/50176]	Loss: 2.4937
Training Epoch: 4 [31488/50176]	Loss: 2.5913
Training Epoch: 4 [31744/50176]	Loss: 2.6741
Training Epoch: 4 [32000/50176]	Loss: 2.7305
Training Epoch: 4 [32256/50176]	Loss: 2.6321
Training Epoch: 4 [32512/50176]	Loss: 2.5522
Training Epoch: 4 [32768/50176]	Loss: 2.5337
Training Epoch: 4 [33024/50176]	Loss: 2.4210
Training Epoch: 4 [33280/50176]	Loss: 2.6563
Training Epoch: 4 [33536/50176]	Loss: 2.7220
Training Epoch: 4 [33792/50176]	Loss: 2.3946
Training Epoch: 4 [34048/50176]	Loss: 2.4628
Training Epoch: 4 [34304/50176]	Loss: 2.5262
Training Epoch: 4 [34560/50176]	Loss: 2.6115
Training Epoch: 4 [34816/50176]	Loss: 2.5112
Training Epoch: 4 [35072/50176]	Loss: 2.4851
Training Epoch: 4 [35328/50176]	Loss: 2.4728
Training Epoch: 4 [35584/50176]	Loss: 2.5910
Training Epoch: 4 [35840/50176]	Loss: 2.5570
Training Epoch: 4 [36096/50176]	Loss: 2.5499
Training Epoch: 4 [36352/50176]	Loss: 2.4766
Training Epoch: 4 [36608/50176]	Loss: 2.5356
Training Epoch: 4 [36864/50176]	Loss: 2.5831
Training Epoch: 4 [37120/50176]	Loss: 2.4915
Training Epoch: 4 [37376/50176]	Loss: 2.5922
Training Epoch: 4 [37632/50176]	Loss: 2.5411
Training Epoch: 4 [37888/50176]	Loss: 2.5128
Training Epoch: 4 [38144/50176]	Loss: 2.7132
Training Epoch: 4 [38400/50176]	Loss: 2.6006
Training Epoch: 4 [38656/50176]	Loss: 2.4449
Training Epoch: 4 [38912/50176]	Loss: 2.6875
Training Epoch: 4 [39168/50176]	Loss: 2.5702
Training Epoch: 4 [39424/50176]	Loss: 2.2987
Training Epoch: 4 [39680/50176]	Loss: 2.8723
Training Epoch: 4 [39936/50176]	Loss: 2.3272
Training Epoch: 4 [40192/50176]	Loss: 2.4468
Training Epoch: 4 [40448/50176]	Loss: 2.6149
Training Epoch: 4 [40704/50176]	Loss: 2.6202
Training Epoch: 4 [40960/50176]	Loss: 2.4441
Training Epoch: 4 [41216/50176]	Loss: 2.4659
Training Epoch: 4 [41472/50176]	Loss: 2.4544
Training Epoch: 4 [41728/50176]	Loss: 2.4576
Training Epoch: 4 [41984/50176]	Loss: 2.2982
Training Epoch: 4 [42240/50176]	Loss: 2.7155
Training Epoch: 4 [42496/50176]	Loss: 2.4397
Training Epoch: 4 [42752/50176]	Loss: 2.4701
Training Epoch: 4 [43008/50176]	Loss: 2.5322
Training Epoch: 4 [43264/50176]	Loss: 2.6153
Training Epoch: 4 [43520/50176]	Loss: 2.6760
Training Epoch: 4 [43776/50176]	Loss: 2.5625
Training Epoch: 4 [44032/50176]	Loss: 2.7166
Training Epoch: 4 [44288/50176]	Loss: 2.5661
Training Epoch: 4 [44544/50176]	Loss: 2.4061
Training Epoch: 4 [44800/50176]	Loss: 2.4556
Training Epoch: 4 [45056/50176]	Loss: 2.5835
Training Epoch: 4 [45312/50176]	Loss: 2.4095
Training Epoch: 4 [45568/50176]	Loss: 2.3657
Training Epoch: 4 [45824/50176]	Loss: 2.6304
Training Epoch: 4 [46080/50176]	Loss: 2.3978
Training Epoch: 4 [46336/50176]	Loss: 2.4554
Training Epoch: 4 [46592/50176]	Loss: 2.5074
Training Epoch: 4 [46848/50176]	Loss: 2.4367
Training Epoch: 4 [47104/50176]	Loss: 2.4094
Training Epoch: 4 [47360/50176]	Loss: 2.5617
Training Epoch: 4 [47616/50176]	Loss: 2.5763
Training Epoch: 4 [47872/50176]	Loss: 2.8059
Training Epoch: 4 [48128/50176]	Loss: 2.6682
Training Epoch: 4 [48384/50176]	Loss: 2.3626
Training Epoch: 4 [48640/50176]	Loss: 2.4217
Training Epoch: 4 [48896/50176]	Loss: 2.5211
Training Epoch: 4 [49152/50176]	Loss: 2.6041
Training Epoch: 4 [49408/50176]	Loss: 2.4981
Training Epoch: 4 [49664/50176]	Loss: 2.5407
Training Epoch: 4 [49920/50176]	Loss: 2.3443
Training Epoch: 4 [50176/50176]	Loss: 2.5043
Validation Epoch: 4, Average loss: 0.0102, Accuracy: 0.3253
[Training Loop] Model's accuracy 0.32529296875 surpasses threshold 0.3! Reprofiling...
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.5430
Profiling... [256/50048]	Loss: 2.5719
Profiling... [384/50048]	Loss: 2.4187
Profiling... [512/50048]	Loss: 2.3428
Profiling... [640/50048]	Loss: 2.4549
Profiling... [768/50048]	Loss: 2.1800
Profiling... [896/50048]	Loss: 2.3505
Profiling... [1024/50048]	Loss: 2.4919
Profiling... [1152/50048]	Loss: 2.4029
Profiling... [1280/50048]	Loss: 2.4196
Profiling... [1408/50048]	Loss: 2.2827
Profiling... [1536/50048]	Loss: 2.2695
Profiling... [1664/50048]	Loss: 2.2490
Profile done
epoch 1 train time consumed: 3.35s
Validation Epoch: 5, Average loss: 0.0180, Accuracy: 0.3899
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 120.40680758536836,
                        "time": 2.1956106259999615,
                        "accuracy": 0.38993275316455694,
                        "total_cost": 493028.89138669986
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.4997
Profiling... [256/50048]	Loss: 2.5213
Profiling... [384/50048]	Loss: 2.4634
Profiling... [512/50048]	Loss: 2.4741
Profiling... [640/50048]	Loss: 2.4929
Profiling... [768/50048]	Loss: 2.2454
Profiling... [896/50048]	Loss: 2.2544
Profiling... [1024/50048]	Loss: 2.5764
Profiling... [1152/50048]	Loss: 2.2737
Profiling... [1280/50048]	Loss: 2.1672
Profiling... [1408/50048]	Loss: 2.5386
Profiling... [1536/50048]	Loss: 2.2289
Profiling... [1664/50048]	Loss: 2.5823
Profile done
epoch 1 train time consumed: 3.32s
Validation Epoch: 5, Average loss: 0.0180, Accuracy: 0.3853
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 120.4478173032923,
                        "time": 2.189342860999659,
                        "accuracy": 0.3852848101265823,
                        "total_cost": 497552.31995509204
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.6159
Profiling... [256/50048]	Loss: 2.1879
Profiling... [384/50048]	Loss: 2.3396
Profiling... [512/50048]	Loss: 2.5305
Profiling... [640/50048]	Loss: 2.3265
Profiling... [768/50048]	Loss: 2.3771
Profiling... [896/50048]	Loss: 2.4281
Profiling... [1024/50048]	Loss: 2.3124
Profiling... [1152/50048]	Loss: 2.5350
Profiling... [1280/50048]	Loss: 2.3209
Profiling... [1408/50048]	Loss: 2.3777
Profiling... [1536/50048]	Loss: 2.0157
Profiling... [1664/50048]	Loss: 2.2743
Profile done
epoch 1 train time consumed: 3.50s
Validation Epoch: 5, Average loss: 0.0181, Accuracy: 0.3880
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 120.46811976164786,
                        "time": 2.3407727089997934,
                        "accuracy": 0.38795490506329117,
                        "total_cost": 528305.2324021169
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.4774
Profiling... [256/50048]	Loss: 2.2152
Profiling... [384/50048]	Loss: 2.5000
Profiling... [512/50048]	Loss: 2.3944
Profiling... [640/50048]	Loss: 2.4552
Profiling... [768/50048]	Loss: 2.2809
Profiling... [896/50048]	Loss: 2.3786
Profiling... [1024/50048]	Loss: 2.0557
Profiling... [1152/50048]	Loss: 2.6571
Profiling... [1280/50048]	Loss: 2.3849
Profiling... [1408/50048]	Loss: 2.5375
Profiling... [1536/50048]	Loss: 2.4373
Profiling... [1664/50048]	Loss: 2.4756
Profile done
epoch 1 train time consumed: 3.88s
Validation Epoch: 5, Average loss: 0.0180, Accuracy: 0.3915
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 120.41706571505716,
                        "time": 2.784930822000206,
                        "accuracy": 0.3915150316455696,
                        "total_cost": 622834.6393215163
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.2356
Profiling... [256/50048]	Loss: 2.5338
Profiling... [384/50048]	Loss: 2.2200
Profiling... [512/50048]	Loss: 2.4718
Profiling... [640/50048]	Loss: 2.4083
Profiling... [768/50048]	Loss: 2.2321
Profiling... [896/50048]	Loss: 2.5686
Profiling... [1024/50048]	Loss: 2.6506
Profiling... [1152/50048]	Loss: 2.4366
Profiling... [1280/50048]	Loss: 2.4526
Profiling... [1408/50048]	Loss: 2.0837
Profiling... [1536/50048]	Loss: 2.4218
Profiling... [1664/50048]	Loss: 2.2163
Profile done
epoch 1 train time consumed: 3.33s
Validation Epoch: 5, Average loss: 0.0180, Accuracy: 0.3901
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 120.3598707769238,
                        "time": 2.1779006610004217,
                        "accuracy": 0.39013053797468356,
                        "total_cost": 488804.01608287625
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.2950
Profiling... [256/50048]	Loss: 2.4384
Profiling... [384/50048]	Loss: 2.3469
Profiling... [512/50048]	Loss: 2.3888
Profiling... [640/50048]	Loss: 2.4470
Profiling... [768/50048]	Loss: 2.4730
Profiling... [896/50048]	Loss: 2.3179
Profiling... [1024/50048]	Loss: 2.4619
Profiling... [1152/50048]	Loss: 2.4711
Profiling... [1280/50048]	Loss: 2.9298
Profiling... [1408/50048]	Loss: 2.2392
Profiling... [1536/50048]	Loss: 2.5500
Profiling... [1664/50048]	Loss: 2.3240
Profile done
epoch 1 train time consumed: 3.27s
Validation Epoch: 5, Average loss: 0.0181, Accuracy: 0.3909
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 120.3946266572596,
                        "time": 2.1819207950002237,
                        "accuracy": 0.3909216772151899,
                        "total_cost": 488715.3270528609
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.4358
Profiling... [256/50048]	Loss: 2.2338
Profiling... [384/50048]	Loss: 2.3694
Profiling... [512/50048]	Loss: 2.7517
Profiling... [640/50048]	Loss: 2.4744
Profiling... [768/50048]	Loss: 2.5307
Profiling... [896/50048]	Loss: 2.5635
Profiling... [1024/50048]	Loss: 2.4602
Profiling... [1152/50048]	Loss: 2.5781
Profiling... [1280/50048]	Loss: 2.3256
Profiling... [1408/50048]	Loss: 2.3759
Profiling... [1536/50048]	Loss: 2.5058
Profiling... [1664/50048]	Loss: 2.3923
Profile done
epoch 1 train time consumed: 3.50s
Validation Epoch: 5, Average loss: 0.0180, Accuracy: 0.3918
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 120.4194653971252,
                        "time": 2.3345098369995867,
                        "accuracy": 0.3918117088607595,
                        "total_cost": 521705.1107152447
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.3545
Profiling... [256/50048]	Loss: 2.6751
Profiling... [384/50048]	Loss: 2.3672
Profiling... [512/50048]	Loss: 2.3661
Profiling... [640/50048]	Loss: 2.3080
Profiling... [768/50048]	Loss: 2.4915
Profiling... [896/50048]	Loss: 2.2168
Profiling... [1024/50048]	Loss: 2.6782
Profiling... [1152/50048]	Loss: 2.4264
Profiling... [1280/50048]	Loss: 2.6175
Profiling... [1408/50048]	Loss: 2.2292
Profiling... [1536/50048]	Loss: 2.3446
Profiling... [1664/50048]	Loss: 2.0748
Profile done
epoch 1 train time consumed: 3.91s
Validation Epoch: 5, Average loss: 0.0178, Accuracy: 0.3937
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 120.3691886230683,
                        "time": 2.774365722999846,
                        "accuracy": 0.393690664556962,
                        "total_cost": 617042.7615076179
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.5422
Profiling... [256/50048]	Loss: 2.2573
Profiling... [384/50048]	Loss: 2.4354
Profiling... [512/50048]	Loss: 2.4399
Profiling... [640/50048]	Loss: 2.4668
Profiling... [768/50048]	Loss: 2.7293
Profiling... [896/50048]	Loss: 2.2733
Profiling... [1024/50048]	Loss: 2.3835
Profiling... [1152/50048]	Loss: 2.4594
Profiling... [1280/50048]	Loss: 2.2108
Profiling... [1408/50048]	Loss: 2.2914
Profiling... [1536/50048]	Loss: 2.4889
Profiling... [1664/50048]	Loss: 2.2584
Profile done
epoch 1 train time consumed: 3.23s
Validation Epoch: 5, Average loss: 0.0181, Accuracy: 0.3922
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 120.31449901131143,
                        "time": 2.1853157759996975,
                        "accuracy": 0.39220727848101267,
                        "total_cost": 487871.09134595614
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.8591
Profiling... [256/50048]	Loss: 2.2450
Profiling... [384/50048]	Loss: 2.3613
Profiling... [512/50048]	Loss: 2.2315
Profiling... [640/50048]	Loss: 2.4551
Profiling... [768/50048]	Loss: 2.4657
Profiling... [896/50048]	Loss: 2.4123
Profiling... [1024/50048]	Loss: 2.1559
Profiling... [1152/50048]	Loss: 2.4409
Profiling... [1280/50048]	Loss: 2.0979
Profiling... [1408/50048]	Loss: 2.2558
Profiling... [1536/50048]	Loss: 2.1662
Profiling... [1664/50048]	Loss: 2.3720
Profile done
epoch 1 train time consumed: 3.25s
Validation Epoch: 5, Average loss: 0.0180, Accuracy: 0.3916
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 120.35237593037554,
                        "time": 2.191684521999832,
                        "accuracy": 0.3916139240506329,
                        "total_cost": 490034.3708665724
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.4660
Profiling... [256/50048]	Loss: 2.3661
Profiling... [384/50048]	Loss: 2.5921
Profiling... [512/50048]	Loss: 2.3506
Profiling... [640/50048]	Loss: 2.3552
Profiling... [768/50048]	Loss: 2.5129
Profiling... [896/50048]	Loss: 2.2633
Profiling... [1024/50048]	Loss: 2.3273
Profiling... [1152/50048]	Loss: 2.2699
Profiling... [1280/50048]	Loss: 2.2518
Profiling... [1408/50048]	Loss: 2.3322
Profiling... [1536/50048]	Loss: 2.5723
Profiling... [1664/50048]	Loss: 2.4588
Profile done
epoch 1 train time consumed: 3.39s
Validation Epoch: 5, Average loss: 0.0180, Accuracy: 0.3900
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 120.37076858813886,
                        "time": 2.3377414149999822,
                        "accuracy": 0.3900316455696203,
                        "total_cost": 524811.4454379649
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.2699
Profiling... [256/50048]	Loss: 2.4392
Profiling... [384/50048]	Loss: 2.6254
Profiling... [512/50048]	Loss: 2.2103
Profiling... [640/50048]	Loss: 2.3689
Profiling... [768/50048]	Loss: 2.2659
Profiling... [896/50048]	Loss: 2.4096
Profiling... [1024/50048]	Loss: 2.2704
Profiling... [1152/50048]	Loss: 2.4536
Profiling... [1280/50048]	Loss: 2.2094
Profiling... [1408/50048]	Loss: 2.2065
Profiling... [1536/50048]	Loss: 2.4020
Profiling... [1664/50048]	Loss: 2.0916
Profile done
epoch 1 train time consumed: 3.96s
Validation Epoch: 5, Average loss: 0.0179, Accuracy: 0.3930
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 120.32308961468325,
                        "time": 2.7583575329999803,
                        "accuracy": 0.392998417721519,
                        "total_cost": 614562.8590264672
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.3827
Profiling... [256/50048]	Loss: 2.2922
Profiling... [384/50048]	Loss: 2.6933
Profiling... [512/50048]	Loss: 2.6960
Profiling... [640/50048]	Loss: 2.6796
Profiling... [768/50048]	Loss: 2.7688
Profiling... [896/50048]	Loss: 2.5923
Profiling... [1024/50048]	Loss: 2.6676
Profiling... [1152/50048]	Loss: 2.1628
Profiling... [1280/50048]	Loss: 2.6263
Profiling... [1408/50048]	Loss: 2.5322
Profiling... [1536/50048]	Loss: 2.5483
Profiling... [1664/50048]	Loss: 2.3765
Profile done
epoch 1 train time consumed: 3.24s
Validation Epoch: 5, Average loss: 0.0210, Accuracy: 0.3179
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 120.26482643335224,
                        "time": 2.1853169260002687,
                        "accuracy": 0.317939082278481,
                        "total_cost": 601834.2823227618
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.4060
Profiling... [256/50048]	Loss: 2.3041
Profiling... [384/50048]	Loss: 2.5666
Profiling... [512/50048]	Loss: 2.4539
Profiling... [640/50048]	Loss: 2.2449
Profiling... [768/50048]	Loss: 2.1356
Profiling... [896/50048]	Loss: 2.5867
Profiling... [1024/50048]	Loss: 2.6223
Profiling... [1152/50048]	Loss: 2.4264
Profiling... [1280/50048]	Loss: 2.3680
Profiling... [1408/50048]	Loss: 2.5471
Profiling... [1536/50048]	Loss: 2.6782
Profiling... [1664/50048]	Loss: 2.5832
Profile done
epoch 1 train time consumed: 3.37s
Validation Epoch: 5, Average loss: 0.0196, Accuracy: 0.3432
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 120.29634900029347,
                        "time": 2.199622954000006,
                        "accuracy": 0.3431566455696203,
                        "total_cost": 561257.7062599749
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.4438
Profiling... [256/50048]	Loss: 2.4485
Profiling... [384/50048]	Loss: 2.0603
Profiling... [512/50048]	Loss: 2.4817
Profiling... [640/50048]	Loss: 2.7390
Profiling... [768/50048]	Loss: 2.2449
Profiling... [896/50048]	Loss: 2.6941
Profiling... [1024/50048]	Loss: 2.5784
Profiling... [1152/50048]	Loss: 2.6342
Profiling... [1280/50048]	Loss: 2.4507
Profiling... [1408/50048]	Loss: 2.4857
Profiling... [1536/50048]	Loss: 2.2580
Profiling... [1664/50048]	Loss: 2.3918
Profile done
epoch 1 train time consumed: 3.42s
Validation Epoch: 5, Average loss: 0.0199, Accuracy: 0.3388
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 120.31028646183137,
                        "time": 2.3174358269998265,
                        "accuracy": 0.33880537974683544,
                        "total_cost": 598913.2778771402
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.3179
Profiling... [256/50048]	Loss: 2.5129
Profiling... [384/50048]	Loss: 2.3321
Profiling... [512/50048]	Loss: 2.3732
Profiling... [640/50048]	Loss: 2.7073
Profiling... [768/50048]	Loss: 2.3524
Profiling... [896/50048]	Loss: 2.2546
Profiling... [1024/50048]	Loss: 2.5542
Profiling... [1152/50048]	Loss: 2.6832
Profiling... [1280/50048]	Loss: 2.7346
Profiling... [1408/50048]	Loss: 2.2363
Profiling... [1536/50048]	Loss: 2.3091
Profiling... [1664/50048]	Loss: 2.4304
Profile done
epoch 1 train time consumed: 4.02s
Validation Epoch: 5, Average loss: 0.0194, Accuracy: 0.3521
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 120.26600639600517,
                        "time": 2.7654103390000273,
                        "accuracy": 0.35205696202531644,
                        "total_cost": 687785.5665693354
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.3990
Profiling... [256/50048]	Loss: 2.4710
Profiling... [384/50048]	Loss: 2.2975
Profiling... [512/50048]	Loss: 2.2842
Profiling... [640/50048]	Loss: 2.3702
Profiling... [768/50048]	Loss: 2.3352
Profiling... [896/50048]	Loss: 2.6036
Profiling... [1024/50048]	Loss: 2.3076
Profiling... [1152/50048]	Loss: 2.5946
Profiling... [1280/50048]	Loss: 2.9258
Profiling... [1408/50048]	Loss: 2.4099
Profiling... [1536/50048]	Loss: 2.6527
Profiling... [1664/50048]	Loss: 2.4201
Profile done
epoch 1 train time consumed: 3.30s
Validation Epoch: 5, Average loss: 0.0198, Accuracy: 0.3380
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 120.20522630905562,
                        "time": 2.1955523970000286,
                        "accuracy": 0.3380142405063291,
                        "total_cost": 568741.6982370607
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.3246
Profiling... [256/50048]	Loss: 2.3304
Profiling... [384/50048]	Loss: 2.3716
Profiling... [512/50048]	Loss: 2.5903
Profiling... [640/50048]	Loss: 2.4216
Profiling... [768/50048]	Loss: 2.4474
Profiling... [896/50048]	Loss: 2.4386
Profiling... [1024/50048]	Loss: 2.3148
Profiling... [1152/50048]	Loss: 2.4029
Profiling... [1280/50048]	Loss: 2.7994
Profiling... [1408/50048]	Loss: 2.4343
Profiling... [1536/50048]	Loss: 2.6995
Profiling... [1664/50048]	Loss: 2.2181
Profile done
epoch 1 train time consumed: 3.34s
Validation Epoch: 5, Average loss: 0.0210, Accuracy: 0.3313
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 120.23681439138448,
                        "time": 2.181628173000263,
                        "accuracy": 0.33128955696202533,
                        "total_cost": 576606.2259857877
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.6177
Profiling... [256/50048]	Loss: 2.4658
Profiling... [384/50048]	Loss: 2.3761
Profiling... [512/50048]	Loss: 2.6236
Profiling... [640/50048]	Loss: 2.3853
Profiling... [768/50048]	Loss: 2.6732
Profiling... [896/50048]	Loss: 2.3954
Profiling... [1024/50048]	Loss: 2.7225
Profiling... [1152/50048]	Loss: 2.4463
Profiling... [1280/50048]	Loss: 2.7752
Profiling... [1408/50048]	Loss: 2.4479
Profiling... [1536/50048]	Loss: 2.1451
Profiling... [1664/50048]	Loss: 2.1628
Profile done
epoch 1 train time consumed: 3.46s
Validation Epoch: 5, Average loss: 0.0199, Accuracy: 0.3366
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 120.25800575047334,
                        "time": 2.309181019999869,
                        "accuracy": 0.33662974683544306,
                        "total_cost": 600636.7229958988
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.6937
Profiling... [256/50048]	Loss: 2.6403
Profiling... [384/50048]	Loss: 2.3707
Profiling... [512/50048]	Loss: 2.5068
Profiling... [640/50048]	Loss: 2.4676
Profiling... [768/50048]	Loss: 2.5314
Profiling... [896/50048]	Loss: 2.4837
Profiling... [1024/50048]	Loss: 2.3497
Profiling... [1152/50048]	Loss: 2.7502
Profiling... [1280/50048]	Loss: 2.5254
Profiling... [1408/50048]	Loss: 2.4449
Profiling... [1536/50048]	Loss: 2.5996
Profiling... [1664/50048]	Loss: 2.5583
Profile done
epoch 1 train time consumed: 3.98s
Validation Epoch: 5, Average loss: 0.0198, Accuracy: 0.3429
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 120.2080745088223,
                        "time": 2.7498451449996537,
                        "accuracy": 0.3428599683544304,
                        "total_cost": 702259.6663533681
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.1785
Profiling... [256/50048]	Loss: 2.2867
Profiling... [384/50048]	Loss: 2.3070
Profiling... [512/50048]	Loss: 2.4162
Profiling... [640/50048]	Loss: 2.5810
Profiling... [768/50048]	Loss: 2.7254
Profiling... [896/50048]	Loss: 2.5629
Profiling... [1024/50048]	Loss: 2.5240
Profiling... [1152/50048]	Loss: 2.4727
Profiling... [1280/50048]	Loss: 2.4816
Profiling... [1408/50048]	Loss: 2.7204
Profiling... [1536/50048]	Loss: 2.3022
Profiling... [1664/50048]	Loss: 2.3504
Profile done
epoch 1 train time consumed: 3.21s
Validation Epoch: 5, Average loss: 0.0199, Accuracy: 0.3368
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 120.15231094711157,
                        "time": 2.180322321999938,
                        "accuracy": 0.3368275316455696,
                        "total_cost": 566786.1757769841
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.5165
Profiling... [256/50048]	Loss: 2.6247
Profiling... [384/50048]	Loss: 2.4032
Profiling... [512/50048]	Loss: 2.3837
Profiling... [640/50048]	Loss: 2.7622
Profiling... [768/50048]	Loss: 2.6115
Profiling... [896/50048]	Loss: 2.5305
Profiling... [1024/50048]	Loss: 2.6190
Profiling... [1152/50048]	Loss: 2.6433
Profiling... [1280/50048]	Loss: 2.3950
Profiling... [1408/50048]	Loss: 2.3687
Profiling... [1536/50048]	Loss: 2.6036
Profiling... [1664/50048]	Loss: 2.6210
Profile done
epoch 1 train time consumed: 3.34s
Validation Epoch: 5, Average loss: 0.0226, Accuracy: 0.2979
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 120.19152207401471,
                        "time": 2.178106112000023,
                        "accuracy": 0.2978639240506329,
                        "total_cost": 640276.1944142084
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.5781
Profiling... [256/50048]	Loss: 2.5437
Profiling... [384/50048]	Loss: 2.3817
Profiling... [512/50048]	Loss: 2.7040
Profiling... [640/50048]	Loss: 2.6162
Profiling... [768/50048]	Loss: 2.4147
Profiling... [896/50048]	Loss: 2.5679
Profiling... [1024/50048]	Loss: 2.6741
Profiling... [1152/50048]	Loss: 2.4744
Profiling... [1280/50048]	Loss: 2.3202
Profiling... [1408/50048]	Loss: 2.2512
Profiling... [1536/50048]	Loss: 2.3768
Profiling... [1664/50048]	Loss: 2.5026
Profile done
epoch 1 train time consumed: 3.46s
Validation Epoch: 5, Average loss: 0.0199, Accuracy: 0.3438
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 120.20890512947697,
                        "time": 2.320079464000173,
                        "accuracy": 0.34375,
                        "total_cost": 590971.3460541253
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.3985
Profiling... [256/50048]	Loss: 2.5999
Profiling... [384/50048]	Loss: 2.3305
Profiling... [512/50048]	Loss: 2.4435
Profiling... [640/50048]	Loss: 2.2668
Profiling... [768/50048]	Loss: 2.5242
Profiling... [896/50048]	Loss: 2.3832
Profiling... [1024/50048]	Loss: 2.6683
Profiling... [1152/50048]	Loss: 2.7101
Profiling... [1280/50048]	Loss: 2.4373
Profiling... [1408/50048]	Loss: 2.3828
Profiling... [1536/50048]	Loss: 2.6276
Profiling... [1664/50048]	Loss: 2.2939
Profile done
epoch 1 train time consumed: 3.94s
Validation Epoch: 5, Average loss: 0.0206, Accuracy: 0.3331
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 120.16145679736154,
                        "time": 2.740477687999828,
                        "accuracy": 0.33306962025316456,
                        "total_cost": 720439.3105959367
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.5956
Profiling... [256/50048]	Loss: 2.5269
Profiling... [384/50048]	Loss: 2.4602
Profiling... [512/50048]	Loss: 2.7657
Profiling... [640/50048]	Loss: 2.6130
Profiling... [768/50048]	Loss: 2.7415
Profiling... [896/50048]	Loss: 2.8829
Profiling... [1024/50048]	Loss: 2.9157
Profiling... [1152/50048]	Loss: 2.8253
Profiling... [1280/50048]	Loss: 2.7986
Profiling... [1408/50048]	Loss: 2.8665
Profiling... [1536/50048]	Loss: 2.8074
Profiling... [1664/50048]	Loss: 2.7361
Profile done
epoch 1 train time consumed: 3.24s
Validation Epoch: 5, Average loss: 0.0352, Accuracy: 0.2154
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 120.10666701472252,
                        "time": 2.168690518999938,
                        "accuracy": 0.2153876582278481,
                        "total_cost": 881622.7404572561
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.3405
Profiling... [256/50048]	Loss: 2.4949
Profiling... [384/50048]	Loss: 2.4789
Profiling... [512/50048]	Loss: 2.9376
Profiling... [640/50048]	Loss: 2.5555
Profiling... [768/50048]	Loss: 2.9745
Profiling... [896/50048]	Loss: 2.8265
Profiling... [1024/50048]	Loss: 2.9420
Profiling... [1152/50048]	Loss: 3.0044
Profiling... [1280/50048]	Loss: 2.5637
Profiling... [1408/50048]	Loss: 2.8017
Profiling... [1536/50048]	Loss: 2.5727
Profiling... [1664/50048]	Loss: 2.7322
Profile done
epoch 1 train time consumed: 3.37s
Validation Epoch: 5, Average loss: 0.0257, Accuracy: 0.2331
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 120.136815960122,
                        "time": 2.1751766309998857,
                        "accuracy": 0.23308939873417722,
                        "total_cost": 817105.4352711888
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.5202
Profiling... [256/50048]	Loss: 2.5038
Profiling... [384/50048]	Loss: 2.9823
Profiling... [512/50048]	Loss: 3.1042
Profiling... [640/50048]	Loss: 2.7016
Profiling... [768/50048]	Loss: 3.0064
Profiling... [896/50048]	Loss: 2.5931
Profiling... [1024/50048]	Loss: 2.6874
Profiling... [1152/50048]	Loss: 2.7885
Profiling... [1280/50048]	Loss: 2.6610
Profiling... [1408/50048]	Loss: 2.6941
Profiling... [1536/50048]	Loss: 2.5254
Profiling... [1664/50048]	Loss: 2.9024
Profile done
epoch 1 train time consumed: 3.34s
Validation Epoch: 5, Average loss: 0.0323, Accuracy: 0.1932
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 120.15666209345443,
                        "time": 2.3044182519997776,
                        "accuracy": 0.1932357594936709,
                        "total_cost": 1044191.008855186
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.4586
Profiling... [256/50048]	Loss: 2.7366
Profiling... [384/50048]	Loss: 2.8014
Profiling... [512/50048]	Loss: 2.7869
Profiling... [640/50048]	Loss: 2.7471
Profiling... [768/50048]	Loss: 2.6684
Profiling... [896/50048]	Loss: 2.5445
Profiling... [1024/50048]	Loss: 2.7277
Profiling... [1152/50048]	Loss: 2.7053
Profiling... [1280/50048]	Loss: 2.5858
Profiling... [1408/50048]	Loss: 2.9475
Profiling... [1536/50048]	Loss: 2.7544
Profiling... [1664/50048]	Loss: 2.7924
Profile done
epoch 1 train time consumed: 3.92s
Validation Epoch: 5, Average loss: 0.0280, Accuracy: 0.2387
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 120.11275147532122,
                        "time": 2.7413055020001593,
                        "accuracy": 0.2387262658227848,
                        "total_cost": 1005456.4522717242
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.2361
Profiling... [256/50048]	Loss: 2.4707
Profiling... [384/50048]	Loss: 2.9655
Profiling... [512/50048]	Loss: 2.8105
Profiling... [640/50048]	Loss: 2.7102
Profiling... [768/50048]	Loss: 2.9400
Profiling... [896/50048]	Loss: 2.7797
Profiling... [1024/50048]	Loss: 2.7135
Profiling... [1152/50048]	Loss: 2.9547
Profiling... [1280/50048]	Loss: 2.9209
Profiling... [1408/50048]	Loss: 2.9031
Profiling... [1536/50048]	Loss: 2.5725
Profiling... [1664/50048]	Loss: 2.9523
Profile done
epoch 1 train time consumed: 3.31s
Validation Epoch: 5, Average loss: 0.0277, Accuracy: 0.2063
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 120.05257859426878,
                        "time": 2.1773125439999603,
                        "accuracy": 0.2062895569620253,
                        "total_cost": 924164.7827463598
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.4503
Profiling... [256/50048]	Loss: 2.4812
Profiling... [384/50048]	Loss: 2.8281
Profiling... [512/50048]	Loss: 2.6108
Profiling... [640/50048]	Loss: 3.1543
Profiling... [768/50048]	Loss: 2.6586
Profiling... [896/50048]	Loss: 2.6605
Profiling... [1024/50048]	Loss: 2.7142
Profiling... [1152/50048]	Loss: 2.8072
Profiling... [1280/50048]	Loss: 2.7816
Profiling... [1408/50048]	Loss: 2.8387
Profiling... [1536/50048]	Loss: 2.8222
Profiling... [1664/50048]	Loss: 2.8736
Profile done
epoch 1 train time consumed: 3.38s
Validation Epoch: 5, Average loss: 0.0271, Accuracy: 0.2269
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 120.08769273213983,
                        "time": 2.18496156800029,
                        "accuracy": 0.22685917721518986,
                        "total_cost": 843321.980821855
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.3638
Profiling... [256/50048]	Loss: 2.5510
Profiling... [384/50048]	Loss: 2.6775
Profiling... [512/50048]	Loss: 2.8831
Profiling... [640/50048]	Loss: 2.7881
Profiling... [768/50048]	Loss: 3.1637
Profiling... [896/50048]	Loss: 2.8217
Profiling... [1024/50048]	Loss: 3.0656
Profiling... [1152/50048]	Loss: 2.7160
Profiling... [1280/50048]	Loss: 2.5449
Profiling... [1408/50048]	Loss: 2.4689
Profiling... [1536/50048]	Loss: 2.6324
Profiling... [1664/50048]	Loss: 2.6456
Profile done
epoch 1 train time consumed: 3.38s
Validation Epoch: 5, Average loss: 0.0275, Accuracy: 0.2265
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 120.10420166080097,
                        "time": 2.316257894000046,
                        "accuracy": 0.22646360759493672,
                        "total_cost": 895559.6178630255
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.4654
Profiling... [256/50048]	Loss: 2.4489
Profiling... [384/50048]	Loss: 2.4144
Profiling... [512/50048]	Loss: 2.8359
Profiling... [640/50048]	Loss: 2.9489
Profiling... [768/50048]	Loss: 2.6340
Profiling... [896/50048]	Loss: 2.5685
Profiling... [1024/50048]	Loss: 2.8314
Profiling... [1152/50048]	Loss: 2.7269
Profiling... [1280/50048]	Loss: 2.8842
Profiling... [1408/50048]	Loss: 2.7247
Profiling... [1536/50048]	Loss: 2.5240
Profiling... [1664/50048]	Loss: 2.7213
Profile done
epoch 1 train time consumed: 4.00s
Validation Epoch: 5, Average loss: 0.0325, Accuracy: 0.2164
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 120.06479845696317,
                        "time": 2.7474435510002877,
                        "accuracy": 0.216376582278481,
                        "total_cost": 1111794.279248865
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.2325
Profiling... [256/50048]	Loss: 2.4646
Profiling... [384/50048]	Loss: 2.8351
Profiling... [512/50048]	Loss: 2.6984
Profiling... [640/50048]	Loss: 2.8030
Profiling... [768/50048]	Loss: 2.8666
Profiling... [896/50048]	Loss: 2.4910
Profiling... [1024/50048]	Loss: 2.6083
Profiling... [1152/50048]	Loss: 2.7222
Profiling... [1280/50048]	Loss: 2.8274
Profiling... [1408/50048]	Loss: 2.5634
Profiling... [1536/50048]	Loss: 2.6660
Profiling... [1664/50048]	Loss: 2.8093
Profile done
epoch 1 train time consumed: 3.25s
Validation Epoch: 5, Average loss: 0.0355, Accuracy: 0.2291
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 120.0090493884967,
                        "time": 2.1771365429999605,
                        "accuracy": 0.22913370253164558,
                        "total_cost": 831960.0453784221
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.4837
Profiling... [256/50048]	Loss: 2.8046
Profiling... [384/50048]	Loss: 2.7595
Profiling... [512/50048]	Loss: 2.8758
Profiling... [640/50048]	Loss: 2.6446
Profiling... [768/50048]	Loss: 2.6652
Profiling... [896/50048]	Loss: 2.8253
Profiling... [1024/50048]	Loss: 2.7014
Profiling... [1152/50048]	Loss: 2.6891
Profiling... [1280/50048]	Loss: 2.9404
Profiling... [1408/50048]	Loss: 2.7178
Profiling... [1536/50048]	Loss: 2.5291
Profiling... [1664/50048]	Loss: 2.7109
Profile done
epoch 1 train time consumed: 3.31s
Validation Epoch: 5, Average loss: 0.0342, Accuracy: 0.1620
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 120.03879618050084,
                        "time": 2.1832564090000233,
                        "accuracy": 0.1619857594936709,
                        "total_cost": 1180140.6131044263
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.2604
Profiling... [256/50048]	Loss: 2.4894
Profiling... [384/50048]	Loss: 2.7803
Profiling... [512/50048]	Loss: 2.8390
Profiling... [640/50048]	Loss: 2.7570
Profiling... [768/50048]	Loss: 2.5648
Profiling... [896/50048]	Loss: 2.7984
Profiling... [1024/50048]	Loss: 2.8997
Profiling... [1152/50048]	Loss: 2.7422
Profiling... [1280/50048]	Loss: 2.7129
Profiling... [1408/50048]	Loss: 2.6775
Profiling... [1536/50048]	Loss: 2.7798
Profiling... [1664/50048]	Loss: 2.9099
Profile done
epoch 1 train time consumed: 3.47s
Validation Epoch: 5, Average loss: 0.0284, Accuracy: 0.2337
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 120.05618200954679,
                        "time": 2.3070075629998428,
                        "accuracy": 0.23368275316455697,
                        "total_cost": 864426.8534451032
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.5386
Profiling... [256/50048]	Loss: 2.5787
Profiling... [384/50048]	Loss: 2.7263
Profiling... [512/50048]	Loss: 2.7354
Profiling... [640/50048]	Loss: 3.0113
Profiling... [768/50048]	Loss: 2.7111
Profiling... [896/50048]	Loss: 2.9303
Profiling... [1024/50048]	Loss: 2.7230
Profiling... [1152/50048]	Loss: 2.7642
Profiling... [1280/50048]	Loss: 2.8410
Profiling... [1408/50048]	Loss: 2.7047
Profiling... [1536/50048]	Loss: 2.7583
Profiling... [1664/50048]	Loss: 2.6727
Profile done
epoch 1 train time consumed: 3.85s
Validation Epoch: 5, Average loss: 0.0277, Accuracy: 0.2168
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 120.01295383562231,
                        "time": 2.7476239490001717,
                        "accuracy": 0.21677215189873417,
                        "total_cost": 1109837.9965475372
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.5114
Profiling... [512/50176]	Loss: 2.2607
Profiling... [768/50176]	Loss: 2.2353
Profiling... [1024/50176]	Loss: 2.3916
Profiling... [1280/50176]	Loss: 2.2312
Profiling... [1536/50176]	Loss: 2.3072
Profiling... [1792/50176]	Loss: 2.3236
Profiling... [2048/50176]	Loss: 2.2885
Profiling... [2304/50176]	Loss: 2.4307
Profiling... [2560/50176]	Loss: 2.1885
Profiling... [2816/50176]	Loss: 2.4846
Profiling... [3072/50176]	Loss: 2.4569
Profiling... [3328/50176]	Loss: 2.3234
Profile done
epoch 1 train time consumed: 3.77s
Validation Epoch: 5, Average loss: 0.0088, Accuracy: 0.3967
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.97656007297432,
                        "time": 2.426545336000345,
                        "accuracy": 0.3966796875,
                        "total_cost": 535616.7403483886
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.3846
Profiling... [512/50176]	Loss: 2.5128
Profiling... [768/50176]	Loss: 2.2977
Profiling... [1024/50176]	Loss: 2.5295
Profiling... [1280/50176]	Loss: 2.4096
Profiling... [1536/50176]	Loss: 2.3715
Profiling... [1792/50176]	Loss: 2.2577
Profiling... [2048/50176]	Loss: 2.5112
Profiling... [2304/50176]	Loss: 2.5169
Profiling... [2560/50176]	Loss: 2.1505
Profiling... [2816/50176]	Loss: 2.2802
Profiling... [3072/50176]	Loss: 2.2612
Profiling... [3328/50176]	Loss: 2.3195
Profile done
epoch 1 train time consumed: 3.66s
Validation Epoch: 5, Average loss: 0.0089, Accuracy: 0.3926
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 120.0153048383457,
                        "time": 2.423196917000041,
                        "accuracy": 0.392578125,
                        "total_cost": 540466.0297764734
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.1833
Profiling... [512/50176]	Loss: 2.3626
Profiling... [768/50176]	Loss: 2.4438
Profiling... [1024/50176]	Loss: 2.3559
Profiling... [1280/50176]	Loss: 2.3405
Profiling... [1536/50176]	Loss: 2.4573
Profiling... [1792/50176]	Loss: 2.2208
Profiling... [2048/50176]	Loss: 2.3134
Profiling... [2304/50176]	Loss: 2.4486
Profiling... [2560/50176]	Loss: 2.3764
Profiling... [2816/50176]	Loss: 2.3253
Profiling... [3072/50176]	Loss: 2.1600
Profiling... [3328/50176]	Loss: 2.3414
Profile done
epoch 1 train time consumed: 4.03s
Validation Epoch: 5, Average loss: 0.0089, Accuracy: 0.3945
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 120.03853646711606,
                        "time": 2.645558795999932,
                        "accuracy": 0.39453125,
                        "total_cost": 587140.2560709674
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.4116
Profiling... [512/50176]	Loss: 2.5839
Profiling... [768/50176]	Loss: 2.4192
Profiling... [1024/50176]	Loss: 2.2868
Profiling... [1280/50176]	Loss: 2.3296
Profiling... [1536/50176]	Loss: 2.2388
Profiling... [1792/50176]	Loss: 2.5223
Profiling... [2048/50176]	Loss: 2.5826
Profiling... [2304/50176]	Loss: 2.2909
Profiling... [2560/50176]	Loss: 2.4539
Profiling... [2816/50176]	Loss: 2.2400
Profiling... [3072/50176]	Loss: 2.2922
Profiling... [3328/50176]	Loss: 1.9414
Profile done
epoch 1 train time consumed: 4.58s
Validation Epoch: 5, Average loss: 0.0089, Accuracy: 0.3935
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.98645765406913,
                        "time": 3.2280248899996877,
                        "accuracy": 0.39345703125,
                        "total_cost": 718365.1963542278
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.3393
Profiling... [512/50176]	Loss: 2.3392
Profiling... [768/50176]	Loss: 2.4544
Profiling... [1024/50176]	Loss: 2.4570
Profiling... [1280/50176]	Loss: 2.4464
Profiling... [1536/50176]	Loss: 2.3877
Profiling... [1792/50176]	Loss: 2.2913
Profiling... [2048/50176]	Loss: 2.4432
Profiling... [2304/50176]	Loss: 2.3519
Profiling... [2560/50176]	Loss: 2.3580
Profiling... [2816/50176]	Loss: 2.3198
Profiling... [3072/50176]	Loss: 2.1003
Profiling... [3328/50176]	Loss: 2.2421
Profile done
epoch 1 train time consumed: 3.74s
Validation Epoch: 5, Average loss: 0.0089, Accuracy: 0.3947
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.93698007020977,
                        "time": 2.4338125089998357,
                        "accuracy": 0.3947265625,
                        "total_cost": 539878.9107299534
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.4621
Profiling... [512/50176]	Loss: 2.4267
Profiling... [768/50176]	Loss: 2.4242
Profiling... [1024/50176]	Loss: 2.4846
Profiling... [1280/50176]	Loss: 2.1802
Profiling... [1536/50176]	Loss: 2.4104
Profiling... [1792/50176]	Loss: 2.1381
Profiling... [2048/50176]	Loss: 2.3806
Profiling... [2304/50176]	Loss: 2.4868
Profiling... [2560/50176]	Loss: 2.2238
Profiling... [2816/50176]	Loss: 2.0968
Profiling... [3072/50176]	Loss: 2.2047
Profiling... [3328/50176]	Loss: 2.1764
Profile done
epoch 1 train time consumed: 3.71s
Validation Epoch: 5, Average loss: 0.0088, Accuracy: 0.3940
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.98227556409798,
                        "time": 2.4335053519998837,
                        "accuracy": 0.39404296875,
                        "total_cost": 540747.3903944772
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.2408
Profiling... [512/50176]	Loss: 2.3114
Profiling... [768/50176]	Loss: 2.3870
Profiling... [1024/50176]	Loss: 2.3455
Profiling... [1280/50176]	Loss: 2.3115
Profiling... [1536/50176]	Loss: 2.2373
Profiling... [1792/50176]	Loss: 2.3930
Profiling... [2048/50176]	Loss: 2.4445
Profiling... [2304/50176]	Loss: 2.1810
Profiling... [2560/50176]	Loss: 2.3049
Profiling... [2816/50176]	Loss: 2.4049
Profiling... [3072/50176]	Loss: 2.5479
Profiling... [3328/50176]	Loss: 2.2385
Profile done
epoch 1 train time consumed: 3.96s
Validation Epoch: 5, Average loss: 0.0089, Accuracy: 0.3979
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 120.0030766719837,
                        "time": 2.6511813810002423,
                        "accuracy": 0.3978515625,
                        "total_cost": 583477.5269954833
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.3577
Profiling... [512/50176]	Loss: 2.3042
Profiling... [768/50176]	Loss: 2.4923
Profiling... [1024/50176]	Loss: 2.2776
Profiling... [1280/50176]	Loss: 2.3177
Profiling... [1536/50176]	Loss: 2.3447
Profiling... [1792/50176]	Loss: 2.3978
Profiling... [2048/50176]	Loss: 2.1363
Profiling... [2304/50176]	Loss: 2.3126
Profiling... [2560/50176]	Loss: 2.3818
Profiling... [2816/50176]	Loss: 2.2227
Profiling... [3072/50176]	Loss: 2.1865
Profiling... [3328/50176]	Loss: 2.3172
Profile done
epoch 1 train time consumed: 4.64s
Validation Epoch: 5, Average loss: 0.0089, Accuracy: 0.3922
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.95398715812776,
                        "time": 3.236978178999834,
                        "accuracy": 0.3921875,
                        "total_cost": 722689.3638424043
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.3745
Profiling... [512/50176]	Loss: 2.5150
Profiling... [768/50176]	Loss: 2.4541
Profiling... [1024/50176]	Loss: 2.3336
Profiling... [1280/50176]	Loss: 2.4269
Profiling... [1536/50176]	Loss: 2.3811
Profiling... [1792/50176]	Loss: 2.4576
Profiling... [2048/50176]	Loss: 2.4580
Profiling... [2304/50176]	Loss: 2.4568
Profiling... [2560/50176]	Loss: 2.2893
Profiling... [2816/50176]	Loss: 2.2272
Profiling... [3072/50176]	Loss: 2.4224
Profiling... [3328/50176]	Loss: 2.2605
Profile done
epoch 1 train time consumed: 3.72s
Validation Epoch: 5, Average loss: 0.0088, Accuracy: 0.3978
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.90814350743541,
                        "time": 2.4260010519997195,
                        "accuracy": 0.39775390625,
                        "total_cost": 534050.1685924589
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.5762
Profiling... [512/50176]	Loss: 2.4901
Profiling... [768/50176]	Loss: 2.4437
Profiling... [1024/50176]	Loss: 2.5214
Profiling... [1280/50176]	Loss: 2.4430
Profiling... [1536/50176]	Loss: 2.3225
Profiling... [1792/50176]	Loss: 2.2485
Profiling... [2048/50176]	Loss: 2.3695
Profiling... [2304/50176]	Loss: 2.2102
Profiling... [2560/50176]	Loss: 2.2506
Profiling... [2816/50176]	Loss: 2.2472
Profiling... [3072/50176]	Loss: 2.3270
Profiling... [3328/50176]	Loss: 2.1182
Profile done
epoch 1 train time consumed: 3.67s
Validation Epoch: 5, Average loss: 0.0089, Accuracy: 0.3930
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.94908749564581,
                        "time": 2.442051542000172,
                        "accuracy": 0.39296875,
                        "total_cost": 544129.7071384307
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.2787
Profiling... [512/50176]	Loss: 2.5481
Profiling... [768/50176]	Loss: 2.2852
Profiling... [1024/50176]	Loss: 2.4844
Profiling... [1280/50176]	Loss: 2.3152
Profiling... [1536/50176]	Loss: 2.1676
Profiling... [1792/50176]	Loss: 2.4389
Profiling... [2048/50176]	Loss: 2.2881
Profiling... [2304/50176]	Loss: 2.4747
Profiling... [2560/50176]	Loss: 2.3852
Profiling... [2816/50176]	Loss: 2.3444
Profiling... [3072/50176]	Loss: 2.0467
Profiling... [3328/50176]	Loss: 2.1742
Profile done
epoch 1 train time consumed: 3.96s
Validation Epoch: 5, Average loss: 0.0089, Accuracy: 0.3942
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.97425146450725,
                        "time": 2.6471017070002745,
                        "accuracy": 0.39423828125,
                        "total_cost": 587919.0388374998
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.2624
Profiling... [512/50176]	Loss: 2.3504
Profiling... [768/50176]	Loss: 2.4061
Profiling... [1024/50176]	Loss: 2.1972
Profiling... [1280/50176]	Loss: 2.0857
Profiling... [1536/50176]	Loss: 2.3947
Profiling... [1792/50176]	Loss: 2.2399
Profiling... [2048/50176]	Loss: 2.2944
Profiling... [2304/50176]	Loss: 2.2687
Profiling... [2560/50176]	Loss: 2.4440
Profiling... [2816/50176]	Loss: 2.0765
Profiling... [3072/50176]	Loss: 2.3745
Profiling... [3328/50176]	Loss: 2.2851
Profile done
epoch 1 train time consumed: 4.56s
Validation Epoch: 5, Average loss: 0.0089, Accuracy: 0.3951
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.92407196617262,
                        "time": 3.229497889999948,
                        "accuracy": 0.3951171875,
                        "total_cost": 715673.0245850995
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.2232
Profiling... [512/50176]	Loss: 2.1754
Profiling... [768/50176]	Loss: 2.3402
Profiling... [1024/50176]	Loss: 2.4929
Profiling... [1280/50176]	Loss: 2.3278
Profiling... [1536/50176]	Loss: 2.4132
Profiling... [1792/50176]	Loss: 2.4273
Profiling... [2048/50176]	Loss: 2.2927
Profiling... [2304/50176]	Loss: 2.4933
Profiling... [2560/50176]	Loss: 2.3195
Profiling... [2816/50176]	Loss: 2.3987
Profiling... [3072/50176]	Loss: 2.2593
Profiling... [3328/50176]	Loss: 2.3038
Profile done
epoch 1 train time consumed: 3.67s
Validation Epoch: 5, Average loss: 0.0102, Accuracy: 0.3320
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.87820754547818,
                        "time": 2.4319899210004223,
                        "accuracy": 0.33203125,
                        "total_cost": 641339.9021441085
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.4102
Profiling... [512/50176]	Loss: 2.3550
Profiling... [768/50176]	Loss: 2.4731
Profiling... [1024/50176]	Loss: 2.3965
Profiling... [1280/50176]	Loss: 2.4783
Profiling... [1536/50176]	Loss: 2.5525
Profiling... [1792/50176]	Loss: 2.2765
Profiling... [2048/50176]	Loss: 2.3858
Profiling... [2304/50176]	Loss: 2.4398
Profiling... [2560/50176]	Loss: 2.5087
Profiling... [2816/50176]	Loss: 2.4806
Profiling... [3072/50176]	Loss: 2.4103
Profiling... [3328/50176]	Loss: 2.4780
Profile done
epoch 1 train time consumed: 3.71s
Validation Epoch: 5, Average loss: 0.0096, Accuracy: 0.3558
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.92082855467052,
                        "time": 2.4439416380000694,
                        "accuracy": 0.35576171875,
                        "total_cost": 601502.1341530725
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.4954
Profiling... [512/50176]	Loss: 2.3559
Profiling... [768/50176]	Loss: 2.3908
Profiling... [1024/50176]	Loss: 2.2782
Profiling... [1280/50176]	Loss: 2.3635
Profiling... [1536/50176]	Loss: 2.5063
Profiling... [1792/50176]	Loss: 2.3881
Profiling... [2048/50176]	Loss: 2.4478
Profiling... [2304/50176]	Loss: 2.3367
Profiling... [2560/50176]	Loss: 2.3529
Profiling... [2816/50176]	Loss: 2.3667
Profiling... [3072/50176]	Loss: 2.5119
Profiling... [3328/50176]	Loss: 2.3758
Profile done
epoch 1 train time consumed: 3.92s
Validation Epoch: 5, Average loss: 0.0096, Accuracy: 0.3526
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.94277069252325,
                        "time": 2.648905042000024,
                        "accuracy": 0.35263671875,
                        "total_cost": 657725.1810366714
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.3090
Profiling... [512/50176]	Loss: 2.3730
Profiling... [768/50176]	Loss: 2.4587
Profiling... [1024/50176]	Loss: 2.4976
Profiling... [1280/50176]	Loss: 2.4209
Profiling... [1536/50176]	Loss: 2.4770
Profiling... [1792/50176]	Loss: 2.4799
Profiling... [2048/50176]	Loss: 2.5680
Profiling... [2304/50176]	Loss: 2.5182
Profiling... [2560/50176]	Loss: 2.2828
Profiling... [2816/50176]	Loss: 2.5072
Profiling... [3072/50176]	Loss: 2.5323
Profiling... [3328/50176]	Loss: 2.3416
Profile done
epoch 1 train time consumed: 4.65s
Validation Epoch: 5, Average loss: 0.0098, Accuracy: 0.3464
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.89504691745401,
                        "time": 3.213806269000088,
                        "accuracy": 0.34638671875,
                        "total_cost": 812388.850472375
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.4040
Profiling... [512/50176]	Loss: 2.3653
Profiling... [768/50176]	Loss: 2.3559
Profiling... [1024/50176]	Loss: 2.5472
Profiling... [1280/50176]	Loss: 2.3785
Profiling... [1536/50176]	Loss: 2.3442
Profiling... [1792/50176]	Loss: 2.4885
Profiling... [2048/50176]	Loss: 2.4251
Profiling... [2304/50176]	Loss: 2.2032
Profiling... [2560/50176]	Loss: 2.2730
Profiling... [2816/50176]	Loss: 2.2141
Profiling... [3072/50176]	Loss: 2.5240
Profiling... [3328/50176]	Loss: 2.3476
Profile done
epoch 1 train time consumed: 3.68s
Validation Epoch: 5, Average loss: 0.0097, Accuracy: 0.3560
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.85354026979682,
                        "time": 2.4272993179997684,
                        "accuracy": 0.35595703125,
                        "total_cost": 597078.107396588
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.5182
Profiling... [512/50176]	Loss: 2.3766
Profiling... [768/50176]	Loss: 2.3203
Profiling... [1024/50176]	Loss: 2.4248
Profiling... [1280/50176]	Loss: 2.3433
Profiling... [1536/50176]	Loss: 2.4562
Profiling... [1792/50176]	Loss: 2.4103
Profiling... [2048/50176]	Loss: 2.3039
Profiling... [2304/50176]	Loss: 2.5066
Profiling... [2560/50176]	Loss: 2.5201
Profiling... [2816/50176]	Loss: 2.3252
Profiling... [3072/50176]	Loss: 2.3626
Profiling... [3328/50176]	Loss: 2.3195
Profile done
epoch 1 train time consumed: 3.81s
Validation Epoch: 5, Average loss: 0.0096, Accuracy: 0.3623
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.8965617240976,
                        "time": 2.4393522999998822,
                        "accuracy": 0.3623046875,
                        "total_cost": 589530.2175100662
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.3107
Profiling... [512/50176]	Loss: 2.2703
Profiling... [768/50176]	Loss: 2.5267
Profiling... [1024/50176]	Loss: 2.4837
Profiling... [1280/50176]	Loss: 2.1398
Profiling... [1536/50176]	Loss: 2.1801
Profiling... [1792/50176]	Loss: 2.4074
Profiling... [2048/50176]	Loss: 2.3891
Profiling... [2304/50176]	Loss: 2.5387
Profiling... [2560/50176]	Loss: 2.5062
Profiling... [2816/50176]	Loss: 2.4402
Profiling... [3072/50176]	Loss: 2.2806
Profiling... [3328/50176]	Loss: 2.4521
Profile done
epoch 1 train time consumed: 3.87s
Validation Epoch: 5, Average loss: 0.0098, Accuracy: 0.3521
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.91031232266491,
                        "time": 2.655348732999755,
                        "accuracy": 0.35205078125,
                        "total_cost": 660422.3832706759
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.4593
Profiling... [512/50176]	Loss: 2.1554
Profiling... [768/50176]	Loss: 2.5731
Profiling... [1024/50176]	Loss: 2.1361
Profiling... [1280/50176]	Loss: 2.3734
Profiling... [1536/50176]	Loss: 2.3658
Profiling... [1792/50176]	Loss: 2.3141
Profiling... [2048/50176]	Loss: 2.3407
Profiling... [2304/50176]	Loss: 2.4376
Profiling... [2560/50176]	Loss: 2.3462
Profiling... [2816/50176]	Loss: 2.2293
Profiling... [3072/50176]	Loss: 2.4358
Profiling... [3328/50176]	Loss: 2.3922
Profile done
epoch 1 train time consumed: 4.66s
Validation Epoch: 5, Average loss: 0.0102, Accuracy: 0.3405
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.86552635578369,
                        "time": 3.2308815629999117,
                        "accuracy": 0.34052734375,
                        "total_cost": 830757.8748499939
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.2368
Profiling... [512/50176]	Loss: 2.3080
Profiling... [768/50176]	Loss: 2.4387
Profiling... [1024/50176]	Loss: 2.4022
Profiling... [1280/50176]	Loss: 2.2471
Profiling... [1536/50176]	Loss: 2.3909
Profiling... [1792/50176]	Loss: 2.3395
Profiling... [2048/50176]	Loss: 2.4021
Profiling... [2304/50176]	Loss: 2.3647
Profiling... [2560/50176]	Loss: 2.2881
Profiling... [2816/50176]	Loss: 2.3004
Profiling... [3072/50176]	Loss: 2.4318
Profiling... [3328/50176]	Loss: 2.2247
Profile done
epoch 1 train time consumed: 3.66s
Validation Epoch: 5, Average loss: 0.0094, Accuracy: 0.3648
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.82257614644666,
                        "time": 2.423878895999678,
                        "accuracy": 0.36484375,
                        "total_cost": 581713.7366523744
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.3989
Profiling... [512/50176]	Loss: 2.4818
Profiling... [768/50176]	Loss: 2.4268
Profiling... [1024/50176]	Loss: 2.3925
Profiling... [1280/50176]	Loss: 2.4053
Profiling... [1536/50176]	Loss: 2.3885
Profiling... [1792/50176]	Loss: 2.3659
Profiling... [2048/50176]	Loss: 2.3958
Profiling... [2304/50176]	Loss: 2.3944
Profiling... [2560/50176]	Loss: 2.4314
Profiling... [2816/50176]	Loss: 2.4197
Profiling... [3072/50176]	Loss: 2.2881
Profiling... [3328/50176]	Loss: 2.3761
Profile done
epoch 1 train time consumed: 3.70s
Validation Epoch: 5, Average loss: 0.0096, Accuracy: 0.3543
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.86609826594855,
                        "time": 2.424921045999781,
                        "accuracy": 0.354296875,
                        "total_cost": 599288.1659968757
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.3682
Profiling... [512/50176]	Loss: 2.4559
Profiling... [768/50176]	Loss: 2.4472
Profiling... [1024/50176]	Loss: 2.4182
Profiling... [1280/50176]	Loss: 2.5014
Profiling... [1536/50176]	Loss: 2.5309
Profiling... [1792/50176]	Loss: 2.4135
Profiling... [2048/50176]	Loss: 2.3304
Profiling... [2304/50176]	Loss: 2.3469
Profiling... [2560/50176]	Loss: 2.2478
Profiling... [2816/50176]	Loss: 2.4492
Profiling... [3072/50176]	Loss: 2.2561
Profiling... [3328/50176]	Loss: 2.2727
Profile done
epoch 1 train time consumed: 4.03s
Validation Epoch: 5, Average loss: 0.0098, Accuracy: 0.3421
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.88389344074682,
                        "time": 2.6486834940001245,
                        "accuracy": 0.34208984375,
                        "total_cost": 677946.3851589056
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.3468
Profiling... [512/50176]	Loss: 2.3378
Profiling... [768/50176]	Loss: 2.3545
Profiling... [1024/50176]	Loss: 2.3994
Profiling... [1280/50176]	Loss: 2.3919
Profiling... [1536/50176]	Loss: 2.3626
Profiling... [1792/50176]	Loss: 2.4471
Profiling... [2048/50176]	Loss: 2.5651
Profiling... [2304/50176]	Loss: 2.4575
Profiling... [2560/50176]	Loss: 2.4556
Profiling... [2816/50176]	Loss: 2.3039
Profiling... [3072/50176]	Loss: 2.4797
Profiling... [3328/50176]	Loss: 2.3718
Profile done
epoch 1 train time consumed: 4.64s
Validation Epoch: 5, Average loss: 0.0104, Accuracy: 0.3151
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.83384000483672,
                        "time": 3.218413817000055,
                        "accuracy": 0.31513671875,
                        "total_cost": 894227.9006664078
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.3362
Profiling... [512/50176]	Loss: 2.7079
Profiling... [768/50176]	Loss: 2.6865
Profiling... [1024/50176]	Loss: 2.4794
Profiling... [1280/50176]	Loss: 2.7677
Profiling... [1536/50176]	Loss: 2.5227
Profiling... [1792/50176]	Loss: 2.6296
Profiling... [2048/50176]	Loss: 2.5951
Profiling... [2304/50176]	Loss: 2.5591
Profiling... [2560/50176]	Loss: 2.6361
Profiling... [2816/50176]	Loss: 2.6647
Profiling... [3072/50176]	Loss: 2.6128
Profiling... [3328/50176]	Loss: 2.5151
Profile done
epoch 1 train time consumed: 3.71s
Validation Epoch: 5, Average loss: 0.0144, Accuracy: 0.2224
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.79058919204184,
                        "time": 2.4226276499998676,
                        "accuracy": 0.22236328125,
                        "total_cost": 953957.0660197822
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.4116
Profiling... [512/50176]	Loss: 2.7451
Profiling... [768/50176]	Loss: 2.5776
Profiling... [1024/50176]	Loss: 2.6444
Profiling... [1280/50176]	Loss: 2.6309
Profiling... [1536/50176]	Loss: 2.5202
Profiling... [1792/50176]	Loss: 2.6820
Profiling... [2048/50176]	Loss: 2.6452
Profiling... [2304/50176]	Loss: 2.5766
Profiling... [2560/50176]	Loss: 2.7234
Profiling... [2816/50176]	Loss: 2.6411
Profiling... [3072/50176]	Loss: 2.4872
Profiling... [3328/50176]	Loss: 2.7122
Profile done
epoch 1 train time consumed: 3.67s
Validation Epoch: 5, Average loss: 0.0133, Accuracy: 0.2380
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.83161850517872,
                        "time": 2.4336521830000493,
                        "accuracy": 0.23798828125,
                        "total_cost": 895381.8193621166
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.4420
Profiling... [512/50176]	Loss: 2.5613
Profiling... [768/50176]	Loss: 2.3747
Profiling... [1024/50176]	Loss: 2.5736
Profiling... [1280/50176]	Loss: 2.6359
Profiling... [1536/50176]	Loss: 2.5679
Profiling... [1792/50176]	Loss: 2.7219
Profiling... [2048/50176]	Loss: 2.6485
Profiling... [2304/50176]	Loss: 2.4740
Profiling... [2560/50176]	Loss: 2.5851
Profiling... [2816/50176]	Loss: 2.3548
Profiling... [3072/50176]	Loss: 2.3602
Profiling... [3328/50176]	Loss: 2.5535
Profile done
epoch 1 train time consumed: 4.06s
Validation Epoch: 5, Average loss: 0.0141, Accuracy: 0.2444
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.85374156051984,
                        "time": 2.637766038999871,
                        "accuracy": 0.24443359375,
                        "total_cost": 944888.9489113294
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.5030
Profiling... [512/50176]	Loss: 2.5217
Profiling... [768/50176]	Loss: 2.6971
Profiling... [1024/50176]	Loss: 2.6280
Profiling... [1280/50176]	Loss: 2.5507
Profiling... [1536/50176]	Loss: 2.7031
Profiling... [1792/50176]	Loss: 2.9417
Profiling... [2048/50176]	Loss: 2.4651
Profiling... [2304/50176]	Loss: 2.6958
Profiling... [2560/50176]	Loss: 2.6736
Profiling... [2816/50176]	Loss: 2.5234
Profiling... [3072/50176]	Loss: 2.5961
Profiling... [3328/50176]	Loss: 2.6654
Profile done
epoch 1 train time consumed: 4.57s
Validation Epoch: 5, Average loss: 0.0132, Accuracy: 0.2443
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.80233422249873,
                        "time": 3.2225257740001325,
                        "accuracy": 0.2443359375,
                        "total_cost": 1154820.0447587466
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.2363
Profiling... [512/50176]	Loss: 2.5265
Profiling... [768/50176]	Loss: 2.4482
Profiling... [1024/50176]	Loss: 2.5188
Profiling... [1280/50176]	Loss: 2.6184
Profiling... [1536/50176]	Loss: 2.6440
Profiling... [1792/50176]	Loss: 2.6776
Profiling... [2048/50176]	Loss: 2.5445
Profiling... [2304/50176]	Loss: 2.4897
Profiling... [2560/50176]	Loss: 2.5412
Profiling... [2816/50176]	Loss: 2.7313
Profiling... [3072/50176]	Loss: 2.5122
Profiling... [3328/50176]	Loss: 2.6018
Profile done
epoch 1 train time consumed: 3.75s
Validation Epoch: 5, Average loss: 0.0130, Accuracy: 0.2585
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.76138994413394,
                        "time": 2.43207363800002,
                        "accuracy": 0.25849609375,
                        "total_cost": 823811.5883894993
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.5350
Profiling... [512/50176]	Loss: 2.7084
Profiling... [768/50176]	Loss: 2.5739
Profiling... [1024/50176]	Loss: 2.5229
Profiling... [1280/50176]	Loss: 2.5824
Profiling... [1536/50176]	Loss: 2.8345
Profiling... [1792/50176]	Loss: 2.7281
Profiling... [2048/50176]	Loss: 2.7178
Profiling... [2304/50176]	Loss: 2.5365
Profiling... [2560/50176]	Loss: 2.6421
Profiling... [2816/50176]	Loss: 2.5882
Profiling... [3072/50176]	Loss: 2.6668
Profiling... [3328/50176]	Loss: 2.6860
Profile done
epoch 1 train time consumed: 3.84s
Validation Epoch: 5, Average loss: 0.0131, Accuracy: 0.2278
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.79870616756455,
                        "time": 2.4287544110002273,
                        "accuracy": 0.22783203125,
                        "total_cost": 933413.4915699575
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.4992
Profiling... [512/50176]	Loss: 2.4744
Profiling... [768/50176]	Loss: 2.6587
Profiling... [1024/50176]	Loss: 2.7118
Profiling... [1280/50176]	Loss: 2.6672
Profiling... [1536/50176]	Loss: 2.5984
Profiling... [1792/50176]	Loss: 2.3315
Profiling... [2048/50176]	Loss: 2.5477
Profiling... [2304/50176]	Loss: 2.6060
Profiling... [2560/50176]	Loss: 2.4425
Profiling... [2816/50176]	Loss: 2.5921
Profiling... [3072/50176]	Loss: 2.6640
Profiling... [3328/50176]	Loss: 2.6873
Profile done
epoch 1 train time consumed: 3.94s
Validation Epoch: 5, Average loss: 0.0125, Accuracy: 0.2712
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.81721257291575,
                        "time": 2.657149471000139,
                        "accuracy": 0.27119140625,
                        "total_cost": 857917.1738928013
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.4774
Profiling... [512/50176]	Loss: 2.5358
Profiling... [768/50176]	Loss: 2.6951
Profiling... [1024/50176]	Loss: 2.4906
Profiling... [1280/50176]	Loss: 2.5831
Profiling... [1536/50176]	Loss: 2.7153
Profiling... [1792/50176]	Loss: 2.6735
Profiling... [2048/50176]	Loss: 2.4548
Profiling... [2304/50176]	Loss: 2.4956
Profiling... [2560/50176]	Loss: 2.6606
Profiling... [2816/50176]	Loss: 2.7198
Profiling... [3072/50176]	Loss: 2.5850
Profiling... [3328/50176]	Loss: 2.6152
Profile done
epoch 1 train time consumed: 4.73s
Validation Epoch: 5, Average loss: 0.0119, Accuracy: 0.2549
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.77322697676217,
                        "time": 3.2417545530001917,
                        "accuracy": 0.2548828125,
                        "total_cost": 1113639.8657302815
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.6003
Profiling... [512/50176]	Loss: 2.5812
Profiling... [768/50176]	Loss: 2.5448
Profiling... [1024/50176]	Loss: 2.3941
Profiling... [1280/50176]	Loss: 2.4396
Profiling... [1536/50176]	Loss: 2.6337
Profiling... [1792/50176]	Loss: 2.5623
Profiling... [2048/50176]	Loss: 2.6559
Profiling... [2304/50176]	Loss: 2.5411
Profiling... [2560/50176]	Loss: 2.4707
Profiling... [2816/50176]	Loss: 2.5173
Profiling... [3072/50176]	Loss: 2.6264
Profiling... [3328/50176]	Loss: 2.5717
Profile done
epoch 1 train time consumed: 3.73s
Validation Epoch: 5, Average loss: 0.0137, Accuracy: 0.2351
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.72922409804099,
                        "time": 2.4810706039997967,
                        "accuracy": 0.23505859375,
                        "total_cost": 924204.4832881456
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.3962
Profiling... [512/50176]	Loss: 2.3993
Profiling... [768/50176]	Loss: 2.8068
Profiling... [1024/50176]	Loss: 2.6635
Profiling... [1280/50176]	Loss: 2.7133
Profiling... [1536/50176]	Loss: 2.5295
Profiling... [1792/50176]	Loss: 2.5536
Profiling... [2048/50176]	Loss: 2.4337
Profiling... [2304/50176]	Loss: 2.5479
Profiling... [2560/50176]	Loss: 2.6787
Profiling... [2816/50176]	Loss: 2.6853
Profiling... [3072/50176]	Loss: 2.6078
Profiling... [3328/50176]	Loss: 2.5354
Profile done
epoch 1 train time consumed: 3.85s
Validation Epoch: 5, Average loss: 0.0154, Accuracy: 0.2037
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.77186335842542,
                        "time": 2.4846129139996265,
                        "accuracy": 0.2037109375,
                        "total_cost": 1067946.6993969548
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.4351
Profiling... [512/50176]	Loss: 2.5322
Profiling... [768/50176]	Loss: 2.6737
Profiling... [1024/50176]	Loss: 2.5583
Profiling... [1280/50176]	Loss: 2.6297
Profiling... [1536/50176]	Loss: 2.4731
Profiling... [1792/50176]	Loss: 2.5896
Profiling... [2048/50176]	Loss: 2.7066
Profiling... [2304/50176]	Loss: 2.5849
Profiling... [2560/50176]	Loss: 2.7180
Profiling... [2816/50176]	Loss: 2.5693
Profiling... [3072/50176]	Loss: 2.5525
Profiling... [3328/50176]	Loss: 2.4332
Profile done
epoch 1 train time consumed: 4.06s
Validation Epoch: 5, Average loss: 0.0130, Accuracy: 0.2309
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.78718198706726,
                        "time": 2.7078533889998653,
                        "accuracy": 0.230859375,
                        "total_cost": 1027029.3532625193
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.2977
Profiling... [512/50176]	Loss: 2.6200
Profiling... [768/50176]	Loss: 2.7337
Profiling... [1024/50176]	Loss: 2.6707
Profiling... [1280/50176]	Loss: 2.7013
Profiling... [1536/50176]	Loss: 2.4609
Profiling... [1792/50176]	Loss: 2.6487
Profiling... [2048/50176]	Loss: 2.7389
Profiling... [2304/50176]	Loss: 2.5971
Profiling... [2560/50176]	Loss: 2.7942
Profiling... [2816/50176]	Loss: 2.4743
Profiling... [3072/50176]	Loss: 2.4747
Profiling... [3328/50176]	Loss: 2.6044
Profile done
epoch 1 train time consumed: 4.64s
Validation Epoch: 5, Average loss: 0.0114, Accuracy: 0.2798
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.74462393879075,
                        "time": 3.2548662609997336,
                        "accuracy": 0.27978515625,
                        "total_cost": 1018623.2823267368
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.3388
Profiling... [1024/50176]	Loss: 2.4202
Profiling... [1536/50176]	Loss: 2.3831
Profiling... [2048/50176]	Loss: 2.4938
Profiling... [2560/50176]	Loss: 2.3282
Profiling... [3072/50176]	Loss: 2.2961
Profiling... [3584/50176]	Loss: 2.4497
Profiling... [4096/50176]	Loss: 2.2511
Profiling... [4608/50176]	Loss: 2.3852
Profiling... [5120/50176]	Loss: 2.2214
Profiling... [5632/50176]	Loss: 2.1408
Profiling... [6144/50176]	Loss: 2.1965
Profiling... [6656/50176]	Loss: 2.2653
Profile done
epoch 1 train time consumed: 6.91s
Validation Epoch: 5, Average loss: 0.0044, Accuracy: 0.4006
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.75558621937765,
                        "time": 4.7015662660001,
                        "accuracy": 0.4005859375,
                        "total_cost": 1027666.0490289322
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.3877
Profiling... [1024/50176]	Loss: 2.4635
Profiling... [1536/50176]	Loss: 2.3295
Profiling... [2048/50176]	Loss: 2.5068
Profiling... [2560/50176]	Loss: 2.1694
Profiling... [3072/50176]	Loss: 2.4004
Profiling... [3584/50176]	Loss: 2.2268
Profiling... [4096/50176]	Loss: 2.2869
Profiling... [4608/50176]	Loss: 2.3294
Profiling... [5120/50176]	Loss: 2.2845
Profiling... [5632/50176]	Loss: 2.1538
Profiling... [6144/50176]	Loss: 2.2322
Profiling... [6656/50176]	Loss: 2.2364
Profile done
epoch 1 train time consumed: 6.89s
Validation Epoch: 5, Average loss: 0.0044, Accuracy: 0.4009
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.81985787301471,
                        "time": 4.720954888000051,
                        "accuracy": 0.40087890625,
                        "total_cost": 1031150.2509290661
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.3536
Profiling... [1024/50176]	Loss: 2.3659
Profiling... [1536/50176]	Loss: 2.3733
Profiling... [2048/50176]	Loss: 2.3665
Profiling... [2560/50176]	Loss: 2.4078
Profiling... [3072/50176]	Loss: 2.2166
Profiling... [3584/50176]	Loss: 2.2648
Profiling... [4096/50176]	Loss: 2.3010
Profiling... [4608/50176]	Loss: 2.2950
Profiling... [5120/50176]	Loss: 2.2270
Profiling... [5632/50176]	Loss: 2.2783
Profiling... [6144/50176]	Loss: 2.2943
Profiling... [6656/50176]	Loss: 2.2756
Profile done
epoch 1 train time consumed: 7.47s
Validation Epoch: 5, Average loss: 0.0044, Accuracy: 0.3986
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.83848035530255,
                        "time": 5.175189954999951,
                        "accuracy": 0.3986328125,
                        "total_cost": 1136733.3553666617
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.2753
Profiling... [1024/50176]	Loss: 2.3819
Profiling... [1536/50176]	Loss: 2.3531
Profiling... [2048/50176]	Loss: 2.2343
Profiling... [2560/50176]	Loss: 2.5044
Profiling... [3072/50176]	Loss: 2.3619
Profiling... [3584/50176]	Loss: 2.3005
Profiling... [4096/50176]	Loss: 2.3683
Profiling... [4608/50176]	Loss: 2.3045
Profiling... [5120/50176]	Loss: 2.2765
Profiling... [5632/50176]	Loss: 2.3060
Profiling... [6144/50176]	Loss: 2.1914
Profiling... [6656/50176]	Loss: 2.2622
Profile done
epoch 1 train time consumed: 9.99s
Validation Epoch: 5, Average loss: 0.0044, Accuracy: 0.4045
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.76422122575708,
                        "time": 6.705731280000236,
                        "accuracy": 0.4044921875,
                        "total_cost": 1451580.669508654
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.4009
Profiling... [1024/50176]	Loss: 2.3986
Profiling... [1536/50176]	Loss: 2.2861
Profiling... [2048/50176]	Loss: 2.3765
Profiling... [2560/50176]	Loss: 2.3055
Profiling... [3072/50176]	Loss: 2.3552
Profiling... [3584/50176]	Loss: 2.4008
Profiling... [4096/50176]	Loss: 2.3574
Profiling... [4608/50176]	Loss: 2.3176
Profiling... [5120/50176]	Loss: 2.2556
Profiling... [5632/50176]	Loss: 2.2515
Profiling... [6144/50176]	Loss: 2.0828
Profiling... [6656/50176]	Loss: 2.2900
Profile done
epoch 1 train time consumed: 7.01s
Validation Epoch: 5, Average loss: 0.0044, Accuracy: 0.4001
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.73219217691887,
                        "time": 4.703033637999852,
                        "accuracy": 0.40009765625,
                        "total_cost": 1029241.2093795333
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.3768
Profiling... [1024/50176]	Loss: 2.4428
Profiling... [1536/50176]	Loss: 2.4246
Profiling... [2048/50176]	Loss: 2.2982
Profiling... [2560/50176]	Loss: 2.1994
Profiling... [3072/50176]	Loss: 2.2188
Profiling... [3584/50176]	Loss: 2.3087
Profiling... [4096/50176]	Loss: 2.2439
Profiling... [4608/50176]	Loss: 2.2621
Profiling... [5120/50176]	Loss: 2.2294
Profiling... [5632/50176]	Loss: 2.2466
Profiling... [6144/50176]	Loss: 2.2799
Profiling... [6656/50176]	Loss: 2.2294
Profile done
epoch 1 train time consumed: 6.94s
Validation Epoch: 5, Average loss: 0.0044, Accuracy: 0.3985
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.7917851838457,
                        "time": 4.737984405000134,
                        "accuracy": 0.39853515625,
                        "total_cost": 1040955.6465384873
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.2049
Profiling... [1024/50176]	Loss: 2.3557
Profiling... [1536/50176]	Loss: 2.3753
Profiling... [2048/50176]	Loss: 2.2298
Profiling... [2560/50176]	Loss: 2.3553
Profiling... [3072/50176]	Loss: 2.2593
Profiling... [3584/50176]	Loss: 2.2289
Profiling... [4096/50176]	Loss: 2.3384
Profiling... [4608/50176]	Loss: 2.2986
Profiling... [5120/50176]	Loss: 2.3094
Profiling... [5632/50176]	Loss: 2.3331
Profiling... [6144/50176]	Loss: 2.2699
Profiling... [6656/50176]	Loss: 2.3147
Profile done
epoch 1 train time consumed: 7.41s
Validation Epoch: 5, Average loss: 0.0044, Accuracy: 0.3990
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.81317190379801,
                        "time": 5.1655115450002995,
                        "accuracy": 0.3990234375,
                        "total_cost": 1133496.5965473703
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.4138
Profiling... [1024/50176]	Loss: 2.3850
Profiling... [1536/50176]	Loss: 2.3398
Profiling... [2048/50176]	Loss: 2.4220
Profiling... [2560/50176]	Loss: 2.2967
Profiling... [3072/50176]	Loss: 2.2752
Profiling... [3584/50176]	Loss: 2.3018
Profiling... [4096/50176]	Loss: 2.1322
Profiling... [4608/50176]	Loss: 2.3811
Profiling... [5120/50176]	Loss: 2.2936
Profiling... [5632/50176]	Loss: 2.3119
Profiling... [6144/50176]	Loss: 2.2979
Profiling... [6656/50176]	Loss: 2.2534
Profile done
epoch 1 train time consumed: 13.90s
Validation Epoch: 5, Average loss: 0.0044, Accuracy: 0.3988
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.70926020995022,
                        "time": 10.113371488000212,
                        "accuracy": 0.398828125,
                        "total_cost": 2220318.1816968075
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.5093
Profiling... [1024/50176]	Loss: 2.2701
Profiling... [1536/50176]	Loss: 2.3458
Profiling... [2048/50176]	Loss: 2.3860
Profiling... [2560/50176]	Loss: 2.3363
Profiling... [3072/50176]	Loss: 2.2822
Profiling... [3584/50176]	Loss: 2.2537
Profiling... [4096/50176]	Loss: 2.2809
Profiling... [4608/50176]	Loss: 2.3417
Profiling... [5120/50176]	Loss: 2.2898
Profiling... [5632/50176]	Loss: 2.2487
Profiling... [6144/50176]	Loss: 2.1709
Profiling... [6656/50176]	Loss: 2.1899
Profile done
epoch 1 train time consumed: 6.65s
Validation Epoch: 5, Average loss: 0.0044, Accuracy: 0.3978
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.6673771767115,
                        "time": 4.576610325000274,
                        "accuracy": 0.39775390625,
                        "total_cost": 1007475.3072635608
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.4646
Profiling... [1024/50176]	Loss: 2.3191
Profiling... [1536/50176]	Loss: 2.3443
Profiling... [2048/50176]	Loss: 2.3182
Profiling... [2560/50176]	Loss: 2.2268
Profiling... [3072/50176]	Loss: 2.3199
Profiling... [3584/50176]	Loss: 2.4065
Profiling... [4096/50176]	Loss: 2.2925
Profiling... [4608/50176]	Loss: 2.4465
Profiling... [5120/50176]	Loss: 2.3078
Profiling... [5632/50176]	Loss: 2.3362
Profiling... [6144/50176]	Loss: 2.3096
Profiling... [6656/50176]	Loss: 2.1361
Profile done
epoch 1 train time consumed: 6.75s
Validation Epoch: 5, Average loss: 0.0044, Accuracy: 0.3972
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.72741350624874,
                        "time": 4.596896061999814,
                        "accuracy": 0.39716796875,
                        "total_cost": 1013434.1747890864
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.4592
Profiling... [1024/50176]	Loss: 2.2878
Profiling... [1536/50176]	Loss: 2.3634
Profiling... [2048/50176]	Loss: 2.4750
Profiling... [2560/50176]	Loss: 2.2870
Profiling... [3072/50176]	Loss: 2.2683
Profiling... [3584/50176]	Loss: 2.3893
Profiling... [4096/50176]	Loss: 2.3930
Profiling... [4608/50176]	Loss: 2.1499
Profiling... [5120/50176]	Loss: 2.3135
Profiling... [5632/50176]	Loss: 2.4123
Profiling... [6144/50176]	Loss: 2.2160
Profiling... [6656/50176]	Loss: 2.1991
Profile done
epoch 1 train time consumed: 7.39s
Validation Epoch: 5, Average loss: 0.0044, Accuracy: 0.3969
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.74411536058834,
                        "time": 5.089894967999953,
                        "accuracy": 0.396875,
                        "total_cost": 1122949.4228286152
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.3007
Profiling... [1024/50176]	Loss: 2.3638
Profiling... [1536/50176]	Loss: 2.3403
Profiling... [2048/50176]	Loss: 2.2499
Profiling... [2560/50176]	Loss: 2.3337
Profiling... [3072/50176]	Loss: 2.3363
Profiling... [3584/50176]	Loss: 2.2475
Profiling... [4096/50176]	Loss: 2.3134
Profiling... [4608/50176]	Loss: 2.3342
Profiling... [5120/50176]	Loss: 2.3304
Profiling... [5632/50176]	Loss: 2.2358
Profiling... [6144/50176]	Loss: 2.4397
Profiling... [6656/50176]	Loss: 2.2209
Profile done
epoch 1 train time consumed: 12.99s
Validation Epoch: 5, Average loss: 0.0044, Accuracy: 0.3996
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.65145519094928,
                        "time": 9.131428730999687,
                        "accuracy": 0.399609375,
                        "total_cost": 2000819.6963104054
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.4347
Profiling... [1024/50176]	Loss: 2.3406
Profiling... [1536/50176]	Loss: 2.2718
Profiling... [2048/50176]	Loss: 2.3087
Profiling... [2560/50176]	Loss: 2.3135
Profiling... [3072/50176]	Loss: 2.2614
Profiling... [3584/50176]	Loss: 2.2286
Profiling... [4096/50176]	Loss: 2.4298
Profiling... [4608/50176]	Loss: 2.3635
Profiling... [5120/50176]	Loss: 2.3308
Profiling... [5632/50176]	Loss: 2.3108
Profiling... [6144/50176]	Loss: 2.3087
Profiling... [6656/50176]	Loss: 2.3331
Profile done
epoch 1 train time consumed: 6.78s
Validation Epoch: 5, Average loss: 0.0047, Accuracy: 0.3704
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.60736640473037,
                        "time": 4.591506269000092,
                        "accuracy": 0.37041015625,
                        "total_cost": 1085368.1540321994
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.3994
Profiling... [1024/50176]	Loss: 2.2424
Profiling... [1536/50176]	Loss: 2.3079
Profiling... [2048/50176]	Loss: 2.4787
Profiling... [2560/50176]	Loss: 2.3002
Profiling... [3072/50176]	Loss: 2.3382
Profiling... [3584/50176]	Loss: 2.4273
Profiling... [4096/50176]	Loss: 2.3613
Profiling... [4608/50176]	Loss: 2.2663
Profiling... [5120/50176]	Loss: 2.2763
Profiling... [5632/50176]	Loss: 2.3150
Profiling... [6144/50176]	Loss: 2.3568
Profiling... [6656/50176]	Loss: 2.4285
Profile done
epoch 1 train time consumed: 6.81s
Validation Epoch: 5, Average loss: 0.0048, Accuracy: 0.3564
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.668016110389,
                        "time": 4.6215035699997316,
                        "accuracy": 0.3564453125,
                        "total_cost": 1135259.9438569164
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.4069
Profiling... [1024/50176]	Loss: 2.3553
Profiling... [1536/50176]	Loss: 2.5236
Profiling... [2048/50176]	Loss: 2.3257
Profiling... [2560/50176]	Loss: 2.4054
Profiling... [3072/50176]	Loss: 2.3973
Profiling... [3584/50176]	Loss: 2.2593
Profiling... [4096/50176]	Loss: 2.4903
Profiling... [4608/50176]	Loss: 2.3004
Profiling... [5120/50176]	Loss: 2.2858
Profiling... [5632/50176]	Loss: 2.3527
Profiling... [6144/50176]	Loss: 2.2077
Profiling... [6656/50176]	Loss: 2.3559
Profile done
epoch 1 train time consumed: 7.33s
Validation Epoch: 5, Average loss: 0.0045, Accuracy: 0.3868
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.67994771919314,
                        "time": 5.082563465000021,
                        "accuracy": 0.38681640625,
                        "total_cost": 1150490.0941682516
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.4885
Profiling... [1024/50176]	Loss: 2.2867
Profiling... [1536/50176]	Loss: 2.3431
Profiling... [2048/50176]	Loss: 2.3642
Profiling... [2560/50176]	Loss: 2.4353
Profiling... [3072/50176]	Loss: 2.2268
Profiling... [3584/50176]	Loss: 2.1311
Profiling... [4096/50176]	Loss: 2.3955
Profiling... [4608/50176]	Loss: 2.3112
Profiling... [5120/50176]	Loss: 2.2372
Profiling... [5632/50176]	Loss: 2.3345
Profiling... [6144/50176]	Loss: 2.3284
Profiling... [6656/50176]	Loss: 2.2191
Profile done
epoch 1 train time consumed: 13.08s
Validation Epoch: 5, Average loss: 0.0046, Accuracy: 0.3738
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.5952528317783,
                        "time": 9.127497461000075,
                        "accuracy": 0.373828125,
                        "total_cost": 2137885.774433883
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.4092
Profiling... [1024/50176]	Loss: 2.2780
Profiling... [1536/50176]	Loss: 2.2918
Profiling... [2048/50176]	Loss: 2.3664
Profiling... [2560/50176]	Loss: 2.3294
Profiling... [3072/50176]	Loss: 2.3147
Profiling... [3584/50176]	Loss: 2.3677
Profiling... [4096/50176]	Loss: 2.4094
Profiling... [4608/50176]	Loss: 2.3119
Profiling... [5120/50176]	Loss: 2.2368
Profiling... [5632/50176]	Loss: 2.3418
Profiling... [6144/50176]	Loss: 2.3879
Profiling... [6656/50176]	Loss: 2.2466
Profile done
epoch 1 train time consumed: 6.67s
Validation Epoch: 5, Average loss: 0.0046, Accuracy: 0.3695
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.54712384041831,
                        "time": 4.593447976999869,
                        "accuracy": 0.36953125,
                        "total_cost": 1088409.3421992566
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.4343
Profiling... [1024/50176]	Loss: 2.3494
Profiling... [1536/50176]	Loss: 2.3822
Profiling... [2048/50176]	Loss: 2.3138
Profiling... [2560/50176]	Loss: 2.2887
Profiling... [3072/50176]	Loss: 2.3972
Profiling... [3584/50176]	Loss: 2.4024
Profiling... [4096/50176]	Loss: 2.2780
Profiling... [4608/50176]	Loss: 2.3408
Profiling... [5120/50176]	Loss: 2.3041
Profiling... [5632/50176]	Loss: 2.3613
Profiling... [6144/50176]	Loss: 2.3603
Profiling... [6656/50176]	Loss: 2.2421
Profile done
epoch 1 train time consumed: 6.79s
Validation Epoch: 5, Average loss: 0.0049, Accuracy: 0.3512
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.61105587781731,
                        "time": 4.62500241600037,
                        "accuracy": 0.351171875,
                        "total_cost": 1153179.7986704945
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.4323
Profiling... [1024/50176]	Loss: 2.3812
Profiling... [1536/50176]	Loss: 2.1999
Profiling... [2048/50176]	Loss: 2.3469
Profiling... [2560/50176]	Loss: 2.2968
Profiling... [3072/50176]	Loss: 2.4144
Profiling... [3584/50176]	Loss: 2.4347
Profiling... [4096/50176]	Loss: 2.3311
Profiling... [4608/50176]	Loss: 2.3801
Profiling... [5120/50176]	Loss: 2.2237
Profiling... [5632/50176]	Loss: 2.2835
Profiling... [6144/50176]	Loss: 2.1598
Profiling... [6656/50176]	Loss: 2.1831
Profile done
epoch 1 train time consumed: 7.34s
Validation Epoch: 5, Average loss: 0.0048, Accuracy: 0.3609
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.62428631636735,
                        "time": 5.056809719999819,
                        "accuracy": 0.3609375,
                        "total_cost": 1226731.246065816
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.3978
Profiling... [1024/50176]	Loss: 2.2939
Profiling... [1536/50176]	Loss: 2.5072
Profiling... [2048/50176]	Loss: 2.2800
Profiling... [2560/50176]	Loss: 2.1933
Profiling... [3072/50176]	Loss: 2.4104
Profiling... [3584/50176]	Loss: 2.1569
Profiling... [4096/50176]	Loss: 2.4575
Profiling... [4608/50176]	Loss: 2.3893
Profiling... [5120/50176]	Loss: 2.2878
Profiling... [5632/50176]	Loss: 2.3940
Profiling... [6144/50176]	Loss: 2.3532
Profiling... [6656/50176]	Loss: 2.2483
Profile done
epoch 1 train time consumed: 14.35s
Validation Epoch: 5, Average loss: 0.0046, Accuracy: 0.3717
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.52833784149044,
                        "time": 10.391687814000306,
                        "accuracy": 0.3716796875,
                        "total_cost": 2448058.81222905
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.3054
Profiling... [1024/50176]	Loss: 2.3321
Profiling... [1536/50176]	Loss: 2.3495
Profiling... [2048/50176]	Loss: 2.3201
Profiling... [2560/50176]	Loss: 2.3073
Profiling... [3072/50176]	Loss: 2.5282
Profiling... [3584/50176]	Loss: 2.3297
Profiling... [4096/50176]	Loss: 2.2449
Profiling... [4608/50176]	Loss: 2.3043
Profiling... [5120/50176]	Loss: 2.3696
Profiling... [5632/50176]	Loss: 2.3558
Profiling... [6144/50176]	Loss: 2.2461
Profiling... [6656/50176]	Loss: 2.3640
Profile done
epoch 1 train time consumed: 6.76s
Validation Epoch: 5, Average loss: 0.0046, Accuracy: 0.3699
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.48303378744946,
                        "time": 4.580892151999706,
                        "accuracy": 0.369921875,
                        "total_cost": 1084287.6830299946
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.4053
Profiling... [1024/50176]	Loss: 2.4505
Profiling... [1536/50176]	Loss: 2.2214
Profiling... [2048/50176]	Loss: 2.2622
Profiling... [2560/50176]	Loss: 2.3812
Profiling... [3072/50176]	Loss: 2.2151
Profiling... [3584/50176]	Loss: 2.4200
Profiling... [4096/50176]	Loss: 2.2904
Profiling... [4608/50176]	Loss: 2.4040
Profiling... [5120/50176]	Loss: 2.4138
Profiling... [5632/50176]	Loss: 2.4162
Profiling... [6144/50176]	Loss: 2.1754
Profiling... [6656/50176]	Loss: 2.2396
Profile done
epoch 1 train time consumed: 6.80s
Validation Epoch: 5, Average loss: 0.0047, Accuracy: 0.3635
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.54339538316603,
                        "time": 4.610827253000025,
                        "accuracy": 0.3634765625,
                        "total_cost": 1110726.311576628
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.2725
Profiling... [1024/50176]	Loss: 2.3546
Profiling... [1536/50176]	Loss: 2.3682
Profiling... [2048/50176]	Loss: 2.3477
Profiling... [2560/50176]	Loss: 2.3190
Profiling... [3072/50176]	Loss: 2.2480
Profiling... [3584/50176]	Loss: 2.3743
Profiling... [4096/50176]	Loss: 2.2344
Profiling... [4608/50176]	Loss: 2.4541
Profiling... [5120/50176]	Loss: 2.4110
Profiling... [5632/50176]	Loss: 2.3619
Profiling... [6144/50176]	Loss: 2.3007
Profiling... [6656/50176]	Loss: 2.3228
Profile done
epoch 1 train time consumed: 7.30s
Validation Epoch: 5, Average loss: 0.0049, Accuracy: 0.3487
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.55856946016449,
                        "time": 5.093941364999864,
                        "accuracy": 0.34873046875,
                        "total_cost": 1278994.5862989489
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.4522
Profiling... [1024/50176]	Loss: 2.4068
Profiling... [1536/50176]	Loss: 2.2623
Profiling... [2048/50176]	Loss: 2.3827
Profiling... [2560/50176]	Loss: 2.3610
Profiling... [3072/50176]	Loss: 2.3919
Profiling... [3584/50176]	Loss: 2.3204
Profiling... [4096/50176]	Loss: 2.2777
Profiling... [4608/50176]	Loss: 2.3778
Profiling... [5120/50176]	Loss: 2.3088
Profiling... [5632/50176]	Loss: 2.3984
Profiling... [6144/50176]	Loss: 2.3725
Profiling... [6656/50176]	Loss: 2.3391
Profile done
epoch 1 train time consumed: 13.75s
Validation Epoch: 5, Average loss: 0.0048, Accuracy: 0.3555
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.46806169021251,
                        "time": 9.816238950000297,
                        "accuracy": 0.35546875,
                        "total_cost": 2417954.5224305885
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.3475
Profiling... [1024/50176]	Loss: 2.5203
Profiling... [1536/50176]	Loss: 2.5565
Profiling... [2048/50176]	Loss: 2.5215
Profiling... [2560/50176]	Loss: 2.5773
Profiling... [3072/50176]	Loss: 2.4494
Profiling... [3584/50176]	Loss: 2.5050
Profiling... [4096/50176]	Loss: 2.5572
Profiling... [4608/50176]	Loss: 2.5172
Profiling... [5120/50176]	Loss: 2.3914
Profiling... [5632/50176]	Loss: 2.4916
Profiling... [6144/50176]	Loss: 2.2760
Profiling... [6656/50176]	Loss: 2.5363
Profile done
epoch 1 train time consumed: 6.86s
Validation Epoch: 5, Average loss: 0.0056, Accuracy: 0.2982
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.42548336061266,
                        "time": 4.597675804000119,
                        "accuracy": 0.2982421875,
                        "total_cost": 1349812.9692923625
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.3606
Profiling... [1024/50176]	Loss: 2.4444
Profiling... [1536/50176]	Loss: 2.5065
Profiling... [2048/50176]	Loss: 2.4942
Profiling... [2560/50176]	Loss: 2.3864
Profiling... [3072/50176]	Loss: 2.4570
Profiling... [3584/50176]	Loss: 2.4171
Profiling... [4096/50176]	Loss: 2.2713
Profiling... [4608/50176]	Loss: 2.5245
Profiling... [5120/50176]	Loss: 2.5197
Profiling... [5632/50176]	Loss: 2.3378
Profiling... [6144/50176]	Loss: 2.4283
Profiling... [6656/50176]	Loss: 2.5552
Profile done
epoch 1 train time consumed: 6.73s
Validation Epoch: 5, Average loss: 0.0057, Accuracy: 0.2951
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.48172695397787,
                        "time": 4.609713412000019,
                        "accuracy": 0.2951171875,
                        "total_cost": 1367678.091638143
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.4929
Profiling... [1024/50176]	Loss: 2.4474
Profiling... [1536/50176]	Loss: 2.3039
Profiling... [2048/50176]	Loss: 2.5792
Profiling... [2560/50176]	Loss: 2.5574
Profiling... [3072/50176]	Loss: 2.5479
Profiling... [3584/50176]	Loss: 2.4087
Profiling... [4096/50176]	Loss: 2.5457
Profiling... [4608/50176]	Loss: 2.5043
Profiling... [5120/50176]	Loss: 2.4010
Profiling... [5632/50176]	Loss: 2.4201
Profiling... [6144/50176]	Loss: 2.4367
Profiling... [6656/50176]	Loss: 2.4753
Profile done
epoch 1 train time consumed: 7.30s
Validation Epoch: 5, Average loss: 0.0075, Accuracy: 0.2028
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.49795993336555,
                        "time": 5.098799976000009,
                        "accuracy": 0.20283203125,
                        "total_cost": 2201080.5849858634
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.4121
Profiling... [1024/50176]	Loss: 2.4515
Profiling... [1536/50176]	Loss: 2.3769
Profiling... [2048/50176]	Loss: 2.4550
Profiling... [2560/50176]	Loss: 2.5021
Profiling... [3072/50176]	Loss: 2.3301
Profiling... [3584/50176]	Loss: 2.4986
Profiling... [4096/50176]	Loss: 2.5206
Profiling... [4608/50176]	Loss: 2.3978
Profiling... [5120/50176]	Loss: 2.5659
Profiling... [5632/50176]	Loss: 2.3634
Profiling... [6144/50176]	Loss: 2.3691
Profiling... [6656/50176]	Loss: 2.5689
Profile done
epoch 1 train time consumed: 13.60s
Validation Epoch: 5, Average loss: 0.0058, Accuracy: 0.2790
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.41041397537752,
                        "time": 9.764238148000004,
                        "accuracy": 0.27900390625,
                        "total_cost": 3064307.6840781826
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.4705
Profiling... [1024/50176]	Loss: 2.4819
Profiling... [1536/50176]	Loss: 2.4932
Profiling... [2048/50176]	Loss: 2.5259
Profiling... [2560/50176]	Loss: 2.5805
Profiling... [3072/50176]	Loss: 2.5727
Profiling... [3584/50176]	Loss: 2.4452
Profiling... [4096/50176]	Loss: 2.3615
Profiling... [4608/50176]	Loss: 2.4415
Profiling... [5120/50176]	Loss: 2.3772
Profiling... [5632/50176]	Loss: 2.6088
Profiling... [6144/50176]	Loss: 2.4305
Profiling... [6656/50176]	Loss: 2.4172
Profile done
epoch 1 train time consumed: 6.76s
Validation Epoch: 5, Average loss: 0.0063, Accuracy: 0.2611
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.36720164676943,
                        "time": 4.585008916000334,
                        "accuracy": 0.2611328125,
                        "total_cost": 1537385.9996699705
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.4477
Profiling... [1024/50176]	Loss: 2.5348
Profiling... [1536/50176]	Loss: 2.5011
Profiling... [2048/50176]	Loss: 2.6636
Profiling... [2560/50176]	Loss: 2.5492
Profiling... [3072/50176]	Loss: 2.3973
Profiling... [3584/50176]	Loss: 2.4019
Profiling... [4096/50176]	Loss: 2.4507
Profiling... [4608/50176]	Loss: 2.4083
Profiling... [5120/50176]	Loss: 2.5215
Profiling... [5632/50176]	Loss: 2.5900
Profiling... [6144/50176]	Loss: 2.4623
Profiling... [6656/50176]	Loss: 2.4044
Profile done
epoch 1 train time consumed: 6.85s
Validation Epoch: 5, Average loss: 0.0058, Accuracy: 0.2750
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.42476554012323,
                        "time": 4.616191496999818,
                        "accuracy": 0.275,
                        "total_cost": 1469790.5446585193
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.3896
Profiling... [1024/50176]	Loss: 2.4836
Profiling... [1536/50176]	Loss: 2.4921
Profiling... [2048/50176]	Loss: 2.6713
Profiling... [2560/50176]	Loss: 2.4388
Profiling... [3072/50176]	Loss: 2.5332
Profiling... [3584/50176]	Loss: 2.5705
Profiling... [4096/50176]	Loss: 2.4217
Profiling... [4608/50176]	Loss: 2.3479
Profiling... [5120/50176]	Loss: 2.5581
Profiling... [5632/50176]	Loss: 2.4758
Profiling... [6144/50176]	Loss: 2.3885
Profiling... [6656/50176]	Loss: 2.4904
Profile done
epoch 1 train time consumed: 7.41s
Validation Epoch: 5, Average loss: 0.0056, Accuracy: 0.2913
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.44066067141256,
                        "time": 5.071253819000049,
                        "accuracy": 0.29130859375,
                        "total_cost": 1524285.8454662089
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.4078
Profiling... [1024/50176]	Loss: 2.4946
Profiling... [1536/50176]	Loss: 2.4417
Profiling... [2048/50176]	Loss: 2.5072
Profiling... [2560/50176]	Loss: 2.5643
Profiling... [3072/50176]	Loss: 2.4939
Profiling... [3584/50176]	Loss: 2.4568
Profiling... [4096/50176]	Loss: 2.5550
Profiling... [4608/50176]	Loss: 2.5952
Profiling... [5120/50176]	Loss: 2.5063
Profiling... [5632/50176]	Loss: 2.5518
Profiling... [6144/50176]	Loss: 2.6077
Profiling... [6656/50176]	Loss: 2.5504
Profile done
epoch 1 train time consumed: 12.37s
Validation Epoch: 5, Average loss: 0.0058, Accuracy: 0.2759
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.36449006811795,
                        "time": 8.398249917000157,
                        "accuracy": 0.27587890625,
                        "total_cost": 2665474.153651127
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.3709
Profiling... [1024/50176]	Loss: 2.4814
Profiling... [1536/50176]	Loss: 2.4759
Profiling... [2048/50176]	Loss: 2.3725
Profiling... [2560/50176]	Loss: 2.5094
Profiling... [3072/50176]	Loss: 2.5817
Profiling... [3584/50176]	Loss: 2.6013
Profiling... [4096/50176]	Loss: 2.4237
Profiling... [4608/50176]	Loss: 2.4789
Profiling... [5120/50176]	Loss: 2.6350
Profiling... [5632/50176]	Loss: 2.4137
Profiling... [6144/50176]	Loss: 2.3649
Profiling... [6656/50176]	Loss: 2.3907
Profile done
epoch 1 train time consumed: 6.80s
Validation Epoch: 5, Average loss: 0.0052, Accuracy: 0.3164
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.3270199635445,
                        "time": 4.586225207000098,
                        "accuracy": 0.31640625,
                        "total_cost": 1269154.2468137878
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.3502
Profiling... [1024/50176]	Loss: 2.6033
Profiling... [1536/50176]	Loss: 2.4909
Profiling... [2048/50176]	Loss: 2.5951
Profiling... [2560/50176]	Loss: 2.4241
Profiling... [3072/50176]	Loss: 2.4327
Profiling... [3584/50176]	Loss: 2.4728
Profiling... [4096/50176]	Loss: 2.3861
Profiling... [4608/50176]	Loss: 2.4680
Profiling... [5120/50176]	Loss: 2.4927
Profiling... [5632/50176]	Loss: 2.3085
Profiling... [6144/50176]	Loss: 2.5023
Profiling... [6656/50176]	Loss: 2.5889
Profile done
epoch 1 train time consumed: 6.84s
Validation Epoch: 5, Average loss: 0.0060, Accuracy: 0.2801
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.38209427789698,
                        "time": 4.605168762000176,
                        "accuracy": 0.280078125,
                        "total_cost": 1439695.2779538592
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.3491
Profiling... [1024/50176]	Loss: 2.4417
Profiling... [1536/50176]	Loss: 2.6337
Profiling... [2048/50176]	Loss: 2.3906
Profiling... [2560/50176]	Loss: 2.4357
Profiling... [3072/50176]	Loss: 2.4997
Profiling... [3584/50176]	Loss: 2.5864
Profiling... [4096/50176]	Loss: 2.5684
Profiling... [4608/50176]	Loss: 2.5058
Profiling... [5120/50176]	Loss: 2.5124
Profiling... [5632/50176]	Loss: 2.4738
Profiling... [6144/50176]	Loss: 2.4011
Profiling... [6656/50176]	Loss: 2.4994
Profile done
epoch 1 train time consumed: 7.35s
Validation Epoch: 5, Average loss: 0.0052, Accuracy: 0.3256
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.39826209321387,
                        "time": 5.075025568000001,
                        "accuracy": 0.3255859375,
                        "total_cost": 1364824.6457709773
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.4100
Profiling... [1024/50176]	Loss: 2.3767
Profiling... [1536/50176]	Loss: 2.5425
Profiling... [2048/50176]	Loss: 2.3865
Profiling... [2560/50176]	Loss: 2.5309
Profiling... [3072/50176]	Loss: 2.5149
Profiling... [3584/50176]	Loss: 2.4892
Profiling... [4096/50176]	Loss: 2.4701
Profiling... [4608/50176]	Loss: 2.4443
Profiling... [5120/50176]	Loss: 2.5667
Profiling... [5632/50176]	Loss: 2.3817
Profiling... [6144/50176]	Loss: 2.2486
Profiling... [6656/50176]	Loss: 2.4688
Profile done
epoch 1 train time consumed: 13.71s
Validation Epoch: 5, Average loss: 0.0059, Accuracy: 0.2702
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.31748550065652,
                        "time": 9.654553034999935,
                        "accuracy": 0.27021484375,
                        "total_cost": 3128434.2389439098
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.3670
Profiling... [2048/50176]	Loss: 2.4154
Profiling... [3072/50176]	Loss: 2.3363
Profiling... [4096/50176]	Loss: 2.3355
Profiling... [5120/50176]	Loss: 2.3112
Profiling... [6144/50176]	Loss: 2.3460
Profiling... [7168/50176]	Loss: 2.3392
Profiling... [8192/50176]	Loss: 2.2174
Profiling... [9216/50176]	Loss: 2.2434
Profiling... [10240/50176]	Loss: 2.2524
Profiling... [11264/50176]	Loss: 2.1413
Profiling... [12288/50176]	Loss: 2.2935
Profiling... [13312/50176]	Loss: 2.2618
Profile done
epoch 1 train time consumed: 12.81s
Validation Epoch: 5, Average loss: 0.0022, Accuracy: 0.4054
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.35537883777177,
                        "time": 8.94864874699988,
                        "accuracy": 0.40537109375,
                        "total_cost": 1932897.5650305147
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.3663
Profiling... [2048/50176]	Loss: 2.4442
Profiling... [3072/50176]	Loss: 2.2905
Profiling... [4096/50176]	Loss: 2.3258
Profiling... [5120/50176]	Loss: 2.3058
Profiling... [6144/50176]	Loss: 2.2459
Profiling... [7168/50176]	Loss: 2.2305
Profiling... [8192/50176]	Loss: 2.3294
Profiling... [9216/50176]	Loss: 2.2510
Profiling... [10240/50176]	Loss: 2.3393
Profiling... [11264/50176]	Loss: 2.3805
Profiling... [12288/50176]	Loss: 2.2055
Profiling... [13312/50176]	Loss: 2.2014
Profile done
epoch 1 train time consumed: 12.94s
Validation Epoch: 5, Average loss: 0.0022, Accuracy: 0.4044
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.45361633953726,
                        "time": 9.04711565200023,
                        "accuracy": 0.40439453125,
                        "total_cost": 1958886.4677334994
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.3880
Profiling... [2048/50176]	Loss: 2.4162
Profiling... [3072/50176]	Loss: 2.4411
Profiling... [4096/50176]	Loss: 2.2668
Profiling... [5120/50176]	Loss: 2.3994
Profiling... [6144/50176]	Loss: 2.3159
Profiling... [7168/50176]	Loss: 2.2549
Profiling... [8192/50176]	Loss: 2.2861
Profiling... [9216/50176]	Loss: 2.2048
Profiling... [10240/50176]	Loss: 2.2341
Profiling... [11264/50176]	Loss: 2.2621
Profiling... [12288/50176]	Loss: 2.2319
Profiling... [13312/50176]	Loss: 2.1743
Profile done
epoch 1 train time consumed: 14.11s
Validation Epoch: 5, Average loss: 0.0022, Accuracy: 0.4025
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.46722557654867,
                        "time": 9.98591111199994,
                        "accuracy": 0.4025390625,
                        "total_cost": 2172121.4120622072
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.3676
Profiling... [2048/50176]	Loss: 2.3451
Profiling... [3072/50176]	Loss: 2.3605
Profiling... [4096/50176]	Loss: 2.3352
Profiling... [5120/50176]	Loss: 2.2527
Profiling... [6144/50176]	Loss: 2.3482
Profiling... [7168/50176]	Loss: 2.2734
Profiling... [8192/50176]	Loss: 2.3044
Profiling... [9216/50176]	Loss: 2.3033
Profiling... [10240/50176]	Loss: 2.2728
Profiling... [11264/50176]	Loss: 2.2664
Profiling... [12288/50176]	Loss: 2.2668
Profiling... [13312/50176]	Loss: 2.1987
Profile done
epoch 1 train time consumed: 25.36s
Validation Epoch: 5, Average loss: 0.0022, Accuracy: 0.4043
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.25535040653257,
                        "time": 18.669966521000333,
                        "accuracy": 0.404296875,
                        "total_cost": 4043403.296865204
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.4231
Profiling... [2048/50176]	Loss: 2.4052
Profiling... [3072/50176]	Loss: 2.3913
Profiling... [4096/50176]	Loss: 2.3411
Profiling... [5120/50176]	Loss: 2.2700
Profiling... [6144/50176]	Loss: 2.2784
Profiling... [7168/50176]	Loss: 2.2850
Profiling... [8192/50176]	Loss: 2.3126
Profiling... [9216/50176]	Loss: 2.2562
Profiling... [10240/50176]	Loss: 2.1882
Profiling... [11264/50176]	Loss: 2.3674
Profiling... [12288/50176]	Loss: 2.2874
Profiling... [13312/50176]	Loss: 2.1920
Profile done
epoch 1 train time consumed: 12.73s
Validation Epoch: 5, Average loss: 0.0022, Accuracy: 0.3985
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.29670349112607,
                        "time": 8.945482871999957,
                        "accuracy": 0.39853515625,
                        "total_cost": 1965355.6839977757
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.4002
Profiling... [2048/50176]	Loss: 2.3387
Profiling... [3072/50176]	Loss: 2.3100
Profiling... [4096/50176]	Loss: 2.3642
Profiling... [5120/50176]	Loss: 2.2840
Profiling... [6144/50176]	Loss: 2.2747
Profiling... [7168/50176]	Loss: 2.2409
Profiling... [8192/50176]	Loss: 2.2417
Profiling... [9216/50176]	Loss: 2.2311
Profiling... [10240/50176]	Loss: 2.1876
Profiling... [11264/50176]	Loss: 2.2531
Profiling... [12288/50176]	Loss: 2.2922
Profiling... [13312/50176]	Loss: 2.2387
Profile done
epoch 1 train time consumed: 12.98s
Validation Epoch: 5, Average loss: 0.0022, Accuracy: 0.4054
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.3838108686859,
                        "time": 9.012196041000152,
                        "accuracy": 0.40537109375,
                        "total_cost": 1946624.021563818
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.3370
Profiling... [2048/50176]	Loss: 2.4263
Profiling... [3072/50176]	Loss: 2.2971
Profiling... [4096/50176]	Loss: 2.3582
Profiling... [5120/50176]	Loss: 2.2799
Profiling... [6144/50176]	Loss: 2.4114
Profiling... [7168/50176]	Loss: 2.2395
Profiling... [8192/50176]	Loss: 2.2859
Profiling... [9216/50176]	Loss: 2.2458
Profiling... [10240/50176]	Loss: 2.2452
Profiling... [11264/50176]	Loss: 2.3138
Profiling... [12288/50176]	Loss: 2.1446
Profiling... [13312/50176]	Loss: 2.2020
Profile done
epoch 1 train time consumed: 14.15s
Validation Epoch: 5, Average loss: 0.0022, Accuracy: 0.4026
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.39663483707983,
                        "time": 9.93894998199994,
                        "accuracy": 0.40263671875,
                        "total_cost": 2161381.268746561
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.3570
Profiling... [2048/50176]	Loss: 2.4499
Profiling... [3072/50176]	Loss: 2.3653
Profiling... [4096/50176]	Loss: 2.3690
Profiling... [5120/50176]	Loss: 2.2439
Profiling... [6144/50176]	Loss: 2.2899
Profiling... [7168/50176]	Loss: 2.2489
Profiling... [8192/50176]	Loss: 2.3501
Profiling... [9216/50176]	Loss: 2.2790
Profiling... [10240/50176]	Loss: 2.1963
Profiling... [11264/50176]	Loss: 2.2011
Profiling... [12288/50176]	Loss: 2.2210
Profiling... [13312/50176]	Loss: 2.2389
Profile done
epoch 1 train time consumed: 25.55s
Validation Epoch: 5, Average loss: 0.0022, Accuracy: 0.4024
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.19121029264608,
                        "time": 18.77730435100011,
                        "accuracy": 0.40244140625,
                        "total_cost": 4085397.650427091
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.3563
Profiling... [2048/50176]	Loss: 2.4005
Profiling... [3072/50176]	Loss: 2.2820
Profiling... [4096/50176]	Loss: 2.3058
Profiling... [5120/50176]	Loss: 2.3603
Profiling... [6144/50176]	Loss: 2.3111
Profiling... [7168/50176]	Loss: 2.2632
Profiling... [8192/50176]	Loss: 2.1978
Profiling... [9216/50176]	Loss: 2.2221
Profiling... [10240/50176]	Loss: 2.2426
Profiling... [11264/50176]	Loss: 2.1883
Profiling... [12288/50176]	Loss: 2.2536
Profiling... [13312/50176]	Loss: 2.2064
Profile done
epoch 1 train time consumed: 12.77s
Validation Epoch: 5, Average loss: 0.0022, Accuracy: 0.4031
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.22644424798871,
                        "time": 8.963525629000287,
                        "accuracy": 0.403125,
                        "total_cost": 1946896.9604511275
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.4472
Profiling... [2048/50176]	Loss: 2.3370
Profiling... [3072/50176]	Loss: 2.3309
Profiling... [4096/50176]	Loss: 2.2925
Profiling... [5120/50176]	Loss: 2.2673
Profiling... [6144/50176]	Loss: 2.3462
Profiling... [7168/50176]	Loss: 2.3137
Profiling... [8192/50176]	Loss: 2.2490
Profiling... [9216/50176]	Loss: 2.2023
Profiling... [10240/50176]	Loss: 2.2996
Profiling... [11264/50176]	Loss: 2.3042
Profiling... [12288/50176]	Loss: 2.1878
Profiling... [13312/50176]	Loss: 2.3092
Profile done
epoch 1 train time consumed: 12.94s
Validation Epoch: 5, Average loss: 0.0022, Accuracy: 0.4074
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.31972248627588,
                        "time": 9.031919627999741,
                        "accuracy": 0.407421875,
                        "total_cost": 1941063.6960073463
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.3952
Profiling... [2048/50176]	Loss: 2.3242
Profiling... [3072/50176]	Loss: 2.3922
Profiling... [4096/50176]	Loss: 2.3898
Profiling... [5120/50176]	Loss: 2.2708
Profiling... [6144/50176]	Loss: 2.2593
Profiling... [7168/50176]	Loss: 2.1646
Profiling... [8192/50176]	Loss: 2.3222
Profiling... [9216/50176]	Loss: 2.2578
Profiling... [10240/50176]	Loss: 2.2514
Profiling... [11264/50176]	Loss: 2.2902
Profiling... [12288/50176]	Loss: 2.2096
Profiling... [13312/50176]	Loss: 2.2293
Profile done
epoch 1 train time consumed: 14.06s
Validation Epoch: 5, Average loss: 0.0022, Accuracy: 0.4017
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.32736669586204,
                        "time": 9.971613358000468,
                        "accuracy": 0.40166015625,
                        "total_cost": 2173755.8441406908
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.3686
Profiling... [2048/50176]	Loss: 2.4301
Profiling... [3072/50176]	Loss: 2.2903
Profiling... [4096/50176]	Loss: 2.2996
Profiling... [5120/50176]	Loss: 2.2767
Profiling... [6144/50176]	Loss: 2.3042
Profiling... [7168/50176]	Loss: 2.3134
Profiling... [8192/50176]	Loss: 2.2010
Profiling... [9216/50176]	Loss: 2.3141
Profiling... [10240/50176]	Loss: 2.2809
Profiling... [11264/50176]	Loss: 2.2676
Profiling... [12288/50176]	Loss: 2.2407
Profiling... [13312/50176]	Loss: 2.2725
Profile done
epoch 1 train time consumed: 25.42s
Validation Epoch: 5, Average loss: 0.0022, Accuracy: 0.4008
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.12292891567982,
                        "time": 18.740412246000233,
                        "accuracy": 0.40078125,
                        "total_cost": 4094259.095012394
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.3976
Profiling... [2048/50176]	Loss: 2.4124
Profiling... [3072/50176]	Loss: 2.3159
Profiling... [4096/50176]	Loss: 2.3327
Profiling... [5120/50176]	Loss: 2.2954
Profiling... [6144/50176]	Loss: 2.3614
Profiling... [7168/50176]	Loss: 2.1946
Profiling... [8192/50176]	Loss: 2.3185
Profiling... [9216/50176]	Loss: 2.2886
Profiling... [10240/50176]	Loss: 2.2288
Profiling... [11264/50176]	Loss: 2.3513
Profiling... [12288/50176]	Loss: 2.2397
Profiling... [13312/50176]	Loss: 2.2247
Profile done
epoch 1 train time consumed: 12.80s
Validation Epoch: 5, Average loss: 0.0023, Accuracy: 0.3872
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.15915139720619,
                        "time": 8.932180361000064,
                        "accuracy": 0.38720703125,
                        "total_cost": 2019844.408761566
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.4053
Profiling... [2048/50176]	Loss: 2.3156
Profiling... [3072/50176]	Loss: 2.2275
Profiling... [4096/50176]	Loss: 2.2511
Profiling... [5120/50176]	Loss: 2.3405
Profiling... [6144/50176]	Loss: 2.3641
Profiling... [7168/50176]	Loss: 2.2227
Profiling... [8192/50176]	Loss: 2.2245
Profiling... [9216/50176]	Loss: 2.2736
Profiling... [10240/50176]	Loss: 2.2330
Profiling... [11264/50176]	Loss: 2.1901
Profiling... [12288/50176]	Loss: 2.3005
Profiling... [13312/50176]	Loss: 2.2599
Profile done
epoch 1 train time consumed: 12.90s
Validation Epoch: 5, Average loss: 0.0022, Accuracy: 0.3892
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.25273704492052,
                        "time": 9.028545574000418,
                        "accuracy": 0.38916015625,
                        "total_cost": 2031390.0701663976
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.3979
Profiling... [2048/50176]	Loss: 2.3079
Profiling... [3072/50176]	Loss: 2.3433
Profiling... [4096/50176]	Loss: 2.2790
Profiling... [5120/50176]	Loss: 2.2580
Profiling... [6144/50176]	Loss: 2.2860
Profiling... [7168/50176]	Loss: 2.1585
Profiling... [8192/50176]	Loss: 2.3122
Profiling... [9216/50176]	Loss: 2.2333
Profiling... [10240/50176]	Loss: 2.3568
Profiling... [11264/50176]	Loss: 2.1923
Profiling... [12288/50176]	Loss: 2.1913
Profiling... [13312/50176]	Loss: 2.2835
Profile done
epoch 1 train time consumed: 14.12s
Validation Epoch: 5, Average loss: 0.0023, Accuracy: 0.3804
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.26437224484482,
                        "time": 9.971789840999918,
                        "accuracy": 0.38037109375,
                        "total_cost": 2295459.000596743
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.3505
Profiling... [2048/50176]	Loss: 2.3900
Profiling... [3072/50176]	Loss: 2.3390
Profiling... [4096/50176]	Loss: 2.2631
Profiling... [5120/50176]	Loss: 2.3171
Profiling... [6144/50176]	Loss: 2.3012
Profiling... [7168/50176]	Loss: 2.2761
Profiling... [8192/50176]	Loss: 2.2249
Profiling... [9216/50176]	Loss: 2.2726
Profiling... [10240/50176]	Loss: 2.2145
Profiling... [11264/50176]	Loss: 2.2838
Profiling... [12288/50176]	Loss: 2.2841
Profiling... [13312/50176]	Loss: 2.2322
Profile done
epoch 1 train time consumed: 25.48s
Validation Epoch: 5, Average loss: 0.0022, Accuracy: 0.3911
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.06922183903282,
                        "time": 18.775840375999906,
                        "accuracy": 0.39111328125,
                        "total_cost": 4203395.599344567
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.3605
Profiling... [2048/50176]	Loss: 2.3638
Profiling... [3072/50176]	Loss: 2.3440
Profiling... [4096/50176]	Loss: 2.2992
Profiling... [5120/50176]	Loss: 2.2647
Profiling... [6144/50176]	Loss: 2.3587
Profiling... [7168/50176]	Loss: 2.2359
Profiling... [8192/50176]	Loss: 2.2042
Profiling... [9216/50176]	Loss: 2.2837
Profiling... [10240/50176]	Loss: 2.1748
Profiling... [11264/50176]	Loss: 2.2106
Profiling... [12288/50176]	Loss: 2.3620
Profiling... [13312/50176]	Loss: 2.2386
Profile done
epoch 1 train time consumed: 12.75s
Validation Epoch: 5, Average loss: 0.0023, Accuracy: 0.3790
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.10646252710174,
                        "time": 8.930360656999255,
                        "accuracy": 0.37900390625,
                        "total_cost": 2063140.7128693887
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.3717
Profiling... [2048/50176]	Loss: 2.3576
Profiling... [3072/50176]	Loss: 2.3299
Profiling... [4096/50176]	Loss: 2.3341
Profiling... [5120/50176]	Loss: 2.3354
Profiling... [6144/50176]	Loss: 2.1691
Profiling... [7168/50176]	Loss: 2.3139
Profiling... [8192/50176]	Loss: 2.2958
Profiling... [9216/50176]	Loss: 2.2922
Profiling... [10240/50176]	Loss: 2.2456
Profiling... [11264/50176]	Loss: 2.2662
Profiling... [12288/50176]	Loss: 2.2795
Profiling... [13312/50176]	Loss: 2.2993
Profile done
epoch 1 train time consumed: 12.89s
Validation Epoch: 5, Average loss: 0.0022, Accuracy: 0.4011
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.1910413109873,
                        "time": 9.012109977000364,
                        "accuracy": 0.40107421875,
                        "total_cost": 1967458.0600895723
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.4190
Profiling... [2048/50176]	Loss: 2.3264
Profiling... [3072/50176]	Loss: 2.3066
Profiling... [4096/50176]	Loss: 2.3156
Profiling... [5120/50176]	Loss: 2.2564
Profiling... [6144/50176]	Loss: 2.2819
Profiling... [7168/50176]	Loss: 2.2277
Profiling... [8192/50176]	Loss: 2.1982
Profiling... [9216/50176]	Loss: 2.2914
Profiling... [10240/50176]	Loss: 2.3002
Profiling... [11264/50176]	Loss: 2.2173
Profiling... [12288/50176]	Loss: 2.2825
Profiling... [13312/50176]	Loss: 2.2118
Profile done
epoch 1 train time consumed: 14.08s
Validation Epoch: 5, Average loss: 0.0023, Accuracy: 0.3743
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.20035574186578,
                        "time": 9.961740355000074,
                        "accuracy": 0.37431640625,
                        "total_cost": 2330237.1683302848
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.2806
Profiling... [2048/50176]	Loss: 2.3501
Profiling... [3072/50176]	Loss: 2.2860
Profiling... [4096/50176]	Loss: 2.3490
Profiling... [5120/50176]	Loss: 2.3085
Profiling... [6144/50176]	Loss: 2.3094
Profiling... [7168/50176]	Loss: 2.2971
Profiling... [8192/50176]	Loss: 2.2349
Profiling... [9216/50176]	Loss: 2.2363
Profiling... [10240/50176]	Loss: 2.1922
Profiling... [11264/50176]	Loss: 2.2041
Profiling... [12288/50176]	Loss: 2.2565
Profiling... [13312/50176]	Loss: 2.1672
Profile done
epoch 1 train time consumed: 25.44s
Validation Epoch: 5, Average loss: 0.0022, Accuracy: 0.3897
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.0078785267735,
                        "time": 18.760140315999706,
                        "accuracy": 0.38974609375,
                        "total_cost": 4214612.041637454
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.3016
Profiling... [2048/50176]	Loss: 2.2906
Profiling... [3072/50176]	Loss: 2.2778
Profiling... [4096/50176]	Loss: 2.2453
Profiling... [5120/50176]	Loss: 2.2767
Profiling... [6144/50176]	Loss: 2.3065
Profiling... [7168/50176]	Loss: 2.2568
Profiling... [8192/50176]	Loss: 2.3425
Profiling... [9216/50176]	Loss: 2.2406
Profiling... [10240/50176]	Loss: 2.2759
Profiling... [11264/50176]	Loss: 2.3097
Profiling... [12288/50176]	Loss: 2.3782
Profiling... [13312/50176]	Loss: 2.2466
Profile done
epoch 1 train time consumed: 12.90s
Validation Epoch: 5, Average loss: 0.0023, Accuracy: 0.3845
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.0440023113171,
                        "time": 8.943604978999247,
                        "accuracy": 0.38447265625,
                        "total_cost": 2036810.0674996434
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.4465
Profiling... [2048/50176]	Loss: 2.3089
Profiling... [3072/50176]	Loss: 2.2284
Profiling... [4096/50176]	Loss: 2.2029
Profiling... [5120/50176]	Loss: 2.2496
Profiling... [6144/50176]	Loss: 2.2351
Profiling... [7168/50176]	Loss: 2.2423
Profiling... [8192/50176]	Loss: 2.1886
Profiling... [9216/50176]	Loss: 2.2760
Profiling... [10240/50176]	Loss: 2.1708
Profiling... [11264/50176]	Loss: 2.2870
Profiling... [12288/50176]	Loss: 2.2064
Profiling... [13312/50176]	Loss: 2.3165
Profile done
epoch 1 train time consumed: 12.98s
Validation Epoch: 5, Average loss: 0.0023, Accuracy: 0.3843
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.12932892935957,
                        "time": 9.050575162999849,
                        "accuracy": 0.38427734375,
                        "total_cost": 2062219.9932526809
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.4463
Profiling... [2048/50176]	Loss: 2.3204
Profiling... [3072/50176]	Loss: 2.3404
Profiling... [4096/50176]	Loss: 2.3153
Profiling... [5120/50176]	Loss: 2.3461
Profiling... [6144/50176]	Loss: 2.2008
Profiling... [7168/50176]	Loss: 2.2871
Profiling... [8192/50176]	Loss: 2.2120
Profiling... [9216/50176]	Loss: 2.3831
Profiling... [10240/50176]	Loss: 2.2497
Profiling... [11264/50176]	Loss: 2.2457
Profiling... [12288/50176]	Loss: 2.3197
Profiling... [13312/50176]	Loss: 2.2501
Profile done
epoch 1 train time consumed: 14.17s
Validation Epoch: 5, Average loss: 0.0022, Accuracy: 0.3968
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.14142051735188,
                        "time": 10.026866646000599,
                        "accuracy": 0.39677734375,
                        "total_cost": 2212697.254322056
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.3936
Profiling... [2048/50176]	Loss: 2.2968
Profiling... [3072/50176]	Loss: 2.3439
Profiling... [4096/50176]	Loss: 2.3229
Profiling... [5120/50176]	Loss: 2.2902
Profiling... [6144/50176]	Loss: 2.2715
Profiling... [7168/50176]	Loss: 2.3499
Profiling... [8192/50176]	Loss: 2.2714
Profiling... [9216/50176]	Loss: 2.2698
Profiling... [10240/50176]	Loss: 2.1437
Profiling... [11264/50176]	Loss: 2.3196
Profiling... [12288/50176]	Loss: 2.2771
Profiling... [13312/50176]	Loss: 2.3047
Profile done
epoch 1 train time consumed: 25.58s
Validation Epoch: 5, Average loss: 0.0024, Accuracy: 0.3559
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.95798108812522,
                        "time": 18.82361505800054,
                        "accuracy": 0.355859375,
                        "total_cost": 4631565.283890827
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.3404
Profiling... [2048/50176]	Loss: 2.4827
Profiling... [3072/50176]	Loss: 2.4212
Profiling... [4096/50176]	Loss: 2.4750
Profiling... [5120/50176]	Loss: 2.4367
Profiling... [6144/50176]	Loss: 2.3544
Profiling... [7168/50176]	Loss: 2.4138
Profiling... [8192/50176]	Loss: 2.2738
Profiling... [9216/50176]	Loss: 2.3419
Profiling... [10240/50176]	Loss: 2.3212
Profiling... [11264/50176]	Loss: 2.3307
Profiling... [12288/50176]	Loss: 2.4136
Profiling... [13312/50176]	Loss: 2.3365
Profile done
epoch 1 train time consumed: 12.78s
Validation Epoch: 5, Average loss: 0.0025, Accuracy: 0.3292
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.99524827981847,
                        "time": 8.97053323900036,
                        "accuracy": 0.32919921875,
                        "total_cost": 2385957.6180343535
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.4361
Profiling... [2048/50176]	Loss: 2.5083
Profiling... [3072/50176]	Loss: 2.4096
Profiling... [4096/50176]	Loss: 2.4716
Profiling... [5120/50176]	Loss: 2.3346
Profiling... [6144/50176]	Loss: 2.3801
Profiling... [7168/50176]	Loss: 2.3689
Profiling... [8192/50176]	Loss: 2.4449
Profiling... [9216/50176]	Loss: 2.3761
Profiling... [10240/50176]	Loss: 2.3545
Profiling... [11264/50176]	Loss: 2.4596
Profiling... [12288/50176]	Loss: 2.3132
Profiling... [13312/50176]	Loss: 2.2607
Profile done
epoch 1 train time consumed: 13.00s
Validation Epoch: 5, Average loss: 0.0025, Accuracy: 0.3311
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.079303984051,
                        "time": 9.06962442099939,
                        "accuracy": 0.3310546875,
                        "total_cost": 2398794.4261298864
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.3703
Profiling... [2048/50176]	Loss: 2.5431
Profiling... [3072/50176]	Loss: 2.4616
Profiling... [4096/50176]	Loss: 2.4729
Profiling... [5120/50176]	Loss: 2.3984
Profiling... [6144/50176]	Loss: 2.4613
Profiling... [7168/50176]	Loss: 2.3990
Profiling... [8192/50176]	Loss: 2.4129
Profiling... [9216/50176]	Loss: 2.3662
Profiling... [10240/50176]	Loss: 2.4414
Profiling... [11264/50176]	Loss: 2.2667
Profiling... [12288/50176]	Loss: 2.2651
Profiling... [13312/50176]	Loss: 2.3425
Profile done
epoch 1 train time consumed: 14.15s
Validation Epoch: 5, Average loss: 0.0024, Accuracy: 0.3498
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.08843083408702,
                        "time": 9.970620903999588,
                        "accuracy": 0.3498046875,
                        "total_cost": 2495744.205540166
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.3920
Profiling... [2048/50176]	Loss: 2.4433
Profiling... [3072/50176]	Loss: 2.4923
Profiling... [4096/50176]	Loss: 2.4534
Profiling... [5120/50176]	Loss: 2.3307
Profiling... [6144/50176]	Loss: 2.3901
Profiling... [7168/50176]	Loss: 2.4586
Profiling... [8192/50176]	Loss: 2.3941
Profiling... [9216/50176]	Loss: 2.3564
Profiling... [10240/50176]	Loss: 2.3534
Profiling... [11264/50176]	Loss: 2.3032
Profiling... [12288/50176]	Loss: 2.3110
Profiling... [13312/50176]	Loss: 2.2531
Profile done
epoch 1 train time consumed: 25.44s
Validation Epoch: 5, Average loss: 0.0027, Accuracy: 0.3117
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.9071818035443,
                        "time": 18.731589178999457,
                        "accuracy": 0.31171875,
                        "total_cost": 5261562.589361214
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.2985
Profiling... [2048/50176]	Loss: 2.4304
Profiling... [3072/50176]	Loss: 2.3519
Profiling... [4096/50176]	Loss: 2.4006
Profiling... [5120/50176]	Loss: 2.4353
Profiling... [6144/50176]	Loss: 2.3678
Profiling... [7168/50176]	Loss: 2.3465
Profiling... [8192/50176]	Loss: 2.4399
Profiling... [9216/50176]	Loss: 2.3420
Profiling... [10240/50176]	Loss: 2.2400
Profiling... [11264/50176]	Loss: 2.3873
Profiling... [12288/50176]	Loss: 2.4433
Profiling... [13312/50176]	Loss: 2.3847
Profile done
epoch 1 train time consumed: 12.80s
Validation Epoch: 5, Average loss: 0.0025, Accuracy: 0.3480
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.94292323029728,
                        "time": 9.014675198999612,
                        "accuracy": 0.348046875,
                        "total_cost": 2267856.0059547625
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.3570
Profiling... [2048/50176]	Loss: 2.5120
Profiling... [3072/50176]	Loss: 2.4165
Profiling... [4096/50176]	Loss: 2.4321
Profiling... [5120/50176]	Loss: 2.3711
Profiling... [6144/50176]	Loss: 2.2419
Profiling... [7168/50176]	Loss: 2.3092
Profiling... [8192/50176]	Loss: 2.3939
Profiling... [9216/50176]	Loss: 2.3720
Profiling... [10240/50176]	Loss: 2.3323
Profiling... [11264/50176]	Loss: 2.3400
Profiling... [12288/50176]	Loss: 2.3316
Profiling... [13312/50176]	Loss: 2.2878
Profile done
epoch 1 train time consumed: 12.88s
Validation Epoch: 5, Average loss: 0.0025, Accuracy: 0.3455
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.01939723185643,
                        "time": 9.027940016000684,
                        "accuracy": 0.3455078125,
                        "total_cost": 2287884.5941856517
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.4799
Profiling... [2048/50176]	Loss: 2.4154
Profiling... [3072/50176]	Loss: 2.3662
Profiling... [4096/50176]	Loss: 2.3372
Profiling... [5120/50176]	Loss: 2.3765
Profiling... [6144/50176]	Loss: 2.2829
Profiling... [7168/50176]	Loss: 2.4252
Profiling... [8192/50176]	Loss: 2.3872
Profiling... [9216/50176]	Loss: 2.3997
Profiling... [10240/50176]	Loss: 2.3371
Profiling... [11264/50176]	Loss: 2.4048
Profiling... [12288/50176]	Loss: 2.3385
Profiling... [13312/50176]	Loss: 2.3204
Profile done
epoch 1 train time consumed: 14.16s
Validation Epoch: 5, Average loss: 0.0033, Accuracy: 0.2510
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.02970620582168,
                        "time": 9.98864180199962,
                        "accuracy": 0.2509765625,
                        "total_cost": 3484790.059727232
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.4254
Profiling... [2048/50176]	Loss: 2.4011
Profiling... [3072/50176]	Loss: 2.4152
Profiling... [4096/50176]	Loss: 2.4556
Profiling... [5120/50176]	Loss: 2.4083
Profiling... [6144/50176]	Loss: 2.4425
Profiling... [7168/50176]	Loss: 2.3856
Profiling... [8192/50176]	Loss: 2.3672
Profiling... [9216/50176]	Loss: 2.3835
Profiling... [10240/50176]	Loss: 2.3453
Profiling... [11264/50176]	Loss: 2.3247
Profiling... [12288/50176]	Loss: 2.3133
Profiling... [13312/50176]	Loss: 2.4084
Profile done
epoch 1 train time consumed: 25.40s
Validation Epoch: 5, Average loss: 0.0025, Accuracy: 0.3371
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.85450554369427,
                        "time": 18.714756985999884,
                        "accuracy": 0.337109375,
                        "total_cost": 4860895.378151262
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.4469
Profiling... [2048/50176]	Loss: 2.4699
Profiling... [3072/50176]	Loss: 2.3765
Profiling... [4096/50176]	Loss: 2.4370
Profiling... [5120/50176]	Loss: 2.4970
Profiling... [6144/50176]	Loss: 2.4296
Profiling... [7168/50176]	Loss: 2.3579
Profiling... [8192/50176]	Loss: 2.4442
Profiling... [9216/50176]	Loss: 2.4405
Profiling... [10240/50176]	Loss: 2.1836
Profiling... [11264/50176]	Loss: 2.2540
Profiling... [12288/50176]	Loss: 2.2839
Profiling... [13312/50176]	Loss: 2.2893
Profile done
epoch 1 train time consumed: 12.98s
Validation Epoch: 5, Average loss: 0.0025, Accuracy: 0.3385
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.88867470261533,
                        "time": 8.934627758000715,
                        "accuracy": 0.3384765625,
                        "total_cost": 2311270.937265021
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.4009
Profiling... [2048/50176]	Loss: 2.4399
Profiling... [3072/50176]	Loss: 2.4895
Profiling... [4096/50176]	Loss: 2.4569
Profiling... [5120/50176]	Loss: 2.4956
Profiling... [6144/50176]	Loss: 2.4462
Profiling... [7168/50176]	Loss: 2.4152
Profiling... [8192/50176]	Loss: 2.4844
Profiling... [9216/50176]	Loss: 2.3250
Profiling... [10240/50176]	Loss: 2.3492
Profiling... [11264/50176]	Loss: 2.3595
Profiling... [12288/50176]	Loss: 2.3393
Profiling... [13312/50176]	Loss: 2.2837
Profile done
epoch 1 train time consumed: 13.22s
Validation Epoch: 5, Average loss: 0.0027, Accuracy: 0.3133
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.97188635914975,
                        "time": 9.23690112300028,
                        "accuracy": 0.31328125,
                        "total_cost": 2581636.5136371315
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.5379
Profiling... [2048/50176]	Loss: 2.4917
Profiling... [3072/50176]	Loss: 2.3680
Profiling... [4096/50176]	Loss: 2.4800
Profiling... [5120/50176]	Loss: 2.4154
Profiling... [6144/50176]	Loss: 2.3441
Profiling... [7168/50176]	Loss: 2.4455
Profiling... [8192/50176]	Loss: 2.3454
Profiling... [9216/50176]	Loss: 2.4455
Profiling... [10240/50176]	Loss: 2.4023
Profiling... [11264/50176]	Loss: 2.3429
Profiling... [12288/50176]	Loss: 2.4098
Profiling... [13312/50176]	Loss: 2.3428
Profile done
epoch 1 train time consumed: 14.25s
Validation Epoch: 5, Average loss: 0.0028, Accuracy: 0.3053
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.98189203056268,
                        "time": 10.127574092999566,
                        "accuracy": 0.3052734375,
                        "total_cost": 2904822.769262702
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.3919
Profiling... [2048/50176]	Loss: 2.5573
Profiling... [3072/50176]	Loss: 2.4450
Profiling... [4096/50176]	Loss: 2.4298
Profiling... [5120/50176]	Loss: 2.4001
Profiling... [6144/50176]	Loss: 2.3553
Profiling... [7168/50176]	Loss: 2.4101
Profiling... [8192/50176]	Loss: 2.3814
Profiling... [9216/50176]	Loss: 2.3786
Profiling... [10240/50176]	Loss: 2.3289
Profiling... [11264/50176]	Loss: 2.4094
Profiling... [12288/50176]	Loss: 2.3336
Profiling... [13312/50176]	Loss: 2.3448
Profile done
epoch 1 train time consumed: 24.29s
Validation Epoch: 5, Average loss: 0.0028, Accuracy: 0.2965
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.82088367480073,
                        "time": 17.532112863000293,
                        "accuracy": 0.296484375,
                        "total_cost": 5177680.834222916
                    },
                    
[Training Loop] The optimal parameters are lr: 0.001 dr: 0.5 bs: 128 pl: 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[GPU_0] Set GPU power limit to 175W.
[GPU_0] Set GPU power limit to 175W.
Training Epoch: 5 [128/50048]	Loss: 2.5928
Training Epoch: 5 [256/50048]	Loss: 2.5420
Training Epoch: 5 [384/50048]	Loss: 2.3699
Training Epoch: 5 [512/50048]	Loss: 2.4352
Training Epoch: 5 [640/50048]	Loss: 2.4609
Training Epoch: 5 [768/50048]	Loss: 2.2252
Training Epoch: 5 [896/50048]	Loss: 2.1389
Training Epoch: 5 [1024/50048]	Loss: 2.4180
Training Epoch: 5 [1152/50048]	Loss: 2.4365
Training Epoch: 5 [1280/50048]	Loss: 2.4870
Training Epoch: 5 [1408/50048]	Loss: 2.1857
Training Epoch: 5 [1536/50048]	Loss: 2.6506
Training Epoch: 5 [1664/50048]	Loss: 2.4555
Training Epoch: 5 [1792/50048]	Loss: 2.4072
Training Epoch: 5 [1920/50048]	Loss: 2.4880
Training Epoch: 5 [2048/50048]	Loss: 2.3976
Training Epoch: 5 [2176/50048]	Loss: 2.3272
Training Epoch: 5 [2304/50048]	Loss: 2.3956
Training Epoch: 5 [2432/50048]	Loss: 2.4677
Training Epoch: 5 [2560/50048]	Loss: 2.2481
Training Epoch: 5 [2688/50048]	Loss: 2.2377
Training Epoch: 5 [2816/50048]	Loss: 2.2023
Training Epoch: 5 [2944/50048]	Loss: 2.3228
Training Epoch: 5 [3072/50048]	Loss: 2.2204
Training Epoch: 5 [3200/50048]	Loss: 2.2538
Training Epoch: 5 [3328/50048]	Loss: 2.3721
Training Epoch: 5 [3456/50048]	Loss: 2.3227
Training Epoch: 5 [3584/50048]	Loss: 2.6782
Training Epoch: 5 [3712/50048]	Loss: 2.2150
Training Epoch: 5 [3840/50048]	Loss: 2.1021
Training Epoch: 5 [3968/50048]	Loss: 2.1905
Training Epoch: 5 [4096/50048]	Loss: 2.3308
Training Epoch: 5 [4224/50048]	Loss: 2.5906
Training Epoch: 5 [4352/50048]	Loss: 2.3816
Training Epoch: 5 [4480/50048]	Loss: 2.3261
Training Epoch: 5 [4608/50048]	Loss: 2.6252
Training Epoch: 5 [4736/50048]	Loss: 2.3152
Training Epoch: 5 [4864/50048]	Loss: 2.3189
Training Epoch: 5 [4992/50048]	Loss: 2.1665
Training Epoch: 5 [5120/50048]	Loss: 2.3220
Training Epoch: 5 [5248/50048]	Loss: 2.4127
Training Epoch: 5 [5376/50048]	Loss: 2.4757
Training Epoch: 5 [5504/50048]	Loss: 2.3929
Training Epoch: 5 [5632/50048]	Loss: 2.0985
Training Epoch: 5 [5760/50048]	Loss: 2.4159
Training Epoch: 5 [5888/50048]	Loss: 2.3712
Training Epoch: 5 [6016/50048]	Loss: 2.0746
Training Epoch: 5 [6144/50048]	Loss: 2.4426
Training Epoch: 5 [6272/50048]	Loss: 2.5095
Training Epoch: 5 [6400/50048]	Loss: 2.6157
Training Epoch: 5 [6528/50048]	Loss: 2.4461
Training Epoch: 5 [6656/50048]	Loss: 2.2063
Training Epoch: 5 [6784/50048]	Loss: 2.3057
Training Epoch: 5 [6912/50048]	Loss: 2.1073
Training Epoch: 5 [7040/50048]	Loss: 2.3179
Training Epoch: 5 [7168/50048]	Loss: 2.1994
Training Epoch: 5 [7296/50048]	Loss: 2.0392
Training Epoch: 5 [7424/50048]	Loss: 2.1491
Training Epoch: 5 [7552/50048]	Loss: 2.1841
Training Epoch: 5 [7680/50048]	Loss: 2.4546
Training Epoch: 5 [7808/50048]	Loss: 2.7226
Training Epoch: 5 [7936/50048]	Loss: 2.0697
Training Epoch: 5 [8064/50048]	Loss: 2.5090
Training Epoch: 5 [8192/50048]	Loss: 2.1127
Training Epoch: 5 [8320/50048]	Loss: 2.2080
Training Epoch: 5 [8448/50048]	Loss: 2.3684
Training Epoch: 5 [8576/50048]	Loss: 2.3869
Training Epoch: 5 [8704/50048]	Loss: 2.2882
Training Epoch: 5 [8832/50048]	Loss: 2.2037
Training Epoch: 5 [8960/50048]	Loss: 2.4096
Training Epoch: 5 [9088/50048]	Loss: 2.1847
Training Epoch: 5 [9216/50048]	Loss: 2.4146
Training Epoch: 5 [9344/50048]	Loss: 2.5664
Training Epoch: 5 [9472/50048]	Loss: 2.2007
Training Epoch: 5 [9600/50048]	Loss: 2.2986
Training Epoch: 5 [9728/50048]	Loss: 2.1489
Training Epoch: 5 [9856/50048]	Loss: 2.5274
Training Epoch: 5 [9984/50048]	Loss: 2.2940
Training Epoch: 5 [10112/50048]	Loss: 2.2449
Training Epoch: 5 [10240/50048]	Loss: 2.1743
Training Epoch: 5 [10368/50048]	Loss: 2.3933
Training Epoch: 5 [10496/50048]	Loss: 2.3528
Training Epoch: 5 [10624/50048]	Loss: 2.1788
Training Epoch: 5 [10752/50048]	Loss: 2.2320
Training Epoch: 5 [10880/50048]	Loss: 2.1241
Training Epoch: 5 [11008/50048]	Loss: 2.2622
Training Epoch: 5 [11136/50048]	Loss: 2.1945
Training Epoch: 5 [11264/50048]	Loss: 2.0651
Training Epoch: 5 [11392/50048]	Loss: 2.1757
Training Epoch: 5 [11520/50048]	Loss: 2.3305
Training Epoch: 5 [11648/50048]	Loss: 2.2521
Training Epoch: 5 [11776/50048]	Loss: 2.3127
Training Epoch: 5 [11904/50048]	Loss: 2.2950
Training Epoch: 5 [12032/50048]	Loss: 2.5220
Training Epoch: 5 [12160/50048]	Loss: 2.3311
Training Epoch: 5 [12288/50048]	Loss: 2.2738
Training Epoch: 5 [12416/50048]	Loss: 2.6348
Training Epoch: 5 [12544/50048]	Loss: 2.4644
Training Epoch: 5 [12672/50048]	Loss: 2.4650
Training Epoch: 5 [12800/50048]	Loss: 2.2099
Training Epoch: 5 [12928/50048]	Loss: 2.2427
Training Epoch: 5 [13056/50048]	Loss: 2.1468
Training Epoch: 5 [13184/50048]	Loss: 2.0068
Training Epoch: 5 [13312/50048]	Loss: 2.2559
Training Epoch: 5 [13440/50048]	Loss: 2.4020
Training Epoch: 5 [13568/50048]	Loss: 2.1324
Training Epoch: 5 [13696/50048]	Loss: 2.3166
Training Epoch: 5 [13824/50048]	Loss: 2.4045
Training Epoch: 5 [13952/50048]	Loss: 2.3082
Training Epoch: 5 [14080/50048]	Loss: 2.2747
Training Epoch: 5 [14208/50048]	Loss: 2.4761
Training Epoch: 5 [14336/50048]	Loss: 2.4468
Training Epoch: 5 [14464/50048]	Loss: 2.1422
Training Epoch: 5 [14592/50048]	Loss: 2.1913
Training Epoch: 5 [14720/50048]	Loss: 2.3531
Training Epoch: 5 [14848/50048]	Loss: 2.4071
Training Epoch: 5 [14976/50048]	Loss: 2.1641
Training Epoch: 5 [15104/50048]	Loss: 2.0518
Training Epoch: 5 [15232/50048]	Loss: 2.3045
Training Epoch: 5 [15360/50048]	Loss: 2.2168
Training Epoch: 5 [15488/50048]	Loss: 2.0841
Training Epoch: 5 [15616/50048]	Loss: 2.3075
Training Epoch: 5 [15744/50048]	Loss: 2.3183
Training Epoch: 5 [15872/50048]	Loss: 2.4062
Training Epoch: 5 [16000/50048]	Loss: 2.3273
Training Epoch: 5 [16128/50048]	Loss: 2.1373
Training Epoch: 5 [16256/50048]	Loss: 2.2983
Training Epoch: 5 [16384/50048]	Loss: 2.3343
Training Epoch: 5 [16512/50048]	Loss: 2.3292
Training Epoch: 5 [16640/50048]	Loss: 2.2032
Training Epoch: 5 [16768/50048]	Loss: 2.2616
Training Epoch: 5 [16896/50048]	Loss: 2.2433
Training Epoch: 5 [17024/50048]	Loss: 2.5058
Training Epoch: 5 [17152/50048]	Loss: 2.5150
Training Epoch: 5 [17280/50048]	Loss: 2.1556
Training Epoch: 5 [17408/50048]	Loss: 2.2119
Training Epoch: 5 [17536/50048]	Loss: 2.5681
Training Epoch: 5 [17664/50048]	Loss: 2.3376
Training Epoch: 5 [17792/50048]	Loss: 2.1986
Training Epoch: 5 [17920/50048]	Loss: 2.2873
Training Epoch: 5 [18048/50048]	Loss: 2.3236
Training Epoch: 5 [18176/50048]	Loss: 2.4351
Training Epoch: 5 [18304/50048]	Loss: 2.3571
Training Epoch: 5 [18432/50048]	Loss: 2.3481
Training Epoch: 5 [18560/50048]	Loss: 2.2191
Training Epoch: 5 [18688/50048]	Loss: 2.4171
Training Epoch: 5 [18816/50048]	Loss: 2.2622
Training Epoch: 5 [18944/50048]	Loss: 2.2796
Training Epoch: 5 [19072/50048]	Loss: 2.3582
Training Epoch: 5 [19200/50048]	Loss: 2.1804
Training Epoch: 5 [19328/50048]	Loss: 2.1124
Training Epoch: 5 [19456/50048]	Loss: 2.5071
Training Epoch: 5 [19584/50048]	Loss: 2.4370
Training Epoch: 5 [19712/50048]	Loss: 2.2692
Training Epoch: 5 [19840/50048]	Loss: 2.4713
Training Epoch: 5 [19968/50048]	Loss: 2.2353
Training Epoch: 5 [20096/50048]	Loss: 2.3046
Training Epoch: 5 [20224/50048]	Loss: 2.1675
Training Epoch: 5 [20352/50048]	Loss: 2.3638
Training Epoch: 5 [20480/50048]	Loss: 2.2965
Training Epoch: 5 [20608/50048]	Loss: 2.1440
Training Epoch: 5 [20736/50048]	Loss: 2.1374
Training Epoch: 5 [20864/50048]	Loss: 2.2475
Training Epoch: 5 [20992/50048]	Loss: 2.5160
Training Epoch: 5 [21120/50048]	Loss: 2.5508
Training Epoch: 5 [21248/50048]	Loss: 2.4495
Training Epoch: 5 [21376/50048]	Loss: 2.3798
Training Epoch: 5 [21504/50048]	Loss: 2.0542
Training Epoch: 5 [21632/50048]	Loss: 2.0101
Training Epoch: 5 [21760/50048]	Loss: 2.1755
Training Epoch: 5 [21888/50048]	Loss: 2.4048
Training Epoch: 5 [22016/50048]	Loss: 2.3161
Training Epoch: 5 [22144/50048]	Loss: 2.4630
Training Epoch: 5 [22272/50048]	Loss: 2.3370
Training Epoch: 5 [22400/50048]	Loss: 1.9372
Training Epoch: 5 [22528/50048]	Loss: 2.1877
Training Epoch: 5 [22656/50048]	Loss: 2.2152
Training Epoch: 5 [22784/50048]	Loss: 2.3693
Training Epoch: 5 [22912/50048]	Loss: 2.0870
Training Epoch: 5 [23040/50048]	Loss: 2.2127
Training Epoch: 5 [23168/50048]	Loss: 2.1213
Training Epoch: 5 [23296/50048]	Loss: 2.2836
Training Epoch: 5 [23424/50048]	Loss: 2.0960
Training Epoch: 5 [23552/50048]	Loss: 2.3777
Training Epoch: 5 [23680/50048]	Loss: 2.2361
Training Epoch: 5 [23808/50048]	Loss: 2.3465
Training Epoch: 5 [23936/50048]	Loss: 2.0857
Training Epoch: 5 [24064/50048]	Loss: 2.1698
Training Epoch: 5 [24192/50048]	Loss: 2.4169
Training Epoch: 5 [24320/50048]	Loss: 2.1803
Training Epoch: 5 [24448/50048]	Loss: 2.1032
Training Epoch: 5 [24576/50048]	Loss: 2.2125
Training Epoch: 5 [24704/50048]	Loss: 2.3353
Training Epoch: 5 [24832/50048]	Loss: 2.0435
Training Epoch: 5 [24960/50048]	Loss: 2.1279
Training Epoch: 5 [25088/50048]	Loss: 2.2826
Training Epoch: 5 [25216/50048]	Loss: 2.2930
Training Epoch: 5 [25344/50048]	Loss: 2.1025
Training Epoch: 5 [25472/50048]	Loss: 2.1938
Training Epoch: 5 [25600/50048]	Loss: 2.2195
Training Epoch: 5 [25728/50048]	Loss: 2.2914
Training Epoch: 5 [25856/50048]	Loss: 2.3825
Training Epoch: 5 [25984/50048]	Loss: 2.2807
Training Epoch: 5 [26112/50048]	Loss: 2.1362
Training Epoch: 5 [26240/50048]	Loss: 2.3338
Training Epoch: 5 [26368/50048]	Loss: 2.2272
Training Epoch: 5 [26496/50048]	Loss: 2.3082
Training Epoch: 5 [26624/50048]	Loss: 2.3717
Training Epoch: 5 [26752/50048]	Loss: 2.2711
Training Epoch: 5 [26880/50048]	Loss: 2.3804
Training Epoch: 5 [27008/50048]	Loss: 1.7831
Training Epoch: 5 [27136/50048]	Loss: 2.2220
Training Epoch: 5 [27264/50048]	Loss: 2.3334
Training Epoch: 5 [27392/50048]	Loss: 2.1640
Training Epoch: 5 [27520/50048]	Loss: 1.9494
Training Epoch: 5 [27648/50048]	Loss: 2.1191
Training Epoch: 5 [27776/50048]	Loss: 2.3509
Training Epoch: 5 [27904/50048]	Loss: 2.2334
Training Epoch: 5 [28032/50048]	Loss: 2.4907
Training Epoch: 5 [28160/50048]	Loss: 2.3182
Training Epoch: 5 [28288/50048]	Loss: 2.2506
Training Epoch: 5 [28416/50048]	Loss: 2.2131
Training Epoch: 5 [28544/50048]	Loss: 2.1563
Training Epoch: 5 [28672/50048]	Loss: 2.0927
Training Epoch: 5 [28800/50048]	Loss: 2.2465
Training Epoch: 5 [28928/50048]	Loss: 2.1504
Training Epoch: 5 [29056/50048]	Loss: 2.5143
Training Epoch: 5 [29184/50048]	Loss: 2.3892
Training Epoch: 5 [29312/50048]	Loss: 2.3134
Training Epoch: 5 [29440/50048]	Loss: 2.1246
Training Epoch: 5 [29568/50048]	Loss: 2.2670
Training Epoch: 5 [29696/50048]	Loss: 2.2933
Training Epoch: 5 [29824/50048]	Loss: 2.6053
Training Epoch: 5 [29952/50048]	Loss: 2.1434
Training Epoch: 5 [30080/50048]	Loss: 2.1575
Training Epoch: 5 [30208/50048]	Loss: 1.9214
Training Epoch: 5 [30336/50048]	Loss: 1.8374
Training Epoch: 5 [30464/50048]	Loss: 2.2026
Training Epoch: 5 [30592/50048]	Loss: 2.4868
Training Epoch: 5 [30720/50048]	Loss: 2.0203
Training Epoch: 5 [30848/50048]	Loss: 2.4705
Training Epoch: 5 [30976/50048]	Loss: 2.2688
Training Epoch: 5 [31104/50048]	Loss: 2.3372
Training Epoch: 5 [31232/50048]	Loss: 2.1807
Training Epoch: 5 [31360/50048]	Loss: 2.1994
Training Epoch: 5 [31488/50048]	Loss: 2.0833
Training Epoch: 5 [31616/50048]	Loss: 2.1676
Training Epoch: 5 [31744/50048]	Loss: 2.4267
Training Epoch: 5 [31872/50048]	Loss: 2.1046
Training Epoch: 5 [32000/50048]	Loss: 2.4497
Training Epoch: 5 [32128/50048]	Loss: 2.1921
Training Epoch: 5 [32256/50048]	Loss: 2.1232
Training Epoch: 5 [32384/50048]	Loss: 2.3395
Training Epoch: 5 [32512/50048]	Loss: 2.3485
Training Epoch: 5 [32640/50048]	Loss: 1.9253
Training Epoch: 5 [32768/50048]	Loss: 1.9502
Training Epoch: 5 [32896/50048]	Loss: 2.2260
Training Epoch: 5 [33024/50048]	Loss: 2.2079
Training Epoch: 5 [33152/50048]	Loss: 2.1886
Training Epoch: 5 [33280/50048]	Loss: 2.2868
Training Epoch: 5 [33408/50048]	Loss: 2.1666
Training Epoch: 5 [33536/50048]	Loss: 2.4098
Training Epoch: 5 [33664/50048]	Loss: 2.1768
Training Epoch: 5 [33792/50048]	Loss: 2.3692
Training Epoch: 5 [33920/50048]	Loss: 2.0998
Training Epoch: 5 [34048/50048]	Loss: 2.2435
Training Epoch: 5 [34176/50048]	Loss: 2.1746
Training Epoch: 5 [34304/50048]	Loss: 2.3630
Training Epoch: 5 [34432/50048]	Loss: 2.3158
Training Epoch: 5 [34560/50048]	Loss: 2.1417
Training Epoch: 5 [34688/50048]	Loss: 2.0219
Training Epoch: 5 [34816/50048]	Loss: 2.1664
Training Epoch: 5 [34944/50048]	Loss: 2.2662
Training Epoch: 5 [35072/50048]	Loss: 2.3363
Training Epoch: 5 [35200/50048]	Loss: 2.3024
Training Epoch: 5 [35328/50048]	Loss: 2.0086
Training Epoch: 5 [35456/50048]	Loss: 2.1981
Training Epoch: 5 [35584/50048]	Loss: 2.2255
Training Epoch: 5 [35712/50048]	Loss: 2.0519
Training Epoch: 5 [35840/50048]	Loss: 2.0240
Training Epoch: 5 [35968/50048]	Loss: 2.0551
Training Epoch: 5 [36096/50048]	Loss: 2.2783
Training Epoch: 5 [36224/50048]	Loss: 2.2265
Training Epoch: 5 [36352/50048]	Loss: 2.3546
Training Epoch: 5 [36480/50048]	Loss: 2.0052
Training Epoch: 5 [36608/50048]	Loss: 2.3323
Training Epoch: 5 [36736/50048]	Loss: 2.3024
Training Epoch: 5 [36864/50048]	Loss: 2.2321
Training Epoch: 5 [36992/50048]	Loss: 2.1281
Training Epoch: 5 [37120/50048]	Loss: 2.2377
Training Epoch: 5 [37248/50048]	Loss: 2.2817
Training Epoch: 5 [37376/50048]	Loss: 2.2979
Training Epoch: 5 [37504/50048]	Loss: 2.1397
Training Epoch: 5 [37632/50048]	Loss: 2.0856
Training Epoch: 5 [37760/50048]	Loss: 2.3382
Training Epoch: 5 [37888/50048]	Loss: 2.3987
Training Epoch: 5 [38016/50048]	Loss: 2.4770
Training Epoch: 5 [38144/50048]	Loss: 2.3588
Training Epoch: 5 [38272/50048]	Loss: 2.0812
Training Epoch: 5 [38400/50048]	Loss: 1.9643
Training Epoch: 5 [38528/50048]	Loss: 2.2341
Training Epoch: 5 [38656/50048]	Loss: 2.1559
Training Epoch: 5 [38784/50048]	Loss: 2.0157
Training Epoch: 5 [38912/50048]	Loss: 2.2836
Training Epoch: 5 [39040/50048]	Loss: 2.1377
Training Epoch: 5 [39168/50048]	Loss: 2.1484
Training Epoch: 5 [39296/50048]	Loss: 2.3365
Training Epoch: 5 [39424/50048]	Loss: 2.5416
Training Epoch: 5 [39552/50048]	Loss: 2.0364
Training Epoch: 5 [39680/50048]	Loss: 2.3754
Training Epoch: 5 [39808/50048]	Loss: 2.1010
Training Epoch: 5 [39936/50048]	Loss: 2.0061
Training Epoch: 5 [40064/50048]	Loss: 2.3562
Training Epoch: 5 [40192/50048]	Loss: 2.1174
Training Epoch: 5 [40320/50048]	Loss: 2.2822
Training Epoch: 5 [40448/50048]	Loss: 2.0749
Training Epoch: 5 [40576/50048]	Loss: 2.1419
Training Epoch: 5 [40704/50048]	Loss: 2.3405
Training Epoch: 5 [40832/50048]	Loss: 2.3723
Training Epoch: 5 [40960/50048]	Loss: 2.1119
Training Epoch: 5 [41088/50048]	Loss: 2.0101
Training Epoch: 5 [41216/50048]	Loss: 2.4124
Training Epoch: 5 [41344/50048]	Loss: 2.2239
Training Epoch: 5 [41472/50048]	Loss: 2.0715
Training Epoch: 5 [41600/50048]	Loss: 2.1822
Training Epoch: 5 [41728/50048]	Loss: 2.3761
Training Epoch: 5 [41856/50048]	Loss: 2.4129
Training Epoch: 5 [41984/50048]	Loss: 2.4178
Training Epoch: 5 [42112/50048]	Loss: 2.3742
Training Epoch: 5 [42240/50048]	Loss: 2.1405
Training Epoch: 5 [42368/50048]	Loss: 2.3791
Training Epoch: 5 [42496/50048]	Loss: 2.2684
Training Epoch: 5 [42624/50048]	Loss: 2.4095
Training Epoch: 5 [42752/50048]	Loss: 2.1709
Training Epoch: 5 [42880/50048]	Loss: 2.2477
Training Epoch: 5 [43008/50048]	Loss: 1.9854
Training Epoch: 5 [43136/50048]	Loss: 2.2029
Training Epoch: 5 [43264/50048]	Loss: 2.4801
Training Epoch: 5 [43392/50048]	Loss: 2.2721
Training Epoch: 5 [43520/50048]	Loss: 2.2477
Training Epoch: 5 [43648/50048]	Loss: 2.2065
Training Epoch: 5 [43776/50048]	Loss: 2.1904
Training Epoch: 5 [43904/50048]	Loss: 2.2814
Training Epoch: 5 [44032/50048]	Loss: 2.5544
Training Epoch: 5 [44160/50048]	Loss: 2.3769
Training Epoch: 5 [44288/50048]	Loss: 2.2683
Training Epoch: 5 [44416/50048]	Loss: 2.4116
Training Epoch: 5 [44544/50048]	Loss: 2.0093
Training Epoch: 5 [44672/50048]	Loss: 2.1705
Training Epoch: 5 [44800/50048]	Loss: 2.3668
Training Epoch: 5 [44928/50048]	Loss: 2.0576
Training Epoch: 5 [45056/50048]	Loss: 1.9889
Training Epoch: 5 [45184/50048]	Loss: 2.3447
Training Epoch: 5 [45312/50048]	Loss: 2.0341
Training Epoch: 5 [45440/50048]	Loss: 2.1909
Training Epoch: 5 [45568/50048]	Loss: 2.6185
Training Epoch: 5 [45696/50048]	Loss: 2.0506
Training Epoch: 5 [45824/50048]	Loss: 2.2798
Training Epoch: 5 [45952/50048]	Loss: 2.3749
Training Epoch: 5 [46080/50048]	Loss: 2.4253
Training Epoch: 5 [46208/50048]	Loss: 2.1051
Training Epoch: 5 [46336/50048]	Loss: 2.2201
Training Epoch: 5 [46464/50048]	Loss: 1.9226
Training Epoch: 5 [46592/50048]	Loss: 2.3876
Training Epoch: 5 [46720/50048]	Loss: 2.1332
Training Epoch: 5 [46848/50048]	Loss: 2.2500
Training Epoch: 5 [46976/50048]	Loss: 2.0120
Training Epoch: 5 [47104/50048]	Loss: 2.0009
Training Epoch: 5 [47232/50048]	Loss: 2.0864
Training Epoch: 5 [47360/50048]	Loss: 1.9665
Training Epoch: 5 [47488/50048]	Loss: 2.2902
Training Epoch: 5 [47616/50048]	Loss: 2.1677
Training Epoch: 5 [47744/50048]	Loss: 1.9664
Training Epoch: 5 [47872/50048]	Loss: 2.1898
Training Epoch: 5 [48000/50048]	Loss: 2.1108
Training Epoch: 5 [48128/50048]	Loss: 1.9945
Training Epoch: 5 [48256/50048]	Loss: 2.3798
Training Epoch: 5 [48384/50048]	Loss: 2.0994
Training Epoch: 5 [48512/50048]	Loss: 2.5130
Training Epoch: 5 [48640/50048]	Loss: 2.1202
Training Epoch: 5 [48768/50048]	Loss: 2.3434
Training Epoch: 5 [48896/50048]	Loss: 1.9277
Training Epoch: 5 [49024/50048]	Loss: 2.1656
Training Epoch: 5 [49152/50048]	Loss: 2.3545
Training Epoch: 5 [49280/50048]	Loss: 2.2883
Training Epoch: 5 [49408/50048]	Loss: 2.2544
Training Epoch: 5 [49536/50048]	Loss: 1.8983
Training Epoch: 5 [49664/50048]	Loss: 2.2818
Training Epoch: 5 [49792/50048]	Loss: 2.2307
Training Epoch: 5 [49920/50048]	Loss: 2.1080
Training Epoch: 5 [50048/50048]	Loss: 2.7481
Validation Epoch: 5, Average loss: 0.0170, Accuracy: 0.4257
[Training Loop] Model's accuracy 0.42573180379746833 surpasses threshold 0.4! Reprofiling...
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.8993
Profiling... [256/50048]	Loss: 2.2061
Profiling... [384/50048]	Loss: 2.1199
Profiling... [512/50048]	Loss: 2.2167
Profiling... [640/50048]	Loss: 2.0834
Profiling... [768/50048]	Loss: 2.0669
Profiling... [896/50048]	Loss: 2.2066
Profiling... [1024/50048]	Loss: 2.0292
Profiling... [1152/50048]	Loss: 2.1226
Profiling... [1280/50048]	Loss: 2.0669
Profiling... [1408/50048]	Loss: 2.0032
Profiling... [1536/50048]	Loss: 2.3495
Profiling... [1664/50048]	Loss: 2.3794
Profile done
epoch 1 train time consumed: 3.44s
Validation Epoch: 6, Average loss: 0.0170, Accuracy: 0.4291
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.18340912910088,
                        "time": 2.178342705000432,
                        "accuracy": 0.4290941455696203,
                        "total_cost": 444505.7100214104
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.0328
Profiling... [256/50048]	Loss: 2.0847
Profiling... [384/50048]	Loss: 2.1344
Profiling... [512/50048]	Loss: 2.1130
Profiling... [640/50048]	Loss: 1.9937
Profiling... [768/50048]	Loss: 1.9441
Profiling... [896/50048]	Loss: 2.0575
Profiling... [1024/50048]	Loss: 2.1239
Profiling... [1152/50048]	Loss: 2.4999
Profiling... [1280/50048]	Loss: 2.2155
Profiling... [1408/50048]	Loss: 2.3766
Profiling... [1536/50048]	Loss: 2.2177
Profiling... [1664/50048]	Loss: 2.1028
Profile done
epoch 1 train time consumed: 3.43s
Validation Epoch: 6, Average loss: 0.0171, Accuracy: 0.4260
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.20382740021499,
                        "time": 2.172794306000469,
                        "accuracy": 0.4260284810126582,
                        "total_cost": 446564.0513552958
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.9744
Profiling... [256/50048]	Loss: 2.1098
Profiling... [384/50048]	Loss: 2.0704
Profiling... [512/50048]	Loss: 2.3227
Profiling... [640/50048]	Loss: 2.2587
Profiling... [768/50048]	Loss: 2.2069
Profiling... [896/50048]	Loss: 2.0871
Profiling... [1024/50048]	Loss: 2.3995
Profiling... [1152/50048]	Loss: 1.9761
Profiling... [1280/50048]	Loss: 2.1555
Profiling... [1408/50048]	Loss: 2.1926
Profiling... [1536/50048]	Loss: 2.3377
Profiling... [1664/50048]	Loss: 1.9270
Profile done
epoch 1 train time consumed: 3.61s
Validation Epoch: 6, Average loss: 0.0169, Accuracy: 0.4285
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.21457629739902,
                        "time": 2.3203780690000713,
                        "accuracy": 0.4285007911392405,
                        "total_cost": 474144.7313119637
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.2580
Profiling... [256/50048]	Loss: 2.1656
Profiling... [384/50048]	Loss: 2.2224
Profiling... [512/50048]	Loss: 2.1390
Profiling... [640/50048]	Loss: 2.0682
Profiling... [768/50048]	Loss: 2.1915
Profiling... [896/50048]	Loss: 2.0144
Profiling... [1024/50048]	Loss: 2.2631
Profiling... [1152/50048]	Loss: 2.2767
Profiling... [1280/50048]	Loss: 2.3512
Profiling... [1408/50048]	Loss: 2.0935
Profiling... [1536/50048]	Loss: 2.2039
Profiling... [1664/50048]	Loss: 1.9302
Profile done
epoch 1 train time consumed: 4.05s
Validation Epoch: 6, Average loss: 0.0170, Accuracy: 0.4212
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.18912067952732,
                        "time": 2.767862039000647,
                        "accuracy": 0.42118275316455694,
                        "total_cost": 575410.2610159924
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.1912
Profiling... [256/50048]	Loss: 2.1748
Profiling... [384/50048]	Loss: 2.0028
Profiling... [512/50048]	Loss: 1.9106
Profiling... [640/50048]	Loss: 2.3483
Profiling... [768/50048]	Loss: 1.9960
Profiling... [896/50048]	Loss: 2.2073
Profiling... [1024/50048]	Loss: 2.2014
Profiling... [1152/50048]	Loss: 2.3079
Profiling... [1280/50048]	Loss: 2.0194
Profiling... [1408/50048]	Loss: 2.3414
Profiling... [1536/50048]	Loss: 2.3656
Profiling... [1664/50048]	Loss: 2.2038
Profile done
epoch 1 train time consumed: 3.42s
Validation Epoch: 6, Average loss: 0.0170, Accuracy: 0.4256
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.15810830755795,
                        "time": 2.185929982999369,
                        "accuracy": 0.42563291139240506,
                        "total_cost": 449681.1783824232
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.3003
Profiling... [256/50048]	Loss: 2.1051
Profiling... [384/50048]	Loss: 2.3316
Profiling... [512/50048]	Loss: 2.4094
Profiling... [640/50048]	Loss: 2.3458
Profiling... [768/50048]	Loss: 2.2013
Profiling... [896/50048]	Loss: 2.2025
Profiling... [1024/50048]	Loss: 2.0292
Profiling... [1152/50048]	Loss: 2.1114
Profiling... [1280/50048]	Loss: 1.9993
Profiling... [1408/50048]	Loss: 2.2145
Profiling... [1536/50048]	Loss: 2.0688
Profiling... [1664/50048]	Loss: 1.9386
Profile done
epoch 1 train time consumed: 3.41s
Validation Epoch: 6, Average loss: 0.0171, Accuracy: 0.4217
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.17615437635187,
                        "time": 2.1917110180002055,
                        "accuracy": 0.42167721518987344,
                        "total_cost": 455100.0315108829
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.1653
Profiling... [256/50048]	Loss: 2.1242
Profiling... [384/50048]	Loss: 2.1122
Profiling... [512/50048]	Loss: 2.1200
Profiling... [640/50048]	Loss: 2.1302
Profiling... [768/50048]	Loss: 2.1097
Profiling... [896/50048]	Loss: 2.2478
Profiling... [1024/50048]	Loss: 2.4218
Profiling... [1152/50048]	Loss: 2.3927
Profiling... [1280/50048]	Loss: 2.1861
Profiling... [1408/50048]	Loss: 2.0629
Profiling... [1536/50048]	Loss: 2.3741
Profiling... [1664/50048]	Loss: 2.2954
Profile done
epoch 1 train time consumed: 3.51s
Validation Epoch: 6, Average loss: 0.0168, Accuracy: 0.4275
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.18677779750402,
                        "time": 2.3303388089998407,
                        "accuracy": 0.4275118670886076,
                        "total_cost": 477281.53130333073
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.1798
Profiling... [256/50048]	Loss: 2.3509
Profiling... [384/50048]	Loss: 2.1106
Profiling... [512/50048]	Loss: 2.1579
Profiling... [640/50048]	Loss: 2.0677
Profiling... [768/50048]	Loss: 1.9688
Profiling... [896/50048]	Loss: 2.0908
Profiling... [1024/50048]	Loss: 2.2478
Profiling... [1152/50048]	Loss: 2.0338
Profiling... [1280/50048]	Loss: 2.1539
Profiling... [1408/50048]	Loss: 2.2627
Profiling... [1536/50048]	Loss: 2.0981
Profiling... [1664/50048]	Loss: 2.1991
Profile done
epoch 1 train time consumed: 4.08s
Validation Epoch: 6, Average loss: 0.0171, Accuracy: 0.4241
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.16462411645412,
                        "time": 2.7673552659998677,
                        "accuracy": 0.4241495253164557,
                        "total_cost": 571280.7789169117
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.0479
Profiling... [256/50048]	Loss: 2.2188
Profiling... [384/50048]	Loss: 2.1120
Profiling... [512/50048]	Loss: 2.2318
Profiling... [640/50048]	Loss: 2.1151
Profiling... [768/50048]	Loss: 2.1444
Profiling... [896/50048]	Loss: 2.0921
Profiling... [1024/50048]	Loss: 2.2534
Profiling... [1152/50048]	Loss: 2.0753
Profiling... [1280/50048]	Loss: 2.0323
Profiling... [1408/50048]	Loss: 2.0328
Profiling... [1536/50048]	Loss: 2.1138
Profiling... [1664/50048]	Loss: 1.8102
Profile done
epoch 1 train time consumed: 3.43s
Validation Epoch: 6, Average loss: 0.0169, Accuracy: 0.4289
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.13434099004616,
                        "time": 2.184976190999805,
                        "accuracy": 0.42889636075949367,
                        "total_cost": 446064.8004170434
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.1991
Profiling... [256/50048]	Loss: 1.8562
Profiling... [384/50048]	Loss: 2.1449
Profiling... [512/50048]	Loss: 2.0837
Profiling... [640/50048]	Loss: 2.0926
Profiling... [768/50048]	Loss: 2.3012
Profiling... [896/50048]	Loss: 2.1263
Profiling... [1024/50048]	Loss: 2.0408
Profiling... [1152/50048]	Loss: 2.1120
Profiling... [1280/50048]	Loss: 2.2182
Profiling... [1408/50048]	Loss: 1.9757
Profiling... [1536/50048]	Loss: 2.2861
Profiling... [1664/50048]	Loss: 2.1012
Profile done
epoch 1 train time consumed: 3.38s
Validation Epoch: 6, Average loss: 0.0171, Accuracy: 0.4266
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.15233473939514,
                        "time": 2.185302693000267,
                        "accuracy": 0.426621835443038,
                        "total_cost": 448510.04261844215
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.4714
Profiling... [256/50048]	Loss: 2.3089
Profiling... [384/50048]	Loss: 2.1410
Profiling... [512/50048]	Loss: 1.9772
Profiling... [640/50048]	Loss: 2.1004
Profiling... [768/50048]	Loss: 2.0873
Profiling... [896/50048]	Loss: 2.0654
Profiling... [1024/50048]	Loss: 2.0127
Profiling... [1152/50048]	Loss: 2.2395
Profiling... [1280/50048]	Loss: 2.0968
Profiling... [1408/50048]	Loss: 1.9438
Profiling... [1536/50048]	Loss: 2.1229
Profiling... [1664/50048]	Loss: 2.2264
Profile done
epoch 1 train time consumed: 3.62s
Validation Epoch: 6, Average loss: 0.0168, Accuracy: 0.4294
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.16472523168352,
                        "time": 2.332262812999943,
                        "accuracy": 0.4293908227848101,
                        "total_cost": 475585.28741187317
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.9906
Profiling... [256/50048]	Loss: 2.1457
Profiling... [384/50048]	Loss: 2.0812
Profiling... [512/50048]	Loss: 2.1158
Profiling... [640/50048]	Loss: 1.8394
Profiling... [768/50048]	Loss: 2.2715
Profiling... [896/50048]	Loss: 1.9932
Profiling... [1024/50048]	Loss: 2.1758
Profiling... [1152/50048]	Loss: 2.1094
Profiling... [1280/50048]	Loss: 2.3111
Profiling... [1408/50048]	Loss: 1.9671
Profiling... [1536/50048]	Loss: 2.1871
Profiling... [1664/50048]	Loss: 2.1294
Profile done
epoch 1 train time consumed: 4.10s
Validation Epoch: 6, Average loss: 0.0171, Accuracy: 0.4239
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.1407292465231,
                        "time": 2.7417983280001863,
                        "accuracy": 0.42385284810126583,
                        "total_cost": 566401.0155920472
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.2118
Profiling... [256/50048]	Loss: 2.1995
Profiling... [384/50048]	Loss: 2.0393
Profiling... [512/50048]	Loss: 2.3143
Profiling... [640/50048]	Loss: 2.4603
Profiling... [768/50048]	Loss: 2.1910
Profiling... [896/50048]	Loss: 2.5249
Profiling... [1024/50048]	Loss: 2.3855
Profiling... [1152/50048]	Loss: 2.4102
Profiling... [1280/50048]	Loss: 2.7786
Profiling... [1408/50048]	Loss: 2.7079
Profiling... [1536/50048]	Loss: 2.5231
Profiling... [1664/50048]	Loss: 2.3186
Profile done
epoch 1 train time consumed: 3.42s
Validation Epoch: 6, Average loss: 0.0203, Accuracy: 0.3460
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.10841833975479,
                        "time": 2.1843643750007686,
                        "accuracy": 0.3460245253164557,
                        "total_cost": 552741.0831083178
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.2639
Profiling... [256/50048]	Loss: 2.3158
Profiling... [384/50048]	Loss: 2.2548
Profiling... [512/50048]	Loss: 2.4199
Profiling... [640/50048]	Loss: 2.4792
Profiling... [768/50048]	Loss: 2.3907
Profiling... [896/50048]	Loss: 2.4415
Profiling... [1024/50048]	Loss: 2.3192
Profiling... [1152/50048]	Loss: 2.5235
Profiling... [1280/50048]	Loss: 2.1464
Profiling... [1408/50048]	Loss: 2.5636
Profiling... [1536/50048]	Loss: 2.2292
Profiling... [1664/50048]	Loss: 2.5425
Profile done
epoch 1 train time consumed: 3.36s
Validation Epoch: 6, Average loss: 0.0198, Accuracy: 0.3604
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.12615607925201,
                        "time": 2.188610990999223,
                        "accuracy": 0.3603639240506329,
                        "total_cost": 531778.5974956854
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.0738
Profiling... [256/50048]	Loss: 2.3652
Profiling... [384/50048]	Loss: 2.5850
Profiling... [512/50048]	Loss: 2.4659
Profiling... [640/50048]	Loss: 2.1583
Profiling... [768/50048]	Loss: 2.6245
Profiling... [896/50048]	Loss: 2.1839
Profiling... [1024/50048]	Loss: 2.3805
Profiling... [1152/50048]	Loss: 2.3732
Profiling... [1280/50048]	Loss: 2.4013
Profiling... [1408/50048]	Loss: 2.1906
Profiling... [1536/50048]	Loss: 2.3953
Profiling... [1664/50048]	Loss: 2.3654
Profile done
epoch 1 train time consumed: 3.45s
Validation Epoch: 6, Average loss: 0.0199, Accuracy: 0.3524
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.13723611620294,
                        "time": 2.3047344549995614,
                        "accuracy": 0.35235363924050633,
                        "total_cost": 572724.4795709148
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.0673
Profiling... [256/50048]	Loss: 2.3387
Profiling... [384/50048]	Loss: 2.1499
Profiling... [512/50048]	Loss: 2.4089
Profiling... [640/50048]	Loss: 2.5611
Profiling... [768/50048]	Loss: 2.3701
Profiling... [896/50048]	Loss: 2.1190
Profiling... [1024/50048]	Loss: 2.4877
Profiling... [1152/50048]	Loss: 2.3789
Profiling... [1280/50048]	Loss: 2.5893
Profiling... [1408/50048]	Loss: 2.5179
Profiling... [1536/50048]	Loss: 2.8253
Profiling... [1664/50048]	Loss: 2.4778
Profile done
epoch 1 train time consumed: 4.00s
Validation Epoch: 6, Average loss: 0.0198, Accuracy: 0.3608
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.11410471200121,
                        "time": 2.7218728559992087,
                        "accuracy": 0.36075949367088606,
                        "total_cost": 660622.8964317765
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.9695
Profiling... [256/50048]	Loss: 2.3940
Profiling... [384/50048]	Loss: 2.0585
Profiling... [512/50048]	Loss: 2.3800
Profiling... [640/50048]	Loss: 2.3563
Profiling... [768/50048]	Loss: 2.3977
Profiling... [896/50048]	Loss: 2.7026
Profiling... [1024/50048]	Loss: 2.2693
Profiling... [1152/50048]	Loss: 2.6745
Profiling... [1280/50048]	Loss: 2.6190
Profiling... [1408/50048]	Loss: 2.1350
Profiling... [1536/50048]	Loss: 2.5245
Profiling... [1664/50048]	Loss: 2.3191
Profile done
epoch 1 train time consumed: 3.51s
Validation Epoch: 6, Average loss: 0.0201, Accuracy: 0.3595
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.08530220329227,
                        "time": 2.1802052980001463,
                        "accuracy": 0.3594738924050633,
                        "total_cost": 531047.6861092701
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.3478
Profiling... [256/50048]	Loss: 2.0691
Profiling... [384/50048]	Loss: 2.3127
Profiling... [512/50048]	Loss: 2.2120
Profiling... [640/50048]	Loss: 2.4589
Profiling... [768/50048]	Loss: 2.5663
Profiling... [896/50048]	Loss: 2.5391
Profiling... [1024/50048]	Loss: 2.4791
Profiling... [1152/50048]	Loss: 2.3506
Profiling... [1280/50048]	Loss: 2.6479
Profiling... [1408/50048]	Loss: 2.4160
Profiling... [1536/50048]	Loss: 2.3870
Profiling... [1664/50048]	Loss: 2.1803
Profile done
epoch 1 train time consumed: 3.44s
Validation Epoch: 6, Average loss: 0.0217, Accuracy: 0.3248
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.10069475495133,
                        "time": 2.179239979000158,
                        "accuracy": 0.3247626582278481,
                        "total_cost": 587546.8371317824
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.1359
Profiling... [256/50048]	Loss: 2.3344
Profiling... [384/50048]	Loss: 2.2964
Profiling... [512/50048]	Loss: 2.0609
Profiling... [640/50048]	Loss: 2.1875
Profiling... [768/50048]	Loss: 2.5428
Profiling... [896/50048]	Loss: 2.2494
Profiling... [1024/50048]	Loss: 2.2690
Profiling... [1152/50048]	Loss: 2.5771
Profiling... [1280/50048]	Loss: 2.5813
Profiling... [1408/50048]	Loss: 2.3979
Profiling... [1536/50048]	Loss: 2.2107
Profiling... [1664/50048]	Loss: 2.4449
Profile done
epoch 1 train time consumed: 3.61s
Validation Epoch: 6, Average loss: 0.0193, Accuracy: 0.3653
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.11048370502331,
                        "time": 2.2977510660002736,
                        "accuracy": 0.36530854430379744,
                        "total_cost": 550740.0922661229
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.1859
Profiling... [256/50048]	Loss: 2.3729
Profiling... [384/50048]	Loss: 2.4660
Profiling... [512/50048]	Loss: 2.4726
Profiling... [640/50048]	Loss: 2.3815
Profiling... [768/50048]	Loss: 2.4235
Profiling... [896/50048]	Loss: 2.4382
Profiling... [1024/50048]	Loss: 2.5505
Profiling... [1152/50048]	Loss: 2.5581
Profiling... [1280/50048]	Loss: 2.2603
Profiling... [1408/50048]	Loss: 2.4843
Profiling... [1536/50048]	Loss: 2.3969
Profiling... [1664/50048]	Loss: 2.3614
Profile done
epoch 1 train time consumed: 4.05s
Validation Epoch: 6, Average loss: 0.0210, Accuracy: 0.3353
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.08589722870772,
                        "time": 2.7557112929998766,
                        "accuracy": 0.3353441455696203,
                        "total_cost": 719525.9690714079
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.1370
Profiling... [256/50048]	Loss: 2.2881
Profiling... [384/50048]	Loss: 2.1215
Profiling... [512/50048]	Loss: 2.2256
Profiling... [640/50048]	Loss: 2.5515
Profiling... [768/50048]	Loss: 2.5189
Profiling... [896/50048]	Loss: 2.2113
Profiling... [1024/50048]	Loss: 2.4747
Profiling... [1152/50048]	Loss: 2.5286
Profiling... [1280/50048]	Loss: 2.4161
Profiling... [1408/50048]	Loss: 2.4377
Profiling... [1536/50048]	Loss: 2.5441
Profiling... [1664/50048]	Loss: 2.4645
Profile done
epoch 1 train time consumed: 3.42s
Validation Epoch: 6, Average loss: 0.0205, Accuracy: 0.3495
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.05705228281538,
                        "time": 2.169656070000201,
                        "accuracy": 0.3494857594936709,
                        "total_cost": 543581.6979448023
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.1561
Profiling... [256/50048]	Loss: 2.2254
Profiling... [384/50048]	Loss: 2.1946
Profiling... [512/50048]	Loss: 2.1681
Profiling... [640/50048]	Loss: 2.3878
Profiling... [768/50048]	Loss: 2.4772
Profiling... [896/50048]	Loss: 2.3013
Profiling... [1024/50048]	Loss: 2.3268
Profiling... [1152/50048]	Loss: 2.4541
Profiling... [1280/50048]	Loss: 2.4913
Profiling... [1408/50048]	Loss: 2.8254
Profiling... [1536/50048]	Loss: 2.3895
Profiling... [1664/50048]	Loss: 2.7018
Profile done
epoch 1 train time consumed: 3.44s
Validation Epoch: 6, Average loss: 0.0193, Accuracy: 0.3610
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.0732358268432,
                        "time": 2.1727439579999555,
                        "accuracy": 0.36095727848101267,
                        "total_cost": 527055.320653008
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.1277
Profiling... [256/50048]	Loss: 2.5094
Profiling... [384/50048]	Loss: 2.2053
Profiling... [512/50048]	Loss: 2.6269
Profiling... [640/50048]	Loss: 2.4702
Profiling... [768/50048]	Loss: 2.6021
Profiling... [896/50048]	Loss: 2.4129
Profiling... [1024/50048]	Loss: 2.3142
Profiling... [1152/50048]	Loss: 2.5940
Profiling... [1280/50048]	Loss: 2.2402
Profiling... [1408/50048]	Loss: 2.6341
Profiling... [1536/50048]	Loss: 2.1930
Profiling... [1664/50048]	Loss: 2.2678
Profile done
epoch 1 train time consumed: 3.45s
Validation Epoch: 6, Average loss: 0.0196, Accuracy: 0.3594
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.08252675874135,
                        "time": 2.299950173999605,
                        "accuracy": 0.359375,
                        "total_cost": 560368.9242825321
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.0049
Profiling... [256/50048]	Loss: 2.2787
Profiling... [384/50048]	Loss: 2.1954
Profiling... [512/50048]	Loss: 2.0617
Profiling... [640/50048]	Loss: 2.4540
Profiling... [768/50048]	Loss: 2.1992
Profiling... [896/50048]	Loss: 2.6568
Profiling... [1024/50048]	Loss: 2.2715
Profiling... [1152/50048]	Loss: 2.3152
Profiling... [1280/50048]	Loss: 2.3500
Profiling... [1408/50048]	Loss: 2.4153
Profiling... [1536/50048]	Loss: 2.5390
Profiling... [1664/50048]	Loss: 2.4850
Profile done
epoch 1 train time consumed: 3.93s
Validation Epoch: 6, Average loss: 0.0207, Accuracy: 0.3393
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.05897452168348,
                        "time": 2.7384012709999297,
                        "accuracy": 0.3392998417721519,
                        "total_cost": 706670.3172737829
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.9178
Profiling... [256/50048]	Loss: 2.3485
Profiling... [384/50048]	Loss: 2.7612
Profiling... [512/50048]	Loss: 2.8588
Profiling... [640/50048]	Loss: 2.2811
Profiling... [768/50048]	Loss: 2.5700
Profiling... [896/50048]	Loss: 2.9258
Profiling... [1024/50048]	Loss: 3.0047
Profiling... [1152/50048]	Loss: 2.9369
Profiling... [1280/50048]	Loss: 2.9661
Profiling... [1408/50048]	Loss: 2.6224
Profiling... [1536/50048]	Loss: 3.0744
Profiling... [1664/50048]	Loss: 2.6734
Profile done
epoch 1 train time consumed: 3.42s
Validation Epoch: 6, Average loss: 0.0360, Accuracy: 0.2101
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.0311717777305,
                        "time": 2.1682200240002203,
                        "accuracy": 0.21014636075949367,
                        "total_cost": 903409.8630066661
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.1810
Profiling... [256/50048]	Loss: 2.5604
Profiling... [384/50048]	Loss: 2.5453
Profiling... [512/50048]	Loss: 2.8162
Profiling... [640/50048]	Loss: 2.8362
Profiling... [768/50048]	Loss: 2.8501
Profiling... [896/50048]	Loss: 2.4914
Profiling... [1024/50048]	Loss: 2.7697
Profiling... [1152/50048]	Loss: 2.7598
Profiling... [1280/50048]	Loss: 2.6416
Profiling... [1408/50048]	Loss: 2.5568
Profiling... [1536/50048]	Loss: 2.7627
Profiling... [1664/50048]	Loss: 2.7717
Profile done
epoch 1 train time consumed: 3.50s
Validation Epoch: 6, Average loss: 0.0297, Accuracy: 0.2338
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.04625394177306,
                        "time": 2.17810337300034,
                        "accuracy": 0.23378164556962025,
                        "total_cost": 815777.0136165885
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.1623
Profiling... [256/50048]	Loss: 2.5617
Profiling... [384/50048]	Loss: 2.5582
Profiling... [512/50048]	Loss: 2.6319
Profiling... [640/50048]	Loss: 2.8203
Profiling... [768/50048]	Loss: 2.6845
Profiling... [896/50048]	Loss: 2.5637
Profiling... [1024/50048]	Loss: 2.9750
Profiling... [1152/50048]	Loss: 2.6545
Profiling... [1280/50048]	Loss: 3.1827
Profiling... [1408/50048]	Loss: 2.7302
Profiling... [1536/50048]	Loss: 2.8586
Profiling... [1664/50048]	Loss: 2.7417
Profile done
epoch 1 train time consumed: 3.53s
Validation Epoch: 6, Average loss: 0.0255, Accuracy: 0.2509
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.05362273901957,
                        "time": 2.309823182999935,
                        "accuracy": 0.2508900316455696,
                        "total_cost": 806118.2167935569
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.1728
Profiling... [256/50048]	Loss: 2.2370
Profiling... [384/50048]	Loss: 2.3079
Profiling... [512/50048]	Loss: 2.4050
Profiling... [640/50048]	Loss: 2.6490
Profiling... [768/50048]	Loss: 3.0906
Profiling... [896/50048]	Loss: 2.7862
Profiling... [1024/50048]	Loss: 2.7284
Profiling... [1152/50048]	Loss: 2.6544
Profiling... [1280/50048]	Loss: 2.8429
Profiling... [1408/50048]	Loss: 2.9596
Profiling... [1536/50048]	Loss: 2.5768
Profiling... [1664/50048]	Loss: 2.6467
Profile done
epoch 1 train time consumed: 4.10s
Validation Epoch: 6, Average loss: 0.0276, Accuracy: 0.2335
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.0316060208947,
                        "time": 2.731454347000181,
                        "accuracy": 0.2334849683544304,
                        "total_cost": 1024326.41273211
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.1999
Profiling... [256/50048]	Loss: 2.6005
Profiling... [384/50048]	Loss: 2.4501
Profiling... [512/50048]	Loss: 2.5165
Profiling... [640/50048]	Loss: 2.7805
Profiling... [768/50048]	Loss: 2.6619
Profiling... [896/50048]	Loss: 2.6003
Profiling... [1024/50048]	Loss: 2.8530
Profiling... [1152/50048]	Loss: 2.9412
Profiling... [1280/50048]	Loss: 2.8666
Profiling... [1408/50048]	Loss: 2.8032
Profiling... [1536/50048]	Loss: 2.7105
Profiling... [1664/50048]	Loss: 2.8272
Profile done
epoch 1 train time consumed: 3.47s
Validation Epoch: 6, Average loss: 0.0271, Accuracy: 0.2301
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.00123674628654,
                        "time": 2.187431565999759,
                        "accuracy": 0.23012262658227847,
                        "total_cost": 832297.1904169741
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.9282
Profiling... [256/50048]	Loss: 2.3536
Profiling... [384/50048]	Loss: 2.5137
Profiling... [512/50048]	Loss: 2.6102
Profiling... [640/50048]	Loss: 2.6006
Profiling... [768/50048]	Loss: 2.7588
Profiling... [896/50048]	Loss: 2.6141
Profiling... [1024/50048]	Loss: 2.6176
Profiling... [1152/50048]	Loss: 2.5037
Profiling... [1280/50048]	Loss: 2.8773
Profiling... [1408/50048]	Loss: 2.7330
Profiling... [1536/50048]	Loss: 2.9146
Profiling... [1664/50048]	Loss: 2.7823
Profile done
epoch 1 train time consumed: 3.45s
Validation Epoch: 6, Average loss: 0.0398, Accuracy: 0.2092
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.01756585935686,
                        "time": 2.1898365139995803,
                        "accuracy": 0.20915743670886075,
                        "total_cost": 916730.537999577
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.2409
Profiling... [256/50048]	Loss: 2.2776
Profiling... [384/50048]	Loss: 2.5562
Profiling... [512/50048]	Loss: 2.4423
Profiling... [640/50048]	Loss: 2.5106
Profiling... [768/50048]	Loss: 2.6821
Profiling... [896/50048]	Loss: 2.4846
Profiling... [1024/50048]	Loss: 2.7291
Profiling... [1152/50048]	Loss: 2.5188
Profiling... [1280/50048]	Loss: 3.1696
Profiling... [1408/50048]	Loss: 2.8744
Profiling... [1536/50048]	Loss: 2.5357
Profiling... [1664/50048]	Loss: 2.8939
Profile done
epoch 1 train time consumed: 3.54s
Validation Epoch: 6, Average loss: 0.0277, Accuracy: 0.2396
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.02714168371678,
                        "time": 2.3047361250000904,
                        "accuracy": 0.23961629746835442,
                        "total_cost": 842186.3460301834
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.9625
Profiling... [256/50048]	Loss: 2.5298
Profiling... [384/50048]	Loss: 2.8297
Profiling... [512/50048]	Loss: 2.5583
Profiling... [640/50048]	Loss: 2.7039
Profiling... [768/50048]	Loss: 2.5495
Profiling... [896/50048]	Loss: 2.6257
Profiling... [1024/50048]	Loss: 2.6677
Profiling... [1152/50048]	Loss: 2.5836
Profiling... [1280/50048]	Loss: 2.7574
Profiling... [1408/50048]	Loss: 2.9270
Profiling... [1536/50048]	Loss: 2.9226
Profiling... [1664/50048]	Loss: 2.8641
Profile done
epoch 1 train time consumed: 4.03s
Validation Epoch: 6, Average loss: 0.0394, Accuracy: 0.1683
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.00527482244702,
                        "time": 2.7467390669999077,
                        "accuracy": 0.16831487341772153,
                        "total_cost": 1428888.022179606
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.0526
Profiling... [256/50048]	Loss: 2.7361
Profiling... [384/50048]	Loss: 2.7718
Profiling... [512/50048]	Loss: 2.6753
Profiling... [640/50048]	Loss: 2.7528
Profiling... [768/50048]	Loss: 2.8259
Profiling... [896/50048]	Loss: 2.9883
Profiling... [1024/50048]	Loss: 2.8004
Profiling... [1152/50048]	Loss: 2.9842
Profiling... [1280/50048]	Loss: 2.5697
Profiling... [1408/50048]	Loss: 2.8662
Profiling... [1536/50048]	Loss: 2.6298
Profiling... [1664/50048]	Loss: 2.4258
Profile done
epoch 1 train time consumed: 3.49s
Validation Epoch: 6, Average loss: 0.0406, Accuracy: 0.2086
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.97383461868195,
                        "time": 2.1809002480003983,
                        "accuracy": 0.208564082278481,
                        "total_cost": 915586.7330875029
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.0851
Profiling... [256/50048]	Loss: 2.3556
Profiling... [384/50048]	Loss: 2.6500
Profiling... [512/50048]	Loss: 2.6558
Profiling... [640/50048]	Loss: 1.9667
Profiling... [768/50048]	Loss: 2.9862
Profiling... [896/50048]	Loss: 2.8401
Profiling... [1024/50048]	Loss: 2.6867
Profiling... [1152/50048]	Loss: 2.4584
Profiling... [1280/50048]	Loss: 2.9831
Profiling... [1408/50048]	Loss: 2.8066
Profiling... [1536/50048]	Loss: 2.9270
Profiling... [1664/50048]	Loss: 2.8257
Profile done
epoch 1 train time consumed: 3.37s
Validation Epoch: 6, Average loss: 0.0412, Accuracy: 0.1942
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.98919667949335,
                        "time": 2.1888749150002695,
                        "accuracy": 0.1942246835443038,
                        "total_cost": 986778.7028865949
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.3994
Profiling... [256/50048]	Loss: 2.5605
Profiling... [384/50048]	Loss: 2.5036
Profiling... [512/50048]	Loss: 2.9130
Profiling... [640/50048]	Loss: 2.7416
Profiling... [768/50048]	Loss: 2.8525
Profiling... [896/50048]	Loss: 2.8988
Profiling... [1024/50048]	Loss: 2.6963
Profiling... [1152/50048]	Loss: 2.4359
Profiling... [1280/50048]	Loss: 2.8082
Profiling... [1408/50048]	Loss: 2.8112
Profiling... [1536/50048]	Loss: 2.7476
Profiling... [1664/50048]	Loss: 2.6294
Profile done
epoch 1 train time consumed: 3.49s
Validation Epoch: 6, Average loss: 0.0338, Accuracy: 0.2006
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.00049811592507,
                        "time": 2.3124758959993414,
                        "accuracy": 0.20055379746835442,
                        "total_cost": 1009600.597683244
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.0532
Profiling... [256/50048]	Loss: 2.6564
Profiling... [384/50048]	Loss: 2.3698
Profiling... [512/50048]	Loss: 2.5997
Profiling... [640/50048]	Loss: 2.4904
Profiling... [768/50048]	Loss: 2.8815
Profiling... [896/50048]	Loss: 2.6836
Profiling... [1024/50048]	Loss: 2.8292
Profiling... [1152/50048]	Loss: 2.8135
Profiling... [1280/50048]	Loss: 2.7181
Profiling... [1408/50048]	Loss: 2.7417
Profiling... [1536/50048]	Loss: 3.0950
Profiling... [1664/50048]	Loss: 2.8961
Profile done
epoch 1 train time consumed: 4.07s
Validation Epoch: 6, Average loss: 0.0275, Accuracy: 0.2486
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.97746274131923,
                        "time": 2.7531495279999945,
                        "accuracy": 0.24861550632911392,
                        "total_cost": 969627.23135047
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.0553
Profiling... [512/50176]	Loss: 2.0411
Profiling... [768/50176]	Loss: 2.2444
Profiling... [1024/50176]	Loss: 1.9806
Profiling... [1280/50176]	Loss: 2.2696
Profiling... [1536/50176]	Loss: 2.1066
Profiling... [1792/50176]	Loss: 2.2583
Profiling... [2048/50176]	Loss: 2.2906
Profiling... [2304/50176]	Loss: 2.1893
Profiling... [2560/50176]	Loss: 2.0524
Profiling... [2816/50176]	Loss: 1.9459
Profiling... [3072/50176]	Loss: 2.1391
Profiling... [3328/50176]	Loss: 2.2895
Profile done
epoch 1 train time consumed: 3.81s
Validation Epoch: 6, Average loss: 0.0084, Accuracy: 0.4254
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.95870861002018,
                        "time": 2.4144291650000014,
                        "accuracy": 0.425390625,
                        "total_cost": 496969.48686927336
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.2229
Profiling... [512/50176]	Loss: 2.1259
Profiling... [768/50176]	Loss: 2.0982
Profiling... [1024/50176]	Loss: 2.3549
Profiling... [1280/50176]	Loss: 2.1785
Profiling... [1536/50176]	Loss: 2.0477
Profiling... [1792/50176]	Loss: 2.0881
Profiling... [2048/50176]	Loss: 2.0722
Profiling... [2304/50176]	Loss: 2.1531
Profiling... [2560/50176]	Loss: 2.0796
Profiling... [2816/50176]	Loss: 1.9348
Profiling... [3072/50176]	Loss: 2.1726
Profiling... [3328/50176]	Loss: 2.1113
Profile done
epoch 1 train time consumed: 3.90s
Validation Epoch: 6, Average loss: 0.0084, Accuracy: 0.4244
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.98257531716322,
                        "time": 2.4401274810006726,
                        "accuracy": 0.4244140625,
                        "total_cost": 503414.80123188585
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.1934
Profiling... [512/50176]	Loss: 2.0911
Profiling... [768/50176]	Loss: 2.2463
Profiling... [1024/50176]	Loss: 2.1086
Profiling... [1280/50176]	Loss: 2.0525
Profiling... [1536/50176]	Loss: 2.0521
Profiling... [1792/50176]	Loss: 2.1799
Profiling... [2048/50176]	Loss: 2.2717
Profiling... [2304/50176]	Loss: 2.1068
Profiling... [2560/50176]	Loss: 2.1324
Profiling... [2816/50176]	Loss: 2.0534
Profiling... [3072/50176]	Loss: 2.1227
Profiling... [3328/50176]	Loss: 2.2012
Profile done
epoch 1 train time consumed: 4.10s
Validation Epoch: 6, Average loss: 0.0085, Accuracy: 0.4263
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.9916569060163,
                        "time": 2.660678371999893,
                        "accuracy": 0.42626953125,
                        "total_cost": 546526.6450802183
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.0793
Profiling... [512/50176]	Loss: 2.1555
Profiling... [768/50176]	Loss: 1.8841
Profiling... [1024/50176]	Loss: 2.0582
Profiling... [1280/50176]	Loss: 2.0490
Profiling... [1536/50176]	Loss: 2.3356
Profiling... [1792/50176]	Loss: 2.1757
Profiling... [2048/50176]	Loss: 2.0373
Profiling... [2304/50176]	Loss: 1.9277
Profiling... [2560/50176]	Loss: 2.3500
Profiling... [2816/50176]	Loss: 2.1636
Profiling... [3072/50176]	Loss: 2.1095
Profiling... [3328/50176]	Loss: 2.1897
Profile done
epoch 1 train time consumed: 5.40s
Validation Epoch: 6, Average loss: 0.0084, Accuracy: 0.4246
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.95973682319732,
                        "time": 3.8747112689998175,
                        "accuracy": 0.424609375,
                        "total_cost": 799011.334485719
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.0652
Profiling... [512/50176]	Loss: 2.2201
Profiling... [768/50176]	Loss: 2.2130
Profiling... [1024/50176]	Loss: 2.1846
Profiling... [1280/50176]	Loss: 2.1960
Profiling... [1536/50176]	Loss: 2.1458
Profiling... [1792/50176]	Loss: 2.2735
Profiling... [2048/50176]	Loss: 1.9174
Profiling... [2304/50176]	Loss: 2.1279
Profiling... [2560/50176]	Loss: 2.3243
Profiling... [2816/50176]	Loss: 2.0228
Profiling... [3072/50176]	Loss: 2.0721
Profiling... [3328/50176]	Loss: 2.0552
Profile done
epoch 1 train time consumed: 3.80s
Validation Epoch: 6, Average loss: 0.0083, Accuracy: 0.4265
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.93423637006885,
                        "time": 2.427899026999512,
                        "accuracy": 0.42646484375,
                        "total_cost": 498483.1648759727
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.0624
Profiling... [512/50176]	Loss: 1.9708
Profiling... [768/50176]	Loss: 2.0663
Profiling... [1024/50176]	Loss: 2.2828
Profiling... [1280/50176]	Loss: 2.2128
Profiling... [1536/50176]	Loss: 2.0957
Profiling... [1792/50176]	Loss: 2.0239
Profiling... [2048/50176]	Loss: 2.1276
Profiling... [2304/50176]	Loss: 2.0723
Profiling... [2560/50176]	Loss: 2.1875
Profiling... [2816/50176]	Loss: 2.0958
Profiling... [3072/50176]	Loss: 2.1046
Profiling... [3328/50176]	Loss: 2.1506
Profile done
epoch 1 train time consumed: 3.89s
Validation Epoch: 6, Average loss: 0.0083, Accuracy: 0.4318
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.95892448491743,
                        "time": 2.420235562000016,
                        "accuracy": 0.4318359375,
                        "total_cost": 490729.343212037
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.0856
Profiling... [512/50176]	Loss: 2.0791
Profiling... [768/50176]	Loss: 2.2098
Profiling... [1024/50176]	Loss: 2.1384
Profiling... [1280/50176]	Loss: 2.0533
Profiling... [1536/50176]	Loss: 2.1505
Profiling... [1792/50176]	Loss: 2.2517
Profiling... [2048/50176]	Loss: 2.2088
Profiling... [2304/50176]	Loss: 2.0377
Profiling... [2560/50176]	Loss: 2.1007
Profiling... [2816/50176]	Loss: 2.3008
Profiling... [3072/50176]	Loss: 2.1800
Profiling... [3328/50176]	Loss: 1.9619
Profile done
epoch 1 train time consumed: 4.04s
Validation Epoch: 6, Average loss: 0.0084, Accuracy: 0.4247
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.96871934903406,
                        "time": 2.6513315760003024,
                        "accuracy": 0.42470703125,
                        "total_cost": 546610.2715037395
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.2315
Profiling... [512/50176]	Loss: 1.8559
Profiling... [768/50176]	Loss: 2.1796
Profiling... [1024/50176]	Loss: 2.0583
Profiling... [1280/50176]	Loss: 2.1247
Profiling... [1536/50176]	Loss: 2.3570
Profiling... [1792/50176]	Loss: 1.9227
Profiling... [2048/50176]	Loss: 2.2902
Profiling... [2304/50176]	Loss: 2.1342
Profiling... [2560/50176]	Loss: 2.1984
Profiling... [2816/50176]	Loss: 2.1026
Profiling... [3072/50176]	Loss: 2.1548
Profiling... [3328/50176]	Loss: 2.0670
Profile done
epoch 1 train time consumed: 6.10s
Validation Epoch: 6, Average loss: 0.0084, Accuracy: 0.4286
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.94416852998174,
                        "time": 3.874549326000306,
                        "accuracy": 0.42861328125,
                        "total_cost": 791514.1886402558
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.9108
Profiling... [512/50176]	Loss: 2.3476
Profiling... [768/50176]	Loss: 2.2069
Profiling... [1024/50176]	Loss: 2.1111
Profiling... [1280/50176]	Loss: 2.1489
Profiling... [1536/50176]	Loss: 2.0681
Profiling... [1792/50176]	Loss: 2.1008
Profiling... [2048/50176]	Loss: 2.1871
Profiling... [2304/50176]	Loss: 2.1665
Profiling... [2560/50176]	Loss: 2.0892
Profiling... [2816/50176]	Loss: 2.0565
Profiling... [3072/50176]	Loss: 2.1395
Profiling... [3328/50176]	Loss: 2.2047
Profile done
epoch 1 train time consumed: 3.80s
Validation Epoch: 6, Average loss: 0.0084, Accuracy: 0.4225
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.90891029025204,
                        "time": 2.4163433650001025,
                        "accuracy": 0.4224609375,
                        "total_cost": 500812.47290637036
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.9920
Profiling... [512/50176]	Loss: 1.9550
Profiling... [768/50176]	Loss: 2.1072
Profiling... [1024/50176]	Loss: 1.9710
Profiling... [1280/50176]	Loss: 2.1327
Profiling... [1536/50176]	Loss: 2.0413
Profiling... [1792/50176]	Loss: 2.0284
Profiling... [2048/50176]	Loss: 1.9806
Profiling... [2304/50176]	Loss: 2.1455
Profiling... [2560/50176]	Loss: 2.3425
Profiling... [2816/50176]	Loss: 2.0551
Profiling... [3072/50176]	Loss: 2.2195
Profiling... [3328/50176]	Loss: 2.0802
Profile done
epoch 1 train time consumed: 3.84s
Validation Epoch: 6, Average loss: 0.0084, Accuracy: 0.4272
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.93104472518255,
                        "time": 2.423442222999256,
                        "accuracy": 0.42724609375,
                        "total_cost": 496658.2699625703
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.0689
Profiling... [512/50176]	Loss: 2.2170
Profiling... [768/50176]	Loss: 2.0673
Profiling... [1024/50176]	Loss: 2.0625
Profiling... [1280/50176]	Loss: 2.0859
Profiling... [1536/50176]	Loss: 2.0745
Profiling... [1792/50176]	Loss: 2.0158
Profiling... [2048/50176]	Loss: 2.2025
Profiling... [2304/50176]	Loss: 2.0925
Profiling... [2560/50176]	Loss: 2.1506
Profiling... [2816/50176]	Loss: 2.1139
Profiling... [3072/50176]	Loss: 2.1589
Profiling... [3328/50176]	Loss: 2.0712
Profile done
epoch 1 train time consumed: 4.03s
Validation Epoch: 6, Average loss: 0.0085, Accuracy: 0.4200
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.94506713466347,
                        "time": 2.6392353990004267,
                        "accuracy": 0.42001953125,
                        "total_cost": 550188.8417918576
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.9893
Profiling... [512/50176]	Loss: 2.2688
Profiling... [768/50176]	Loss: 2.3504
Profiling... [1024/50176]	Loss: 2.2997
Profiling... [1280/50176]	Loss: 2.1356
Profiling... [1536/50176]	Loss: 2.1431
Profiling... [1792/50176]	Loss: 2.1401
Profiling... [2048/50176]	Loss: 2.1297
Profiling... [2304/50176]	Loss: 2.1631
Profiling... [2560/50176]	Loss: 2.3012
Profiling... [2816/50176]	Loss: 2.1307
Profiling... [3072/50176]	Loss: 1.9776
Profiling... [3328/50176]	Loss: 2.0340
Profile done
epoch 1 train time consumed: 4.63s
Validation Epoch: 6, Average loss: 0.0083, Accuracy: 0.4293
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.91716998991251,
                        "time": 3.206611245000204,
                        "accuracy": 0.429296875,
                        "total_cost": 654020.9371539786
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.1132
Profiling... [512/50176]	Loss: 2.1028
Profiling... [768/50176]	Loss: 2.3090
Profiling... [1024/50176]	Loss: 2.2781
Profiling... [1280/50176]	Loss: 2.4112
Profiling... [1536/50176]	Loss: 2.4029
Profiling... [1792/50176]	Loss: 2.3764
Profiling... [2048/50176]	Loss: 2.4034
Profiling... [2304/50176]	Loss: 2.2465
Profiling... [2560/50176]	Loss: 2.2764
Profiling... [2816/50176]	Loss: 2.2808
Profiling... [3072/50176]	Loss: 2.1582
Profiling... [3328/50176]	Loss: 2.5173
Profile done
epoch 1 train time consumed: 3.80s
Validation Epoch: 6, Average loss: 0.0094, Accuracy: 0.3657
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.8961990993149,
                        "time": 2.410780430000159,
                        "accuracy": 0.36572265625,
                        "total_cost": 577176.7221216799
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.2855
Profiling... [512/50176]	Loss: 2.2303
Profiling... [768/50176]	Loss: 2.3206
Profiling... [1024/50176]	Loss: 2.1116
Profiling... [1280/50176]	Loss: 2.4236
Profiling... [1536/50176]	Loss: 2.1773
Profiling... [1792/50176]	Loss: 2.4613
Profiling... [2048/50176]	Loss: 2.4122
Profiling... [2304/50176]	Loss: 2.3973
Profiling... [2560/50176]	Loss: 2.3358
Profiling... [2816/50176]	Loss: 2.2466
Profiling... [3072/50176]	Loss: 2.2731
Profiling... [3328/50176]	Loss: 2.2398
Profile done
epoch 1 train time consumed: 3.90s
Validation Epoch: 6, Average loss: 0.0096, Accuracy: 0.3606
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.9206928744662,
                        "time": 2.4286629299995184,
                        "accuracy": 0.36064453125,
                        "total_cost": 589645.4738329125
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.1260
Profiling... [512/50176]	Loss: 1.9345
Profiling... [768/50176]	Loss: 2.4088
Profiling... [1024/50176]	Loss: 2.2323
Profiling... [1280/50176]	Loss: 2.1769
Profiling... [1536/50176]	Loss: 2.2133
Profiling... [1792/50176]	Loss: 2.2686
Profiling... [2048/50176]	Loss: 2.2860
Profiling... [2304/50176]	Loss: 2.4211
Profiling... [2560/50176]	Loss: 2.4822
Profiling... [2816/50176]	Loss: 2.2392
Profiling... [3072/50176]	Loss: 2.0384
Profiling... [3328/50176]	Loss: 2.2879
Profile done
epoch 1 train time consumed: 4.02s
Validation Epoch: 6, Average loss: 0.0097, Accuracy: 0.3621
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.92973580788588,
                        "time": 2.6483254020004097,
                        "accuracy": 0.362109375,
                        "total_cost": 640375.4528455178
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.1597
Profiling... [512/50176]	Loss: 2.1491
Profiling... [768/50176]	Loss: 2.1193
Profiling... [1024/50176]	Loss: 2.3641
Profiling... [1280/50176]	Loss: 2.3593
Profiling... [1536/50176]	Loss: 2.2302
Profiling... [1792/50176]	Loss: 2.2581
Profiling... [2048/50176]	Loss: 2.2452
Profiling... [2304/50176]	Loss: 2.2445
Profiling... [2560/50176]	Loss: 2.2941
Profiling... [2816/50176]	Loss: 2.4316
Profiling... [3072/50176]	Loss: 2.3381
Profiling... [3328/50176]	Loss: 2.2531
Profile done
epoch 1 train time consumed: 4.74s
Validation Epoch: 6, Average loss: 0.0104, Accuracy: 0.3390
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.90398086418001,
                        "time": 3.2038857269999426,
                        "accuracy": 0.33896484375,
                        "total_cost": 827609.3632382253
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.9452
Profiling... [512/50176]	Loss: 1.9883
Profiling... [768/50176]	Loss: 2.2032
Profiling... [1024/50176]	Loss: 2.4067
Profiling... [1280/50176]	Loss: 2.2387
Profiling... [1536/50176]	Loss: 2.3283
Profiling... [1792/50176]	Loss: 2.4005
Profiling... [2048/50176]	Loss: 2.4718
Profiling... [2304/50176]	Loss: 2.5844
Profiling... [2560/50176]	Loss: 2.3466
Profiling... [2816/50176]	Loss: 2.3033
Profiling... [3072/50176]	Loss: 2.4836
Profiling... [3328/50176]	Loss: 2.2896
Profile done
epoch 1 train time consumed: 3.88s
Validation Epoch: 6, Average loss: 0.0095, Accuracy: 0.3787
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.8823117483151,
                        "time": 2.4050531390003016,
                        "accuracy": 0.3787109375,
                        "total_cost": 556057.6364421673
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.0221
Profiling... [512/50176]	Loss: 2.2642
Profiling... [768/50176]	Loss: 2.4141
Profiling... [1024/50176]	Loss: 2.3111
Profiling... [1280/50176]	Loss: 2.3491
Profiling... [1536/50176]	Loss: 2.3979
Profiling... [1792/50176]	Loss: 2.2827
Profiling... [2048/50176]	Loss: 2.2558
Profiling... [2304/50176]	Loss: 2.4506
Profiling... [2560/50176]	Loss: 2.4983
Profiling... [2816/50176]	Loss: 2.2374
Profiling... [3072/50176]	Loss: 2.2878
Profiling... [3328/50176]	Loss: 2.3098
Profile done
epoch 1 train time consumed: 3.91s
Validation Epoch: 6, Average loss: 0.0096, Accuracy: 0.3614
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.90796415610238,
                        "time": 2.418220344000474,
                        "accuracy": 0.36142578125,
                        "total_cost": 585841.0326920596
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.1039
Profiling... [512/50176]	Loss: 2.1714
Profiling... [768/50176]	Loss: 2.2983
Profiling... [1024/50176]	Loss: 2.4190
Profiling... [1280/50176]	Loss: 2.3228
Profiling... [1536/50176]	Loss: 2.3850
Profiling... [1792/50176]	Loss: 2.3382
Profiling... [2048/50176]	Loss: 2.4688
Profiling... [2304/50176]	Loss: 2.3766
Profiling... [2560/50176]	Loss: 2.2813
Profiling... [2816/50176]	Loss: 2.2061
Profiling... [3072/50176]	Loss: 2.3784
Profiling... [3328/50176]	Loss: 2.5242
Profile done
epoch 1 train time consumed: 4.05s
Validation Epoch: 6, Average loss: 0.0091, Accuracy: 0.3877
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.91776239554311,
                        "time": 2.650029500000528,
                        "accuracy": 0.3876953125,
                        "total_cost": 598498.7219551625
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.3118
Profiling... [512/50176]	Loss: 2.1473
Profiling... [768/50176]	Loss: 2.1688
Profiling... [1024/50176]	Loss: 2.5651
Profiling... [1280/50176]	Loss: 2.2903
Profiling... [1536/50176]	Loss: 2.3817
Profiling... [1792/50176]	Loss: 2.2961
Profiling... [2048/50176]	Loss: 2.1509
Profiling... [2304/50176]	Loss: 2.3937
Profiling... [2560/50176]	Loss: 2.3673
Profiling... [2816/50176]	Loss: 2.3860
Profiling... [3072/50176]	Loss: 2.4660
Profiling... [3328/50176]	Loss: 2.4556
Profile done
epoch 1 train time consumed: 4.75s
Validation Epoch: 6, Average loss: 0.0101, Accuracy: 0.3413
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.89002296008609,
                        "time": 3.221038630000294,
                        "accuracy": 0.34130859375,
                        "total_cost": 826326.5559903991
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.1430
Profiling... [512/50176]	Loss: 2.1315
Profiling... [768/50176]	Loss: 2.4205
Profiling... [1024/50176]	Loss: 2.1865
Profiling... [1280/50176]	Loss: 2.4311
Profiling... [1536/50176]	Loss: 2.3737
Profiling... [1792/50176]	Loss: 2.2324
Profiling... [2048/50176]	Loss: 2.3687
Profiling... [2304/50176]	Loss: 2.1684
Profiling... [2560/50176]	Loss: 2.4255
Profiling... [2816/50176]	Loss: 2.2147
Profiling... [3072/50176]	Loss: 2.3234
Profiling... [3328/50176]	Loss: 2.2108
Profile done
epoch 1 train time consumed: 3.88s
Validation Epoch: 6, Average loss: 0.0098, Accuracy: 0.3531
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.86872471636875,
                        "time": 2.4059324219997507,
                        "accuracy": 0.353125,
                        "total_cost": 596565.1879203018
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.1935
Profiling... [512/50176]	Loss: 2.2678
Profiling... [768/50176]	Loss: 2.3014
Profiling... [1024/50176]	Loss: 2.3142
Profiling... [1280/50176]	Loss: 2.1832
Profiling... [1536/50176]	Loss: 2.2013
Profiling... [1792/50176]	Loss: 2.0714
Profiling... [2048/50176]	Loss: 2.1818
Profiling... [2304/50176]	Loss: 2.2613
Profiling... [2560/50176]	Loss: 2.3978
Profiling... [2816/50176]	Loss: 2.1664
Profiling... [3072/50176]	Loss: 2.2972
Profiling... [3328/50176]	Loss: 2.2343
Profile done
epoch 1 train time consumed: 3.89s
Validation Epoch: 6, Average loss: 0.0096, Accuracy: 0.3689
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.89011636287795,
                        "time": 2.4200695549998272,
                        "accuracy": 0.3689453125,
                        "total_cost": 574339.7193533527
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.0838
Profiling... [512/50176]	Loss: 2.2971
Profiling... [768/50176]	Loss: 2.2193
Profiling... [1024/50176]	Loss: 2.2645
Profiling... [1280/50176]	Loss: 2.1186
Profiling... [1536/50176]	Loss: 2.1691
Profiling... [1792/50176]	Loss: 2.3078
Profiling... [2048/50176]	Loss: 2.3221
Profiling... [2304/50176]	Loss: 2.4269
Profiling... [2560/50176]	Loss: 2.4728
Profiling... [2816/50176]	Loss: 2.2113
Profiling... [3072/50176]	Loss: 2.1758
Profiling... [3328/50176]	Loss: 2.4033
Profile done
epoch 1 train time consumed: 4.07s
Validation Epoch: 6, Average loss: 0.0096, Accuracy: 0.3716
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.90201091614793,
                        "time": 2.6404668079994735,
                        "accuracy": 0.37158203125,
                        "total_cost": 622198.612050265
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.0644
Profiling... [512/50176]	Loss: 2.1278
Profiling... [768/50176]	Loss: 2.3803
Profiling... [1024/50176]	Loss: 2.3402
Profiling... [1280/50176]	Loss: 2.2527
Profiling... [1536/50176]	Loss: 2.2497
Profiling... [1792/50176]	Loss: 2.3521
Profiling... [2048/50176]	Loss: 2.2424
Profiling... [2304/50176]	Loss: 2.2967
Profiling... [2560/50176]	Loss: 2.3467
Profiling... [2816/50176]	Loss: 2.1772
Profiling... [3072/50176]	Loss: 2.3255
Profiling... [3328/50176]	Loss: 2.2897
Profile done
epoch 1 train time consumed: 4.75s
Validation Epoch: 6, Average loss: 0.0095, Accuracy: 0.3733
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.87566925004762,
                        "time": 3.2264578229996914,
                        "accuracy": 0.37333984375,
                        "total_cost": 756701.5359018483
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.1264
Profiling... [512/50176]	Loss: 2.5768
Profiling... [768/50176]	Loss: 2.7027
Profiling... [1024/50176]	Loss: 2.4814
Profiling... [1280/50176]	Loss: 2.4575
Profiling... [1536/50176]	Loss: 2.5428
Profiling... [1792/50176]	Loss: 2.6336
Profiling... [2048/50176]	Loss: 2.5717
Profiling... [2304/50176]	Loss: 2.7789
Profiling... [2560/50176]	Loss: 2.6468
Profiling... [2816/50176]	Loss: 2.6126
Profiling... [3072/50176]	Loss: 2.7583
Profiling... [3328/50176]	Loss: 2.5207
Profile done
epoch 1 train time consumed: 3.85s
Validation Epoch: 6, Average loss: 0.0140, Accuracy: 0.2573
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.8512307976841,
                        "time": 2.4197667739999815,
                        "accuracy": 0.25732421875,
                        "total_cost": 823371.3479589258
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.2829
Profiling... [512/50176]	Loss: 2.2947
Profiling... [768/50176]	Loss: 2.4844
Profiling... [1024/50176]	Loss: 2.5024
Profiling... [1280/50176]	Loss: 2.6581
Profiling... [1536/50176]	Loss: 2.6148
Profiling... [1792/50176]	Loss: 2.9649
Profiling... [2048/50176]	Loss: 2.4593
Profiling... [2304/50176]	Loss: 2.7667
Profiling... [2560/50176]	Loss: 2.4380
Profiling... [2816/50176]	Loss: 2.6977
Profiling... [3072/50176]	Loss: 2.7128
Profiling... [3328/50176]	Loss: 2.7402
Profile done
epoch 1 train time consumed: 3.82s
Validation Epoch: 6, Average loss: 0.0154, Accuracy: 0.2329
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.87424674002344,
                        "time": 2.430562409000231,
                        "accuracy": 0.23291015625,
                        "total_cost": 913737.2103123239
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.1555
Profiling... [512/50176]	Loss: 2.3204
Profiling... [768/50176]	Loss: 2.3957
Profiling... [1024/50176]	Loss: 2.5545
Profiling... [1280/50176]	Loss: 2.6500
Profiling... [1536/50176]	Loss: 2.5464
Profiling... [1792/50176]	Loss: 2.4659
Profiling... [2048/50176]	Loss: 2.5847
Profiling... [2304/50176]	Loss: 2.4830
Profiling... [2560/50176]	Loss: 2.6037
Profiling... [2816/50176]	Loss: 2.5118
Profiling... [3072/50176]	Loss: 2.7290
Profiling... [3328/50176]	Loss: 2.7002
Profile done
epoch 1 train time consumed: 4.08s
Validation Epoch: 6, Average loss: 0.0163, Accuracy: 0.2160
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.88594550170804,
                        "time": 2.6481676399998832,
                        "accuracy": 0.216015625,
                        "total_cost": 1073404.2199810962
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.0856
Profiling... [512/50176]	Loss: 2.5012
Profiling... [768/50176]	Loss: 2.5035
Profiling... [1024/50176]	Loss: 2.3597
Profiling... [1280/50176]	Loss: 2.6154
Profiling... [1536/50176]	Loss: 2.4088
Profiling... [1792/50176]	Loss: 2.5759
Profiling... [2048/50176]	Loss: 2.5774
Profiling... [2304/50176]	Loss: 2.3275
Profiling... [2560/50176]	Loss: 2.5522
Profiling... [2816/50176]	Loss: 2.5838
Profiling... [3072/50176]	Loss: 2.5644
Profiling... [3328/50176]	Loss: 2.4891
Profile done
epoch 1 train time consumed: 4.76s
Validation Epoch: 6, Average loss: 0.0125, Accuracy: 0.2869
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.86168119643277,
                        "time": 3.2273003740001514,
                        "accuracy": 0.2869140625,
                        "total_cost": 984896.2488518769
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.1501
Profiling... [512/50176]	Loss: 2.1680
Profiling... [768/50176]	Loss: 2.4266
Profiling... [1024/50176]	Loss: 2.2814
Profiling... [1280/50176]	Loss: 2.4523
Profiling... [1536/50176]	Loss: 2.3596
Profiling... [1792/50176]	Loss: 2.6205
Profiling... [2048/50176]	Loss: 2.6170
Profiling... [2304/50176]	Loss: 2.6754
Profiling... [2560/50176]	Loss: 2.5167
Profiling... [2816/50176]	Loss: 2.5811
Profiling... [3072/50176]	Loss: 2.6185
Profiling... [3328/50176]	Loss: 2.4932
Profile done
epoch 1 train time consumed: 3.87s
Validation Epoch: 6, Average loss: 0.0128, Accuracy: 0.2659
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.83875266130262,
                        "time": 2.429815526999846,
                        "accuracy": 0.26591796875,
                        "total_cost": 800070.9306549989
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.1426
Profiling... [512/50176]	Loss: 2.5252
Profiling... [768/50176]	Loss: 2.4184
Profiling... [1024/50176]	Loss: 2.6449
Profiling... [1280/50176]	Loss: 2.6123
Profiling... [1536/50176]	Loss: 2.7060
Profiling... [1792/50176]	Loss: 2.3921
Profiling... [2048/50176]	Loss: 2.6121
Profiling... [2304/50176]	Loss: 2.7957
Profiling... [2560/50176]	Loss: 2.6679
Profiling... [2816/50176]	Loss: 2.2538
Profiling... [3072/50176]	Loss: 2.4118
Profiling... [3328/50176]	Loss: 2.5594
Profile done
epoch 1 train time consumed: 3.83s
Validation Epoch: 6, Average loss: 0.0154, Accuracy: 0.2202
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.86191073045391,
                        "time": 2.429413320999629,
                        "accuracy": 0.22021484375,
                        "total_cost": 965956.9006329146
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.1709
Profiling... [512/50176]	Loss: 2.2748
Profiling... [768/50176]	Loss: 2.4781
Profiling... [1024/50176]	Loss: 2.5807
Profiling... [1280/50176]	Loss: 2.6040
Profiling... [1536/50176]	Loss: 2.5225
Profiling... [1792/50176]	Loss: 2.5645
Profiling... [2048/50176]	Loss: 2.4873
Profiling... [2304/50176]	Loss: 2.5372
Profiling... [2560/50176]	Loss: 2.4706
Profiling... [2816/50176]	Loss: 2.6889
Profiling... [3072/50176]	Loss: 2.5730
Profiling... [3328/50176]	Loss: 2.4620
Profile done
epoch 1 train time consumed: 4.07s
Validation Epoch: 6, Average loss: 0.0225, Accuracy: 0.1662
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.87237180917364,
                        "time": 2.649256429000161,
                        "accuracy": 0.1662109375,
                        "total_cost": 1395620.5453394852
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.1349
Profiling... [512/50176]	Loss: 2.3523
Profiling... [768/50176]	Loss: 2.4845
Profiling... [1024/50176]	Loss: 2.3795
Profiling... [1280/50176]	Loss: 2.7725
Profiling... [1536/50176]	Loss: 2.6885
Profiling... [1792/50176]	Loss: 2.4661
Profiling... [2048/50176]	Loss: 2.5663
Profiling... [2304/50176]	Loss: 2.5733
Profiling... [2560/50176]	Loss: 2.8135
Profiling... [2816/50176]	Loss: 2.8063
Profiling... [3072/50176]	Loss: 2.4628
Profiling... [3328/50176]	Loss: 2.5932
Profile done
epoch 1 train time consumed: 4.80s
Validation Epoch: 6, Average loss: 0.0143, Accuracy: 0.2617
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.84869338881,
                        "time": 3.235004757999377,
                        "accuracy": 0.26171875,
                        "total_cost": 1082288.3510227788
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.9495
Profiling... [512/50176]	Loss: 2.4665
Profiling... [768/50176]	Loss: 2.5285
Profiling... [1024/50176]	Loss: 2.5003
Profiling... [1280/50176]	Loss: 2.6141
Profiling... [1536/50176]	Loss: 2.5919
Profiling... [1792/50176]	Loss: 2.7149
Profiling... [2048/50176]	Loss: 2.6550
Profiling... [2304/50176]	Loss: 2.6318
Profiling... [2560/50176]	Loss: 2.5170
Profiling... [2816/50176]	Loss: 2.3002
Profiling... [3072/50176]	Loss: 2.6846
Profiling... [3328/50176]	Loss: 2.3286
Profile done
epoch 1 train time consumed: 3.84s
Validation Epoch: 6, Average loss: 0.0152, Accuracy: 0.2090
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.82321282732286,
                        "time": 2.4144543049997083,
                        "accuracy": 0.208984375,
                        "total_cost": 1011598.1077357907
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.1834
Profiling... [512/50176]	Loss: 2.5824
Profiling... [768/50176]	Loss: 2.4831
Profiling... [1024/50176]	Loss: 2.4658
Profiling... [1280/50176]	Loss: 2.6639
Profiling... [1536/50176]	Loss: 2.6522
Profiling... [1792/50176]	Loss: 2.4491
Profiling... [2048/50176]	Loss: 2.4627
Profiling... [2304/50176]	Loss: 2.5560
Profiling... [2560/50176]	Loss: 2.7565
Profiling... [2816/50176]	Loss: 2.6387
Profiling... [3072/50176]	Loss: 2.5102
Profiling... [3328/50176]	Loss: 2.5721
Profile done
epoch 1 train time consumed: 3.83s
Validation Epoch: 6, Average loss: 0.0137, Accuracy: 0.2344
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.84608625752989,
                        "time": 2.425738258999445,
                        "accuracy": 0.234375,
                        "total_cost": 906223.9676229762
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.0623
Profiling... [512/50176]	Loss: 2.5381
Profiling... [768/50176]	Loss: 2.6977
Profiling... [1024/50176]	Loss: 2.4552
Profiling... [1280/50176]	Loss: 2.6166
Profiling... [1536/50176]	Loss: 2.7491
Profiling... [1792/50176]	Loss: 2.3971
Profiling... [2048/50176]	Loss: 2.5795
Profiling... [2304/50176]	Loss: 2.5810
Profiling... [2560/50176]	Loss: 2.6149
Profiling... [2816/50176]	Loss: 2.6963
Profiling... [3072/50176]	Loss: 2.7701
Profiling... [3328/50176]	Loss: 2.5885
Profile done
epoch 1 train time consumed: 4.07s
Validation Epoch: 6, Average loss: 0.0148, Accuracy: 0.2824
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.85713614742036,
                        "time": 2.6444738089994644,
                        "accuracy": 0.282421875,
                        "total_cost": 819867.8504603455
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.1220
Profiling... [512/50176]	Loss: 2.3792
Profiling... [768/50176]	Loss: 2.6520
Profiling... [1024/50176]	Loss: 2.4476
Profiling... [1280/50176]	Loss: 2.5930
Profiling... [1536/50176]	Loss: 2.7262
Profiling... [1792/50176]	Loss: 2.8110
Profiling... [2048/50176]	Loss: 2.5012
Profiling... [2304/50176]	Loss: 2.6309
Profiling... [2560/50176]	Loss: 2.4317
Profiling... [2816/50176]	Loss: 2.5414
Profiling... [3072/50176]	Loss: 2.4953
Profiling... [3328/50176]	Loss: 2.4821
Profile done
epoch 1 train time consumed: 4.67s
Validation Epoch: 6, Average loss: 0.0115, Accuracy: 0.3029
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.8309936123344,
                        "time": 3.235092168999472,
                        "accuracy": 0.3029296875,
                        "total_cost": 935077.6470064151
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.1832
Profiling... [1024/50176]	Loss: 2.2661
Profiling... [1536/50176]	Loss: 2.1364
Profiling... [2048/50176]	Loss: 2.1366
Profiling... [2560/50176]	Loss: 2.1206
Profiling... [3072/50176]	Loss: 2.1750
Profiling... [3584/50176]	Loss: 2.2434
Profiling... [4096/50176]	Loss: 2.0885
Profiling... [4608/50176]	Loss: 2.0957
Profiling... [5120/50176]	Loss: 2.1284
Profiling... [5632/50176]	Loss: 1.9448
Profiling... [6144/50176]	Loss: 2.1847
Profiling... [6656/50176]	Loss: 2.1564
Profile done
epoch 1 train time consumed: 6.73s
Validation Epoch: 6, Average loss: 0.0042, Accuracy: 0.4226
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.83656356397047,
                        "time": 4.585825660999944,
                        "accuracy": 0.42255859375,
                        "total_cost": 950240.3528358567
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.2169
Profiling... [1024/50176]	Loss: 2.2491
Profiling... [1536/50176]	Loss: 2.2041
Profiling... [2048/50176]	Loss: 2.1690
Profiling... [2560/50176]	Loss: 2.1120
Profiling... [3072/50176]	Loss: 2.0832
Profiling... [3584/50176]	Loss: 2.1453
Profiling... [4096/50176]	Loss: 2.2437
Profiling... [4608/50176]	Loss: 2.0887
Profiling... [5120/50176]	Loss: 2.1133
Profiling... [5632/50176]	Loss: 2.2341
Profiling... [6144/50176]	Loss: 2.0763
Profiling... [6656/50176]	Loss: 2.1048
Profile done
epoch 1 train time consumed: 6.78s
Validation Epoch: 6, Average loss: 0.0042, Accuracy: 0.4271
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.87639590510513,
                        "time": 4.6162624910002705,
                        "accuracy": 0.4271484375,
                        "total_cost": 946269.059655087
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.1040
Profiling... [1024/50176]	Loss: 2.1280
Profiling... [1536/50176]	Loss: 2.2086
Profiling... [2048/50176]	Loss: 2.1721
Profiling... [2560/50176]	Loss: 2.0326
Profiling... [3072/50176]	Loss: 2.1407
Profiling... [3584/50176]	Loss: 2.1583
Profiling... [4096/50176]	Loss: 2.1225
Profiling... [4608/50176]	Loss: 2.1770
Profiling... [5120/50176]	Loss: 2.2064
Profiling... [5632/50176]	Loss: 2.0386
Profiling... [6144/50176]	Loss: 2.1334
Profiling... [6656/50176]	Loss: 2.0652
Profile done
epoch 1 train time consumed: 7.33s
Validation Epoch: 6, Average loss: 0.0041, Accuracy: 0.4305
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.88715703831006,
                        "time": 5.098567283000193,
                        "accuracy": 0.43046875,
                        "total_cost": 1037073.4561966208
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.1331
Profiling... [1024/50176]	Loss: 2.0708
Profiling... [1536/50176]	Loss: 2.0096
Profiling... [2048/50176]	Loss: 2.0636
Profiling... [2560/50176]	Loss: 2.1334
Profiling... [3072/50176]	Loss: 2.0501
Profiling... [3584/50176]	Loss: 2.1238
Profiling... [4096/50176]	Loss: 2.1162
Profiling... [4608/50176]	Loss: 2.1430
Profiling... [5120/50176]	Loss: 2.1349
Profiling... [5632/50176]	Loss: 2.0418
Profiling... [6144/50176]	Loss: 2.1813
Profiling... [6656/50176]	Loss: 2.1331
Profile done
epoch 1 train time consumed: 12.54s
Validation Epoch: 6, Average loss: 0.0042, Accuracy: 0.4272
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.83319554028458,
                        "time": 9.077065138999387,
                        "accuracy": 0.42724609375,
                        "total_cost": 1860245.2769429064
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.1285
Profiling... [1024/50176]	Loss: 2.0039
Profiling... [1536/50176]	Loss: 2.1055
Profiling... [2048/50176]	Loss: 2.2096
Profiling... [2560/50176]	Loss: 2.0973
Profiling... [3072/50176]	Loss: 2.0937
Profiling... [3584/50176]	Loss: 2.1539
Profiling... [4096/50176]	Loss: 2.2701
Profiling... [4608/50176]	Loss: 2.2462
Profiling... [5120/50176]	Loss: 2.0384
Profiling... [5632/50176]	Loss: 2.2392
Profiling... [6144/50176]	Loss: 2.0791
Profiling... [6656/50176]	Loss: 2.0371
Profile done
epoch 1 train time consumed: 6.80s
Validation Epoch: 6, Average loss: 0.0042, Accuracy: 0.4241
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.81164768875198,
                        "time": 4.575279146999492,
                        "accuracy": 0.42412109375,
                        "total_cost": 944562.1297620287
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.0691
Profiling... [1024/50176]	Loss: 2.0823
Profiling... [1536/50176]	Loss: 2.1176
Profiling... [2048/50176]	Loss: 2.1104
Profiling... [2560/50176]	Loss: 2.0926
Profiling... [3072/50176]	Loss: 2.1238
Profiling... [3584/50176]	Loss: 2.2015
Profiling... [4096/50176]	Loss: 2.1164
Profiling... [4608/50176]	Loss: 2.1516
Profiling... [5120/50176]	Loss: 2.0034
Profiling... [5632/50176]	Loss: 2.0501
Profiling... [6144/50176]	Loss: 2.0542
Profiling... [6656/50176]	Loss: 2.2162
Profile done
epoch 1 train time consumed: 6.90s
Validation Epoch: 6, Average loss: 0.0042, Accuracy: 0.4256
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.85200336453528,
                        "time": 4.607780583000022,
                        "accuracy": 0.4255859375,
                        "total_cost": 947998.0127143621
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.1219
Profiling... [1024/50176]	Loss: 2.2256
Profiling... [1536/50176]	Loss: 2.1255
Profiling... [2048/50176]	Loss: 2.0421
Profiling... [2560/50176]	Loss: 2.1658
Profiling... [3072/50176]	Loss: 2.1040
Profiling... [3584/50176]	Loss: 2.0228
Profiling... [4096/50176]	Loss: 2.1748
Profiling... [4608/50176]	Loss: 2.1423
Profiling... [5120/50176]	Loss: 2.0550
Profiling... [5632/50176]	Loss: 2.0743
Profiling... [6144/50176]	Loss: 2.0560
Profiling... [6656/50176]	Loss: 2.1839
Profile done
epoch 1 train time consumed: 7.33s
Validation Epoch: 6, Average loss: 0.0042, Accuracy: 0.4284
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.86313896303719,
                        "time": 5.014848279999569,
                        "accuracy": 0.42841796875,
                        "total_cost": 1024927.2832442427
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.0049
Profiling... [1024/50176]	Loss: 2.0949
Profiling... [1536/50176]	Loss: 2.1758
Profiling... [2048/50176]	Loss: 2.1047
Profiling... [2560/50176]	Loss: 2.2039
Profiling... [3072/50176]	Loss: 2.2322
Profiling... [3584/50176]	Loss: 2.1091
Profiling... [4096/50176]	Loss: 2.1381
Profiling... [4608/50176]	Loss: 2.1749
Profiling... [5120/50176]	Loss: 2.0826
Profiling... [5632/50176]	Loss: 2.1125
Profiling... [6144/50176]	Loss: 1.9894
Profiling... [6656/50176]	Loss: 2.1330
Profile done
epoch 1 train time consumed: 13.83s
Validation Epoch: 6, Average loss: 0.0042, Accuracy: 0.4281
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.80159393633356,
                        "time": 10.470598214999882,
                        "accuracy": 0.428125,
                        "total_cost": 2141429.0351911103
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.0940
Profiling... [1024/50176]	Loss: 2.1659
Profiling... [1536/50176]	Loss: 2.1476
Profiling... [2048/50176]	Loss: 1.9669
Profiling... [2560/50176]	Loss: 2.1333
Profiling... [3072/50176]	Loss: 2.1062
Profiling... [3584/50176]	Loss: 2.0678
Profiling... [4096/50176]	Loss: 2.3189
Profiling... [4608/50176]	Loss: 2.1199
Profiling... [5120/50176]	Loss: 2.1176
Profiling... [5632/50176]	Loss: 2.1482
Profiling... [6144/50176]	Loss: 2.0454
Profiling... [6656/50176]	Loss: 2.2551
Profile done
epoch 1 train time consumed: 6.92s
Validation Epoch: 6, Average loss: 0.0041, Accuracy: 0.4328
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.78245950232682,
                        "time": 4.589008150000154,
                        "accuracy": 0.4328125,
                        "total_cost": 928371.4309600119
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.0169
Profiling... [1024/50176]	Loss: 2.0141
Profiling... [1536/50176]	Loss: 2.0880
Profiling... [2048/50176]	Loss: 2.1017
Profiling... [2560/50176]	Loss: 2.0342
Profiling... [3072/50176]	Loss: 2.0463
Profiling... [3584/50176]	Loss: 2.1371
Profiling... [4096/50176]	Loss: 2.1147
Profiling... [4608/50176]	Loss: 2.1846
Profiling... [5120/50176]	Loss: 1.9880
Profiling... [5632/50176]	Loss: 2.1403
Profiling... [6144/50176]	Loss: 2.1332
Profiling... [6656/50176]	Loss: 2.2272
Profile done
epoch 1 train time consumed: 6.87s
Validation Epoch: 6, Average loss: 0.0041, Accuracy: 0.4291
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.81934861435684,
                        "time": 4.6140441539992025,
                        "accuracy": 0.4291015625,
                        "total_cost": 941508.9984328785
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.1823
Profiling... [1024/50176]	Loss: 2.1164
Profiling... [1536/50176]	Loss: 2.1624
Profiling... [2048/50176]	Loss: 2.1516
Profiling... [2560/50176]	Loss: 2.0143
Profiling... [3072/50176]	Loss: 2.0553
Profiling... [3584/50176]	Loss: 2.2319
Profiling... [4096/50176]	Loss: 2.0777
Profiling... [4608/50176]	Loss: 2.0305
Profiling... [5120/50176]	Loss: 2.2213
Profiling... [5632/50176]	Loss: 2.1827
Profiling... [6144/50176]	Loss: 2.2419
Profiling... [6656/50176]	Loss: 2.1433
Profile done
epoch 1 train time consumed: 7.43s
Validation Epoch: 6, Average loss: 0.0042, Accuracy: 0.4282
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.83137592611669,
                        "time": 5.055880741999317,
                        "accuracy": 0.42822265625,
                        "total_cost": 1033784.5443166818
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.0627
Profiling... [1024/50176]	Loss: 2.1677
Profiling... [1536/50176]	Loss: 2.1292
Profiling... [2048/50176]	Loss: 2.1239
Profiling... [2560/50176]	Loss: 2.0841
Profiling... [3072/50176]	Loss: 2.0488
Profiling... [3584/50176]	Loss: 2.1183
Profiling... [4096/50176]	Loss: 2.1271
Profiling... [4608/50176]	Loss: 2.0895
Profiling... [5120/50176]	Loss: 2.1558
Profiling... [5632/50176]	Loss: 2.1558
Profiling... [6144/50176]	Loss: 2.0933
Profiling... [6656/50176]	Loss: 1.9967
Profile done
epoch 1 train time consumed: 12.94s
Validation Epoch: 6, Average loss: 0.0042, Accuracy: 0.4266
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.78086610707483,
                        "time": 8.892670477000138,
                        "accuracy": 0.4265625,
                        "total_cost": 1825375.663093084
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.1770
Profiling... [1024/50176]	Loss: 2.2005
Profiling... [1536/50176]	Loss: 2.1602
Profiling... [2048/50176]	Loss: 2.1551
Profiling... [2560/50176]	Loss: 2.2382
Profiling... [3072/50176]	Loss: 2.2218
Profiling... [3584/50176]	Loss: 2.2953
Profiling... [4096/50176]	Loss: 2.2866
Profiling... [4608/50176]	Loss: 2.2995
Profiling... [5120/50176]	Loss: 2.2106
Profiling... [5632/50176]	Loss: 2.1814
Profiling... [6144/50176]	Loss: 2.2093
Profiling... [6656/50176]	Loss: 2.2886
Profile done
epoch 1 train time consumed: 6.84s
Validation Epoch: 6, Average loss: 0.0048, Accuracy: 0.3766
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.75430474255766,
                        "time": 4.579447138999967,
                        "accuracy": 0.3765625,
                        "total_cost": 1064825.9961972998
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.1282
Profiling... [1024/50176]	Loss: 2.2421
Profiling... [1536/50176]	Loss: 2.1068
Profiling... [2048/50176]	Loss: 2.1771
Profiling... [2560/50176]	Loss: 2.2967
Profiling... [3072/50176]	Loss: 2.2556
Profiling... [3584/50176]	Loss: 2.2964
Profiling... [4096/50176]	Loss: 2.2033
Profiling... [4608/50176]	Loss: 2.3423
Profiling... [5120/50176]	Loss: 2.2582
Profiling... [5632/50176]	Loss: 2.3336
Profiling... [6144/50176]	Loss: 2.2848
Profiling... [6656/50176]	Loss: 2.2171
Profile done
epoch 1 train time consumed: 6.85s
Validation Epoch: 6, Average loss: 0.0045, Accuracy: 0.3784
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.79174297850363,
                        "time": 4.612690586000099,
                        "accuracy": 0.37841796875,
                        "total_cost": 1067297.1011033007
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.2950
Profiling... [1024/50176]	Loss: 2.2663
Profiling... [1536/50176]	Loss: 2.2674
Profiling... [2048/50176]	Loss: 2.0902
Profiling... [2560/50176]	Loss: 2.3014
Profiling... [3072/50176]	Loss: 2.2399
Profiling... [3584/50176]	Loss: 2.1320
Profiling... [4096/50176]	Loss: 2.2484
Profiling... [4608/50176]	Loss: 2.3545
Profiling... [5120/50176]	Loss: 2.0440
Profiling... [5632/50176]	Loss: 2.2803
Profiling... [6144/50176]	Loss: 2.1634
Profiling... [6656/50176]	Loss: 2.3264
Profile done
epoch 1 train time consumed: 7.51s
Validation Epoch: 6, Average loss: 0.0045, Accuracy: 0.3931
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.80303506474542,
                        "time": 5.124134330999368,
                        "accuracy": 0.39306640625,
                        "total_cost": 1141451.236187259
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.0837
Profiling... [1024/50176]	Loss: 2.2345
Profiling... [1536/50176]	Loss: 2.2517
Profiling... [2048/50176]	Loss: 2.2198
Profiling... [2560/50176]	Loss: 2.3448
Profiling... [3072/50176]	Loss: 2.1886
Profiling... [3584/50176]	Loss: 2.2353
Profiling... [4096/50176]	Loss: 2.2332
Profiling... [4608/50176]	Loss: 2.3083
Profiling... [5120/50176]	Loss: 2.2736
Profiling... [5632/50176]	Loss: 2.3917
Profiling... [6144/50176]	Loss: 2.2770
Profiling... [6656/50176]	Loss: 2.1669
Profile done
epoch 1 train time consumed: 13.15s
Validation Epoch: 6, Average loss: 0.0047, Accuracy: 0.3808
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.74806806971607,
                        "time": 9.50054573899979,
                        "accuracy": 0.38076171875,
                        "total_cost": 2184730.7565987785
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.1908
Profiling... [1024/50176]	Loss: 2.2844
Profiling... [1536/50176]	Loss: 2.2930
Profiling... [2048/50176]	Loss: 2.2744
Profiling... [2560/50176]	Loss: 2.2037
Profiling... [3072/50176]	Loss: 2.1681
Profiling... [3584/50176]	Loss: 2.2325
Profiling... [4096/50176]	Loss: 2.2584
Profiling... [4608/50176]	Loss: 2.2781
Profiling... [5120/50176]	Loss: 2.1706
Profiling... [5632/50176]	Loss: 2.2179
Profiling... [6144/50176]	Loss: 2.2239
Profiling... [6656/50176]	Loss: 2.1667
Profile done
epoch 1 train time consumed: 6.95s
Validation Epoch: 6, Average loss: 0.0048, Accuracy: 0.3743
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.72640985338414,
                        "time": 4.590706795000187,
                        "accuracy": 0.37431640625,
                        "total_cost": 1073849.173905255
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.0640
Profiling... [1024/50176]	Loss: 2.2299
Profiling... [1536/50176]	Loss: 2.0731
Profiling... [2048/50176]	Loss: 2.1948
Profiling... [2560/50176]	Loss: 2.2042
Profiling... [3072/50176]	Loss: 2.3732
Profiling... [3584/50176]	Loss: 2.3254
Profiling... [4096/50176]	Loss: 2.2418
Profiling... [4608/50176]	Loss: 2.3395
Profiling... [5120/50176]	Loss: 2.3413
Profiling... [5632/50176]	Loss: 2.2278
Profiling... [6144/50176]	Loss: 2.2498
Profiling... [6656/50176]	Loss: 2.3662
Profile done
epoch 1 train time consumed: 6.88s
Validation Epoch: 6, Average loss: 0.0050, Accuracy: 0.3458
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.76041696641681,
                        "time": 4.610592798000653,
                        "accuracy": 0.34580078125,
                        "total_cost": 1167437.0610943597
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.0528
Profiling... [1024/50176]	Loss: 2.1191
Profiling... [1536/50176]	Loss: 2.1781
Profiling... [2048/50176]	Loss: 2.3184
Profiling... [2560/50176]	Loss: 2.1254
Profiling... [3072/50176]	Loss: 2.2006
Profiling... [3584/50176]	Loss: 2.0599
Profiling... [4096/50176]	Loss: 2.1720
Profiling... [4608/50176]	Loss: 2.2589
Profiling... [5120/50176]	Loss: 2.4008
Profiling... [5632/50176]	Loss: 2.3106
Profiling... [6144/50176]	Loss: 2.1952
Profiling... [6656/50176]	Loss: 2.2805
Profile done
epoch 1 train time consumed: 7.35s
Validation Epoch: 6, Average loss: 0.0046, Accuracy: 0.3829
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.77332743836857,
                        "time": 5.019071986000199,
                        "accuracy": 0.38291015625,
                        "total_cost": 1147702.2939770415
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.0932
Profiling... [1024/50176]	Loss: 2.2664
Profiling... [1536/50176]	Loss: 2.2650
Profiling... [2048/50176]	Loss: 2.1861
Profiling... [2560/50176]	Loss: 2.2415
Profiling... [3072/50176]	Loss: 2.2777
Profiling... [3584/50176]	Loss: 2.2629
Profiling... [4096/50176]	Loss: 2.3122
Profiling... [4608/50176]	Loss: 2.3968
Profiling... [5120/50176]	Loss: 2.3438
Profiling... [5632/50176]	Loss: 2.2729
Profiling... [6144/50176]	Loss: 2.2987
Profiling... [6656/50176]	Loss: 2.2670
Profile done
epoch 1 train time consumed: 13.57s
Validation Epoch: 6, Average loss: 0.0044, Accuracy: 0.4003
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.72201223489567,
                        "time": 9.51362903900008,
                        "accuracy": 0.40029296875,
                        "total_cost": 2080994.0332085593
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.0603
Profiling... [1024/50176]	Loss: 2.2248
Profiling... [1536/50176]	Loss: 2.0983
Profiling... [2048/50176]	Loss: 2.1508
Profiling... [2560/50176]	Loss: 2.3151
Profiling... [3072/50176]	Loss: 2.1127
Profiling... [3584/50176]	Loss: 2.3076
Profiling... [4096/50176]	Loss: 2.3309
Profiling... [4608/50176]	Loss: 2.2979
Profiling... [5120/50176]	Loss: 2.2622
Profiling... [5632/50176]	Loss: 2.2476
Profiling... [6144/50176]	Loss: 2.2809
Profiling... [6656/50176]	Loss: 2.1541
Profile done
epoch 1 train time consumed: 6.89s
Validation Epoch: 6, Average loss: 0.0046, Accuracy: 0.3815
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.6973277108854,
                        "time": 4.572728791000372,
                        "accuracy": 0.38154296875,
                        "total_cost": 1049384.1777983278
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.0354
Profiling... [1024/50176]	Loss: 2.1610
Profiling... [1536/50176]	Loss: 2.1702
Profiling... [2048/50176]	Loss: 2.1696
Profiling... [2560/50176]	Loss: 2.3414
Profiling... [3072/50176]	Loss: 2.3174
Profiling... [3584/50176]	Loss: 2.3237
Profiling... [4096/50176]	Loss: 2.2865
Profiling... [4608/50176]	Loss: 2.3905
Profiling... [5120/50176]	Loss: 2.2674
Profiling... [5632/50176]	Loss: 2.2157
Profiling... [6144/50176]	Loss: 2.2700
Profiling... [6656/50176]	Loss: 2.2878
Profile done
epoch 1 train time consumed: 6.91s
Validation Epoch: 6, Average loss: 0.0045, Accuracy: 0.3889
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.73243262132294,
                        "time": 4.61488574499981,
                        "accuracy": 0.3888671875,
                        "total_cost": 1039111.7687009014
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.1674
Profiling... [1024/50176]	Loss: 2.1999
Profiling... [1536/50176]	Loss: 2.1217
Profiling... [2048/50176]	Loss: 2.3275
Profiling... [2560/50176]	Loss: 2.1056
Profiling... [3072/50176]	Loss: 2.2366
Profiling... [3584/50176]	Loss: 2.2502
Profiling... [4096/50176]	Loss: 2.2258
Profiling... [4608/50176]	Loss: 2.1944
Profiling... [5120/50176]	Loss: 2.2735
Profiling... [5632/50176]	Loss: 2.3605
Profiling... [6144/50176]	Loss: 2.1912
Profiling... [6656/50176]	Loss: 2.3173
Profile done
epoch 1 train time consumed: 7.45s
Validation Epoch: 6, Average loss: 0.0048, Accuracy: 0.3701
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.74262310946075,
                        "time": 5.0362385660000655,
                        "accuracy": 0.3701171875,
                        "total_cost": 1191433.1393052954
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.1571
Profiling... [1024/50176]	Loss: 2.2234
Profiling... [1536/50176]	Loss: 2.2985
Profiling... [2048/50176]	Loss: 2.2781
Profiling... [2560/50176]	Loss: 2.3115
Profiling... [3072/50176]	Loss: 2.1960
Profiling... [3584/50176]	Loss: 2.1591
Profiling... [4096/50176]	Loss: 2.2517
Profiling... [4608/50176]	Loss: 2.1023
Profiling... [5120/50176]	Loss: 2.2944
Profiling... [5632/50176]	Loss: 2.3704
Profiling... [6144/50176]	Loss: 2.0606
Profiling... [6656/50176]	Loss: 2.1482
Profile done
epoch 1 train time consumed: 13.16s
Validation Epoch: 6, Average loss: 0.0050, Accuracy: 0.3591
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.69099967083936,
                        "time": 9.192703051000535,
                        "accuracy": 0.35908203125,
                        "total_cost": 2241568.757194522
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.1877
Profiling... [1024/50176]	Loss: 2.3323
Profiling... [1536/50176]	Loss: 2.4044
Profiling... [2048/50176]	Loss: 2.4565
Profiling... [2560/50176]	Loss: 2.3370
Profiling... [3072/50176]	Loss: 2.3020
Profiling... [3584/50176]	Loss: 2.5790
Profiling... [4096/50176]	Loss: 2.4141
Profiling... [4608/50176]	Loss: 2.4302
Profiling... [5120/50176]	Loss: 2.4689
Profiling... [5632/50176]	Loss: 2.4391
Profiling... [6144/50176]	Loss: 2.4986
Profiling... [6656/50176]	Loss: 2.3998
Profile done
epoch 1 train time consumed: 6.78s
Validation Epoch: 6, Average loss: 0.0058, Accuracy: 0.2841
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.66854220063777,
                        "time": 4.563506999999845,
                        "accuracy": 0.28408203125,
                        "total_cost": 1406557.22962588
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.0373
Profiling... [1024/50176]	Loss: 2.3915
Profiling... [1536/50176]	Loss: 2.2106
Profiling... [2048/50176]	Loss: 2.3473
Profiling... [2560/50176]	Loss: 2.4087
Profiling... [3072/50176]	Loss: 2.4445
Profiling... [3584/50176]	Loss: 2.4575
Profiling... [4096/50176]	Loss: 2.4319
Profiling... [4608/50176]	Loss: 2.4196
Profiling... [5120/50176]	Loss: 2.5417
Profiling... [5632/50176]	Loss: 2.4910
Profiling... [6144/50176]	Loss: 2.4367
Profiling... [6656/50176]	Loss: 2.3367
Profile done
epoch 1 train time consumed: 7.02s
Validation Epoch: 6, Average loss: 0.0068, Accuracy: 0.2629
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.70366600199448,
                        "time": 4.726858171999993,
                        "accuracy": 0.262890625,
                        "total_cost": 1574345.3679523314
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.1094
Profiling... [1024/50176]	Loss: 2.4357
Profiling... [1536/50176]	Loss: 2.5469
Profiling... [2048/50176]	Loss: 2.3329
Profiling... [2560/50176]	Loss: 2.3473
Profiling... [3072/50176]	Loss: 2.5347
Profiling... [3584/50176]	Loss: 2.4510
Profiling... [4096/50176]	Loss: 2.3166
Profiling... [4608/50176]	Loss: 2.4763
Profiling... [5120/50176]	Loss: 2.4257
Profiling... [5632/50176]	Loss: 2.5219
Profiling... [6144/50176]	Loss: 2.4773
Profiling... [6656/50176]	Loss: 2.4774
Profile done
epoch 1 train time consumed: 7.39s
Validation Epoch: 6, Average loss: 0.0073, Accuracy: 0.2162
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.71379639402275,
                        "time": 5.021051085000181,
                        "accuracy": 0.2162109375,
                        "total_cost": 2033384.661474909
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.1937
Profiling... [1024/50176]	Loss: 2.4149
Profiling... [1536/50176]	Loss: 2.3383
Profiling... [2048/50176]	Loss: 2.4602
Profiling... [2560/50176]	Loss: 2.4874
Profiling... [3072/50176]	Loss: 2.4438
Profiling... [3584/50176]	Loss: 2.3844
Profiling... [4096/50176]	Loss: 2.3946
Profiling... [4608/50176]	Loss: 2.4632
Profiling... [5120/50176]	Loss: 2.2895
Profiling... [5632/50176]	Loss: 2.4302
Profiling... [6144/50176]	Loss: 2.4590
Profiling... [6656/50176]	Loss: 2.3727
Profile done
epoch 1 train time consumed: 13.85s
Validation Epoch: 6, Average loss: 0.0070, Accuracy: 0.2685
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.66109684811302,
                        "time": 9.848424958999203,
                        "accuracy": 0.26845703125,
                        "total_cost": 3212139.731826677
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.0317
Profiling... [1024/50176]	Loss: 2.4912
Profiling... [1536/50176]	Loss: 2.2978
Profiling... [2048/50176]	Loss: 2.3331
Profiling... [2560/50176]	Loss: 2.5606
Profiling... [3072/50176]	Loss: 2.4366
Profiling... [3584/50176]	Loss: 2.4933
Profiling... [4096/50176]	Loss: 2.3261
Profiling... [4608/50176]	Loss: 2.4617
Profiling... [5120/50176]	Loss: 2.5317
Profiling... [5632/50176]	Loss: 2.3236
Profiling... [6144/50176]	Loss: 2.4264
Profiling... [6656/50176]	Loss: 2.3784
Profile done
epoch 1 train time consumed: 6.86s
Validation Epoch: 6, Average loss: 0.0067, Accuracy: 0.2938
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.63566500216837,
                        "time": 4.573174896999262,
                        "accuracy": 0.29375,
                        "total_cost": 1363145.7848850172
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.1714
Profiling... [1024/50176]	Loss: 2.4296
Profiling... [1536/50176]	Loss: 2.3726
Profiling... [2048/50176]	Loss: 2.3561
Profiling... [2560/50176]	Loss: 2.5807
Profiling... [3072/50176]	Loss: 2.3043
Profiling... [3584/50176]	Loss: 2.4780
Profiling... [4096/50176]	Loss: 2.4420
Profiling... [4608/50176]	Loss: 2.4420
Profiling... [5120/50176]	Loss: 2.3572
Profiling... [5632/50176]	Loss: 2.2224
Profiling... [6144/50176]	Loss: 2.5135
Profiling... [6656/50176]	Loss: 2.4689
Profile done
epoch 1 train time consumed: 6.87s
Validation Epoch: 6, Average loss: 0.0060, Accuracy: 0.2716
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.67347222440135,
                        "time": 4.606217237999772,
                        "accuracy": 0.27158203125,
                        "total_cost": 1485066.3144589274
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.0535
Profiling... [1024/50176]	Loss: 2.4085
Profiling... [1536/50176]	Loss: 2.4816
Profiling... [2048/50176]	Loss: 2.3140
Profiling... [2560/50176]	Loss: 2.3823
Profiling... [3072/50176]	Loss: 2.3907
Profiling... [3584/50176]	Loss: 2.5390
Profiling... [4096/50176]	Loss: 2.4268
Profiling... [4608/50176]	Loss: 2.3893
Profiling... [5120/50176]	Loss: 2.5113
Profiling... [5632/50176]	Loss: 2.3855
Profiling... [6144/50176]	Loss: 2.4014
Profiling... [6656/50176]	Loss: 2.5489
Profile done
epoch 1 train time consumed: 7.41s
Validation Epoch: 6, Average loss: 0.0060, Accuracy: 0.2867
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.68079587965279,
                        "time": 5.064171171000453,
                        "accuracy": 0.28671875,
                        "total_cost": 1546517.2312416339
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.1375
Profiling... [1024/50176]	Loss: 2.4019
Profiling... [1536/50176]	Loss: 2.5034
Profiling... [2048/50176]	Loss: 2.4414
Profiling... [2560/50176]	Loss: 2.4747
Profiling... [3072/50176]	Loss: 2.5650
Profiling... [3584/50176]	Loss: 2.4669
Profiling... [4096/50176]	Loss: 2.3361
Profiling... [4608/50176]	Loss: 2.3474
Profiling... [5120/50176]	Loss: 2.4635
Profiling... [5632/50176]	Loss: 2.4728
Profiling... [6144/50176]	Loss: 2.3215
Profiling... [6656/50176]	Loss: 2.3995
Profile done
epoch 1 train time consumed: 13.07s
Validation Epoch: 6, Average loss: 0.0064, Accuracy: 0.2712
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.63435892273776,
                        "time": 9.023911117000353,
                        "accuracy": 0.27119140625,
                        "total_cost": 2913541.791823285
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.0419
Profiling... [1024/50176]	Loss: 2.4505
Profiling... [1536/50176]	Loss: 2.4068
Profiling... [2048/50176]	Loss: 2.3738
Profiling... [2560/50176]	Loss: 2.5029
Profiling... [3072/50176]	Loss: 2.3344
Profiling... [3584/50176]	Loss: 2.4753
Profiling... [4096/50176]	Loss: 2.5974
Profiling... [4608/50176]	Loss: 2.3919
Profiling... [5120/50176]	Loss: 2.4507
Profiling... [5632/50176]	Loss: 2.4099
Profiling... [6144/50176]	Loss: 2.4102
Profiling... [6656/50176]	Loss: 2.4073
Profile done
epoch 1 train time consumed: 6.85s
Validation Epoch: 6, Average loss: 0.0053, Accuracy: 0.3248
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.61103948542588,
                        "time": 4.575224549000268,
                        "accuracy": 0.3248046875,
                        "total_cost": 1233367.3112625307
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.0639
Profiling... [1024/50176]	Loss: 2.3714
Profiling... [1536/50176]	Loss: 2.5143
Profiling... [2048/50176]	Loss: 2.3643
Profiling... [2560/50176]	Loss: 2.4648
Profiling... [3072/50176]	Loss: 2.4456
Profiling... [3584/50176]	Loss: 2.3708
Profiling... [4096/50176]	Loss: 2.4148
Profiling... [4608/50176]	Loss: 2.3601
Profiling... [5120/50176]	Loss: 2.3429
Profiling... [5632/50176]	Loss: 2.4428
Profiling... [6144/50176]	Loss: 2.4694
Profiling... [6656/50176]	Loss: 2.6039
Profile done
epoch 1 train time consumed: 6.86s
Validation Epoch: 6, Average loss: 0.0062, Accuracy: 0.2905
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.64580024952198,
                        "time": 4.608004874000471,
                        "accuracy": 0.29052734375,
                        "total_cost": 1388763.5548519897
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.0737
Profiling... [1024/50176]	Loss: 2.4096
Profiling... [1536/50176]	Loss: 2.4284
Profiling... [2048/50176]	Loss: 2.3558
Profiling... [2560/50176]	Loss: 2.5623
Profiling... [3072/50176]	Loss: 2.3327
Profiling... [3584/50176]	Loss: 2.4252
Profiling... [4096/50176]	Loss: 2.3573
Profiling... [4608/50176]	Loss: 2.3103
Profiling... [5120/50176]	Loss: 2.4237
Profiling... [5632/50176]	Loss: 2.6206
Profiling... [6144/50176]	Loss: 2.3544
Profiling... [6656/50176]	Loss: 2.4371
Profile done
epoch 1 train time consumed: 7.44s
Validation Epoch: 6, Average loss: 0.0066, Accuracy: 0.2468
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.65810373884682,
                        "time": 5.032507912000256,
                        "accuracy": 0.24677734375,
                        "total_cost": 1785589.4286201738
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.1618
Profiling... [1024/50176]	Loss: 2.5674
Profiling... [1536/50176]	Loss: 2.2980
Profiling... [2048/50176]	Loss: 2.3501
Profiling... [2560/50176]	Loss: 2.3976
Profiling... [3072/50176]	Loss: 2.4133
Profiling... [3584/50176]	Loss: 2.5504
Profiling... [4096/50176]	Loss: 2.4278
Profiling... [4608/50176]	Loss: 2.4475
Profiling... [5120/50176]	Loss: 2.3752
Profiling... [5632/50176]	Loss: 2.4133
Profiling... [6144/50176]	Loss: 2.5303
Profiling... [6656/50176]	Loss: 2.5032
Profile done
epoch 1 train time consumed: 13.16s
Validation Epoch: 6, Average loss: 0.0080, Accuracy: 0.2182
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.60680944602682,
                        "time": 9.269880941000338,
                        "accuracy": 0.2181640625,
                        "total_cost": 3720430.8928685626
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.1465
Profiling... [2048/50176]	Loss: 2.1037
Profiling... [3072/50176]	Loss: 2.0293
Profiling... [4096/50176]	Loss: 2.0103
Profiling... [5120/50176]	Loss: 2.0967
Profiling... [6144/50176]	Loss: 2.0838
Profiling... [7168/50176]	Loss: 2.1503
Profiling... [8192/50176]	Loss: 2.0722
Profiling... [9216/50176]	Loss: 2.0194
Profiling... [10240/50176]	Loss: 2.1685
Profiling... [11264/50176]	Loss: 2.1080
Profiling... [12288/50176]	Loss: 2.0420
Profiling... [13312/50176]	Loss: 2.1015
Profile done
epoch 1 train time consumed: 13.12s
Validation Epoch: 6, Average loss: 0.0021, Accuracy: 0.4290
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.6308906912279,
                        "time": 8.946075859999837,
                        "accuracy": 0.42900390625,
                        "total_cost": 1825886.120876591
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.1056
Profiling... [2048/50176]	Loss: 2.1039
Profiling... [3072/50176]	Loss: 2.0374
Profiling... [4096/50176]	Loss: 2.0253
Profiling... [5120/50176]	Loss: 2.0698
Profiling... [6144/50176]	Loss: 2.1200
Profiling... [7168/50176]	Loss: 2.2385
Profiling... [8192/50176]	Loss: 2.1376
Profiling... [9216/50176]	Loss: 2.0983
Profiling... [10240/50176]	Loss: 2.0573
Profiling... [11264/50176]	Loss: 2.0825
Profiling... [12288/50176]	Loss: 2.1024
Profiling... [13312/50176]	Loss: 2.1061
Profile done
epoch 1 train time consumed: 13.02s
Validation Epoch: 6, Average loss: 0.0021, Accuracy: 0.4290
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.6897028417139,
                        "time": 9.028598496000086,
                        "accuracy": 0.42900390625,
                        "total_cost": 1842729.5363031235
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.0998
Profiling... [2048/50176]	Loss: 2.1176
Profiling... [3072/50176]	Loss: 2.0988
Profiling... [4096/50176]	Loss: 2.1745
Profiling... [5120/50176]	Loss: 2.1644
Profiling... [6144/50176]	Loss: 2.0792
Profiling... [7168/50176]	Loss: 2.0687
Profiling... [8192/50176]	Loss: 2.1833
Profiling... [9216/50176]	Loss: 2.1200
Profiling... [10240/50176]	Loss: 2.1296
Profiling... [11264/50176]	Loss: 2.1397
Profiling... [12288/50176]	Loss: 2.1094
Profiling... [13312/50176]	Loss: 2.0834
Profile done
epoch 1 train time consumed: 14.13s
Validation Epoch: 6, Average loss: 0.0021, Accuracy: 0.4328
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.69750954168786,
                        "time": 9.962691548000294,
                        "accuracy": 0.4328125,
                        "total_cost": 2015484.2657907652
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.0770
Profiling... [2048/50176]	Loss: 2.0928
Profiling... [3072/50176]	Loss: 2.1175
Profiling... [4096/50176]	Loss: 2.1387
Profiling... [5120/50176]	Loss: 2.3031
Profiling... [6144/50176]	Loss: 2.1740
Profiling... [7168/50176]	Loss: 2.1487
Profiling... [8192/50176]	Loss: 2.1917
Profiling... [9216/50176]	Loss: 2.0746
Profiling... [10240/50176]	Loss: 2.0535
Profiling... [11264/50176]	Loss: 2.1542
Profiling... [12288/50176]	Loss: 2.0859
Profiling... [13312/50176]	Loss: 2.2012
Profile done
epoch 1 train time consumed: 25.46s
Validation Epoch: 6, Average loss: 0.0021, Accuracy: 0.4260
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.5686705063747,
                        "time": 18.709823954000058,
                        "accuracy": 0.4259765625,
                        "total_cost": 3845795.613346268
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.1032
Profiling... [2048/50176]	Loss: 2.1913
Profiling... [3072/50176]	Loss: 2.0510
Profiling... [4096/50176]	Loss: 2.1197
Profiling... [5120/50176]	Loss: 2.1421
Profiling... [6144/50176]	Loss: 2.0254
Profiling... [7168/50176]	Loss: 2.0419
Profiling... [8192/50176]	Loss: 2.1215
Profiling... [9216/50176]	Loss: 2.0139
Profiling... [10240/50176]	Loss: 2.1068
Profiling... [11264/50176]	Loss: 2.0510
Profiling... [12288/50176]	Loss: 2.0991
Profiling... [13312/50176]	Loss: 2.0919
Profile done
epoch 1 train time consumed: 12.94s
Validation Epoch: 6, Average loss: 0.0020, Accuracy: 0.4360
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.59493113404803,
                        "time": 8.978235750999374,
                        "accuracy": 0.43603515625,
                        "total_cost": 1802900.5312292243
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.1035
Profiling... [2048/50176]	Loss: 2.0896
Profiling... [3072/50176]	Loss: 2.0993
Profiling... [4096/50176]	Loss: 2.0038
Profiling... [5120/50176]	Loss: 2.1871
Profiling... [6144/50176]	Loss: 2.1248
Profiling... [7168/50176]	Loss: 2.0657
Profiling... [8192/50176]	Loss: 2.1123
Profiling... [9216/50176]	Loss: 2.1001
Profiling... [10240/50176]	Loss: 2.0686
Profiling... [11264/50176]	Loss: 2.0049
Profiling... [12288/50176]	Loss: 2.1111
Profiling... [13312/50176]	Loss: 2.2118
Profile done
epoch 1 train time consumed: 12.97s
Validation Epoch: 6, Average loss: 0.0021, Accuracy: 0.4295
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.65172498421185,
                        "time": 8.996829102999982,
                        "accuracy": 0.4294921875,
                        "total_cost": 1834157.4400785298
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.1294
Profiling... [2048/50176]	Loss: 2.1444
Profiling... [3072/50176]	Loss: 2.1785
Profiling... [4096/50176]	Loss: 2.1075
Profiling... [5120/50176]	Loss: 2.1609
Profiling... [6144/50176]	Loss: 2.1680
Profiling... [7168/50176]	Loss: 2.0850
Profiling... [8192/50176]	Loss: 2.1377
Profiling... [9216/50176]	Loss: 2.0406
Profiling... [10240/50176]	Loss: 2.1127
Profiling... [11264/50176]	Loss: 2.0029
Profiling... [12288/50176]	Loss: 2.1055
Profiling... [13312/50176]	Loss: 2.0949
Profile done
epoch 1 train time consumed: 14.19s
Validation Epoch: 6, Average loss: 0.0021, Accuracy: 0.4309
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.66012187073443,
                        "time": 9.92012842799977,
                        "accuracy": 0.430859375,
                        "total_cost": 2015970.5223406123
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.1291
Profiling... [2048/50176]	Loss: 2.1401
Profiling... [3072/50176]	Loss: 2.0912
Profiling... [4096/50176]	Loss: 2.0536
Profiling... [5120/50176]	Loss: 2.0966
Profiling... [6144/50176]	Loss: 2.0620
Profiling... [7168/50176]	Loss: 2.1723
Profiling... [8192/50176]	Loss: 2.0939
Profiling... [9216/50176]	Loss: 2.0959
Profiling... [10240/50176]	Loss: 2.1368
Profiling... [11264/50176]	Loss: 2.0444
Profiling... [12288/50176]	Loss: 2.1493
Profiling... [13312/50176]	Loss: 2.1350
Profile done
epoch 1 train time consumed: 25.44s
Validation Epoch: 6, Average loss: 0.0021, Accuracy: 0.4316
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.53304440053913,
                        "time": 18.68222314600007,
                        "accuracy": 0.431640625,
                        "total_cost": 3789730.7642621426
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.0754
Profiling... [2048/50176]	Loss: 2.1311
Profiling... [3072/50176]	Loss: 2.0716
Profiling... [4096/50176]	Loss: 2.0958
Profiling... [5120/50176]	Loss: 2.0513
Profiling... [6144/50176]	Loss: 2.1056
Profiling... [7168/50176]	Loss: 2.0731
Profiling... [8192/50176]	Loss: 2.0755
Profiling... [9216/50176]	Loss: 2.0314
Profiling... [10240/50176]	Loss: 2.1003
Profiling... [11264/50176]	Loss: 2.1958
Profiling... [12288/50176]	Loss: 2.0915
Profiling... [13312/50176]	Loss: 2.1065
Profile done
epoch 1 train time consumed: 12.87s
Validation Epoch: 6, Average loss: 0.0021, Accuracy: 0.4329
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.56038313467113,
                        "time": 8.907851611000297,
                        "accuracy": 0.43291015625,
                        "total_cost": 1801678.856113645
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.1723
Profiling... [2048/50176]	Loss: 2.0989
Profiling... [3072/50176]	Loss: 2.1655
Profiling... [4096/50176]	Loss: 2.1003
Profiling... [5120/50176]	Loss: 2.1469
Profiling... [6144/50176]	Loss: 2.1774
Profiling... [7168/50176]	Loss: 2.1106
Profiling... [8192/50176]	Loss: 2.0321
Profiling... [9216/50176]	Loss: 2.0548
Profiling... [10240/50176]	Loss: 2.0809
Profiling... [11264/50176]	Loss: 2.0064
Profiling... [12288/50176]	Loss: 2.0952
Profiling... [13312/50176]	Loss: 2.1231
Profile done
epoch 1 train time consumed: 13.15s
Validation Epoch: 6, Average loss: 0.0020, Accuracy: 0.4320
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.62142368643481,
                        "time": 9.12048302600033,
                        "accuracy": 0.43203125,
                        "total_cost": 1848438.5264159825
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.0334
Profiling... [2048/50176]	Loss: 2.1697
Profiling... [3072/50176]	Loss: 2.0764
Profiling... [4096/50176]	Loss: 2.0782
Profiling... [5120/50176]	Loss: 2.0921
Profiling... [6144/50176]	Loss: 2.0588
Profiling... [7168/50176]	Loss: 2.1050
Profiling... [8192/50176]	Loss: 2.0504
Profiling... [9216/50176]	Loss: 2.0751
Profiling... [10240/50176]	Loss: 2.1244
Profiling... [11264/50176]	Loss: 2.1268
Profiling... [12288/50176]	Loss: 2.0121
Profiling... [13312/50176]	Loss: 2.1014
Profile done
epoch 1 train time consumed: 14.35s
Validation Epoch: 6, Average loss: 0.0021, Accuracy: 0.4252
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.62973582511589,
                        "time": 10.021419085999696,
                        "accuracy": 0.4251953125,
                        "total_cost": 2063683.5905248977
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.1111
Profiling... [2048/50176]	Loss: 2.1504
Profiling... [3072/50176]	Loss: 2.1146
Profiling... [4096/50176]	Loss: 2.0630
Profiling... [5120/50176]	Loss: 2.1510
Profiling... [6144/50176]	Loss: 2.0799
Profiling... [7168/50176]	Loss: 2.0517
Profiling... [8192/50176]	Loss: 2.2087
Profiling... [9216/50176]	Loss: 2.0906
Profiling... [10240/50176]	Loss: 2.0917
Profiling... [11264/50176]	Loss: 2.1177
Profiling... [12288/50176]	Loss: 2.1252
Profiling... [13312/50176]	Loss: 2.1395
Profile done
epoch 1 train time consumed: 25.04s
Validation Epoch: 6, Average loss: 0.0021, Accuracy: 0.4252
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.5047003296087,
                        "time": 18.142839746999925,
                        "accuracy": 0.4251953125,
                        "total_cost": 3736103.0026787203
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.0562
Profiling... [2048/50176]	Loss: 2.1758
Profiling... [3072/50176]	Loss: 2.3007
Profiling... [4096/50176]	Loss: 2.2503
Profiling... [5120/50176]	Loss: 2.1107
Profiling... [6144/50176]	Loss: 2.1557
Profiling... [7168/50176]	Loss: 2.2320
Profiling... [8192/50176]	Loss: 2.1443
Profiling... [9216/50176]	Loss: 2.1721
Profiling... [10240/50176]	Loss: 2.1877
Profiling... [11264/50176]	Loss: 2.2388
Profiling... [12288/50176]	Loss: 2.2659
Profiling... [13312/50176]	Loss: 2.2384
Profile done
epoch 1 train time consumed: 13.08s
Validation Epoch: 6, Average loss: 0.0022, Accuracy: 0.4006
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.53179918515887,
                        "time": 9.070305319999534,
                        "accuracy": 0.4005859375,
                        "total_cost": 1982569.0344018338
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.2649
Profiling... [2048/50176]	Loss: 2.1396
Profiling... [3072/50176]	Loss: 2.1549
Profiling... [4096/50176]	Loss: 2.1388
Profiling... [5120/50176]	Loss: 2.1925
Profiling... [6144/50176]	Loss: 2.2725
Profiling... [7168/50176]	Loss: 2.1472
Profiling... [8192/50176]	Loss: 2.1724
Profiling... [9216/50176]	Loss: 2.2552
Profiling... [10240/50176]	Loss: 2.1046
Profiling... [11264/50176]	Loss: 2.2268
Profiling... [12288/50176]	Loss: 2.1746
Profiling... [13312/50176]	Loss: 2.1763
Profile done
epoch 1 train time consumed: 13.16s
Validation Epoch: 6, Average loss: 0.0021, Accuracy: 0.4083
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.5903453889883,
                        "time": 9.118491955000536,
                        "accuracy": 0.40830078125,
                        "total_cost": 1955442.5690135313
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.0496
Profiling... [2048/50176]	Loss: 2.1838
Profiling... [3072/50176]	Loss: 2.1425
Profiling... [4096/50176]	Loss: 2.1210
Profiling... [5120/50176]	Loss: 2.2095
Profiling... [6144/50176]	Loss: 2.1086
Profiling... [7168/50176]	Loss: 2.1803
Profiling... [8192/50176]	Loss: 2.1480
Profiling... [9216/50176]	Loss: 2.1989
Profiling... [10240/50176]	Loss: 2.1647
Profiling... [11264/50176]	Loss: 2.1822
Profiling... [12288/50176]	Loss: 2.2620
Profiling... [13312/50176]	Loss: 2.1444
Profile done
epoch 1 train time consumed: 14.25s
Validation Epoch: 6, Average loss: 0.0022, Accuracy: 0.4014
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.59900004225953,
                        "time": 10.028603130000192,
                        "accuracy": 0.4013671875,
                        "total_cost": 2187765.9469274282
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.0712
Profiling... [2048/50176]	Loss: 2.1995
Profiling... [3072/50176]	Loss: 2.1944
Profiling... [4096/50176]	Loss: 2.1165
Profiling... [5120/50176]	Loss: 2.1894
Profiling... [6144/50176]	Loss: 2.1855
Profiling... [7168/50176]	Loss: 2.2228
Profiling... [8192/50176]	Loss: 2.1616
Profiling... [9216/50176]	Loss: 2.2159
Profiling... [10240/50176]	Loss: 2.2495
Profiling... [11264/50176]	Loss: 2.2030
Profiling... [12288/50176]	Loss: 2.1844
Profiling... [13312/50176]	Loss: 2.2167
Profile done
epoch 1 train time consumed: 25.55s
Validation Epoch: 6, Average loss: 0.0022, Accuracy: 0.3969
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.47638285175465,
                        "time": 18.739882595999916,
                        "accuracy": 0.396875,
                        "total_cost": 4134424.803533891
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.1961
Profiling... [2048/50176]	Loss: 2.1294
Profiling... [3072/50176]	Loss: 2.1554
Profiling... [4096/50176]	Loss: 2.1369
Profiling... [5120/50176]	Loss: 2.2442
Profiling... [6144/50176]	Loss: 2.1347
Profiling... [7168/50176]	Loss: 2.1413
Profiling... [8192/50176]	Loss: 2.1937
Profiling... [9216/50176]	Loss: 2.2312
Profiling... [10240/50176]	Loss: 2.1503
Profiling... [11264/50176]	Loss: 2.2936
Profiling... [12288/50176]	Loss: 2.2211
Profiling... [13312/50176]	Loss: 2.2273
Profile done
epoch 1 train time consumed: 12.87s
Validation Epoch: 6, Average loss: 0.0022, Accuracy: 0.4174
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.5016267702979,
                        "time": 8.921732961999624,
                        "accuracy": 0.4173828125,
                        "total_cost": 1871615.7702583182
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.1251
Profiling... [2048/50176]	Loss: 2.2485
Profiling... [3072/50176]	Loss: 2.2119
Profiling... [4096/50176]	Loss: 2.2007
Profiling... [5120/50176]	Loss: 2.1549
Profiling... [6144/50176]	Loss: 2.1465
Profiling... [7168/50176]	Loss: 2.2593
Profiling... [8192/50176]	Loss: 2.2513
Profiling... [9216/50176]	Loss: 2.1369
Profiling... [10240/50176]	Loss: 2.1003
Profiling... [11264/50176]	Loss: 2.2221
Profiling... [12288/50176]	Loss: 2.1912
Profiling... [13312/50176]	Loss: 2.2019
Profile done
epoch 1 train time consumed: 13.08s
Validation Epoch: 6, Average loss: 0.0022, Accuracy: 0.4020
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.55582174382144,
                        "time": 9.01995118099967,
                        "accuracy": 0.401953125,
                        "total_cost": 1964857.0021682188
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.1712
Profiling... [2048/50176]	Loss: 2.1489
Profiling... [3072/50176]	Loss: 2.1769
Profiling... [4096/50176]	Loss: 2.2021
Profiling... [5120/50176]	Loss: 2.0876
Profiling... [6144/50176]	Loss: 2.2966
Profiling... [7168/50176]	Loss: 2.2142
Profiling... [8192/50176]	Loss: 2.1723
Profiling... [9216/50176]	Loss: 2.1472
Profiling... [10240/50176]	Loss: 2.2792
Profiling... [11264/50176]	Loss: 2.1636
Profiling... [12288/50176]	Loss: 2.2622
Profiling... [13312/50176]	Loss: 2.2092
Profile done
epoch 1 train time consumed: 14.24s
Validation Epoch: 6, Average loss: 0.0022, Accuracy: 0.4079
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.55889180007965,
                        "time": 9.92981512300048,
                        "accuracy": 0.40791015625,
                        "total_cost": 2131468.0301023354
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.0944
Profiling... [2048/50176]	Loss: 2.2484
Profiling... [3072/50176]	Loss: 2.1527
Profiling... [4096/50176]	Loss: 2.2054
Profiling... [5120/50176]	Loss: 2.2225
Profiling... [6144/50176]	Loss: 2.3521
Profiling... [7168/50176]	Loss: 2.2215
Profiling... [8192/50176]	Loss: 2.1967
Profiling... [9216/50176]	Loss: 2.1864
Profiling... [10240/50176]	Loss: 2.2442
Profiling... [11264/50176]	Loss: 2.1634
Profiling... [12288/50176]	Loss: 2.1161
Profiling... [13312/50176]	Loss: 2.0763
Profile done
epoch 1 train time consumed: 25.49s
Validation Epoch: 6, Average loss: 0.0023, Accuracy: 0.3771
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.43728657498576,
                        "time": 18.64880496800015,
                        "accuracy": 0.3771484375,
                        "total_cost": 4329528.189094383
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.0990
Profiling... [2048/50176]	Loss: 2.1177
Profiling... [3072/50176]	Loss: 2.1617
Profiling... [4096/50176]	Loss: 2.1821
Profiling... [5120/50176]	Loss: 2.1233
Profiling... [6144/50176]	Loss: 2.2581
Profiling... [7168/50176]	Loss: 2.1563
Profiling... [8192/50176]	Loss: 2.1910
Profiling... [9216/50176]	Loss: 2.1538
Profiling... [10240/50176]	Loss: 2.2122
Profiling... [11264/50176]	Loss: 2.2605
Profiling... [12288/50176]	Loss: 2.1686
Profiling... [13312/50176]	Loss: 2.1715
Profile done
epoch 1 train time consumed: 12.92s
Validation Epoch: 6, Average loss: 0.0022, Accuracy: 0.4018
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.46073846967582,
                        "time": 8.918877864000024,
                        "accuracy": 0.4017578125,
                        "total_cost": 1943783.1878603413
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.1762
Profiling... [2048/50176]	Loss: 2.2957
Profiling... [3072/50176]	Loss: 2.1742
Profiling... [4096/50176]	Loss: 2.1243
Profiling... [5120/50176]	Loss: 2.1664
Profiling... [6144/50176]	Loss: 2.2616
Profiling... [7168/50176]	Loss: 2.1301
Profiling... [8192/50176]	Loss: 2.1461
Profiling... [9216/50176]	Loss: 2.2973
Profiling... [10240/50176]	Loss: 2.1853
Profiling... [11264/50176]	Loss: 2.1765
Profiling... [12288/50176]	Loss: 2.1407
Profiling... [13312/50176]	Loss: 2.2565
Profile done
epoch 1 train time consumed: 12.99s
Validation Epoch: 6, Average loss: 0.0022, Accuracy: 0.4040
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.5179755608584,
                        "time": 8.992864521999763,
                        "accuracy": 0.40400390625,
                        "total_cost": 1949012.2286999535
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.0818
Profiling... [2048/50176]	Loss: 2.2432
Profiling... [3072/50176]	Loss: 2.1838
Profiling... [4096/50176]	Loss: 2.2182
Profiling... [5120/50176]	Loss: 2.1093
Profiling... [6144/50176]	Loss: 2.2309
Profiling... [7168/50176]	Loss: 2.2686
Profiling... [8192/50176]	Loss: 2.2380
Profiling... [9216/50176]	Loss: 2.2433
Profiling... [10240/50176]	Loss: 2.2385
Profiling... [11264/50176]	Loss: 2.1225
Profiling... [12288/50176]	Loss: 2.1219
Profiling... [13312/50176]	Loss: 2.2108
Profile done
epoch 1 train time consumed: 14.09s
Validation Epoch: 6, Average loss: 0.0022, Accuracy: 0.4084
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.52651696793778,
                        "time": 9.904672288000256,
                        "accuracy": 0.4083984375,
                        "total_cost": 2123528.7129472066
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.1498
Profiling... [2048/50176]	Loss: 2.1872
Profiling... [3072/50176]	Loss: 2.2216
Profiling... [4096/50176]	Loss: 2.1792
Profiling... [5120/50176]	Loss: 2.2544
Profiling... [6144/50176]	Loss: 2.2024
Profiling... [7168/50176]	Loss: 2.2082
Profiling... [8192/50176]	Loss: 2.1452
Profiling... [9216/50176]	Loss: 2.2059
Profiling... [10240/50176]	Loss: 2.1685
Profiling... [11264/50176]	Loss: 2.1940
Profiling... [12288/50176]	Loss: 2.3092
Profiling... [13312/50176]	Loss: 2.1900
Profile done
epoch 1 train time consumed: 25.42s
Validation Epoch: 6, Average loss: 0.0022, Accuracy: 0.4033
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.40899198081257,
                        "time": 18.718266206000408,
                        "accuracy": 0.4033203125,
                        "total_cost": 4063659.6961418255
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.1271
Profiling... [2048/50176]	Loss: 2.4793
Profiling... [3072/50176]	Loss: 2.2245
Profiling... [4096/50176]	Loss: 2.3600
Profiling... [5120/50176]	Loss: 2.3476
Profiling... [6144/50176]	Loss: 2.2277
Profiling... [7168/50176]	Loss: 2.2957
Profiling... [8192/50176]	Loss: 2.3295
Profiling... [9216/50176]	Loss: 2.3397
Profiling... [10240/50176]	Loss: 2.3419
Profiling... [11264/50176]	Loss: 2.4189
Profiling... [12288/50176]	Loss: 2.3236
Profiling... [13312/50176]	Loss: 2.3735
Profile done
epoch 1 train time consumed: 12.97s
Validation Epoch: 6, Average loss: 0.0026, Accuracy: 0.3322
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.43143264249363,
                        "time": 8.914868523000223,
                        "accuracy": 0.3322265625,
                        "total_cost": 2349537.888283748
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.1439
Profiling... [2048/50176]	Loss: 2.2602
Profiling... [3072/50176]	Loss: 2.4645
Profiling... [4096/50176]	Loss: 2.3418
Profiling... [5120/50176]	Loss: 2.3755
Profiling... [6144/50176]	Loss: 2.4388
Profiling... [7168/50176]	Loss: 2.3530
Profiling... [8192/50176]	Loss: 2.2811
Profiling... [9216/50176]	Loss: 2.2879
Profiling... [10240/50176]	Loss: 2.3618
Profiling... [11264/50176]	Loss: 2.2614
Profiling... [12288/50176]	Loss: 2.3441
Profiling... [13312/50176]	Loss: 2.2723
Profile done
epoch 1 train time consumed: 12.96s
Validation Epoch: 6, Average loss: 0.0028, Accuracy: 0.3222
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.48446650718408,
                        "time": 9.003123795999272,
                        "accuracy": 0.32216796875,
                        "total_cost": 2446881.0489400025
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.0637
Profiling... [2048/50176]	Loss: 2.3863
Profiling... [3072/50176]	Loss: 2.3994
Profiling... [4096/50176]	Loss: 2.1893
Profiling... [5120/50176]	Loss: 2.3614
Profiling... [6144/50176]	Loss: 2.3343
Profiling... [7168/50176]	Loss: 2.3141
Profiling... [8192/50176]	Loss: 2.1747
Profiling... [9216/50176]	Loss: 2.3727
Profiling... [10240/50176]	Loss: 2.3568
Profiling... [11264/50176]	Loss: 2.4081
Profiling... [12288/50176]	Loss: 2.3066
Profiling... [13312/50176]	Loss: 2.2387
Profile done
epoch 1 train time consumed: 14.20s
Validation Epoch: 6, Average loss: 0.0033, Accuracy: 0.2469
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.49334431613977,
                        "time": 9.904971717999615,
                        "accuracy": 0.246875,
                        "total_cost": 3512999.9470868963
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.1938
Profiling... [2048/50176]	Loss: 2.3931
Profiling... [3072/50176]	Loss: 2.3183
Profiling... [4096/50176]	Loss: 2.2857
Profiling... [5120/50176]	Loss: 2.3577
Profiling... [6144/50176]	Loss: 2.4392
Profiling... [7168/50176]	Loss: 2.4608
Profiling... [8192/50176]	Loss: 2.3678
Profiling... [9216/50176]	Loss: 2.3907
Profiling... [10240/50176]	Loss: 2.2620
Profiling... [11264/50176]	Loss: 2.4265
Profiling... [12288/50176]	Loss: 2.3029
Profiling... [13312/50176]	Loss: 2.3467
Profile done
epoch 1 train time consumed: 25.59s
Validation Epoch: 6, Average loss: 0.0027, Accuracy: 0.3232
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.37481454460408,
                        "time": 18.767205725000167,
                        "accuracy": 0.3232421875,
                        "total_cost": 5083622.580007825
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.0715
Profiling... [2048/50176]	Loss: 2.4410
Profiling... [3072/50176]	Loss: 2.4466
Profiling... [4096/50176]	Loss: 2.3692
Profiling... [5120/50176]	Loss: 2.3127
Profiling... [6144/50176]	Loss: 2.3143
Profiling... [7168/50176]	Loss: 2.2596
Profiling... [8192/50176]	Loss: 2.3606
Profiling... [9216/50176]	Loss: 2.3935
Profiling... [10240/50176]	Loss: 2.3750
Profiling... [11264/50176]	Loss: 2.3912
Profiling... [12288/50176]	Loss: 2.3642
Profiling... [13312/50176]	Loss: 2.3462
Profile done
epoch 1 train time consumed: 12.87s
Validation Epoch: 6, Average loss: 0.0027, Accuracy: 0.3164
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.39912596062311,
                        "time": 8.904327629000363,
                        "accuracy": 0.31640625,
                        "total_cost": 2464097.342709893
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.0913
Profiling... [2048/50176]	Loss: 2.3543
Profiling... [3072/50176]	Loss: 2.4413
Profiling... [4096/50176]	Loss: 2.2413
Profiling... [5120/50176]	Loss: 2.3959
Profiling... [6144/50176]	Loss: 2.3347
Profiling... [7168/50176]	Loss: 2.3662
Profiling... [8192/50176]	Loss: 2.3784
Profiling... [9216/50176]	Loss: 2.3130
Profiling... [10240/50176]	Loss: 2.4161
Profiling... [11264/50176]	Loss: 2.3865
Profiling... [12288/50176]	Loss: 2.3316
Profiling... [13312/50176]	Loss: 2.2822
Profile done
epoch 1 train time consumed: 13.05s
Validation Epoch: 6, Average loss: 0.0027, Accuracy: 0.3286
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.45305856539413,
                        "time": 9.027544962999855,
                        "accuracy": 0.32861328125,
                        "total_cost": 2405395.3370717247
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.0886
Profiling... [2048/50176]	Loss: 2.4233
Profiling... [3072/50176]	Loss: 2.3122
Profiling... [4096/50176]	Loss: 2.4072
Profiling... [5120/50176]	Loss: 2.3618
Profiling... [6144/50176]	Loss: 2.4339
Profiling... [7168/50176]	Loss: 2.3939
Profiling... [8192/50176]	Loss: 2.4205
Profiling... [9216/50176]	Loss: 2.3408
Profiling... [10240/50176]	Loss: 2.3735
Profiling... [11264/50176]	Loss: 2.3555
Profiling... [12288/50176]	Loss: 2.3662
Profiling... [13312/50176]	Loss: 2.2987
Profile done
epoch 1 train time consumed: 14.11s
Validation Epoch: 6, Average loss: 0.0031, Accuracy: 0.2991
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.4625541965704,
                        "time": 9.90125605600042,
                        "accuracy": 0.29912109375,
                        "total_cost": 2898312.3793525873
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.1156
Profiling... [2048/50176]	Loss: 2.2722
Profiling... [3072/50176]	Loss: 2.3287
Profiling... [4096/50176]	Loss: 2.4294
Profiling... [5120/50176]	Loss: 2.3735
Profiling... [6144/50176]	Loss: 2.3093
Profiling... [7168/50176]	Loss: 2.3262
Profiling... [8192/50176]	Loss: 2.4342
Profiling... [9216/50176]	Loss: 2.3080
Profiling... [10240/50176]	Loss: 2.3890
Profiling... [11264/50176]	Loss: 2.3788
Profiling... [12288/50176]	Loss: 2.2826
Profiling... [13312/50176]	Loss: 2.2835
Profile done
epoch 1 train time consumed: 25.54s
Validation Epoch: 6, Average loss: 0.0025, Accuracy: 0.3469
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.34755131377544,
                        "time": 18.75810697900033,
                        "accuracy": 0.346875,
                        "total_cost": 4734974.698887647
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.1560
Profiling... [2048/50176]	Loss: 2.3754
Profiling... [3072/50176]	Loss: 2.2677
Profiling... [4096/50176]	Loss: 2.3341
Profiling... [5120/50176]	Loss: 2.4275
Profiling... [6144/50176]	Loss: 2.3226
Profiling... [7168/50176]	Loss: 2.3805
Profiling... [8192/50176]	Loss: 2.2795
Profiling... [9216/50176]	Loss: 2.2888
Profiling... [10240/50176]	Loss: 2.2796
Profiling... [11264/50176]	Loss: 2.2690
Profiling... [12288/50176]	Loss: 2.3689
Profiling... [13312/50176]	Loss: 2.3163
Profile done
epoch 1 train time consumed: 12.91s
Validation Epoch: 6, Average loss: 0.0030, Accuracy: 0.2835
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.37144763127291,
                        "time": 8.919027893999555,
                        "accuracy": 0.28349609375,
                        "total_cost": 2754686.3504061922
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.1447
Profiling... [2048/50176]	Loss: 2.3991
Profiling... [3072/50176]	Loss: 2.3569
Profiling... [4096/50176]	Loss: 2.3716
Profiling... [5120/50176]	Loss: 2.3287
Profiling... [6144/50176]	Loss: 2.3755
Profiling... [7168/50176]	Loss: 2.4130
Profiling... [8192/50176]	Loss: 2.3920
Profiling... [9216/50176]	Loss: 2.3394
Profiling... [10240/50176]	Loss: 2.3431
Profiling... [11264/50176]	Loss: 2.2563
Profiling... [12288/50176]	Loss: 2.3143
Profiling... [13312/50176]	Loss: 2.2815
Profile done
epoch 1 train time consumed: 13.16s
Validation Epoch: 6, Average loss: 0.0028, Accuracy: 0.3182
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.4223251701418,
                        "time": 9.03400292600054,
                        "accuracy": 0.3181640625,
                        "total_cost": 2486170.699561936
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.1285
Profiling... [2048/50176]	Loss: 2.3993
Profiling... [3072/50176]	Loss: 2.4513
Profiling... [4096/50176]	Loss: 2.4255
Profiling... [5120/50176]	Loss: 2.3192
Profiling... [6144/50176]	Loss: 2.3704
Profiling... [7168/50176]	Loss: 2.3511
Profiling... [8192/50176]	Loss: 2.3191
Profiling... [9216/50176]	Loss: 2.3210
Profiling... [10240/50176]	Loss: 2.3347
Profiling... [11264/50176]	Loss: 2.3208
Profiling... [12288/50176]	Loss: 2.2107
Profiling... [13312/50176]	Loss: 2.2976
Profile done
epoch 1 train time consumed: 14.19s
Validation Epoch: 6, Average loss: 0.0037, Accuracy: 0.2349
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.42809288322347,
                        "time": 9.922691965000013,
                        "accuracy": 0.23486328125,
                        "total_cost": 3699271.78508387
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.2419
Profiling... [2048/50176]	Loss: 2.5246
Profiling... [3072/50176]	Loss: 2.3287
Profiling... [4096/50176]	Loss: 2.2622
Profiling... [5120/50176]	Loss: 2.2751
Profiling... [6144/50176]	Loss: 2.4046
Profiling... [7168/50176]	Loss: 2.2917
Profiling... [8192/50176]	Loss: 2.3767
Profiling... [9216/50176]	Loss: 2.2322
Profiling... [10240/50176]	Loss: 2.3174
Profiling... [11264/50176]	Loss: 2.3559
Profiling... [12288/50176]	Loss: 2.3199
Profiling... [13312/50176]	Loss: 2.2517
Profile done
epoch 1 train time consumed: 25.63s
Validation Epoch: 6, Average loss: 0.0027, Accuracy: 0.3327
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.31542582832412,
                        "time": 18.748053658000572,
                        "accuracy": 0.33271484375,
                        "total_cost": 4933845.958145802
                    },
                    
[Training Loop] The optimal parameters are lr: 0.001 dr: 0.0 bs: 128 pl: 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[GPU_0] Set GPU power limit to 175W.
[GPU_0] Set GPU power limit to 175W.
Training Epoch: 6 [128/50048]	Loss: 1.8059
Training Epoch: 6 [256/50048]	Loss: 2.1181
Training Epoch: 6 [384/50048]	Loss: 2.0564
Training Epoch: 6 [512/50048]	Loss: 1.9012
Training Epoch: 6 [640/50048]	Loss: 2.0326
Training Epoch: 6 [768/50048]	Loss: 2.1081
Training Epoch: 6 [896/50048]	Loss: 2.0090
Training Epoch: 6 [1024/50048]	Loss: 1.8727
Training Epoch: 6 [1152/50048]	Loss: 2.0521
Training Epoch: 6 [1280/50048]	Loss: 1.9411
Training Epoch: 6 [1408/50048]	Loss: 2.1275
Training Epoch: 6 [1536/50048]	Loss: 1.8138
Training Epoch: 6 [1664/50048]	Loss: 2.1068
Training Epoch: 6 [1792/50048]	Loss: 2.0166
Training Epoch: 6 [1920/50048]	Loss: 2.0197
Training Epoch: 6 [2048/50048]	Loss: 2.1015
Training Epoch: 6 [2176/50048]	Loss: 1.9702
Training Epoch: 6 [2304/50048]	Loss: 2.1011
Training Epoch: 6 [2432/50048]	Loss: 2.2483
Training Epoch: 6 [2560/50048]	Loss: 2.0702
Training Epoch: 6 [2688/50048]	Loss: 2.5113
Training Epoch: 6 [2816/50048]	Loss: 1.9685
Training Epoch: 6 [2944/50048]	Loss: 2.2282
Training Epoch: 6 [3072/50048]	Loss: 2.0527
Training Epoch: 6 [3200/50048]	Loss: 2.0807
Training Epoch: 6 [3328/50048]	Loss: 1.8025
Training Epoch: 6 [3456/50048]	Loss: 2.2481
Training Epoch: 6 [3584/50048]	Loss: 2.0092
Training Epoch: 6 [3712/50048]	Loss: 2.1513
Training Epoch: 6 [3840/50048]	Loss: 2.1815
Training Epoch: 6 [3968/50048]	Loss: 1.9525
Training Epoch: 6 [4096/50048]	Loss: 2.0010
Training Epoch: 6 [4224/50048]	Loss: 2.1290
Training Epoch: 6 [4352/50048]	Loss: 2.0603
Training Epoch: 6 [4480/50048]	Loss: 2.1192
Training Epoch: 6 [4608/50048]	Loss: 1.9675
Training Epoch: 6 [4736/50048]	Loss: 2.2511
Training Epoch: 6 [4864/50048]	Loss: 1.7856
Training Epoch: 6 [4992/50048]	Loss: 2.2065
Training Epoch: 6 [5120/50048]	Loss: 1.9760
Training Epoch: 6 [5248/50048]	Loss: 2.1516
Training Epoch: 6 [5376/50048]	Loss: 2.0264
Training Epoch: 6 [5504/50048]	Loss: 2.3756
Training Epoch: 6 [5632/50048]	Loss: 1.8611
Training Epoch: 6 [5760/50048]	Loss: 2.0429
Training Epoch: 6 [5888/50048]	Loss: 2.2196
Training Epoch: 6 [6016/50048]	Loss: 1.9035
Training Epoch: 6 [6144/50048]	Loss: 2.1258
Training Epoch: 6 [6272/50048]	Loss: 2.1130
Training Epoch: 6 [6400/50048]	Loss: 1.8188
Training Epoch: 6 [6528/50048]	Loss: 2.0709
Training Epoch: 6 [6656/50048]	Loss: 2.3076
Training Epoch: 6 [6784/50048]	Loss: 1.9489
Training Epoch: 6 [6912/50048]	Loss: 2.1643
Training Epoch: 6 [7040/50048]	Loss: 2.1824
Training Epoch: 6 [7168/50048]	Loss: 2.1053
Training Epoch: 6 [7296/50048]	Loss: 1.9117
Training Epoch: 6 [7424/50048]	Loss: 1.7754
Training Epoch: 6 [7552/50048]	Loss: 2.1560
Training Epoch: 6 [7680/50048]	Loss: 2.2434
Training Epoch: 6 [7808/50048]	Loss: 2.2853
Training Epoch: 6 [7936/50048]	Loss: 2.1148
Training Epoch: 6 [8064/50048]	Loss: 1.9798
Training Epoch: 6 [8192/50048]	Loss: 2.3009
Training Epoch: 6 [8320/50048]	Loss: 1.9111
Training Epoch: 6 [8448/50048]	Loss: 2.1085
Training Epoch: 6 [8576/50048]	Loss: 2.2689
Training Epoch: 6 [8704/50048]	Loss: 2.0076
Training Epoch: 6 [8832/50048]	Loss: 1.8971
Training Epoch: 6 [8960/50048]	Loss: 1.9998
Training Epoch: 6 [9088/50048]	Loss: 1.9771
Training Epoch: 6 [9216/50048]	Loss: 1.9553
Training Epoch: 6 [9344/50048]	Loss: 2.0339
Training Epoch: 6 [9472/50048]	Loss: 2.1087
Training Epoch: 6 [9600/50048]	Loss: 2.1360
Training Epoch: 6 [9728/50048]	Loss: 2.2720
Training Epoch: 6 [9856/50048]	Loss: 2.0285
Training Epoch: 6 [9984/50048]	Loss: 2.0368
Training Epoch: 6 [10112/50048]	Loss: 1.9574
Training Epoch: 6 [10240/50048]	Loss: 2.3206
Training Epoch: 6 [10368/50048]	Loss: 2.2055
Training Epoch: 6 [10496/50048]	Loss: 2.0286
Training Epoch: 6 [10624/50048]	Loss: 2.0334
Training Epoch: 6 [10752/50048]	Loss: 2.2708
Training Epoch: 6 [10880/50048]	Loss: 2.0222
Training Epoch: 6 [11008/50048]	Loss: 2.1170
Training Epoch: 6 [11136/50048]	Loss: 1.9218
Training Epoch: 6 [11264/50048]	Loss: 2.1155
Training Epoch: 6 [11392/50048]	Loss: 1.8759
Training Epoch: 6 [11520/50048]	Loss: 2.3127
Training Epoch: 6 [11648/50048]	Loss: 2.1070
Training Epoch: 6 [11776/50048]	Loss: 2.0267
Training Epoch: 6 [11904/50048]	Loss: 2.2631
Training Epoch: 6 [12032/50048]	Loss: 1.9841
Training Epoch: 6 [12160/50048]	Loss: 2.1290
Training Epoch: 6 [12288/50048]	Loss: 2.0619
Training Epoch: 6 [12416/50048]	Loss: 2.4142
Training Epoch: 6 [12544/50048]	Loss: 1.8395
Training Epoch: 6 [12672/50048]	Loss: 2.0285
Training Epoch: 6 [12800/50048]	Loss: 2.1170
Training Epoch: 6 [12928/50048]	Loss: 2.0670
Training Epoch: 6 [13056/50048]	Loss: 2.1117
Training Epoch: 6 [13184/50048]	Loss: 2.0146
Training Epoch: 6 [13312/50048]	Loss: 1.8343
Training Epoch: 6 [13440/50048]	Loss: 2.3871
Training Epoch: 6 [13568/50048]	Loss: 1.8280
Training Epoch: 6 [13696/50048]	Loss: 1.9835
Training Epoch: 6 [13824/50048]	Loss: 2.2863
Training Epoch: 6 [13952/50048]	Loss: 2.0907
Training Epoch: 6 [14080/50048]	Loss: 2.2804
Training Epoch: 6 [14208/50048]	Loss: 2.0951
Training Epoch: 6 [14336/50048]	Loss: 2.0395
Training Epoch: 6 [14464/50048]	Loss: 1.9899
Training Epoch: 6 [14592/50048]	Loss: 2.1899
Training Epoch: 6 [14720/50048]	Loss: 2.2929
Training Epoch: 6 [14848/50048]	Loss: 1.7747
Training Epoch: 6 [14976/50048]	Loss: 2.1097
Training Epoch: 6 [15104/50048]	Loss: 2.1047
Training Epoch: 6 [15232/50048]	Loss: 2.2357
Training Epoch: 6 [15360/50048]	Loss: 1.8599
Training Epoch: 6 [15488/50048]	Loss: 2.0938
Training Epoch: 6 [15616/50048]	Loss: 2.1257
Training Epoch: 6 [15744/50048]	Loss: 2.1745
Training Epoch: 6 [15872/50048]	Loss: 1.9859
Training Epoch: 6 [16000/50048]	Loss: 2.2190
Training Epoch: 6 [16128/50048]	Loss: 2.0833
Training Epoch: 6 [16256/50048]	Loss: 1.9387
Training Epoch: 6 [16384/50048]	Loss: 1.9102
Training Epoch: 6 [16512/50048]	Loss: 2.2874
Training Epoch: 6 [16640/50048]	Loss: 1.9303
Training Epoch: 6 [16768/50048]	Loss: 2.1935
Training Epoch: 6 [16896/50048]	Loss: 1.9970
Training Epoch: 6 [17024/50048]	Loss: 1.9272
Training Epoch: 6 [17152/50048]	Loss: 1.8878
Training Epoch: 6 [17280/50048]	Loss: 1.8410
Training Epoch: 6 [17408/50048]	Loss: 2.2993
Training Epoch: 6 [17536/50048]	Loss: 2.0843
Training Epoch: 6 [17664/50048]	Loss: 2.1909
Training Epoch: 6 [17792/50048]	Loss: 2.1932
Training Epoch: 6 [17920/50048]	Loss: 2.1944
Training Epoch: 6 [18048/50048]	Loss: 1.8448
Training Epoch: 6 [18176/50048]	Loss: 2.0562
Training Epoch: 6 [18304/50048]	Loss: 1.7837
Training Epoch: 6 [18432/50048]	Loss: 2.1762
Training Epoch: 6 [18560/50048]	Loss: 1.8964
Training Epoch: 6 [18688/50048]	Loss: 1.9949
Training Epoch: 6 [18816/50048]	Loss: 2.1345
Training Epoch: 6 [18944/50048]	Loss: 2.0920
Training Epoch: 6 [19072/50048]	Loss: 2.0764
Training Epoch: 6 [19200/50048]	Loss: 1.9876
Training Epoch: 6 [19328/50048]	Loss: 2.2133
Training Epoch: 6 [19456/50048]	Loss: 2.1603
Training Epoch: 6 [19584/50048]	Loss: 2.1401
Training Epoch: 6 [19712/50048]	Loss: 1.8412
Training Epoch: 6 [19840/50048]	Loss: 2.0729
Training Epoch: 6 [19968/50048]	Loss: 1.9889
Training Epoch: 6 [20096/50048]	Loss: 2.0314
Training Epoch: 6 [20224/50048]	Loss: 1.9962
Training Epoch: 6 [20352/50048]	Loss: 2.1795
Training Epoch: 6 [20480/50048]	Loss: 1.9486
Training Epoch: 6 [20608/50048]	Loss: 1.8799
Training Epoch: 6 [20736/50048]	Loss: 2.3379
Training Epoch: 6 [20864/50048]	Loss: 1.7348
Training Epoch: 6 [20992/50048]	Loss: 1.8872
Training Epoch: 6 [21120/50048]	Loss: 2.0715
Training Epoch: 6 [21248/50048]	Loss: 1.9539
Training Epoch: 6 [21376/50048]	Loss: 2.2322
Training Epoch: 6 [21504/50048]	Loss: 2.0142
Training Epoch: 6 [21632/50048]	Loss: 2.1648
Training Epoch: 6 [21760/50048]	Loss: 1.7723
Training Epoch: 6 [21888/50048]	Loss: 1.8457
Training Epoch: 6 [22016/50048]	Loss: 2.1124
Training Epoch: 6 [22144/50048]	Loss: 2.1302
Training Epoch: 6 [22272/50048]	Loss: 2.0753
Training Epoch: 6 [22400/50048]	Loss: 2.0151
Training Epoch: 6 [22528/50048]	Loss: 1.9956
Training Epoch: 6 [22656/50048]	Loss: 2.0126
Training Epoch: 6 [22784/50048]	Loss: 2.1453
Training Epoch: 6 [22912/50048]	Loss: 2.2540
Training Epoch: 6 [23040/50048]	Loss: 1.8764
Training Epoch: 6 [23168/50048]	Loss: 1.8704
Training Epoch: 6 [23296/50048]	Loss: 1.9547
Training Epoch: 6 [23424/50048]	Loss: 2.0509
Training Epoch: 6 [23552/50048]	Loss: 2.2413
Training Epoch: 6 [23680/50048]	Loss: 2.1547
Training Epoch: 6 [23808/50048]	Loss: 1.8593
Training Epoch: 6 [23936/50048]	Loss: 2.2886
Training Epoch: 6 [24064/50048]	Loss: 1.9801
Training Epoch: 6 [24192/50048]	Loss: 2.0892
Training Epoch: 6 [24320/50048]	Loss: 2.1531
Training Epoch: 6 [24448/50048]	Loss: 2.1049
Training Epoch: 6 [24576/50048]	Loss: 1.9788
Training Epoch: 6 [24704/50048]	Loss: 2.0549
Training Epoch: 6 [24832/50048]	Loss: 2.3274
Training Epoch: 6 [24960/50048]	Loss: 2.0822
Training Epoch: 6 [25088/50048]	Loss: 1.9434
Training Epoch: 6 [25216/50048]	Loss: 2.1079
Training Epoch: 6 [25344/50048]	Loss: 2.1854
Training Epoch: 6 [25472/50048]	Loss: 1.9748
Training Epoch: 6 [25600/50048]	Loss: 1.9094
Training Epoch: 6 [25728/50048]	Loss: 2.0208
Training Epoch: 6 [25856/50048]	Loss: 2.0992
Training Epoch: 6 [25984/50048]	Loss: 1.6330
Training Epoch: 6 [26112/50048]	Loss: 1.9089
Training Epoch: 6 [26240/50048]	Loss: 2.2552
Training Epoch: 6 [26368/50048]	Loss: 2.0963
Training Epoch: 6 [26496/50048]	Loss: 1.8306
Training Epoch: 6 [26624/50048]	Loss: 2.0574
Training Epoch: 6 [26752/50048]	Loss: 2.1937
Training Epoch: 6 [26880/50048]	Loss: 2.1207
Training Epoch: 6 [27008/50048]	Loss: 1.9960
Training Epoch: 6 [27136/50048]	Loss: 2.2034
Training Epoch: 6 [27264/50048]	Loss: 1.9648
Training Epoch: 6 [27392/50048]	Loss: 2.0189
Training Epoch: 6 [27520/50048]	Loss: 1.8372
Training Epoch: 6 [27648/50048]	Loss: 2.0178
Training Epoch: 6 [27776/50048]	Loss: 1.9355
Training Epoch: 6 [27904/50048]	Loss: 2.3016
Training Epoch: 6 [28032/50048]	Loss: 2.0442
Training Epoch: 6 [28160/50048]	Loss: 2.1081
Training Epoch: 6 [28288/50048]	Loss: 2.0530
Training Epoch: 6 [28416/50048]	Loss: 1.7777
Training Epoch: 6 [28544/50048]	Loss: 1.9808
Training Epoch: 6 [28672/50048]	Loss: 2.2109
Training Epoch: 6 [28800/50048]	Loss: 1.9955
Training Epoch: 6 [28928/50048]	Loss: 2.0536
Training Epoch: 6 [29056/50048]	Loss: 2.0109
Training Epoch: 6 [29184/50048]	Loss: 2.2652
Training Epoch: 6 [29312/50048]	Loss: 2.0973
Training Epoch: 6 [29440/50048]	Loss: 1.8786
Training Epoch: 6 [29568/50048]	Loss: 1.6445
Training Epoch: 6 [29696/50048]	Loss: 2.2595
Training Epoch: 6 [29824/50048]	Loss: 2.2224
Training Epoch: 6 [29952/50048]	Loss: 1.9405
Training Epoch: 6 [30080/50048]	Loss: 2.0737
Training Epoch: 6 [30208/50048]	Loss: 1.9988
Training Epoch: 6 [30336/50048]	Loss: 2.0159
Training Epoch: 6 [30464/50048]	Loss: 2.2048
Training Epoch: 6 [30592/50048]	Loss: 2.1121
Training Epoch: 6 [30720/50048]	Loss: 1.9358
Training Epoch: 6 [30848/50048]	Loss: 1.9330
Training Epoch: 6 [30976/50048]	Loss: 2.2367
Training Epoch: 6 [31104/50048]	Loss: 1.8390
Training Epoch: 6 [31232/50048]	Loss: 2.0787
Training Epoch: 6 [31360/50048]	Loss: 2.1328
Training Epoch: 6 [31488/50048]	Loss: 1.9706
Training Epoch: 6 [31616/50048]	Loss: 1.9092
Training Epoch: 6 [31744/50048]	Loss: 2.1509
Training Epoch: 6 [31872/50048]	Loss: 1.9828
Training Epoch: 6 [32000/50048]	Loss: 2.3250
Training Epoch: 6 [32128/50048]	Loss: 2.1852
Training Epoch: 6 [32256/50048]	Loss: 1.9243
Training Epoch: 6 [32384/50048]	Loss: 1.9603
Training Epoch: 6 [32512/50048]	Loss: 2.1269
Training Epoch: 6 [32640/50048]	Loss: 2.1522
Training Epoch: 6 [32768/50048]	Loss: 2.0666
Training Epoch: 6 [32896/50048]	Loss: 1.9726
Training Epoch: 6 [33024/50048]	Loss: 2.1296
Training Epoch: 6 [33152/50048]	Loss: 1.8635
Training Epoch: 6 [33280/50048]	Loss: 2.2889
Training Epoch: 6 [33408/50048]	Loss: 2.1229
Training Epoch: 6 [33536/50048]	Loss: 2.2226
Training Epoch: 6 [33664/50048]	Loss: 1.8688
Training Epoch: 6 [33792/50048]	Loss: 2.3702
Training Epoch: 6 [33920/50048]	Loss: 2.2990
Training Epoch: 6 [34048/50048]	Loss: 2.2517
Training Epoch: 6 [34176/50048]	Loss: 2.0242
Training Epoch: 6 [34304/50048]	Loss: 2.0440
Training Epoch: 6 [34432/50048]	Loss: 1.9242
Training Epoch: 6 [34560/50048]	Loss: 1.9979
Training Epoch: 6 [34688/50048]	Loss: 2.1535
Training Epoch: 6 [34816/50048]	Loss: 1.7647
Training Epoch: 6 [34944/50048]	Loss: 2.1133
Training Epoch: 6 [35072/50048]	Loss: 1.9975
Training Epoch: 6 [35200/50048]	Loss: 2.1133
Training Epoch: 6 [35328/50048]	Loss: 2.1904
Training Epoch: 6 [35456/50048]	Loss: 2.3245
Training Epoch: 6 [35584/50048]	Loss: 1.9568
Training Epoch: 6 [35712/50048]	Loss: 2.0895
Training Epoch: 6 [35840/50048]	Loss: 2.0109
Training Epoch: 6 [35968/50048]	Loss: 2.1465
Training Epoch: 6 [36096/50048]	Loss: 2.1071
Training Epoch: 6 [36224/50048]	Loss: 1.7264
Training Epoch: 6 [36352/50048]	Loss: 2.1318
Training Epoch: 6 [36480/50048]	Loss: 1.7289
Training Epoch: 6 [36608/50048]	Loss: 1.9782
Training Epoch: 6 [36736/50048]	Loss: 1.9260
Training Epoch: 6 [36864/50048]	Loss: 1.9058
Training Epoch: 6 [36992/50048]	Loss: 1.7769
Training Epoch: 6 [37120/50048]	Loss: 2.0229
Training Epoch: 6 [37248/50048]	Loss: 2.0730
Training Epoch: 6 [37376/50048]	Loss: 1.7605
Training Epoch: 6 [37504/50048]	Loss: 2.0533
Training Epoch: 6 [37632/50048]	Loss: 1.8308
Training Epoch: 6 [37760/50048]	Loss: 2.1897
Training Epoch: 6 [37888/50048]	Loss: 1.8839
Training Epoch: 6 [38016/50048]	Loss: 1.8944
Training Epoch: 6 [38144/50048]	Loss: 1.9047
Training Epoch: 6 [38272/50048]	Loss: 1.9356
Training Epoch: 6 [38400/50048]	Loss: 1.8368
Training Epoch: 6 [38528/50048]	Loss: 1.9988
Training Epoch: 6 [38656/50048]	Loss: 1.9648
Training Epoch: 6 [38784/50048]	Loss: 1.7930
Training Epoch: 6 [38912/50048]	Loss: 1.7683
Training Epoch: 6 [39040/50048]	Loss: 2.0113
Training Epoch: 6 [39168/50048]	Loss: 2.1102
Training Epoch: 6 [39296/50048]	Loss: 1.9022
Training Epoch: 6 [39424/50048]	Loss: 2.1314
Training Epoch: 6 [39552/50048]	Loss: 1.9745
Training Epoch: 6 [39680/50048]	Loss: 2.0612
Training Epoch: 6 [39808/50048]	Loss: 2.1969
Training Epoch: 6 [39936/50048]	Loss: 1.9458
Training Epoch: 6 [40064/50048]	Loss: 1.8892
Training Epoch: 6 [40192/50048]	Loss: 1.8631
Training Epoch: 6 [40320/50048]	Loss: 1.9809
Training Epoch: 6 [40448/50048]	Loss: 1.8615
Training Epoch: 6 [40576/50048]	Loss: 2.0330
Training Epoch: 6 [40704/50048]	Loss: 2.2062
Training Epoch: 6 [40832/50048]	Loss: 1.9750
Training Epoch: 6 [40960/50048]	Loss: 1.9304
Training Epoch: 6 [41088/50048]	Loss: 1.9316
Training Epoch: 6 [41216/50048]	Loss: 2.0021
Training Epoch: 6 [41344/50048]	Loss: 2.1287
Training Epoch: 6 [41472/50048]	Loss: 2.1555
Training Epoch: 6 [41600/50048]	Loss: 1.7793
Training Epoch: 6 [41728/50048]	Loss: 2.1985
Training Epoch: 6 [41856/50048]	Loss: 1.9342
Training Epoch: 6 [41984/50048]	Loss: 1.8229
Training Epoch: 6 [42112/50048]	Loss: 2.1114
Training Epoch: 6 [42240/50048]	Loss: 1.9607
Training Epoch: 6 [42368/50048]	Loss: 2.1405
Training Epoch: 6 [42496/50048]	Loss: 2.0794
Training Epoch: 6 [42624/50048]	Loss: 2.2811
Training Epoch: 6 [42752/50048]	Loss: 1.9621
Training Epoch: 6 [42880/50048]	Loss: 2.1482
Training Epoch: 6 [43008/50048]	Loss: 2.1696
Training Epoch: 6 [43136/50048]	Loss: 2.0069
Training Epoch: 6 [43264/50048]	Loss: 2.1430
Training Epoch: 6 [43392/50048]	Loss: 1.7624
Training Epoch: 6 [43520/50048]	Loss: 1.9905
Training Epoch: 6 [43648/50048]	Loss: 2.0043
Training Epoch: 6 [43776/50048]	Loss: 2.1872
Training Epoch: 6 [43904/50048]	Loss: 1.8738
Training Epoch: 6 [44032/50048]	Loss: 2.0608
Training Epoch: 6 [44160/50048]	Loss: 2.2804
Training Epoch: 6 [44288/50048]	Loss: 2.2067
Training Epoch: 6 [44416/50048]	Loss: 1.8572
Training Epoch: 6 [44544/50048]	Loss: 1.9682
Training Epoch: 6 [44672/50048]	Loss: 1.9733
Training Epoch: 6 [44800/50048]	Loss: 2.3453
Training Epoch: 6 [44928/50048]	Loss: 2.0254
Training Epoch: 6 [45056/50048]	Loss: 1.9567
Training Epoch: 6 [45184/50048]	Loss: 1.8771
Training Epoch: 6 [45312/50048]	Loss: 2.2058
Training Epoch: 6 [45440/50048]	Loss: 2.1610
Training Epoch: 6 [45568/50048]	Loss: 2.1034
Training Epoch: 6 [45696/50048]	Loss: 1.7635
Training Epoch: 6 [45824/50048]	Loss: 2.0978
Training Epoch: 6 [45952/50048]	Loss: 1.7775
Training Epoch: 6 [46080/50048]	Loss: 1.8947
Training Epoch: 6 [46208/50048]	Loss: 1.8233
Training Epoch: 6 [46336/50048]	Loss: 1.6291
Training Epoch: 6 [46464/50048]	Loss: 1.8010
Training Epoch: 6 [46592/50048]	Loss: 1.9904
Training Epoch: 6 [46720/50048]	Loss: 2.0471
Training Epoch: 6 [46848/50048]	Loss: 1.7871
Training Epoch: 6 [46976/50048]	Loss: 2.2031
Training Epoch: 6 [47104/50048]	Loss: 1.9594
Training Epoch: 6 [47232/50048]	Loss: 1.8916
Training Epoch: 6 [47360/50048]	Loss: 2.0722
Training Epoch: 6 [47488/50048]	Loss: 1.9734
Training Epoch: 6 [47616/50048]	Loss: 2.1410
Training Epoch: 6 [47744/50048]	Loss: 2.1562
Training Epoch: 6 [47872/50048]	Loss: 2.0351
Training Epoch: 6 [48000/50048]	Loss: 1.8159
Training Epoch: 6 [48128/50048]	Loss: 1.9417
Training Epoch: 6 [48256/50048]	Loss: 2.1725
Training Epoch: 6 [48384/50048]	Loss: 1.9610
Training Epoch: 6 [48512/50048]	Loss: 2.2283
Training Epoch: 6 [48640/50048]	Loss: 2.0905
Training Epoch: 6 [48768/50048]	Loss: 1.9961
Training Epoch: 6 [48896/50048]	Loss: 2.1149
Training Epoch: 6 [49024/50048]	Loss: 1.9111
Training Epoch: 6 [49152/50048]	Loss: 2.0160
Training Epoch: 6 [49280/50048]	Loss: 1.6522
Training Epoch: 6 [49408/50048]	Loss: 2.1237
Training Epoch: 6 [49536/50048]	Loss: 2.0035
Training Epoch: 6 [49664/50048]	Loss: 2.1139
Training Epoch: 6 [49792/50048]	Loss: 1.9510
Training Epoch: 6 [49920/50048]	Loss: 1.9329
Training Epoch: 6 [50048/50048]	Loss: 1.9869
Validation Epoch: 6, Average loss: 0.0155, Accuracy: 0.4611
Training Epoch: 7 [128/50048]	Loss: 1.8360
Training Epoch: 7 [256/50048]	Loss: 2.1642
Training Epoch: 7 [384/50048]	Loss: 1.8212
Training Epoch: 7 [512/50048]	Loss: 1.9278
Training Epoch: 7 [640/50048]	Loss: 1.6309
Training Epoch: 7 [768/50048]	Loss: 1.7374
Training Epoch: 7 [896/50048]	Loss: 1.8307
Training Epoch: 7 [1024/50048]	Loss: 2.0501
Training Epoch: 7 [1152/50048]	Loss: 2.4019
Training Epoch: 7 [1280/50048]	Loss: 1.8911
Training Epoch: 7 [1408/50048]	Loss: 2.0052
Training Epoch: 7 [1536/50048]	Loss: 2.0015
Training Epoch: 7 [1664/50048]	Loss: 2.0336
Training Epoch: 7 [1792/50048]	Loss: 1.8700
Training Epoch: 7 [1920/50048]	Loss: 2.1386
Training Epoch: 7 [2048/50048]	Loss: 2.1981
Training Epoch: 7 [2176/50048]	Loss: 1.7059
Training Epoch: 7 [2304/50048]	Loss: 1.7900
Training Epoch: 7 [2432/50048]	Loss: 1.8630
Training Epoch: 7 [2560/50048]	Loss: 1.8413
Training Epoch: 7 [2688/50048]	Loss: 2.1969
Training Epoch: 7 [2816/50048]	Loss: 1.8819
Training Epoch: 7 [2944/50048]	Loss: 1.8926
Training Epoch: 7 [3072/50048]	Loss: 2.2584
Training Epoch: 7 [3200/50048]	Loss: 2.1567
Training Epoch: 7 [3328/50048]	Loss: 1.9940
Training Epoch: 7 [3456/50048]	Loss: 1.9653
Training Epoch: 7 [3584/50048]	Loss: 1.8803
Training Epoch: 7 [3712/50048]	Loss: 1.8461
Training Epoch: 7 [3840/50048]	Loss: 1.9794
Training Epoch: 7 [3968/50048]	Loss: 1.7104
Training Epoch: 7 [4096/50048]	Loss: 2.0213
Training Epoch: 7 [4224/50048]	Loss: 1.8435
Training Epoch: 7 [4352/50048]	Loss: 2.2497
Training Epoch: 7 [4480/50048]	Loss: 2.0448
Training Epoch: 7 [4608/50048]	Loss: 1.9791
Training Epoch: 7 [4736/50048]	Loss: 2.0126
Training Epoch: 7 [4864/50048]	Loss: 1.8006
Training Epoch: 7 [4992/50048]	Loss: 1.9217
Training Epoch: 7 [5120/50048]	Loss: 1.9207
Training Epoch: 7 [5248/50048]	Loss: 1.7163
Training Epoch: 7 [5376/50048]	Loss: 1.9072
Training Epoch: 7 [5504/50048]	Loss: 1.8702
Training Epoch: 7 [5632/50048]	Loss: 2.0621
Training Epoch: 7 [5760/50048]	Loss: 1.9819
Training Epoch: 7 [5888/50048]	Loss: 2.0957
Training Epoch: 7 [6016/50048]	Loss: 2.0076
Training Epoch: 7 [6144/50048]	Loss: 1.9592
Training Epoch: 7 [6272/50048]	Loss: 2.2081
Training Epoch: 7 [6400/50048]	Loss: 2.1009
Training Epoch: 7 [6528/50048]	Loss: 2.0983
Training Epoch: 7 [6656/50048]	Loss: 1.9695
Training Epoch: 7 [6784/50048]	Loss: 1.8777
Training Epoch: 7 [6912/50048]	Loss: 1.8828
Training Epoch: 7 [7040/50048]	Loss: 2.0589
Training Epoch: 7 [7168/50048]	Loss: 2.3057
Training Epoch: 7 [7296/50048]	Loss: 1.8505
Training Epoch: 7 [7424/50048]	Loss: 2.0022
Training Epoch: 7 [7552/50048]	Loss: 2.0727
Training Epoch: 7 [7680/50048]	Loss: 1.9632
Training Epoch: 7 [7808/50048]	Loss: 1.7227
Training Epoch: 7 [7936/50048]	Loss: 2.0158
Training Epoch: 7 [8064/50048]	Loss: 2.1162
Training Epoch: 7 [8192/50048]	Loss: 1.9029
Training Epoch: 7 [8320/50048]	Loss: 1.8296
Training Epoch: 7 [8448/50048]	Loss: 2.1699
Training Epoch: 7 [8576/50048]	Loss: 1.5733
Training Epoch: 7 [8704/50048]	Loss: 1.8607
Training Epoch: 7 [8832/50048]	Loss: 1.8983
Training Epoch: 7 [8960/50048]	Loss: 2.0227
Training Epoch: 7 [9088/50048]	Loss: 1.9842
Training Epoch: 7 [9216/50048]	Loss: 2.0688
Training Epoch: 7 [9344/50048]	Loss: 1.9769
Training Epoch: 7 [9472/50048]	Loss: 2.0145
Training Epoch: 7 [9600/50048]	Loss: 1.8800
Training Epoch: 7 [9728/50048]	Loss: 1.9036
Training Epoch: 7 [9856/50048]	Loss: 2.1121
Training Epoch: 7 [9984/50048]	Loss: 1.6511
Training Epoch: 7 [10112/50048]	Loss: 2.0909
Training Epoch: 7 [10240/50048]	Loss: 2.1635
Training Epoch: 7 [10368/50048]	Loss: 1.8895
Training Epoch: 7 [10496/50048]	Loss: 1.9772
Training Epoch: 7 [10624/50048]	Loss: 2.0156
Training Epoch: 7 [10752/50048]	Loss: 1.9410
Training Epoch: 7 [10880/50048]	Loss: 1.7928
Training Epoch: 7 [11008/50048]	Loss: 1.6720
Training Epoch: 7 [11136/50048]	Loss: 1.9493
Training Epoch: 7 [11264/50048]	Loss: 2.0740
Training Epoch: 7 [11392/50048]	Loss: 1.8639
Training Epoch: 7 [11520/50048]	Loss: 1.7819
Training Epoch: 7 [11648/50048]	Loss: 1.9535
Training Epoch: 7 [11776/50048]	Loss: 2.0468
Training Epoch: 7 [11904/50048]	Loss: 1.9536
Training Epoch: 7 [12032/50048]	Loss: 1.8988
Training Epoch: 7 [12160/50048]	Loss: 1.8536
Training Epoch: 7 [12288/50048]	Loss: 2.0427
Training Epoch: 7 [12416/50048]	Loss: 1.8304
Training Epoch: 7 [12544/50048]	Loss: 2.1766
Training Epoch: 7 [12672/50048]	Loss: 1.8076
Training Epoch: 7 [12800/50048]	Loss: 1.8547
Training Epoch: 7 [12928/50048]	Loss: 2.0086
Training Epoch: 7 [13056/50048]	Loss: 2.0612
Training Epoch: 7 [13184/50048]	Loss: 1.9476
Training Epoch: 7 [13312/50048]	Loss: 2.2487
Training Epoch: 7 [13440/50048]	Loss: 2.0126
Training Epoch: 7 [13568/50048]	Loss: 1.9462
Training Epoch: 7 [13696/50048]	Loss: 1.8770
Training Epoch: 7 [13824/50048]	Loss: 1.9370
Training Epoch: 7 [13952/50048]	Loss: 2.1353
Training Epoch: 7 [14080/50048]	Loss: 1.9297
Training Epoch: 7 [14208/50048]	Loss: 2.1158
Training Epoch: 7 [14336/50048]	Loss: 2.0118
Training Epoch: 7 [14464/50048]	Loss: 1.7850
Training Epoch: 7 [14592/50048]	Loss: 2.1299
Training Epoch: 7 [14720/50048]	Loss: 1.8180
Training Epoch: 7 [14848/50048]	Loss: 1.9761
Training Epoch: 7 [14976/50048]	Loss: 1.8988
Training Epoch: 7 [15104/50048]	Loss: 2.2691
Training Epoch: 7 [15232/50048]	Loss: 2.0470
Training Epoch: 7 [15360/50048]	Loss: 1.8400
Training Epoch: 7 [15488/50048]	Loss: 1.6433
Training Epoch: 7 [15616/50048]	Loss: 1.8470
Training Epoch: 7 [15744/50048]	Loss: 2.0631
Training Epoch: 7 [15872/50048]	Loss: 1.9325
Training Epoch: 7 [16000/50048]	Loss: 2.0915
Training Epoch: 7 [16128/50048]	Loss: 2.0881
Training Epoch: 7 [16256/50048]	Loss: 1.6232
Training Epoch: 7 [16384/50048]	Loss: 1.7658
Training Epoch: 7 [16512/50048]	Loss: 1.7090
Training Epoch: 7 [16640/50048]	Loss: 2.0582
Training Epoch: 7 [16768/50048]	Loss: 1.9241
Training Epoch: 7 [16896/50048]	Loss: 1.9602
Training Epoch: 7 [17024/50048]	Loss: 1.7120
Training Epoch: 7 [17152/50048]	Loss: 1.9851
Training Epoch: 7 [17280/50048]	Loss: 2.2655
Training Epoch: 7 [17408/50048]	Loss: 1.7575
Training Epoch: 7 [17536/50048]	Loss: 2.0001
Training Epoch: 7 [17664/50048]	Loss: 2.0499
Training Epoch: 7 [17792/50048]	Loss: 1.9402
Training Epoch: 7 [17920/50048]	Loss: 1.9460
Training Epoch: 7 [18048/50048]	Loss: 2.0031
Training Epoch: 7 [18176/50048]	Loss: 2.2958
Training Epoch: 7 [18304/50048]	Loss: 1.8282
Training Epoch: 7 [18432/50048]	Loss: 2.0581
Training Epoch: 7 [18560/50048]	Loss: 1.8566
Training Epoch: 7 [18688/50048]	Loss: 2.0526
Training Epoch: 7 [18816/50048]	Loss: 1.7578
Training Epoch: 7 [18944/50048]	Loss: 1.8281
Training Epoch: 7 [19072/50048]	Loss: 1.8200
Training Epoch: 7 [19200/50048]	Loss: 2.0696
Training Epoch: 7 [19328/50048]	Loss: 1.7907
Training Epoch: 7 [19456/50048]	Loss: 2.0495
Training Epoch: 7 [19584/50048]	Loss: 2.0386
Training Epoch: 7 [19712/50048]	Loss: 1.7437
Training Epoch: 7 [19840/50048]	Loss: 1.7985
Training Epoch: 7 [19968/50048]	Loss: 1.9966
Training Epoch: 7 [20096/50048]	Loss: 1.6491
Training Epoch: 7 [20224/50048]	Loss: 1.9186
Training Epoch: 7 [20352/50048]	Loss: 2.2183
Training Epoch: 7 [20480/50048]	Loss: 2.0226
Training Epoch: 7 [20608/50048]	Loss: 2.0497
Training Epoch: 7 [20736/50048]	Loss: 1.9766
Training Epoch: 7 [20864/50048]	Loss: 1.8647
Training Epoch: 7 [20992/50048]	Loss: 1.6909
Training Epoch: 7 [21120/50048]	Loss: 1.9425
Training Epoch: 7 [21248/50048]	Loss: 1.8091
Training Epoch: 7 [21376/50048]	Loss: 2.1729
Training Epoch: 7 [21504/50048]	Loss: 2.1595
Training Epoch: 7 [21632/50048]	Loss: 1.9445
Training Epoch: 7 [21760/50048]	Loss: 1.8207
Training Epoch: 7 [21888/50048]	Loss: 2.0106
Training Epoch: 7 [22016/50048]	Loss: 1.8392
Training Epoch: 7 [22144/50048]	Loss: 1.7612
Training Epoch: 7 [22272/50048]	Loss: 1.9782
Training Epoch: 7 [22400/50048]	Loss: 1.9400
Training Epoch: 7 [22528/50048]	Loss: 2.1265
Training Epoch: 7 [22656/50048]	Loss: 1.9252
Training Epoch: 7 [22784/50048]	Loss: 1.7385
Training Epoch: 7 [22912/50048]	Loss: 1.8522
Training Epoch: 7 [23040/50048]	Loss: 2.1199
Training Epoch: 7 [23168/50048]	Loss: 1.8839
Training Epoch: 7 [23296/50048]	Loss: 1.6644
Training Epoch: 7 [23424/50048]	Loss: 1.9245
Training Epoch: 7 [23552/50048]	Loss: 1.9362
Training Epoch: 7 [23680/50048]	Loss: 2.1563
Training Epoch: 7 [23808/50048]	Loss: 2.1291
Training Epoch: 7 [23936/50048]	Loss: 2.1223
Training Epoch: 7 [24064/50048]	Loss: 2.1322
Training Epoch: 7 [24192/50048]	Loss: 1.8929
Training Epoch: 7 [24320/50048]	Loss: 1.8246
Training Epoch: 7 [24448/50048]	Loss: 1.9723
Training Epoch: 7 [24576/50048]	Loss: 1.8929
Training Epoch: 7 [24704/50048]	Loss: 1.9031
Training Epoch: 7 [24832/50048]	Loss: 1.9660
Training Epoch: 7 [24960/50048]	Loss: 2.1648
Training Epoch: 7 [25088/50048]	Loss: 1.8023
Training Epoch: 7 [25216/50048]	Loss: 1.8521
Training Epoch: 7 [25344/50048]	Loss: 1.6666
Training Epoch: 7 [25472/50048]	Loss: 1.9074
Training Epoch: 7 [25600/50048]	Loss: 2.0790
Training Epoch: 7 [25728/50048]	Loss: 2.0001
Training Epoch: 7 [25856/50048]	Loss: 1.9511
Training Epoch: 7 [25984/50048]	Loss: 1.7694
Training Epoch: 7 [26112/50048]	Loss: 1.8507
Training Epoch: 7 [26240/50048]	Loss: 1.9579
Training Epoch: 7 [26368/50048]	Loss: 1.7247
Training Epoch: 7 [26496/50048]	Loss: 1.8747
Training Epoch: 7 [26624/50048]	Loss: 1.8793
Training Epoch: 7 [26752/50048]	Loss: 2.0415
Training Epoch: 7 [26880/50048]	Loss: 1.8962
Training Epoch: 7 [27008/50048]	Loss: 1.8362
Training Epoch: 7 [27136/50048]	Loss: 1.8434
Training Epoch: 7 [27264/50048]	Loss: 2.2104
Training Epoch: 7 [27392/50048]	Loss: 1.9137
Training Epoch: 7 [27520/50048]	Loss: 2.0791
Training Epoch: 7 [27648/50048]	Loss: 2.0105
Training Epoch: 7 [27776/50048]	Loss: 2.1969
Training Epoch: 7 [27904/50048]	Loss: 1.7819
Training Epoch: 7 [28032/50048]	Loss: 1.8129
Training Epoch: 7 [28160/50048]	Loss: 1.7870
Training Epoch: 7 [28288/50048]	Loss: 1.8400
Training Epoch: 7 [28416/50048]	Loss: 1.9691
Training Epoch: 7 [28544/50048]	Loss: 1.8978
Training Epoch: 7 [28672/50048]	Loss: 1.9930
Training Epoch: 7 [28800/50048]	Loss: 1.6358
Training Epoch: 7 [28928/50048]	Loss: 2.0179
Training Epoch: 7 [29056/50048]	Loss: 2.2606
Training Epoch: 7 [29184/50048]	Loss: 1.9393
Training Epoch: 7 [29312/50048]	Loss: 1.9650
Training Epoch: 7 [29440/50048]	Loss: 1.8084
Training Epoch: 7 [29568/50048]	Loss: 1.9925
Training Epoch: 7 [29696/50048]	Loss: 2.1230
Training Epoch: 7 [29824/50048]	Loss: 2.0022
Training Epoch: 7 [29952/50048]	Loss: 2.0937
Training Epoch: 7 [30080/50048]	Loss: 2.0337
Training Epoch: 7 [30208/50048]	Loss: 2.0957
Training Epoch: 7 [30336/50048]	Loss: 2.0628
Training Epoch: 7 [30464/50048]	Loss: 2.0805
Training Epoch: 7 [30592/50048]	Loss: 2.0472
Training Epoch: 7 [30720/50048]	Loss: 1.8577
Training Epoch: 7 [30848/50048]	Loss: 1.7001
Training Epoch: 7 [30976/50048]	Loss: 1.9771
Training Epoch: 7 [31104/50048]	Loss: 1.8023
Training Epoch: 7 [31232/50048]	Loss: 2.0368
Training Epoch: 7 [31360/50048]	Loss: 1.7848
Training Epoch: 7 [31488/50048]	Loss: 1.9576
Training Epoch: 7 [31616/50048]	Loss: 1.7608
Training Epoch: 7 [31744/50048]	Loss: 1.8504
Training Epoch: 7 [31872/50048]	Loss: 1.9008
Training Epoch: 7 [32000/50048]	Loss: 1.8523
Training Epoch: 7 [32128/50048]	Loss: 1.8313
Training Epoch: 7 [32256/50048]	Loss: 2.0656
Training Epoch: 7 [32384/50048]	Loss: 2.1002
Training Epoch: 7 [32512/50048]	Loss: 1.8168
Training Epoch: 7 [32640/50048]	Loss: 1.7287
Training Epoch: 7 [32768/50048]	Loss: 1.9035
Training Epoch: 7 [32896/50048]	Loss: 1.8732
Training Epoch: 7 [33024/50048]	Loss: 2.0286
Training Epoch: 7 [33152/50048]	Loss: 1.8309
Training Epoch: 7 [33280/50048]	Loss: 1.8842
Training Epoch: 7 [33408/50048]	Loss: 1.9950
Training Epoch: 7 [33536/50048]	Loss: 1.5227
Training Epoch: 7 [33664/50048]	Loss: 2.0698
Training Epoch: 7 [33792/50048]	Loss: 1.9953
Training Epoch: 7 [33920/50048]	Loss: 2.0009
Training Epoch: 7 [34048/50048]	Loss: 1.7596
Training Epoch: 7 [34176/50048]	Loss: 1.8867
Training Epoch: 7 [34304/50048]	Loss: 2.1169
Training Epoch: 7 [34432/50048]	Loss: 1.9287
Training Epoch: 7 [34560/50048]	Loss: 2.0565
Training Epoch: 7 [34688/50048]	Loss: 1.8935
Training Epoch: 7 [34816/50048]	Loss: 1.8255
Training Epoch: 7 [34944/50048]	Loss: 1.8413
Training Epoch: 7 [35072/50048]	Loss: 1.9213
Training Epoch: 7 [35200/50048]	Loss: 2.0505
Training Epoch: 7 [35328/50048]	Loss: 1.8904
Training Epoch: 7 [35456/50048]	Loss: 1.8749
Training Epoch: 7 [35584/50048]	Loss: 1.8481
Training Epoch: 7 [35712/50048]	Loss: 2.1697
Training Epoch: 7 [35840/50048]	Loss: 1.9573
Training Epoch: 7 [35968/50048]	Loss: 1.9028
Training Epoch: 7 [36096/50048]	Loss: 1.8537
Training Epoch: 7 [36224/50048]	Loss: 1.8322
Training Epoch: 7 [36352/50048]	Loss: 1.9756
Training Epoch: 7 [36480/50048]	Loss: 1.9925
Training Epoch: 7 [36608/50048]	Loss: 1.8634
Training Epoch: 7 [36736/50048]	Loss: 2.1609
Training Epoch: 7 [36864/50048]	Loss: 1.7738
Training Epoch: 7 [36992/50048]	Loss: 2.3152
Training Epoch: 7 [37120/50048]	Loss: 2.0036
Training Epoch: 7 [37248/50048]	Loss: 1.7807
Training Epoch: 7 [37376/50048]	Loss: 1.8705
Training Epoch: 7 [37504/50048]	Loss: 1.5828
Training Epoch: 7 [37632/50048]	Loss: 1.9627
Training Epoch: 7 [37760/50048]	Loss: 2.0273
Training Epoch: 7 [37888/50048]	Loss: 2.0189
Training Epoch: 7 [38016/50048]	Loss: 1.9307
Training Epoch: 7 [38144/50048]	Loss: 1.8246
Training Epoch: 7 [38272/50048]	Loss: 1.9851
Training Epoch: 7 [38400/50048]	Loss: 1.9338
Training Epoch: 7 [38528/50048]	Loss: 2.0720
Training Epoch: 7 [38656/50048]	Loss: 1.8652
Training Epoch: 7 [38784/50048]	Loss: 1.9581
Training Epoch: 7 [38912/50048]	Loss: 1.7039
Training Epoch: 7 [39040/50048]	Loss: 2.1609
Training Epoch: 7 [39168/50048]	Loss: 1.9428
Training Epoch: 7 [39296/50048]	Loss: 1.8448
Training Epoch: 7 [39424/50048]	Loss: 1.7654
Training Epoch: 7 [39552/50048]	Loss: 1.9850
Training Epoch: 7 [39680/50048]	Loss: 2.1364
Training Epoch: 7 [39808/50048]	Loss: 1.9061
Training Epoch: 7 [39936/50048]	Loss: 2.1691
Training Epoch: 7 [40064/50048]	Loss: 2.1925
Training Epoch: 7 [40192/50048]	Loss: 1.7687
Training Epoch: 7 [40320/50048]	Loss: 1.8358
Training Epoch: 7 [40448/50048]	Loss: 1.7981
Training Epoch: 7 [40576/50048]	Loss: 1.7091
Training Epoch: 7 [40704/50048]	Loss: 1.8200
Training Epoch: 7 [40832/50048]	Loss: 1.8246
Training Epoch: 7 [40960/50048]	Loss: 1.8330
Training Epoch: 7 [41088/50048]	Loss: 1.9320
Training Epoch: 7 [41216/50048]	Loss: 1.9293
Training Epoch: 7 [41344/50048]	Loss: 1.8473
Training Epoch: 7 [41472/50048]	Loss: 2.2092
Training Epoch: 7 [41600/50048]	Loss: 1.9092
Training Epoch: 7 [41728/50048]	Loss: 1.8364
Training Epoch: 7 [41856/50048]	Loss: 1.7015
Training Epoch: 7 [41984/50048]	Loss: 1.8734
Training Epoch: 7 [42112/50048]	Loss: 1.7453
Training Epoch: 7 [42240/50048]	Loss: 2.1296
Training Epoch: 7 [42368/50048]	Loss: 1.9424
Training Epoch: 7 [42496/50048]	Loss: 1.7577
Training Epoch: 7 [42624/50048]	Loss: 1.7983
Training Epoch: 7 [42752/50048]	Loss: 2.0228
Training Epoch: 7 [42880/50048]	Loss: 1.8791
Training Epoch: 7 [43008/50048]	Loss: 2.1693
Training Epoch: 7 [43136/50048]	Loss: 1.8560
Training Epoch: 7 [43264/50048]	Loss: 1.8705
Training Epoch: 7 [43392/50048]	Loss: 1.8313
Training Epoch: 7 [43520/50048]	Loss: 1.9583
Training Epoch: 7 [43648/50048]	Loss: 2.0333
Training Epoch: 7 [43776/50048]	Loss: 2.0231
Training Epoch: 7 [43904/50048]	Loss: 1.6790
Training Epoch: 7 [44032/50048]	Loss: 2.0099
Training Epoch: 7 [44160/50048]	Loss: 1.9837
Training Epoch: 7 [44288/50048]	Loss: 2.0407
Training Epoch: 7 [44416/50048]	Loss: 1.7309
Training Epoch: 7 [44544/50048]	Loss: 1.9722
Training Epoch: 7 [44672/50048]	Loss: 1.8148
Training Epoch: 7 [44800/50048]	Loss: 2.0468
Training Epoch: 7 [44928/50048]	Loss: 1.9622
Training Epoch: 7 [45056/50048]	Loss: 2.0142
Training Epoch: 7 [45184/50048]	Loss: 1.8445
Training Epoch: 7 [45312/50048]	Loss: 1.9070
Training Epoch: 7 [45440/50048]	Loss: 1.8011
Training Epoch: 7 [45568/50048]	Loss: 1.9830
Training Epoch: 7 [45696/50048]	Loss: 2.0383
Training Epoch: 7 [45824/50048]	Loss: 1.7599
Training Epoch: 7 [45952/50048]	Loss: 1.9446
Training Epoch: 7 [46080/50048]	Loss: 2.0313
Training Epoch: 7 [46208/50048]	Loss: 1.7905
Training Epoch: 7 [46336/50048]	Loss: 1.6790
Training Epoch: 7 [46464/50048]	Loss: 1.9875
Training Epoch: 7 [46592/50048]	Loss: 2.1419
Training Epoch: 7 [46720/50048]	Loss: 1.8081
Training Epoch: 7 [46848/50048]	Loss: 1.9284
Training Epoch: 7 [46976/50048]	Loss: 1.9883
Training Epoch: 7 [47104/50048]	Loss: 2.0077
Training Epoch: 7 [47232/50048]	Loss: 1.7967
Training Epoch: 7 [47360/50048]	Loss: 2.0653
Training Epoch: 7 [47488/50048]	Loss: 1.9869
Training Epoch: 7 [47616/50048]	Loss: 1.9317
Training Epoch: 7 [47744/50048]	Loss: 2.1264
Training Epoch: 7 [47872/50048]	Loss: 1.8929
Training Epoch: 7 [48000/50048]	Loss: 1.7895
Training Epoch: 7 [48128/50048]	Loss: 2.1180
Training Epoch: 7 [48256/50048]	Loss: 1.7830
Training Epoch: 7 [48384/50048]	Loss: 1.9463
Training Epoch: 7 [48512/50048]	Loss: 1.8800
Training Epoch: 7 [48640/50048]	Loss: 2.0914
Training Epoch: 7 [48768/50048]	Loss: 1.7017
Training Epoch: 7 [48896/50048]	Loss: 1.9098
Training Epoch: 7 [49024/50048]	Loss: 2.0947
Training Epoch: 7 [49152/50048]	Loss: 2.2275
Training Epoch: 7 [49280/50048]	Loss: 1.9154
Training Epoch: 7 [49408/50048]	Loss: 1.9005
Training Epoch: 7 [49536/50048]	Loss: 1.8721
Training Epoch: 7 [49664/50048]	Loss: 1.7552
Training Epoch: 7 [49792/50048]	Loss: 1.9923
Training Epoch: 7 [49920/50048]	Loss: 1.8200
Training Epoch: 7 [50048/50048]	Loss: 1.9471
Validation Epoch: 7, Average loss: 0.0149, Accuracy: 0.4778
Training Epoch: 8 [128/50048]	Loss: 1.7332
Training Epoch: 8 [256/50048]	Loss: 2.0392
Training Epoch: 8 [384/50048]	Loss: 1.8602
Training Epoch: 8 [512/50048]	Loss: 1.9297
Training Epoch: 8 [640/50048]	Loss: 1.7508
Training Epoch: 8 [768/50048]	Loss: 1.8237
Training Epoch: 8 [896/50048]	Loss: 1.7738
Training Epoch: 8 [1024/50048]	Loss: 2.0919
Training Epoch: 8 [1152/50048]	Loss: 1.8538
Training Epoch: 8 [1280/50048]	Loss: 1.6606
Training Epoch: 8 [1408/50048]	Loss: 1.9539
Training Epoch: 8 [1536/50048]	Loss: 1.6395
Training Epoch: 8 [1664/50048]	Loss: 1.7608
Training Epoch: 8 [1792/50048]	Loss: 1.8389
Training Epoch: 8 [1920/50048]	Loss: 1.7767
Training Epoch: 8 [2048/50048]	Loss: 1.6845
Training Epoch: 8 [2176/50048]	Loss: 2.0622
Training Epoch: 8 [2304/50048]	Loss: 1.7796
Training Epoch: 8 [2432/50048]	Loss: 1.8833
Training Epoch: 8 [2560/50048]	Loss: 1.7968
Training Epoch: 8 [2688/50048]	Loss: 2.0380
Training Epoch: 8 [2816/50048]	Loss: 1.7522
Training Epoch: 8 [2944/50048]	Loss: 2.1200
Training Epoch: 8 [3072/50048]	Loss: 1.7831
Training Epoch: 8 [3200/50048]	Loss: 1.8513
Training Epoch: 8 [3328/50048]	Loss: 1.7885
Training Epoch: 8 [3456/50048]	Loss: 1.6805
Training Epoch: 8 [3584/50048]	Loss: 1.9387
Training Epoch: 8 [3712/50048]	Loss: 1.8256
Training Epoch: 8 [3840/50048]	Loss: 1.7697
Training Epoch: 8 [3968/50048]	Loss: 1.6768
Training Epoch: 8 [4096/50048]	Loss: 1.9544
Training Epoch: 8 [4224/50048]	Loss: 1.7751
Training Epoch: 8 [4352/50048]	Loss: 1.5626
Training Epoch: 8 [4480/50048]	Loss: 1.8065
Training Epoch: 8 [4608/50048]	Loss: 1.8150
Training Epoch: 8 [4736/50048]	Loss: 1.8379
Training Epoch: 8 [4864/50048]	Loss: 1.9298
Training Epoch: 8 [4992/50048]	Loss: 1.9675
Training Epoch: 8 [5120/50048]	Loss: 1.6972
Training Epoch: 8 [5248/50048]	Loss: 1.7476
Training Epoch: 8 [5376/50048]	Loss: 1.7549
Training Epoch: 8 [5504/50048]	Loss: 1.7588
Training Epoch: 8 [5632/50048]	Loss: 2.1307
Training Epoch: 8 [5760/50048]	Loss: 1.6648
Training Epoch: 8 [5888/50048]	Loss: 1.9551
Training Epoch: 8 [6016/50048]	Loss: 1.8567
Training Epoch: 8 [6144/50048]	Loss: 1.9251
Training Epoch: 8 [6272/50048]	Loss: 1.8950
Training Epoch: 8 [6400/50048]	Loss: 1.7110
Training Epoch: 8 [6528/50048]	Loss: 1.7158
Training Epoch: 8 [6656/50048]	Loss: 1.7258
Training Epoch: 8 [6784/50048]	Loss: 1.8050
Training Epoch: 8 [6912/50048]	Loss: 1.8831
Training Epoch: 8 [7040/50048]	Loss: 1.8678
Training Epoch: 8 [7168/50048]	Loss: 1.7008
Training Epoch: 8 [7296/50048]	Loss: 1.9288
Training Epoch: 8 [7424/50048]	Loss: 1.8122
Training Epoch: 8 [7552/50048]	Loss: 2.1481
Training Epoch: 8 [7680/50048]	Loss: 2.0317
Training Epoch: 8 [7808/50048]	Loss: 1.7523
Training Epoch: 8 [7936/50048]	Loss: 1.9115
Training Epoch: 8 [8064/50048]	Loss: 1.7147
Training Epoch: 8 [8192/50048]	Loss: 1.8846
Training Epoch: 8 [8320/50048]	Loss: 1.8133
Training Epoch: 8 [8448/50048]	Loss: 1.6407
Training Epoch: 8 [8576/50048]	Loss: 1.7780
Training Epoch: 8 [8704/50048]	Loss: 1.9307
Training Epoch: 8 [8832/50048]	Loss: 1.6762
Training Epoch: 8 [8960/50048]	Loss: 1.5905
Training Epoch: 8 [9088/50048]	Loss: 1.7222
Training Epoch: 8 [9216/50048]	Loss: 1.8794
Training Epoch: 8 [9344/50048]	Loss: 1.5805
Training Epoch: 8 [9472/50048]	Loss: 1.6752
Training Epoch: 8 [9600/50048]	Loss: 1.6367
Training Epoch: 8 [9728/50048]	Loss: 1.7420
Training Epoch: 8 [9856/50048]	Loss: 1.9437
Training Epoch: 8 [9984/50048]	Loss: 1.7196
Training Epoch: 8 [10112/50048]	Loss: 1.9985
Training Epoch: 8 [10240/50048]	Loss: 1.5496
Training Epoch: 8 [10368/50048]	Loss: 1.7926
Training Epoch: 8 [10496/50048]	Loss: 1.7818
Training Epoch: 8 [10624/50048]	Loss: 1.7516
Training Epoch: 8 [10752/50048]	Loss: 1.9174
Training Epoch: 8 [10880/50048]	Loss: 1.7366
Training Epoch: 8 [11008/50048]	Loss: 1.8218
Training Epoch: 8 [11136/50048]	Loss: 1.7729
Training Epoch: 8 [11264/50048]	Loss: 1.7914
Training Epoch: 8 [11392/50048]	Loss: 1.8241
Training Epoch: 8 [11520/50048]	Loss: 1.5231
Training Epoch: 8 [11648/50048]	Loss: 1.5743
Training Epoch: 8 [11776/50048]	Loss: 1.8071
Training Epoch: 8 [11904/50048]	Loss: 2.1604
Training Epoch: 8 [12032/50048]	Loss: 1.8514
Training Epoch: 8 [12160/50048]	Loss: 1.7134
Training Epoch: 8 [12288/50048]	Loss: 1.8345
Training Epoch: 8 [12416/50048]	Loss: 2.2509
Training Epoch: 8 [12544/50048]	Loss: 1.9657
Training Epoch: 8 [12672/50048]	Loss: 1.7909
Training Epoch: 8 [12800/50048]	Loss: 1.6468
Training Epoch: 8 [12928/50048]	Loss: 2.0924
Training Epoch: 8 [13056/50048]	Loss: 2.1316
Training Epoch: 8 [13184/50048]	Loss: 1.8993
Training Epoch: 8 [13312/50048]	Loss: 2.0268
Training Epoch: 8 [13440/50048]	Loss: 1.7477
Training Epoch: 8 [13568/50048]	Loss: 1.9823
Training Epoch: 8 [13696/50048]	Loss: 2.2898
Training Epoch: 8 [13824/50048]	Loss: 1.7024
Training Epoch: 8 [13952/50048]	Loss: 1.8849
Training Epoch: 8 [14080/50048]	Loss: 1.8390
Training Epoch: 8 [14208/50048]	Loss: 1.8182
Training Epoch: 8 [14336/50048]	Loss: 2.0702
Training Epoch: 8 [14464/50048]	Loss: 1.6215
Training Epoch: 8 [14592/50048]	Loss: 1.6698
Training Epoch: 8 [14720/50048]	Loss: 2.0035
Training Epoch: 8 [14848/50048]	Loss: 1.7798
Training Epoch: 8 [14976/50048]	Loss: 2.0784
Training Epoch: 8 [15104/50048]	Loss: 2.0091
Training Epoch: 8 [15232/50048]	Loss: 1.7573
Training Epoch: 8 [15360/50048]	Loss: 1.9145
Training Epoch: 8 [15488/50048]	Loss: 1.7344
Training Epoch: 8 [15616/50048]	Loss: 1.8540
Training Epoch: 8 [15744/50048]	Loss: 1.9031
Training Epoch: 8 [15872/50048]	Loss: 1.8133
Training Epoch: 8 [16000/50048]	Loss: 1.8238
Training Epoch: 8 [16128/50048]	Loss: 1.6908
Training Epoch: 8 [16256/50048]	Loss: 2.0797
Training Epoch: 8 [16384/50048]	Loss: 1.8087
Training Epoch: 8 [16512/50048]	Loss: 2.0991
Training Epoch: 8 [16640/50048]	Loss: 2.0504
Training Epoch: 8 [16768/50048]	Loss: 1.7595
Training Epoch: 8 [16896/50048]	Loss: 1.8607
Training Epoch: 8 [17024/50048]	Loss: 2.1175
Training Epoch: 8 [17152/50048]	Loss: 1.9023
Training Epoch: 8 [17280/50048]	Loss: 1.9811
Training Epoch: 8 [17408/50048]	Loss: 1.7120
Training Epoch: 8 [17536/50048]	Loss: 1.7681
Training Epoch: 8 [17664/50048]	Loss: 1.7837
Training Epoch: 8 [17792/50048]	Loss: 1.8527
Training Epoch: 8 [17920/50048]	Loss: 1.9577
Training Epoch: 8 [18048/50048]	Loss: 1.7670
Training Epoch: 8 [18176/50048]	Loss: 1.8002
Training Epoch: 8 [18304/50048]	Loss: 2.1898
Training Epoch: 8 [18432/50048]	Loss: 2.0015
Training Epoch: 8 [18560/50048]	Loss: 1.9673
Training Epoch: 8 [18688/50048]	Loss: 1.6598
Training Epoch: 8 [18816/50048]	Loss: 1.8206
Training Epoch: 8 [18944/50048]	Loss: 2.0831
Training Epoch: 8 [19072/50048]	Loss: 1.7018
Training Epoch: 8 [19200/50048]	Loss: 2.0745
Training Epoch: 8 [19328/50048]	Loss: 1.8174
Training Epoch: 8 [19456/50048]	Loss: 1.5014
Training Epoch: 8 [19584/50048]	Loss: 1.8542
Training Epoch: 8 [19712/50048]	Loss: 1.8487
Training Epoch: 8 [19840/50048]	Loss: 2.1382
Training Epoch: 8 [19968/50048]	Loss: 2.2432
Training Epoch: 8 [20096/50048]	Loss: 1.9774
Training Epoch: 8 [20224/50048]	Loss: 1.6436
Training Epoch: 8 [20352/50048]	Loss: 1.8541
Training Epoch: 8 [20480/50048]	Loss: 1.9975
Training Epoch: 8 [20608/50048]	Loss: 1.8020
Training Epoch: 8 [20736/50048]	Loss: 1.7438
Training Epoch: 8 [20864/50048]	Loss: 2.0728
Training Epoch: 8 [20992/50048]	Loss: 1.8343
Training Epoch: 8 [21120/50048]	Loss: 1.6982
Training Epoch: 8 [21248/50048]	Loss: 1.9687
Training Epoch: 8 [21376/50048]	Loss: 1.6241
Training Epoch: 8 [21504/50048]	Loss: 1.7776
Training Epoch: 8 [21632/50048]	Loss: 1.8184
Training Epoch: 8 [21760/50048]	Loss: 1.8165
Training Epoch: 8 [21888/50048]	Loss: 2.1321
Training Epoch: 8 [22016/50048]	Loss: 1.8606
Training Epoch: 8 [22144/50048]	Loss: 1.9405
Training Epoch: 8 [22272/50048]	Loss: 2.0387
Training Epoch: 8 [22400/50048]	Loss: 1.7480
Training Epoch: 8 [22528/50048]	Loss: 1.8584
Training Epoch: 8 [22656/50048]	Loss: 1.8092
Training Epoch: 8 [22784/50048]	Loss: 1.9399
Training Epoch: 8 [22912/50048]	Loss: 2.2324
Training Epoch: 8 [23040/50048]	Loss: 1.9362
Training Epoch: 8 [23168/50048]	Loss: 2.0756
Training Epoch: 8 [23296/50048]	Loss: 1.9355
Training Epoch: 8 [23424/50048]	Loss: 1.9567
Training Epoch: 8 [23552/50048]	Loss: 2.0070
Training Epoch: 8 [23680/50048]	Loss: 1.6136
Training Epoch: 8 [23808/50048]	Loss: 1.8117
Training Epoch: 8 [23936/50048]	Loss: 1.7289
Training Epoch: 8 [24064/50048]	Loss: 1.7316
Training Epoch: 8 [24192/50048]	Loss: 2.0145
Training Epoch: 8 [24320/50048]	Loss: 1.7902
Training Epoch: 8 [24448/50048]	Loss: 1.8984
Training Epoch: 8 [24576/50048]	Loss: 1.9238
Training Epoch: 8 [24704/50048]	Loss: 1.9260
Training Epoch: 8 [24832/50048]	Loss: 1.9412
Training Epoch: 8 [24960/50048]	Loss: 1.8084
Training Epoch: 8 [25088/50048]	Loss: 1.9211
Training Epoch: 8 [25216/50048]	Loss: 1.8120
Training Epoch: 8 [25344/50048]	Loss: 1.8173
Training Epoch: 8 [25472/50048]	Loss: 1.6766
Training Epoch: 8 [25600/50048]	Loss: 1.6416
Training Epoch: 8 [25728/50048]	Loss: 1.8543
Training Epoch: 8 [25856/50048]	Loss: 2.1896
Training Epoch: 8 [25984/50048]	Loss: 1.7669
Training Epoch: 8 [26112/50048]	Loss: 2.0840
Training Epoch: 8 [26240/50048]	Loss: 1.5394
Training Epoch: 8 [26368/50048]	Loss: 2.0653
Training Epoch: 8 [26496/50048]	Loss: 1.8744
Training Epoch: 8 [26624/50048]	Loss: 1.9727
Training Epoch: 8 [26752/50048]	Loss: 1.9165
Training Epoch: 8 [26880/50048]	Loss: 1.8102
Training Epoch: 8 [27008/50048]	Loss: 1.8440
Training Epoch: 8 [27136/50048]	Loss: 2.1248
Training Epoch: 8 [27264/50048]	Loss: 1.9612
Training Epoch: 8 [27392/50048]	Loss: 1.9317
Training Epoch: 8 [27520/50048]	Loss: 1.9214
Training Epoch: 8 [27648/50048]	Loss: 1.8674
Training Epoch: 8 [27776/50048]	Loss: 1.6836
Training Epoch: 8 [27904/50048]	Loss: 1.8030
Training Epoch: 8 [28032/50048]	Loss: 2.0393
Training Epoch: 8 [28160/50048]	Loss: 1.9686
Training Epoch: 8 [28288/50048]	Loss: 2.0647
Training Epoch: 8 [28416/50048]	Loss: 1.7154
Training Epoch: 8 [28544/50048]	Loss: 1.6556
Training Epoch: 8 [28672/50048]	Loss: 1.9726
Training Epoch: 8 [28800/50048]	Loss: 1.9636
Training Epoch: 8 [28928/50048]	Loss: 1.9419
Training Epoch: 8 [29056/50048]	Loss: 1.8023
Training Epoch: 8 [29184/50048]	Loss: 1.7368
Training Epoch: 8 [29312/50048]	Loss: 1.9676
Training Epoch: 8 [29440/50048]	Loss: 2.0361
Training Epoch: 8 [29568/50048]	Loss: 1.7517
Training Epoch: 8 [29696/50048]	Loss: 1.6607
Training Epoch: 8 [29824/50048]	Loss: 1.7892
Training Epoch: 8 [29952/50048]	Loss: 2.0754
Training Epoch: 8 [30080/50048]	Loss: 2.0550
Training Epoch: 8 [30208/50048]	Loss: 1.8026
Training Epoch: 8 [30336/50048]	Loss: 2.0792
Training Epoch: 8 [30464/50048]	Loss: 1.6266
Training Epoch: 8 [30592/50048]	Loss: 1.8322
Training Epoch: 8 [30720/50048]	Loss: 2.0798
Training Epoch: 8 [30848/50048]	Loss: 1.9412
Training Epoch: 8 [30976/50048]	Loss: 1.9438
Training Epoch: 8 [31104/50048]	Loss: 1.9900
Training Epoch: 8 [31232/50048]	Loss: 1.7906
Training Epoch: 8 [31360/50048]	Loss: 2.0938
Training Epoch: 8 [31488/50048]	Loss: 1.7065
Training Epoch: 8 [31616/50048]	Loss: 1.6219
Training Epoch: 8 [31744/50048]	Loss: 1.9184
Training Epoch: 8 [31872/50048]	Loss: 1.9693
Training Epoch: 8 [32000/50048]	Loss: 1.7994
Training Epoch: 8 [32128/50048]	Loss: 1.7062
Training Epoch: 8 [32256/50048]	Loss: 1.8833
Training Epoch: 8 [32384/50048]	Loss: 2.1141
Training Epoch: 8 [32512/50048]	Loss: 1.9543
Training Epoch: 8 [32640/50048]	Loss: 1.8539
Training Epoch: 8 [32768/50048]	Loss: 1.7539
Training Epoch: 8 [32896/50048]	Loss: 1.9936
Training Epoch: 8 [33024/50048]	Loss: 2.0732
Training Epoch: 8 [33152/50048]	Loss: 1.6341
Training Epoch: 8 [33280/50048]	Loss: 1.7365
Training Epoch: 8 [33408/50048]	Loss: 2.0669
Training Epoch: 8 [33536/50048]	Loss: 1.9379
Training Epoch: 8 [33664/50048]	Loss: 1.7581
Training Epoch: 8 [33792/50048]	Loss: 1.4881
Training Epoch: 8 [33920/50048]	Loss: 1.8938
Training Epoch: 8 [34048/50048]	Loss: 2.0463
Training Epoch: 8 [34176/50048]	Loss: 1.5840
Training Epoch: 8 [34304/50048]	Loss: 1.8984
Training Epoch: 8 [34432/50048]	Loss: 1.8942
Training Epoch: 8 [34560/50048]	Loss: 2.1699
Training Epoch: 8 [34688/50048]	Loss: 2.0377
Training Epoch: 8 [34816/50048]	Loss: 1.9942
Training Epoch: 8 [34944/50048]	Loss: 1.8595
Training Epoch: 8 [35072/50048]	Loss: 1.6585
Training Epoch: 8 [35200/50048]	Loss: 1.6121
Training Epoch: 8 [35328/50048]	Loss: 1.8904
Training Epoch: 8 [35456/50048]	Loss: 1.9405
Training Epoch: 8 [35584/50048]	Loss: 1.8775
Training Epoch: 8 [35712/50048]	Loss: 1.6911
Training Epoch: 8 [35840/50048]	Loss: 1.8673
Training Epoch: 8 [35968/50048]	Loss: 1.9243
Training Epoch: 8 [36096/50048]	Loss: 1.8500
Training Epoch: 8 [36224/50048]	Loss: 1.9349
Training Epoch: 8 [36352/50048]	Loss: 1.8308
Training Epoch: 8 [36480/50048]	Loss: 1.7079
Training Epoch: 8 [36608/50048]	Loss: 1.8469
Training Epoch: 8 [36736/50048]	Loss: 2.3036
Training Epoch: 8 [36864/50048]	Loss: 1.9199
Training Epoch: 8 [36992/50048]	Loss: 2.0055
Training Epoch: 8 [37120/50048]	Loss: 1.8832
Training Epoch: 8 [37248/50048]	Loss: 1.8962
Training Epoch: 8 [37376/50048]	Loss: 1.8604
Training Epoch: 8 [37504/50048]	Loss: 1.4907
Training Epoch: 8 [37632/50048]	Loss: 1.9785
Training Epoch: 8 [37760/50048]	Loss: 2.0408
Training Epoch: 8 [37888/50048]	Loss: 1.9053
Training Epoch: 8 [38016/50048]	Loss: 1.9198
Training Epoch: 8 [38144/50048]	Loss: 1.8647
Training Epoch: 8 [38272/50048]	Loss: 2.0210
Training Epoch: 8 [38400/50048]	Loss: 1.6971
Training Epoch: 8 [38528/50048]	Loss: 1.8729
Training Epoch: 8 [38656/50048]	Loss: 1.6432
Training Epoch: 8 [38784/50048]	Loss: 1.6235
Training Epoch: 8 [38912/50048]	Loss: 1.7890
Training Epoch: 8 [39040/50048]	Loss: 1.9814
Training Epoch: 8 [39168/50048]	Loss: 1.9404
Training Epoch: 8 [39296/50048]	Loss: 2.0350
Training Epoch: 8 [39424/50048]	Loss: 1.8944
Training Epoch: 8 [39552/50048]	Loss: 2.0002
Training Epoch: 8 [39680/50048]	Loss: 1.6254
Training Epoch: 8 [39808/50048]	Loss: 1.7272
Training Epoch: 8 [39936/50048]	Loss: 2.0411
Training Epoch: 8 [40064/50048]	Loss: 1.9125
Training Epoch: 8 [40192/50048]	Loss: 1.7378
Training Epoch: 8 [40320/50048]	Loss: 1.8906
Training Epoch: 8 [40448/50048]	Loss: 1.7441
Training Epoch: 8 [40576/50048]	Loss: 2.0814
Training Epoch: 8 [40704/50048]	Loss: 1.7714
Training Epoch: 8 [40832/50048]	Loss: 2.0556
Training Epoch: 8 [40960/50048]	Loss: 1.7735
Training Epoch: 8 [41088/50048]	Loss: 1.7658
Training Epoch: 8 [41216/50048]	Loss: 1.8906
Training Epoch: 8 [41344/50048]	Loss: 1.7936
Training Epoch: 8 [41472/50048]	Loss: 2.1027
Training Epoch: 8 [41600/50048]	Loss: 2.1280
Training Epoch: 8 [41728/50048]	Loss: 1.5796
Training Epoch: 8 [41856/50048]	Loss: 1.8621
Training Epoch: 8 [41984/50048]	Loss: 2.0129
Training Epoch: 8 [42112/50048]	Loss: 1.6746
Training Epoch: 8 [42240/50048]	Loss: 2.1085
Training Epoch: 8 [42368/50048]	Loss: 1.8600
Training Epoch: 8 [42496/50048]	Loss: 1.8826
Training Epoch: 8 [42624/50048]	Loss: 1.8606
Training Epoch: 8 [42752/50048]	Loss: 1.6563
Training Epoch: 8 [42880/50048]	Loss: 1.8658
Training Epoch: 8 [43008/50048]	Loss: 1.7950
Training Epoch: 8 [43136/50048]	Loss: 1.9638
Training Epoch: 8 [43264/50048]	Loss: 1.8455
Training Epoch: 8 [43392/50048]	Loss: 1.7992
Training Epoch: 8 [43520/50048]	Loss: 1.8925
Training Epoch: 8 [43648/50048]	Loss: 1.9760
Training Epoch: 8 [43776/50048]	Loss: 1.8466
Training Epoch: 8 [43904/50048]	Loss: 1.7356
Training Epoch: 8 [44032/50048]	Loss: 1.7423
Training Epoch: 8 [44160/50048]	Loss: 1.6379
Training Epoch: 8 [44288/50048]	Loss: 1.8192
Training Epoch: 8 [44416/50048]	Loss: 1.9964
Training Epoch: 8 [44544/50048]	Loss: 1.7739
Training Epoch: 8 [44672/50048]	Loss: 1.7157
Training Epoch: 8 [44800/50048]	Loss: 1.9405
Training Epoch: 8 [44928/50048]	Loss: 1.6469
Training Epoch: 8 [45056/50048]	Loss: 1.8593
Training Epoch: 8 [45184/50048]	Loss: 1.8829
Training Epoch: 8 [45312/50048]	Loss: 1.9710
Training Epoch: 8 [45440/50048]	Loss: 1.7108
Training Epoch: 8 [45568/50048]	Loss: 1.7629
Training Epoch: 8 [45696/50048]	Loss: 1.3600
Training Epoch: 8 [45824/50048]	Loss: 1.8872
Training Epoch: 8 [45952/50048]	Loss: 1.7705
Training Epoch: 8 [46080/50048]	Loss: 1.8644
Training Epoch: 8 [46208/50048]	Loss: 1.7866
Training Epoch: 8 [46336/50048]	Loss: 1.7420
Training Epoch: 8 [46464/50048]	Loss: 1.9965
Training Epoch: 8 [46592/50048]	Loss: 1.7397
Training Epoch: 8 [46720/50048]	Loss: 1.7024
Training Epoch: 8 [46848/50048]	Loss: 1.6086
Training Epoch: 8 [46976/50048]	Loss: 1.9216
Training Epoch: 8 [47104/50048]	Loss: 1.6420
Training Epoch: 8 [47232/50048]	Loss: 1.8468
Training Epoch: 8 [47360/50048]	Loss: 1.8936
Training Epoch: 8 [47488/50048]	Loss: 1.9914
Training Epoch: 8 [47616/50048]	Loss: 1.8685
Training Epoch: 8 [47744/50048]	Loss: 1.8054
Training Epoch: 8 [47872/50048]	Loss: 1.9095
Training Epoch: 8 [48000/50048]	Loss: 1.7365
Training Epoch: 8 [48128/50048]	Loss: 1.7305
Training Epoch: 8 [48256/50048]	Loss: 1.6488
Training Epoch: 8 [48384/50048]	Loss: 1.8809
Training Epoch: 8 [48512/50048]	Loss: 1.7069
Training Epoch: 8 [48640/50048]	Loss: 1.9725
Training Epoch: 8 [48768/50048]	Loss: 1.8818
Training Epoch: 8 [48896/50048]	Loss: 1.7264
Training Epoch: 8 [49024/50048]	Loss: 1.8589
Training Epoch: 8 [49152/50048]	Loss: 1.7995
Training Epoch: 8 [49280/50048]	Loss: 2.0275
Training Epoch: 8 [49408/50048]	Loss: 1.6628
Training Epoch: 8 [49536/50048]	Loss: 1.9058
Training Epoch: 8 [49664/50048]	Loss: 2.3321
Training Epoch: 8 [49792/50048]	Loss: 1.7941
Training Epoch: 8 [49920/50048]	Loss: 1.8240
Training Epoch: 8 [50048/50048]	Loss: 1.6889
Validation Epoch: 8, Average loss: 0.0147, Accuracy: 0.4887
Training Epoch: 9 [128/50048]	Loss: 1.8762
Training Epoch: 9 [256/50048]	Loss: 1.6537
Training Epoch: 9 [384/50048]	Loss: 1.7048
Training Epoch: 9 [512/50048]	Loss: 1.7674
Training Epoch: 9 [640/50048]	Loss: 1.7427
Training Epoch: 9 [768/50048]	Loss: 1.6464
Training Epoch: 9 [896/50048]	Loss: 1.7589
Training Epoch: 9 [1024/50048]	Loss: 1.7957
Training Epoch: 9 [1152/50048]	Loss: 1.5381
Training Epoch: 9 [1280/50048]	Loss: 1.7924
Training Epoch: 9 [1408/50048]	Loss: 1.5992
Training Epoch: 9 [1536/50048]	Loss: 1.4152
Training Epoch: 9 [1664/50048]	Loss: 1.7760
Training Epoch: 9 [1792/50048]	Loss: 1.6378
Training Epoch: 9 [1920/50048]	Loss: 1.7495
Training Epoch: 9 [2048/50048]	Loss: 1.6990
Training Epoch: 9 [2176/50048]	Loss: 1.8440
Training Epoch: 9 [2304/50048]	Loss: 1.6272
Training Epoch: 9 [2432/50048]	Loss: 1.6746
Training Epoch: 9 [2560/50048]	Loss: 1.8585
Training Epoch: 9 [2688/50048]	Loss: 1.6038
Training Epoch: 9 [2816/50048]	Loss: 1.7179
Training Epoch: 9 [2944/50048]	Loss: 1.7818
Training Epoch: 9 [3072/50048]	Loss: 1.5968
Training Epoch: 9 [3200/50048]	Loss: 1.8622
Training Epoch: 9 [3328/50048]	Loss: 1.6869
Training Epoch: 9 [3456/50048]	Loss: 1.8120
Training Epoch: 9 [3584/50048]	Loss: 1.8576
Training Epoch: 9 [3712/50048]	Loss: 1.9525
Training Epoch: 9 [3840/50048]	Loss: 1.8832
Training Epoch: 9 [3968/50048]	Loss: 1.5401
Training Epoch: 9 [4096/50048]	Loss: 1.8720
Training Epoch: 9 [4224/50048]	Loss: 1.7181
Training Epoch: 9 [4352/50048]	Loss: 1.7666
Training Epoch: 9 [4480/50048]	Loss: 2.0634
Training Epoch: 9 [4608/50048]	Loss: 1.7748
Training Epoch: 9 [4736/50048]	Loss: 1.7352
Training Epoch: 9 [4864/50048]	Loss: 1.6453
Training Epoch: 9 [4992/50048]	Loss: 1.5249
Training Epoch: 9 [5120/50048]	Loss: 1.6651
Training Epoch: 9 [5248/50048]	Loss: 1.6794
Training Epoch: 9 [5376/50048]	Loss: 1.4727
Training Epoch: 9 [5504/50048]	Loss: 1.8665
Training Epoch: 9 [5632/50048]	Loss: 1.5789
Training Epoch: 9 [5760/50048]	Loss: 2.1705
Training Epoch: 9 [5888/50048]	Loss: 1.5864
Training Epoch: 9 [6016/50048]	Loss: 1.9464
Training Epoch: 9 [6144/50048]	Loss: 1.7067
Training Epoch: 9 [6272/50048]	Loss: 1.7696
Training Epoch: 9 [6400/50048]	Loss: 1.7334
Training Epoch: 9 [6528/50048]	Loss: 1.6264
Training Epoch: 9 [6656/50048]	Loss: 1.6053
Training Epoch: 9 [6784/50048]	Loss: 1.8443
Training Epoch: 9 [6912/50048]	Loss: 1.7005
Training Epoch: 9 [7040/50048]	Loss: 1.6681
Training Epoch: 9 [7168/50048]	Loss: 1.7447
Training Epoch: 9 [7296/50048]	Loss: 1.6036
Training Epoch: 9 [7424/50048]	Loss: 1.9272
Training Epoch: 9 [7552/50048]	Loss: 1.7363
Training Epoch: 9 [7680/50048]	Loss: 1.9469
Training Epoch: 9 [7808/50048]	Loss: 1.7594
Training Epoch: 9 [7936/50048]	Loss: 1.6701
Training Epoch: 9 [8064/50048]	Loss: 1.9217
Training Epoch: 9 [8192/50048]	Loss: 1.7426
Training Epoch: 9 [8320/50048]	Loss: 1.5766
Training Epoch: 9 [8448/50048]	Loss: 2.0079
Training Epoch: 9 [8576/50048]	Loss: 1.6964
Training Epoch: 9 [8704/50048]	Loss: 1.8375
Training Epoch: 9 [8832/50048]	Loss: 1.6281
Training Epoch: 9 [8960/50048]	Loss: 1.9530
Training Epoch: 9 [9088/50048]	Loss: 2.0000
Training Epoch: 9 [9216/50048]	Loss: 1.8434
Training Epoch: 9 [9344/50048]	Loss: 1.6447
Training Epoch: 9 [9472/50048]	Loss: 1.8989
Training Epoch: 9 [9600/50048]	Loss: 2.0609
Training Epoch: 9 [9728/50048]	Loss: 1.9975
Training Epoch: 9 [9856/50048]	Loss: 1.5091
Training Epoch: 9 [9984/50048]	Loss: 1.7855
Training Epoch: 9 [10112/50048]	Loss: 1.9882
Training Epoch: 9 [10240/50048]	Loss: 1.6488
Training Epoch: 9 [10368/50048]	Loss: 1.8694
Training Epoch: 9 [10496/50048]	Loss: 1.6317
Training Epoch: 9 [10624/50048]	Loss: 1.4617
Training Epoch: 9 [10752/50048]	Loss: 1.6993
Training Epoch: 9 [10880/50048]	Loss: 1.7853
Training Epoch: 9 [11008/50048]	Loss: 1.9396
Training Epoch: 9 [11136/50048]	Loss: 1.6543
Training Epoch: 9 [11264/50048]	Loss: 1.8539
Training Epoch: 9 [11392/50048]	Loss: 1.9541
Training Epoch: 9 [11520/50048]	Loss: 1.7912
Training Epoch: 9 [11648/50048]	Loss: 1.8445
Training Epoch: 9 [11776/50048]	Loss: 1.7765
Training Epoch: 9 [11904/50048]	Loss: 1.5979
Training Epoch: 9 [12032/50048]	Loss: 1.8029
Training Epoch: 9 [12160/50048]	Loss: 1.8317
Training Epoch: 9 [12288/50048]	Loss: 2.0652
Training Epoch: 9 [12416/50048]	Loss: 2.1371
Training Epoch: 9 [12544/50048]	Loss: 1.5286
Training Epoch: 9 [12672/50048]	Loss: 1.6892
Training Epoch: 9 [12800/50048]	Loss: 1.8147
Training Epoch: 9 [12928/50048]	Loss: 2.0300
Training Epoch: 9 [13056/50048]	Loss: 1.8015
Training Epoch: 9 [13184/50048]	Loss: 1.6717
Training Epoch: 9 [13312/50048]	Loss: 1.6366
Training Epoch: 9 [13440/50048]	Loss: 1.9155
Training Epoch: 9 [13568/50048]	Loss: 1.8052
Training Epoch: 9 [13696/50048]	Loss: 1.6932
Training Epoch: 9 [13824/50048]	Loss: 1.8067
Training Epoch: 9 [13952/50048]	Loss: 1.5757
Training Epoch: 9 [14080/50048]	Loss: 1.8319
Training Epoch: 9 [14208/50048]	Loss: 1.6956
Training Epoch: 9 [14336/50048]	Loss: 1.9990
Training Epoch: 9 [14464/50048]	Loss: 1.7943
Training Epoch: 9 [14592/50048]	Loss: 1.7855
Training Epoch: 9 [14720/50048]	Loss: 1.9501
Training Epoch: 9 [14848/50048]	Loss: 1.8309
Training Epoch: 9 [14976/50048]	Loss: 1.4997
Training Epoch: 9 [15104/50048]	Loss: 2.0369
Training Epoch: 9 [15232/50048]	Loss: 1.9081
Training Epoch: 9 [15360/50048]	Loss: 1.8115
Training Epoch: 9 [15488/50048]	Loss: 1.6216
Training Epoch: 9 [15616/50048]	Loss: 1.8413
Training Epoch: 9 [15744/50048]	Loss: 1.8245
Training Epoch: 9 [15872/50048]	Loss: 1.8935
Training Epoch: 9 [16000/50048]	Loss: 1.6290
Training Epoch: 9 [16128/50048]	Loss: 1.7347
Training Epoch: 9 [16256/50048]	Loss: 1.8772
Training Epoch: 9 [16384/50048]	Loss: 1.9107
Training Epoch: 9 [16512/50048]	Loss: 1.7533
Training Epoch: 9 [16640/50048]	Loss: 1.8812
Training Epoch: 9 [16768/50048]	Loss: 1.4117
Training Epoch: 9 [16896/50048]	Loss: 1.7883
Training Epoch: 9 [17024/50048]	Loss: 1.8546
Training Epoch: 9 [17152/50048]	Loss: 1.8862
Training Epoch: 9 [17280/50048]	Loss: 2.0808
Training Epoch: 9 [17408/50048]	Loss: 1.9241
Training Epoch: 9 [17536/50048]	Loss: 1.6041
Training Epoch: 9 [17664/50048]	Loss: 1.9892
Training Epoch: 9 [17792/50048]	Loss: 1.7997
Training Epoch: 9 [17920/50048]	Loss: 1.7751
Training Epoch: 9 [18048/50048]	Loss: 1.6532
Training Epoch: 9 [18176/50048]	Loss: 1.8063
Training Epoch: 9 [18304/50048]	Loss: 1.6227
Training Epoch: 9 [18432/50048]	Loss: 1.7630
Training Epoch: 9 [18560/50048]	Loss: 1.7665
Training Epoch: 9 [18688/50048]	Loss: 1.6164
Training Epoch: 9 [18816/50048]	Loss: 1.7674
Training Epoch: 9 [18944/50048]	Loss: 1.7708
Training Epoch: 9 [19072/50048]	Loss: 1.6951
Training Epoch: 9 [19200/50048]	Loss: 1.7151
Training Epoch: 9 [19328/50048]	Loss: 1.7259
Training Epoch: 9 [19456/50048]	Loss: 1.5164
Training Epoch: 9 [19584/50048]	Loss: 1.7132
Training Epoch: 9 [19712/50048]	Loss: 1.8970
Training Epoch: 9 [19840/50048]	Loss: 1.9177
Training Epoch: 9 [19968/50048]	Loss: 1.6810
Training Epoch: 9 [20096/50048]	Loss: 1.7400
Training Epoch: 9 [20224/50048]	Loss: 1.9970
Training Epoch: 9 [20352/50048]	Loss: 1.7719
Training Epoch: 9 [20480/50048]	Loss: 1.6074
Training Epoch: 9 [20608/50048]	Loss: 1.8053
Training Epoch: 9 [20736/50048]	Loss: 1.8063
Training Epoch: 9 [20864/50048]	Loss: 1.8876
Training Epoch: 9 [20992/50048]	Loss: 1.7204
Training Epoch: 9 [21120/50048]	Loss: 1.7645
Training Epoch: 9 [21248/50048]	Loss: 1.7677
Training Epoch: 9 [21376/50048]	Loss: 1.7397
Training Epoch: 9 [21504/50048]	Loss: 1.6787
Training Epoch: 9 [21632/50048]	Loss: 1.9145
Training Epoch: 9 [21760/50048]	Loss: 1.5578
Training Epoch: 9 [21888/50048]	Loss: 1.6758
Training Epoch: 9 [22016/50048]	Loss: 1.9910
Training Epoch: 9 [22144/50048]	Loss: 1.8419
Training Epoch: 9 [22272/50048]	Loss: 1.7535
Training Epoch: 9 [22400/50048]	Loss: 1.9482
Training Epoch: 9 [22528/50048]	Loss: 1.6940
Training Epoch: 9 [22656/50048]	Loss: 1.9267
Training Epoch: 9 [22784/50048]	Loss: 1.6352
Training Epoch: 9 [22912/50048]	Loss: 1.8587
Training Epoch: 9 [23040/50048]	Loss: 1.9557
Training Epoch: 9 [23168/50048]	Loss: 1.9120
Training Epoch: 9 [23296/50048]	Loss: 1.7568
Training Epoch: 9 [23424/50048]	Loss: 1.9603
Training Epoch: 9 [23552/50048]	Loss: 1.8091
Training Epoch: 9 [23680/50048]	Loss: 1.7600
Training Epoch: 9 [23808/50048]	Loss: 1.6747
Training Epoch: 9 [23936/50048]	Loss: 1.7949
Training Epoch: 9 [24064/50048]	Loss: 1.6875
Training Epoch: 9 [24192/50048]	Loss: 1.8736
Training Epoch: 9 [24320/50048]	Loss: 1.8409
Training Epoch: 9 [24448/50048]	Loss: 1.5940
Training Epoch: 9 [24576/50048]	Loss: 1.4516
Training Epoch: 9 [24704/50048]	Loss: 1.6173
Training Epoch: 9 [24832/50048]	Loss: 1.8117
Training Epoch: 9 [24960/50048]	Loss: 1.8968
Training Epoch: 9 [25088/50048]	Loss: 1.7212
Training Epoch: 9 [25216/50048]	Loss: 1.8622
Training Epoch: 9 [25344/50048]	Loss: 1.9517
Training Epoch: 9 [25472/50048]	Loss: 1.9810
Training Epoch: 9 [25600/50048]	Loss: 1.6144
Training Epoch: 9 [25728/50048]	Loss: 1.7643
Training Epoch: 9 [25856/50048]	Loss: 1.7733
Training Epoch: 9 [25984/50048]	Loss: 1.5629
Training Epoch: 9 [26112/50048]	Loss: 1.4398
Training Epoch: 9 [26240/50048]	Loss: 1.8527
Training Epoch: 9 [26368/50048]	Loss: 1.6772
Training Epoch: 9 [26496/50048]	Loss: 1.5213
Training Epoch: 9 [26624/50048]	Loss: 1.7268
Training Epoch: 9 [26752/50048]	Loss: 1.4197
Training Epoch: 9 [26880/50048]	Loss: 1.5431
Training Epoch: 9 [27008/50048]	Loss: 1.6659
Training Epoch: 9 [27136/50048]	Loss: 1.6086
Training Epoch: 9 [27264/50048]	Loss: 1.7520
Training Epoch: 9 [27392/50048]	Loss: 1.8560
Training Epoch: 9 [27520/50048]	Loss: 1.6295
Training Epoch: 9 [27648/50048]	Loss: 1.8248
Training Epoch: 9 [27776/50048]	Loss: 1.8912
Training Epoch: 9 [27904/50048]	Loss: 1.7441
Training Epoch: 9 [28032/50048]	Loss: 1.8164
Training Epoch: 9 [28160/50048]	Loss: 1.9313
Training Epoch: 9 [28288/50048]	Loss: 1.7231
Training Epoch: 9 [28416/50048]	Loss: 1.8903
Training Epoch: 9 [28544/50048]	Loss: 2.1123
Training Epoch: 9 [28672/50048]	Loss: 1.4915
Training Epoch: 9 [28800/50048]	Loss: 1.9235
Training Epoch: 9 [28928/50048]	Loss: 1.8359
Training Epoch: 9 [29056/50048]	Loss: 1.7255
Training Epoch: 9 [29184/50048]	Loss: 1.8759
Training Epoch: 9 [29312/50048]	Loss: 1.7386
Training Epoch: 9 [29440/50048]	Loss: 1.8397
Training Epoch: 9 [29568/50048]	Loss: 1.8408
Training Epoch: 9 [29696/50048]	Loss: 1.6725
Training Epoch: 9 [29824/50048]	Loss: 1.6367
Training Epoch: 9 [29952/50048]	Loss: 1.9593
Training Epoch: 9 [30080/50048]	Loss: 1.6789
Training Epoch: 9 [30208/50048]	Loss: 1.6752
Training Epoch: 9 [30336/50048]	Loss: 1.8166
Training Epoch: 9 [30464/50048]	Loss: 1.8127
Training Epoch: 9 [30592/50048]	Loss: 1.8923
Training Epoch: 9 [30720/50048]	Loss: 1.8149
Training Epoch: 9 [30848/50048]	Loss: 1.7625
Training Epoch: 9 [30976/50048]	Loss: 1.7746
Training Epoch: 9 [31104/50048]	Loss: 1.7780
Training Epoch: 9 [31232/50048]	Loss: 1.6907
Training Epoch: 9 [31360/50048]	Loss: 1.7207
Training Epoch: 9 [31488/50048]	Loss: 2.0460
Training Epoch: 9 [31616/50048]	Loss: 1.8969
Training Epoch: 9 [31744/50048]	Loss: 1.8281
Training Epoch: 9 [31872/50048]	Loss: 1.8451
Training Epoch: 9 [32000/50048]	Loss: 1.9160
Training Epoch: 9 [32128/50048]	Loss: 1.9625
Training Epoch: 9 [32256/50048]	Loss: 1.9956
Training Epoch: 9 [32384/50048]	Loss: 1.9224
Training Epoch: 9 [32512/50048]	Loss: 2.0998
Training Epoch: 9 [32640/50048]	Loss: 1.6819
Training Epoch: 9 [32768/50048]	Loss: 1.7828
Training Epoch: 9 [32896/50048]	Loss: 1.8507
Training Epoch: 9 [33024/50048]	Loss: 1.6346
Training Epoch: 9 [33152/50048]	Loss: 1.7402
Training Epoch: 9 [33280/50048]	Loss: 1.7750
Training Epoch: 9 [33408/50048]	Loss: 1.9248
Training Epoch: 9 [33536/50048]	Loss: 1.8193
Training Epoch: 9 [33664/50048]	Loss: 1.8778
Training Epoch: 9 [33792/50048]	Loss: 1.8174
Training Epoch: 9 [33920/50048]	Loss: 1.6299
Training Epoch: 9 [34048/50048]	Loss: 2.0164
Training Epoch: 9 [34176/50048]	Loss: 1.7345
Training Epoch: 9 [34304/50048]	Loss: 1.6554
Training Epoch: 9 [34432/50048]	Loss: 1.7143
Training Epoch: 9 [34560/50048]	Loss: 1.8215
Training Epoch: 9 [34688/50048]	Loss: 1.7577
Training Epoch: 9 [34816/50048]	Loss: 1.8531
Training Epoch: 9 [34944/50048]	Loss: 1.6542
Training Epoch: 9 [35072/50048]	Loss: 1.8622
Training Epoch: 9 [35200/50048]	Loss: 1.7836
Training Epoch: 9 [35328/50048]	Loss: 1.8452
Training Epoch: 9 [35456/50048]	Loss: 1.7984
Training Epoch: 9 [35584/50048]	Loss: 1.8338
Training Epoch: 9 [35712/50048]	Loss: 1.8665
Training Epoch: 9 [35840/50048]	Loss: 1.6895
Training Epoch: 9 [35968/50048]	Loss: 1.8328
Training Epoch: 9 [36096/50048]	Loss: 1.7148
Training Epoch: 9 [36224/50048]	Loss: 1.6790
Training Epoch: 9 [36352/50048]	Loss: 1.7491
Training Epoch: 9 [36480/50048]	Loss: 1.7062
Training Epoch: 9 [36608/50048]	Loss: 1.4976
Training Epoch: 9 [36736/50048]	Loss: 1.6597
Training Epoch: 9 [36864/50048]	Loss: 1.7344
Training Epoch: 9 [36992/50048]	Loss: 1.8599
Training Epoch: 9 [37120/50048]	Loss: 1.5419
Training Epoch: 9 [37248/50048]	Loss: 1.6116
Training Epoch: 9 [37376/50048]	Loss: 1.8840
Training Epoch: 9 [37504/50048]	Loss: 1.9220
Training Epoch: 9 [37632/50048]	Loss: 1.8496
Training Epoch: 9 [37760/50048]	Loss: 1.4233
Training Epoch: 9 [37888/50048]	Loss: 1.9538
Training Epoch: 9 [38016/50048]	Loss: 1.5514
Training Epoch: 9 [38144/50048]	Loss: 1.9228
Training Epoch: 9 [38272/50048]	Loss: 1.7621
Training Epoch: 9 [38400/50048]	Loss: 1.8028
Training Epoch: 9 [38528/50048]	Loss: 1.9359
Training Epoch: 9 [38656/50048]	Loss: 1.5223
Training Epoch: 9 [38784/50048]	Loss: 1.7816
Training Epoch: 9 [38912/50048]	Loss: 1.8385
Training Epoch: 9 [39040/50048]	Loss: 1.4482
Training Epoch: 9 [39168/50048]	Loss: 1.8554
Training Epoch: 9 [39296/50048]	Loss: 1.8493
Training Epoch: 9 [39424/50048]	Loss: 1.6898
Training Epoch: 9 [39552/50048]	Loss: 1.7584
Training Epoch: 9 [39680/50048]	Loss: 1.6911
Training Epoch: 9 [39808/50048]	Loss: 1.8671
Training Epoch: 9 [39936/50048]	Loss: 1.5872
Training Epoch: 9 [40064/50048]	Loss: 1.9264
Training Epoch: 9 [40192/50048]	Loss: 1.4780
Training Epoch: 9 [40320/50048]	Loss: 1.7078
Training Epoch: 9 [40448/50048]	Loss: 1.7328
Training Epoch: 9 [40576/50048]	Loss: 1.8017
Training Epoch: 9 [40704/50048]	Loss: 2.0725
Training Epoch: 9 [40832/50048]	Loss: 1.6531
Training Epoch: 9 [40960/50048]	Loss: 1.6673
Training Epoch: 9 [41088/50048]	Loss: 1.6400
Training Epoch: 9 [41216/50048]	Loss: 2.1070
Training Epoch: 9 [41344/50048]	Loss: 1.9532
Training Epoch: 9 [41472/50048]	Loss: 1.7986
Training Epoch: 9 [41600/50048]	Loss: 1.9877
Training Epoch: 9 [41728/50048]	Loss: 2.1012
Training Epoch: 9 [41856/50048]	Loss: 1.8377
Training Epoch: 9 [41984/50048]	Loss: 1.7058
Training Epoch: 9 [42112/50048]	Loss: 1.9171
Training Epoch: 9 [42240/50048]	Loss: 1.8700
Training Epoch: 9 [42368/50048]	Loss: 1.7540
Training Epoch: 9 [42496/50048]	Loss: 1.8330
Training Epoch: 9 [42624/50048]	Loss: 1.8150
Training Epoch: 9 [42752/50048]	Loss: 2.0791
Training Epoch: 9 [42880/50048]	Loss: 2.0466
Training Epoch: 9 [43008/50048]	Loss: 1.7162
Training Epoch: 9 [43136/50048]	Loss: 1.9896
Training Epoch: 9 [43264/50048]	Loss: 2.0632
Training Epoch: 9 [43392/50048]	Loss: 1.7508
Training Epoch: 9 [43520/50048]	Loss: 1.8761
Training Epoch: 9 [43648/50048]	Loss: 1.9247
Training Epoch: 9 [43776/50048]	Loss: 1.7504
Training Epoch: 9 [43904/50048]	Loss: 1.4462
Training Epoch: 9 [44032/50048]	Loss: 1.6512
Training Epoch: 9 [44160/50048]	Loss: 1.9385
Training Epoch: 9 [44288/50048]	Loss: 1.5604
Training Epoch: 9 [44416/50048]	Loss: 1.7542
Training Epoch: 9 [44544/50048]	Loss: 1.4374
Training Epoch: 9 [44672/50048]	Loss: 1.5718
Training Epoch: 9 [44800/50048]	Loss: 1.7311
Training Epoch: 9 [44928/50048]	Loss: 1.7504
Training Epoch: 9 [45056/50048]	Loss: 1.8532
Training Epoch: 9 [45184/50048]	Loss: 1.4972
Training Epoch: 9 [45312/50048]	Loss: 1.7849
Training Epoch: 9 [45440/50048]	Loss: 1.8177
Training Epoch: 9 [45568/50048]	Loss: 1.9084
Training Epoch: 9 [45696/50048]	Loss: 1.9149
Training Epoch: 9 [45824/50048]	Loss: 1.6417
Training Epoch: 9 [45952/50048]	Loss: 2.1411
Training Epoch: 9 [46080/50048]	Loss: 1.7495
Training Epoch: 9 [46208/50048]	Loss: 1.7809
Training Epoch: 9 [46336/50048]	Loss: 1.6263
Training Epoch: 9 [46464/50048]	Loss: 1.8717
Training Epoch: 9 [46592/50048]	Loss: 1.6111
Training Epoch: 9 [46720/50048]	Loss: 1.8048
Training Epoch: 9 [46848/50048]	Loss: 1.8338
Training Epoch: 9 [46976/50048]	Loss: 1.7244
Training Epoch: 9 [47104/50048]	Loss: 1.8090
Training Epoch: 9 [47232/50048]	Loss: 1.8712
Training Epoch: 9 [47360/50048]	Loss: 1.8196
Training Epoch: 9 [47488/50048]	Loss: 1.5091
Training Epoch: 9 [47616/50048]	Loss: 1.8176
Training Epoch: 9 [47744/50048]	Loss: 1.8116
Training Epoch: 9 [47872/50048]	Loss: 1.6108
Training Epoch: 9 [48000/50048]	Loss: 1.7088
Training Epoch: 9 [48128/50048]	Loss: 1.7146
Training Epoch: 9 [48256/50048]	Loss: 1.6782
Training Epoch: 9 [48384/50048]	Loss: 1.6213
Training Epoch: 9 [48512/50048]	Loss: 1.7808
Training Epoch: 9 [48640/50048]	Loss: 1.7980
Training Epoch: 9 [48768/50048]	Loss: 1.6824
Training Epoch: 9 [48896/50048]	Loss: 2.0039
Training Epoch: 9 [49024/50048]	Loss: 1.6810
Training Epoch: 9 [49152/50048]	Loss: 1.7190
Training Epoch: 9 [49280/50048]	Loss: 1.6232
Training Epoch: 9 [49408/50048]	Loss: 1.9658
Training Epoch: 9 [49536/50048]	Loss: 1.8850
Training Epoch: 9 [49664/50048]	Loss: 1.8858
Training Epoch: 9 [49792/50048]	Loss: 1.7702
Training Epoch: 9 [49920/50048]	Loss: 1.9412
Training Epoch: 9 [50048/50048]	Loss: 1.8847
Validation Epoch: 9, Average loss: 0.0140, Accuracy: 0.5082
[Training Loop] Model's accuracy 0.5082080696202531 surpasses threshold 0.5! Reprofiling...
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.6365
Profiling... [256/50048]	Loss: 1.7127
Profiling... [384/50048]	Loss: 1.7395
Profiling... [512/50048]	Loss: 1.7625
Profiling... [640/50048]	Loss: 1.8671
Profiling... [768/50048]	Loss: 1.6793
Profiling... [896/50048]	Loss: 1.7072
Profiling... [1024/50048]	Loss: 1.7156
Profiling... [1152/50048]	Loss: 1.7094
Profiling... [1280/50048]	Loss: 1.8340
Profiling... [1408/50048]	Loss: 1.5218
Profiling... [1536/50048]	Loss: 1.4786
Profiling... [1664/50048]	Loss: 1.6561
Profile done
epoch 1 train time consumed: 3.58s
Validation Epoch: 10, Average loss: 0.0143, Accuracy: 0.4967
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.51799249708172,
                        "time": 2.2319852790005825,
                        "accuracy": 0.4967365506329114,
                        "total_cost": 393432.0775539024
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.7136
Profiling... [256/50048]	Loss: 1.5619
Profiling... [384/50048]	Loss: 2.1338
Profiling... [512/50048]	Loss: 1.8592
Profiling... [640/50048]	Loss: 1.6830
Profiling... [768/50048]	Loss: 1.8551
Profiling... [896/50048]	Loss: 1.7053
Profiling... [1024/50048]	Loss: 1.6440
Profiling... [1152/50048]	Loss: 1.8208
Profiling... [1280/50048]	Loss: 1.5018
Profiling... [1408/50048]	Loss: 1.6612
Profiling... [1536/50048]	Loss: 1.5975
Profiling... [1664/50048]	Loss: 1.7976
Profile done
epoch 1 train time consumed: 3.62s
Validation Epoch: 10, Average loss: 0.0143, Accuracy: 0.4991
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.53061650522787,
                        "time": 2.2325584349991914,
                        "accuracy": 0.4991099683544304,
                        "total_cost": 391661.76794664236
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.7711
Profiling... [256/50048]	Loss: 1.9372
Profiling... [384/50048]	Loss: 1.6918
Profiling... [512/50048]	Loss: 1.8387
Profiling... [640/50048]	Loss: 1.6691
Profiling... [768/50048]	Loss: 1.8294
Profiling... [896/50048]	Loss: 1.4657
Profiling... [1024/50048]	Loss: 1.8844
Profiling... [1152/50048]	Loss: 1.5420
Profiling... [1280/50048]	Loss: 1.7364
Profiling... [1408/50048]	Loss: 1.5967
Profiling... [1536/50048]	Loss: 1.7067
Profiling... [1664/50048]	Loss: 1.6741
Profile done
epoch 1 train time consumed: 3.73s
Validation Epoch: 10, Average loss: 0.0141, Accuracy: 0.5036
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.5360411855273,
                        "time": 2.3702035770002112,
                        "accuracy": 0.5035601265822784,
                        "total_cost": 412134.4491119233
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.6163
Profiling... [256/50048]	Loss: 1.5965
Profiling... [384/50048]	Loss: 1.6381
Profiling... [512/50048]	Loss: 1.5873
Profiling... [640/50048]	Loss: 1.6405
Profiling... [768/50048]	Loss: 1.7716
Profiling... [896/50048]	Loss: 1.8943
Profiling... [1024/50048]	Loss: 1.5249
Profiling... [1152/50048]	Loss: 1.8596
Profiling... [1280/50048]	Loss: 1.7725
Profiling... [1408/50048]	Loss: 1.6232
Profiling... [1536/50048]	Loss: 1.6970
Profiling... [1664/50048]	Loss: 1.6215
Profile done
epoch 1 train time consumed: 4.17s
Validation Epoch: 10, Average loss: 0.0143, Accuracy: 0.4988
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.51788419742472,
                        "time": 2.7895700249991933,
                        "accuracy": 0.4988132911392405,
                        "total_cost": 489670.35016886046
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.8866
Profiling... [256/50048]	Loss: 1.5058
Profiling... [384/50048]	Loss: 1.7938
Profiling... [512/50048]	Loss: 1.9320
Profiling... [640/50048]	Loss: 1.6320
Profiling... [768/50048]	Loss: 1.8228
Profiling... [896/50048]	Loss: 2.0018
Profiling... [1024/50048]	Loss: 1.7357
Profiling... [1152/50048]	Loss: 1.6906
Profiling... [1280/50048]	Loss: 1.8716
Profiling... [1408/50048]	Loss: 1.9278
Profiling... [1536/50048]	Loss: 1.7491
Profiling... [1664/50048]	Loss: 1.7350
Profile done
epoch 1 train time consumed: 3.53s
Validation Epoch: 10, Average loss: 0.0142, Accuracy: 0.5006
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.49697698377334,
                        "time": 2.220093411000562,
                        "accuracy": 0.5005933544303798,
                        "total_cost": 388320.8176212459
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.7789
Profiling... [256/50048]	Loss: 1.7395
Profiling... [384/50048]	Loss: 1.5752
Profiling... [512/50048]	Loss: 1.6671
Profiling... [640/50048]	Loss: 1.8066
Profiling... [768/50048]	Loss: 1.4462
Profiling... [896/50048]	Loss: 1.9550
Profiling... [1024/50048]	Loss: 1.7066
Profiling... [1152/50048]	Loss: 1.6605
Profiling... [1280/50048]	Loss: 1.8120
Profiling... [1408/50048]	Loss: 1.7719
Profiling... [1536/50048]	Loss: 1.5251
Profiling... [1664/50048]	Loss: 1.6199
Profile done
epoch 1 train time consumed: 3.64s
Validation Epoch: 10, Average loss: 0.0142, Accuracy: 0.5003
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.5085334660768,
                        "time": 2.2243065760003446,
                        "accuracy": 0.5002966772151899,
                        "total_cost": 389288.4883676683
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.5707
Profiling... [256/50048]	Loss: 1.7216
Profiling... [384/50048]	Loss: 1.3767
Profiling... [512/50048]	Loss: 1.8723
Profiling... [640/50048]	Loss: 1.6068
Profiling... [768/50048]	Loss: 1.6959
Profiling... [896/50048]	Loss: 1.5016
Profiling... [1024/50048]	Loss: 1.3570
Profiling... [1152/50048]	Loss: 1.6332
Profiling... [1280/50048]	Loss: 1.7833
Profiling... [1408/50048]	Loss: 1.7758
Profiling... [1536/50048]	Loss: 1.5224
Profiling... [1664/50048]	Loss: 1.7012
Profile done
epoch 1 train time consumed: 3.70s
Validation Epoch: 10, Average loss: 0.0143, Accuracy: 0.5025
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.51380599103773,
                        "time": 2.3519393249998757,
                        "accuracy": 0.5024723101265823,
                        "total_cost": 409843.94840723753
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.5505
Profiling... [256/50048]	Loss: 1.8000
Profiling... [384/50048]	Loss: 1.6263
Profiling... [512/50048]	Loss: 1.6846
Profiling... [640/50048]	Loss: 1.5470
Profiling... [768/50048]	Loss: 1.8614
Profiling... [896/50048]	Loss: 1.7846
Profiling... [1024/50048]	Loss: 1.6344
Profiling... [1152/50048]	Loss: 1.5472
Profiling... [1280/50048]	Loss: 2.0703
Profiling... [1408/50048]	Loss: 1.4774
Profiling... [1536/50048]	Loss: 1.7505
Profiling... [1664/50048]	Loss: 1.6632
Profile done
epoch 1 train time consumed: 4.14s
Validation Epoch: 10, Average loss: 0.0142, Accuracy: 0.5011
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.49609981420372,
                        "time": 2.7506351880001603,
                        "accuracy": 0.5010878164556962,
                        "total_cost": 480644.1428611291
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.5395
Profiling... [256/50048]	Loss: 1.5001
Profiling... [384/50048]	Loss: 1.8434
Profiling... [512/50048]	Loss: 1.7997
Profiling... [640/50048]	Loss: 1.7555
Profiling... [768/50048]	Loss: 1.5834
Profiling... [896/50048]	Loss: 1.6552
Profiling... [1024/50048]	Loss: 1.6114
Profiling... [1152/50048]	Loss: 1.8241
Profiling... [1280/50048]	Loss: 1.5575
Profiling... [1408/50048]	Loss: 1.6375
Profiling... [1536/50048]	Loss: 1.8529
Profiling... [1664/50048]	Loss: 1.6070
Profile done
epoch 1 train time consumed: 3.49s
Validation Epoch: 10, Average loss: 0.0146, Accuracy: 0.4921
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.4750927037392,
                        "time": 2.1650319790005597,
                        "accuracy": 0.4920886075949367,
                        "total_cost": 385234.75027650356
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.6890
Profiling... [256/50048]	Loss: 1.6874
Profiling... [384/50048]	Loss: 1.7161
Profiling... [512/50048]	Loss: 1.5081
Profiling... [640/50048]	Loss: 1.6481
Profiling... [768/50048]	Loss: 1.8825
Profiling... [896/50048]	Loss: 1.7597
Profiling... [1024/50048]	Loss: 1.6843
Profiling... [1152/50048]	Loss: 1.6857
Profiling... [1280/50048]	Loss: 1.5621
Profiling... [1408/50048]	Loss: 2.0512
Profiling... [1536/50048]	Loss: 2.0108
Profiling... [1664/50048]	Loss: 1.9429
Profile done
epoch 1 train time consumed: 3.48s
Validation Epoch: 10, Average loss: 0.0143, Accuracy: 0.4934
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.48764378787739,
                        "time": 2.1740900909999255,
                        "accuracy": 0.4933742088607595,
                        "total_cost": 385838.51363701076
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.8556
Profiling... [256/50048]	Loss: 1.8091
Profiling... [384/50048]	Loss: 1.6055
Profiling... [512/50048]	Loss: 1.6678
Profiling... [640/50048]	Loss: 1.6594
Profiling... [768/50048]	Loss: 1.7710
Profiling... [896/50048]	Loss: 1.7553
Profiling... [1024/50048]	Loss: 2.0073
Profiling... [1152/50048]	Loss: 1.6224
Profiling... [1280/50048]	Loss: 1.5501
Profiling... [1408/50048]	Loss: 1.8025
Profiling... [1536/50048]	Loss: 1.4885
Profiling... [1664/50048]	Loss: 1.5677
Profile done
epoch 1 train time consumed: 3.62s
Validation Epoch: 10, Average loss: 0.0143, Accuracy: 0.4996
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.49268151011024,
                        "time": 2.302558240000508,
                        "accuracy": 0.49960443037974683,
                        "total_cost": 403542.0888401971
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.5931
Profiling... [256/50048]	Loss: 1.7107
Profiling... [384/50048]	Loss: 2.1619
Profiling... [512/50048]	Loss: 1.9405
Profiling... [640/50048]	Loss: 1.8508
Profiling... [768/50048]	Loss: 1.5435
Profiling... [896/50048]	Loss: 1.9168
Profiling... [1024/50048]	Loss: 1.8002
Profiling... [1152/50048]	Loss: 1.6518
Profiling... [1280/50048]	Loss: 1.4180
Profiling... [1408/50048]	Loss: 1.6920
Profiling... [1536/50048]	Loss: 1.5649
Profiling... [1664/50048]	Loss: 1.5374
Profile done
epoch 1 train time consumed: 4.17s
Validation Epoch: 10, Average loss: 0.0142, Accuracy: 0.5000
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.47662185715032,
                        "time": 2.749781146000714,
                        "accuracy": 0.5,
                        "total_cost": 481540.2351122956
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.8831
Profiling... [256/50048]	Loss: 1.7384
Profiling... [384/50048]	Loss: 1.9132
Profiling... [512/50048]	Loss: 1.7175
Profiling... [640/50048]	Loss: 1.8541
Profiling... [768/50048]	Loss: 2.1676
Profiling... [896/50048]	Loss: 1.9551
Profiling... [1024/50048]	Loss: 2.0994
Profiling... [1152/50048]	Loss: 1.9828
Profiling... [1280/50048]	Loss: 2.0728
Profiling... [1408/50048]	Loss: 2.3447
Profiling... [1536/50048]	Loss: 1.8518
Profiling... [1664/50048]	Loss: 1.9868
Profile done
epoch 1 train time consumed: 3.49s
Validation Epoch: 10, Average loss: 0.0221, Accuracy: 0.3433
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.45703291300885,
                        "time": 2.165139470000213,
                        "accuracy": 0.34325553797468356,
                        "total_cost": 552297.0592464713
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.8212
Profiling... [256/50048]	Loss: 1.7266
Profiling... [384/50048]	Loss: 2.0009
Profiling... [512/50048]	Loss: 1.9781
Profiling... [640/50048]	Loss: 2.1460
Profiling... [768/50048]	Loss: 2.0953
Profiling... [896/50048]	Loss: 1.9383
Profiling... [1024/50048]	Loss: 2.2447
Profiling... [1152/50048]	Loss: 2.0490
Profiling... [1280/50048]	Loss: 2.2893
Profiling... [1408/50048]	Loss: 2.1244
Profiling... [1536/50048]	Loss: 2.2271
Profiling... [1664/50048]	Loss: 1.9885
Profile done
epoch 1 train time consumed: 3.52s
Validation Epoch: 10, Average loss: 0.0187, Accuracy: 0.3934
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.46778108522861,
                        "time": 2.173436926999784,
                        "accuracy": 0.39339398734177217,
                        "total_cost": 483753.0950635665
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.8189
Profiling... [256/50048]	Loss: 1.9210
Profiling... [384/50048]	Loss: 2.1978
Profiling... [512/50048]	Loss: 1.8600
Profiling... [640/50048]	Loss: 1.8962
Profiling... [768/50048]	Loss: 1.7614
Profiling... [896/50048]	Loss: 2.0161
Profiling... [1024/50048]	Loss: 2.1323
Profiling... [1152/50048]	Loss: 2.1879
Profiling... [1280/50048]	Loss: 2.2916
Profiling... [1408/50048]	Loss: 2.1945
Profiling... [1536/50048]	Loss: 1.8730
Profiling... [1664/50048]	Loss: 2.1190
Profile done
epoch 1 train time consumed: 3.77s
Validation Epoch: 10, Average loss: 0.0194, Accuracy: 0.3942
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.47304114339168,
                        "time": 2.3006858250000732,
                        "accuracy": 0.3941851265822785,
                        "total_cost": 511047.8074102887
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.6681
Profiling... [256/50048]	Loss: 1.5698
Profiling... [384/50048]	Loss: 1.9695
Profiling... [512/50048]	Loss: 1.7131
Profiling... [640/50048]	Loss: 2.0402
Profiling... [768/50048]	Loss: 2.0383
Profiling... [896/50048]	Loss: 1.9332
Profiling... [1024/50048]	Loss: 1.9352
Profiling... [1152/50048]	Loss: 2.0512
Profiling... [1280/50048]	Loss: 2.0048
Profiling... [1408/50048]	Loss: 1.7691
Profiling... [1536/50048]	Loss: 2.3283
Profiling... [1664/50048]	Loss: 1.9387
Profile done
epoch 1 train time consumed: 4.13s
Validation Epoch: 10, Average loss: 0.0202, Accuracy: 0.3597
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.4551883853458,
                        "time": 2.7481019990000277,
                        "accuracy": 0.3596716772151899,
                        "total_cost": 669007.5356963265
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.4078
Profiling... [256/50048]	Loss: 1.6076
Profiling... [384/50048]	Loss: 1.7773
Profiling... [512/50048]	Loss: 2.0835
Profiling... [640/50048]	Loss: 2.0785
Profiling... [768/50048]	Loss: 1.9690
Profiling... [896/50048]	Loss: 2.0377
Profiling... [1024/50048]	Loss: 1.8585
Profiling... [1152/50048]	Loss: 2.2179
Profiling... [1280/50048]	Loss: 1.9810
Profiling... [1408/50048]	Loss: 1.8366
Profiling... [1536/50048]	Loss: 2.1543
Profiling... [1664/50048]	Loss: 2.2283
Profile done
epoch 1 train time consumed: 3.53s
Validation Epoch: 10, Average loss: 0.0227, Accuracy: 0.3223
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.43491842764936,
                        "time": 2.167634210999495,
                        "accuracy": 0.32229034810126583,
                        "total_cost": 588902.0263504944
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.4077
Profiling... [256/50048]	Loss: 1.8273
Profiling... [384/50048]	Loss: 1.9349
Profiling... [512/50048]	Loss: 1.7970
Profiling... [640/50048]	Loss: 1.8125
Profiling... [768/50048]	Loss: 2.0310
Profiling... [896/50048]	Loss: 2.0260
Profiling... [1024/50048]	Loss: 2.2938
Profiling... [1152/50048]	Loss: 2.3681
Profiling... [1280/50048]	Loss: 2.1703
Profiling... [1408/50048]	Loss: 2.2606
Profiling... [1536/50048]	Loss: 2.1764
Profiling... [1664/50048]	Loss: 2.0625
Profile done
epoch 1 train time consumed: 3.54s
Validation Epoch: 10, Average loss: 0.0187, Accuracy: 0.3886
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.44645712565466,
                        "time": 2.168592105999778,
                        "accuracy": 0.38864715189873417,
                        "total_cost": 488569.9629325013
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.4931
Profiling... [256/50048]	Loss: 1.7759
Profiling... [384/50048]	Loss: 1.7401
Profiling... [512/50048]	Loss: 2.1775
Profiling... [640/50048]	Loss: 1.8968
Profiling... [768/50048]	Loss: 1.8607
Profiling... [896/50048]	Loss: 2.2033
Profiling... [1024/50048]	Loss: 2.0806
Profiling... [1152/50048]	Loss: 1.6939
Profiling... [1280/50048]	Loss: 2.1728
Profiling... [1408/50048]	Loss: 2.0282
Profiling... [1536/50048]	Loss: 2.0228
Profiling... [1664/50048]	Loss: 1.9026
Profile done
epoch 1 train time consumed: 3.67s
Validation Epoch: 10, Average loss: 0.0198, Accuracy: 0.3744
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.45152785429177,
                        "time": 2.311809417999939,
                        "accuracy": 0.3744066455696203,
                        "total_cost": 540645.8487150404
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.7758
Profiling... [256/50048]	Loss: 2.0866
Profiling... [384/50048]	Loss: 1.6732
Profiling... [512/50048]	Loss: 2.3393
Profiling... [640/50048]	Loss: 2.1889
Profiling... [768/50048]	Loss: 1.9346
Profiling... [896/50048]	Loss: 1.8716
Profiling... [1024/50048]	Loss: 1.8292
Profiling... [1152/50048]	Loss: 2.3432
Profiling... [1280/50048]	Loss: 1.8960
Profiling... [1408/50048]	Loss: 1.9082
Profiling... [1536/50048]	Loss: 2.1474
Profiling... [1664/50048]	Loss: 2.0397
Profile done
epoch 1 train time consumed: 4.18s
Validation Epoch: 10, Average loss: 0.0182, Accuracy: 0.3953
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.43519522458952,
                        "time": 2.7400910790001944,
                        "accuracy": 0.39527294303797467,
                        "total_cost": 606977.0402826283
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.4897
Profiling... [256/50048]	Loss: 1.8734
Profiling... [384/50048]	Loss: 1.9301
Profiling... [512/50048]	Loss: 2.0005
Profiling... [640/50048]	Loss: 2.0246
Profiling... [768/50048]	Loss: 2.1039
Profiling... [896/50048]	Loss: 1.9875
Profiling... [1024/50048]	Loss: 1.7314
Profiling... [1152/50048]	Loss: 2.2327
Profiling... [1280/50048]	Loss: 2.0205
Profiling... [1408/50048]	Loss: 2.0486
Profiling... [1536/50048]	Loss: 2.0518
Profiling... [1664/50048]	Loss: 2.2483
Profile done
epoch 1 train time consumed: 3.50s
Validation Epoch: 10, Average loss: 0.0199, Accuracy: 0.3755
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.41307509622861,
                        "time": 2.169799844999943,
                        "accuracy": 0.37549446202531644,
                        "total_cost": 505964.952581919
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.7670
Profiling... [256/50048]	Loss: 1.7020
Profiling... [384/50048]	Loss: 1.8321
Profiling... [512/50048]	Loss: 1.9152
Profiling... [640/50048]	Loss: 2.2649
Profiling... [768/50048]	Loss: 2.0010
Profiling... [896/50048]	Loss: 2.0301
Profiling... [1024/50048]	Loss: 1.9716
Profiling... [1152/50048]	Loss: 1.8110
Profiling... [1280/50048]	Loss: 1.9146
Profiling... [1408/50048]	Loss: 1.8155
Profiling... [1536/50048]	Loss: 2.1003
Profiling... [1664/50048]	Loss: 2.1926
Profile done
epoch 1 train time consumed: 3.51s
Validation Epoch: 10, Average loss: 0.0214, Accuracy: 0.3508
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.4233837827704,
                        "time": 2.1649000859997614,
                        "accuracy": 0.35077136075949367,
                        "total_cost": 540403.3754678747
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.5607
Profiling... [256/50048]	Loss: 1.9212
Profiling... [384/50048]	Loss: 2.1499
Profiling... [512/50048]	Loss: 2.0566
Profiling... [640/50048]	Loss: 1.8543
Profiling... [768/50048]	Loss: 2.3317
Profiling... [896/50048]	Loss: 1.9882
Profiling... [1024/50048]	Loss: 2.2091
Profiling... [1152/50048]	Loss: 2.4942
Profiling... [1280/50048]	Loss: 2.2392
Profiling... [1408/50048]	Loss: 2.2936
Profiling... [1536/50048]	Loss: 1.8413
Profiling... [1664/50048]	Loss: 2.1337
Profile done
epoch 1 train time consumed: 3.67s
Validation Epoch: 10, Average loss: 0.0214, Accuracy: 0.3688
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.42830367345934,
                        "time": 2.2970594930002335,
                        "accuracy": 0.36876977848101267,
                        "total_cost": 545407.6888440555
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.5187
Profiling... [256/50048]	Loss: 2.0138
Profiling... [384/50048]	Loss: 1.8771
Profiling... [512/50048]	Loss: 2.1250
Profiling... [640/50048]	Loss: 1.7558
Profiling... [768/50048]	Loss: 1.8818
Profiling... [896/50048]	Loss: 2.2207
Profiling... [1024/50048]	Loss: 2.4316
Profiling... [1152/50048]	Loss: 2.4791
Profiling... [1280/50048]	Loss: 1.7392
Profiling... [1408/50048]	Loss: 2.0203
Profiling... [1536/50048]	Loss: 1.9765
Profiling... [1664/50048]	Loss: 2.1751
Profile done
epoch 1 train time consumed: 4.12s
Validation Epoch: 10, Average loss: 0.0211, Accuracy: 0.3556
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.41104150328928,
                        "time": 2.737023431000125,
                        "accuracy": 0.35561708860759494,
                        "total_cost": 673907.3382556508
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.5992
Profiling... [256/50048]	Loss: 2.1946
Profiling... [384/50048]	Loss: 2.5533
Profiling... [512/50048]	Loss: 2.1355
Profiling... [640/50048]	Loss: 2.3471
Profiling... [768/50048]	Loss: 2.4461
Profiling... [896/50048]	Loss: 2.8276
Profiling... [1024/50048]	Loss: 2.6017
Profiling... [1152/50048]	Loss: 2.4936
Profiling... [1280/50048]	Loss: 2.7208
Profiling... [1408/50048]	Loss: 2.6185
Profiling... [1536/50048]	Loss: 2.5859
Profiling... [1664/50048]	Loss: 2.5650
Profile done
epoch 1 train time consumed: 3.57s
Validation Epoch: 10, Average loss: 0.0609, Accuracy: 0.1650
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.39016708575954,
                        "time": 2.1653442900005757,
                        "accuracy": 0.1649525316455696,
                        "total_cost": 1149402.7032620774
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.4140
Profiling... [256/50048]	Loss: 2.5430
Profiling... [384/50048]	Loss: 2.5211
Profiling... [512/50048]	Loss: 2.4312
Profiling... [640/50048]	Loss: 2.8290
Profiling... [768/50048]	Loss: 2.5623
Profiling... [896/50048]	Loss: 2.4027
Profiling... [1024/50048]	Loss: 2.2372
Profiling... [1152/50048]	Loss: 2.6031
Profiling... [1280/50048]	Loss: 2.6094
Profiling... [1408/50048]	Loss: 2.3711
Profiling... [1536/50048]	Loss: 2.7372
Profiling... [1664/50048]	Loss: 2.5607
Profile done
epoch 1 train time consumed: 3.63s
Validation Epoch: 10, Average loss: 0.0621, Accuracy: 0.1706
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.39999714231546,
                        "time": 2.1684667849995094,
                        "accuracy": 0.17058939873417722,
                        "total_cost": 1113025.2088366323
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.6465
Profiling... [256/50048]	Loss: 2.1537
Profiling... [384/50048]	Loss: 2.1817
Profiling... [512/50048]	Loss: 2.6017
Profiling... [640/50048]	Loss: 2.4605
Profiling... [768/50048]	Loss: 2.6058
Profiling... [896/50048]	Loss: 2.4911
Profiling... [1024/50048]	Loss: 2.4208
Profiling... [1152/50048]	Loss: 2.6126
Profiling... [1280/50048]	Loss: 2.4067
Profiling... [1408/50048]	Loss: 2.7212
Profiling... [1536/50048]	Loss: 2.1947
Profiling... [1664/50048]	Loss: 2.3510
Profile done
epoch 1 train time consumed: 3.65s
Validation Epoch: 10, Average loss: 0.0653, Accuracy: 0.1333
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.40377721077567,
                        "time": 2.2880502790003447,
                        "accuracy": 0.13330696202531644,
                        "total_cost": 1502854.7443558394
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.7078
Profiling... [256/50048]	Loss: 2.0998
Profiling... [384/50048]	Loss: 2.6778
Profiling... [512/50048]	Loss: 2.6422
Profiling... [640/50048]	Loss: 2.5694
Profiling... [768/50048]	Loss: 2.3555
Profiling... [896/50048]	Loss: 2.4619
Profiling... [1024/50048]	Loss: 2.6408
Profiling... [1152/50048]	Loss: 2.7927
Profiling... [1280/50048]	Loss: 2.9756
Profiling... [1408/50048]	Loss: 2.4757
Profiling... [1536/50048]	Loss: 2.7008
Profiling... [1664/50048]	Loss: 2.3308
Profile done
epoch 1 train time consumed: 4.07s
Validation Epoch: 10, Average loss: 0.0546, Accuracy: 0.1134
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.38653292152614,
                        "time": 2.725633117999678,
                        "accuracy": 0.11342958860759493,
                        "total_cost": 2103997.754894256
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.6403
Profiling... [256/50048]	Loss: 2.3345
Profiling... [384/50048]	Loss: 2.1473
Profiling... [512/50048]	Loss: 2.4886
Profiling... [640/50048]	Loss: 2.2680
Profiling... [768/50048]	Loss: 2.6271
Profiling... [896/50048]	Loss: 2.4332
Profiling... [1024/50048]	Loss: 2.6418
Profiling... [1152/50048]	Loss: 2.6287
Profiling... [1280/50048]	Loss: 2.3976
Profiling... [1408/50048]	Loss: 2.5050
Profiling... [1536/50048]	Loss: 2.4281
Profiling... [1664/50048]	Loss: 2.5367
Profile done
epoch 1 train time consumed: 3.54s
Validation Epoch: 10, Average loss: 0.0446, Accuracy: 0.2284
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.36688363711015,
                        "time": 2.1654135629996745,
                        "accuracy": 0.22844145569620253,
                        "total_cost": 829984.7570093372
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.5625
Profiling... [256/50048]	Loss: 2.3805
Profiling... [384/50048]	Loss: 2.6648
Profiling... [512/50048]	Loss: 2.5984
Profiling... [640/50048]	Loss: 2.4960
Profiling... [768/50048]	Loss: 2.5610
Profiling... [896/50048]	Loss: 2.5640
Profiling... [1024/50048]	Loss: 2.7946
Profiling... [1152/50048]	Loss: 2.6297
Profiling... [1280/50048]	Loss: 2.5028
Profiling... [1408/50048]	Loss: 2.4042
Profiling... [1536/50048]	Loss: 2.8211
Profiling... [1664/50048]	Loss: 2.5877
Profile done
epoch 1 train time consumed: 3.57s
Validation Epoch: 10, Average loss: 0.0316, Accuracy: 0.2276
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.37638219942374,
                        "time": 2.1640323559995522,
                        "accuracy": 0.22755142405063292,
                        "total_cost": 832699.6814777554
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.9537
Profiling... [256/50048]	Loss: 2.5320
Profiling... [384/50048]	Loss: 2.3856
Profiling... [512/50048]	Loss: 2.3670
Profiling... [640/50048]	Loss: 2.3721
Profiling... [768/50048]	Loss: 2.4190
Profiling... [896/50048]	Loss: 2.8767
Profiling... [1024/50048]	Loss: 2.5194
Profiling... [1152/50048]	Loss: 2.5132
Profiling... [1280/50048]	Loss: 2.4566
Profiling... [1408/50048]	Loss: 2.5524
Profiling... [1536/50048]	Loss: 2.7430
Profiling... [1664/50048]	Loss: 2.4408
Profile done
epoch 1 train time consumed: 3.70s
Validation Epoch: 10, Average loss: 0.0438, Accuracy: 0.1542
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.38066784129869,
                        "time": 2.303357123000751,
                        "accuracy": 0.1541732594936709,
                        "total_cost": 1308146.6726508143
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.5392
Profiling... [256/50048]	Loss: 2.1994
Profiling... [384/50048]	Loss: 2.5228
Profiling... [512/50048]	Loss: 2.3856
Profiling... [640/50048]	Loss: 2.5729
Profiling... [768/50048]	Loss: 2.4304
Profiling... [896/50048]	Loss: 2.4150
Profiling... [1024/50048]	Loss: 2.6537
Profiling... [1152/50048]	Loss: 2.7125
Profiling... [1280/50048]	Loss: 2.3949
Profiling... [1408/50048]	Loss: 2.5857
Profiling... [1536/50048]	Loss: 2.4321
Profiling... [1664/50048]	Loss: 2.7090
Profile done
epoch 1 train time consumed: 4.22s
Validation Epoch: 10, Average loss: 0.1071, Accuracy: 0.1182
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.36416632017496,
                        "time": 2.7218781029996535,
                        "accuracy": 0.11817642405063292,
                        "total_cost": 2016703.274636682
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.7882
Profiling... [256/50048]	Loss: 2.6365
Profiling... [384/50048]	Loss: 2.7927
Profiling... [512/50048]	Loss: 2.9059
Profiling... [640/50048]	Loss: 2.4671
Profiling... [768/50048]	Loss: 2.5740
Profiling... [896/50048]	Loss: 2.7105
Profiling... [1024/50048]	Loss: 2.3585
Profiling... [1152/50048]	Loss: 2.1469
Profiling... [1280/50048]	Loss: 2.7267
Profiling... [1408/50048]	Loss: 2.4353
Profiling... [1536/50048]	Loss: 2.4513
Profiling... [1664/50048]	Loss: 2.4904
Profile done
epoch 1 train time consumed: 3.51s
Validation Epoch: 10, Average loss: 0.1040, Accuracy: 0.1722
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.34378653871431,
                        "time": 2.1658758860003218,
                        "accuracy": 0.17217167721518986,
                        "total_cost": 1101478.3906803979
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.8780
Profiling... [256/50048]	Loss: 2.3429
Profiling... [384/50048]	Loss: 2.4631
Profiling... [512/50048]	Loss: 2.5897
Profiling... [640/50048]	Loss: 2.6215
Profiling... [768/50048]	Loss: 2.4328
Profiling... [896/50048]	Loss: 2.5418
Profiling... [1024/50048]	Loss: 2.7406
Profiling... [1152/50048]	Loss: 2.6540
Profiling... [1280/50048]	Loss: 2.5775
Profiling... [1408/50048]	Loss: 2.4490
Profiling... [1536/50048]	Loss: 2.2488
Profiling... [1664/50048]	Loss: 2.4003
Profile done
epoch 1 train time consumed: 3.50s
Validation Epoch: 10, Average loss: 0.0384, Accuracy: 0.2319
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.3531833185621,
                        "time": 2.1622102179999274,
                        "accuracy": 0.23190268987341772,
                        "total_cost": 816387.3714211548
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.9328
Profiling... [256/50048]	Loss: 2.6563
Profiling... [384/50048]	Loss: 2.3766
Profiling... [512/50048]	Loss: 2.4490
Profiling... [640/50048]	Loss: 2.7120
Profiling... [768/50048]	Loss: 2.6334
Profiling... [896/50048]	Loss: 2.5522
Profiling... [1024/50048]	Loss: 2.4559
Profiling... [1152/50048]	Loss: 2.5571
Profiling... [1280/50048]	Loss: 2.7805
Profiling... [1408/50048]	Loss: 2.5852
Profiling... [1536/50048]	Loss: 2.4922
Profiling... [1664/50048]	Loss: 2.3638
Profile done
epoch 1 train time consumed: 3.71s
Validation Epoch: 10, Average loss: 0.0329, Accuracy: 0.1989
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.35899428794964,
                        "time": 2.29339236099986,
                        "accuracy": 0.19887262658227847,
                        "total_cost": 1009735.2438158029
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.7006
Profiling... [256/50048]	Loss: 2.1814
Profiling... [384/50048]	Loss: 2.2946
Profiling... [512/50048]	Loss: 2.6041
Profiling... [640/50048]	Loss: 2.7247
Profiling... [768/50048]	Loss: 2.4192
Profiling... [896/50048]	Loss: 2.3654
Profiling... [1024/50048]	Loss: 2.4740
Profiling... [1152/50048]	Loss: 2.3558
Profiling... [1280/50048]	Loss: 2.6335
Profiling... [1408/50048]	Loss: 2.7233
Profiling... [1536/50048]	Loss: 2.6277
Profiling... [1664/50048]	Loss: 2.2466
Profile done
epoch 1 train time consumed: 4.15s
Validation Epoch: 10, Average loss: 0.0887, Accuracy: 0.0865
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.3419033412048,
                        "time": 2.7189451450003617,
                        "accuracy": 0.08653085443037975,
                        "total_cost": 2751272.2923985254
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.6781
Profiling... [512/50176]	Loss: 1.7363
Profiling... [768/50176]	Loss: 1.6241
Profiling... [1024/50176]	Loss: 1.7558
Profiling... [1280/50176]	Loss: 1.5332
Profiling... [1536/50176]	Loss: 1.5366
Profiling... [1792/50176]	Loss: 1.6699
Profiling... [2048/50176]	Loss: 1.6604
Profiling... [2304/50176]	Loss: 1.6796
Profiling... [2560/50176]	Loss: 1.6405
Profiling... [2816/50176]	Loss: 1.8171
Profiling... [3072/50176]	Loss: 1.6893
Profiling... [3328/50176]	Loss: 1.7311
Profile done
epoch 1 train time consumed: 3.97s
Validation Epoch: 10, Average loss: 0.0071, Accuracy: 0.4940
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.32725868705052,
                        "time": 2.392529534999994,
                        "accuracy": 0.49404296875,
                        "total_cost": 424030.0835328287
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.6309
Profiling... [512/50176]	Loss: 1.9834
Profiling... [768/50176]	Loss: 1.6657
Profiling... [1024/50176]	Loss: 1.6237
Profiling... [1280/50176]	Loss: 1.7123
Profiling... [1536/50176]	Loss: 1.6946
Profiling... [1792/50176]	Loss: 1.6245
Profiling... [2048/50176]	Loss: 1.6882
Profiling... [2304/50176]	Loss: 1.4700
Profiling... [2560/50176]	Loss: 1.6998
Profiling... [2816/50176]	Loss: 1.5151
Profiling... [3072/50176]	Loss: 1.6560
Profiling... [3328/50176]	Loss: 1.6758
Profile done
epoch 1 train time consumed: 3.89s
Validation Epoch: 10, Average loss: 0.0071, Accuracy: 0.4977
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.34192759902476,
                        "time": 2.4102591299997584,
                        "accuracy": 0.49765625,
                        "total_cost": 424070.8247113796
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.5515
Profiling... [512/50176]	Loss: 1.7252
Profiling... [768/50176]	Loss: 1.7641
Profiling... [1024/50176]	Loss: 1.7343
Profiling... [1280/50176]	Loss: 1.6271
Profiling... [1536/50176]	Loss: 1.7848
Profiling... [1792/50176]	Loss: 1.4628
Profiling... [2048/50176]	Loss: 1.7335
Profiling... [2304/50176]	Loss: 1.5665
Profiling... [2560/50176]	Loss: 1.6423
Profiling... [2816/50176]	Loss: 1.7680
Profiling... [3072/50176]	Loss: 1.6375
Profiling... [3328/50176]	Loss: 1.7304
Profile done
epoch 1 train time consumed: 4.11s
Validation Epoch: 10, Average loss: 0.0070, Accuracy: 0.4929
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.3490550571801,
                        "time": 2.646766930999547,
                        "accuracy": 0.49287109375,
                        "total_cost": 470204.1851659029
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.4991
Profiling... [512/50176]	Loss: 1.6905
Profiling... [768/50176]	Loss: 1.7733
Profiling... [1024/50176]	Loss: 1.6356
Profiling... [1280/50176]	Loss: 1.6435
Profiling... [1536/50176]	Loss: 1.5722
Profiling... [1792/50176]	Loss: 1.6140
Profiling... [2048/50176]	Loss: 1.6479
Profiling... [2304/50176]	Loss: 1.9341
Profiling... [2560/50176]	Loss: 1.5949
Profiling... [2816/50176]	Loss: 1.6342
Profiling... [3072/50176]	Loss: 1.6956
Profiling... [3328/50176]	Loss: 1.5538
Profile done
epoch 1 train time consumed: 4.86s
Validation Epoch: 10, Average loss: 0.0071, Accuracy: 0.4998
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.32929093561025,
                        "time": 3.2037171489992033,
                        "accuracy": 0.4998046875,
                        "total_cost": 561252.037447737
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.7137
Profiling... [512/50176]	Loss: 1.6269
Profiling... [768/50176]	Loss: 1.7030
Profiling... [1024/50176]	Loss: 1.6890
Profiling... [1280/50176]	Loss: 1.8293
Profiling... [1536/50176]	Loss: 1.6451
Profiling... [1792/50176]	Loss: 1.6573
Profiling... [2048/50176]	Loss: 1.6423
Profiling... [2304/50176]	Loss: 1.5604
Profiling... [2560/50176]	Loss: 1.5216
Profiling... [2816/50176]	Loss: 1.4768
Profiling... [3072/50176]	Loss: 1.6683
Profiling... [3328/50176]	Loss: 1.6560
Profile done
epoch 1 train time consumed: 3.91s
Validation Epoch: 10, Average loss: 0.0072, Accuracy: 0.4937
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.31095849587862,
                        "time": 2.395434361000298,
                        "accuracy": 0.49365234375,
                        "total_cost": 424880.8093952021
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.7641
Profiling... [512/50176]	Loss: 1.7674
Profiling... [768/50176]	Loss: 1.7096
Profiling... [1024/50176]	Loss: 1.6430
Profiling... [1280/50176]	Loss: 1.7563
Profiling... [1536/50176]	Loss: 1.6644
Profiling... [1792/50176]	Loss: 1.6008
Profiling... [2048/50176]	Loss: 1.7159
Profiling... [2304/50176]	Loss: 1.5472
Profiling... [2560/50176]	Loss: 1.6212
Profiling... [2816/50176]	Loss: 1.6069
Profiling... [3072/50176]	Loss: 1.7532
Profiling... [3328/50176]	Loss: 1.6217
Profile done
epoch 1 train time consumed: 3.93s
Validation Epoch: 10, Average loss: 0.0071, Accuracy: 0.4992
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.32639458254972,
                        "time": 2.426364205000027,
                        "accuracy": 0.49921875,
                        "total_cost": 425568.21550425695
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.6104
Profiling... [512/50176]	Loss: 1.7777
Profiling... [768/50176]	Loss: 1.6228
Profiling... [1024/50176]	Loss: 1.8025
Profiling... [1280/50176]	Loss: 1.6576
Profiling... [1536/50176]	Loss: 1.6686
Profiling... [1792/50176]	Loss: 1.7379
Profiling... [2048/50176]	Loss: 1.6742
Profiling... [2304/50176]	Loss: 1.7382
Profiling... [2560/50176]	Loss: 1.8890
Profiling... [2816/50176]	Loss: 1.5977
Profiling... [3072/50176]	Loss: 1.6413
Profiling... [3328/50176]	Loss: 1.5358
Profile done
epoch 1 train time consumed: 4.20s
Validation Epoch: 10, Average loss: 0.0070, Accuracy: 0.5004
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.33162048157746,
                        "time": 2.640473552000003,
                        "accuracy": 0.500390625,
                        "total_cost": 462036.9971837496
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.6082
Profiling... [512/50176]	Loss: 1.5290
Profiling... [768/50176]	Loss: 1.6296
Profiling... [1024/50176]	Loss: 1.7259
Profiling... [1280/50176]	Loss: 1.7636
Profiling... [1536/50176]	Loss: 1.7402
Profiling... [1792/50176]	Loss: 1.8646
Profiling... [2048/50176]	Loss: 1.8324
Profiling... [2304/50176]	Loss: 1.6155
Profiling... [2560/50176]	Loss: 1.6385
Profiling... [2816/50176]	Loss: 1.5089
Profiling... [3072/50176]	Loss: 1.8039
Profiling... [3328/50176]	Loss: 1.7594
Profile done
epoch 1 train time consumed: 4.83s
Validation Epoch: 10, Average loss: 0.0070, Accuracy: 0.5026
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.3122448985332,
                        "time": 3.1882768799996484,
                        "accuracy": 0.50263671875,
                        "total_cost": 555399.9873510527
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.8647
Profiling... [512/50176]	Loss: 1.6289
Profiling... [768/50176]	Loss: 1.6834
Profiling... [1024/50176]	Loss: 1.7373
Profiling... [1280/50176]	Loss: 1.7203
Profiling... [1536/50176]	Loss: 1.8142
Profiling... [1792/50176]	Loss: 1.6974
Profiling... [2048/50176]	Loss: 1.5212
Profiling... [2304/50176]	Loss: 1.6369
Profiling... [2560/50176]	Loss: 1.7097
Profiling... [2816/50176]	Loss: 1.7380
Profiling... [3072/50176]	Loss: 1.6713
Profiling... [3328/50176]	Loss: 1.7061
Profile done
epoch 1 train time consumed: 3.91s
Validation Epoch: 10, Average loss: 0.0071, Accuracy: 0.4955
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.29510945519161,
                        "time": 2.4061433409997335,
                        "accuracy": 0.4955078125,
                        "total_cost": 425182.1214304091
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.5597
Profiling... [512/50176]	Loss: 1.7313
Profiling... [768/50176]	Loss: 1.5399
Profiling... [1024/50176]	Loss: 1.7764
Profiling... [1280/50176]	Loss: 1.8197
Profiling... [1536/50176]	Loss: 1.5970
Profiling... [1792/50176]	Loss: 1.6838
Profiling... [2048/50176]	Loss: 1.6440
Profiling... [2304/50176]	Loss: 1.7136
Profiling... [2560/50176]	Loss: 1.8117
Profiling... [2816/50176]	Loss: 1.7822
Profiling... [3072/50176]	Loss: 1.6457
Profiling... [3328/50176]	Loss: 1.7179
Profile done
epoch 1 train time consumed: 3.98s
Validation Epoch: 10, Average loss: 0.0071, Accuracy: 0.4950
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.3093448551502,
                        "time": 2.4031770290002896,
                        "accuracy": 0.49501953125,
                        "total_cost": 425076.86564332066
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.6476
Profiling... [512/50176]	Loss: 1.6661
Profiling... [768/50176]	Loss: 1.7339
Profiling... [1024/50176]	Loss: 1.7030
Profiling... [1280/50176]	Loss: 1.7021
Profiling... [1536/50176]	Loss: 1.6043
Profiling... [1792/50176]	Loss: 1.7401
Profiling... [2048/50176]	Loss: 1.5423
Profiling... [2304/50176]	Loss: 1.7577
Profiling... [2560/50176]	Loss: 1.6344
Profiling... [2816/50176]	Loss: 1.6605
Profiling... [3072/50176]	Loss: 1.6855
Profiling... [3328/50176]	Loss: 1.7182
Profile done
epoch 1 train time consumed: 4.12s
Validation Epoch: 10, Average loss: 0.0071, Accuracy: 0.4981
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.31372530493113,
                        "time": 2.616199252000115,
                        "accuracy": 0.49814453125,
                        "total_cost": 459853.50519585394
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.7075
Profiling... [512/50176]	Loss: 1.6634
Profiling... [768/50176]	Loss: 1.7566
Profiling... [1024/50176]	Loss: 1.6149
Profiling... [1280/50176]	Loss: 1.6189
Profiling... [1536/50176]	Loss: 1.5898
Profiling... [1792/50176]	Loss: 1.8038
Profiling... [2048/50176]	Loss: 1.6966
Profiling... [2304/50176]	Loss: 1.5423
Profiling... [2560/50176]	Loss: 1.7214
Profiling... [2816/50176]	Loss: 1.5856
Profiling... [3072/50176]	Loss: 1.6659
Profiling... [3328/50176]	Loss: 1.5991
Profile done
epoch 1 train time consumed: 4.85s
Validation Epoch: 10, Average loss: 0.0070, Accuracy: 0.5030
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.29551773597697,
                        "time": 3.2080028150003272,
                        "accuracy": 0.50302734375,
                        "total_cost": 558402.2419077206
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.8418
Profiling... [512/50176]	Loss: 1.9063
Profiling... [768/50176]	Loss: 1.9242
Profiling... [1024/50176]	Loss: 2.1451
Profiling... [1280/50176]	Loss: 1.7302
Profiling... [1536/50176]	Loss: 1.9559
Profiling... [1792/50176]	Loss: 1.8107
Profiling... [2048/50176]	Loss: 2.0221
Profiling... [2304/50176]	Loss: 1.9371
Profiling... [2560/50176]	Loss: 2.0053
Profiling... [2816/50176]	Loss: 1.9418
Profiling... [3072/50176]	Loss: 1.8643
Profiling... [3328/50176]	Loss: 1.9202
Profile done
epoch 1 train time consumed: 3.87s
Validation Epoch: 10, Average loss: 0.0101, Accuracy: 0.3646
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.27894322763572,
                        "time": 2.4044080989997383,
                        "accuracy": 0.3646484375,
                        "total_cost": 577348.1650831026
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.7854
Profiling... [512/50176]	Loss: 1.9698
Profiling... [768/50176]	Loss: 1.8227
Profiling... [1024/50176]	Loss: 1.8895
Profiling... [1280/50176]	Loss: 1.9871
Profiling... [1536/50176]	Loss: 2.1217
Profiling... [1792/50176]	Loss: 1.9858
Profiling... [2048/50176]	Loss: 1.9870
Profiling... [2304/50176]	Loss: 2.0449
Profiling... [2560/50176]	Loss: 1.7814
Profiling... [2816/50176]	Loss: 1.7183
Profiling... [3072/50176]	Loss: 1.6672
Profiling... [3328/50176]	Loss: 1.7725
Profile done
epoch 1 train time consumed: 3.90s
Validation Epoch: 10, Average loss: 0.0088, Accuracy: 0.4194
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.2954101467902,
                        "time": 2.408246770000005,
                        "accuracy": 0.41943359375,
                        "total_cost": 502738.077994215
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.7452
Profiling... [512/50176]	Loss: 1.8150
Profiling... [768/50176]	Loss: 1.8096
Profiling... [1024/50176]	Loss: 1.9939
Profiling... [1280/50176]	Loss: 1.6703
Profiling... [1536/50176]	Loss: 1.9655
Profiling... [1792/50176]	Loss: 1.9491
Profiling... [2048/50176]	Loss: 1.9121
Profiling... [2304/50176]	Loss: 2.0691
Profiling... [2560/50176]	Loss: 1.9813
Profiling... [2816/50176]	Loss: 2.0280
Profiling... [3072/50176]	Loss: 1.8288
Profiling... [3328/50176]	Loss: 1.9768
Profile done
epoch 1 train time consumed: 4.14s
Validation Epoch: 10, Average loss: 0.0085, Accuracy: 0.4187
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.30088035141432,
                        "time": 2.6206592340004136,
                        "accuracy": 0.41865234375,
                        "total_cost": 548101.568945044
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.7624
Profiling... [512/50176]	Loss: 1.6572
Profiling... [768/50176]	Loss: 1.8077
Profiling... [1024/50176]	Loss: 1.8230
Profiling... [1280/50176]	Loss: 1.8484
Profiling... [1536/50176]	Loss: 1.8890
Profiling... [1792/50176]	Loss: 1.9795
Profiling... [2048/50176]	Loss: 2.1177
Profiling... [2304/50176]	Loss: 1.9732
Profiling... [2560/50176]	Loss: 1.7974
Profiling... [2816/50176]	Loss: 1.9683
Profiling... [3072/50176]	Loss: 1.8578
Profiling... [3328/50176]	Loss: 1.9264
Profile done
epoch 1 train time consumed: 4.86s
Validation Epoch: 10, Average loss: 0.0086, Accuracy: 0.4201
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.281378481437,
                        "time": 3.19947543499984,
                        "accuracy": 0.4201171875,
                        "total_cost": 666825.6567880556
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.7703
Profiling... [512/50176]	Loss: 1.9200
Profiling... [768/50176]	Loss: 1.7223
Profiling... [1024/50176]	Loss: 1.8807
Profiling... [1280/50176]	Loss: 1.9072
Profiling... [1536/50176]	Loss: 1.7583
Profiling... [1792/50176]	Loss: 1.8929
Profiling... [2048/50176]	Loss: 2.1066
Profiling... [2304/50176]	Loss: 1.8300
Profiling... [2560/50176]	Loss: 1.8165
Profiling... [2816/50176]	Loss: 2.0170
Profiling... [3072/50176]	Loss: 2.0988
Profiling... [3328/50176]	Loss: 1.7889
Profile done
epoch 1 train time consumed: 3.97s
Validation Epoch: 10, Average loss: 0.0095, Accuracy: 0.3902
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.2623908683777,
                        "time": 2.4058272329993997,
                        "accuracy": 0.390234375,
                        "total_cost": 539812.376194074
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.5276
Profiling... [512/50176]	Loss: 1.6137
Profiling... [768/50176]	Loss: 1.7762
Profiling... [1024/50176]	Loss: 1.9092
Profiling... [1280/50176]	Loss: 1.6950
Profiling... [1536/50176]	Loss: 2.0719
Profiling... [1792/50176]	Loss: 1.6805
Profiling... [2048/50176]	Loss: 1.9065
Profiling... [2304/50176]	Loss: 2.1745
Profiling... [2560/50176]	Loss: 1.9591
Profiling... [2816/50176]	Loss: 2.1304
Profiling... [3072/50176]	Loss: 1.9655
Profiling... [3328/50176]	Loss: 2.0336
Profile done
epoch 1 train time consumed: 3.92s
Validation Epoch: 10, Average loss: 0.0099, Accuracy: 0.3819
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.27522806859794,
                        "time": 2.4216675319994465,
                        "accuracy": 0.38193359375,
                        "total_cost": 555175.9127591649
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.6067
Profiling... [512/50176]	Loss: 1.7086
Profiling... [768/50176]	Loss: 1.6221
Profiling... [1024/50176]	Loss: 1.8325
Profiling... [1280/50176]	Loss: 1.8359
Profiling... [1536/50176]	Loss: 1.9854
Profiling... [1792/50176]	Loss: 1.9971
Profiling... [2048/50176]	Loss: 1.9763
Profiling... [2304/50176]	Loss: 1.9228
Profiling... [2560/50176]	Loss: 1.9811
Profiling... [2816/50176]	Loss: 1.9533
Profiling... [3072/50176]	Loss: 2.0865
Profiling... [3328/50176]	Loss: 2.0349
Profile done
epoch 1 train time consumed: 4.13s
Validation Epoch: 10, Average loss: 0.0091, Accuracy: 0.3954
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.28137753297048,
                        "time": 2.6250677600000927,
                        "accuracy": 0.39541015625,
                        "total_cost": 581295.10892975
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.6770
Profiling... [512/50176]	Loss: 1.6547
Profiling... [768/50176]	Loss: 1.7815
Profiling... [1024/50176]	Loss: 1.9026
Profiling... [1280/50176]	Loss: 1.7268
Profiling... [1536/50176]	Loss: 1.9156
Profiling... [1792/50176]	Loss: 2.1078
Profiling... [2048/50176]	Loss: 1.7612
Profiling... [2304/50176]	Loss: 2.0702
Profiling... [2560/50176]	Loss: 1.8555
Profiling... [2816/50176]	Loss: 1.9931
Profiling... [3072/50176]	Loss: 2.1014
Profiling... [3328/50176]	Loss: 2.0664
Profile done
epoch 1 train time consumed: 4.86s
Validation Epoch: 10, Average loss: 0.0113, Accuracy: 0.3435
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.26229627398112,
                        "time": 3.199075460000131,
                        "accuracy": 0.34345703125,
                        "total_cost": 815560.1481594887
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.6146
Profiling... [512/50176]	Loss: 1.5978
Profiling... [768/50176]	Loss: 2.0059
Profiling... [1024/50176]	Loss: 1.9695
Profiling... [1280/50176]	Loss: 2.0869
Profiling... [1536/50176]	Loss: 1.9089
Profiling... [1792/50176]	Loss: 1.9170
Profiling... [2048/50176]	Loss: 1.8712
Profiling... [2304/50176]	Loss: 1.8726
Profiling... [2560/50176]	Loss: 1.9853
Profiling... [2816/50176]	Loss: 1.8109
Profiling... [3072/50176]	Loss: 1.9919
Profiling... [3328/50176]	Loss: 1.8743
Profile done
epoch 1 train time consumed: 3.99s
Validation Epoch: 10, Average loss: 0.0088, Accuracy: 0.4068
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.24593152664235,
                        "time": 2.396556619999501,
                        "accuracy": 0.4068359375,
                        "total_cost": 515789.2278463242
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.4437
Profiling... [512/50176]	Loss: 1.8201
Profiling... [768/50176]	Loss: 1.9679
Profiling... [1024/50176]	Loss: 1.9272
Profiling... [1280/50176]	Loss: 1.9757
Profiling... [1536/50176]	Loss: 2.0428
Profiling... [1792/50176]	Loss: 1.7418
Profiling... [2048/50176]	Loss: 1.8859
Profiling... [2304/50176]	Loss: 1.8992
Profiling... [2560/50176]	Loss: 2.0181
Profiling... [2816/50176]	Loss: 1.9999
Profiling... [3072/50176]	Loss: 1.9157
Profiling... [3328/50176]	Loss: 1.7459
Profile done
epoch 1 train time consumed: 4.01s
Validation Epoch: 10, Average loss: 0.0087, Accuracy: 0.4162
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.25949792144321,
                        "time": 2.406646788999751,
                        "accuracy": 0.4162109375,
                        "total_cost": 506294.0033413788
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.5210
Profiling... [512/50176]	Loss: 1.7254
Profiling... [768/50176]	Loss: 1.6638
Profiling... [1024/50176]	Loss: 1.7914
Profiling... [1280/50176]	Loss: 2.1348
Profiling... [1536/50176]	Loss: 1.9690
Profiling... [1792/50176]	Loss: 2.0856
Profiling... [2048/50176]	Loss: 2.0955
Profiling... [2304/50176]	Loss: 1.9364
Profiling... [2560/50176]	Loss: 1.7935
Profiling... [2816/50176]	Loss: 1.7689
Profiling... [3072/50176]	Loss: 1.9158
Profiling... [3328/50176]	Loss: 1.8128
Profile done
epoch 1 train time consumed: 4.15s
Validation Epoch: 10, Average loss: 0.0092, Accuracy: 0.4117
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.26420151579568,
                        "time": 2.645719432000078,
                        "accuracy": 0.41171875,
                        "total_cost": 562661.3315710129
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.5897
Profiling... [512/50176]	Loss: 1.7600
Profiling... [768/50176]	Loss: 1.9784
Profiling... [1024/50176]	Loss: 1.6977
Profiling... [1280/50176]	Loss: 1.6910
Profiling... [1536/50176]	Loss: 1.9238
Profiling... [1792/50176]	Loss: 1.7679
Profiling... [2048/50176]	Loss: 1.9981
Profiling... [2304/50176]	Loss: 2.0852
Profiling... [2560/50176]	Loss: 1.8512
Profiling... [2816/50176]	Loss: 1.8970
Profiling... [3072/50176]	Loss: 1.9283
Profiling... [3328/50176]	Loss: 1.9843
Profile done
epoch 1 train time consumed: 4.91s
Validation Epoch: 10, Average loss: 0.0094, Accuracy: 0.3847
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.24582647250361,
                        "time": 3.2173396070002127,
                        "accuracy": 0.38466796875,
                        "total_cost": 732343.3861368484
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.6156
Profiling... [512/50176]	Loss: 2.0615
Profiling... [768/50176]	Loss: 2.2535
Profiling... [1024/50176]	Loss: 2.3103
Profiling... [1280/50176]	Loss: 2.3273
Profiling... [1536/50176]	Loss: 2.3565
Profiling... [1792/50176]	Loss: 2.5083
Profiling... [2048/50176]	Loss: 2.2998
Profiling... [2304/50176]	Loss: 2.3885
Profiling... [2560/50176]	Loss: 2.2449
Profiling... [2816/50176]	Loss: 2.3652
Profiling... [3072/50176]	Loss: 2.2907
Profiling... [3328/50176]	Loss: 2.3271
Profile done
epoch 1 train time consumed: 3.89s
Validation Epoch: 10, Average loss: 0.0231, Accuracy: 0.2020
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.22878621127961,
                        "time": 2.3931736790000286,
                        "accuracy": 0.201953125,
                        "total_cost": 1037594.067974796
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.5564
Profiling... [512/50176]	Loss: 2.4775
Profiling... [768/50176]	Loss: 2.3493
Profiling... [1024/50176]	Loss: 2.1221
Profiling... [1280/50176]	Loss: 2.3163
Profiling... [1536/50176]	Loss: 2.5097
Profiling... [1792/50176]	Loss: 2.3512
Profiling... [2048/50176]	Loss: 2.3087
Profiling... [2304/50176]	Loss: 2.2676
Profiling... [2560/50176]	Loss: 2.2300
Profiling... [2816/50176]	Loss: 2.1342
Profiling... [3072/50176]	Loss: 2.1597
Profiling... [3328/50176]	Loss: 2.1706
Profile done
epoch 1 train time consumed: 3.93s
Validation Epoch: 10, Average loss: 0.0171, Accuracy: 0.1867
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.24446955350015,
                        "time": 2.41178204300013,
                        "accuracy": 0.18671875,
                        "total_cost": 1130977.6045400342
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.6643
Profiling... [512/50176]	Loss: 2.0953
Profiling... [768/50176]	Loss: 2.2647
Profiling... [1024/50176]	Loss: 2.2147
Profiling... [1280/50176]	Loss: 2.1815
Profiling... [1536/50176]	Loss: 2.3626
Profiling... [1792/50176]	Loss: 2.2040
Profiling... [2048/50176]	Loss: 2.1330
Profiling... [2304/50176]	Loss: 2.2951
Profiling... [2560/50176]	Loss: 2.4156
Profiling... [2816/50176]	Loss: 2.3814
Profiling... [3072/50176]	Loss: 2.3630
Profiling... [3328/50176]	Loss: 2.2177
Profile done
epoch 1 train time consumed: 4.19s
Validation Epoch: 10, Average loss: 0.0163, Accuracy: 0.2685
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.24947756142637,
                        "time": 2.6314873660003286,
                        "accuracy": 0.26845703125,
                        "total_cost": 858282.7769456357
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.7753
Profiling... [512/50176]	Loss: 2.0215
Profiling... [768/50176]	Loss: 2.1111
Profiling... [1024/50176]	Loss: 2.1807
Profiling... [1280/50176]	Loss: 2.2745
Profiling... [1536/50176]	Loss: 2.2652
Profiling... [1792/50176]	Loss: 2.3337
Profiling... [2048/50176]	Loss: 2.4276
Profiling... [2304/50176]	Loss: 2.3753
Profiling... [2560/50176]	Loss: 2.4212
Profiling... [2816/50176]	Loss: 2.3019
Profiling... [3072/50176]	Loss: 2.4708
Profiling... [3328/50176]	Loss: 2.3713
Profile done
epoch 1 train time consumed: 4.82s
Validation Epoch: 10, Average loss: 0.0171, Accuracy: 0.2428
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.2306688611577,
                        "time": 3.201134536000609,
                        "accuracy": 0.2427734375,
                        "total_cost": 1154533.6734212295
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.5811
Profiling... [512/50176]	Loss: 2.1462
Profiling... [768/50176]	Loss: 2.1955
Profiling... [1024/50176]	Loss: 2.1825
Profiling... [1280/50176]	Loss: 2.3017
Profiling... [1536/50176]	Loss: 2.4700
Profiling... [1792/50176]	Loss: 2.2197
Profiling... [2048/50176]	Loss: 2.3517
Profiling... [2304/50176]	Loss: 2.3665
Profiling... [2560/50176]	Loss: 2.5428
Profiling... [2816/50176]	Loss: 2.2455
Profiling... [3072/50176]	Loss: 2.1384
Profiling... [3328/50176]	Loss: 2.2415
Profile done
epoch 1 train time consumed: 3.97s
Validation Epoch: 10, Average loss: 0.0214, Accuracy: 0.2040
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.21373314921715,
                        "time": 2.405466090999653,
                        "accuracy": 0.20400390625,
                        "total_cost": 1032439.3739828479
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.6309
Profiling... [512/50176]	Loss: 2.4141
Profiling... [768/50176]	Loss: 2.3634
Profiling... [1024/50176]	Loss: 2.1968
Profiling... [1280/50176]	Loss: 2.2560
Profiling... [1536/50176]	Loss: 2.1199
Profiling... [1792/50176]	Loss: 2.1953
Profiling... [2048/50176]	Loss: 2.1646
Profiling... [2304/50176]	Loss: 2.1557
Profiling... [2560/50176]	Loss: 2.3072
Profiling... [2816/50176]	Loss: 2.4334
Profiling... [3072/50176]	Loss: 2.1496
Profiling... [3328/50176]	Loss: 1.9209
Profile done
epoch 1 train time consumed: 3.91s
Validation Epoch: 10, Average loss: 0.0121, Accuracy: 0.3255
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.22693559799384,
                        "time": 2.4136538639995706,
                        "accuracy": 0.32548828125,
                        "total_cost": 649297.1069964193
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.8184
Profiling... [512/50176]	Loss: 2.4109
Profiling... [768/50176]	Loss: 2.1993
Profiling... [1024/50176]	Loss: 2.2287
Profiling... [1280/50176]	Loss: 2.1023
Profiling... [1536/50176]	Loss: 2.2975
Profiling... [1792/50176]	Loss: 2.2476
Profiling... [2048/50176]	Loss: 2.3928
Profiling... [2304/50176]	Loss: 2.3888
Profiling... [2560/50176]	Loss: 2.2651
Profiling... [2816/50176]	Loss: 2.4091
Profiling... [3072/50176]	Loss: 2.2377
Profiling... [3328/50176]	Loss: 2.3692
Profile done
epoch 1 train time consumed: 4.17s
Validation Epoch: 10, Average loss: 0.0195, Accuracy: 0.2147
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.23380057856855,
                        "time": 2.623125352999523,
                        "accuracy": 0.21474609375,
                        "total_cost": 1069541.4616363624
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.5661
Profiling... [512/50176]	Loss: 2.3940
Profiling... [768/50176]	Loss: 2.3152
Profiling... [1024/50176]	Loss: 2.3537
Profiling... [1280/50176]	Loss: 2.3923
Profiling... [1536/50176]	Loss: 2.2107
Profiling... [1792/50176]	Loss: 2.2202
Profiling... [2048/50176]	Loss: 2.2441
Profiling... [2304/50176]	Loss: 2.2035
Profiling... [2560/50176]	Loss: 2.5726
Profiling... [2816/50176]	Loss: 2.1763
Profiling... [3072/50176]	Loss: 2.1908
Profiling... [3328/50176]	Loss: 2.3701
Profile done
epoch 1 train time consumed: 5.05s
Validation Epoch: 10, Average loss: 0.0214, Accuracy: 0.2096
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.21408680696047,
                        "time": 3.214351624000301,
                        "accuracy": 0.2095703125,
                        "total_cost": 1342973.4476193625
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.6125
Profiling... [512/50176]	Loss: 2.3662
Profiling... [768/50176]	Loss: 2.1304
Profiling... [1024/50176]	Loss: 2.3615
Profiling... [1280/50176]	Loss: 2.2474
Profiling... [1536/50176]	Loss: 2.1885
Profiling... [1792/50176]	Loss: 2.1606
Profiling... [2048/50176]	Loss: 2.0792
Profiling... [2304/50176]	Loss: 2.4384
Profiling... [2560/50176]	Loss: 2.3920
Profiling... [2816/50176]	Loss: 2.4961
Profiling... [3072/50176]	Loss: 2.3513
Profiling... [3328/50176]	Loss: 2.2289
Profile done
epoch 1 train time consumed: 3.93s
Validation Epoch: 10, Average loss: 0.0186, Accuracy: 0.2441
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.19181790856203,
                        "time": 2.3966192890002276,
                        "accuracy": 0.244140625,
                        "total_cost": 859533.3795531376
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.6491
Profiling... [512/50176]	Loss: 1.9875
Profiling... [768/50176]	Loss: 2.3741
Profiling... [1024/50176]	Loss: 2.1257
Profiling... [1280/50176]	Loss: 2.0360
Profiling... [1536/50176]	Loss: 2.3930
Profiling... [1792/50176]	Loss: 2.2744
Profiling... [2048/50176]	Loss: 2.2701
Profiling... [2304/50176]	Loss: 2.4807
Profiling... [2560/50176]	Loss: 2.4002
Profiling... [2816/50176]	Loss: 2.2795
Profiling... [3072/50176]	Loss: 2.2458
Profiling... [3328/50176]	Loss: 2.1938
Profile done
epoch 1 train time consumed: 3.97s
Validation Epoch: 10, Average loss: 0.0173, Accuracy: 0.2345
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.20489868643435,
                        "time": 2.410916570000154,
                        "accuracy": 0.23447265625,
                        "total_cost": 900313.4940505356
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.6256
Profiling... [512/50176]	Loss: 2.3483
Profiling... [768/50176]	Loss: 2.3188
Profiling... [1024/50176]	Loss: 2.1997
Profiling... [1280/50176]	Loss: 2.3544
Profiling... [1536/50176]	Loss: 2.1178
Profiling... [1792/50176]	Loss: 2.2007
Profiling... [2048/50176]	Loss: 2.3585
Profiling... [2304/50176]	Loss: 2.5409
Profiling... [2560/50176]	Loss: 2.1844
Profiling... [2816/50176]	Loss: 2.2948
Profiling... [3072/50176]	Loss: 2.2329
Profiling... [3328/50176]	Loss: 2.1566
Profile done
epoch 1 train time consumed: 4.24s
Validation Epoch: 10, Average loss: 0.0191, Accuracy: 0.2230
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.20960233167035,
                        "time": 2.6339091879999614,
                        "accuracy": 0.223046875,
                        "total_cost": 1033971.2115824812
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.5999
Profiling... [512/50176]	Loss: 2.2224
Profiling... [768/50176]	Loss: 2.2986
Profiling... [1024/50176]	Loss: 2.0394
Profiling... [1280/50176]	Loss: 2.3161
Profiling... [1536/50176]	Loss: 2.3535
Profiling... [1792/50176]	Loss: 2.3164
Profiling... [2048/50176]	Loss: 2.2328
Profiling... [2304/50176]	Loss: 2.3016
Profiling... [2560/50176]	Loss: 2.5494
Profiling... [2816/50176]	Loss: 2.2357
Profiling... [3072/50176]	Loss: 2.0465
Profiling... [3328/50176]	Loss: 2.1991
Profile done
epoch 1 train time consumed: 5.00s
Validation Epoch: 10, Average loss: 0.0188, Accuracy: 0.2298
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.18949042095227,
                        "time": 3.2015746259994557,
                        "accuracy": 0.22978515625,
                        "total_cost": 1219959.467242833
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.6058
Profiling... [1024/50176]	Loss: 1.6340
Profiling... [1536/50176]	Loss: 1.6276
Profiling... [2048/50176]	Loss: 1.6645
Profiling... [2560/50176]	Loss: 1.7638
Profiling... [3072/50176]	Loss: 1.6875
Profiling... [3584/50176]	Loss: 1.7173
Profiling... [4096/50176]	Loss: 1.6541
Profiling... [4608/50176]	Loss: 1.5749
Profiling... [5120/50176]	Loss: 1.6708
Profiling... [5632/50176]	Loss: 1.7224
Profiling... [6144/50176]	Loss: 1.6317
Profiling... [6656/50176]	Loss: 1.6803
Profile done
epoch 1 train time consumed: 6.95s
Validation Epoch: 10, Average loss: 0.0035, Accuracy: 0.5039
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.19084895232278,
                        "time": 4.565673176000018,
                        "accuracy": 0.50390625,
                        "total_cost": 793339.0310815648
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.6030
Profiling... [1024/50176]	Loss: 1.6374
Profiling... [1536/50176]	Loss: 1.5628
Profiling... [2048/50176]	Loss: 1.6850
Profiling... [2560/50176]	Loss: 1.5926
Profiling... [3072/50176]	Loss: 1.6902
Profiling... [3584/50176]	Loss: 1.5936
Profiling... [4096/50176]	Loss: 1.7093
Profiling... [4608/50176]	Loss: 1.6691
Profiling... [5120/50176]	Loss: 1.6519
Profiling... [5632/50176]	Loss: 1.5288
Profiling... [6144/50176]	Loss: 1.5394
Profiling... [6656/50176]	Loss: 1.5728
Profile done
epoch 1 train time consumed: 6.97s
Validation Epoch: 10, Average loss: 0.0035, Accuracy: 0.5057
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.21488744997896,
                        "time": 4.593775948999792,
                        "accuracy": 0.5056640625,
                        "total_cost": 795447.5087476618
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.5998
Profiling... [1024/50176]	Loss: 1.6703
Profiling... [1536/50176]	Loss: 1.6648
Profiling... [2048/50176]	Loss: 1.6848
Profiling... [2560/50176]	Loss: 1.8291
Profiling... [3072/50176]	Loss: 1.5448
Profiling... [3584/50176]	Loss: 1.7285
Profiling... [4096/50176]	Loss: 1.7245
Profiling... [4608/50176]	Loss: 1.7720
Profiling... [5120/50176]	Loss: 1.7230
Profiling... [5632/50176]	Loss: 1.6676
Profiling... [6144/50176]	Loss: 1.6928
Profiling... [6656/50176]	Loss: 1.6554
Profile done
epoch 1 train time consumed: 7.50s
Validation Epoch: 10, Average loss: 0.0035, Accuracy: 0.5015
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.22034037424405,
                        "time": 5.024263302999316,
                        "accuracy": 0.50146484375,
                        "total_cost": 877274.9310065605
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.5383
Profiling... [1024/50176]	Loss: 1.6145
Profiling... [1536/50176]	Loss: 1.6389
Profiling... [2048/50176]	Loss: 1.6968
Profiling... [2560/50176]	Loss: 1.6040
Profiling... [3072/50176]	Loss: 1.7351
Profiling... [3584/50176]	Loss: 1.6356
Profiling... [4096/50176]	Loss: 1.6821
Profiling... [4608/50176]	Loss: 1.6698
Profiling... [5120/50176]	Loss: 1.7225
Profiling... [5632/50176]	Loss: 1.6843
Profiling... [6144/50176]	Loss: 1.7586
Profiling... [6656/50176]	Loss: 1.7080
Profile done
epoch 1 train time consumed: 12.64s
Validation Epoch: 10, Average loss: 0.0035, Accuracy: 0.5010
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.18076193238754,
                        "time": 9.075577091000014,
                        "accuracy": 0.5009765625,
                        "total_cost": 1586209.5595716457
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.6307
Profiling... [1024/50176]	Loss: 1.6119
Profiling... [1536/50176]	Loss: 1.5770
Profiling... [2048/50176]	Loss: 1.8013
Profiling... [2560/50176]	Loss: 1.7155
Profiling... [3072/50176]	Loss: 1.5843
Profiling... [3584/50176]	Loss: 1.6692
Profiling... [4096/50176]	Loss: 1.6959
Profiling... [4608/50176]	Loss: 1.5292
Profiling... [5120/50176]	Loss: 1.6617
Profiling... [5632/50176]	Loss: 1.6117
Profiling... [6144/50176]	Loss: 1.7846
Profiling... [6656/50176]	Loss: 1.7423
Profile done
epoch 1 train time consumed: 6.98s
Validation Epoch: 10, Average loss: 0.0035, Accuracy: 0.4967
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.16553379972551,
                        "time": 4.55800614599957,
                        "accuracy": 0.4966796875,
                        "total_cost": 803530.1753560994
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.7959
Profiling... [1024/50176]	Loss: 1.6432
Profiling... [1536/50176]	Loss: 1.7234
Profiling... [2048/50176]	Loss: 1.7340
Profiling... [2560/50176]	Loss: 1.7075
Profiling... [3072/50176]	Loss: 1.5227
Profiling... [3584/50176]	Loss: 1.6623
Profiling... [4096/50176]	Loss: 1.6250
Profiling... [4608/50176]	Loss: 1.6807
Profiling... [5120/50176]	Loss: 1.6891
Profiling... [5632/50176]	Loss: 1.7097
Profiling... [6144/50176]	Loss: 1.5865
Profiling... [6656/50176]	Loss: 1.6242
Profile done
epoch 1 train time consumed: 7.04s
Validation Epoch: 10, Average loss: 0.0035, Accuracy: 0.4971
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.18809514188764,
                        "time": 4.590062416999899,
                        "accuracy": 0.4970703125,
                        "total_cost": 808545.575502507
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.5668
Profiling... [1024/50176]	Loss: 1.7627
Profiling... [1536/50176]	Loss: 1.7051
Profiling... [2048/50176]	Loss: 1.5937
Profiling... [2560/50176]	Loss: 1.4994
Profiling... [3072/50176]	Loss: 1.7411
Profiling... [3584/50176]	Loss: 1.7108
Profiling... [4096/50176]	Loss: 1.5617
Profiling... [4608/50176]	Loss: 1.7480
Profiling... [5120/50176]	Loss: 1.6498
Profiling... [5632/50176]	Loss: 1.6216
Profiling... [6144/50176]	Loss: 1.5811
Profiling... [6656/50176]	Loss: 1.8259
Profile done
epoch 1 train time consumed: 7.59s
Validation Epoch: 10, Average loss: 0.0035, Accuracy: 0.5004
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.19442683980603,
                        "time": 5.060732232000191,
                        "accuracy": 0.500390625,
                        "total_cost": 885539.5239247502
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.6391
Profiling... [1024/50176]	Loss: 1.7096
Profiling... [1536/50176]	Loss: 1.6595
Profiling... [2048/50176]	Loss: 1.6478
Profiling... [2560/50176]	Loss: 1.6593
Profiling... [3072/50176]	Loss: 1.6246
Profiling... [3584/50176]	Loss: 1.8064
Profiling... [4096/50176]	Loss: 1.5823
Profiling... [4608/50176]	Loss: 1.7214
Profiling... [5120/50176]	Loss: 1.6263
Profiling... [5632/50176]	Loss: 1.7036
Profiling... [6144/50176]	Loss: 1.7856
Profiling... [6656/50176]	Loss: 1.7832
Profile done
epoch 1 train time consumed: 13.17s
Validation Epoch: 10, Average loss: 0.0035, Accuracy: 0.5029
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.1559204239312,
                        "time": 9.125395179999941,
                        "accuracy": 0.5029296875,
                        "total_cost": 1588722.579994738
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.6213
Profiling... [1024/50176]	Loss: 1.6267
Profiling... [1536/50176]	Loss: 1.6469
Profiling... [2048/50176]	Loss: 1.6222
Profiling... [2560/50176]	Loss: 1.5479
Profiling... [3072/50176]	Loss: 1.6131
Profiling... [3584/50176]	Loss: 1.7304
Profiling... [4096/50176]	Loss: 1.7005
Profiling... [4608/50176]	Loss: 1.6031
Profiling... [5120/50176]	Loss: 1.6039
Profiling... [5632/50176]	Loss: 1.6471
Profiling... [6144/50176]	Loss: 1.6980
Profiling... [6656/50176]	Loss: 1.7157
Profile done
epoch 1 train time consumed: 6.95s
Validation Epoch: 10, Average loss: 0.0035, Accuracy: 0.4997
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.13626972957154,
                        "time": 4.55945971500023,
                        "accuracy": 0.49970703125,
                        "total_cost": 798916.7624379587
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.6638
Profiling... [1024/50176]	Loss: 1.6069
Profiling... [1536/50176]	Loss: 1.5611
Profiling... [2048/50176]	Loss: 1.5686
Profiling... [2560/50176]	Loss: 1.6239
Profiling... [3072/50176]	Loss: 1.6379
Profiling... [3584/50176]	Loss: 1.6477
Profiling... [4096/50176]	Loss: 1.7368
Profiling... [4608/50176]	Loss: 1.5846
Profiling... [5120/50176]	Loss: 1.7185
Profiling... [5632/50176]	Loss: 1.7486
Profiling... [6144/50176]	Loss: 1.6241
Profiling... [6656/50176]	Loss: 1.5642
Profile done
epoch 1 train time consumed: 6.92s
Validation Epoch: 10, Average loss: 0.0035, Accuracy: 0.5005
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.16169122327874,
                        "time": 4.590862410000227,
                        "accuracy": 0.50048828125,
                        "total_cost": 803163.6371895458
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.7877
Profiling... [1024/50176]	Loss: 1.6259
Profiling... [1536/50176]	Loss: 1.7007
Profiling... [2048/50176]	Loss: 1.8704
Profiling... [2560/50176]	Loss: 1.6004
Profiling... [3072/50176]	Loss: 1.5800
Profiling... [3584/50176]	Loss: 1.7211
Profiling... [4096/50176]	Loss: 1.4835
Profiling... [4608/50176]	Loss: 1.6590
Profiling... [5120/50176]	Loss: 1.5634
Profiling... [5632/50176]	Loss: 1.7055
Profiling... [6144/50176]	Loss: 1.7400
Profiling... [6656/50176]	Loss: 1.7943
Profile done
epoch 1 train time consumed: 7.45s
Validation Epoch: 10, Average loss: 0.0035, Accuracy: 0.5018
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.16768393263062,
                        "time": 5.012627576000341,
                        "accuracy": 0.5017578125,
                        "total_cost": 874731.9395437501
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7656
Profiling... [1024/50176]	Loss: 1.6886
Profiling... [1536/50176]	Loss: 1.6489
Profiling... [2048/50176]	Loss: 1.6596
Profiling... [2560/50176]	Loss: 1.6383
Profiling... [3072/50176]	Loss: 1.6888
Profiling... [3584/50176]	Loss: 1.6854
Profiling... [4096/50176]	Loss: 1.7723
Profiling... [4608/50176]	Loss: 1.6276
Profiling... [5120/50176]	Loss: 1.6114
Profiling... [5632/50176]	Loss: 1.6322
Profiling... [6144/50176]	Loss: 1.6187
Profiling... [6656/50176]	Loss: 1.5845
Profile done
epoch 1 train time consumed: 13.18s
Validation Epoch: 10, Average loss: 0.0035, Accuracy: 0.5035
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.13102182097074,
                        "time": 9.269835341000544,
                        "accuracy": 0.503515625,
                        "total_cost": 1611991.1965307735
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.6842
Profiling... [1024/50176]	Loss: 1.7217
Profiling... [1536/50176]	Loss: 1.7334
Profiling... [2048/50176]	Loss: 2.0173
Profiling... [2560/50176]	Loss: 1.6981
Profiling... [3072/50176]	Loss: 1.8191
Profiling... [3584/50176]	Loss: 1.8091
Profiling... [4096/50176]	Loss: 1.8434
Profiling... [4608/50176]	Loss: 1.6935
Profiling... [5120/50176]	Loss: 1.9364
Profiling... [5632/50176]	Loss: 1.8982
Profiling... [6144/50176]	Loss: 1.8829
Profiling... [6656/50176]	Loss: 2.0282
Profile done
epoch 1 train time consumed: 6.91s
Validation Epoch: 10, Average loss: 0.0042, Accuracy: 0.4386
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.11292000339876,
                        "time": 4.550552859000163,
                        "accuracy": 0.43857421875,
                        "total_cost": 908499.3438909178
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.6892
Profiling... [1024/50176]	Loss: 1.7493
Profiling... [1536/50176]	Loss: 1.9056
Profiling... [2048/50176]	Loss: 1.6611
Profiling... [2560/50176]	Loss: 2.0038
Profiling... [3072/50176]	Loss: 1.6965
Profiling... [3584/50176]	Loss: 1.7913
Profiling... [4096/50176]	Loss: 1.9507
Profiling... [4608/50176]	Loss: 1.8936
Profiling... [5120/50176]	Loss: 1.8177
Profiling... [5632/50176]	Loss: 1.9177
Profiling... [6144/50176]	Loss: 1.7502
Profiling... [6656/50176]	Loss: 1.9545
Profile done
epoch 1 train time consumed: 7.03s
Validation Epoch: 10, Average loss: 0.0043, Accuracy: 0.4205
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.13671676946585,
                        "time": 4.58121263799967,
                        "accuracy": 0.4205078125,
                        "total_cost": 953915.6924491816
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.5356
Profiling... [1024/50176]	Loss: 1.7735
Profiling... [1536/50176]	Loss: 1.9049
Profiling... [2048/50176]	Loss: 1.8507
Profiling... [2560/50176]	Loss: 1.7820
Profiling... [3072/50176]	Loss: 1.8835
Profiling... [3584/50176]	Loss: 1.9274
Profiling... [4096/50176]	Loss: 1.8186
Profiling... [4608/50176]	Loss: 1.8920
Profiling... [5120/50176]	Loss: 1.8297
Profiling... [5632/50176]	Loss: 1.7545
Profiling... [6144/50176]	Loss: 1.8946
Profiling... [6656/50176]	Loss: 1.8788
Profile done
epoch 1 train time consumed: 7.49s
Validation Epoch: 10, Average loss: 0.0041, Accuracy: 0.4388
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.1405795602889,
                        "time": 5.048980322999341,
                        "accuracy": 0.43876953125,
                        "total_cost": 1007559.8144290645
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7992
Profiling... [1024/50176]	Loss: 1.8240
Profiling... [1536/50176]	Loss: 1.7729
Profiling... [2048/50176]	Loss: 1.6977
Profiling... [2560/50176]	Loss: 1.7240
Profiling... [3072/50176]	Loss: 1.7564
Profiling... [3584/50176]	Loss: 1.8720
Profiling... [4096/50176]	Loss: 1.9801
Profiling... [4608/50176]	Loss: 1.9373
Profiling... [5120/50176]	Loss: 1.7609
Profiling... [5632/50176]	Loss: 1.8480
Profiling... [6144/50176]	Loss: 1.7960
Profiling... [6656/50176]	Loss: 1.9209
Profile done
epoch 1 train time consumed: 13.28s
Validation Epoch: 10, Average loss: 0.0042, Accuracy: 0.4213
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.10239561555446,
                        "time": 9.20979058499961,
                        "accuracy": 0.4212890625,
                        "total_cost": 1914137.3513544784
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.7439
Profiling... [1024/50176]	Loss: 1.7967
Profiling... [1536/50176]	Loss: 1.8464
Profiling... [2048/50176]	Loss: 1.8305
Profiling... [2560/50176]	Loss: 1.8742
Profiling... [3072/50176]	Loss: 1.8337
Profiling... [3584/50176]	Loss: 1.7892
Profiling... [4096/50176]	Loss: 1.9283
Profiling... [4608/50176]	Loss: 1.8982
Profiling... [5120/50176]	Loss: 1.8661
Profiling... [5632/50176]	Loss: 1.8110
Profiling... [6144/50176]	Loss: 1.8112
Profiling... [6656/50176]	Loss: 1.7709
Profile done
epoch 1 train time consumed: 6.94s
Validation Epoch: 10, Average loss: 0.0047, Accuracy: 0.4014
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.08424446128502,
                        "time": 4.545130693999454,
                        "accuracy": 0.4013671875,
                        "total_cost": 991534.8709273564
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.6343
Profiling... [1024/50176]	Loss: 1.7437
Profiling... [1536/50176]	Loss: 1.8480
Profiling... [2048/50176]	Loss: 1.8624
Profiling... [2560/50176]	Loss: 2.0414
Profiling... [3072/50176]	Loss: 1.8339
Profiling... [3584/50176]	Loss: 1.8819
Profiling... [4096/50176]	Loss: 1.8088
Profiling... [4608/50176]	Loss: 1.7242
Profiling... [5120/50176]	Loss: 1.8374
Profiling... [5632/50176]	Loss: 1.9023
Profiling... [6144/50176]	Loss: 2.0914
Profiling... [6656/50176]	Loss: 1.8533
Profile done
epoch 1 train time consumed: 7.11s
Validation Epoch: 10, Average loss: 0.0044, Accuracy: 0.4156
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.10883665861688,
                        "time": 4.588396893999743,
                        "accuracy": 0.415625,
                        "total_cost": 966635.759478048
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.5416
Profiling... [1024/50176]	Loss: 1.8290
Profiling... [1536/50176]	Loss: 1.8398
Profiling... [2048/50176]	Loss: 1.9478
Profiling... [2560/50176]	Loss: 2.0260
Profiling... [3072/50176]	Loss: 1.6781
Profiling... [3584/50176]	Loss: 1.7646
Profiling... [4096/50176]	Loss: 1.9576
Profiling... [4608/50176]	Loss: 1.8600
Profiling... [5120/50176]	Loss: 1.7952
Profiling... [5632/50176]	Loss: 1.7769
Profiling... [6144/50176]	Loss: 1.8548
Profiling... [6656/50176]	Loss: 1.7940
Profile done
epoch 1 train time consumed: 7.48s
Validation Epoch: 10, Average loss: 0.0040, Accuracy: 0.4427
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.112076855124,
                        "time": 5.052193423999597,
                        "accuracy": 0.44267578125,
                        "total_cost": 999304.3034261724
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7072
Profiling... [1024/50176]	Loss: 1.6545
Profiling... [1536/50176]	Loss: 1.8222
Profiling... [2048/50176]	Loss: 1.7297
Profiling... [2560/50176]	Loss: 1.8599
Profiling... [3072/50176]	Loss: 1.7085
Profiling... [3584/50176]	Loss: 1.8817
Profiling... [4096/50176]	Loss: 1.7348
Profiling... [4608/50176]	Loss: 1.8271
Profiling... [5120/50176]	Loss: 1.9369
Profiling... [5632/50176]	Loss: 1.9401
Profiling... [6144/50176]	Loss: 1.8755
Profiling... [6656/50176]	Loss: 1.8353
Profile done
epoch 1 train time consumed: 14.23s
Validation Epoch: 10, Average loss: 0.0044, Accuracy: 0.4221
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.07379935981166,
                        "time": 10.030252493000262,
                        "accuracy": 0.4220703125,
                        "total_cost": 2080800.8458874084
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.6214
Profiling... [1024/50176]	Loss: 1.6873
Profiling... [1536/50176]	Loss: 1.6252
Profiling... [2048/50176]	Loss: 1.8531
Profiling... [2560/50176]	Loss: 1.8289
Profiling... [3072/50176]	Loss: 1.8703
Profiling... [3584/50176]	Loss: 1.7483
Profiling... [4096/50176]	Loss: 1.8956
Profiling... [4608/50176]	Loss: 1.8408
Profiling... [5120/50176]	Loss: 1.7883
Profiling... [5632/50176]	Loss: 1.8498
Profiling... [6144/50176]	Loss: 1.8517
Profiling... [6656/50176]	Loss: 1.7917
Profile done
epoch 1 train time consumed: 6.97s
Validation Epoch: 10, Average loss: 0.0047, Accuracy: 0.3962
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.05746579180335,
                        "time": 4.5498111560000325,
                        "accuracy": 0.39619140625,
                        "total_cost": 1005522.3671147842
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.7328
Profiling... [1024/50176]	Loss: 1.7729
Profiling... [1536/50176]	Loss: 1.6815
Profiling... [2048/50176]	Loss: 1.7738
Profiling... [2560/50176]	Loss: 1.9175
Profiling... [3072/50176]	Loss: 1.8954
Profiling... [3584/50176]	Loss: 1.7720
Profiling... [4096/50176]	Loss: 1.8727
Profiling... [4608/50176]	Loss: 1.8344
Profiling... [5120/50176]	Loss: 1.8869
Profiling... [5632/50176]	Loss: 1.7994
Profiling... [6144/50176]	Loss: 1.8588
Profiling... [6656/50176]	Loss: 1.8328
Profile done
epoch 1 train time consumed: 7.01s
Validation Epoch: 10, Average loss: 0.0044, Accuracy: 0.4192
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.07971167994317,
                        "time": 4.582362072000251,
                        "accuracy": 0.41923828125,
                        "total_cost": 957044.0783243609
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.7629
Profiling... [1024/50176]	Loss: 1.8573
Profiling... [1536/50176]	Loss: 1.8140
Profiling... [2048/50176]	Loss: 1.8184
Profiling... [2560/50176]	Loss: 1.8667
Profiling... [3072/50176]	Loss: 1.7850
Profiling... [3584/50176]	Loss: 1.8725
Profiling... [4096/50176]	Loss: 1.9758
Profiling... [4608/50176]	Loss: 1.8667
Profiling... [5120/50176]	Loss: 1.8697
Profiling... [5632/50176]	Loss: 1.8884
Profiling... [6144/50176]	Loss: 1.8562
Profiling... [6656/50176]	Loss: 1.8590
Profile done
epoch 1 train time consumed: 7.46s
Validation Epoch: 10, Average loss: 0.0045, Accuracy: 0.4077
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.08500722362507,
                        "time": 5.045904214999609,
                        "accuracy": 0.40771484375,
                        "total_cost": 1083642.333251234
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.6161
Profiling... [1024/50176]	Loss: 1.8302
Profiling... [1536/50176]	Loss: 1.8062
Profiling... [2048/50176]	Loss: 1.8335
Profiling... [2560/50176]	Loss: 1.7767
Profiling... [3072/50176]	Loss: 1.7427
Profiling... [3584/50176]	Loss: 1.8205
Profiling... [4096/50176]	Loss: 1.9064
Profiling... [4608/50176]	Loss: 1.7583
Profiling... [5120/50176]	Loss: 1.8634
Profiling... [5632/50176]	Loss: 1.8044
Profiling... [6144/50176]	Loss: 1.8191
Profiling... [6656/50176]	Loss: 1.7647
Profile done
epoch 1 train time consumed: 14.59s
Validation Epoch: 10, Average loss: 0.0042, Accuracy: 0.4396
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.04562662866678,
                        "time": 10.302727954000147,
                        "accuracy": 0.4396484375,
                        "total_cost": 2051871.1460850975
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.6869
Profiling... [1024/50176]	Loss: 2.2013
Profiling... [1536/50176]	Loss: 2.1024
Profiling... [2048/50176]	Loss: 1.9949
Profiling... [2560/50176]	Loss: 2.1494
Profiling... [3072/50176]	Loss: 2.1890
Profiling... [3584/50176]	Loss: 2.1134
Profiling... [4096/50176]	Loss: 2.0672
Profiling... [4608/50176]	Loss: 2.1862
Profiling... [5120/50176]	Loss: 2.2188
Profiling... [5632/50176]	Loss: 2.1275
Profiling... [6144/50176]	Loss: 2.2149
Profiling... [6656/50176]	Loss: 2.1185
Profile done
epoch 1 train time consumed: 6.88s
Validation Epoch: 10, Average loss: 0.0070, Accuracy: 0.2424
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.02607335755577,
                        "time": 4.546501476999765,
                        "accuracy": 0.2423828125,
                        "total_cost": 1642399.6868451235
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.6233
Profiling... [1024/50176]	Loss: 2.1271
Profiling... [1536/50176]	Loss: 2.1426
Profiling... [2048/50176]	Loss: 2.1973
Profiling... [2560/50176]	Loss: 2.0144
Profiling... [3072/50176]	Loss: 2.2076
Profiling... [3584/50176]	Loss: 2.2589
Profiling... [4096/50176]	Loss: 2.1306
Profiling... [4608/50176]	Loss: 2.2704
Profiling... [5120/50176]	Loss: 2.1046
Profiling... [5632/50176]	Loss: 2.1562
Profiling... [6144/50176]	Loss: 2.1101
Profiling... [6656/50176]	Loss: 2.0755
Profile done
epoch 1 train time consumed: 7.01s
Validation Epoch: 10, Average loss: 0.0065, Accuracy: 0.3131
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.04974330851636,
                        "time": 4.585323489000075,
                        "accuracy": 0.3130859375,
                        "total_cost": 1282359.5632738457
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.6825
Profiling... [1024/50176]	Loss: 2.2161
Profiling... [1536/50176]	Loss: 2.1398
Profiling... [2048/50176]	Loss: 2.0998
Profiling... [2560/50176]	Loss: 2.1393
Profiling... [3072/50176]	Loss: 2.0493
Profiling... [3584/50176]	Loss: 2.0656
Profiling... [4096/50176]	Loss: 2.1089
Profiling... [4608/50176]	Loss: 2.1162
Profiling... [5120/50176]	Loss: 2.0962
Profiling... [5632/50176]	Loss: 2.1812
Profiling... [6144/50176]	Loss: 2.1925
Profiling... [6656/50176]	Loss: 1.9894
Profile done
epoch 1 train time consumed: 7.51s
Validation Epoch: 10, Average loss: 0.0060, Accuracy: 0.3220
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.0538045057047,
                        "time": 5.0652967710002486,
                        "accuracy": 0.32197265625,
                        "total_cost": 1377492.7164744004
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.5878
Profiling... [1024/50176]	Loss: 2.2595
Profiling... [1536/50176]	Loss: 2.1446
Profiling... [2048/50176]	Loss: 2.0976
Profiling... [2560/50176]	Loss: 2.1442
Profiling... [3072/50176]	Loss: 2.0727
Profiling... [3584/50176]	Loss: 2.1941
Profiling... [4096/50176]	Loss: 2.0977
Profiling... [4608/50176]	Loss: 2.2018
Profiling... [5120/50176]	Loss: 2.1528
Profiling... [5632/50176]	Loss: 2.2885
Profiling... [6144/50176]	Loss: 2.0707
Profiling... [6656/50176]	Loss: 2.2304
Profile done
epoch 1 train time consumed: 13.24s
Validation Epoch: 10, Average loss: 0.0072, Accuracy: 0.2815
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.0193822725494,
                        "time": 9.048847363999812,
                        "accuracy": 0.28154296875,
                        "total_cost": 2814180.1657469184
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.6237
Profiling... [1024/50176]	Loss: 2.2618
Profiling... [1536/50176]	Loss: 2.0905
Profiling... [2048/50176]	Loss: 2.1092
Profiling... [2560/50176]	Loss: 2.2851
Profiling... [3072/50176]	Loss: 2.2246
Profiling... [3584/50176]	Loss: 2.1762
Profiling... [4096/50176]	Loss: 2.0091
Profiling... [4608/50176]	Loss: 2.0971
Profiling... [5120/50176]	Loss: 2.1657
Profiling... [5632/50176]	Loss: 2.2000
Profiling... [6144/50176]	Loss: 2.0656
Profiling... [6656/50176]	Loss: 2.1017
Profile done
epoch 1 train time consumed: 6.92s
Validation Epoch: 10, Average loss: 0.0074, Accuracy: 0.2694
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.00124698813012,
                        "time": 4.555792229999497,
                        "accuracy": 0.26943359375,
                        "total_cost": 1480523.965297666
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.6748
Profiling... [1024/50176]	Loss: 2.2754
Profiling... [1536/50176]	Loss: 2.1444
Profiling... [2048/50176]	Loss: 2.2409
Profiling... [2560/50176]	Loss: 2.0465
Profiling... [3072/50176]	Loss: 2.1767
Profiling... [3584/50176]	Loss: 2.2063
Profiling... [4096/50176]	Loss: 2.0079
Profiling... [4608/50176]	Loss: 2.0669
Profiling... [5120/50176]	Loss: 2.2266
Profiling... [5632/50176]	Loss: 2.2212
Profiling... [6144/50176]	Loss: 2.0104
Profiling... [6656/50176]	Loss: 2.2446
Profile done
epoch 1 train time consumed: 7.10s
Validation Epoch: 10, Average loss: 0.0059, Accuracy: 0.3015
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.02531396740751,
                        "time": 4.589214848000665,
                        "accuracy": 0.30146484375,
                        "total_cost": 1332922.9722798541
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.7099
Profiling... [1024/50176]	Loss: 1.9950
Profiling... [1536/50176]	Loss: 2.2434
Profiling... [2048/50176]	Loss: 2.1632
Profiling... [2560/50176]	Loss: 2.1143
Profiling... [3072/50176]	Loss: 2.1410
Profiling... [3584/50176]	Loss: 2.2218
Profiling... [4096/50176]	Loss: 2.1098
Profiling... [4608/50176]	Loss: 2.2126
Profiling... [5120/50176]	Loss: 2.1936
Profiling... [5632/50176]	Loss: 2.0769
Profiling... [6144/50176]	Loss: 2.0001
Profiling... [6656/50176]	Loss: 1.9989
Profile done
epoch 1 train time consumed: 7.51s
Validation Epoch: 10, Average loss: 0.0072, Accuracy: 0.2839
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.02872086012562,
                        "time": 5.052963575000831,
                        "accuracy": 0.28388671875,
                        "total_cost": 1558491.4949742267
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7269
Profiling... [1024/50176]	Loss: 2.2363
Profiling... [1536/50176]	Loss: 2.1103
Profiling... [2048/50176]	Loss: 2.2002
Profiling... [2560/50176]	Loss: 2.2248
Profiling... [3072/50176]	Loss: 2.0032
Profiling... [3584/50176]	Loss: 2.0028
Profiling... [4096/50176]	Loss: 2.1814
Profiling... [4608/50176]	Loss: 2.0521
Profiling... [5120/50176]	Loss: 2.0528
Profiling... [5632/50176]	Loss: 2.1812
Profiling... [6144/50176]	Loss: 2.0304
Profiling... [6656/50176]	Loss: 2.0260
Profile done
epoch 1 train time consumed: 11.88s
Validation Epoch: 10, Average loss: 0.0095, Accuracy: 0.2086
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.99796941715707,
                        "time": 7.653727908000292,
                        "accuracy": 0.20859375,
                        "total_cost": 3212735.669164523
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.7039
Profiling... [1024/50176]	Loss: 2.2514
Profiling... [1536/50176]	Loss: 2.0382
Profiling... [2048/50176]	Loss: 2.1780
Profiling... [2560/50176]	Loss: 2.1869
Profiling... [3072/50176]	Loss: 2.1395
Profiling... [3584/50176]	Loss: 2.1272
Profiling... [4096/50176]	Loss: 2.0300
Profiling... [4608/50176]	Loss: 2.1250
Profiling... [5120/50176]	Loss: 2.2156
Profiling... [5632/50176]	Loss: 2.2772
Profiling... [6144/50176]	Loss: 2.0786
Profiling... [6656/50176]	Loss: 2.0160
Profile done
epoch 1 train time consumed: 6.89s
Validation Epoch: 10, Average loss: 0.0072, Accuracy: 0.2735
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.98065170779806,
                        "time": 4.549170053000125,
                        "accuracy": 0.27353515625,
                        "total_cost": 1456204.0825285278
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.6618
Profiling... [1024/50176]	Loss: 2.2634
Profiling... [1536/50176]	Loss: 2.0551
Profiling... [2048/50176]	Loss: 2.0960
Profiling... [2560/50176]	Loss: 2.1154
Profiling... [3072/50176]	Loss: 2.1503
Profiling... [3584/50176]	Loss: 2.1827
Profiling... [4096/50176]	Loss: 2.2250
Profiling... [4608/50176]	Loss: 2.0586
Profiling... [5120/50176]	Loss: 2.1392
Profiling... [5632/50176]	Loss: 2.1297
Profiling... [6144/50176]	Loss: 2.1752
Profiling... [6656/50176]	Loss: 2.0991
Profile done
epoch 1 train time consumed: 7.03s
Validation Epoch: 10, Average loss: 0.0081, Accuracy: 0.2278
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.00418672846413,
                        "time": 4.5900058409997655,
                        "accuracy": 0.22783203125,
                        "total_cost": 1764012.8292696904
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.6824
Profiling... [1024/50176]	Loss: 2.0635
Profiling... [1536/50176]	Loss: 2.1153
Profiling... [2048/50176]	Loss: 2.1298
Profiling... [2560/50176]	Loss: 2.2097
Profiling... [3072/50176]	Loss: 2.1408
Profiling... [3584/50176]	Loss: 2.1272
Profiling... [4096/50176]	Loss: 2.1635
Profiling... [4608/50176]	Loss: 2.1597
Profiling... [5120/50176]	Loss: 2.1754
Profiling... [5632/50176]	Loss: 2.2214
Profiling... [6144/50176]	Loss: 2.2061
Profiling... [6656/50176]	Loss: 2.1646
Profile done
epoch 1 train time consumed: 7.44s
Validation Epoch: 10, Average loss: 0.0096, Accuracy: 0.1690
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.0098654330185,
                        "time": 5.01182911900014,
                        "accuracy": 0.16904296875,
                        "total_cost": 2595986.5690481435
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.6564
Profiling... [1024/50176]	Loss: 2.1914
Profiling... [1536/50176]	Loss: 2.2273
Profiling... [2048/50176]	Loss: 2.1297
Profiling... [2560/50176]	Loss: 1.9828
Profiling... [3072/50176]	Loss: 2.1789
Profiling... [3584/50176]	Loss: 2.2454
Profiling... [4096/50176]	Loss: 2.1905
Profiling... [4608/50176]	Loss: 2.3613
Profiling... [5120/50176]	Loss: 2.0359
Profiling... [5632/50176]	Loss: 2.0518
Profiling... [6144/50176]	Loss: 2.0560
Profiling... [6656/50176]	Loss: 2.1599
Profile done
epoch 1 train time consumed: 12.51s
Validation Epoch: 10, Average loss: 0.0088, Accuracy: 0.2142
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.97847100830857,
                        "time": 8.283584197999517,
                        "accuracy": 0.21416015625,
                        "total_cost": 3386747.629047506
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.6982
Profiling... [2048/50176]	Loss: 1.6442
Profiling... [3072/50176]	Loss: 1.5557
Profiling... [4096/50176]	Loss: 1.6415
Profiling... [5120/50176]	Loss: 1.5720
Profiling... [6144/50176]	Loss: 1.6552
Profiling... [7168/50176]	Loss: 1.6652
Profiling... [8192/50176]	Loss: 1.6196
Profiling... [9216/50176]	Loss: 1.5982
Profiling... [10240/50176]	Loss: 1.6140
Profiling... [11264/50176]	Loss: 1.6356
Profiling... [12288/50176]	Loss: 1.6445
Profiling... [13312/50176]	Loss: 1.6724
Profile done
epoch 1 train time consumed: 12.97s
Validation Epoch: 10, Average loss: 0.0017, Accuracy: 0.5043
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.9940117918514,
                        "time": 8.900835389999884,
                        "accuracy": 0.504296875,
                        "total_cost": 1545424.3488632909
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.5987
Profiling... [2048/50176]	Loss: 1.6328
Profiling... [3072/50176]	Loss: 1.6544
Profiling... [4096/50176]	Loss: 1.6133
Profiling... [5120/50176]	Loss: 1.6197
Profiling... [6144/50176]	Loss: 1.6487
Profiling... [7168/50176]	Loss: 1.5913
Profiling... [8192/50176]	Loss: 1.6279
Profiling... [9216/50176]	Loss: 1.5804
Profiling... [10240/50176]	Loss: 1.5787
Profiling... [11264/50176]	Loss: 1.5570
Profiling... [12288/50176]	Loss: 1.6356
Profiling... [13312/50176]	Loss: 1.7002
Profile done
epoch 1 train time consumed: 13.18s
Validation Epoch: 10, Average loss: 0.0017, Accuracy: 0.5068
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.03457184829064,
                        "time": 8.999854986999708,
                        "accuracy": 0.5068359375,
                        "total_cost": 1554789.035268462
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.6737
Profiling... [2048/50176]	Loss: 1.6339
Profiling... [3072/50176]	Loss: 1.7152
Profiling... [4096/50176]	Loss: 1.6610
Profiling... [5120/50176]	Loss: 1.7280
Profiling... [6144/50176]	Loss: 1.6586
Profiling... [7168/50176]	Loss: 1.5769
Profiling... [8192/50176]	Loss: 1.6094
Profiling... [9216/50176]	Loss: 1.6260
Profiling... [10240/50176]	Loss: 1.6086
Profiling... [11264/50176]	Loss: 1.6652
Profiling... [12288/50176]	Loss: 1.5858
Profiling... [13312/50176]	Loss: 1.6599
Profile done
epoch 1 train time consumed: 14.21s
Validation Epoch: 10, Average loss: 0.0017, Accuracy: 0.5064
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.0373443098184,
                        "time": 9.903764292999767,
                        "accuracy": 0.5064453125,
                        "total_cost": 1712265.4966572213
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.7125
Profiling... [2048/50176]	Loss: 1.5748
Profiling... [3072/50176]	Loss: 1.6928
Profiling... [4096/50176]	Loss: 1.6501
Profiling... [5120/50176]	Loss: 1.5626
Profiling... [6144/50176]	Loss: 1.6905
Profiling... [7168/50176]	Loss: 1.5692
Profiling... [8192/50176]	Loss: 1.6564
Profiling... [9216/50176]	Loss: 1.6027
Profiling... [10240/50176]	Loss: 1.6154
Profiling... [11264/50176]	Loss: 1.6642
Profiling... [12288/50176]	Loss: 1.7296
Profiling... [13312/50176]	Loss: 1.6027
Profile done
epoch 1 train time consumed: 25.51s
Validation Epoch: 10, Average loss: 0.0018, Accuracy: 0.5007
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.94502404595782,
                        "time": 18.64093120800044,
                        "accuracy": 0.50068359375,
                        "total_cost": 3259923.2810500404
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.6326
Profiling... [2048/50176]	Loss: 1.7061
Profiling... [3072/50176]	Loss: 1.6208
Profiling... [4096/50176]	Loss: 1.6681
Profiling... [5120/50176]	Loss: 1.6275
Profiling... [6144/50176]	Loss: 1.5758
Profiling... [7168/50176]	Loss: 1.6366
Profiling... [8192/50176]	Loss: 1.6058
Profiling... [9216/50176]	Loss: 1.6668
Profiling... [10240/50176]	Loss: 1.6465
Profiling... [11264/50176]	Loss: 1.5840
Profiling... [12288/50176]	Loss: 1.6841
Profiling... [13312/50176]	Loss: 1.5457
Profile done
epoch 1 train time consumed: 12.93s
Validation Epoch: 10, Average loss: 0.0017, Accuracy: 0.5095
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.96134242552004,
                        "time": 8.870491208999738,
                        "accuracy": 0.50947265625,
                        "total_cost": 1524508.90941917
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.6027
Profiling... [2048/50176]	Loss: 1.6766
Profiling... [3072/50176]	Loss: 1.5966
Profiling... [4096/50176]	Loss: 1.6451
Profiling... [5120/50176]	Loss: 1.5824
Profiling... [6144/50176]	Loss: 1.6316
Profiling... [7168/50176]	Loss: 1.6277
Profiling... [8192/50176]	Loss: 1.6172
Profiling... [9216/50176]	Loss: 1.6249
Profiling... [10240/50176]	Loss: 1.5918
Profiling... [11264/50176]	Loss: 1.6261
Profiling... [12288/50176]	Loss: 1.6945
Profiling... [13312/50176]	Loss: 1.5911
Profile done
epoch 1 train time consumed: 13.16s
Validation Epoch: 10, Average loss: 0.0017, Accuracy: 0.5079
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.99897639890307,
                        "time": 8.986555869999393,
                        "accuracy": 0.50791015625,
                        "total_cost": 1549207.717974014
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.6817
Profiling... [2048/50176]	Loss: 1.6328
Profiling... [3072/50176]	Loss: 1.7266
Profiling... [4096/50176]	Loss: 1.5943
Profiling... [5120/50176]	Loss: 1.5784
Profiling... [6144/50176]	Loss: 1.5899
Profiling... [7168/50176]	Loss: 1.5973
Profiling... [8192/50176]	Loss: 1.6028
Profiling... [9216/50176]	Loss: 1.6035
Profiling... [10240/50176]	Loss: 1.6541
Profiling... [11264/50176]	Loss: 1.6352
Profiling... [12288/50176]	Loss: 1.5430
Profiling... [13312/50176]	Loss: 1.7605
Profile done
epoch 1 train time consumed: 14.23s
Validation Epoch: 10, Average loss: 0.0018, Accuracy: 0.5015
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.00145671618631,
                        "time": 9.914213191000272,
                        "accuracy": 0.50146484375,
                        "total_cost": 1731095.5452567271
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.6537
Profiling... [2048/50176]	Loss: 1.5997
Profiling... [3072/50176]	Loss: 1.5507
Profiling... [4096/50176]	Loss: 1.6848
Profiling... [5120/50176]	Loss: 1.5923
Profiling... [6144/50176]	Loss: 1.6268
Profiling... [7168/50176]	Loss: 1.5589
Profiling... [8192/50176]	Loss: 1.5766
Profiling... [9216/50176]	Loss: 1.7296
Profiling... [10240/50176]	Loss: 1.6471
Profiling... [11264/50176]	Loss: 1.6639
Profiling... [12288/50176]	Loss: 1.6009
Profiling... [13312/50176]	Loss: 1.6420
Profile done
epoch 1 train time consumed: 25.58s
Validation Epoch: 10, Average loss: 0.0017, Accuracy: 0.5041
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.90751062417183,
                        "time": 18.72183826699984,
                        "accuracy": 0.5041015625,
                        "total_cost": 3251872.3485484226
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.6686
Profiling... [2048/50176]	Loss: 1.6749
Profiling... [3072/50176]	Loss: 1.6869
Profiling... [4096/50176]	Loss: 1.6425
Profiling... [5120/50176]	Loss: 1.6206
Profiling... [6144/50176]	Loss: 1.6095
Profiling... [7168/50176]	Loss: 1.7008
Profiling... [8192/50176]	Loss: 1.6602
Profiling... [9216/50176]	Loss: 1.5190
Profiling... [10240/50176]	Loss: 1.6628
Profiling... [11264/50176]	Loss: 1.5937
Profiling... [12288/50176]	Loss: 1.6602
Profiling... [13312/50176]	Loss: 1.6310
Profile done
epoch 1 train time consumed: 13.05s
Validation Epoch: 10, Average loss: 0.0017, Accuracy: 0.4998
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.92347015714728,
                        "time": 8.880304626999532,
                        "accuracy": 0.4998046875,
                        "total_cost": 1555717.088354852
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.6683
Profiling... [2048/50176]	Loss: 1.5993
Profiling... [3072/50176]	Loss: 1.6489
Profiling... [4096/50176]	Loss: 1.6141
Profiling... [5120/50176]	Loss: 1.6469
Profiling... [6144/50176]	Loss: 1.5967
Profiling... [7168/50176]	Loss: 1.6324
Profiling... [8192/50176]	Loss: 1.6383
Profiling... [9216/50176]	Loss: 1.6235
Profiling... [10240/50176]	Loss: 1.5917
Profiling... [11264/50176]	Loss: 1.6318
Profiling... [12288/50176]	Loss: 1.6163
Profiling... [13312/50176]	Loss: 1.6997
Profile done
epoch 1 train time consumed: 13.06s
Validation Epoch: 10, Average loss: 0.0017, Accuracy: 0.5059
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.96306911582408,
                        "time": 8.989971988999969,
                        "accuracy": 0.505859375,
                        "total_cost": 1556079.2688027886
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.5428
Profiling... [2048/50176]	Loss: 1.6714
Profiling... [3072/50176]	Loss: 1.5991
Profiling... [4096/50176]	Loss: 1.7368
Profiling... [5120/50176]	Loss: 1.5763
Profiling... [6144/50176]	Loss: 1.5827
Profiling... [7168/50176]	Loss: 1.6456
Profiling... [8192/50176]	Loss: 1.6496
Profiling... [9216/50176]	Loss: 1.5791
Profiling... [10240/50176]	Loss: 1.6471
Profiling... [11264/50176]	Loss: 1.6001
Profiling... [12288/50176]	Loss: 1.5518
Profiling... [13312/50176]	Loss: 1.6714
Profile done
epoch 1 train time consumed: 14.34s
Validation Epoch: 10, Average loss: 0.0017, Accuracy: 0.5031
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.9661212126113,
                        "time": 9.902766922999945,
                        "accuracy": 0.503125,
                        "total_cost": 1723391.1108522653
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.7170
Profiling... [2048/50176]	Loss: 1.5944
Profiling... [3072/50176]	Loss: 1.5884
Profiling... [4096/50176]	Loss: 1.6177
Profiling... [5120/50176]	Loss: 1.5587
Profiling... [6144/50176]	Loss: 1.6865
Profiling... [7168/50176]	Loss: 1.6380
Profiling... [8192/50176]	Loss: 1.6712
Profiling... [9216/50176]	Loss: 1.5559
Profiling... [10240/50176]	Loss: 1.6601
Profiling... [11264/50176]	Loss: 1.6162
Profiling... [12288/50176]	Loss: 1.5602
Profiling... [13312/50176]	Loss: 1.6776
Profile done
epoch 1 train time consumed: 25.54s
Validation Epoch: 10, Average loss: 0.0017, Accuracy: 0.5077
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.87512059150072,
                        "time": 18.667319479999605,
                        "accuracy": 0.50771484375,
                        "total_cost": 3219326.782637755
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.6702
Profiling... [2048/50176]	Loss: 1.7262
Profiling... [3072/50176]	Loss: 1.8168
Profiling... [4096/50176]	Loss: 1.8351
Profiling... [5120/50176]	Loss: 1.6724
Profiling... [6144/50176]	Loss: 1.7683
Profiling... [7168/50176]	Loss: 1.7777
Profiling... [8192/50176]	Loss: 1.7993
Profiling... [9216/50176]	Loss: 1.9132
Profiling... [10240/50176]	Loss: 1.7721
Profiling... [11264/50176]	Loss: 1.7561
Profiling... [12288/50176]	Loss: 1.8203
Profiling... [13312/50176]	Loss: 1.8574
Profile done
epoch 1 train time consumed: 12.99s
Validation Epoch: 10, Average loss: 0.0021, Accuracy: 0.4471
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.89217170509274,
                        "time": 8.885308936998626,
                        "accuracy": 0.4470703125,
                        "total_cost": 1740202.171946443
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.6644
Profiling... [2048/50176]	Loss: 1.7683
Profiling... [3072/50176]	Loss: 1.7418
Profiling... [4096/50176]	Loss: 1.8307
Profiling... [5120/50176]	Loss: 1.7461
Profiling... [6144/50176]	Loss: 1.7836
Profiling... [7168/50176]	Loss: 1.8003
Profiling... [8192/50176]	Loss: 1.7901
Profiling... [9216/50176]	Loss: 1.8367
Profiling... [10240/50176]	Loss: 1.7141
Profiling... [11264/50176]	Loss: 1.9149
Profiling... [12288/50176]	Loss: 1.8225
Profiling... [13312/50176]	Loss: 1.6702
Profile done
epoch 1 train time consumed: 13.08s
Validation Epoch: 10, Average loss: 0.0019, Accuracy: 0.4597
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.93036289549266,
                        "time": 8.99363083499884,
                        "accuracy": 0.45966796875,
                        "total_cost": 1713144.1812082443
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.7387
Profiling... [2048/50176]	Loss: 1.7552
Profiling... [3072/50176]	Loss: 1.7909
Profiling... [4096/50176]	Loss: 1.7760
Profiling... [5120/50176]	Loss: 1.8213
Profiling... [6144/50176]	Loss: 1.6732
Profiling... [7168/50176]	Loss: 1.7760
Profiling... [8192/50176]	Loss: 1.7644
Profiling... [9216/50176]	Loss: 1.8571
Profiling... [10240/50176]	Loss: 1.7926
Profiling... [11264/50176]	Loss: 1.8302
Profiling... [12288/50176]	Loss: 1.7599
Profiling... [13312/50176]	Loss: 1.6959
Profile done
epoch 1 train time consumed: 14.38s
Validation Epoch: 10, Average loss: 0.0020, Accuracy: 0.4433
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.93269377819834,
                        "time": 9.92408837900075,
                        "accuracy": 0.44326171875,
                        "total_cost": 1960349.4858412459
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.6716
Profiling... [2048/50176]	Loss: 1.8079
Profiling... [3072/50176]	Loss: 1.7749
Profiling... [4096/50176]	Loss: 1.8635
Profiling... [5120/50176]	Loss: 1.7620
Profiling... [6144/50176]	Loss: 1.7937
Profiling... [7168/50176]	Loss: 1.7311
Profiling... [8192/50176]	Loss: 1.7825
Profiling... [9216/50176]	Loss: 1.7651
Profiling... [10240/50176]	Loss: 1.7571
Profiling... [11264/50176]	Loss: 1.7824
Profiling... [12288/50176]	Loss: 1.7811
Profiling... [13312/50176]	Loss: 1.7674
Profile done
epoch 1 train time consumed: 25.62s
Validation Epoch: 10, Average loss: 0.0022, Accuracy: 0.4184
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.84211081787996,
                        "time": 18.714657294000062,
                        "accuracy": 0.418359375,
                        "total_cost": 3916834.7976259203
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.6398
Profiling... [2048/50176]	Loss: 1.7721
Profiling... [3072/50176]	Loss: 1.6989
Profiling... [4096/50176]	Loss: 1.6749
Profiling... [5120/50176]	Loss: 1.7632
Profiling... [6144/50176]	Loss: 1.7976
Profiling... [7168/50176]	Loss: 1.8044
Profiling... [8192/50176]	Loss: 1.7729
Profiling... [9216/50176]	Loss: 1.8120
Profiling... [10240/50176]	Loss: 1.7843
Profiling... [11264/50176]	Loss: 1.7685
Profiling... [12288/50176]	Loss: 1.7751
Profiling... [13312/50176]	Loss: 1.7595
Profile done
epoch 1 train time consumed: 12.99s
Validation Epoch: 10, Average loss: 0.0020, Accuracy: 0.4503
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.85677885824494,
                        "time": 8.91512142099964,
                        "accuracy": 0.45029296875,
                        "total_cost": 1733544.5805694333
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7174
Profiling... [2048/50176]	Loss: 1.7212
Profiling... [3072/50176]	Loss: 1.7578
Profiling... [4096/50176]	Loss: 1.6995
Profiling... [5120/50176]	Loss: 1.7621
Profiling... [6144/50176]	Loss: 1.7131
Profiling... [7168/50176]	Loss: 1.8218
Profiling... [8192/50176]	Loss: 1.8813
Profiling... [9216/50176]	Loss: 1.8854
Profiling... [10240/50176]	Loss: 1.7826
Profiling... [11264/50176]	Loss: 1.7835
Profiling... [12288/50176]	Loss: 1.7804
Profiling... [13312/50176]	Loss: 1.8667
Profile done
epoch 1 train time consumed: 13.16s
Validation Epoch: 10, Average loss: 0.0019, Accuracy: 0.4623
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.8948640679464,
                        "time": 8.988696582000557,
                        "accuracy": 0.4623046875,
                        "total_cost": 1702438.515409129
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.6336
Profiling... [2048/50176]	Loss: 1.7028
Profiling... [3072/50176]	Loss: 1.7090
Profiling... [4096/50176]	Loss: 1.7176
Profiling... [5120/50176]	Loss: 1.7173
Profiling... [6144/50176]	Loss: 1.8850
Profiling... [7168/50176]	Loss: 1.8349
Profiling... [8192/50176]	Loss: 1.8973
Profiling... [9216/50176]	Loss: 1.7138
Profiling... [10240/50176]	Loss: 1.7683
Profiling... [11264/50176]	Loss: 1.7542
Profiling... [12288/50176]	Loss: 1.8175
Profiling... [13312/50176]	Loss: 1.7977
Profile done
epoch 1 train time consumed: 14.29s
Validation Epoch: 10, Average loss: 0.0020, Accuracy: 0.4567
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.89733559817606,
                        "time": 9.915042403999905,
                        "accuracy": 0.45673828125,
                        "total_cost": 1900772.6789094952
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.5861
Profiling... [2048/50176]	Loss: 1.6978
Profiling... [3072/50176]	Loss: 1.7312
Profiling... [4096/50176]	Loss: 1.7265
Profiling... [5120/50176]	Loss: 1.8989
Profiling... [6144/50176]	Loss: 1.8683
Profiling... [7168/50176]	Loss: 1.7607
Profiling... [8192/50176]	Loss: 1.7309
Profiling... [9216/50176]	Loss: 1.8190
Profiling... [10240/50176]	Loss: 1.6955
Profiling... [11264/50176]	Loss: 1.7498
Profiling... [12288/50176]	Loss: 1.7669
Profiling... [13312/50176]	Loss: 1.7234
Profile done
epoch 1 train time consumed: 25.66s
Validation Epoch: 10, Average loss: 0.0020, Accuracy: 0.4431
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.80733485681297,
                        "time": 18.749161412999456,
                        "accuracy": 0.44306640625,
                        "total_cost": 3705235.5345132686
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.7215
Profiling... [2048/50176]	Loss: 1.7111
Profiling... [3072/50176]	Loss: 1.8063
Profiling... [4096/50176]	Loss: 1.7089
Profiling... [5120/50176]	Loss: 1.6991
Profiling... [6144/50176]	Loss: 1.7282
Profiling... [7168/50176]	Loss: 1.7426
Profiling... [8192/50176]	Loss: 1.7551
Profiling... [9216/50176]	Loss: 1.7551
Profiling... [10240/50176]	Loss: 1.8955
Profiling... [11264/50176]	Loss: 1.8301
Profiling... [12288/50176]	Loss: 1.7929
Profiling... [13312/50176]	Loss: 1.8201
Profile done
epoch 1 train time consumed: 13.10s
Validation Epoch: 10, Average loss: 0.0019, Accuracy: 0.4595
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.82284208147696,
                        "time": 8.899528422000003,
                        "accuracy": 0.45947265625,
                        "total_cost": 1695938.7244388252
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.6525
Profiling... [2048/50176]	Loss: 1.7879
Profiling... [3072/50176]	Loss: 1.7055
Profiling... [4096/50176]	Loss: 1.7218
Profiling... [5120/50176]	Loss: 1.7675
Profiling... [6144/50176]	Loss: 1.8206
Profiling... [7168/50176]	Loss: 1.7593
Profiling... [8192/50176]	Loss: 1.7961
Profiling... [9216/50176]	Loss: 1.8312
Profiling... [10240/50176]	Loss: 1.8462
Profiling... [11264/50176]	Loss: 1.8368
Profiling... [12288/50176]	Loss: 1.7213
Profiling... [13312/50176]	Loss: 1.7533
Profile done
epoch 1 train time consumed: 13.19s
Validation Epoch: 10, Average loss: 0.0020, Accuracy: 0.4580
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.8594472726801,
                        "time": 9.02316630399946,
                        "accuracy": 0.4580078125,
                        "total_cost": 1724999.6055028518
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.6625
Profiling... [2048/50176]	Loss: 1.7584
Profiling... [3072/50176]	Loss: 1.8032
Profiling... [4096/50176]	Loss: 1.8327
Profiling... [5120/50176]	Loss: 1.7991
Profiling... [6144/50176]	Loss: 1.7970
Profiling... [7168/50176]	Loss: 1.6866
Profiling... [8192/50176]	Loss: 1.8669
Profiling... [9216/50176]	Loss: 1.8192
Profiling... [10240/50176]	Loss: 1.7378
Profiling... [11264/50176]	Loss: 1.8106
Profiling... [12288/50176]	Loss: 1.7851
Profiling... [13312/50176]	Loss: 1.8492
Profile done
epoch 1 train time consumed: 14.41s
Validation Epoch: 10, Average loss: 0.0021, Accuracy: 0.4359
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.86188525393797,
                        "time": 9.9343043079989,
                        "accuracy": 0.4359375,
                        "total_cost": 1995336.5609044218
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.6612
Profiling... [2048/50176]	Loss: 1.8006
Profiling... [3072/50176]	Loss: 1.8170
Profiling... [4096/50176]	Loss: 1.8546
Profiling... [5120/50176]	Loss: 1.7010
Profiling... [6144/50176]	Loss: 1.7824
Profiling... [7168/50176]	Loss: 1.7614
Profiling... [8192/50176]	Loss: 1.8227
Profiling... [9216/50176]	Loss: 1.8424
Profiling... [10240/50176]	Loss: 1.8220
Profiling... [11264/50176]	Loss: 1.7243
Profiling... [12288/50176]	Loss: 1.9258
Profiling... [13312/50176]	Loss: 1.7372
Profile done
epoch 1 train time consumed: 26.31s
Validation Epoch: 10, Average loss: 0.0020, Accuracy: 0.4427
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.77258339062621,
                        "time": 18.628056544001083,
                        "accuracy": 0.44267578125,
                        "total_cost": 3684550.336578264
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.6248
Profiling... [2048/50176]	Loss: 2.1711
Profiling... [3072/50176]	Loss: 2.0323
Profiling... [4096/50176]	Loss: 2.1050
Profiling... [5120/50176]	Loss: 2.0852
Profiling... [6144/50176]	Loss: 2.0174
Profiling... [7168/50176]	Loss: 2.0006
Profiling... [8192/50176]	Loss: 1.9404
Profiling... [9216/50176]	Loss: 2.0364
Profiling... [10240/50176]	Loss: 1.9734
Profiling... [11264/50176]	Loss: 2.0577
Profiling... [12288/50176]	Loss: 2.0550
Profiling... [13312/50176]	Loss: 2.0730
Profile done
epoch 1 train time consumed: 12.96s
Validation Epoch: 10, Average loss: 0.0028, Accuracy: 0.3213
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.78824033517772,
                        "time": 8.901282807000825,
                        "accuracy": 0.3212890625,
                        "total_cost": 2425824.655868131
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.5221
Profiling... [2048/50176]	Loss: 2.2581
Profiling... [3072/50176]	Loss: 1.8722
Profiling... [4096/50176]	Loss: 1.9849
Profiling... [5120/50176]	Loss: 2.1121
Profiling... [6144/50176]	Loss: 2.0371
Profiling... [7168/50176]	Loss: 2.0301
Profiling... [8192/50176]	Loss: 2.0587
Profiling... [9216/50176]	Loss: 2.0111
Profiling... [10240/50176]	Loss: 2.0300
Profiling... [11264/50176]	Loss: 1.9022
Profiling... [12288/50176]	Loss: 1.9242
Profiling... [13312/50176]	Loss: 2.0054
Profile done
epoch 1 train time consumed: 13.26s
Validation Epoch: 10, Average loss: 0.0024, Accuracy: 0.3720
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.82510214264747,
                        "time": 9.011260941000728,
                        "accuracy": 0.37197265625,
                        "total_cost": 2121179.3423012877
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.6035
Profiling... [2048/50176]	Loss: 2.1936
Profiling... [3072/50176]	Loss: 1.9094
Profiling... [4096/50176]	Loss: 1.9240
Profiling... [5120/50176]	Loss: 1.9927
Profiling... [6144/50176]	Loss: 2.0357
Profiling... [7168/50176]	Loss: 2.0413
Profiling... [8192/50176]	Loss: 1.9995
Profiling... [9216/50176]	Loss: 1.9873
Profiling... [10240/50176]	Loss: 1.9241
Profiling... [11264/50176]	Loss: 2.0512
Profiling... [12288/50176]	Loss: 1.9868
Profiling... [13312/50176]	Loss: 2.0020
Profile done
epoch 1 train time consumed: 14.34s
Validation Epoch: 10, Average loss: 0.0032, Accuracy: 0.3029
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.82695386252759,
                        "time": 9.916320567999719,
                        "accuracy": 0.3029296875,
                        "total_cost": 2866233.4812704194
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.6453
Profiling... [2048/50176]	Loss: 2.1004
Profiling... [3072/50176]	Loss: 1.9171
Profiling... [4096/50176]	Loss: 1.9813
Profiling... [5120/50176]	Loss: 2.0343
Profiling... [6144/50176]	Loss: 2.0672
Profiling... [7168/50176]	Loss: 2.0433
Profiling... [8192/50176]	Loss: 2.0036
Profiling... [9216/50176]	Loss: 2.0116
Profiling... [10240/50176]	Loss: 2.0167
Profiling... [11264/50176]	Loss: 2.0613
Profiling... [12288/50176]	Loss: 2.0018
Profiling... [13312/50176]	Loss: 2.0239
Profile done
epoch 1 train time consumed: 25.53s
Validation Epoch: 10, Average loss: 0.0043, Accuracy: 0.2053
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.74171916875352,
                        "time": 18.622228938998887,
                        "accuracy": 0.2052734375,
                        "total_cost": 7943310.492385983
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.6302
Profiling... [2048/50176]	Loss: 2.2144
Profiling... [3072/50176]	Loss: 2.0267
Profiling... [4096/50176]	Loss: 1.8961
Profiling... [5120/50176]	Loss: 2.0588
Profiling... [6144/50176]	Loss: 2.0390
Profiling... [7168/50176]	Loss: 2.0008
Profiling... [8192/50176]	Loss: 2.0857
Profiling... [9216/50176]	Loss: 1.9908
Profiling... [10240/50176]	Loss: 2.0183
Profiling... [11264/50176]	Loss: 1.9961
Profiling... [12288/50176]	Loss: 1.9210
Profiling... [13312/50176]	Loss: 1.9476
Profile done
epoch 1 train time consumed: 13.15s
Validation Epoch: 10, Average loss: 0.0036, Accuracy: 0.3233
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.75782980508268,
                        "time": 8.89854447899961,
                        "accuracy": 0.32333984375,
                        "total_cost": 2409696.926897339
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.6390
Profiling... [2048/50176]	Loss: 2.2289
Profiling... [3072/50176]	Loss: 1.9540
Profiling... [4096/50176]	Loss: 2.0449
Profiling... [5120/50176]	Loss: 2.0407
Profiling... [6144/50176]	Loss: 2.1543
Profiling... [7168/50176]	Loss: 2.1513
Profiling... [8192/50176]	Loss: 2.0311
Profiling... [9216/50176]	Loss: 2.1112
Profiling... [10240/50176]	Loss: 2.0087
Profiling... [11264/50176]	Loss: 2.0097
Profiling... [12288/50176]	Loss: 1.9844
Profiling... [13312/50176]	Loss: 2.0486
Profile done
epoch 1 train time consumed: 13.10s
Validation Epoch: 10, Average loss: 0.0031, Accuracy: 0.2988
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.7952915474475,
                        "time": 8.993040069000926,
                        "accuracy": 0.298828125,
                        "total_cost": 2635043.7109827884
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.6099
Profiling... [2048/50176]	Loss: 2.1443
Profiling... [3072/50176]	Loss: 2.0038
Profiling... [4096/50176]	Loss: 1.8733
Profiling... [5120/50176]	Loss: 2.0170
Profiling... [6144/50176]	Loss: 2.0679
Profiling... [7168/50176]	Loss: 2.1037
Profiling... [8192/50176]	Loss: 1.9683
Profiling... [9216/50176]	Loss: 1.9655
Profiling... [10240/50176]	Loss: 1.8952
Profiling... [11264/50176]	Loss: 1.9671
Profiling... [12288/50176]	Loss: 2.0917
Profiling... [13312/50176]	Loss: 1.9307
Profile done
epoch 1 train time consumed: 14.43s
Validation Epoch: 10, Average loss: 0.0047, Accuracy: 0.1989
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.79879811967045,
                        "time": 10.009375978999742,
                        "accuracy": 0.19892578125,
                        "total_cost": 4405738.379276034
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.6178
Profiling... [2048/50176]	Loss: 2.0629
Profiling... [3072/50176]	Loss: 1.9933
Profiling... [4096/50176]	Loss: 1.9644
Profiling... [5120/50176]	Loss: 1.9927
Profiling... [6144/50176]	Loss: 2.0122
Profiling... [7168/50176]	Loss: 1.9793
Profiling... [8192/50176]	Loss: 1.9983
Profiling... [9216/50176]	Loss: 1.9499
Profiling... [10240/50176]	Loss: 1.9606
Profiling... [11264/50176]	Loss: 2.0181
Profiling... [12288/50176]	Loss: 1.9902
Profiling... [13312/50176]	Loss: 1.9490
Profile done
epoch 1 train time consumed: 25.20s
Validation Epoch: 10, Average loss: 0.0032, Accuracy: 0.2851
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.71639097873965,
                        "time": 18.199205431001246,
                        "accuracy": 0.28505859375,
                        "total_cost": 5590116.495852298
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.6146
Profiling... [2048/50176]	Loss: 2.1145
Profiling... [3072/50176]	Loss: 1.9878
Profiling... [4096/50176]	Loss: 1.9252
Profiling... [5120/50176]	Loss: 2.0566
Profiling... [6144/50176]	Loss: 2.0484
Profiling... [7168/50176]	Loss: 2.1077
Profiling... [8192/50176]	Loss: 2.0238
Profiling... [9216/50176]	Loss: 1.9505
Profiling... [10240/50176]	Loss: 1.9348
Profiling... [11264/50176]	Loss: 2.0109
Profiling... [12288/50176]	Loss: 1.9855
Profiling... [13312/50176]	Loss: 1.9818
Profile done
epoch 1 train time consumed: 13.15s
Validation Epoch: 10, Average loss: 0.0030, Accuracy: 0.2894
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.73284743467521,
                        "time": 9.035704591000467,
                        "accuracy": 0.28935546875,
                        "total_cost": 2734216.7493762802
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7271
Profiling... [2048/50176]	Loss: 2.1786
Profiling... [3072/50176]	Loss: 1.9464
Profiling... [4096/50176]	Loss: 1.9779
Profiling... [5120/50176]	Loss: 2.0935
Profiling... [6144/50176]	Loss: 2.0408
Profiling... [7168/50176]	Loss: 2.0083
Profiling... [8192/50176]	Loss: 1.9858
Profiling... [9216/50176]	Loss: 2.0086
Profiling... [10240/50176]	Loss: 1.9131
Profiling... [11264/50176]	Loss: 2.0689
Profiling... [12288/50176]	Loss: 2.0157
Profiling... [13312/50176]	Loss: 2.0738
Profile done
epoch 1 train time consumed: 13.39s
Validation Epoch: 10, Average loss: 0.0029, Accuracy: 0.3285
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.77135897041899,
                        "time": 9.121836139000152,
                        "accuracy": 0.328515625,
                        "total_cost": 2431246.211195527
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.7526
Profiling... [2048/50176]	Loss: 2.3137
Profiling... [3072/50176]	Loss: 2.0194
Profiling... [4096/50176]	Loss: 2.0866
Profiling... [5120/50176]	Loss: 1.9355
Profiling... [6144/50176]	Loss: 1.9422
Profiling... [7168/50176]	Loss: 2.0405
Profiling... [8192/50176]	Loss: 1.9351
Profiling... [9216/50176]	Loss: 1.9964
Profiling... [10240/50176]	Loss: 1.9736
Profiling... [11264/50176]	Loss: 1.9472
Profiling... [12288/50176]	Loss: 1.9192
Profiling... [13312/50176]	Loss: 2.0542
Profile done
epoch 1 train time consumed: 14.39s
Validation Epoch: 10, Average loss: 0.0028, Accuracy: 0.3487
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.7749056819435,
                        "time": 10.005610743999569,
                        "accuracy": 0.34873046875,
                        "total_cost": 2512212.801412026
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.6709
Profiling... [2048/50176]	Loss: 2.2668
Profiling... [3072/50176]	Loss: 2.0982
Profiling... [4096/50176]	Loss: 1.9959
Profiling... [5120/50176]	Loss: 2.0252
Profiling... [6144/50176]	Loss: 2.0640
Profiling... [7168/50176]	Loss: 1.9542
Profiling... [8192/50176]	Loss: 2.0754
Profiling... [9216/50176]	Loss: 1.9977
Profiling... [10240/50176]	Loss: 2.1339
Profiling... [11264/50176]	Loss: 2.1259
Profiling... [12288/50176]	Loss: 1.9611
Profiling... [13312/50176]	Loss: 2.0996
Profile done
epoch 1 train time consumed: 25.61s
Validation Epoch: 10, Average loss: 0.0028, Accuracy: 0.3258
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.69176568037413,
                        "time": 18.653933204001078,
                        "accuracy": 0.32578125,
                        "total_cost": 5013567.2003362
                    },
                    
[Training Loop] The optimal parameters are lr: 0.001 dr: 0.5 bs: 128 pl: 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[GPU_0] Set GPU power limit to 175W.
[GPU_0] Set GPU power limit to 175W.
Training Epoch: 10 [128/50048]	Loss: 1.7680
Training Epoch: 10 [256/50048]	Loss: 1.7027
Training Epoch: 10 [384/50048]	Loss: 1.7838
Training Epoch: 10 [512/50048]	Loss: 1.8033
Training Epoch: 10 [640/50048]	Loss: 1.9435
Training Epoch: 10 [768/50048]	Loss: 1.7029
Training Epoch: 10 [896/50048]	Loss: 1.8257
Training Epoch: 10 [1024/50048]	Loss: 2.0261
Training Epoch: 10 [1152/50048]	Loss: 1.8242
Training Epoch: 10 [1280/50048]	Loss: 1.9786
Training Epoch: 10 [1408/50048]	Loss: 2.0334
Training Epoch: 10 [1536/50048]	Loss: 1.9616
Training Epoch: 10 [1664/50048]	Loss: 1.9751
Training Epoch: 10 [1792/50048]	Loss: 1.6906
Training Epoch: 10 [1920/50048]	Loss: 1.6637
Training Epoch: 10 [2048/50048]	Loss: 2.0070
Training Epoch: 10 [2176/50048]	Loss: 1.8434
Training Epoch: 10 [2304/50048]	Loss: 1.9532
Training Epoch: 10 [2432/50048]	Loss: 1.8543
Training Epoch: 10 [2560/50048]	Loss: 1.5500
Training Epoch: 10 [2688/50048]	Loss: 2.0060
Training Epoch: 10 [2816/50048]	Loss: 2.0585
Training Epoch: 10 [2944/50048]	Loss: 2.3854
Training Epoch: 10 [3072/50048]	Loss: 1.8973
Training Epoch: 10 [3200/50048]	Loss: 1.9525
Training Epoch: 10 [3328/50048]	Loss: 1.7514
Training Epoch: 10 [3456/50048]	Loss: 1.9626
Training Epoch: 10 [3584/50048]	Loss: 1.8755
Training Epoch: 10 [3712/50048]	Loss: 1.7169
Training Epoch: 10 [3840/50048]	Loss: 1.7817
Training Epoch: 10 [3968/50048]	Loss: 1.6367
Training Epoch: 10 [4096/50048]	Loss: 1.9963
Training Epoch: 10 [4224/50048]	Loss: 1.8254
Training Epoch: 10 [4352/50048]	Loss: 1.8607
Training Epoch: 10 [4480/50048]	Loss: 2.0803
Training Epoch: 10 [4608/50048]	Loss: 1.8397
Training Epoch: 10 [4736/50048]	Loss: 2.1105
Training Epoch: 10 [4864/50048]	Loss: 1.7581
Training Epoch: 10 [4992/50048]	Loss: 1.9032
Training Epoch: 10 [5120/50048]	Loss: 1.8202
Training Epoch: 10 [5248/50048]	Loss: 1.7227
Training Epoch: 10 [5376/50048]	Loss: 2.1359
Training Epoch: 10 [5504/50048]	Loss: 1.9740
Training Epoch: 10 [5632/50048]	Loss: 1.8770
Training Epoch: 10 [5760/50048]	Loss: 2.2196
Training Epoch: 10 [5888/50048]	Loss: 1.9485
Training Epoch: 10 [6016/50048]	Loss: 2.2174
Training Epoch: 10 [6144/50048]	Loss: 1.7952
Training Epoch: 10 [6272/50048]	Loss: 2.0172
Training Epoch: 10 [6400/50048]	Loss: 1.9321
Training Epoch: 10 [6528/50048]	Loss: 1.6675
Training Epoch: 10 [6656/50048]	Loss: 1.5926
Training Epoch: 10 [6784/50048]	Loss: 1.7909
Training Epoch: 10 [6912/50048]	Loss: 1.9092
Training Epoch: 10 [7040/50048]	Loss: 1.9385
Training Epoch: 10 [7168/50048]	Loss: 1.9754
Training Epoch: 10 [7296/50048]	Loss: 1.7966
Training Epoch: 10 [7424/50048]	Loss: 1.9802
Training Epoch: 10 [7552/50048]	Loss: 1.9792
Training Epoch: 10 [7680/50048]	Loss: 1.6138
Training Epoch: 10 [7808/50048]	Loss: 1.9505
Training Epoch: 10 [7936/50048]	Loss: 1.7542
Training Epoch: 10 [8064/50048]	Loss: 1.9149
Training Epoch: 10 [8192/50048]	Loss: 1.8270
Training Epoch: 10 [8320/50048]	Loss: 1.7068
Training Epoch: 10 [8448/50048]	Loss: 1.8113
Training Epoch: 10 [8576/50048]	Loss: 1.9305
Training Epoch: 10 [8704/50048]	Loss: 2.1392
Training Epoch: 10 [8832/50048]	Loss: 1.9172
Training Epoch: 10 [8960/50048]	Loss: 1.7829
Training Epoch: 10 [9088/50048]	Loss: 1.4784
Training Epoch: 10 [9216/50048]	Loss: 2.0811
Training Epoch: 10 [9344/50048]	Loss: 1.6982
Training Epoch: 10 [9472/50048]	Loss: 1.6916
Training Epoch: 10 [9600/50048]	Loss: 1.4558
Training Epoch: 10 [9728/50048]	Loss: 1.7865
Training Epoch: 10 [9856/50048]	Loss: 1.7419
Training Epoch: 10 [9984/50048]	Loss: 1.8531
Training Epoch: 10 [10112/50048]	Loss: 1.7756
Training Epoch: 10 [10240/50048]	Loss: 1.5657
Training Epoch: 10 [10368/50048]	Loss: 2.0840
Training Epoch: 10 [10496/50048]	Loss: 1.7529
Training Epoch: 10 [10624/50048]	Loss: 1.6232
Training Epoch: 10 [10752/50048]	Loss: 1.6963
Training Epoch: 10 [10880/50048]	Loss: 1.7590
Training Epoch: 10 [11008/50048]	Loss: 1.8080
Training Epoch: 10 [11136/50048]	Loss: 1.7076
Training Epoch: 10 [11264/50048]	Loss: 1.6876
Training Epoch: 10 [11392/50048]	Loss: 1.9250
Training Epoch: 10 [11520/50048]	Loss: 2.0986
Training Epoch: 10 [11648/50048]	Loss: 1.9193
Training Epoch: 10 [11776/50048]	Loss: 1.6168
Training Epoch: 10 [11904/50048]	Loss: 1.9091
Training Epoch: 10 [12032/50048]	Loss: 1.7352
Training Epoch: 10 [12160/50048]	Loss: 1.8611
Training Epoch: 10 [12288/50048]	Loss: 1.9296
Training Epoch: 10 [12416/50048]	Loss: 1.8625
Training Epoch: 10 [12544/50048]	Loss: 1.8131
Training Epoch: 10 [12672/50048]	Loss: 1.8928
Training Epoch: 10 [12800/50048]	Loss: 1.7930
Training Epoch: 10 [12928/50048]	Loss: 1.6343
Training Epoch: 10 [13056/50048]	Loss: 1.6828
Training Epoch: 10 [13184/50048]	Loss: 1.6601
Training Epoch: 10 [13312/50048]	Loss: 1.6510
Training Epoch: 10 [13440/50048]	Loss: 1.8174
Training Epoch: 10 [13568/50048]	Loss: 1.8330
Training Epoch: 10 [13696/50048]	Loss: 1.6401
Training Epoch: 10 [13824/50048]	Loss: 1.8814
Training Epoch: 10 [13952/50048]	Loss: 2.0483
Training Epoch: 10 [14080/50048]	Loss: 1.8767
Training Epoch: 10 [14208/50048]	Loss: 1.7965
Training Epoch: 10 [14336/50048]	Loss: 2.0758
Training Epoch: 10 [14464/50048]	Loss: 1.8464
Training Epoch: 10 [14592/50048]	Loss: 1.7527
Training Epoch: 10 [14720/50048]	Loss: 1.9098
Training Epoch: 10 [14848/50048]	Loss: 1.7148
Training Epoch: 10 [14976/50048]	Loss: 1.7559
Training Epoch: 10 [15104/50048]	Loss: 1.7713
Training Epoch: 10 [15232/50048]	Loss: 1.5340
Training Epoch: 10 [15360/50048]	Loss: 1.8070
Training Epoch: 10 [15488/50048]	Loss: 1.7148
Training Epoch: 10 [15616/50048]	Loss: 1.7560
Training Epoch: 10 [15744/50048]	Loss: 1.8707
Training Epoch: 10 [15872/50048]	Loss: 1.6659
Training Epoch: 10 [16000/50048]	Loss: 2.0039
Training Epoch: 10 [16128/50048]	Loss: 1.5647
Training Epoch: 10 [16256/50048]	Loss: 1.6650
Training Epoch: 10 [16384/50048]	Loss: 2.1133
Training Epoch: 10 [16512/50048]	Loss: 1.7577
Training Epoch: 10 [16640/50048]	Loss: 1.6977
Training Epoch: 10 [16768/50048]	Loss: 1.6838
Training Epoch: 10 [16896/50048]	Loss: 1.9739
Training Epoch: 10 [17024/50048]	Loss: 1.8237
Training Epoch: 10 [17152/50048]	Loss: 1.6949
Training Epoch: 10 [17280/50048]	Loss: 1.9391
Training Epoch: 10 [17408/50048]	Loss: 1.8147
Training Epoch: 10 [17536/50048]	Loss: 1.7817
Training Epoch: 10 [17664/50048]	Loss: 2.0428
Training Epoch: 10 [17792/50048]	Loss: 1.8032
Training Epoch: 10 [17920/50048]	Loss: 1.8340
Training Epoch: 10 [18048/50048]	Loss: 1.9429
Training Epoch: 10 [18176/50048]	Loss: 1.6352
Training Epoch: 10 [18304/50048]	Loss: 2.0325
Training Epoch: 10 [18432/50048]	Loss: 1.7572
Training Epoch: 10 [18560/50048]	Loss: 1.7732
Training Epoch: 10 [18688/50048]	Loss: 2.0940
Training Epoch: 10 [18816/50048]	Loss: 1.8644
Training Epoch: 10 [18944/50048]	Loss: 1.6729
Training Epoch: 10 [19072/50048]	Loss: 2.2068
Training Epoch: 10 [19200/50048]	Loss: 1.8113
Training Epoch: 10 [19328/50048]	Loss: 1.8338
Training Epoch: 10 [19456/50048]	Loss: 1.8547
Training Epoch: 10 [19584/50048]	Loss: 1.7989
Training Epoch: 10 [19712/50048]	Loss: 1.7491
Training Epoch: 10 [19840/50048]	Loss: 1.7679
Training Epoch: 10 [19968/50048]	Loss: 1.7213
Training Epoch: 10 [20096/50048]	Loss: 1.5990
Training Epoch: 10 [20224/50048]	Loss: 1.8724
Training Epoch: 10 [20352/50048]	Loss: 1.9256
Training Epoch: 10 [20480/50048]	Loss: 1.8592
Training Epoch: 10 [20608/50048]	Loss: 1.7290
Training Epoch: 10 [20736/50048]	Loss: 1.9084
Training Epoch: 10 [20864/50048]	Loss: 1.7481
Training Epoch: 10 [20992/50048]	Loss: 1.8347
Training Epoch: 10 [21120/50048]	Loss: 1.9876
Training Epoch: 10 [21248/50048]	Loss: 1.7265
Training Epoch: 10 [21376/50048]	Loss: 1.8288
Training Epoch: 10 [21504/50048]	Loss: 1.4796
Training Epoch: 10 [21632/50048]	Loss: 1.8312
Training Epoch: 10 [21760/50048]	Loss: 1.6568
Training Epoch: 10 [21888/50048]	Loss: 1.9582
Training Epoch: 10 [22016/50048]	Loss: 1.7490
Training Epoch: 10 [22144/50048]	Loss: 1.7963
Training Epoch: 10 [22272/50048]	Loss: 1.4980
Training Epoch: 10 [22400/50048]	Loss: 1.8515
Training Epoch: 10 [22528/50048]	Loss: 2.0033
Training Epoch: 10 [22656/50048]	Loss: 1.8643
Training Epoch: 10 [22784/50048]	Loss: 1.6761
Training Epoch: 10 [22912/50048]	Loss: 1.7937
Training Epoch: 10 [23040/50048]	Loss: 1.8886
Training Epoch: 10 [23168/50048]	Loss: 1.8824
Training Epoch: 10 [23296/50048]	Loss: 1.6196
Training Epoch: 10 [23424/50048]	Loss: 1.9235
Training Epoch: 10 [23552/50048]	Loss: 1.9523
Training Epoch: 10 [23680/50048]	Loss: 1.7927
Training Epoch: 10 [23808/50048]	Loss: 1.7513
Training Epoch: 10 [23936/50048]	Loss: 1.9922
Training Epoch: 10 [24064/50048]	Loss: 1.9561
Training Epoch: 10 [24192/50048]	Loss: 1.7314
Training Epoch: 10 [24320/50048]	Loss: 1.9248
Training Epoch: 10 [24448/50048]	Loss: 2.0047
Training Epoch: 10 [24576/50048]	Loss: 1.8701
Training Epoch: 10 [24704/50048]	Loss: 1.8034
Training Epoch: 10 [24832/50048]	Loss: 1.8180
Training Epoch: 10 [24960/50048]	Loss: 1.8272
Training Epoch: 10 [25088/50048]	Loss: 1.7635
Training Epoch: 10 [25216/50048]	Loss: 1.7882
Training Epoch: 10 [25344/50048]	Loss: 2.1976
Training Epoch: 10 [25472/50048]	Loss: 1.8164
Training Epoch: 10 [25600/50048]	Loss: 1.9934
Training Epoch: 10 [25728/50048]	Loss: 1.8074
Training Epoch: 10 [25856/50048]	Loss: 1.6682
Training Epoch: 10 [25984/50048]	Loss: 1.5644
Training Epoch: 10 [26112/50048]	Loss: 2.0117
Training Epoch: 10 [26240/50048]	Loss: 1.9545
Training Epoch: 10 [26368/50048]	Loss: 1.8186
Training Epoch: 10 [26496/50048]	Loss: 1.9703
Training Epoch: 10 [26624/50048]	Loss: 1.7561
Training Epoch: 10 [26752/50048]	Loss: 1.9621
Training Epoch: 10 [26880/50048]	Loss: 1.6464
Training Epoch: 10 [27008/50048]	Loss: 1.6246
Training Epoch: 10 [27136/50048]	Loss: 1.8627
Training Epoch: 10 [27264/50048]	Loss: 1.6916
Training Epoch: 10 [27392/50048]	Loss: 1.9059
Training Epoch: 10 [27520/50048]	Loss: 1.7687
Training Epoch: 10 [27648/50048]	Loss: 1.8863
Training Epoch: 10 [27776/50048]	Loss: 2.0632
Training Epoch: 10 [27904/50048]	Loss: 1.7415
Training Epoch: 10 [28032/50048]	Loss: 1.7583
Training Epoch: 10 [28160/50048]	Loss: 2.0512
Training Epoch: 10 [28288/50048]	Loss: 1.6435
Training Epoch: 10 [28416/50048]	Loss: 1.9687
Training Epoch: 10 [28544/50048]	Loss: 1.8308
Training Epoch: 10 [28672/50048]	Loss: 1.7918
Training Epoch: 10 [28800/50048]	Loss: 1.8511
Training Epoch: 10 [28928/50048]	Loss: 1.8857
Training Epoch: 10 [29056/50048]	Loss: 1.8269
Training Epoch: 10 [29184/50048]	Loss: 1.5124
Training Epoch: 10 [29312/50048]	Loss: 1.9191
Training Epoch: 10 [29440/50048]	Loss: 1.6786
Training Epoch: 10 [29568/50048]	Loss: 1.9723
Training Epoch: 10 [29696/50048]	Loss: 1.9866
Training Epoch: 10 [29824/50048]	Loss: 1.7816
Training Epoch: 10 [29952/50048]	Loss: 1.6446
Training Epoch: 10 [30080/50048]	Loss: 1.8414
Training Epoch: 10 [30208/50048]	Loss: 1.6961
Training Epoch: 10 [30336/50048]	Loss: 1.6195
Training Epoch: 10 [30464/50048]	Loss: 2.0541
Training Epoch: 10 [30592/50048]	Loss: 1.9302
Training Epoch: 10 [30720/50048]	Loss: 1.6213
Training Epoch: 10 [30848/50048]	Loss: 1.8388
Training Epoch: 10 [30976/50048]	Loss: 1.6276
Training Epoch: 10 [31104/50048]	Loss: 1.9197
Training Epoch: 10 [31232/50048]	Loss: 1.6813
Training Epoch: 10 [31360/50048]	Loss: 1.7644
Training Epoch: 10 [31488/50048]	Loss: 1.6697
Training Epoch: 10 [31616/50048]	Loss: 1.9530
Training Epoch: 10 [31744/50048]	Loss: 1.9267
Training Epoch: 10 [31872/50048]	Loss: 1.9185
Training Epoch: 10 [32000/50048]	Loss: 1.8694
Training Epoch: 10 [32128/50048]	Loss: 1.7836
Training Epoch: 10 [32256/50048]	Loss: 2.0646
Training Epoch: 10 [32384/50048]	Loss: 1.8336
Training Epoch: 10 [32512/50048]	Loss: 1.9019
Training Epoch: 10 [32640/50048]	Loss: 1.9017
Training Epoch: 10 [32768/50048]	Loss: 1.9010
Training Epoch: 10 [32896/50048]	Loss: 1.9285
Training Epoch: 10 [33024/50048]	Loss: 1.7530
Training Epoch: 10 [33152/50048]	Loss: 1.8626
Training Epoch: 10 [33280/50048]	Loss: 1.9549
Training Epoch: 10 [33408/50048]	Loss: 1.7091
Training Epoch: 10 [33536/50048]	Loss: 1.6076
Training Epoch: 10 [33664/50048]	Loss: 1.7478
Training Epoch: 10 [33792/50048]	Loss: 1.7812
Training Epoch: 10 [33920/50048]	Loss: 2.1158
Training Epoch: 10 [34048/50048]	Loss: 1.6567
Training Epoch: 10 [34176/50048]	Loss: 1.8195
Training Epoch: 10 [34304/50048]	Loss: 1.9245
Training Epoch: 10 [34432/50048]	Loss: 1.8218
Training Epoch: 10 [34560/50048]	Loss: 1.9821
Training Epoch: 10 [34688/50048]	Loss: 1.6432
Training Epoch: 10 [34816/50048]	Loss: 1.9248
Training Epoch: 10 [34944/50048]	Loss: 1.7589
Training Epoch: 10 [35072/50048]	Loss: 1.8867
Training Epoch: 10 [35200/50048]	Loss: 1.5951
Training Epoch: 10 [35328/50048]	Loss: 1.7018
Training Epoch: 10 [35456/50048]	Loss: 1.8307
Training Epoch: 10 [35584/50048]	Loss: 1.6928
Training Epoch: 10 [35712/50048]	Loss: 1.9185
Training Epoch: 10 [35840/50048]	Loss: 2.0554
Training Epoch: 10 [35968/50048]	Loss: 1.9113
Training Epoch: 10 [36096/50048]	Loss: 1.6393
Training Epoch: 10 [36224/50048]	Loss: 1.8887
Training Epoch: 10 [36352/50048]	Loss: 1.9331
Training Epoch: 10 [36480/50048]	Loss: 2.0194
Training Epoch: 10 [36608/50048]	Loss: 1.6351
Training Epoch: 10 [36736/50048]	Loss: 1.8539
Training Epoch: 10 [36864/50048]	Loss: 2.0588
Training Epoch: 10 [36992/50048]	Loss: 1.7525
Training Epoch: 10 [37120/50048]	Loss: 1.6622
Training Epoch: 10 [37248/50048]	Loss: 1.8829
Training Epoch: 10 [37376/50048]	Loss: 1.9103
Training Epoch: 10 [37504/50048]	Loss: 1.6649
Training Epoch: 10 [37632/50048]	Loss: 1.8354
Training Epoch: 10 [37760/50048]	Loss: 1.8871
Training Epoch: 10 [37888/50048]	Loss: 2.0114
Training Epoch: 10 [38016/50048]	Loss: 1.8352
Training Epoch: 10 [38144/50048]	Loss: 1.8067
Training Epoch: 10 [38272/50048]	Loss: 2.1191
Training Epoch: 10 [38400/50048]	Loss: 1.8245
Training Epoch: 10 [38528/50048]	Loss: 1.5997
Training Epoch: 10 [38656/50048]	Loss: 1.9478
Training Epoch: 10 [38784/50048]	Loss: 1.8369
Training Epoch: 10 [38912/50048]	Loss: 1.7227
Training Epoch: 10 [39040/50048]	Loss: 1.8319
Training Epoch: 10 [39168/50048]	Loss: 1.7936
Training Epoch: 10 [39296/50048]	Loss: 1.9849
Training Epoch: 10 [39424/50048]	Loss: 1.7586
Training Epoch: 10 [39552/50048]	Loss: 1.4487
Training Epoch: 10 [39680/50048]	Loss: 1.9653
Training Epoch: 10 [39808/50048]	Loss: 1.9877
Training Epoch: 10 [39936/50048]	Loss: 1.9963
Training Epoch: 10 [40064/50048]	Loss: 2.1578
Training Epoch: 10 [40192/50048]	Loss: 1.7306
Training Epoch: 10 [40320/50048]	Loss: 1.8967
Training Epoch: 10 [40448/50048]	Loss: 1.7971
Training Epoch: 10 [40576/50048]	Loss: 1.9559
Training Epoch: 10 [40704/50048]	Loss: 2.0131
Training Epoch: 10 [40832/50048]	Loss: 1.8149
Training Epoch: 10 [40960/50048]	Loss: 1.7208
Training Epoch: 10 [41088/50048]	Loss: 1.5637
Training Epoch: 10 [41216/50048]	Loss: 1.7407
Training Epoch: 10 [41344/50048]	Loss: 1.8768
Training Epoch: 10 [41472/50048]	Loss: 1.7790
Training Epoch: 10 [41600/50048]	Loss: 1.7507
Training Epoch: 10 [41728/50048]	Loss: 1.7121
Training Epoch: 10 [41856/50048]	Loss: 1.7390
Training Epoch: 10 [41984/50048]	Loss: 1.8215
Training Epoch: 10 [42112/50048]	Loss: 1.6709
Training Epoch: 10 [42240/50048]	Loss: 1.8772
Training Epoch: 10 [42368/50048]	Loss: 1.7902
Training Epoch: 10 [42496/50048]	Loss: 1.7711
Training Epoch: 10 [42624/50048]	Loss: 1.7428
Training Epoch: 10 [42752/50048]	Loss: 1.8278
Training Epoch: 10 [42880/50048]	Loss: 1.7463
Training Epoch: 10 [43008/50048]	Loss: 1.7300
Training Epoch: 10 [43136/50048]	Loss: 1.9015
Training Epoch: 10 [43264/50048]	Loss: 1.7350
Training Epoch: 10 [43392/50048]	Loss: 1.7894
Training Epoch: 10 [43520/50048]	Loss: 1.9507
Training Epoch: 10 [43648/50048]	Loss: 1.8315
Training Epoch: 10 [43776/50048]	Loss: 1.7242
Training Epoch: 10 [43904/50048]	Loss: 1.8031
Training Epoch: 10 [44032/50048]	Loss: 1.6392
Training Epoch: 10 [44160/50048]	Loss: 1.8344
Training Epoch: 10 [44288/50048]	Loss: 1.7877
Training Epoch: 10 [44416/50048]	Loss: 1.8336
Training Epoch: 10 [44544/50048]	Loss: 1.9860
Training Epoch: 10 [44672/50048]	Loss: 1.7357
Training Epoch: 10 [44800/50048]	Loss: 1.6471
Training Epoch: 10 [44928/50048]	Loss: 1.8581
Training Epoch: 10 [45056/50048]	Loss: 2.0593
Training Epoch: 10 [45184/50048]	Loss: 1.7527
Training Epoch: 10 [45312/50048]	Loss: 1.5345
Training Epoch: 10 [45440/50048]	Loss: 1.8833
Training Epoch: 10 [45568/50048]	Loss: 2.1083
Training Epoch: 10 [45696/50048]	Loss: 2.1185
Training Epoch: 10 [45824/50048]	Loss: 1.6811
Training Epoch: 10 [45952/50048]	Loss: 1.6292
Training Epoch: 10 [46080/50048]	Loss: 1.9315
Training Epoch: 10 [46208/50048]	Loss: 1.8297
Training Epoch: 10 [46336/50048]	Loss: 1.9354
Training Epoch: 10 [46464/50048]	Loss: 1.7557
Training Epoch: 10 [46592/50048]	Loss: 1.7302
Training Epoch: 10 [46720/50048]	Loss: 1.8040
Training Epoch: 10 [46848/50048]	Loss: 1.9263
Training Epoch: 10 [46976/50048]	Loss: 1.8792
Training Epoch: 10 [47104/50048]	Loss: 2.0434
Training Epoch: 10 [47232/50048]	Loss: 1.8476
Training Epoch: 10 [47360/50048]	Loss: 1.9243
Training Epoch: 10 [47488/50048]	Loss: 1.6596
Training Epoch: 10 [47616/50048]	Loss: 1.6856
Training Epoch: 10 [47744/50048]	Loss: 1.8578
Training Epoch: 10 [47872/50048]	Loss: 1.3366
Training Epoch: 10 [48000/50048]	Loss: 1.8206
Training Epoch: 10 [48128/50048]	Loss: 1.8374
Training Epoch: 10 [48256/50048]	Loss: 1.8718
Training Epoch: 10 [48384/50048]	Loss: 1.9914
Training Epoch: 10 [48512/50048]	Loss: 1.5654
Training Epoch: 10 [48640/50048]	Loss: 1.7859
Training Epoch: 10 [48768/50048]	Loss: 1.6386
Training Epoch: 10 [48896/50048]	Loss: 1.7338
Training Epoch: 10 [49024/50048]	Loss: 1.6770
Training Epoch: 10 [49152/50048]	Loss: 1.7369
Training Epoch: 10 [49280/50048]	Loss: 1.7758
Training Epoch: 10 [49408/50048]	Loss: 1.6960
Training Epoch: 10 [49536/50048]	Loss: 2.0008
Training Epoch: 10 [49664/50048]	Loss: 1.7111
Training Epoch: 10 [49792/50048]	Loss: 1.6886
Training Epoch: 10 [49920/50048]	Loss: 1.9145
Training Epoch: 10 [50048/50048]	Loss: 1.9343
Validation Epoch: 10, Average loss: 0.0145, Accuracy: 0.4962
Training Epoch: 11 [128/50048]	Loss: 1.8489
Training Epoch: 11 [256/50048]	Loss: 1.5923
Training Epoch: 11 [384/50048]	Loss: 1.5872
Training Epoch: 11 [512/50048]	Loss: 2.0206
Training Epoch: 11 [640/50048]	Loss: 1.7922
Training Epoch: 11 [768/50048]	Loss: 2.0506
Training Epoch: 11 [896/50048]	Loss: 1.5837
Training Epoch: 11 [1024/50048]	Loss: 1.9211
Training Epoch: 11 [1152/50048]	Loss: 1.6740
Training Epoch: 11 [1280/50048]	Loss: 1.6318
Training Epoch: 11 [1408/50048]	Loss: 1.9163
Training Epoch: 11 [1536/50048]	Loss: 1.7472
Training Epoch: 11 [1664/50048]	Loss: 1.9149
Training Epoch: 11 [1792/50048]	Loss: 1.8855
Training Epoch: 11 [1920/50048]	Loss: 1.6778
Training Epoch: 11 [2048/50048]	Loss: 1.6714
Training Epoch: 11 [2176/50048]	Loss: 1.6050
Training Epoch: 11 [2304/50048]	Loss: 1.6508
Training Epoch: 11 [2432/50048]	Loss: 1.6579
Training Epoch: 11 [2560/50048]	Loss: 1.7312
Training Epoch: 11 [2688/50048]	Loss: 1.4877
Training Epoch: 11 [2816/50048]	Loss: 1.4632
Training Epoch: 11 [2944/50048]	Loss: 1.5926
Training Epoch: 11 [3072/50048]	Loss: 1.7690
Training Epoch: 11 [3200/50048]	Loss: 1.5822
Training Epoch: 11 [3328/50048]	Loss: 1.6466
Training Epoch: 11 [3456/50048]	Loss: 1.3468
Training Epoch: 11 [3584/50048]	Loss: 1.7020
Training Epoch: 11 [3712/50048]	Loss: 1.6089
Training Epoch: 11 [3840/50048]	Loss: 1.6821
Training Epoch: 11 [3968/50048]	Loss: 1.7816
Training Epoch: 11 [4096/50048]	Loss: 1.7245
Training Epoch: 11 [4224/50048]	Loss: 1.7849
Training Epoch: 11 [4352/50048]	Loss: 1.9150
Training Epoch: 11 [4480/50048]	Loss: 1.9861
Training Epoch: 11 [4608/50048]	Loss: 1.9257
Training Epoch: 11 [4736/50048]	Loss: 1.8601
Training Epoch: 11 [4864/50048]	Loss: 1.9333
Training Epoch: 11 [4992/50048]	Loss: 1.8132
Training Epoch: 11 [5120/50048]	Loss: 1.8498
Training Epoch: 11 [5248/50048]	Loss: 1.6662
Training Epoch: 11 [5376/50048]	Loss: 1.9170
Training Epoch: 11 [5504/50048]	Loss: 1.4647
Training Epoch: 11 [5632/50048]	Loss: 1.3737
Training Epoch: 11 [5760/50048]	Loss: 1.8277
Training Epoch: 11 [5888/50048]	Loss: 1.6483
Training Epoch: 11 [6016/50048]	Loss: 1.9599
Training Epoch: 11 [6144/50048]	Loss: 1.9446
Training Epoch: 11 [6272/50048]	Loss: 1.7730
Training Epoch: 11 [6400/50048]	Loss: 1.6006
Training Epoch: 11 [6528/50048]	Loss: 1.7531
Training Epoch: 11 [6656/50048]	Loss: 1.7292
Training Epoch: 11 [6784/50048]	Loss: 1.7536
Training Epoch: 11 [6912/50048]	Loss: 1.7092
Training Epoch: 11 [7040/50048]	Loss: 1.8773
Training Epoch: 11 [7168/50048]	Loss: 1.7165
Training Epoch: 11 [7296/50048]	Loss: 1.8036
Training Epoch: 11 [7424/50048]	Loss: 1.8598
Training Epoch: 11 [7552/50048]	Loss: 1.6871
Training Epoch: 11 [7680/50048]	Loss: 2.0563
Training Epoch: 11 [7808/50048]	Loss: 1.7939
Training Epoch: 11 [7936/50048]	Loss: 1.9956
Training Epoch: 11 [8064/50048]	Loss: 1.6226
Training Epoch: 11 [8192/50048]	Loss: 1.7945
Training Epoch: 11 [8320/50048]	Loss: 1.8488
Training Epoch: 11 [8448/50048]	Loss: 1.8414
Training Epoch: 11 [8576/50048]	Loss: 1.7108
Training Epoch: 11 [8704/50048]	Loss: 1.7439
Training Epoch: 11 [8832/50048]	Loss: 1.6761
Training Epoch: 11 [8960/50048]	Loss: 1.7791
Training Epoch: 11 [9088/50048]	Loss: 2.0229
Training Epoch: 11 [9216/50048]	Loss: 1.6773
Training Epoch: 11 [9344/50048]	Loss: 1.5712
Training Epoch: 11 [9472/50048]	Loss: 1.5075
Training Epoch: 11 [9600/50048]	Loss: 1.8230
Training Epoch: 11 [9728/50048]	Loss: 1.6727
Training Epoch: 11 [9856/50048]	Loss: 1.6931
Training Epoch: 11 [9984/50048]	Loss: 1.8623
Training Epoch: 11 [10112/50048]	Loss: 1.6960
Training Epoch: 11 [10240/50048]	Loss: 1.8267
Training Epoch: 11 [10368/50048]	Loss: 1.8388
Training Epoch: 11 [10496/50048]	Loss: 1.6913
Training Epoch: 11 [10624/50048]	Loss: 1.8003
Training Epoch: 11 [10752/50048]	Loss: 1.6284
Training Epoch: 11 [10880/50048]	Loss: 1.6674
Training Epoch: 11 [11008/50048]	Loss: 2.0560
Training Epoch: 11 [11136/50048]	Loss: 1.7405
Training Epoch: 11 [11264/50048]	Loss: 1.6962
Training Epoch: 11 [11392/50048]	Loss: 1.7662
Training Epoch: 11 [11520/50048]	Loss: 1.6406
Training Epoch: 11 [11648/50048]	Loss: 1.8470
Training Epoch: 11 [11776/50048]	Loss: 1.6905
Training Epoch: 11 [11904/50048]	Loss: 1.5979
Training Epoch: 11 [12032/50048]	Loss: 1.8436
Training Epoch: 11 [12160/50048]	Loss: 1.6893
Training Epoch: 11 [12288/50048]	Loss: 1.5773
Training Epoch: 11 [12416/50048]	Loss: 1.6616
Training Epoch: 11 [12544/50048]	Loss: 1.6951
Training Epoch: 11 [12672/50048]	Loss: 1.5762
Training Epoch: 11 [12800/50048]	Loss: 1.6888
Training Epoch: 11 [12928/50048]	Loss: 1.6091
Training Epoch: 11 [13056/50048]	Loss: 1.8036
Training Epoch: 11 [13184/50048]	Loss: 1.9374
Training Epoch: 11 [13312/50048]	Loss: 1.7964
Training Epoch: 11 [13440/50048]	Loss: 1.9154
Training Epoch: 11 [13568/50048]	Loss: 1.6692
Training Epoch: 11 [13696/50048]	Loss: 1.6640
Training Epoch: 11 [13824/50048]	Loss: 1.8434
Training Epoch: 11 [13952/50048]	Loss: 1.5715
Training Epoch: 11 [14080/50048]	Loss: 1.7886
Training Epoch: 11 [14208/50048]	Loss: 1.8078
Training Epoch: 11 [14336/50048]	Loss: 1.8971
Training Epoch: 11 [14464/50048]	Loss: 1.6429
Training Epoch: 11 [14592/50048]	Loss: 1.9688
Training Epoch: 11 [14720/50048]	Loss: 1.6123
Training Epoch: 11 [14848/50048]	Loss: 1.7201
Training Epoch: 11 [14976/50048]	Loss: 1.9067
Training Epoch: 11 [15104/50048]	Loss: 1.7961
Training Epoch: 11 [15232/50048]	Loss: 1.6927
Training Epoch: 11 [15360/50048]	Loss: 1.8470
Training Epoch: 11 [15488/50048]	Loss: 1.6285
Training Epoch: 11 [15616/50048]	Loss: 1.8479
Training Epoch: 11 [15744/50048]	Loss: 1.8680
Training Epoch: 11 [15872/50048]	Loss: 1.7121
Training Epoch: 11 [16000/50048]	Loss: 1.6750
Training Epoch: 11 [16128/50048]	Loss: 1.5233
Training Epoch: 11 [16256/50048]	Loss: 1.9599
Training Epoch: 11 [16384/50048]	Loss: 1.7026
Training Epoch: 11 [16512/50048]	Loss: 1.6014
Training Epoch: 11 [16640/50048]	Loss: 1.9304
Training Epoch: 11 [16768/50048]	Loss: 1.4989
Training Epoch: 11 [16896/50048]	Loss: 1.9021
Training Epoch: 11 [17024/50048]	Loss: 2.0483
Training Epoch: 11 [17152/50048]	Loss: 1.7744
Training Epoch: 11 [17280/50048]	Loss: 1.7960
Training Epoch: 11 [17408/50048]	Loss: 1.8172
Training Epoch: 11 [17536/50048]	Loss: 1.8596
Training Epoch: 11 [17664/50048]	Loss: 1.8531
Training Epoch: 11 [17792/50048]	Loss: 1.6570
Training Epoch: 11 [17920/50048]	Loss: 1.8882
Training Epoch: 11 [18048/50048]	Loss: 1.9905
Training Epoch: 11 [18176/50048]	Loss: 1.8739
Training Epoch: 11 [18304/50048]	Loss: 1.8942
Training Epoch: 11 [18432/50048]	Loss: 1.6105
Training Epoch: 11 [18560/50048]	Loss: 1.6795
Training Epoch: 11 [18688/50048]	Loss: 1.7468
Training Epoch: 11 [18816/50048]	Loss: 1.6463
Training Epoch: 11 [18944/50048]	Loss: 1.7005
Training Epoch: 11 [19072/50048]	Loss: 1.8118
Training Epoch: 11 [19200/50048]	Loss: 1.6429
Training Epoch: 11 [19328/50048]	Loss: 1.7646
Training Epoch: 11 [19456/50048]	Loss: 1.9797
Training Epoch: 11 [19584/50048]	Loss: 1.7156
Training Epoch: 11 [19712/50048]	Loss: 1.8011
Training Epoch: 11 [19840/50048]	Loss: 2.0217
Training Epoch: 11 [19968/50048]	Loss: 1.7430
Training Epoch: 11 [20096/50048]	Loss: 2.0067
Training Epoch: 11 [20224/50048]	Loss: 1.7362
Training Epoch: 11 [20352/50048]	Loss: 1.8354
Training Epoch: 11 [20480/50048]	Loss: 1.6351
Training Epoch: 11 [20608/50048]	Loss: 1.6487
Training Epoch: 11 [20736/50048]	Loss: 1.8679
Training Epoch: 11 [20864/50048]	Loss: 1.7166
Training Epoch: 11 [20992/50048]	Loss: 1.7992
Training Epoch: 11 [21120/50048]	Loss: 1.8492
Training Epoch: 11 [21248/50048]	Loss: 1.8296
Training Epoch: 11 [21376/50048]	Loss: 1.6688
Training Epoch: 11 [21504/50048]	Loss: 1.9042
Training Epoch: 11 [21632/50048]	Loss: 1.6902
Training Epoch: 11 [21760/50048]	Loss: 2.0283
Training Epoch: 11 [21888/50048]	Loss: 1.6447
Training Epoch: 11 [22016/50048]	Loss: 1.7827
Training Epoch: 11 [22144/50048]	Loss: 1.7848
Training Epoch: 11 [22272/50048]	Loss: 1.5982
Training Epoch: 11 [22400/50048]	Loss: 1.5442
Training Epoch: 11 [22528/50048]	Loss: 1.6289
Training Epoch: 11 [22656/50048]	Loss: 1.6267
Training Epoch: 11 [22784/50048]	Loss: 1.5943
Training Epoch: 11 [22912/50048]	Loss: 1.6002
Training Epoch: 11 [23040/50048]	Loss: 1.6416
Training Epoch: 11 [23168/50048]	Loss: 1.4820
Training Epoch: 11 [23296/50048]	Loss: 1.8955
Training Epoch: 11 [23424/50048]	Loss: 1.5733
Training Epoch: 11 [23552/50048]	Loss: 1.6148
Training Epoch: 11 [23680/50048]	Loss: 1.8993
Training Epoch: 11 [23808/50048]	Loss: 1.7980
Training Epoch: 11 [23936/50048]	Loss: 1.6525
Training Epoch: 11 [24064/50048]	Loss: 1.6895
Training Epoch: 11 [24192/50048]	Loss: 1.7460
Training Epoch: 11 [24320/50048]	Loss: 1.6922
Training Epoch: 11 [24448/50048]	Loss: 1.9991
Training Epoch: 11 [24576/50048]	Loss: 1.6642
Training Epoch: 11 [24704/50048]	Loss: 1.6138
Training Epoch: 11 [24832/50048]	Loss: 1.7844
Training Epoch: 11 [24960/50048]	Loss: 1.9215
Training Epoch: 11 [25088/50048]	Loss: 1.8731
Training Epoch: 11 [25216/50048]	Loss: 1.7268
Training Epoch: 11 [25344/50048]	Loss: 1.9198
Training Epoch: 11 [25472/50048]	Loss: 1.6959
Training Epoch: 11 [25600/50048]	Loss: 1.9711
Training Epoch: 11 [25728/50048]	Loss: 1.6948
Training Epoch: 11 [25856/50048]	Loss: 1.7524
Training Epoch: 11 [25984/50048]	Loss: 1.8772
Training Epoch: 11 [26112/50048]	Loss: 1.7997
Training Epoch: 11 [26240/50048]	Loss: 1.8645
Training Epoch: 11 [26368/50048]	Loss: 1.6782
Training Epoch: 11 [26496/50048]	Loss: 1.7738
Training Epoch: 11 [26624/50048]	Loss: 1.5474
Training Epoch: 11 [26752/50048]	Loss: 1.8547
Training Epoch: 11 [26880/50048]	Loss: 1.9812
Training Epoch: 11 [27008/50048]	Loss: 1.6362
Training Epoch: 11 [27136/50048]	Loss: 1.6781
Training Epoch: 11 [27264/50048]	Loss: 1.7127
Training Epoch: 11 [27392/50048]	Loss: 1.4538
Training Epoch: 11 [27520/50048]	Loss: 1.6187
Training Epoch: 11 [27648/50048]	Loss: 1.9166
Training Epoch: 11 [27776/50048]	Loss: 1.7970
Training Epoch: 11 [27904/50048]	Loss: 1.7653
Training Epoch: 11 [28032/50048]	Loss: 1.5928
Training Epoch: 11 [28160/50048]	Loss: 1.9457
Training Epoch: 11 [28288/50048]	Loss: 1.4907
Training Epoch: 11 [28416/50048]	Loss: 1.6682
Training Epoch: 11 [28544/50048]	Loss: 1.6149
Training Epoch: 11 [28672/50048]	Loss: 1.8246
Training Epoch: 11 [28800/50048]	Loss: 1.7100
Training Epoch: 11 [28928/50048]	Loss: 1.7628
Training Epoch: 11 [29056/50048]	Loss: 1.8224
Training Epoch: 11 [29184/50048]	Loss: 1.6311
Training Epoch: 11 [29312/50048]	Loss: 1.8620
Training Epoch: 11 [29440/50048]	Loss: 1.9200
Training Epoch: 11 [29568/50048]	Loss: 1.6487
Training Epoch: 11 [29696/50048]	Loss: 1.7531
Training Epoch: 11 [29824/50048]	Loss: 1.8226
Training Epoch: 11 [29952/50048]	Loss: 1.5463
Training Epoch: 11 [30080/50048]	Loss: 1.8262
Training Epoch: 11 [30208/50048]	Loss: 1.9122
Training Epoch: 11 [30336/50048]	Loss: 1.6375
Training Epoch: 11 [30464/50048]	Loss: 1.7049
Training Epoch: 11 [30592/50048]	Loss: 1.5980
Training Epoch: 11 [30720/50048]	Loss: 1.5488
Training Epoch: 11 [30848/50048]	Loss: 1.8868
Training Epoch: 11 [30976/50048]	Loss: 1.8539
Training Epoch: 11 [31104/50048]	Loss: 1.6712
Training Epoch: 11 [31232/50048]	Loss: 1.7999
Training Epoch: 11 [31360/50048]	Loss: 1.9912
Training Epoch: 11 [31488/50048]	Loss: 1.7535
Training Epoch: 11 [31616/50048]	Loss: 1.8986
Training Epoch: 11 [31744/50048]	Loss: 1.8327
Training Epoch: 11 [31872/50048]	Loss: 1.9509
Training Epoch: 11 [32000/50048]	Loss: 1.9667
Training Epoch: 11 [32128/50048]	Loss: 1.7158
Training Epoch: 11 [32256/50048]	Loss: 1.5777
Training Epoch: 11 [32384/50048]	Loss: 1.5868
Training Epoch: 11 [32512/50048]	Loss: 1.5401
Training Epoch: 11 [32640/50048]	Loss: 1.9714
Training Epoch: 11 [32768/50048]	Loss: 2.1840
Training Epoch: 11 [32896/50048]	Loss: 1.8081
Training Epoch: 11 [33024/50048]	Loss: 1.7765
Training Epoch: 11 [33152/50048]	Loss: 1.7639
Training Epoch: 11 [33280/50048]	Loss: 1.9653
Training Epoch: 11 [33408/50048]	Loss: 1.7433
Training Epoch: 11 [33536/50048]	Loss: 1.7998
Training Epoch: 11 [33664/50048]	Loss: 1.6721
Training Epoch: 11 [33792/50048]	Loss: 1.6433
Training Epoch: 11 [33920/50048]	Loss: 1.6314
Training Epoch: 11 [34048/50048]	Loss: 1.9302
Training Epoch: 11 [34176/50048]	Loss: 1.8291
Training Epoch: 11 [34304/50048]	Loss: 1.7423
Training Epoch: 11 [34432/50048]	Loss: 1.6979
Training Epoch: 11 [34560/50048]	Loss: 1.9217
Training Epoch: 11 [34688/50048]	Loss: 1.9910
Training Epoch: 11 [34816/50048]	Loss: 1.7581
Training Epoch: 11 [34944/50048]	Loss: 1.7861
Training Epoch: 11 [35072/50048]	Loss: 1.7569
Training Epoch: 11 [35200/50048]	Loss: 1.8900
Training Epoch: 11 [35328/50048]	Loss: 1.5362
Training Epoch: 11 [35456/50048]	Loss: 2.0002
Training Epoch: 11 [35584/50048]	Loss: 1.8821
Training Epoch: 11 [35712/50048]	Loss: 1.7799
Training Epoch: 11 [35840/50048]	Loss: 1.7400
Training Epoch: 11 [35968/50048]	Loss: 2.0041
Training Epoch: 11 [36096/50048]	Loss: 1.7115
Training Epoch: 11 [36224/50048]	Loss: 1.9731
Training Epoch: 11 [36352/50048]	Loss: 1.5074
Training Epoch: 11 [36480/50048]	Loss: 1.5995
Training Epoch: 11 [36608/50048]	Loss: 1.9344
Training Epoch: 11 [36736/50048]	Loss: 1.8159
Training Epoch: 11 [36864/50048]	Loss: 1.7011
Training Epoch: 11 [36992/50048]	Loss: 1.9496
Training Epoch: 11 [37120/50048]	Loss: 1.4488
Training Epoch: 11 [37248/50048]	Loss: 1.7838
Training Epoch: 11 [37376/50048]	Loss: 1.8215
Training Epoch: 11 [37504/50048]	Loss: 2.0617
Training Epoch: 11 [37632/50048]	Loss: 1.6934
Training Epoch: 11 [37760/50048]	Loss: 1.9503
Training Epoch: 11 [37888/50048]	Loss: 1.7525
Training Epoch: 11 [38016/50048]	Loss: 1.7832
Training Epoch: 11 [38144/50048]	Loss: 1.5530
Training Epoch: 11 [38272/50048]	Loss: 2.1443
Training Epoch: 11 [38400/50048]	Loss: 1.7445
Training Epoch: 11 [38528/50048]	Loss: 1.7714
Training Epoch: 11 [38656/50048]	Loss: 2.0113
Training Epoch: 11 [38784/50048]	Loss: 1.5817
Training Epoch: 11 [38912/50048]	Loss: 1.8826
Training Epoch: 11 [39040/50048]	Loss: 1.7912
Training Epoch: 11 [39168/50048]	Loss: 1.6047
Training Epoch: 11 [39296/50048]	Loss: 1.8242
Training Epoch: 11 [39424/50048]	Loss: 1.5462
Training Epoch: 11 [39552/50048]	Loss: 1.8960
Training Epoch: 11 [39680/50048]	Loss: 1.7524
Training Epoch: 11 [39808/50048]	Loss: 1.6356
Training Epoch: 11 [39936/50048]	Loss: 1.9142
Training Epoch: 11 [40064/50048]	Loss: 1.5908
Training Epoch: 11 [40192/50048]	Loss: 1.5964
Training Epoch: 11 [40320/50048]	Loss: 2.0562
Training Epoch: 11 [40448/50048]	Loss: 1.8286
Training Epoch: 11 [40576/50048]	Loss: 1.9513
Training Epoch: 11 [40704/50048]	Loss: 1.7401
Training Epoch: 11 [40832/50048]	Loss: 1.6046
Training Epoch: 11 [40960/50048]	Loss: 1.7861
Training Epoch: 11 [41088/50048]	Loss: 1.8374
Training Epoch: 11 [41216/50048]	Loss: 1.7717
Training Epoch: 11 [41344/50048]	Loss: 1.9994
Training Epoch: 11 [41472/50048]	Loss: 1.7553
Training Epoch: 11 [41600/50048]	Loss: 1.8356
Training Epoch: 11 [41728/50048]	Loss: 1.6133
Training Epoch: 11 [41856/50048]	Loss: 1.6056
Training Epoch: 11 [41984/50048]	Loss: 2.0362
Training Epoch: 11 [42112/50048]	Loss: 1.5858
Training Epoch: 11 [42240/50048]	Loss: 1.8705
Training Epoch: 11 [42368/50048]	Loss: 1.6036
Training Epoch: 11 [42496/50048]	Loss: 1.6490
Training Epoch: 11 [42624/50048]	Loss: 1.8254
Training Epoch: 11 [42752/50048]	Loss: 1.6435
Training Epoch: 11 [42880/50048]	Loss: 1.7868
Training Epoch: 11 [43008/50048]	Loss: 1.7859
Training Epoch: 11 [43136/50048]	Loss: 1.4746
Training Epoch: 11 [43264/50048]	Loss: 1.6244
Training Epoch: 11 [43392/50048]	Loss: 1.8203
Training Epoch: 11 [43520/50048]	Loss: 1.6942
Training Epoch: 11 [43648/50048]	Loss: 1.7341
Training Epoch: 11 [43776/50048]	Loss: 1.6729
Training Epoch: 11 [43904/50048]	Loss: 1.6861
Training Epoch: 11 [44032/50048]	Loss: 1.8102
Training Epoch: 11 [44160/50048]	Loss: 1.8165
Training Epoch: 11 [44288/50048]	Loss: 1.8167
Training Epoch: 11 [44416/50048]	Loss: 2.0010
Training Epoch: 11 [44544/50048]	Loss: 1.7284
Training Epoch: 11 [44672/50048]	Loss: 1.9278
Training Epoch: 11 [44800/50048]	Loss: 1.7200
Training Epoch: 11 [44928/50048]	Loss: 1.6343
Training Epoch: 11 [45056/50048]	Loss: 1.7227
Training Epoch: 11 [45184/50048]	Loss: 1.4768
Training Epoch: 11 [45312/50048]	Loss: 1.7822
Training Epoch: 11 [45440/50048]	Loss: 1.6175
Training Epoch: 11 [45568/50048]	Loss: 1.8952
Training Epoch: 11 [45696/50048]	Loss: 1.6541
Training Epoch: 11 [45824/50048]	Loss: 1.6280
Training Epoch: 11 [45952/50048]	Loss: 1.9358
Training Epoch: 11 [46080/50048]	Loss: 1.6644
Training Epoch: 11 [46208/50048]	Loss: 1.6753
Training Epoch: 11 [46336/50048]	Loss: 1.8133
Training Epoch: 11 [46464/50048]	Loss: 1.5883
Training Epoch: 11 [46592/50048]	Loss: 1.8690
Training Epoch: 11 [46720/50048]	Loss: 1.5919
Training Epoch: 11 [46848/50048]	Loss: 1.5258
Training Epoch: 11 [46976/50048]	Loss: 1.7573
Training Epoch: 11 [47104/50048]	Loss: 1.5149
Training Epoch: 11 [47232/50048]	Loss: 1.8699
Training Epoch: 11 [47360/50048]	Loss: 1.6682
Training Epoch: 11 [47488/50048]	Loss: 1.6739
Training Epoch: 11 [47616/50048]	Loss: 1.6995
Training Epoch: 11 [47744/50048]	Loss: 1.8113
Training Epoch: 11 [47872/50048]	Loss: 1.8889
Training Epoch: 11 [48000/50048]	Loss: 1.8564
Training Epoch: 11 [48128/50048]	Loss: 1.8731
Training Epoch: 11 [48256/50048]	Loss: 1.7704
Training Epoch: 11 [48384/50048]	Loss: 1.9673
Training Epoch: 11 [48512/50048]	Loss: 1.9291
Training Epoch: 11 [48640/50048]	Loss: 1.8317
Training Epoch: 11 [48768/50048]	Loss: 1.5669
Training Epoch: 11 [48896/50048]	Loss: 1.6891
Training Epoch: 11 [49024/50048]	Loss: 1.8371
Training Epoch: 11 [49152/50048]	Loss: 1.9629
Training Epoch: 11 [49280/50048]	Loss: 1.7212
Training Epoch: 11 [49408/50048]	Loss: 1.5449
Training Epoch: 11 [49536/50048]	Loss: 1.7129
Training Epoch: 11 [49664/50048]	Loss: 1.8035
Training Epoch: 11 [49792/50048]	Loss: 1.5758
Training Epoch: 11 [49920/50048]	Loss: 1.4769
Training Epoch: 11 [50048/50048]	Loss: 1.7894
Validation Epoch: 11, Average loss: 0.0143, Accuracy: 0.5027
Training Epoch: 12 [128/50048]	Loss: 1.5972
Training Epoch: 12 [256/50048]	Loss: 1.9224
Training Epoch: 12 [384/50048]	Loss: 1.7784
Training Epoch: 12 [512/50048]	Loss: 1.7645
Training Epoch: 12 [640/50048]	Loss: 1.3145
Training Epoch: 12 [768/50048]	Loss: 1.7228
Training Epoch: 12 [896/50048]	Loss: 1.6137
Training Epoch: 12 [1024/50048]	Loss: 1.8697
Training Epoch: 12 [1152/50048]	Loss: 1.8473
Training Epoch: 12 [1280/50048]	Loss: 1.6025
Training Epoch: 12 [1408/50048]	Loss: 1.6778
Training Epoch: 12 [1536/50048]	Loss: 1.7376
Training Epoch: 12 [1664/50048]	Loss: 1.6902
Training Epoch: 12 [1792/50048]	Loss: 1.9090
Training Epoch: 12 [1920/50048]	Loss: 1.7942
Training Epoch: 12 [2048/50048]	Loss: 1.5249
Training Epoch: 12 [2176/50048]	Loss: 1.5462
Training Epoch: 12 [2304/50048]	Loss: 1.8527
Training Epoch: 12 [2432/50048]	Loss: 1.7900
Training Epoch: 12 [2560/50048]	Loss: 1.7157
Training Epoch: 12 [2688/50048]	Loss: 1.8164
Training Epoch: 12 [2816/50048]	Loss: 1.9580
Training Epoch: 12 [2944/50048]	Loss: 1.7940
Training Epoch: 12 [3072/50048]	Loss: 1.6625
Training Epoch: 12 [3200/50048]	Loss: 1.8904
Training Epoch: 12 [3328/50048]	Loss: 1.6855
Training Epoch: 12 [3456/50048]	Loss: 1.7756
Training Epoch: 12 [3584/50048]	Loss: 1.6605
Training Epoch: 12 [3712/50048]	Loss: 1.7823
Training Epoch: 12 [3840/50048]	Loss: 1.5713
Training Epoch: 12 [3968/50048]	Loss: 1.4402
Training Epoch: 12 [4096/50048]	Loss: 1.7356
Training Epoch: 12 [4224/50048]	Loss: 1.6878
Training Epoch: 12 [4352/50048]	Loss: 1.5525
Training Epoch: 12 [4480/50048]	Loss: 1.4570
Training Epoch: 12 [4608/50048]	Loss: 1.5971
Training Epoch: 12 [4736/50048]	Loss: 1.8152
Training Epoch: 12 [4864/50048]	Loss: 1.9570
Training Epoch: 12 [4992/50048]	Loss: 1.7361
Training Epoch: 12 [5120/50048]	Loss: 1.7949
Training Epoch: 12 [5248/50048]	Loss: 1.6054
Training Epoch: 12 [5376/50048]	Loss: 1.6629
Training Epoch: 12 [5504/50048]	Loss: 1.7786
Training Epoch: 12 [5632/50048]	Loss: 1.5840
Training Epoch: 12 [5760/50048]	Loss: 1.9041
Training Epoch: 12 [5888/50048]	Loss: 1.6477
Training Epoch: 12 [6016/50048]	Loss: 1.7169
Training Epoch: 12 [6144/50048]	Loss: 1.5414
Training Epoch: 12 [6272/50048]	Loss: 1.6026
Training Epoch: 12 [6400/50048]	Loss: 2.0061
Training Epoch: 12 [6528/50048]	Loss: 1.8038
Training Epoch: 12 [6656/50048]	Loss: 1.6441
Training Epoch: 12 [6784/50048]	Loss: 1.3668
Training Epoch: 12 [6912/50048]	Loss: 1.6848
Training Epoch: 12 [7040/50048]	Loss: 1.4941
Training Epoch: 12 [7168/50048]	Loss: 1.7700
Training Epoch: 12 [7296/50048]	Loss: 1.6587
Training Epoch: 12 [7424/50048]	Loss: 1.8174
Training Epoch: 12 [7552/50048]	Loss: 1.6786
Training Epoch: 12 [7680/50048]	Loss: 1.4648
Training Epoch: 12 [7808/50048]	Loss: 1.6372
Training Epoch: 12 [7936/50048]	Loss: 1.8765
Training Epoch: 12 [8064/50048]	Loss: 1.7159
Training Epoch: 12 [8192/50048]	Loss: 1.7202
Training Epoch: 12 [8320/50048]	Loss: 1.5699
Training Epoch: 12 [8448/50048]	Loss: 1.8546
Training Epoch: 12 [8576/50048]	Loss: 1.7470
Training Epoch: 12 [8704/50048]	Loss: 1.8920
Training Epoch: 12 [8832/50048]	Loss: 1.8392
Training Epoch: 12 [8960/50048]	Loss: 1.5617
Training Epoch: 12 [9088/50048]	Loss: 1.8992
Training Epoch: 12 [9216/50048]	Loss: 1.5895
Training Epoch: 12 [9344/50048]	Loss: 1.7300
Training Epoch: 12 [9472/50048]	Loss: 1.8962
Training Epoch: 12 [9600/50048]	Loss: 1.6599
Training Epoch: 12 [9728/50048]	Loss: 1.6288
Training Epoch: 12 [9856/50048]	Loss: 1.7638
Training Epoch: 12 [9984/50048]	Loss: 1.7164
Training Epoch: 12 [10112/50048]	Loss: 1.6743
Training Epoch: 12 [10240/50048]	Loss: 1.9066
Training Epoch: 12 [10368/50048]	Loss: 1.7769
Training Epoch: 12 [10496/50048]	Loss: 1.6600
Training Epoch: 12 [10624/50048]	Loss: 1.6419
Training Epoch: 12 [10752/50048]	Loss: 1.6645
Training Epoch: 12 [10880/50048]	Loss: 1.4721
Training Epoch: 12 [11008/50048]	Loss: 1.6073
Training Epoch: 12 [11136/50048]	Loss: 1.8455
Training Epoch: 12 [11264/50048]	Loss: 1.7211
Training Epoch: 12 [11392/50048]	Loss: 1.6452
Training Epoch: 12 [11520/50048]	Loss: 1.7817
Training Epoch: 12 [11648/50048]	Loss: 1.5273
Training Epoch: 12 [11776/50048]	Loss: 1.3773
Training Epoch: 12 [11904/50048]	Loss: 1.4812
Training Epoch: 12 [12032/50048]	Loss: 1.6080
Training Epoch: 12 [12160/50048]	Loss: 1.5722
Training Epoch: 12 [12288/50048]	Loss: 1.8945
Training Epoch: 12 [12416/50048]	Loss: 1.5792
Training Epoch: 12 [12544/50048]	Loss: 1.4980
Training Epoch: 12 [12672/50048]	Loss: 1.7286
Training Epoch: 12 [12800/50048]	Loss: 1.6808
Training Epoch: 12 [12928/50048]	Loss: 1.8089
Training Epoch: 12 [13056/50048]	Loss: 1.8045
Training Epoch: 12 [13184/50048]	Loss: 1.7006
Training Epoch: 12 [13312/50048]	Loss: 1.4402
Training Epoch: 12 [13440/50048]	Loss: 1.7742
Training Epoch: 12 [13568/50048]	Loss: 1.7643
Training Epoch: 12 [13696/50048]	Loss: 1.6553
Training Epoch: 12 [13824/50048]	Loss: 1.6011
Training Epoch: 12 [13952/50048]	Loss: 2.0654
Training Epoch: 12 [14080/50048]	Loss: 1.6564
Training Epoch: 12 [14208/50048]	Loss: 1.6501
Training Epoch: 12 [14336/50048]	Loss: 1.8277
Training Epoch: 12 [14464/50048]	Loss: 1.3948
Training Epoch: 12 [14592/50048]	Loss: 1.4726
Training Epoch: 12 [14720/50048]	Loss: 1.6963
Training Epoch: 12 [14848/50048]	Loss: 1.7845
Training Epoch: 12 [14976/50048]	Loss: 1.6988
Training Epoch: 12 [15104/50048]	Loss: 1.5727
Training Epoch: 12 [15232/50048]	Loss: 1.6637
Training Epoch: 12 [15360/50048]	Loss: 1.5802
Training Epoch: 12 [15488/50048]	Loss: 1.4730
Training Epoch: 12 [15616/50048]	Loss: 1.5531
Training Epoch: 12 [15744/50048]	Loss: 1.9392
Training Epoch: 12 [15872/50048]	Loss: 1.5886
Training Epoch: 12 [16000/50048]	Loss: 1.6519
Training Epoch: 12 [16128/50048]	Loss: 1.8207
Training Epoch: 12 [16256/50048]	Loss: 1.7604
Training Epoch: 12 [16384/50048]	Loss: 1.9375
Training Epoch: 12 [16512/50048]	Loss: 1.6930
Training Epoch: 12 [16640/50048]	Loss: 1.5366
Training Epoch: 12 [16768/50048]	Loss: 1.7128
Training Epoch: 12 [16896/50048]	Loss: 1.8017
Training Epoch: 12 [17024/50048]	Loss: 1.5737
Training Epoch: 12 [17152/50048]	Loss: 1.7796
Training Epoch: 12 [17280/50048]	Loss: 1.9370
Training Epoch: 12 [17408/50048]	Loss: 1.6837
Training Epoch: 12 [17536/50048]	Loss: 1.5316
Training Epoch: 12 [17664/50048]	Loss: 1.5349
Training Epoch: 12 [17792/50048]	Loss: 1.5975
Training Epoch: 12 [17920/50048]	Loss: 1.4609
Training Epoch: 12 [18048/50048]	Loss: 1.7395
Training Epoch: 12 [18176/50048]	Loss: 1.6894
Training Epoch: 12 [18304/50048]	Loss: 1.6720
Training Epoch: 12 [18432/50048]	Loss: 1.6456
Training Epoch: 12 [18560/50048]	Loss: 1.7936
Training Epoch: 12 [18688/50048]	Loss: 1.5836
Training Epoch: 12 [18816/50048]	Loss: 1.6613
Training Epoch: 12 [18944/50048]	Loss: 1.3720
Training Epoch: 12 [19072/50048]	Loss: 1.5459
Training Epoch: 12 [19200/50048]	Loss: 1.6645
Training Epoch: 12 [19328/50048]	Loss: 1.8087
Training Epoch: 12 [19456/50048]	Loss: 1.6883
Training Epoch: 12 [19584/50048]	Loss: 1.5424
Training Epoch: 12 [19712/50048]	Loss: 1.6728
Training Epoch: 12 [19840/50048]	Loss: 1.7560
Training Epoch: 12 [19968/50048]	Loss: 1.7156
Training Epoch: 12 [20096/50048]	Loss: 1.8847
Training Epoch: 12 [20224/50048]	Loss: 1.4809
Training Epoch: 12 [20352/50048]	Loss: 1.7528
Training Epoch: 12 [20480/50048]	Loss: 1.9726
Training Epoch: 12 [20608/50048]	Loss: 1.8297
Training Epoch: 12 [20736/50048]	Loss: 1.5631
Training Epoch: 12 [20864/50048]	Loss: 1.6327
Training Epoch: 12 [20992/50048]	Loss: 1.5882
Training Epoch: 12 [21120/50048]	Loss: 1.6491
Training Epoch: 12 [21248/50048]	Loss: 1.6813
Training Epoch: 12 [21376/50048]	Loss: 1.9514
Training Epoch: 12 [21504/50048]	Loss: 1.7291
Training Epoch: 12 [21632/50048]	Loss: 1.6862
Training Epoch: 12 [21760/50048]	Loss: 1.6239
Training Epoch: 12 [21888/50048]	Loss: 1.6088
Training Epoch: 12 [22016/50048]	Loss: 1.7081
Training Epoch: 12 [22144/50048]	Loss: 1.7377
Training Epoch: 12 [22272/50048]	Loss: 1.6866
Training Epoch: 12 [22400/50048]	Loss: 1.6955
Training Epoch: 12 [22528/50048]	Loss: 1.8286
Training Epoch: 12 [22656/50048]	Loss: 1.7645
Training Epoch: 12 [22784/50048]	Loss: 1.6168
Training Epoch: 12 [22912/50048]	Loss: 1.5340
Training Epoch: 12 [23040/50048]	Loss: 1.5587
Training Epoch: 12 [23168/50048]	Loss: 1.5645
Training Epoch: 12 [23296/50048]	Loss: 1.5117
Training Epoch: 12 [23424/50048]	Loss: 1.8805
Training Epoch: 12 [23552/50048]	Loss: 1.6281
Training Epoch: 12 [23680/50048]	Loss: 1.8195
Training Epoch: 12 [23808/50048]	Loss: 1.4560
Training Epoch: 12 [23936/50048]	Loss: 1.6806
Training Epoch: 12 [24064/50048]	Loss: 1.9322
Training Epoch: 12 [24192/50048]	Loss: 1.8097
Training Epoch: 12 [24320/50048]	Loss: 1.6663
Training Epoch: 12 [24448/50048]	Loss: 1.7109
Training Epoch: 12 [24576/50048]	Loss: 1.7402
Training Epoch: 12 [24704/50048]	Loss: 1.9966
Training Epoch: 12 [24832/50048]	Loss: 1.6795
Training Epoch: 12 [24960/50048]	Loss: 1.7807
Training Epoch: 12 [25088/50048]	Loss: 1.6372
Training Epoch: 12 [25216/50048]	Loss: 1.6758
Training Epoch: 12 [25344/50048]	Loss: 1.6617
Training Epoch: 12 [25472/50048]	Loss: 1.8739
Training Epoch: 12 [25600/50048]	Loss: 1.7505
Training Epoch: 12 [25728/50048]	Loss: 1.6881
Training Epoch: 12 [25856/50048]	Loss: 1.8675
Training Epoch: 12 [25984/50048]	Loss: 1.6029
Training Epoch: 12 [26112/50048]	Loss: 1.5570
Training Epoch: 12 [26240/50048]	Loss: 1.7951
Training Epoch: 12 [26368/50048]	Loss: 1.8233
Training Epoch: 12 [26496/50048]	Loss: 1.6599
Training Epoch: 12 [26624/50048]	Loss: 1.7936
Training Epoch: 12 [26752/50048]	Loss: 1.8252
Training Epoch: 12 [26880/50048]	Loss: 1.5473
Training Epoch: 12 [27008/50048]	Loss: 1.4601
Training Epoch: 12 [27136/50048]	Loss: 1.8982
Training Epoch: 12 [27264/50048]	Loss: 1.5271
Training Epoch: 12 [27392/50048]	Loss: 1.6849
Training Epoch: 12 [27520/50048]	Loss: 1.7561
Training Epoch: 12 [27648/50048]	Loss: 1.8706
Training Epoch: 12 [27776/50048]	Loss: 1.6104
Training Epoch: 12 [27904/50048]	Loss: 1.8151
Training Epoch: 12 [28032/50048]	Loss: 1.8546
Training Epoch: 12 [28160/50048]	Loss: 1.8519
Training Epoch: 12 [28288/50048]	Loss: 1.5809
Training Epoch: 12 [28416/50048]	Loss: 1.5780
Training Epoch: 12 [28544/50048]	Loss: 1.6454
Training Epoch: 12 [28672/50048]	Loss: 1.5302
Training Epoch: 12 [28800/50048]	Loss: 1.8911
Training Epoch: 12 [28928/50048]	Loss: 1.8041
Training Epoch: 12 [29056/50048]	Loss: 1.6857
Training Epoch: 12 [29184/50048]	Loss: 1.6478
Training Epoch: 12 [29312/50048]	Loss: 1.7974
Training Epoch: 12 [29440/50048]	Loss: 1.7514
Training Epoch: 12 [29568/50048]	Loss: 1.6878
Training Epoch: 12 [29696/50048]	Loss: 1.5594
Training Epoch: 12 [29824/50048]	Loss: 1.7648
Training Epoch: 12 [29952/50048]	Loss: 1.6277
Training Epoch: 12 [30080/50048]	Loss: 1.8035
Training Epoch: 12 [30208/50048]	Loss: 1.5988
Training Epoch: 12 [30336/50048]	Loss: 1.6950
Training Epoch: 12 [30464/50048]	Loss: 1.9549
Training Epoch: 12 [30592/50048]	Loss: 1.9279
Training Epoch: 12 [30720/50048]	Loss: 1.7133
Training Epoch: 12 [30848/50048]	Loss: 1.5737
Training Epoch: 12 [30976/50048]	Loss: 1.5936
Training Epoch: 12 [31104/50048]	Loss: 1.6982
Training Epoch: 12 [31232/50048]	Loss: 1.5892
Training Epoch: 12 [31360/50048]	Loss: 1.8800
Training Epoch: 12 [31488/50048]	Loss: 1.7835
Training Epoch: 12 [31616/50048]	Loss: 1.6623
Training Epoch: 12 [31744/50048]	Loss: 1.8619
Training Epoch: 12 [31872/50048]	Loss: 1.5798
Training Epoch: 12 [32000/50048]	Loss: 1.7988
Training Epoch: 12 [32128/50048]	Loss: 1.9169
Training Epoch: 12 [32256/50048]	Loss: 1.7596
Training Epoch: 12 [32384/50048]	Loss: 1.6039
Training Epoch: 12 [32512/50048]	Loss: 1.7278
Training Epoch: 12 [32640/50048]	Loss: 1.5942
Training Epoch: 12 [32768/50048]	Loss: 1.7860
Training Epoch: 12 [32896/50048]	Loss: 1.9199
Training Epoch: 12 [33024/50048]	Loss: 1.9919
Training Epoch: 12 [33152/50048]	Loss: 2.0605
Training Epoch: 12 [33280/50048]	Loss: 1.4686
Training Epoch: 12 [33408/50048]	Loss: 1.6644
Training Epoch: 12 [33536/50048]	Loss: 1.6591
Training Epoch: 12 [33664/50048]	Loss: 1.7248
Training Epoch: 12 [33792/50048]	Loss: 1.7024
Training Epoch: 12 [33920/50048]	Loss: 1.7977
Training Epoch: 12 [34048/50048]	Loss: 1.6734
Training Epoch: 12 [34176/50048]	Loss: 1.9283
Training Epoch: 12 [34304/50048]	Loss: 1.5664
Training Epoch: 12 [34432/50048]	Loss: 1.7502
Training Epoch: 12 [34560/50048]	Loss: 1.6306
Training Epoch: 12 [34688/50048]	Loss: 1.5204
Training Epoch: 12 [34816/50048]	Loss: 1.7472
Training Epoch: 12 [34944/50048]	Loss: 1.6326
Training Epoch: 12 [35072/50048]	Loss: 1.5141
Training Epoch: 12 [35200/50048]	Loss: 1.5029
Training Epoch: 12 [35328/50048]	Loss: 1.8793
Training Epoch: 12 [35456/50048]	Loss: 1.7001
Training Epoch: 12 [35584/50048]	Loss: 1.6079
Training Epoch: 12 [35712/50048]	Loss: 1.5547
Training Epoch: 12 [35840/50048]	Loss: 1.5869
Training Epoch: 12 [35968/50048]	Loss: 1.7907
Training Epoch: 12 [36096/50048]	Loss: 1.8159
Training Epoch: 12 [36224/50048]	Loss: 1.5596
Training Epoch: 12 [36352/50048]	Loss: 1.5387
Training Epoch: 12 [36480/50048]	Loss: 1.6741
Training Epoch: 12 [36608/50048]	Loss: 1.7050
Training Epoch: 12 [36736/50048]	Loss: 1.7647
Training Epoch: 12 [36864/50048]	Loss: 1.7298
Training Epoch: 12 [36992/50048]	Loss: 1.5738
Training Epoch: 12 [37120/50048]	Loss: 1.9597
Training Epoch: 12 [37248/50048]	Loss: 1.7554
Training Epoch: 12 [37376/50048]	Loss: 1.6958
Training Epoch: 12 [37504/50048]	Loss: 1.4416
Training Epoch: 12 [37632/50048]	Loss: 1.9991
Training Epoch: 12 [37760/50048]	Loss: 1.9045
Training Epoch: 12 [37888/50048]	Loss: 2.0078
Training Epoch: 12 [38016/50048]	Loss: 1.8211
Training Epoch: 12 [38144/50048]	Loss: 1.8299
Training Epoch: 12 [38272/50048]	Loss: 1.8000
Training Epoch: 12 [38400/50048]	Loss: 1.7935
Training Epoch: 12 [38528/50048]	Loss: 1.7237
Training Epoch: 12 [38656/50048]	Loss: 1.9262
Training Epoch: 12 [38784/50048]	Loss: 1.7578
Training Epoch: 12 [38912/50048]	Loss: 1.7133
Training Epoch: 12 [39040/50048]	Loss: 1.7118
Training Epoch: 12 [39168/50048]	Loss: 1.5265
Training Epoch: 12 [39296/50048]	Loss: 1.8017
Training Epoch: 12 [39424/50048]	Loss: 1.4547
Training Epoch: 12 [39552/50048]	Loss: 1.5246
Training Epoch: 12 [39680/50048]	Loss: 1.4858
Training Epoch: 12 [39808/50048]	Loss: 1.8018
Training Epoch: 12 [39936/50048]	Loss: 1.6049
Training Epoch: 12 [40064/50048]	Loss: 1.4512
Training Epoch: 12 [40192/50048]	Loss: 1.4973
Training Epoch: 12 [40320/50048]	Loss: 1.6482
Training Epoch: 12 [40448/50048]	Loss: 1.6510
Training Epoch: 12 [40576/50048]	Loss: 1.9757
Training Epoch: 12 [40704/50048]	Loss: 1.6753
Training Epoch: 12 [40832/50048]	Loss: 1.8580
Training Epoch: 12 [40960/50048]	Loss: 1.8105
Training Epoch: 12 [41088/50048]	Loss: 1.8890
Training Epoch: 12 [41216/50048]	Loss: 1.4011
Training Epoch: 12 [41344/50048]	Loss: 1.7442
Training Epoch: 12 [41472/50048]	Loss: 1.7812
Training Epoch: 12 [41600/50048]	Loss: 1.3983
Training Epoch: 12 [41728/50048]	Loss: 1.9488
Training Epoch: 12 [41856/50048]	Loss: 1.6214
Training Epoch: 12 [41984/50048]	Loss: 1.7350
Training Epoch: 12 [42112/50048]	Loss: 1.7142
Training Epoch: 12 [42240/50048]	Loss: 2.0182
Training Epoch: 12 [42368/50048]	Loss: 1.4923
Training Epoch: 12 [42496/50048]	Loss: 1.4876
Training Epoch: 12 [42624/50048]	Loss: 1.8686
Training Epoch: 12 [42752/50048]	Loss: 1.7920
Training Epoch: 12 [42880/50048]	Loss: 1.6263
Training Epoch: 12 [43008/50048]	Loss: 1.7359
Training Epoch: 12 [43136/50048]	Loss: 1.6402
Training Epoch: 12 [43264/50048]	Loss: 1.8610
Training Epoch: 12 [43392/50048]	Loss: 1.5156
Training Epoch: 12 [43520/50048]	Loss: 1.9465
Training Epoch: 12 [43648/50048]	Loss: 1.7852
Training Epoch: 12 [43776/50048]	Loss: 1.5332
Training Epoch: 12 [43904/50048]	Loss: 1.4204
Training Epoch: 12 [44032/50048]	Loss: 1.4859
Training Epoch: 12 [44160/50048]	Loss: 1.7032
Training Epoch: 12 [44288/50048]	Loss: 1.6073
Training Epoch: 12 [44416/50048]	Loss: 1.8129
Training Epoch: 12 [44544/50048]	Loss: 1.9741
Training Epoch: 12 [44672/50048]	Loss: 1.6547
Training Epoch: 12 [44800/50048]	Loss: 1.8855
Training Epoch: 12 [44928/50048]	Loss: 1.5896
Training Epoch: 12 [45056/50048]	Loss: 1.7055
Training Epoch: 12 [45184/50048]	Loss: 1.4015
Training Epoch: 12 [45312/50048]	Loss: 1.5904
Training Epoch: 12 [45440/50048]	Loss: 1.6372
Training Epoch: 12 [45568/50048]	Loss: 1.6076
Training Epoch: 12 [45696/50048]	Loss: 1.7762
Training Epoch: 12 [45824/50048]	Loss: 1.5712
Training Epoch: 12 [45952/50048]	Loss: 1.5324
Training Epoch: 12 [46080/50048]	Loss: 1.6208
Training Epoch: 12 [46208/50048]	Loss: 1.5831
Training Epoch: 12 [46336/50048]	Loss: 1.5576
Training Epoch: 12 [46464/50048]	Loss: 1.8285
Training Epoch: 12 [46592/50048]	Loss: 1.7863
Training Epoch: 12 [46720/50048]	Loss: 1.7807
Training Epoch: 12 [46848/50048]	Loss: 1.7552
Training Epoch: 12 [46976/50048]	Loss: 1.7226
Training Epoch: 12 [47104/50048]	Loss: 1.6539
Training Epoch: 12 [47232/50048]	Loss: 1.7134
Training Epoch: 12 [47360/50048]	Loss: 1.8370
Training Epoch: 12 [47488/50048]	Loss: 2.0087
Training Epoch: 12 [47616/50048]	Loss: 1.6371
Training Epoch: 12 [47744/50048]	Loss: 1.6655
Training Epoch: 12 [47872/50048]	Loss: 1.8462
Training Epoch: 12 [48000/50048]	Loss: 1.7310
Training Epoch: 12 [48128/50048]	Loss: 1.9662
Training Epoch: 12 [48256/50048]	Loss: 1.7030
Training Epoch: 12 [48384/50048]	Loss: 1.6724
Training Epoch: 12 [48512/50048]	Loss: 1.6934
Training Epoch: 12 [48640/50048]	Loss: 1.8140
Training Epoch: 12 [48768/50048]	Loss: 1.6631
Training Epoch: 12 [48896/50048]	Loss: 1.8874
Training Epoch: 12 [49024/50048]	Loss: 1.8396
Training Epoch: 12 [49152/50048]	Loss: 1.8178
Training Epoch: 12 [49280/50048]	Loss: 1.8022
Training Epoch: 12 [49408/50048]	Loss: 1.8543
Training Epoch: 12 [49536/50048]	Loss: 1.9462
Training Epoch: 12 [49664/50048]	Loss: 1.9434
Training Epoch: 12 [49792/50048]	Loss: 1.6567
Training Epoch: 12 [49920/50048]	Loss: 1.6856
Training Epoch: 12 [50048/50048]	Loss: 1.8531
Validation Epoch: 12, Average loss: 0.0139, Accuracy: 0.5135
Training Epoch: 13 [128/50048]	Loss: 1.5611
Training Epoch: 13 [256/50048]	Loss: 1.5654
Training Epoch: 13 [384/50048]	Loss: 1.7380
Training Epoch: 13 [512/50048]	Loss: 1.5079
Training Epoch: 13 [640/50048]	Loss: 1.5661
Training Epoch: 13 [768/50048]	Loss: 1.5935
Training Epoch: 13 [896/50048]	Loss: 1.5657
Training Epoch: 13 [1024/50048]	Loss: 1.6737
Training Epoch: 13 [1152/50048]	Loss: 1.4508
Training Epoch: 13 [1280/50048]	Loss: 1.7963
Training Epoch: 13 [1408/50048]	Loss: 1.8116
Training Epoch: 13 [1536/50048]	Loss: 1.7761
Training Epoch: 13 [1664/50048]	Loss: 1.5587
Training Epoch: 13 [1792/50048]	Loss: 1.4883
Training Epoch: 13 [1920/50048]	Loss: 1.7475
Training Epoch: 13 [2048/50048]	Loss: 1.7149
Training Epoch: 13 [2176/50048]	Loss: 1.7238
Training Epoch: 13 [2304/50048]	Loss: 1.5699
Training Epoch: 13 [2432/50048]	Loss: 1.5135
Training Epoch: 13 [2560/50048]	Loss: 1.6232
Training Epoch: 13 [2688/50048]	Loss: 1.5745
Training Epoch: 13 [2816/50048]	Loss: 1.6629
Training Epoch: 13 [2944/50048]	Loss: 1.6501
Training Epoch: 13 [3072/50048]	Loss: 1.7059
Training Epoch: 13 [3200/50048]	Loss: 1.7561
Training Epoch: 13 [3328/50048]	Loss: 1.6741
Training Epoch: 13 [3456/50048]	Loss: 1.7333
Training Epoch: 13 [3584/50048]	Loss: 1.7705
Training Epoch: 13 [3712/50048]	Loss: 1.8458
Training Epoch: 13 [3840/50048]	Loss: 1.6601
Training Epoch: 13 [3968/50048]	Loss: 1.8390
Training Epoch: 13 [4096/50048]	Loss: 1.7855
Training Epoch: 13 [4224/50048]	Loss: 1.8983
Training Epoch: 13 [4352/50048]	Loss: 1.5276
Training Epoch: 13 [4480/50048]	Loss: 1.6832
Training Epoch: 13 [4608/50048]	Loss: 1.5832
Training Epoch: 13 [4736/50048]	Loss: 1.8304
Training Epoch: 13 [4864/50048]	Loss: 1.6927
Training Epoch: 13 [4992/50048]	Loss: 1.5501
Training Epoch: 13 [5120/50048]	Loss: 1.5808
Training Epoch: 13 [5248/50048]	Loss: 1.6094
Training Epoch: 13 [5376/50048]	Loss: 1.6887
Training Epoch: 13 [5504/50048]	Loss: 1.6592
Training Epoch: 13 [5632/50048]	Loss: 1.4907
Training Epoch: 13 [5760/50048]	Loss: 1.5144
Training Epoch: 13 [5888/50048]	Loss: 1.5006
Training Epoch: 13 [6016/50048]	Loss: 1.7270
Training Epoch: 13 [6144/50048]	Loss: 1.7431
Training Epoch: 13 [6272/50048]	Loss: 1.5956
Training Epoch: 13 [6400/50048]	Loss: 1.5967
Training Epoch: 13 [6528/50048]	Loss: 1.7091
Training Epoch: 13 [6656/50048]	Loss: 1.8218
Training Epoch: 13 [6784/50048]	Loss: 1.7278
Training Epoch: 13 [6912/50048]	Loss: 1.8367
Training Epoch: 13 [7040/50048]	Loss: 1.6739
Training Epoch: 13 [7168/50048]	Loss: 1.6352
Training Epoch: 13 [7296/50048]	Loss: 1.5680
Training Epoch: 13 [7424/50048]	Loss: 1.4742
Training Epoch: 13 [7552/50048]	Loss: 1.6725
Training Epoch: 13 [7680/50048]	Loss: 1.6809
Training Epoch: 13 [7808/50048]	Loss: 1.8132
Training Epoch: 13 [7936/50048]	Loss: 1.5430
Training Epoch: 13 [8064/50048]	Loss: 1.5247
Training Epoch: 13 [8192/50048]	Loss: 1.4164
Training Epoch: 13 [8320/50048]	Loss: 1.8351
Training Epoch: 13 [8448/50048]	Loss: 1.5039
Training Epoch: 13 [8576/50048]	Loss: 1.6128
Training Epoch: 13 [8704/50048]	Loss: 1.3973
Training Epoch: 13 [8832/50048]	Loss: 1.5957
Training Epoch: 13 [8960/50048]	Loss: 1.5296
Training Epoch: 13 [9088/50048]	Loss: 1.7571
Training Epoch: 13 [9216/50048]	Loss: 1.4416
Training Epoch: 13 [9344/50048]	Loss: 1.8603
Training Epoch: 13 [9472/50048]	Loss: 1.5782
Training Epoch: 13 [9600/50048]	Loss: 1.8606
Training Epoch: 13 [9728/50048]	Loss: 1.4107
Training Epoch: 13 [9856/50048]	Loss: 1.7190
Training Epoch: 13 [9984/50048]	Loss: 1.6691
Training Epoch: 13 [10112/50048]	Loss: 1.7968
Training Epoch: 13 [10240/50048]	Loss: 1.7985
Training Epoch: 13 [10368/50048]	Loss: 1.5274
Training Epoch: 13 [10496/50048]	Loss: 1.4388
Training Epoch: 13 [10624/50048]	Loss: 1.5687
Training Epoch: 13 [10752/50048]	Loss: 1.5659
Training Epoch: 13 [10880/50048]	Loss: 1.5039
Training Epoch: 13 [11008/50048]	Loss: 1.6906
Training Epoch: 13 [11136/50048]	Loss: 1.6204
Training Epoch: 13 [11264/50048]	Loss: 1.9999
Training Epoch: 13 [11392/50048]	Loss: 1.5159
Training Epoch: 13 [11520/50048]	Loss: 1.9544
Training Epoch: 13 [11648/50048]	Loss: 1.6656
Training Epoch: 13 [11776/50048]	Loss: 1.7018
Training Epoch: 13 [11904/50048]	Loss: 1.5418
Training Epoch: 13 [12032/50048]	Loss: 1.4290
Training Epoch: 13 [12160/50048]	Loss: 1.6787
Training Epoch: 13 [12288/50048]	Loss: 1.8583
Training Epoch: 13 [12416/50048]	Loss: 1.7045
Training Epoch: 13 [12544/50048]	Loss: 1.7665
Training Epoch: 13 [12672/50048]	Loss: 1.6643
Training Epoch: 13 [12800/50048]	Loss: 1.5914
Training Epoch: 13 [12928/50048]	Loss: 1.5949
Training Epoch: 13 [13056/50048]	Loss: 1.6427
Training Epoch: 13 [13184/50048]	Loss: 1.6572
Training Epoch: 13 [13312/50048]	Loss: 1.7027
Training Epoch: 13 [13440/50048]	Loss: 1.8202
Training Epoch: 13 [13568/50048]	Loss: 1.5038
Training Epoch: 13 [13696/50048]	Loss: 1.5303
Training Epoch: 13 [13824/50048]	Loss: 1.4695
Training Epoch: 13 [13952/50048]	Loss: 1.5837
Training Epoch: 13 [14080/50048]	Loss: 1.5815
Training Epoch: 13 [14208/50048]	Loss: 1.5411
Training Epoch: 13 [14336/50048]	Loss: 1.5786
Training Epoch: 13 [14464/50048]	Loss: 1.7156
Training Epoch: 13 [14592/50048]	Loss: 1.5317
Training Epoch: 13 [14720/50048]	Loss: 1.8003
Training Epoch: 13 [14848/50048]	Loss: 1.5388
Training Epoch: 13 [14976/50048]	Loss: 1.6373
Training Epoch: 13 [15104/50048]	Loss: 1.4757
Training Epoch: 13 [15232/50048]	Loss: 1.3519
Training Epoch: 13 [15360/50048]	Loss: 1.7945
Training Epoch: 13 [15488/50048]	Loss: 1.7486
Training Epoch: 13 [15616/50048]	Loss: 1.4491
Training Epoch: 13 [15744/50048]	Loss: 1.7755
Training Epoch: 13 [15872/50048]	Loss: 1.7663
Training Epoch: 13 [16000/50048]	Loss: 1.6081
Training Epoch: 13 [16128/50048]	Loss: 1.5242
Training Epoch: 13 [16256/50048]	Loss: 1.8590
Training Epoch: 13 [16384/50048]	Loss: 1.4982
Training Epoch: 13 [16512/50048]	Loss: 1.5792
Training Epoch: 13 [16640/50048]	Loss: 1.6464
Training Epoch: 13 [16768/50048]	Loss: 1.6799
Training Epoch: 13 [16896/50048]	Loss: 1.6221
Training Epoch: 13 [17024/50048]	Loss: 1.4453
Training Epoch: 13 [17152/50048]	Loss: 2.0350
Training Epoch: 13 [17280/50048]	Loss: 1.7917
Training Epoch: 13 [17408/50048]	Loss: 1.5381
Training Epoch: 13 [17536/50048]	Loss: 1.7182
Training Epoch: 13 [17664/50048]	Loss: 1.6527
Training Epoch: 13 [17792/50048]	Loss: 1.5516
Training Epoch: 13 [17920/50048]	Loss: 1.6206
Training Epoch: 13 [18048/50048]	Loss: 1.6637
Training Epoch: 13 [18176/50048]	Loss: 1.6950
Training Epoch: 13 [18304/50048]	Loss: 1.6787
Training Epoch: 13 [18432/50048]	Loss: 1.3785
Training Epoch: 13 [18560/50048]	Loss: 1.4477
Training Epoch: 13 [18688/50048]	Loss: 1.6800
Training Epoch: 13 [18816/50048]	Loss: 1.5028
Training Epoch: 13 [18944/50048]	Loss: 1.5405
Training Epoch: 13 [19072/50048]	Loss: 1.7442
Training Epoch: 13 [19200/50048]	Loss: 1.4959
Training Epoch: 13 [19328/50048]	Loss: 1.4782
Training Epoch: 13 [19456/50048]	Loss: 1.8553
Training Epoch: 13 [19584/50048]	Loss: 1.7731
Training Epoch: 13 [19712/50048]	Loss: 1.5019
Training Epoch: 13 [19840/50048]	Loss: 1.6403
Training Epoch: 13 [19968/50048]	Loss: 1.4283
Training Epoch: 13 [20096/50048]	Loss: 1.3822
Training Epoch: 13 [20224/50048]	Loss: 1.5849
Training Epoch: 13 [20352/50048]	Loss: 1.4861
Training Epoch: 13 [20480/50048]	Loss: 2.0953
Training Epoch: 13 [20608/50048]	Loss: 1.5467
Training Epoch: 13 [20736/50048]	Loss: 1.6537
Training Epoch: 13 [20864/50048]	Loss: 1.6690
Training Epoch: 13 [20992/50048]	Loss: 1.7302
Training Epoch: 13 [21120/50048]	Loss: 1.6147
Training Epoch: 13 [21248/50048]	Loss: 2.0219
Training Epoch: 13 [21376/50048]	Loss: 1.9616
Training Epoch: 13 [21504/50048]	Loss: 1.6570
Training Epoch: 13 [21632/50048]	Loss: 1.5718
Training Epoch: 13 [21760/50048]	Loss: 1.9010
Training Epoch: 13 [21888/50048]	Loss: 1.6238
Training Epoch: 13 [22016/50048]	Loss: 1.5057
Training Epoch: 13 [22144/50048]	Loss: 1.4314
Training Epoch: 13 [22272/50048]	Loss: 1.5682
Training Epoch: 13 [22400/50048]	Loss: 1.5033
Training Epoch: 13 [22528/50048]	Loss: 1.6075
Training Epoch: 13 [22656/50048]	Loss: 1.4955
Training Epoch: 13 [22784/50048]	Loss: 1.5469
Training Epoch: 13 [22912/50048]	Loss: 1.5646
Training Epoch: 13 [23040/50048]	Loss: 1.6174
Training Epoch: 13 [23168/50048]	Loss: 1.5888
Training Epoch: 13 [23296/50048]	Loss: 1.6531
Training Epoch: 13 [23424/50048]	Loss: 1.5367
Training Epoch: 13 [23552/50048]	Loss: 1.5968
Training Epoch: 13 [23680/50048]	Loss: 1.7274
Training Epoch: 13 [23808/50048]	Loss: 1.7074
Training Epoch: 13 [23936/50048]	Loss: 1.7348
Training Epoch: 13 [24064/50048]	Loss: 1.7203
Training Epoch: 13 [24192/50048]	Loss: 1.6337
Training Epoch: 13 [24320/50048]	Loss: 1.6996
Training Epoch: 13 [24448/50048]	Loss: 1.6979
Training Epoch: 13 [24576/50048]	Loss: 1.6705
Training Epoch: 13 [24704/50048]	Loss: 1.4946
Training Epoch: 13 [24832/50048]	Loss: 1.5988
Training Epoch: 13 [24960/50048]	Loss: 1.6900
Training Epoch: 13 [25088/50048]	Loss: 1.4527
Training Epoch: 13 [25216/50048]	Loss: 1.4307
Training Epoch: 13 [25344/50048]	Loss: 1.6645
Training Epoch: 13 [25472/50048]	Loss: 1.6688
Training Epoch: 13 [25600/50048]	Loss: 1.5242
Training Epoch: 13 [25728/50048]	Loss: 1.3688
Training Epoch: 13 [25856/50048]	Loss: 1.7133
Training Epoch: 13 [25984/50048]	Loss: 1.5447
Training Epoch: 13 [26112/50048]	Loss: 1.7557
Training Epoch: 13 [26240/50048]	Loss: 1.5643
Training Epoch: 13 [26368/50048]	Loss: 1.3376
Training Epoch: 13 [26496/50048]	Loss: 1.6543
Training Epoch: 13 [26624/50048]	Loss: 1.7340
Training Epoch: 13 [26752/50048]	Loss: 1.6514
Training Epoch: 13 [26880/50048]	Loss: 1.5452
Training Epoch: 13 [27008/50048]	Loss: 1.7032
Training Epoch: 13 [27136/50048]	Loss: 1.7079
Training Epoch: 13 [27264/50048]	Loss: 1.6718
Training Epoch: 13 [27392/50048]	Loss: 1.6195
Training Epoch: 13 [27520/50048]	Loss: 1.9215
Training Epoch: 13 [27648/50048]	Loss: 1.8288
Training Epoch: 13 [27776/50048]	Loss: 1.8219
Training Epoch: 13 [27904/50048]	Loss: 1.7097
Training Epoch: 13 [28032/50048]	Loss: 1.5509
Training Epoch: 13 [28160/50048]	Loss: 1.7984
Training Epoch: 13 [28288/50048]	Loss: 1.8017
Training Epoch: 13 [28416/50048]	Loss: 1.9363
Training Epoch: 13 [28544/50048]	Loss: 1.6368
Training Epoch: 13 [28672/50048]	Loss: 1.5445
Training Epoch: 13 [28800/50048]	Loss: 1.8657
Training Epoch: 13 [28928/50048]	Loss: 1.8434
Training Epoch: 13 [29056/50048]	Loss: 1.7490
Training Epoch: 13 [29184/50048]	Loss: 1.7639
Training Epoch: 13 [29312/50048]	Loss: 1.9062
Training Epoch: 13 [29440/50048]	Loss: 1.3765
Training Epoch: 13 [29568/50048]	Loss: 1.6252
Training Epoch: 13 [29696/50048]	Loss: 1.5884
Training Epoch: 13 [29824/50048]	Loss: 1.9978
Training Epoch: 13 [29952/50048]	Loss: 1.8015
Training Epoch: 13 [30080/50048]	Loss: 1.3939
Training Epoch: 13 [30208/50048]	Loss: 1.6578
Training Epoch: 13 [30336/50048]	Loss: 1.5186
Training Epoch: 13 [30464/50048]	Loss: 1.8062
Training Epoch: 13 [30592/50048]	Loss: 1.6534
Training Epoch: 13 [30720/50048]	Loss: 1.5871
Training Epoch: 13 [30848/50048]	Loss: 1.8359
Training Epoch: 13 [30976/50048]	Loss: 1.5927
Training Epoch: 13 [31104/50048]	Loss: 1.7918
Training Epoch: 13 [31232/50048]	Loss: 1.7891
Training Epoch: 13 [31360/50048]	Loss: 1.6034
Training Epoch: 13 [31488/50048]	Loss: 1.6762
Training Epoch: 13 [31616/50048]	Loss: 1.7749
Training Epoch: 13 [31744/50048]	Loss: 1.8943
Training Epoch: 13 [31872/50048]	Loss: 1.8370
Training Epoch: 13 [32000/50048]	Loss: 1.4399
Training Epoch: 13 [32128/50048]	Loss: 1.5558
Training Epoch: 13 [32256/50048]	Loss: 1.9344
Training Epoch: 13 [32384/50048]	Loss: 1.7420
Training Epoch: 13 [32512/50048]	Loss: 1.8678
Training Epoch: 13 [32640/50048]	Loss: 1.4003
Training Epoch: 13 [32768/50048]	Loss: 2.0510
Training Epoch: 13 [32896/50048]	Loss: 1.7280
Training Epoch: 13 [33024/50048]	Loss: 1.6036
Training Epoch: 13 [33152/50048]	Loss: 1.8091
Training Epoch: 13 [33280/50048]	Loss: 1.7535
Training Epoch: 13 [33408/50048]	Loss: 1.4084
Training Epoch: 13 [33536/50048]	Loss: 1.5215
Training Epoch: 13 [33664/50048]	Loss: 1.7119
Training Epoch: 13 [33792/50048]	Loss: 1.4591
Training Epoch: 13 [33920/50048]	Loss: 1.7645
Training Epoch: 13 [34048/50048]	Loss: 1.8057
Training Epoch: 13 [34176/50048]	Loss: 1.6455
Training Epoch: 13 [34304/50048]	Loss: 1.8341
Training Epoch: 13 [34432/50048]	Loss: 1.5366
Training Epoch: 13 [34560/50048]	Loss: 1.8557
Training Epoch: 13 [34688/50048]	Loss: 1.4654
Training Epoch: 13 [34816/50048]	Loss: 1.7461
Training Epoch: 13 [34944/50048]	Loss: 1.6477
Training Epoch: 13 [35072/50048]	Loss: 1.6911
Training Epoch: 13 [35200/50048]	Loss: 1.8713
Training Epoch: 13 [35328/50048]	Loss: 1.8092
Training Epoch: 13 [35456/50048]	Loss: 1.5811
Training Epoch: 13 [35584/50048]	Loss: 1.5189
Training Epoch: 13 [35712/50048]	Loss: 1.7987
Training Epoch: 13 [35840/50048]	Loss: 1.6301
Training Epoch: 13 [35968/50048]	Loss: 1.4869
Training Epoch: 13 [36096/50048]	Loss: 1.5967
Training Epoch: 13 [36224/50048]	Loss: 1.5556
Training Epoch: 13 [36352/50048]	Loss: 1.6844
Training Epoch: 13 [36480/50048]	Loss: 1.6033
Training Epoch: 13 [36608/50048]	Loss: 1.5048
Training Epoch: 13 [36736/50048]	Loss: 1.5258
Training Epoch: 13 [36864/50048]	Loss: 1.6506
Training Epoch: 13 [36992/50048]	Loss: 1.6031
Training Epoch: 13 [37120/50048]	Loss: 1.7135
Training Epoch: 13 [37248/50048]	Loss: 1.6093
Training Epoch: 13 [37376/50048]	Loss: 1.5240
Training Epoch: 13 [37504/50048]	Loss: 1.4939
Training Epoch: 13 [37632/50048]	Loss: 1.5689
Training Epoch: 13 [37760/50048]	Loss: 1.5890
Training Epoch: 13 [37888/50048]	Loss: 1.4629
Training Epoch: 13 [38016/50048]	Loss: 1.4735
Training Epoch: 13 [38144/50048]	Loss: 1.7819
Training Epoch: 13 [38272/50048]	Loss: 1.4431
Training Epoch: 13 [38400/50048]	Loss: 1.4476
Training Epoch: 13 [38528/50048]	Loss: 1.6412
Training Epoch: 13 [38656/50048]	Loss: 1.7035
Training Epoch: 13 [38784/50048]	Loss: 1.8615
Training Epoch: 13 [38912/50048]	Loss: 1.7865
Training Epoch: 13 [39040/50048]	Loss: 1.6118
Training Epoch: 13 [39168/50048]	Loss: 1.5456
Training Epoch: 13 [39296/50048]	Loss: 1.3667
Training Epoch: 13 [39424/50048]	Loss: 1.7742
Training Epoch: 13 [39552/50048]	Loss: 1.5350
Training Epoch: 13 [39680/50048]	Loss: 1.4223
Training Epoch: 13 [39808/50048]	Loss: 1.5024
Training Epoch: 13 [39936/50048]	Loss: 1.6226
Training Epoch: 13 [40064/50048]	Loss: 1.7385
Training Epoch: 13 [40192/50048]	Loss: 1.8854
Training Epoch: 13 [40320/50048]	Loss: 1.6871
Training Epoch: 13 [40448/50048]	Loss: 1.5257
Training Epoch: 13 [40576/50048]	Loss: 1.7691
Training Epoch: 13 [40704/50048]	Loss: 1.6039
Training Epoch: 13 [40832/50048]	Loss: 1.6501
Training Epoch: 13 [40960/50048]	Loss: 1.9545
Training Epoch: 13 [41088/50048]	Loss: 1.6061
Training Epoch: 13 [41216/50048]	Loss: 1.4519
Training Epoch: 13 [41344/50048]	Loss: 1.7222
Training Epoch: 13 [41472/50048]	Loss: 1.5528
Training Epoch: 13 [41600/50048]	Loss: 1.6990
Training Epoch: 13 [41728/50048]	Loss: 1.9499
Training Epoch: 13 [41856/50048]	Loss: 1.6599
Training Epoch: 13 [41984/50048]	Loss: 1.7823
Training Epoch: 13 [42112/50048]	Loss: 1.7691
Training Epoch: 13 [42240/50048]	Loss: 1.9386
Training Epoch: 13 [42368/50048]	Loss: 1.9026
Training Epoch: 13 [42496/50048]	Loss: 1.5389
Training Epoch: 13 [42624/50048]	Loss: 1.5859
Training Epoch: 13 [42752/50048]	Loss: 1.7749
Training Epoch: 13 [42880/50048]	Loss: 1.4198
Training Epoch: 13 [43008/50048]	Loss: 1.7293
Training Epoch: 13 [43136/50048]	Loss: 1.7913
Training Epoch: 13 [43264/50048]	Loss: 1.6058
Training Epoch: 13 [43392/50048]	Loss: 1.4373
Training Epoch: 13 [43520/50048]	Loss: 1.6161
Training Epoch: 13 [43648/50048]	Loss: 1.5598
Training Epoch: 13 [43776/50048]	Loss: 1.7348
Training Epoch: 13 [43904/50048]	Loss: 1.5786
Training Epoch: 13 [44032/50048]	Loss: 1.7469
Training Epoch: 13 [44160/50048]	Loss: 1.4476
Training Epoch: 13 [44288/50048]	Loss: 1.7453
Training Epoch: 13 [44416/50048]	Loss: 1.5995
Training Epoch: 13 [44544/50048]	Loss: 1.4156
Training Epoch: 13 [44672/50048]	Loss: 1.7152
Training Epoch: 13 [44800/50048]	Loss: 1.8688
Training Epoch: 13 [44928/50048]	Loss: 1.6070
Training Epoch: 13 [45056/50048]	Loss: 1.5621
Training Epoch: 13 [45184/50048]	Loss: 1.7632
Training Epoch: 13 [45312/50048]	Loss: 1.5676
Training Epoch: 13 [45440/50048]	Loss: 1.5245
Training Epoch: 13 [45568/50048]	Loss: 1.5059
Training Epoch: 13 [45696/50048]	Loss: 1.6944
Training Epoch: 13 [45824/50048]	Loss: 1.7181
Training Epoch: 13 [45952/50048]	Loss: 1.6790
Training Epoch: 13 [46080/50048]	Loss: 1.7240
Training Epoch: 13 [46208/50048]	Loss: 1.4958
Training Epoch: 13 [46336/50048]	Loss: 1.4816
Training Epoch: 13 [46464/50048]	Loss: 1.8183
Training Epoch: 13 [46592/50048]	Loss: 1.5330
Training Epoch: 13 [46720/50048]	Loss: 1.5162
Training Epoch: 13 [46848/50048]	Loss: 1.3850
Training Epoch: 13 [46976/50048]	Loss: 1.6212
Training Epoch: 13 [47104/50048]	Loss: 1.6520
Training Epoch: 13 [47232/50048]	Loss: 1.6888
Training Epoch: 13 [47360/50048]	Loss: 1.6626
Training Epoch: 13 [47488/50048]	Loss: 1.6212
Training Epoch: 13 [47616/50048]	Loss: 1.7174
Training Epoch: 13 [47744/50048]	Loss: 1.6546
Training Epoch: 13 [47872/50048]	Loss: 1.6569
Training Epoch: 13 [48000/50048]	Loss: 1.6912
Training Epoch: 13 [48128/50048]	Loss: 1.8418
Training Epoch: 13 [48256/50048]	Loss: 1.7537
Training Epoch: 13 [48384/50048]	Loss: 1.7853
Training Epoch: 13 [48512/50048]	Loss: 1.4678
Training Epoch: 13 [48640/50048]	Loss: 1.6427
Training Epoch: 13 [48768/50048]	Loss: 1.7398
Training Epoch: 13 [48896/50048]	Loss: 1.6408
Training Epoch: 13 [49024/50048]	Loss: 1.5234
Training Epoch: 13 [49152/50048]	Loss: 1.8034
Training Epoch: 13 [49280/50048]	Loss: 1.3842
Training Epoch: 13 [49408/50048]	Loss: 1.4746
Training Epoch: 13 [49536/50048]	Loss: 1.8974
Training Epoch: 13 [49664/50048]	Loss: 1.6357
Training Epoch: 13 [49792/50048]	Loss: 1.5270
Training Epoch: 13 [49920/50048]	Loss: 1.5482
Training Epoch: 13 [50048/50048]	Loss: 1.5371
Validation Epoch: 13, Average loss: 0.0135, Accuracy: 0.5245
Training Epoch: 14 [128/50048]	Loss: 1.7689
Training Epoch: 14 [256/50048]	Loss: 1.7113
Training Epoch: 14 [384/50048]	Loss: 1.5398
Training Epoch: 14 [512/50048]	Loss: 1.6964
Training Epoch: 14 [640/50048]	Loss: 1.5093
Training Epoch: 14 [768/50048]	Loss: 1.5083
Training Epoch: 14 [896/50048]	Loss: 1.8530
Training Epoch: 14 [1024/50048]	Loss: 1.6402
Training Epoch: 14 [1152/50048]	Loss: 1.3214
Training Epoch: 14 [1280/50048]	Loss: 1.6388
Training Epoch: 14 [1408/50048]	Loss: 1.6518
Training Epoch: 14 [1536/50048]	Loss: 1.6112
Training Epoch: 14 [1664/50048]	Loss: 1.6210
Training Epoch: 14 [1792/50048]	Loss: 1.5377
Training Epoch: 14 [1920/50048]	Loss: 1.7800
Training Epoch: 14 [2048/50048]	Loss: 1.5283
Training Epoch: 14 [2176/50048]	Loss: 1.5462
Training Epoch: 14 [2304/50048]	Loss: 1.5009
Training Epoch: 14 [2432/50048]	Loss: 1.6691
Training Epoch: 14 [2560/50048]	Loss: 1.5517
Training Epoch: 14 [2688/50048]	Loss: 1.4788
Training Epoch: 14 [2816/50048]	Loss: 1.3911
Training Epoch: 14 [2944/50048]	Loss: 1.5583
Training Epoch: 14 [3072/50048]	Loss: 1.3845
Training Epoch: 14 [3200/50048]	Loss: 1.5364
Training Epoch: 14 [3328/50048]	Loss: 1.3875
Training Epoch: 14 [3456/50048]	Loss: 1.3821
Training Epoch: 14 [3584/50048]	Loss: 1.5377
Training Epoch: 14 [3712/50048]	Loss: 1.8274
Training Epoch: 14 [3840/50048]	Loss: 1.6555
Training Epoch: 14 [3968/50048]	Loss: 1.6899
Training Epoch: 14 [4096/50048]	Loss: 1.5287
Training Epoch: 14 [4224/50048]	Loss: 1.6021
Training Epoch: 14 [4352/50048]	Loss: 1.5813
Training Epoch: 14 [4480/50048]	Loss: 1.2162
Training Epoch: 14 [4608/50048]	Loss: 1.5062
Training Epoch: 14 [4736/50048]	Loss: 1.7255
Training Epoch: 14 [4864/50048]	Loss: 1.8567
Training Epoch: 14 [4992/50048]	Loss: 1.4194
Training Epoch: 14 [5120/50048]	Loss: 1.5312
Training Epoch: 14 [5248/50048]	Loss: 1.7492
Training Epoch: 14 [5376/50048]	Loss: 1.6340
Training Epoch: 14 [5504/50048]	Loss: 1.6545
Training Epoch: 14 [5632/50048]	Loss: 1.5181
Training Epoch: 14 [5760/50048]	Loss: 1.6153
Training Epoch: 14 [5888/50048]	Loss: 1.8733
Training Epoch: 14 [6016/50048]	Loss: 1.8073
Training Epoch: 14 [6144/50048]	Loss: 1.5176
Training Epoch: 14 [6272/50048]	Loss: 1.5580
Training Epoch: 14 [6400/50048]	Loss: 1.5096
Training Epoch: 14 [6528/50048]	Loss: 1.5865
Training Epoch: 14 [6656/50048]	Loss: 1.3998
Training Epoch: 14 [6784/50048]	Loss: 1.5914
Training Epoch: 14 [6912/50048]	Loss: 1.7146
Training Epoch: 14 [7040/50048]	Loss: 1.5184
Training Epoch: 14 [7168/50048]	Loss: 1.7003
Training Epoch: 14 [7296/50048]	Loss: 1.6229
Training Epoch: 14 [7424/50048]	Loss: 1.5341
Training Epoch: 14 [7552/50048]	Loss: 1.3899
Training Epoch: 14 [7680/50048]	Loss: 1.5702
Training Epoch: 14 [7808/50048]	Loss: 1.5218
Training Epoch: 14 [7936/50048]	Loss: 1.3695
Training Epoch: 14 [8064/50048]	Loss: 1.4317
Training Epoch: 14 [8192/50048]	Loss: 1.5810
Training Epoch: 14 [8320/50048]	Loss: 1.5702
Training Epoch: 14 [8448/50048]	Loss: 1.6356
Training Epoch: 14 [8576/50048]	Loss: 1.6135
Training Epoch: 14 [8704/50048]	Loss: 1.5266
Training Epoch: 14 [8832/50048]	Loss: 1.7251
Training Epoch: 14 [8960/50048]	Loss: 1.4281
Training Epoch: 14 [9088/50048]	Loss: 1.5875
Training Epoch: 14 [9216/50048]	Loss: 1.7068
Training Epoch: 14 [9344/50048]	Loss: 1.2817
Training Epoch: 14 [9472/50048]	Loss: 1.4904
Training Epoch: 14 [9600/50048]	Loss: 1.3971
Training Epoch: 14 [9728/50048]	Loss: 1.7229
Training Epoch: 14 [9856/50048]	Loss: 1.7419
Training Epoch: 14 [9984/50048]	Loss: 1.8487
Training Epoch: 14 [10112/50048]	Loss: 1.3262
Training Epoch: 14 [10240/50048]	Loss: 1.6442
Training Epoch: 14 [10368/50048]	Loss: 1.8701
Training Epoch: 14 [10496/50048]	Loss: 1.8919
Training Epoch: 14 [10624/50048]	Loss: 1.4855
Training Epoch: 14 [10752/50048]	Loss: 1.6138
Training Epoch: 14 [10880/50048]	Loss: 1.6661
Training Epoch: 14 [11008/50048]	Loss: 1.5296
Training Epoch: 14 [11136/50048]	Loss: 1.7345
Training Epoch: 14 [11264/50048]	Loss: 1.5644
Training Epoch: 14 [11392/50048]	Loss: 1.4652
Training Epoch: 14 [11520/50048]	Loss: 1.7323
Training Epoch: 14 [11648/50048]	Loss: 1.3741
Training Epoch: 14 [11776/50048]	Loss: 1.6313
Training Epoch: 14 [11904/50048]	Loss: 1.4888
Training Epoch: 14 [12032/50048]	Loss: 1.4909
Training Epoch: 14 [12160/50048]	Loss: 1.4365
Training Epoch: 14 [12288/50048]	Loss: 1.3713
Training Epoch: 14 [12416/50048]	Loss: 1.7585
Training Epoch: 14 [12544/50048]	Loss: 1.4134
Training Epoch: 14 [12672/50048]	Loss: 1.5292
Training Epoch: 14 [12800/50048]	Loss: 1.6845
Training Epoch: 14 [12928/50048]	Loss: 1.6456
Training Epoch: 14 [13056/50048]	Loss: 1.5613
Training Epoch: 14 [13184/50048]	Loss: 1.4028
Training Epoch: 14 [13312/50048]	Loss: 1.3583
Training Epoch: 14 [13440/50048]	Loss: 1.6243
Training Epoch: 14 [13568/50048]	Loss: 1.5078
Training Epoch: 14 [13696/50048]	Loss: 1.7188
Training Epoch: 14 [13824/50048]	Loss: 1.3432
Training Epoch: 14 [13952/50048]	Loss: 1.7362
Training Epoch: 14 [14080/50048]	Loss: 1.4993
Training Epoch: 14 [14208/50048]	Loss: 1.6603
Training Epoch: 14 [14336/50048]	Loss: 1.6320
Training Epoch: 14 [14464/50048]	Loss: 1.8798
Training Epoch: 14 [14592/50048]	Loss: 1.5509
Training Epoch: 14 [14720/50048]	Loss: 1.6511
Training Epoch: 14 [14848/50048]	Loss: 1.6830
Training Epoch: 14 [14976/50048]	Loss: 1.8009
Training Epoch: 14 [15104/50048]	Loss: 1.6835
Training Epoch: 14 [15232/50048]	Loss: 1.6055
Training Epoch: 14 [15360/50048]	Loss: 1.3690
Training Epoch: 14 [15488/50048]	Loss: 1.7429
Training Epoch: 14 [15616/50048]	Loss: 1.8766
Training Epoch: 14 [15744/50048]	Loss: 1.5794
Training Epoch: 14 [15872/50048]	Loss: 1.6732
Training Epoch: 14 [16000/50048]	Loss: 1.7288
Training Epoch: 14 [16128/50048]	Loss: 1.5380
Training Epoch: 14 [16256/50048]	Loss: 1.5274
Training Epoch: 14 [16384/50048]	Loss: 1.6111
Training Epoch: 14 [16512/50048]	Loss: 1.3884
Training Epoch: 14 [16640/50048]	Loss: 1.5565
Training Epoch: 14 [16768/50048]	Loss: 1.5684
Training Epoch: 14 [16896/50048]	Loss: 1.5272
Training Epoch: 14 [17024/50048]	Loss: 1.7139
Training Epoch: 14 [17152/50048]	Loss: 1.6303
Training Epoch: 14 [17280/50048]	Loss: 1.6108
Training Epoch: 14 [17408/50048]	Loss: 1.4819
Training Epoch: 14 [17536/50048]	Loss: 1.5593
Training Epoch: 14 [17664/50048]	Loss: 1.5688
Training Epoch: 14 [17792/50048]	Loss: 1.6608
Training Epoch: 14 [17920/50048]	Loss: 1.6320
Training Epoch: 14 [18048/50048]	Loss: 1.6305
Training Epoch: 14 [18176/50048]	Loss: 1.4254
Training Epoch: 14 [18304/50048]	Loss: 1.6361
Training Epoch: 14 [18432/50048]	Loss: 1.5605
Training Epoch: 14 [18560/50048]	Loss: 1.7424
Training Epoch: 14 [18688/50048]	Loss: 1.5890
Training Epoch: 14 [18816/50048]	Loss: 1.6241
Training Epoch: 14 [18944/50048]	Loss: 1.7223
Training Epoch: 14 [19072/50048]	Loss: 1.2884
Training Epoch: 14 [19200/50048]	Loss: 1.5762
Training Epoch: 14 [19328/50048]	Loss: 1.4384
Training Epoch: 14 [19456/50048]	Loss: 1.7610
Training Epoch: 14 [19584/50048]	Loss: 1.8103
Training Epoch: 14 [19712/50048]	Loss: 1.5495
Training Epoch: 14 [19840/50048]	Loss: 1.3773
Training Epoch: 14 [19968/50048]	Loss: 1.5306
Training Epoch: 14 [20096/50048]	Loss: 1.7057
Training Epoch: 14 [20224/50048]	Loss: 1.3797
Training Epoch: 14 [20352/50048]	Loss: 1.5968
Training Epoch: 14 [20480/50048]	Loss: 1.6547
Training Epoch: 14 [20608/50048]	Loss: 1.5129
Training Epoch: 14 [20736/50048]	Loss: 1.6454
Training Epoch: 14 [20864/50048]	Loss: 1.5497
Training Epoch: 14 [20992/50048]	Loss: 1.6302
Training Epoch: 14 [21120/50048]	Loss: 1.7575
Training Epoch: 14 [21248/50048]	Loss: 1.2527
Training Epoch: 14 [21376/50048]	Loss: 1.5914
Training Epoch: 14 [21504/50048]	Loss: 1.4346
Training Epoch: 14 [21632/50048]	Loss: 1.3877
Training Epoch: 14 [21760/50048]	Loss: 1.5532
Training Epoch: 14 [21888/50048]	Loss: 1.7974
Training Epoch: 14 [22016/50048]	Loss: 1.6257
Training Epoch: 14 [22144/50048]	Loss: 1.4397
Training Epoch: 14 [22272/50048]	Loss: 1.5128
Training Epoch: 14 [22400/50048]	Loss: 1.5873
Training Epoch: 14 [22528/50048]	Loss: 1.4422
Training Epoch: 14 [22656/50048]	Loss: 1.4911
Training Epoch: 14 [22784/50048]	Loss: 1.4487
Training Epoch: 14 [22912/50048]	Loss: 1.5767
Training Epoch: 14 [23040/50048]	Loss: 1.6393
Training Epoch: 14 [23168/50048]	Loss: 1.6082
Training Epoch: 14 [23296/50048]	Loss: 1.5120
Training Epoch: 14 [23424/50048]	Loss: 1.6203
Training Epoch: 14 [23552/50048]	Loss: 1.6409
Training Epoch: 14 [23680/50048]	Loss: 1.7189
Training Epoch: 14 [23808/50048]	Loss: 1.3672
Training Epoch: 14 [23936/50048]	Loss: 1.4782
Training Epoch: 14 [24064/50048]	Loss: 1.4379
Training Epoch: 14 [24192/50048]	Loss: 1.5609
Training Epoch: 14 [24320/50048]	Loss: 1.5865
Training Epoch: 14 [24448/50048]	Loss: 1.4633
Training Epoch: 14 [24576/50048]	Loss: 1.7010
Training Epoch: 14 [24704/50048]	Loss: 1.4461
Training Epoch: 14 [24832/50048]	Loss: 1.6822
Training Epoch: 14 [24960/50048]	Loss: 1.4660
Training Epoch: 14 [25088/50048]	Loss: 1.7578
Training Epoch: 14 [25216/50048]	Loss: 1.6460
Training Epoch: 14 [25344/50048]	Loss: 1.7666
Training Epoch: 14 [25472/50048]	Loss: 1.6528
Training Epoch: 14 [25600/50048]	Loss: 1.5595
Training Epoch: 14 [25728/50048]	Loss: 1.5039
Training Epoch: 14 [25856/50048]	Loss: 1.6015
Training Epoch: 14 [25984/50048]	Loss: 1.7290
Training Epoch: 14 [26112/50048]	Loss: 1.8156
Training Epoch: 14 [26240/50048]	Loss: 1.6391
Training Epoch: 14 [26368/50048]	Loss: 1.3924
Training Epoch: 14 [26496/50048]	Loss: 1.6202
Training Epoch: 14 [26624/50048]	Loss: 1.6106
Training Epoch: 14 [26752/50048]	Loss: 1.8309
Training Epoch: 14 [26880/50048]	Loss: 1.4826
Training Epoch: 14 [27008/50048]	Loss: 1.6421
Training Epoch: 14 [27136/50048]	Loss: 1.4902
Training Epoch: 14 [27264/50048]	Loss: 1.6308
Training Epoch: 14 [27392/50048]	Loss: 1.7151
Training Epoch: 14 [27520/50048]	Loss: 1.5487
Training Epoch: 14 [27648/50048]	Loss: 1.5910
Training Epoch: 14 [27776/50048]	Loss: 1.5391
Training Epoch: 14 [27904/50048]	Loss: 1.5541
Training Epoch: 14 [28032/50048]	Loss: 1.6351
Training Epoch: 14 [28160/50048]	Loss: 1.5850
Training Epoch: 14 [28288/50048]	Loss: 1.6267
Training Epoch: 14 [28416/50048]	Loss: 1.6990
Training Epoch: 14 [28544/50048]	Loss: 1.6786
Training Epoch: 14 [28672/50048]	Loss: 1.5855
Training Epoch: 14 [28800/50048]	Loss: 1.5870
Training Epoch: 14 [28928/50048]	Loss: 1.4729
Training Epoch: 14 [29056/50048]	Loss: 1.7605
Training Epoch: 14 [29184/50048]	Loss: 1.7549
Training Epoch: 14 [29312/50048]	Loss: 1.8328
Training Epoch: 14 [29440/50048]	Loss: 1.4548
Training Epoch: 14 [29568/50048]	Loss: 1.7027
Training Epoch: 14 [29696/50048]	Loss: 1.5005
Training Epoch: 14 [29824/50048]	Loss: 1.4604
Training Epoch: 14 [29952/50048]	Loss: 1.5663
Training Epoch: 14 [30080/50048]	Loss: 1.8715
Training Epoch: 14 [30208/50048]	Loss: 1.7283
Training Epoch: 14 [30336/50048]	Loss: 1.5553
Training Epoch: 14 [30464/50048]	Loss: 1.6309
Training Epoch: 14 [30592/50048]	Loss: 1.4348
Training Epoch: 14 [30720/50048]	Loss: 1.5963
Training Epoch: 14 [30848/50048]	Loss: 1.3774
Training Epoch: 14 [30976/50048]	Loss: 1.4409
Training Epoch: 14 [31104/50048]	Loss: 1.7530
Training Epoch: 14 [31232/50048]	Loss: 1.6536
Training Epoch: 14 [31360/50048]	Loss: 1.9699
Training Epoch: 14 [31488/50048]	Loss: 1.4302
Training Epoch: 14 [31616/50048]	Loss: 1.5750
Training Epoch: 14 [31744/50048]	Loss: 1.6070
Training Epoch: 14 [31872/50048]	Loss: 1.7659
Training Epoch: 14 [32000/50048]	Loss: 1.5739
Training Epoch: 14 [32128/50048]	Loss: 1.5917
Training Epoch: 14 [32256/50048]	Loss: 1.5697
Training Epoch: 14 [32384/50048]	Loss: 1.5254
Training Epoch: 14 [32512/50048]	Loss: 1.6342
Training Epoch: 14 [32640/50048]	Loss: 1.4219
Training Epoch: 14 [32768/50048]	Loss: 1.5528
Training Epoch: 14 [32896/50048]	Loss: 1.5851
Training Epoch: 14 [33024/50048]	Loss: 1.5612
Training Epoch: 14 [33152/50048]	Loss: 1.4617
Training Epoch: 14 [33280/50048]	Loss: 1.3675
Training Epoch: 14 [33408/50048]	Loss: 1.7053
Training Epoch: 14 [33536/50048]	Loss: 1.7884
Training Epoch: 14 [33664/50048]	Loss: 1.7202
Training Epoch: 14 [33792/50048]	Loss: 1.3730
Training Epoch: 14 [33920/50048]	Loss: 1.7641
Training Epoch: 14 [34048/50048]	Loss: 1.8226
Training Epoch: 14 [34176/50048]	Loss: 1.6676
Training Epoch: 14 [34304/50048]	Loss: 1.7794
Training Epoch: 14 [34432/50048]	Loss: 1.3367
Training Epoch: 14 [34560/50048]	Loss: 1.7279
Training Epoch: 14 [34688/50048]	Loss: 1.6510
Training Epoch: 14 [34816/50048]	Loss: 1.5476
Training Epoch: 14 [34944/50048]	Loss: 1.6301
Training Epoch: 14 [35072/50048]	Loss: 1.8835
Training Epoch: 14 [35200/50048]	Loss: 1.6189
Training Epoch: 14 [35328/50048]	Loss: 1.3290
Training Epoch: 14 [35456/50048]	Loss: 1.6684
Training Epoch: 14 [35584/50048]	Loss: 1.5541
Training Epoch: 14 [35712/50048]	Loss: 1.5541
Training Epoch: 14 [35840/50048]	Loss: 1.8932
Training Epoch: 14 [35968/50048]	Loss: 1.6722
Training Epoch: 14 [36096/50048]	Loss: 1.6756
Training Epoch: 14 [36224/50048]	Loss: 1.6295
Training Epoch: 14 [36352/50048]	Loss: 1.6223
Training Epoch: 14 [36480/50048]	Loss: 1.5097
Training Epoch: 14 [36608/50048]	Loss: 1.6885
Training Epoch: 14 [36736/50048]	Loss: 1.7476
Training Epoch: 14 [36864/50048]	Loss: 1.4000
Training Epoch: 14 [36992/50048]	Loss: 1.7153
Training Epoch: 14 [37120/50048]	Loss: 1.6957
Training Epoch: 14 [37248/50048]	Loss: 1.3282
Training Epoch: 14 [37376/50048]	Loss: 1.5352
Training Epoch: 14 [37504/50048]	Loss: 1.7142
Training Epoch: 14 [37632/50048]	Loss: 1.7618
Training Epoch: 14 [37760/50048]	Loss: 1.6883
Training Epoch: 14 [37888/50048]	Loss: 1.5070
Training Epoch: 14 [38016/50048]	Loss: 1.5305
Training Epoch: 14 [38144/50048]	Loss: 1.5203
Training Epoch: 14 [38272/50048]	Loss: 1.7224
Training Epoch: 14 [38400/50048]	Loss: 1.4656
Training Epoch: 14 [38528/50048]	Loss: 1.6270
Training Epoch: 14 [38656/50048]	Loss: 1.7708
Training Epoch: 14 [38784/50048]	Loss: 1.5639
Training Epoch: 14 [38912/50048]	Loss: 1.7553
Training Epoch: 14 [39040/50048]	Loss: 1.5711
Training Epoch: 14 [39168/50048]	Loss: 1.6014
Training Epoch: 14 [39296/50048]	Loss: 1.5912
Training Epoch: 14 [39424/50048]	Loss: 1.8048
Training Epoch: 14 [39552/50048]	Loss: 1.6208
Training Epoch: 14 [39680/50048]	Loss: 1.5156
Training Epoch: 14 [39808/50048]	Loss: 1.6001
Training Epoch: 14 [39936/50048]	Loss: 1.4398
Training Epoch: 14 [40064/50048]	Loss: 1.4798
Training Epoch: 14 [40192/50048]	Loss: 1.5627
Training Epoch: 14 [40320/50048]	Loss: 1.5077
Training Epoch: 14 [40448/50048]	Loss: 1.5160
Training Epoch: 14 [40576/50048]	Loss: 1.4681
Training Epoch: 14 [40704/50048]	Loss: 1.6173
Training Epoch: 14 [40832/50048]	Loss: 1.2773
Training Epoch: 14 [40960/50048]	Loss: 1.6122
Training Epoch: 14 [41088/50048]	Loss: 1.6612
Training Epoch: 14 [41216/50048]	Loss: 1.7079
Training Epoch: 14 [41344/50048]	Loss: 1.6163
Training Epoch: 14 [41472/50048]	Loss: 1.4918
Training Epoch: 14 [41600/50048]	Loss: 1.6280
Training Epoch: 14 [41728/50048]	Loss: 1.5898
Training Epoch: 14 [41856/50048]	Loss: 1.3580
Training Epoch: 14 [41984/50048]	Loss: 1.7916
Training Epoch: 14 [42112/50048]	Loss: 1.6435
Training Epoch: 14 [42240/50048]	Loss: 1.6051
Training Epoch: 14 [42368/50048]	Loss: 1.7120
Training Epoch: 14 [42496/50048]	Loss: 1.5637
Training Epoch: 14 [42624/50048]	Loss: 1.5997
Training Epoch: 14 [42752/50048]	Loss: 1.5824
Training Epoch: 14 [42880/50048]	Loss: 1.7590
Training Epoch: 14 [43008/50048]	Loss: 1.8978
Training Epoch: 14 [43136/50048]	Loss: 1.7577
Training Epoch: 14 [43264/50048]	Loss: 1.4019
Training Epoch: 14 [43392/50048]	Loss: 1.5647
Training Epoch: 14 [43520/50048]	Loss: 1.4907
Training Epoch: 14 [43648/50048]	Loss: 1.5061
Training Epoch: 14 [43776/50048]	Loss: 1.6996
Training Epoch: 14 [43904/50048]	Loss: 1.5231
Training Epoch: 14 [44032/50048]	Loss: 1.7372
Training Epoch: 14 [44160/50048]	Loss: 1.7739
Training Epoch: 14 [44288/50048]	Loss: 1.7827
Training Epoch: 14 [44416/50048]	Loss: 1.8470
Training Epoch: 14 [44544/50048]	Loss: 1.6711
Training Epoch: 14 [44672/50048]	Loss: 1.4432
Training Epoch: 14 [44800/50048]	Loss: 1.5192
Training Epoch: 14 [44928/50048]	Loss: 1.6281
Training Epoch: 14 [45056/50048]	Loss: 1.8639
Training Epoch: 14 [45184/50048]	Loss: 1.8350
Training Epoch: 14 [45312/50048]	Loss: 1.5581
Training Epoch: 14 [45440/50048]	Loss: 1.7731
Training Epoch: 14 [45568/50048]	Loss: 1.5405
Training Epoch: 14 [45696/50048]	Loss: 1.8637
Training Epoch: 14 [45824/50048]	Loss: 1.7101
Training Epoch: 14 [45952/50048]	Loss: 1.6672
Training Epoch: 14 [46080/50048]	Loss: 1.7127
Training Epoch: 14 [46208/50048]	Loss: 1.3814
Training Epoch: 14 [46336/50048]	Loss: 1.4577
Training Epoch: 14 [46464/50048]	Loss: 1.6337
Training Epoch: 14 [46592/50048]	Loss: 1.4918
Training Epoch: 14 [46720/50048]	Loss: 1.4223
Training Epoch: 14 [46848/50048]	Loss: 1.8152
Training Epoch: 14 [46976/50048]	Loss: 1.8553
Training Epoch: 14 [47104/50048]	Loss: 1.6456
Training Epoch: 14 [47232/50048]	Loss: 1.7865
Training Epoch: 14 [47360/50048]	Loss: 1.6207
Training Epoch: 14 [47488/50048]	Loss: 1.4540
Training Epoch: 14 [47616/50048]	Loss: 1.5976
Training Epoch: 14 [47744/50048]	Loss: 1.5953
Training Epoch: 14 [47872/50048]	Loss: 1.6920
Training Epoch: 14 [48000/50048]	Loss: 1.4658
Training Epoch: 14 [48128/50048]	Loss: 1.5389
Training Epoch: 14 [48256/50048]	Loss: 1.7218
Training Epoch: 14 [48384/50048]	Loss: 1.6361
Training Epoch: 14 [48512/50048]	Loss: 1.7290
Training Epoch: 14 [48640/50048]	Loss: 1.7024
Training Epoch: 14 [48768/50048]	Loss: 1.7638
Training Epoch: 14 [48896/50048]	Loss: 1.6675
Training Epoch: 14 [49024/50048]	Loss: 1.6116
Training Epoch: 14 [49152/50048]	Loss: 1.6673
Training Epoch: 14 [49280/50048]	Loss: 1.5423
Training Epoch: 14 [49408/50048]	Loss: 1.4899
Training Epoch: 14 [49536/50048]	Loss: 1.5519
Training Epoch: 14 [49664/50048]	Loss: 1.4284
Training Epoch: 14 [49792/50048]	Loss: 1.4794
Training Epoch: 14 [49920/50048]	Loss: 1.8008
Training Epoch: 14 [50048/50048]	Loss: 1.4891
Validation Epoch: 14, Average loss: 0.0132, Accuracy: 0.5368
Training Epoch: 15 [128/50048]	Loss: 1.5243
Training Epoch: 15 [256/50048]	Loss: 1.3672
Training Epoch: 15 [384/50048]	Loss: 1.5513
Training Epoch: 15 [512/50048]	Loss: 1.4586
Training Epoch: 15 [640/50048]	Loss: 1.7339
Training Epoch: 15 [768/50048]	Loss: 1.5498
Training Epoch: 15 [896/50048]	Loss: 1.5473
Training Epoch: 15 [1024/50048]	Loss: 1.6532
Training Epoch: 15 [1152/50048]	Loss: 1.3772
Training Epoch: 15 [1280/50048]	Loss: 1.4323
Training Epoch: 15 [1408/50048]	Loss: 1.6807
Training Epoch: 15 [1536/50048]	Loss: 1.6364
Training Epoch: 15 [1664/50048]	Loss: 1.3971
Training Epoch: 15 [1792/50048]	Loss: 1.5191
Training Epoch: 15 [1920/50048]	Loss: 1.7279
Training Epoch: 15 [2048/50048]	Loss: 1.4194
Training Epoch: 15 [2176/50048]	Loss: 1.5864
Training Epoch: 15 [2304/50048]	Loss: 1.6664
Training Epoch: 15 [2432/50048]	Loss: 1.6682
Training Epoch: 15 [2560/50048]	Loss: 1.6886
Training Epoch: 15 [2688/50048]	Loss: 1.6648
Training Epoch: 15 [2816/50048]	Loss: 1.5950
Training Epoch: 15 [2944/50048]	Loss: 1.7708
Training Epoch: 15 [3072/50048]	Loss: 1.6441
Training Epoch: 15 [3200/50048]	Loss: 1.6399
Training Epoch: 15 [3328/50048]	Loss: 1.5008
Training Epoch: 15 [3456/50048]	Loss: 1.6139
Training Epoch: 15 [3584/50048]	Loss: 1.4681
Training Epoch: 15 [3712/50048]	Loss: 1.3318
Training Epoch: 15 [3840/50048]	Loss: 1.5864
Training Epoch: 15 [3968/50048]	Loss: 1.3172
Training Epoch: 15 [4096/50048]	Loss: 1.7505
Training Epoch: 15 [4224/50048]	Loss: 1.6944
Training Epoch: 15 [4352/50048]	Loss: 1.7187
Training Epoch: 15 [4480/50048]	Loss: 1.7746
Training Epoch: 15 [4608/50048]	Loss: 1.5974
Training Epoch: 15 [4736/50048]	Loss: 1.7859
Training Epoch: 15 [4864/50048]	Loss: 1.4927
Training Epoch: 15 [4992/50048]	Loss: 1.4927
Training Epoch: 15 [5120/50048]	Loss: 1.4916
Training Epoch: 15 [5248/50048]	Loss: 1.6165
Training Epoch: 15 [5376/50048]	Loss: 1.5317
Training Epoch: 15 [5504/50048]	Loss: 1.5357
Training Epoch: 15 [5632/50048]	Loss: 1.4485
Training Epoch: 15 [5760/50048]	Loss: 1.5903
Training Epoch: 15 [5888/50048]	Loss: 1.5098
Training Epoch: 15 [6016/50048]	Loss: 1.3995
Training Epoch: 15 [6144/50048]	Loss: 1.2786
Training Epoch: 15 [6272/50048]	Loss: 1.3583
Training Epoch: 15 [6400/50048]	Loss: 1.5251
Training Epoch: 15 [6528/50048]	Loss: 1.3354
Training Epoch: 15 [6656/50048]	Loss: 1.4646
Training Epoch: 15 [6784/50048]	Loss: 1.4699
Training Epoch: 15 [6912/50048]	Loss: 1.1635
Training Epoch: 15 [7040/50048]	Loss: 1.9211
Training Epoch: 15 [7168/50048]	Loss: 1.4420
Training Epoch: 15 [7296/50048]	Loss: 1.7606
Training Epoch: 15 [7424/50048]	Loss: 1.2971
Training Epoch: 15 [7552/50048]	Loss: 1.5969
Training Epoch: 15 [7680/50048]	Loss: 1.5822
Training Epoch: 15 [7808/50048]	Loss: 1.5915
Training Epoch: 15 [7936/50048]	Loss: 1.3317
Training Epoch: 15 [8064/50048]	Loss: 1.5227
Training Epoch: 15 [8192/50048]	Loss: 1.4649
Training Epoch: 15 [8320/50048]	Loss: 1.3841
Training Epoch: 15 [8448/50048]	Loss: 1.4640
Training Epoch: 15 [8576/50048]	Loss: 1.5371
Training Epoch: 15 [8704/50048]	Loss: 1.6716
Training Epoch: 15 [8832/50048]	Loss: 1.6830
Training Epoch: 15 [8960/50048]	Loss: 1.4587
Training Epoch: 15 [9088/50048]	Loss: 1.5313
Training Epoch: 15 [9216/50048]	Loss: 1.7858
Training Epoch: 15 [9344/50048]	Loss: 1.7455
Training Epoch: 15 [9472/50048]	Loss: 1.6722
Training Epoch: 15 [9600/50048]	Loss: 1.4721
Training Epoch: 15 [9728/50048]	Loss: 1.4987
Training Epoch: 15 [9856/50048]	Loss: 1.2292
Training Epoch: 15 [9984/50048]	Loss: 1.4623
Training Epoch: 15 [10112/50048]	Loss: 1.9046
Training Epoch: 15 [10240/50048]	Loss: 1.6213
Training Epoch: 15 [10368/50048]	Loss: 1.4029
Training Epoch: 15 [10496/50048]	Loss: 1.7725
Training Epoch: 15 [10624/50048]	Loss: 1.4571
Training Epoch: 15 [10752/50048]	Loss: 1.4823
Training Epoch: 15 [10880/50048]	Loss: 1.6730
Training Epoch: 15 [11008/50048]	Loss: 1.5183
Training Epoch: 15 [11136/50048]	Loss: 1.4138
Training Epoch: 15 [11264/50048]	Loss: 1.4928
Training Epoch: 15 [11392/50048]	Loss: 1.6527
Training Epoch: 15 [11520/50048]	Loss: 1.9527
Training Epoch: 15 [11648/50048]	Loss: 1.6083
Training Epoch: 15 [11776/50048]	Loss: 1.6155
Training Epoch: 15 [11904/50048]	Loss: 1.5603
Training Epoch: 15 [12032/50048]	Loss: 1.6683
Training Epoch: 15 [12160/50048]	Loss: 1.4620
Training Epoch: 15 [12288/50048]	Loss: 1.5161
Training Epoch: 15 [12416/50048]	Loss: 1.7119
Training Epoch: 15 [12544/50048]	Loss: 1.5594
Training Epoch: 15 [12672/50048]	Loss: 1.5399
Training Epoch: 15 [12800/50048]	Loss: 1.2975
Training Epoch: 15 [12928/50048]	Loss: 1.5750
Training Epoch: 15 [13056/50048]	Loss: 1.5319
Training Epoch: 15 [13184/50048]	Loss: 1.4882
Training Epoch: 15 [13312/50048]	Loss: 1.4060
Training Epoch: 15 [13440/50048]	Loss: 1.5986
Training Epoch: 15 [13568/50048]	Loss: 1.5412
Training Epoch: 15 [13696/50048]	Loss: 1.8753
Training Epoch: 15 [13824/50048]	Loss: 1.5666
Training Epoch: 15 [13952/50048]	Loss: 1.6398
Training Epoch: 15 [14080/50048]	Loss: 1.5974
Training Epoch: 15 [14208/50048]	Loss: 1.5448
Training Epoch: 15 [14336/50048]	Loss: 1.6946
Training Epoch: 15 [14464/50048]	Loss: 1.4394
Training Epoch: 15 [14592/50048]	Loss: 1.5275
Training Epoch: 15 [14720/50048]	Loss: 1.3775
Training Epoch: 15 [14848/50048]	Loss: 1.5649
Training Epoch: 15 [14976/50048]	Loss: 1.6595
Training Epoch: 15 [15104/50048]	Loss: 1.3671
Training Epoch: 15 [15232/50048]	Loss: 1.6526
Training Epoch: 15 [15360/50048]	Loss: 1.6900
Training Epoch: 15 [15488/50048]	Loss: 1.7361
Training Epoch: 15 [15616/50048]	Loss: 1.5472
Training Epoch: 15 [15744/50048]	Loss: 1.2401
Training Epoch: 15 [15872/50048]	Loss: 1.6998
Training Epoch: 15 [16000/50048]	Loss: 1.4061
Training Epoch: 15 [16128/50048]	Loss: 1.5632
Training Epoch: 15 [16256/50048]	Loss: 1.4327
Training Epoch: 15 [16384/50048]	Loss: 1.7552
Training Epoch: 15 [16512/50048]	Loss: 1.5517
Training Epoch: 15 [16640/50048]	Loss: 1.6672
Training Epoch: 15 [16768/50048]	Loss: 1.6967
Training Epoch: 15 [16896/50048]	Loss: 1.6821
Training Epoch: 15 [17024/50048]	Loss: 1.4692
Training Epoch: 15 [17152/50048]	Loss: 1.5011
Training Epoch: 15 [17280/50048]	Loss: 1.5841
Training Epoch: 15 [17408/50048]	Loss: 1.3931
Training Epoch: 15 [17536/50048]	Loss: 1.5870
Training Epoch: 15 [17664/50048]	Loss: 1.3124
Training Epoch: 15 [17792/50048]	Loss: 1.5366
Training Epoch: 15 [17920/50048]	Loss: 1.5136
Training Epoch: 15 [18048/50048]	Loss: 1.4881
Training Epoch: 15 [18176/50048]	Loss: 1.5254
Training Epoch: 15 [18304/50048]	Loss: 1.5254
Training Epoch: 15 [18432/50048]	Loss: 1.4968
Training Epoch: 15 [18560/50048]	Loss: 1.5580
Training Epoch: 15 [18688/50048]	Loss: 1.4597
Training Epoch: 15 [18816/50048]	Loss: 1.5832
Training Epoch: 15 [18944/50048]	Loss: 1.5618
Training Epoch: 15 [19072/50048]	Loss: 1.6466
Training Epoch: 15 [19200/50048]	Loss: 1.5481
Training Epoch: 15 [19328/50048]	Loss: 1.6170
Training Epoch: 15 [19456/50048]	Loss: 1.5219
Training Epoch: 15 [19584/50048]	Loss: 1.4932
Training Epoch: 15 [19712/50048]	Loss: 1.6843
Training Epoch: 15 [19840/50048]	Loss: 1.3954
Training Epoch: 15 [19968/50048]	Loss: 1.9486
Training Epoch: 15 [20096/50048]	Loss: 1.6830
Training Epoch: 15 [20224/50048]	Loss: 1.4760
Training Epoch: 15 [20352/50048]	Loss: 1.6421
Training Epoch: 15 [20480/50048]	Loss: 1.4428
Training Epoch: 15 [20608/50048]	Loss: 1.3516
Training Epoch: 15 [20736/50048]	Loss: 1.4368
Training Epoch: 15 [20864/50048]	Loss: 1.5582
Training Epoch: 15 [20992/50048]	Loss: 1.7732
Training Epoch: 15 [21120/50048]	Loss: 1.6728
Training Epoch: 15 [21248/50048]	Loss: 1.3294
Training Epoch: 15 [21376/50048]	Loss: 1.7173
Training Epoch: 15 [21504/50048]	Loss: 1.8408
Training Epoch: 15 [21632/50048]	Loss: 1.5046
Training Epoch: 15 [21760/50048]	Loss: 1.6359
Training Epoch: 15 [21888/50048]	Loss: 1.5992
Training Epoch: 15 [22016/50048]	Loss: 1.3989
Training Epoch: 15 [22144/50048]	Loss: 1.6263
Training Epoch: 15 [22272/50048]	Loss: 1.6755
Training Epoch: 15 [22400/50048]	Loss: 1.4541
Training Epoch: 15 [22528/50048]	Loss: 1.4660
Training Epoch: 15 [22656/50048]	Loss: 1.6774
Training Epoch: 15 [22784/50048]	Loss: 1.7360
Training Epoch: 15 [22912/50048]	Loss: 1.6422
Training Epoch: 15 [23040/50048]	Loss: 1.4796
Training Epoch: 15 [23168/50048]	Loss: 1.2431
Training Epoch: 15 [23296/50048]	Loss: 1.4548
Training Epoch: 15 [23424/50048]	Loss: 1.5441
Training Epoch: 15 [23552/50048]	Loss: 1.5546
Training Epoch: 15 [23680/50048]	Loss: 1.4857
Training Epoch: 15 [23808/50048]	Loss: 1.7288
Training Epoch: 15 [23936/50048]	Loss: 1.7965
Training Epoch: 15 [24064/50048]	Loss: 1.7357
Training Epoch: 15 [24192/50048]	Loss: 1.5798
Training Epoch: 15 [24320/50048]	Loss: 1.8865
Training Epoch: 15 [24448/50048]	Loss: 1.6472
Training Epoch: 15 [24576/50048]	Loss: 1.6338
Training Epoch: 15 [24704/50048]	Loss: 1.6076
Training Epoch: 15 [24832/50048]	Loss: 1.5040
Training Epoch: 15 [24960/50048]	Loss: 1.4287
Training Epoch: 15 [25088/50048]	Loss: 1.4972
Training Epoch: 15 [25216/50048]	Loss: 1.5684
Training Epoch: 15 [25344/50048]	Loss: 1.4979
Training Epoch: 15 [25472/50048]	Loss: 1.5619
Training Epoch: 15 [25600/50048]	Loss: 1.6394
Training Epoch: 15 [25728/50048]	Loss: 1.7412
Training Epoch: 15 [25856/50048]	Loss: 1.6549
Training Epoch: 15 [25984/50048]	Loss: 1.5114
Training Epoch: 15 [26112/50048]	Loss: 1.6773
Training Epoch: 15 [26240/50048]	Loss: 1.2325
Training Epoch: 15 [26368/50048]	Loss: 1.7277
Training Epoch: 15 [26496/50048]	Loss: 1.5273
Training Epoch: 15 [26624/50048]	Loss: 1.6154
Training Epoch: 15 [26752/50048]	Loss: 1.6013
Training Epoch: 15 [26880/50048]	Loss: 1.5426
Training Epoch: 15 [27008/50048]	Loss: 1.5412
Training Epoch: 15 [27136/50048]	Loss: 1.3510
Training Epoch: 15 [27264/50048]	Loss: 1.3618
Training Epoch: 15 [27392/50048]	Loss: 1.5167
Training Epoch: 15 [27520/50048]	Loss: 1.6415
Training Epoch: 15 [27648/50048]	Loss: 1.5343
Training Epoch: 15 [27776/50048]	Loss: 1.8227
Training Epoch: 15 [27904/50048]	Loss: 1.5060
Training Epoch: 15 [28032/50048]	Loss: 1.5618
Training Epoch: 15 [28160/50048]	Loss: 1.6163
Training Epoch: 15 [28288/50048]	Loss: 1.5595
Training Epoch: 15 [28416/50048]	Loss: 1.6649
Training Epoch: 15 [28544/50048]	Loss: 1.4220
Training Epoch: 15 [28672/50048]	Loss: 1.7048
Training Epoch: 15 [28800/50048]	Loss: 1.7135
Training Epoch: 15 [28928/50048]	Loss: 1.5911
Training Epoch: 15 [29056/50048]	Loss: 1.4137
Training Epoch: 15 [29184/50048]	Loss: 1.7292
Training Epoch: 15 [29312/50048]	Loss: 1.4631
Training Epoch: 15 [29440/50048]	Loss: 1.3642
Training Epoch: 15 [29568/50048]	Loss: 1.7550
Training Epoch: 15 [29696/50048]	Loss: 1.5179
Training Epoch: 15 [29824/50048]	Loss: 1.3753
Training Epoch: 15 [29952/50048]	Loss: 1.3846
Training Epoch: 15 [30080/50048]	Loss: 1.4644
Training Epoch: 15 [30208/50048]	Loss: 1.5186
Training Epoch: 15 [30336/50048]	Loss: 1.7952
Training Epoch: 15 [30464/50048]	Loss: 1.8903
Training Epoch: 15 [30592/50048]	Loss: 1.6893
Training Epoch: 15 [30720/50048]	Loss: 1.6798
Training Epoch: 15 [30848/50048]	Loss: 1.1386
Training Epoch: 15 [30976/50048]	Loss: 1.5684
Training Epoch: 15 [31104/50048]	Loss: 1.3235
Training Epoch: 15 [31232/50048]	Loss: 1.2649
Training Epoch: 15 [31360/50048]	Loss: 1.6619
Training Epoch: 15 [31488/50048]	Loss: 1.5780
Training Epoch: 15 [31616/50048]	Loss: 1.5410
Training Epoch: 15 [31744/50048]	Loss: 1.6433
Training Epoch: 15 [31872/50048]	Loss: 1.3588
Training Epoch: 15 [32000/50048]	Loss: 1.6086
Training Epoch: 15 [32128/50048]	Loss: 1.4545
Training Epoch: 15 [32256/50048]	Loss: 1.5019
Training Epoch: 15 [32384/50048]	Loss: 1.5390
Training Epoch: 15 [32512/50048]	Loss: 1.5707
Training Epoch: 15 [32640/50048]	Loss: 1.5682
Training Epoch: 15 [32768/50048]	Loss: 1.6814
Training Epoch: 15 [32896/50048]	Loss: 1.5922
Training Epoch: 15 [33024/50048]	Loss: 1.7047
Training Epoch: 15 [33152/50048]	Loss: 1.4880
Training Epoch: 15 [33280/50048]	Loss: 1.2794
Training Epoch: 15 [33408/50048]	Loss: 1.4010
Training Epoch: 15 [33536/50048]	Loss: 1.7560
Training Epoch: 15 [33664/50048]	Loss: 1.5208
Training Epoch: 15 [33792/50048]	Loss: 1.6724
Training Epoch: 15 [33920/50048]	Loss: 1.4006
Training Epoch: 15 [34048/50048]	Loss: 1.3730
Training Epoch: 15 [34176/50048]	Loss: 1.5677
Training Epoch: 15 [34304/50048]	Loss: 1.6425
Training Epoch: 15 [34432/50048]	Loss: 1.5639
Training Epoch: 15 [34560/50048]	Loss: 1.4524
Training Epoch: 15 [34688/50048]	Loss: 1.5561
Training Epoch: 15 [34816/50048]	Loss: 1.4040
Training Epoch: 15 [34944/50048]	Loss: 1.5108
Training Epoch: 15 [35072/50048]	Loss: 1.7126
Training Epoch: 15 [35200/50048]	Loss: 1.4287
Training Epoch: 15 [35328/50048]	Loss: 1.5535
Training Epoch: 15 [35456/50048]	Loss: 1.5945
Training Epoch: 15 [35584/50048]	Loss: 1.7173
Training Epoch: 15 [35712/50048]	Loss: 1.6225
Training Epoch: 15 [35840/50048]	Loss: 1.5485
Training Epoch: 15 [35968/50048]	Loss: 1.6098
Training Epoch: 15 [36096/50048]	Loss: 1.3206
Training Epoch: 15 [36224/50048]	Loss: 1.4274
Training Epoch: 15 [36352/50048]	Loss: 1.6606
Training Epoch: 15 [36480/50048]	Loss: 1.5547
Training Epoch: 15 [36608/50048]	Loss: 1.5093
Training Epoch: 15 [36736/50048]	Loss: 1.7235
Training Epoch: 15 [36864/50048]	Loss: 1.5452
Training Epoch: 15 [36992/50048]	Loss: 1.4766
Training Epoch: 15 [37120/50048]	Loss: 1.4993
Training Epoch: 15 [37248/50048]	Loss: 1.6057
Training Epoch: 15 [37376/50048]	Loss: 1.7797
Training Epoch: 15 [37504/50048]	Loss: 1.5420
Training Epoch: 15 [37632/50048]	Loss: 1.9135
Training Epoch: 15 [37760/50048]	Loss: 1.6062
Training Epoch: 15 [37888/50048]	Loss: 1.6414
Training Epoch: 15 [38016/50048]	Loss: 1.6820
Training Epoch: 15 [38144/50048]	Loss: 1.5493
Training Epoch: 15 [38272/50048]	Loss: 1.6540
Training Epoch: 15 [38400/50048]	Loss: 1.7268
Training Epoch: 15 [38528/50048]	Loss: 1.4947
Training Epoch: 15 [38656/50048]	Loss: 1.4163
Training Epoch: 15 [38784/50048]	Loss: 1.5345
Training Epoch: 15 [38912/50048]	Loss: 1.6417
Training Epoch: 15 [39040/50048]	Loss: 1.8926
Training Epoch: 15 [39168/50048]	Loss: 1.6252
Training Epoch: 15 [39296/50048]	Loss: 1.2925
Training Epoch: 15 [39424/50048]	Loss: 1.4351
Training Epoch: 15 [39552/50048]	Loss: 1.6062
Training Epoch: 15 [39680/50048]	Loss: 1.7119
Training Epoch: 15 [39808/50048]	Loss: 1.6029
Training Epoch: 15 [39936/50048]	Loss: 1.4046
Training Epoch: 15 [40064/50048]	Loss: 1.6597
Training Epoch: 15 [40192/50048]	Loss: 1.6136
Training Epoch: 15 [40320/50048]	Loss: 1.7138
Training Epoch: 15 [40448/50048]	Loss: 1.4587
Training Epoch: 15 [40576/50048]	Loss: 1.4900
Training Epoch: 15 [40704/50048]	Loss: 1.6626
Training Epoch: 15 [40832/50048]	Loss: 1.7088
Training Epoch: 15 [40960/50048]	Loss: 1.5124
Training Epoch: 15 [41088/50048]	Loss: 1.8083
Training Epoch: 15 [41216/50048]	Loss: 1.3475
Training Epoch: 15 [41344/50048]	Loss: 1.4227
Training Epoch: 15 [41472/50048]	Loss: 1.4584
Training Epoch: 15 [41600/50048]	Loss: 1.8849
Training Epoch: 15 [41728/50048]	Loss: 1.6011
Training Epoch: 15 [41856/50048]	Loss: 1.5366
Training Epoch: 15 [41984/50048]	Loss: 1.3928
Training Epoch: 15 [42112/50048]	Loss: 1.5858
Training Epoch: 15 [42240/50048]	Loss: 1.5732
Training Epoch: 15 [42368/50048]	Loss: 1.5684
Training Epoch: 15 [42496/50048]	Loss: 1.6357
Training Epoch: 15 [42624/50048]	Loss: 1.6232
Training Epoch: 15 [42752/50048]	Loss: 1.3662
Training Epoch: 15 [42880/50048]	Loss: 1.5437
Training Epoch: 15 [43008/50048]	Loss: 1.5317
Training Epoch: 15 [43136/50048]	Loss: 1.7511
Training Epoch: 15 [43264/50048]	Loss: 1.6143
Training Epoch: 15 [43392/50048]	Loss: 1.5098
Training Epoch: 15 [43520/50048]	Loss: 1.6796
Training Epoch: 15 [43648/50048]	Loss: 1.2347
Training Epoch: 15 [43776/50048]	Loss: 1.5577
Training Epoch: 15 [43904/50048]	Loss: 1.6333
Training Epoch: 15 [44032/50048]	Loss: 1.5642
Training Epoch: 15 [44160/50048]	Loss: 1.6389
Training Epoch: 15 [44288/50048]	Loss: 1.5548
Training Epoch: 15 [44416/50048]	Loss: 1.7803
Training Epoch: 15 [44544/50048]	Loss: 1.7084
Training Epoch: 15 [44672/50048]	Loss: 1.4406
Training Epoch: 15 [44800/50048]	Loss: 1.5381
Training Epoch: 15 [44928/50048]	Loss: 1.5279
Training Epoch: 15 [45056/50048]	Loss: 1.4252
Training Epoch: 15 [45184/50048]	Loss: 1.5678
Training Epoch: 15 [45312/50048]	Loss: 1.4727
Training Epoch: 15 [45440/50048]	Loss: 1.5316
Training Epoch: 15 [45568/50048]	Loss: 1.3923
Training Epoch: 15 [45696/50048]	Loss: 1.6243
Training Epoch: 15 [45824/50048]	Loss: 1.7115
Training Epoch: 15 [45952/50048]	Loss: 1.3822
Training Epoch: 15 [46080/50048]	Loss: 1.5263
Training Epoch: 15 [46208/50048]	Loss: 1.3014
Training Epoch: 15 [46336/50048]	Loss: 1.4213
Training Epoch: 15 [46464/50048]	Loss: 1.4743
Training Epoch: 15 [46592/50048]	Loss: 1.3706
Training Epoch: 15 [46720/50048]	Loss: 1.6533
Training Epoch: 15 [46848/50048]	Loss: 1.4391
Training Epoch: 15 [46976/50048]	Loss: 1.4300
Training Epoch: 15 [47104/50048]	Loss: 1.5477
Training Epoch: 15 [47232/50048]	Loss: 1.6277
Training Epoch: 15 [47360/50048]	Loss: 1.4342
Training Epoch: 15 [47488/50048]	Loss: 1.6253
Training Epoch: 15 [47616/50048]	Loss: 1.7080
Training Epoch: 15 [47744/50048]	Loss: 1.5266
Training Epoch: 15 [47872/50048]	Loss: 1.3543
Training Epoch: 15 [48000/50048]	Loss: 1.5892
Training Epoch: 15 [48128/50048]	Loss: 1.7548
Training Epoch: 15 [48256/50048]	Loss: 1.6749
Training Epoch: 15 [48384/50048]	Loss: 1.5772
Training Epoch: 15 [48512/50048]	Loss: 1.4932
Training Epoch: 15 [48640/50048]	Loss: 1.7539
Training Epoch: 15 [48768/50048]	Loss: 1.3028
Training Epoch: 15 [48896/50048]	Loss: 1.6241
Training Epoch: 15 [49024/50048]	Loss: 1.7908
Training Epoch: 15 [49152/50048]	Loss: 1.5453
Training Epoch: 15 [49280/50048]	Loss: 1.4580
Training Epoch: 15 [49408/50048]	Loss: 1.7769
Training Epoch: 15 [49536/50048]	Loss: 1.4320
Training Epoch: 15 [49664/50048]	Loss: 1.6148
Training Epoch: 15 [49792/50048]	Loss: 1.6034
Training Epoch: 15 [49920/50048]	Loss: 1.2154
Training Epoch: 15 [50048/50048]	Loss: 1.4861
Validation Epoch: 15, Average loss: 0.0130, Accuracy: 0.5384
Training Epoch: 16 [128/50048]	Loss: 1.2872
Training Epoch: 16 [256/50048]	Loss: 1.4206
Training Epoch: 16 [384/50048]	Loss: 1.5872
Training Epoch: 16 [512/50048]	Loss: 1.6499
Training Epoch: 16 [640/50048]	Loss: 1.7144
Training Epoch: 16 [768/50048]	Loss: 1.6517
Training Epoch: 16 [896/50048]	Loss: 1.6022
Training Epoch: 16 [1024/50048]	Loss: 1.5049
Training Epoch: 16 [1152/50048]	Loss: 1.4576
Training Epoch: 16 [1280/50048]	Loss: 1.3315
Training Epoch: 16 [1408/50048]	Loss: 1.5249
Training Epoch: 16 [1536/50048]	Loss: 1.5643
Training Epoch: 16 [1664/50048]	Loss: 1.4636
Training Epoch: 16 [1792/50048]	Loss: 1.5349
Training Epoch: 16 [1920/50048]	Loss: 1.6661
Training Epoch: 16 [2048/50048]	Loss: 1.4699
Training Epoch: 16 [2176/50048]	Loss: 1.5414
Training Epoch: 16 [2304/50048]	Loss: 1.6095
Training Epoch: 16 [2432/50048]	Loss: 1.7025
Training Epoch: 16 [2560/50048]	Loss: 1.6915
Training Epoch: 16 [2688/50048]	Loss: 1.5467
Training Epoch: 16 [2816/50048]	Loss: 1.7265
Training Epoch: 16 [2944/50048]	Loss: 1.4294
Training Epoch: 16 [3072/50048]	Loss: 1.2921
Training Epoch: 16 [3200/50048]	Loss: 1.4543
Training Epoch: 16 [3328/50048]	Loss: 1.3788
Training Epoch: 16 [3456/50048]	Loss: 1.2836
Training Epoch: 16 [3584/50048]	Loss: 1.8860
Training Epoch: 16 [3712/50048]	Loss: 1.2130
Training Epoch: 16 [3840/50048]	Loss: 1.6076
Training Epoch: 16 [3968/50048]	Loss: 1.6408
Training Epoch: 16 [4096/50048]	Loss: 1.3849
Training Epoch: 16 [4224/50048]	Loss: 1.4726
Training Epoch: 16 [4352/50048]	Loss: 1.3572
Training Epoch: 16 [4480/50048]	Loss: 1.6090
Training Epoch: 16 [4608/50048]	Loss: 1.4269
Training Epoch: 16 [4736/50048]	Loss: 1.5112
Training Epoch: 16 [4864/50048]	Loss: 1.4347
Training Epoch: 16 [4992/50048]	Loss: 1.6426
Training Epoch: 16 [5120/50048]	Loss: 1.5149
Training Epoch: 16 [5248/50048]	Loss: 1.7193
Training Epoch: 16 [5376/50048]	Loss: 1.7103
Training Epoch: 16 [5504/50048]	Loss: 1.5829
Training Epoch: 16 [5632/50048]	Loss: 1.4912
Training Epoch: 16 [5760/50048]	Loss: 1.5517
Training Epoch: 16 [5888/50048]	Loss: 1.3564
Training Epoch: 16 [6016/50048]	Loss: 1.5660
Training Epoch: 16 [6144/50048]	Loss: 1.4185
Training Epoch: 16 [6272/50048]	Loss: 1.6130
Training Epoch: 16 [6400/50048]	Loss: 1.5928
Training Epoch: 16 [6528/50048]	Loss: 1.5526
Training Epoch: 16 [6656/50048]	Loss: 1.4899
Training Epoch: 16 [6784/50048]	Loss: 1.3552
Training Epoch: 16 [6912/50048]	Loss: 1.2239
Training Epoch: 16 [7040/50048]	Loss: 1.5196
Training Epoch: 16 [7168/50048]	Loss: 1.5938
Training Epoch: 16 [7296/50048]	Loss: 1.5271
Training Epoch: 16 [7424/50048]	Loss: 1.5056
Training Epoch: 16 [7552/50048]	Loss: 1.6563
Training Epoch: 16 [7680/50048]	Loss: 1.4346
Training Epoch: 16 [7808/50048]	Loss: 1.5464
Training Epoch: 16 [7936/50048]	Loss: 1.3224
Training Epoch: 16 [8064/50048]	Loss: 1.3313
Training Epoch: 16 [8192/50048]	Loss: 1.3662
Training Epoch: 16 [8320/50048]	Loss: 1.4232
Training Epoch: 16 [8448/50048]	Loss: 1.5445
Training Epoch: 16 [8576/50048]	Loss: 1.6662
Training Epoch: 16 [8704/50048]	Loss: 1.5433
Training Epoch: 16 [8832/50048]	Loss: 1.5255
Training Epoch: 16 [8960/50048]	Loss: 1.4855
Training Epoch: 16 [9088/50048]	Loss: 1.5773
Training Epoch: 16 [9216/50048]	Loss: 1.3996
Training Epoch: 16 [9344/50048]	Loss: 1.4643
Training Epoch: 16 [9472/50048]	Loss: 1.5264
Training Epoch: 16 [9600/50048]	Loss: 1.3204
Training Epoch: 16 [9728/50048]	Loss: 1.4126
Training Epoch: 16 [9856/50048]	Loss: 1.3903
Training Epoch: 16 [9984/50048]	Loss: 1.3865
Training Epoch: 16 [10112/50048]	Loss: 1.3729
Training Epoch: 16 [10240/50048]	Loss: 1.4194
Training Epoch: 16 [10368/50048]	Loss: 1.6668
Training Epoch: 16 [10496/50048]	Loss: 1.5246
Training Epoch: 16 [10624/50048]	Loss: 1.4118
Training Epoch: 16 [10752/50048]	Loss: 1.4951
Training Epoch: 16 [10880/50048]	Loss: 1.5157
Training Epoch: 16 [11008/50048]	Loss: 1.5157
Training Epoch: 16 [11136/50048]	Loss: 1.6069
Training Epoch: 16 [11264/50048]	Loss: 1.3278
Training Epoch: 16 [11392/50048]	Loss: 1.8387
Training Epoch: 16 [11520/50048]	Loss: 1.1292
Training Epoch: 16 [11648/50048]	Loss: 1.6323
Training Epoch: 16 [11776/50048]	Loss: 1.4906
Training Epoch: 16 [11904/50048]	Loss: 1.5595
Training Epoch: 16 [12032/50048]	Loss: 1.3633
Training Epoch: 16 [12160/50048]	Loss: 1.6417
Training Epoch: 16 [12288/50048]	Loss: 1.4348
Training Epoch: 16 [12416/50048]	Loss: 1.3458
Training Epoch: 16 [12544/50048]	Loss: 1.4027
Training Epoch: 16 [12672/50048]	Loss: 1.5030
Training Epoch: 16 [12800/50048]	Loss: 1.5101
Training Epoch: 16 [12928/50048]	Loss: 1.4179
Training Epoch: 16 [13056/50048]	Loss: 1.2290
Training Epoch: 16 [13184/50048]	Loss: 1.6281
Training Epoch: 16 [13312/50048]	Loss: 1.7074
Training Epoch: 16 [13440/50048]	Loss: 1.4057
Training Epoch: 16 [13568/50048]	Loss: 1.2783
Training Epoch: 16 [13696/50048]	Loss: 1.4131
Training Epoch: 16 [13824/50048]	Loss: 1.6426
Training Epoch: 16 [13952/50048]	Loss: 1.5435
Training Epoch: 16 [14080/50048]	Loss: 1.5934
Training Epoch: 16 [14208/50048]	Loss: 1.6556
Training Epoch: 16 [14336/50048]	Loss: 1.4948
Training Epoch: 16 [14464/50048]	Loss: 1.7291
Training Epoch: 16 [14592/50048]	Loss: 1.5915
Training Epoch: 16 [14720/50048]	Loss: 1.6436
Training Epoch: 16 [14848/50048]	Loss: 1.4084
Training Epoch: 16 [14976/50048]	Loss: 1.3176
Training Epoch: 16 [15104/50048]	Loss: 1.5313
Training Epoch: 16 [15232/50048]	Loss: 1.4880
Training Epoch: 16 [15360/50048]	Loss: 1.4714
Training Epoch: 16 [15488/50048]	Loss: 1.6980
Training Epoch: 16 [15616/50048]	Loss: 1.2997
Training Epoch: 16 [15744/50048]	Loss: 1.5869
Training Epoch: 16 [15872/50048]	Loss: 1.5320
Training Epoch: 16 [16000/50048]	Loss: 1.4356
Training Epoch: 16 [16128/50048]	Loss: 1.5442
Training Epoch: 16 [16256/50048]	Loss: 1.4486
Training Epoch: 16 [16384/50048]	Loss: 1.3777
Training Epoch: 16 [16512/50048]	Loss: 1.5153
Training Epoch: 16 [16640/50048]	Loss: 1.6867
Training Epoch: 16 [16768/50048]	Loss: 1.5377
Training Epoch: 16 [16896/50048]	Loss: 1.4410
Training Epoch: 16 [17024/50048]	Loss: 1.5320
Training Epoch: 16 [17152/50048]	Loss: 1.6384
Training Epoch: 16 [17280/50048]	Loss: 1.4323
Training Epoch: 16 [17408/50048]	Loss: 1.4504
Training Epoch: 16 [17536/50048]	Loss: 1.4408
Training Epoch: 16 [17664/50048]	Loss: 1.4396
Training Epoch: 16 [17792/50048]	Loss: 1.4955
Training Epoch: 16 [17920/50048]	Loss: 1.4911
Training Epoch: 16 [18048/50048]	Loss: 1.4461
Training Epoch: 16 [18176/50048]	Loss: 1.4911
Training Epoch: 16 [18304/50048]	Loss: 1.5373
Training Epoch: 16 [18432/50048]	Loss: 1.4502
Training Epoch: 16 [18560/50048]	Loss: 1.5047
Training Epoch: 16 [18688/50048]	Loss: 1.4858
Training Epoch: 16 [18816/50048]	Loss: 1.4337
Training Epoch: 16 [18944/50048]	Loss: 1.3057
Training Epoch: 16 [19072/50048]	Loss: 1.2602
Training Epoch: 16 [19200/50048]	Loss: 1.5480
Training Epoch: 16 [19328/50048]	Loss: 1.5189
Training Epoch: 16 [19456/50048]	Loss: 1.4998
Training Epoch: 16 [19584/50048]	Loss: 1.4024
Training Epoch: 16 [19712/50048]	Loss: 1.5019
Training Epoch: 16 [19840/50048]	Loss: 1.6011
Training Epoch: 16 [19968/50048]	Loss: 1.6361
Training Epoch: 16 [20096/50048]	Loss: 1.4947
Training Epoch: 16 [20224/50048]	Loss: 1.5484
Training Epoch: 16 [20352/50048]	Loss: 1.4073
Training Epoch: 16 [20480/50048]	Loss: 1.5280
Training Epoch: 16 [20608/50048]	Loss: 1.5194
Training Epoch: 16 [20736/50048]	Loss: 1.4544
Training Epoch: 16 [20864/50048]	Loss: 1.6550
Training Epoch: 16 [20992/50048]	Loss: 1.4389
Training Epoch: 16 [21120/50048]	Loss: 1.5537
Training Epoch: 16 [21248/50048]	Loss: 1.5925
Training Epoch: 16 [21376/50048]	Loss: 1.5325
Training Epoch: 16 [21504/50048]	Loss: 1.4163
Training Epoch: 16 [21632/50048]	Loss: 1.4223
Training Epoch: 16 [21760/50048]	Loss: 1.5439
Training Epoch: 16 [21888/50048]	Loss: 1.4268
Training Epoch: 16 [22016/50048]	Loss: 1.7105
Training Epoch: 16 [22144/50048]	Loss: 1.4318
Training Epoch: 16 [22272/50048]	Loss: 1.8305
Training Epoch: 16 [22400/50048]	Loss: 1.4558
Training Epoch: 16 [22528/50048]	Loss: 1.6562
Training Epoch: 16 [22656/50048]	Loss: 1.8791
Training Epoch: 16 [22784/50048]	Loss: 1.6837
Training Epoch: 16 [22912/50048]	Loss: 1.2952
Training Epoch: 16 [23040/50048]	Loss: 1.5684
Training Epoch: 16 [23168/50048]	Loss: 1.3875
Training Epoch: 16 [23296/50048]	Loss: 1.6051
Training Epoch: 16 [23424/50048]	Loss: 1.5813
Training Epoch: 16 [23552/50048]	Loss: 1.3961
Training Epoch: 16 [23680/50048]	Loss: 1.6416
Training Epoch: 16 [23808/50048]	Loss: 1.7458
Training Epoch: 16 [23936/50048]	Loss: 1.5051
Training Epoch: 16 [24064/50048]	Loss: 1.5465
Training Epoch: 16 [24192/50048]	Loss: 1.6641
Training Epoch: 16 [24320/50048]	Loss: 1.3660
Training Epoch: 16 [24448/50048]	Loss: 1.6097
Training Epoch: 16 [24576/50048]	Loss: 1.4931
Training Epoch: 16 [24704/50048]	Loss: 1.3161
Training Epoch: 16 [24832/50048]	Loss: 1.3590
Training Epoch: 16 [24960/50048]	Loss: 1.3919
Training Epoch: 16 [25088/50048]	Loss: 1.6350
Training Epoch: 16 [25216/50048]	Loss: 1.6194
Training Epoch: 16 [25344/50048]	Loss: 1.4673
Training Epoch: 16 [25472/50048]	Loss: 1.3656
Training Epoch: 16 [25600/50048]	Loss: 1.4929
Training Epoch: 16 [25728/50048]	Loss: 1.5243
Training Epoch: 16 [25856/50048]	Loss: 1.5230
Training Epoch: 16 [25984/50048]	Loss: 1.6210
Training Epoch: 16 [26112/50048]	Loss: 1.7377
Training Epoch: 16 [26240/50048]	Loss: 1.5221
Training Epoch: 16 [26368/50048]	Loss: 1.4451
Training Epoch: 16 [26496/50048]	Loss: 1.6714
Training Epoch: 16 [26624/50048]	Loss: 1.5897
Training Epoch: 16 [26752/50048]	Loss: 1.4598
Training Epoch: 16 [26880/50048]	Loss: 1.6448
Training Epoch: 16 [27008/50048]	Loss: 1.4628
Training Epoch: 16 [27136/50048]	Loss: 1.3625
Training Epoch: 16 [27264/50048]	Loss: 1.5258
Training Epoch: 16 [27392/50048]	Loss: 1.5964
Training Epoch: 16 [27520/50048]	Loss: 1.8693
Training Epoch: 16 [27648/50048]	Loss: 1.2847
Training Epoch: 16 [27776/50048]	Loss: 1.2827
Training Epoch: 16 [27904/50048]	Loss: 1.4245
Training Epoch: 16 [28032/50048]	Loss: 1.3662
Training Epoch: 16 [28160/50048]	Loss: 1.7252
Training Epoch: 16 [28288/50048]	Loss: 1.4464
Training Epoch: 16 [28416/50048]	Loss: 1.5436
Training Epoch: 16 [28544/50048]	Loss: 1.6428
Training Epoch: 16 [28672/50048]	Loss: 1.6300
Training Epoch: 16 [28800/50048]	Loss: 1.6182
Training Epoch: 16 [28928/50048]	Loss: 1.4074
Training Epoch: 16 [29056/50048]	Loss: 1.5218
Training Epoch: 16 [29184/50048]	Loss: 1.2862
Training Epoch: 16 [29312/50048]	Loss: 1.5143
Training Epoch: 16 [29440/50048]	Loss: 1.5166
Training Epoch: 16 [29568/50048]	Loss: 1.5762
Training Epoch: 16 [29696/50048]	Loss: 1.7264
Training Epoch: 16 [29824/50048]	Loss: 1.4893
Training Epoch: 16 [29952/50048]	Loss: 1.5752
Training Epoch: 16 [30080/50048]	Loss: 1.5267
Training Epoch: 16 [30208/50048]	Loss: 1.5978
Training Epoch: 16 [30336/50048]	Loss: 1.4811
Training Epoch: 16 [30464/50048]	Loss: 1.5008
Training Epoch: 16 [30592/50048]	Loss: 1.4595
Training Epoch: 16 [30720/50048]	Loss: 1.7250
Training Epoch: 16 [30848/50048]	Loss: 1.6152
Training Epoch: 16 [30976/50048]	Loss: 1.5744
Training Epoch: 16 [31104/50048]	Loss: 1.6242
Training Epoch: 16 [31232/50048]	Loss: 1.4474
Training Epoch: 16 [31360/50048]	Loss: 1.7431
Training Epoch: 16 [31488/50048]	Loss: 1.7134
Training Epoch: 16 [31616/50048]	Loss: 1.4042
Training Epoch: 16 [31744/50048]	Loss: 1.7092
Training Epoch: 16 [31872/50048]	Loss: 1.6013
Training Epoch: 16 [32000/50048]	Loss: 1.5817
Training Epoch: 16 [32128/50048]	Loss: 1.5523
Training Epoch: 16 [32256/50048]	Loss: 1.6460
Training Epoch: 16 [32384/50048]	Loss: 1.5961
Training Epoch: 16 [32512/50048]	Loss: 1.5029
Training Epoch: 16 [32640/50048]	Loss: 1.3702
Training Epoch: 16 [32768/50048]	Loss: 1.4371
Training Epoch: 16 [32896/50048]	Loss: 1.6833
Training Epoch: 16 [33024/50048]	Loss: 1.3329
Training Epoch: 16 [33152/50048]	Loss: 1.6344
Training Epoch: 16 [33280/50048]	Loss: 1.6563
Training Epoch: 16 [33408/50048]	Loss: 1.3002
Training Epoch: 16 [33536/50048]	Loss: 1.5877
Training Epoch: 16 [33664/50048]	Loss: 1.2687
Training Epoch: 16 [33792/50048]	Loss: 1.5506
Training Epoch: 16 [33920/50048]	Loss: 1.6125
Training Epoch: 16 [34048/50048]	Loss: 1.5927
Training Epoch: 16 [34176/50048]	Loss: 1.5062
Training Epoch: 16 [34304/50048]	Loss: 1.5203
Training Epoch: 16 [34432/50048]	Loss: 1.2915
Training Epoch: 16 [34560/50048]	Loss: 1.3286
Training Epoch: 16 [34688/50048]	Loss: 1.4280
Training Epoch: 16 [34816/50048]	Loss: 1.2044
Training Epoch: 16 [34944/50048]	Loss: 1.1946
Training Epoch: 16 [35072/50048]	Loss: 1.4755
Training Epoch: 16 [35200/50048]	Loss: 1.5799
Training Epoch: 16 [35328/50048]	Loss: 1.5524
Training Epoch: 16 [35456/50048]	Loss: 1.5749
Training Epoch: 16 [35584/50048]	Loss: 1.6251
Training Epoch: 16 [35712/50048]	Loss: 1.4554
Training Epoch: 16 [35840/50048]	Loss: 1.2487
Training Epoch: 16 [35968/50048]	Loss: 1.7321
Training Epoch: 16 [36096/50048]	Loss: 1.3969
Training Epoch: 16 [36224/50048]	Loss: 1.4651
Training Epoch: 16 [36352/50048]	Loss: 1.4773
Training Epoch: 16 [36480/50048]	Loss: 1.3706
Training Epoch: 16 [36608/50048]	Loss: 1.6908
Training Epoch: 16 [36736/50048]	Loss: 1.6449
Training Epoch: 16 [36864/50048]	Loss: 1.4147
Training Epoch: 16 [36992/50048]	Loss: 1.6855
Training Epoch: 16 [37120/50048]	Loss: 1.5510
Training Epoch: 16 [37248/50048]	Loss: 1.3176
Training Epoch: 16 [37376/50048]	Loss: 1.5659
Training Epoch: 16 [37504/50048]	Loss: 1.6883
Training Epoch: 16 [37632/50048]	Loss: 1.6865
Training Epoch: 16 [37760/50048]	Loss: 1.5562
Training Epoch: 16 [37888/50048]	Loss: 1.4231
Training Epoch: 16 [38016/50048]	Loss: 1.5812
Training Epoch: 16 [38144/50048]	Loss: 1.7153
Training Epoch: 16 [38272/50048]	Loss: 1.6801
Training Epoch: 16 [38400/50048]	Loss: 1.6664
Training Epoch: 16 [38528/50048]	Loss: 1.6167
Training Epoch: 16 [38656/50048]	Loss: 1.2552
Training Epoch: 16 [38784/50048]	Loss: 1.4401
Training Epoch: 16 [38912/50048]	Loss: 1.3526
Training Epoch: 16 [39040/50048]	Loss: 1.5145
Training Epoch: 16 [39168/50048]	Loss: 1.5643
Training Epoch: 16 [39296/50048]	Loss: 1.6473
Training Epoch: 16 [39424/50048]	Loss: 1.5306
Training Epoch: 16 [39552/50048]	Loss: 1.7169
Training Epoch: 16 [39680/50048]	Loss: 1.4840
Training Epoch: 16 [39808/50048]	Loss: 1.1768
Training Epoch: 16 [39936/50048]	Loss: 1.5439
Training Epoch: 16 [40064/50048]	Loss: 1.5120
Training Epoch: 16 [40192/50048]	Loss: 1.2397
Training Epoch: 16 [40320/50048]	Loss: 1.5258
Training Epoch: 16 [40448/50048]	Loss: 1.4367
Training Epoch: 16 [40576/50048]	Loss: 1.6539
Training Epoch: 16 [40704/50048]	Loss: 1.6935
Training Epoch: 16 [40832/50048]	Loss: 1.5732
Training Epoch: 16 [40960/50048]	Loss: 1.6359
Training Epoch: 16 [41088/50048]	Loss: 1.3319
Training Epoch: 16 [41216/50048]	Loss: 1.6119
Training Epoch: 16 [41344/50048]	Loss: 1.6972
Training Epoch: 16 [41472/50048]	Loss: 1.5238
Training Epoch: 16 [41600/50048]	Loss: 1.4972
Training Epoch: 16 [41728/50048]	Loss: 1.4950
Training Epoch: 16 [41856/50048]	Loss: 1.4856
Training Epoch: 16 [41984/50048]	Loss: 1.3754
Training Epoch: 16 [42112/50048]	Loss: 1.6260
Training Epoch: 16 [42240/50048]	Loss: 1.3757
Training Epoch: 16 [42368/50048]	Loss: 1.5043
Training Epoch: 16 [42496/50048]	Loss: 1.4697
Training Epoch: 16 [42624/50048]	Loss: 1.5367
Training Epoch: 16 [42752/50048]	Loss: 1.5046
Training Epoch: 16 [42880/50048]	Loss: 1.5131
Training Epoch: 16 [43008/50048]	Loss: 1.3516
Training Epoch: 16 [43136/50048]	Loss: 1.3965
Training Epoch: 16 [43264/50048]	Loss: 1.3870
Training Epoch: 16 [43392/50048]	Loss: 1.5616
Training Epoch: 16 [43520/50048]	Loss: 1.4617
Training Epoch: 16 [43648/50048]	Loss: 1.3587
Training Epoch: 16 [43776/50048]	Loss: 1.4847
Training Epoch: 16 [43904/50048]	Loss: 1.5833
Training Epoch: 16 [44032/50048]	Loss: 1.4500
Training Epoch: 16 [44160/50048]	Loss: 1.8443
Training Epoch: 16 [44288/50048]	Loss: 1.4930
Training Epoch: 16 [44416/50048]	Loss: 1.4685
Training Epoch: 16 [44544/50048]	Loss: 1.5697
Training Epoch: 16 [44672/50048]	Loss: 1.4411
Training Epoch: 16 [44800/50048]	Loss: 1.4188
Training Epoch: 16 [44928/50048]	Loss: 1.3775
Training Epoch: 16 [45056/50048]	Loss: 1.4743
Training Epoch: 16 [45184/50048]	Loss: 1.5952
Training Epoch: 16 [45312/50048]	Loss: 1.5199
Training Epoch: 16 [45440/50048]	Loss: 1.6638
Training Epoch: 16 [45568/50048]	Loss: 1.4553
Training Epoch: 16 [45696/50048]	Loss: 1.5254
Training Epoch: 16 [45824/50048]	Loss: 1.5143
Training Epoch: 16 [45952/50048]	Loss: 1.5567
Training Epoch: 16 [46080/50048]	Loss: 1.3359
Training Epoch: 16 [46208/50048]	Loss: 1.3924
Training Epoch: 16 [46336/50048]	Loss: 1.4933
Training Epoch: 16 [46464/50048]	Loss: 1.4106
Training Epoch: 16 [46592/50048]	Loss: 1.4440
Training Epoch: 16 [46720/50048]	Loss: 1.5878
Training Epoch: 16 [46848/50048]	Loss: 1.5163
Training Epoch: 16 [46976/50048]	Loss: 1.6283
Training Epoch: 16 [47104/50048]	Loss: 1.4632
Training Epoch: 16 [47232/50048]	Loss: 1.3594
Training Epoch: 16 [47360/50048]	Loss: 1.7831
Training Epoch: 16 [47488/50048]	Loss: 1.7282
Training Epoch: 16 [47616/50048]	Loss: 1.4360
Training Epoch: 16 [47744/50048]	Loss: 1.3122
Training Epoch: 16 [47872/50048]	Loss: 1.3914
Training Epoch: 16 [48000/50048]	Loss: 1.3683
Training Epoch: 16 [48128/50048]	Loss: 1.4006
Training Epoch: 16 [48256/50048]	Loss: 1.4710
Training Epoch: 16 [48384/50048]	Loss: 1.8732
Training Epoch: 16 [48512/50048]	Loss: 1.6643
Training Epoch: 16 [48640/50048]	Loss: 1.7178
Training Epoch: 16 [48768/50048]	Loss: 1.5162
Training Epoch: 16 [48896/50048]	Loss: 1.3167
Training Epoch: 16 [49024/50048]	Loss: 1.4806
Training Epoch: 16 [49152/50048]	Loss: 1.6650
Training Epoch: 16 [49280/50048]	Loss: 1.4101
Training Epoch: 16 [49408/50048]	Loss: 1.4331
Training Epoch: 16 [49536/50048]	Loss: 1.4750
Training Epoch: 16 [49664/50048]	Loss: 1.4493
Training Epoch: 16 [49792/50048]	Loss: 1.6051
Training Epoch: 16 [49920/50048]	Loss: 1.2323
Training Epoch: 16 [50048/50048]	Loss: 1.4724
Validation Epoch: 16, Average loss: 0.0130, Accuracy: 0.5471
Training Epoch: 17 [128/50048]	Loss: 1.3300
Training Epoch: 17 [256/50048]	Loss: 1.3179
Training Epoch: 17 [384/50048]	Loss: 1.6508
Training Epoch: 17 [512/50048]	Loss: 1.3334
Training Epoch: 17 [640/50048]	Loss: 1.5046
Training Epoch: 17 [768/50048]	Loss: 1.3860
Training Epoch: 17 [896/50048]	Loss: 1.4120
Training Epoch: 17 [1024/50048]	Loss: 1.5140
Training Epoch: 17 [1152/50048]	Loss: 1.3966
Training Epoch: 17 [1280/50048]	Loss: 1.3026
Training Epoch: 17 [1408/50048]	Loss: 1.4648
Training Epoch: 17 [1536/50048]	Loss: 1.4370
Training Epoch: 17 [1664/50048]	Loss: 1.4867
Training Epoch: 17 [1792/50048]	Loss: 1.5752
Training Epoch: 17 [1920/50048]	Loss: 1.4101
Training Epoch: 17 [2048/50048]	Loss: 1.6062
Training Epoch: 17 [2176/50048]	Loss: 1.2386
Training Epoch: 17 [2304/50048]	Loss: 1.4968
Training Epoch: 17 [2432/50048]	Loss: 1.4813
Training Epoch: 17 [2560/50048]	Loss: 1.8693
Training Epoch: 17 [2688/50048]	Loss: 1.3527
Training Epoch: 17 [2816/50048]	Loss: 1.2074
Training Epoch: 17 [2944/50048]	Loss: 1.5610
Training Epoch: 17 [3072/50048]	Loss: 1.5107
Training Epoch: 17 [3200/50048]	Loss: 1.6027
Training Epoch: 17 [3328/50048]	Loss: 1.5931
Training Epoch: 17 [3456/50048]	Loss: 1.4967
Training Epoch: 17 [3584/50048]	Loss: 1.3325
Training Epoch: 17 [3712/50048]	Loss: 1.5296
Training Epoch: 17 [3840/50048]	Loss: 1.3867
Training Epoch: 17 [3968/50048]	Loss: 1.3753
Training Epoch: 17 [4096/50048]	Loss: 1.5212
Training Epoch: 17 [4224/50048]	Loss: 1.4240
Training Epoch: 17 [4352/50048]	Loss: 1.1498
Training Epoch: 17 [4480/50048]	Loss: 1.5895
Training Epoch: 17 [4608/50048]	Loss: 1.4152
Training Epoch: 17 [4736/50048]	Loss: 1.4520
Training Epoch: 17 [4864/50048]	Loss: 1.0853
Training Epoch: 17 [4992/50048]	Loss: 1.5030
Training Epoch: 17 [5120/50048]	Loss: 1.2632
Training Epoch: 17 [5248/50048]	Loss: 1.2186
Training Epoch: 17 [5376/50048]	Loss: 1.4196
Training Epoch: 17 [5504/50048]	Loss: 1.6450
Training Epoch: 17 [5632/50048]	Loss: 1.4356
Training Epoch: 17 [5760/50048]	Loss: 1.3060
Training Epoch: 17 [5888/50048]	Loss: 1.1907
Training Epoch: 17 [6016/50048]	Loss: 1.8331
Training Epoch: 17 [6144/50048]	Loss: 1.8351
Training Epoch: 17 [6272/50048]	Loss: 1.4403
Training Epoch: 17 [6400/50048]	Loss: 1.4314
Training Epoch: 17 [6528/50048]	Loss: 1.3212
Training Epoch: 17 [6656/50048]	Loss: 1.4062
Training Epoch: 17 [6784/50048]	Loss: 1.4987
Training Epoch: 17 [6912/50048]	Loss: 1.3100
Training Epoch: 17 [7040/50048]	Loss: 1.1689
Training Epoch: 17 [7168/50048]	Loss: 1.4467
Training Epoch: 17 [7296/50048]	Loss: 1.2603
Training Epoch: 17 [7424/50048]	Loss: 1.3571
Training Epoch: 17 [7552/50048]	Loss: 1.6188
Training Epoch: 17 [7680/50048]	Loss: 1.3069
Training Epoch: 17 [7808/50048]	Loss: 1.4124
Training Epoch: 17 [7936/50048]	Loss: 1.6110
Training Epoch: 17 [8064/50048]	Loss: 1.5096
Training Epoch: 17 [8192/50048]	Loss: 1.5210
Training Epoch: 17 [8320/50048]	Loss: 1.5250
Training Epoch: 17 [8448/50048]	Loss: 1.6126
Training Epoch: 17 [8576/50048]	Loss: 1.5251
Training Epoch: 17 [8704/50048]	Loss: 1.4955
Training Epoch: 17 [8832/50048]	Loss: 1.3064
Training Epoch: 17 [8960/50048]	Loss: 1.4076
Training Epoch: 17 [9088/50048]	Loss: 1.6064
Training Epoch: 17 [9216/50048]	Loss: 1.3980
Training Epoch: 17 [9344/50048]	Loss: 1.5570
Training Epoch: 17 [9472/50048]	Loss: 1.3071
Training Epoch: 17 [9600/50048]	Loss: 1.3184
Training Epoch: 17 [9728/50048]	Loss: 1.3697
Training Epoch: 17 [9856/50048]	Loss: 1.6565
Training Epoch: 17 [9984/50048]	Loss: 1.6312
Training Epoch: 17 [10112/50048]	Loss: 1.5927
Training Epoch: 17 [10240/50048]	Loss: 1.2037
Training Epoch: 17 [10368/50048]	Loss: 1.3099
Training Epoch: 17 [10496/50048]	Loss: 1.5261
Training Epoch: 17 [10624/50048]	Loss: 1.3080
Training Epoch: 17 [10752/50048]	Loss: 1.5638
Training Epoch: 17 [10880/50048]	Loss: 1.3936
Training Epoch: 17 [11008/50048]	Loss: 1.3153
Training Epoch: 17 [11136/50048]	Loss: 1.4016
Training Epoch: 17 [11264/50048]	Loss: 1.3927
Training Epoch: 17 [11392/50048]	Loss: 1.4330
Training Epoch: 17 [11520/50048]	Loss: 1.3798
Training Epoch: 17 [11648/50048]	Loss: 1.5239
Training Epoch: 17 [11776/50048]	Loss: 1.4686
Training Epoch: 17 [11904/50048]	Loss: 1.3115
Training Epoch: 17 [12032/50048]	Loss: 1.5067
Training Epoch: 17 [12160/50048]	Loss: 1.3858
Training Epoch: 17 [12288/50048]	Loss: 1.4188
Training Epoch: 17 [12416/50048]	Loss: 1.3667
Training Epoch: 17 [12544/50048]	Loss: 1.6554
Training Epoch: 17 [12672/50048]	Loss: 1.3604
Training Epoch: 17 [12800/50048]	Loss: 1.4915
Training Epoch: 17 [12928/50048]	Loss: 1.6638
Training Epoch: 17 [13056/50048]	Loss: 1.5394
Training Epoch: 17 [13184/50048]	Loss: 1.5758
Training Epoch: 17 [13312/50048]	Loss: 1.4332
Training Epoch: 17 [13440/50048]	Loss: 1.5620
Training Epoch: 17 [13568/50048]	Loss: 1.3245
Training Epoch: 17 [13696/50048]	Loss: 1.5191
Training Epoch: 17 [13824/50048]	Loss: 1.2529
Training Epoch: 17 [13952/50048]	Loss: 1.3570
Training Epoch: 17 [14080/50048]	Loss: 1.4387
Training Epoch: 17 [14208/50048]	Loss: 1.5687
Training Epoch: 17 [14336/50048]	Loss: 1.3810
Training Epoch: 17 [14464/50048]	Loss: 1.2736
Training Epoch: 17 [14592/50048]	Loss: 1.5908
Training Epoch: 17 [14720/50048]	Loss: 1.3918
Training Epoch: 17 [14848/50048]	Loss: 1.4307
Training Epoch: 17 [14976/50048]	Loss: 1.5044
Training Epoch: 17 [15104/50048]	Loss: 1.5180
Training Epoch: 17 [15232/50048]	Loss: 1.5301
Training Epoch: 17 [15360/50048]	Loss: 1.5698
Training Epoch: 17 [15488/50048]	Loss: 1.4974
Training Epoch: 17 [15616/50048]	Loss: 1.3082
Training Epoch: 17 [15744/50048]	Loss: 1.6604
Training Epoch: 17 [15872/50048]	Loss: 1.5543
Training Epoch: 17 [16000/50048]	Loss: 1.4995
Training Epoch: 17 [16128/50048]	Loss: 1.4439
Training Epoch: 17 [16256/50048]	Loss: 1.5450
Training Epoch: 17 [16384/50048]	Loss: 1.4416
Training Epoch: 17 [16512/50048]	Loss: 1.3508
Training Epoch: 17 [16640/50048]	Loss: 1.7716
Training Epoch: 17 [16768/50048]	Loss: 1.4680
Training Epoch: 17 [16896/50048]	Loss: 1.5693
Training Epoch: 17 [17024/50048]	Loss: 1.4177
Training Epoch: 17 [17152/50048]	Loss: 1.4924
Training Epoch: 17 [17280/50048]	Loss: 1.4688
Training Epoch: 17 [17408/50048]	Loss: 1.5403
Training Epoch: 17 [17536/50048]	Loss: 1.5162
Training Epoch: 17 [17664/50048]	Loss: 1.3260
Training Epoch: 17 [17792/50048]	Loss: 1.7515
Training Epoch: 17 [17920/50048]	Loss: 1.3614
Training Epoch: 17 [18048/50048]	Loss: 1.4633
Training Epoch: 17 [18176/50048]	Loss: 1.3675
Training Epoch: 17 [18304/50048]	Loss: 1.4146
Training Epoch: 17 [18432/50048]	Loss: 1.3841
Training Epoch: 17 [18560/50048]	Loss: 1.4292
Training Epoch: 17 [18688/50048]	Loss: 1.2988
Training Epoch: 17 [18816/50048]	Loss: 1.2315
Training Epoch: 17 [18944/50048]	Loss: 1.5381
Training Epoch: 17 [19072/50048]	Loss: 1.3622
Training Epoch: 17 [19200/50048]	Loss: 1.5424
Training Epoch: 17 [19328/50048]	Loss: 1.5727
Training Epoch: 17 [19456/50048]	Loss: 1.3871
Training Epoch: 17 [19584/50048]	Loss: 1.3007
Training Epoch: 17 [19712/50048]	Loss: 1.3790
Training Epoch: 17 [19840/50048]	Loss: 1.5733
Training Epoch: 17 [19968/50048]	Loss: 1.3433
Training Epoch: 17 [20096/50048]	Loss: 1.6446
Training Epoch: 17 [20224/50048]	Loss: 1.6435
Training Epoch: 17 [20352/50048]	Loss: 1.3764
Training Epoch: 17 [20480/50048]	Loss: 1.5142
Training Epoch: 17 [20608/50048]	Loss: 1.6149
Training Epoch: 17 [20736/50048]	Loss: 1.3145
Training Epoch: 17 [20864/50048]	Loss: 1.3581
Training Epoch: 17 [20992/50048]	Loss: 1.4184
Training Epoch: 17 [21120/50048]	Loss: 1.6717
Training Epoch: 17 [21248/50048]	Loss: 1.5572
Training Epoch: 17 [21376/50048]	Loss: 1.4184
Training Epoch: 17 [21504/50048]	Loss: 1.6051
Training Epoch: 17 [21632/50048]	Loss: 1.4479
Training Epoch: 17 [21760/50048]	Loss: 1.3772
Training Epoch: 17 [21888/50048]	Loss: 1.5733
Training Epoch: 17 [22016/50048]	Loss: 1.5010
Training Epoch: 17 [22144/50048]	Loss: 1.5444
Training Epoch: 17 [22272/50048]	Loss: 1.7311
Training Epoch: 17 [22400/50048]	Loss: 1.4660
Training Epoch: 17 [22528/50048]	Loss: 1.6579
Training Epoch: 17 [22656/50048]	Loss: 1.5853
Training Epoch: 17 [22784/50048]	Loss: 1.3717
Training Epoch: 17 [22912/50048]	Loss: 1.4105
Training Epoch: 17 [23040/50048]	Loss: 1.5305
Training Epoch: 17 [23168/50048]	Loss: 1.4847
Training Epoch: 17 [23296/50048]	Loss: 1.4843
Training Epoch: 17 [23424/50048]	Loss: 1.7336
Training Epoch: 17 [23552/50048]	Loss: 1.2928
Training Epoch: 17 [23680/50048]	Loss: 1.6137
Training Epoch: 17 [23808/50048]	Loss: 1.4173
Training Epoch: 17 [23936/50048]	Loss: 1.3788
Training Epoch: 17 [24064/50048]	Loss: 1.3174
Training Epoch: 17 [24192/50048]	Loss: 1.5300
Training Epoch: 17 [24320/50048]	Loss: 1.4775
Training Epoch: 17 [24448/50048]	Loss: 1.4693
Training Epoch: 17 [24576/50048]	Loss: 1.9237
Training Epoch: 17 [24704/50048]	Loss: 1.6194
Training Epoch: 17 [24832/50048]	Loss: 1.4135
Training Epoch: 17 [24960/50048]	Loss: 1.6086
Training Epoch: 17 [25088/50048]	Loss: 1.8444
Training Epoch: 17 [25216/50048]	Loss: 1.4627
Training Epoch: 17 [25344/50048]	Loss: 1.4979
Training Epoch: 17 [25472/50048]	Loss: 1.6364
Training Epoch: 17 [25600/50048]	Loss: 1.6485
Training Epoch: 17 [25728/50048]	Loss: 1.1993
Training Epoch: 17 [25856/50048]	Loss: 1.2759
Training Epoch: 17 [25984/50048]	Loss: 1.6639
Training Epoch: 17 [26112/50048]	Loss: 1.4040
Training Epoch: 17 [26240/50048]	Loss: 1.5814
Training Epoch: 17 [26368/50048]	Loss: 1.6550
Training Epoch: 17 [26496/50048]	Loss: 1.5845
Training Epoch: 17 [26624/50048]	Loss: 1.5502
Training Epoch: 17 [26752/50048]	Loss: 1.2875
Training Epoch: 17 [26880/50048]	Loss: 1.5455
Training Epoch: 17 [27008/50048]	Loss: 1.5694
Training Epoch: 17 [27136/50048]	Loss: 1.1869
Training Epoch: 17 [27264/50048]	Loss: 1.7417
Training Epoch: 17 [27392/50048]	Loss: 1.3280
Training Epoch: 17 [27520/50048]	Loss: 1.3771
Training Epoch: 17 [27648/50048]	Loss: 1.3700
Training Epoch: 17 [27776/50048]	Loss: 1.5666
Training Epoch: 17 [27904/50048]	Loss: 1.4648
Training Epoch: 17 [28032/50048]	Loss: 1.3694
Training Epoch: 17 [28160/50048]	Loss: 1.5424
Training Epoch: 17 [28288/50048]	Loss: 1.5091
Training Epoch: 17 [28416/50048]	Loss: 1.2291
Training Epoch: 17 [28544/50048]	Loss: 1.5353
Training Epoch: 17 [28672/50048]	Loss: 1.4254
Training Epoch: 17 [28800/50048]	Loss: 1.6128
Training Epoch: 17 [28928/50048]	Loss: 1.3744
Training Epoch: 17 [29056/50048]	Loss: 1.0418
Training Epoch: 17 [29184/50048]	Loss: 1.4027
Training Epoch: 17 [29312/50048]	Loss: 1.4618
Training Epoch: 17 [29440/50048]	Loss: 1.4985
Training Epoch: 17 [29568/50048]	Loss: 1.3495
Training Epoch: 17 [29696/50048]	Loss: 1.3523
Training Epoch: 17 [29824/50048]	Loss: 1.4241
Training Epoch: 17 [29952/50048]	Loss: 1.2830
Training Epoch: 17 [30080/50048]	Loss: 1.4340
Training Epoch: 17 [30208/50048]	Loss: 1.4545
Training Epoch: 17 [30336/50048]	Loss: 1.4593
Training Epoch: 17 [30464/50048]	Loss: 1.4656
Training Epoch: 17 [30592/50048]	Loss: 1.2779
Training Epoch: 17 [30720/50048]	Loss: 1.6246
Training Epoch: 17 [30848/50048]	Loss: 1.3424
Training Epoch: 17 [30976/50048]	Loss: 1.5092
Training Epoch: 17 [31104/50048]	Loss: 1.5877
Training Epoch: 17 [31232/50048]	Loss: 1.4076
Training Epoch: 17 [31360/50048]	Loss: 1.8653
Training Epoch: 17 [31488/50048]	Loss: 1.3333
Training Epoch: 17 [31616/50048]	Loss: 1.6521
Training Epoch: 17 [31744/50048]	Loss: 1.5572
Training Epoch: 17 [31872/50048]	Loss: 1.5302
Training Epoch: 17 [32000/50048]	Loss: 1.4010
Training Epoch: 17 [32128/50048]	Loss: 1.4701
Training Epoch: 17 [32256/50048]	Loss: 1.6901
Training Epoch: 17 [32384/50048]	Loss: 1.6111
Training Epoch: 17 [32512/50048]	Loss: 1.7217
Training Epoch: 17 [32640/50048]	Loss: 1.5865
Training Epoch: 17 [32768/50048]	Loss: 1.4224
Training Epoch: 17 [32896/50048]	Loss: 1.4661
Training Epoch: 17 [33024/50048]	Loss: 1.5999
Training Epoch: 17 [33152/50048]	Loss: 1.3613
Training Epoch: 17 [33280/50048]	Loss: 1.5080
Training Epoch: 17 [33408/50048]	Loss: 1.6614
Training Epoch: 17 [33536/50048]	Loss: 1.5596
Training Epoch: 17 [33664/50048]	Loss: 1.3655
Training Epoch: 17 [33792/50048]	Loss: 1.4369
Training Epoch: 17 [33920/50048]	Loss: 1.5512
Training Epoch: 17 [34048/50048]	Loss: 1.7669
Training Epoch: 17 [34176/50048]	Loss: 1.1549
Training Epoch: 17 [34304/50048]	Loss: 1.5046
Training Epoch: 17 [34432/50048]	Loss: 1.8292
Training Epoch: 17 [34560/50048]	Loss: 1.4163
Training Epoch: 17 [34688/50048]	Loss: 1.4673
Training Epoch: 17 [34816/50048]	Loss: 1.4021
Training Epoch: 17 [34944/50048]	Loss: 1.4115
Training Epoch: 17 [35072/50048]	Loss: 1.3091
Training Epoch: 17 [35200/50048]	Loss: 1.5596
Training Epoch: 17 [35328/50048]	Loss: 1.6037
Training Epoch: 17 [35456/50048]	Loss: 1.5528
Training Epoch: 17 [35584/50048]	Loss: 1.3652
Training Epoch: 17 [35712/50048]	Loss: 1.4804
Training Epoch: 17 [35840/50048]	Loss: 1.4188
Training Epoch: 17 [35968/50048]	Loss: 1.5050
Training Epoch: 17 [36096/50048]	Loss: 1.3341
Training Epoch: 17 [36224/50048]	Loss: 1.2060
Training Epoch: 17 [36352/50048]	Loss: 1.4023
Training Epoch: 17 [36480/50048]	Loss: 1.7011
Training Epoch: 17 [36608/50048]	Loss: 1.4291
Training Epoch: 17 [36736/50048]	Loss: 1.7262
Training Epoch: 17 [36864/50048]	Loss: 1.5264
Training Epoch: 17 [36992/50048]	Loss: 1.3058
Training Epoch: 17 [37120/50048]	Loss: 1.7065
Training Epoch: 17 [37248/50048]	Loss: 1.6679
Training Epoch: 17 [37376/50048]	Loss: 1.6541
Training Epoch: 17 [37504/50048]	Loss: 1.4993
Training Epoch: 17 [37632/50048]	Loss: 1.2498
Training Epoch: 17 [37760/50048]	Loss: 1.6706
Training Epoch: 17 [37888/50048]	Loss: 1.4186
Training Epoch: 17 [38016/50048]	Loss: 1.1040
Training Epoch: 17 [38144/50048]	Loss: 1.3756
Training Epoch: 17 [38272/50048]	Loss: 1.3221
Training Epoch: 17 [38400/50048]	Loss: 1.5925
Training Epoch: 17 [38528/50048]	Loss: 1.3059
Training Epoch: 17 [38656/50048]	Loss: 1.8892
Training Epoch: 17 [38784/50048]	Loss: 1.3017
Training Epoch: 17 [38912/50048]	Loss: 1.3523
Training Epoch: 17 [39040/50048]	Loss: 1.1376
Training Epoch: 17 [39168/50048]	Loss: 1.4526
Training Epoch: 17 [39296/50048]	Loss: 1.3284
Training Epoch: 17 [39424/50048]	Loss: 1.6566
Training Epoch: 17 [39552/50048]	Loss: 1.2947
Training Epoch: 17 [39680/50048]	Loss: 1.5170
Training Epoch: 17 [39808/50048]	Loss: 1.2607
Training Epoch: 17 [39936/50048]	Loss: 1.4429
Training Epoch: 17 [40064/50048]	Loss: 1.7755
Training Epoch: 17 [40192/50048]	Loss: 1.5012
Training Epoch: 17 [40320/50048]	Loss: 1.2930
Training Epoch: 17 [40448/50048]	Loss: 1.5998
Training Epoch: 17 [40576/50048]	Loss: 1.1227
Training Epoch: 17 [40704/50048]	Loss: 1.5219
Training Epoch: 17 [40832/50048]	Loss: 1.3362
Training Epoch: 17 [40960/50048]	Loss: 1.4362
Training Epoch: 17 [41088/50048]	Loss: 1.3498
Training Epoch: 17 [41216/50048]	Loss: 1.4659
Training Epoch: 17 [41344/50048]	Loss: 1.4526
Training Epoch: 17 [41472/50048]	Loss: 1.4305
Training Epoch: 17 [41600/50048]	Loss: 1.8118
Training Epoch: 17 [41728/50048]	Loss: 1.3002
Training Epoch: 17 [41856/50048]	Loss: 1.5220
Training Epoch: 17 [41984/50048]	Loss: 1.6109
Training Epoch: 17 [42112/50048]	Loss: 1.4773
Training Epoch: 17 [42240/50048]	Loss: 1.5275
Training Epoch: 17 [42368/50048]	Loss: 1.6813
Training Epoch: 17 [42496/50048]	Loss: 1.3757
Training Epoch: 17 [42624/50048]	Loss: 1.4361
Training Epoch: 17 [42752/50048]	Loss: 1.4919
Training Epoch: 17 [42880/50048]	Loss: 1.2096
Training Epoch: 17 [43008/50048]	Loss: 1.5049
Training Epoch: 17 [43136/50048]	Loss: 1.3643
Training Epoch: 17 [43264/50048]	Loss: 1.6379
Training Epoch: 17 [43392/50048]	Loss: 1.4042
Training Epoch: 17 [43520/50048]	Loss: 1.5762
Training Epoch: 17 [43648/50048]	Loss: 1.4726
Training Epoch: 17 [43776/50048]	Loss: 1.5447
Training Epoch: 17 [43904/50048]	Loss: 1.4652
Training Epoch: 17 [44032/50048]	Loss: 1.5254
Training Epoch: 17 [44160/50048]	Loss: 1.5360
Training Epoch: 17 [44288/50048]	Loss: 1.3692
Training Epoch: 17 [44416/50048]	Loss: 1.6145
Training Epoch: 17 [44544/50048]	Loss: 1.4028
Training Epoch: 17 [44672/50048]	Loss: 1.5423
Training Epoch: 17 [44800/50048]	Loss: 1.2222
Training Epoch: 17 [44928/50048]	Loss: 1.7223
Training Epoch: 17 [45056/50048]	Loss: 1.3774
Training Epoch: 17 [45184/50048]	Loss: 1.6835
Training Epoch: 17 [45312/50048]	Loss: 1.3466
Training Epoch: 17 [45440/50048]	Loss: 1.4035
Training Epoch: 17 [45568/50048]	Loss: 1.2250
Training Epoch: 17 [45696/50048]	Loss: 1.6327
Training Epoch: 17 [45824/50048]	Loss: 1.5744
Training Epoch: 17 [45952/50048]	Loss: 1.3356
Training Epoch: 17 [46080/50048]	Loss: 1.3621
Training Epoch: 17 [46208/50048]	Loss: 1.5070
Training Epoch: 17 [46336/50048]	Loss: 1.2275
Training Epoch: 17 [46464/50048]	Loss: 1.5851
Training Epoch: 17 [46592/50048]	Loss: 1.2344
Training Epoch: 17 [46720/50048]	Loss: 1.4810
Training Epoch: 17 [46848/50048]	Loss: 1.4457
Training Epoch: 17 [46976/50048]	Loss: 1.5090
Training Epoch: 17 [47104/50048]	Loss: 1.3565
Training Epoch: 17 [47232/50048]	Loss: 1.3424
Training Epoch: 17 [47360/50048]	Loss: 1.5901
Training Epoch: 17 [47488/50048]	Loss: 1.2766
Training Epoch: 17 [47616/50048]	Loss: 1.3427
Training Epoch: 17 [47744/50048]	Loss: 1.7720
Training Epoch: 17 [47872/50048]	Loss: 1.4086
Training Epoch: 17 [48000/50048]	Loss: 1.7264
Training Epoch: 17 [48128/50048]	Loss: 1.3476
Training Epoch: 17 [48256/50048]	Loss: 1.5712
Training Epoch: 17 [48384/50048]	Loss: 1.4679
Training Epoch: 17 [48512/50048]	Loss: 1.4593
Training Epoch: 17 [48640/50048]	Loss: 1.3516
Training Epoch: 17 [48768/50048]	Loss: 1.2955
Training Epoch: 17 [48896/50048]	Loss: 1.4020
Training Epoch: 17 [49024/50048]	Loss: 1.4539
Training Epoch: 17 [49152/50048]	Loss: 1.4655
Training Epoch: 17 [49280/50048]	Loss: 1.6609
Training Epoch: 17 [49408/50048]	Loss: 1.6140
Training Epoch: 17 [49536/50048]	Loss: 1.5477
Training Epoch: 17 [49664/50048]	Loss: 1.7730
Training Epoch: 17 [49792/50048]	Loss: 1.5030
Training Epoch: 17 [49920/50048]	Loss: 1.4153
Training Epoch: 17 [50048/50048]	Loss: 1.6288
Validation Epoch: 17, Average loss: 0.0129, Accuracy: 0.5504
Training Epoch: 18 [128/50048]	Loss: 1.3705
Training Epoch: 18 [256/50048]	Loss: 1.4859
Training Epoch: 18 [384/50048]	Loss: 1.3005
Training Epoch: 18 [512/50048]	Loss: 1.4567
Training Epoch: 18 [640/50048]	Loss: 1.2038
Training Epoch: 18 [768/50048]	Loss: 1.4908
Training Epoch: 18 [896/50048]	Loss: 1.4087
Training Epoch: 18 [1024/50048]	Loss: 1.1641
Training Epoch: 18 [1152/50048]	Loss: 1.4922
Training Epoch: 18 [1280/50048]	Loss: 1.4508
Training Epoch: 18 [1408/50048]	Loss: 1.3137
Training Epoch: 18 [1536/50048]	Loss: 1.4792
Training Epoch: 18 [1664/50048]	Loss: 1.2028
Training Epoch: 18 [1792/50048]	Loss: 1.0832
Training Epoch: 18 [1920/50048]	Loss: 1.3871
Training Epoch: 18 [2048/50048]	Loss: 1.4220
Training Epoch: 18 [2176/50048]	Loss: 1.2589
Training Epoch: 18 [2304/50048]	Loss: 1.3396
Training Epoch: 18 [2432/50048]	Loss: 1.5405
Training Epoch: 18 [2560/50048]	Loss: 1.3033
Training Epoch: 18 [2688/50048]	Loss: 1.3909
Training Epoch: 18 [2816/50048]	Loss: 1.5120
Training Epoch: 18 [2944/50048]	Loss: 1.2828
Training Epoch: 18 [3072/50048]	Loss: 1.4409
Training Epoch: 18 [3200/50048]	Loss: 1.7173
Training Epoch: 18 [3328/50048]	Loss: 1.4980
Training Epoch: 18 [3456/50048]	Loss: 1.5096
Training Epoch: 18 [3584/50048]	Loss: 1.2646
Training Epoch: 18 [3712/50048]	Loss: 1.4322
Training Epoch: 18 [3840/50048]	Loss: 1.4683
Training Epoch: 18 [3968/50048]	Loss: 1.3567
Training Epoch: 18 [4096/50048]	Loss: 1.4109
Training Epoch: 18 [4224/50048]	Loss: 1.3419
Training Epoch: 18 [4352/50048]	Loss: 1.2404
Training Epoch: 18 [4480/50048]	Loss: 1.2153
Training Epoch: 18 [4608/50048]	Loss: 1.2063
Training Epoch: 18 [4736/50048]	Loss: 1.1617
Training Epoch: 18 [4864/50048]	Loss: 1.3775
Training Epoch: 18 [4992/50048]	Loss: 1.3192
Training Epoch: 18 [5120/50048]	Loss: 1.3600
Training Epoch: 18 [5248/50048]	Loss: 1.5296
Training Epoch: 18 [5376/50048]	Loss: 1.3115
Training Epoch: 18 [5504/50048]	Loss: 1.7305
Training Epoch: 18 [5632/50048]	Loss: 1.4380
Training Epoch: 18 [5760/50048]	Loss: 1.4520
Training Epoch: 18 [5888/50048]	Loss: 1.5497
Training Epoch: 18 [6016/50048]	Loss: 1.3257
Training Epoch: 18 [6144/50048]	Loss: 1.4685
Training Epoch: 18 [6272/50048]	Loss: 1.3317
Training Epoch: 18 [6400/50048]	Loss: 1.4713
Training Epoch: 18 [6528/50048]	Loss: 1.4004
Training Epoch: 18 [6656/50048]	Loss: 1.4275
Training Epoch: 18 [6784/50048]	Loss: 1.3905
Training Epoch: 18 [6912/50048]	Loss: 1.5725
Training Epoch: 18 [7040/50048]	Loss: 1.5054
Training Epoch: 18 [7168/50048]	Loss: 1.5837
Training Epoch: 18 [7296/50048]	Loss: 1.8091
Training Epoch: 18 [7424/50048]	Loss: 1.3395
Training Epoch: 18 [7552/50048]	Loss: 1.3049
Training Epoch: 18 [7680/50048]	Loss: 1.2926
Training Epoch: 18 [7808/50048]	Loss: 1.4937
Training Epoch: 18 [7936/50048]	Loss: 1.3853
Training Epoch: 18 [8064/50048]	Loss: 1.5828
Training Epoch: 18 [8192/50048]	Loss: 1.4943
Training Epoch: 18 [8320/50048]	Loss: 1.3486
Training Epoch: 18 [8448/50048]	Loss: 1.3633
Training Epoch: 18 [8576/50048]	Loss: 1.2030
Training Epoch: 18 [8704/50048]	Loss: 1.4233
Training Epoch: 18 [8832/50048]	Loss: 1.2860
Training Epoch: 18 [8960/50048]	Loss: 1.3535
Training Epoch: 18 [9088/50048]	Loss: 1.3599
Training Epoch: 18 [9216/50048]	Loss: 1.3281
Training Epoch: 18 [9344/50048]	Loss: 1.1373
Training Epoch: 18 [9472/50048]	Loss: 1.3770
Training Epoch: 18 [9600/50048]	Loss: 1.2580
Training Epoch: 18 [9728/50048]	Loss: 1.5068
Training Epoch: 18 [9856/50048]	Loss: 1.4396
Training Epoch: 18 [9984/50048]	Loss: 1.3477
Training Epoch: 18 [10112/50048]	Loss: 1.6754
Training Epoch: 18 [10240/50048]	Loss: 1.3268
Training Epoch: 18 [10368/50048]	Loss: 1.4968
Training Epoch: 18 [10496/50048]	Loss: 1.3109
Training Epoch: 18 [10624/50048]	Loss: 1.4865
Training Epoch: 18 [10752/50048]	Loss: 1.3509
Training Epoch: 18 [10880/50048]	Loss: 1.4258
Training Epoch: 18 [11008/50048]	Loss: 1.2325
Training Epoch: 18 [11136/50048]	Loss: 1.5130
Training Epoch: 18 [11264/50048]	Loss: 1.5432
Training Epoch: 18 [11392/50048]	Loss: 1.1869
Training Epoch: 18 [11520/50048]	Loss: 1.4271
Training Epoch: 18 [11648/50048]	Loss: 1.4422
Training Epoch: 18 [11776/50048]	Loss: 1.4452
Training Epoch: 18 [11904/50048]	Loss: 1.4630
Training Epoch: 18 [12032/50048]	Loss: 1.3632
Training Epoch: 18 [12160/50048]	Loss: 1.4453
Training Epoch: 18 [12288/50048]	Loss: 1.6024
Training Epoch: 18 [12416/50048]	Loss: 1.1642
Training Epoch: 18 [12544/50048]	Loss: 1.2202
Training Epoch: 18 [12672/50048]	Loss: 1.5957
Training Epoch: 18 [12800/50048]	Loss: 1.2724
Training Epoch: 18 [12928/50048]	Loss: 1.4507
Training Epoch: 18 [13056/50048]	Loss: 1.4802
Training Epoch: 18 [13184/50048]	Loss: 1.1309
Training Epoch: 18 [13312/50048]	Loss: 1.6230
Training Epoch: 18 [13440/50048]	Loss: 1.2619
Training Epoch: 18 [13568/50048]	Loss: 1.3638
Training Epoch: 18 [13696/50048]	Loss: 1.5491
Training Epoch: 18 [13824/50048]	Loss: 1.3720
Training Epoch: 18 [13952/50048]	Loss: 1.3361
Training Epoch: 18 [14080/50048]	Loss: 1.3027
Training Epoch: 18 [14208/50048]	Loss: 1.4576
Training Epoch: 18 [14336/50048]	Loss: 1.5372
Training Epoch: 18 [14464/50048]	Loss: 1.1568
Training Epoch: 18 [14592/50048]	Loss: 1.4007
Training Epoch: 18 [14720/50048]	Loss: 1.3845
Training Epoch: 18 [14848/50048]	Loss: 1.6058
Training Epoch: 18 [14976/50048]	Loss: 1.3228
Training Epoch: 18 [15104/50048]	Loss: 1.3709
Training Epoch: 18 [15232/50048]	Loss: 1.6607
Training Epoch: 18 [15360/50048]	Loss: 1.1875
Training Epoch: 18 [15488/50048]	Loss: 1.6277
Training Epoch: 18 [15616/50048]	Loss: 1.2350
Training Epoch: 18 [15744/50048]	Loss: 1.4746
Training Epoch: 18 [15872/50048]	Loss: 1.3361
Training Epoch: 18 [16000/50048]	Loss: 1.5114
Training Epoch: 18 [16128/50048]	Loss: 1.3032
Training Epoch: 18 [16256/50048]	Loss: 1.7007
Training Epoch: 18 [16384/50048]	Loss: 1.3629
Training Epoch: 18 [16512/50048]	Loss: 1.3694
Training Epoch: 18 [16640/50048]	Loss: 1.3561
Training Epoch: 18 [16768/50048]	Loss: 1.3548
Training Epoch: 18 [16896/50048]	Loss: 1.3684
Training Epoch: 18 [17024/50048]	Loss: 1.1414
Training Epoch: 18 [17152/50048]	Loss: 1.3200
Training Epoch: 18 [17280/50048]	Loss: 1.3507
Training Epoch: 18 [17408/50048]	Loss: 1.3053
Training Epoch: 18 [17536/50048]	Loss: 1.6093
Training Epoch: 18 [17664/50048]	Loss: 1.1999
Training Epoch: 18 [17792/50048]	Loss: 1.4679
Training Epoch: 18 [17920/50048]	Loss: 1.3532
Training Epoch: 18 [18048/50048]	Loss: 1.3939
Training Epoch: 18 [18176/50048]	Loss: 1.5238
Training Epoch: 18 [18304/50048]	Loss: 1.4741
Training Epoch: 18 [18432/50048]	Loss: 1.3432
Training Epoch: 18 [18560/50048]	Loss: 1.4464
Training Epoch: 18 [18688/50048]	Loss: 1.2292
Training Epoch: 18 [18816/50048]	Loss: 1.5433
Training Epoch: 18 [18944/50048]	Loss: 1.5201
Training Epoch: 18 [19072/50048]	Loss: 1.2138
Training Epoch: 18 [19200/50048]	Loss: 1.5954
Training Epoch: 18 [19328/50048]	Loss: 1.6489
Training Epoch: 18 [19456/50048]	Loss: 1.2842
Training Epoch: 18 [19584/50048]	Loss: 1.0925
Training Epoch: 18 [19712/50048]	Loss: 1.3151
Training Epoch: 18 [19840/50048]	Loss: 1.3922
Training Epoch: 18 [19968/50048]	Loss: 1.5478
Training Epoch: 18 [20096/50048]	Loss: 1.4820
Training Epoch: 18 [20224/50048]	Loss: 1.3572
Training Epoch: 18 [20352/50048]	Loss: 1.5201
Training Epoch: 18 [20480/50048]	Loss: 1.5128
Training Epoch: 18 [20608/50048]	Loss: 1.5082
Training Epoch: 18 [20736/50048]	Loss: 1.4534
Training Epoch: 18 [20864/50048]	Loss: 1.3639
Training Epoch: 18 [20992/50048]	Loss: 1.4554
Training Epoch: 18 [21120/50048]	Loss: 1.4473
Training Epoch: 18 [21248/50048]	Loss: 1.5062
Training Epoch: 18 [21376/50048]	Loss: 1.4672
Training Epoch: 18 [21504/50048]	Loss: 1.4204
Training Epoch: 18 [21632/50048]	Loss: 1.6430
Training Epoch: 18 [21760/50048]	Loss: 1.3754
Training Epoch: 18 [21888/50048]	Loss: 1.5781
Training Epoch: 18 [22016/50048]	Loss: 1.4060
Training Epoch: 18 [22144/50048]	Loss: 1.3555
Training Epoch: 18 [22272/50048]	Loss: 1.4891
Training Epoch: 18 [22400/50048]	Loss: 1.3651
Training Epoch: 18 [22528/50048]	Loss: 1.2516
Training Epoch: 18 [22656/50048]	Loss: 1.3932
Training Epoch: 18 [22784/50048]	Loss: 1.4028
Training Epoch: 18 [22912/50048]	Loss: 1.3934
Training Epoch: 18 [23040/50048]	Loss: 1.3205
Training Epoch: 18 [23168/50048]	Loss: 1.3947
Training Epoch: 18 [23296/50048]	Loss: 1.3423
Training Epoch: 18 [23424/50048]	Loss: 1.2528
Training Epoch: 18 [23552/50048]	Loss: 1.5355
Training Epoch: 18 [23680/50048]	Loss: 1.4273
Training Epoch: 18 [23808/50048]	Loss: 1.3457
Training Epoch: 18 [23936/50048]	Loss: 1.6220
Training Epoch: 18 [24064/50048]	Loss: 1.4236
Training Epoch: 18 [24192/50048]	Loss: 0.9215
Training Epoch: 18 [24320/50048]	Loss: 1.4714
Training Epoch: 18 [24448/50048]	Loss: 1.3654
Training Epoch: 18 [24576/50048]	Loss: 1.6095
Training Epoch: 18 [24704/50048]	Loss: 1.3709
Training Epoch: 18 [24832/50048]	Loss: 1.3614
Training Epoch: 18 [24960/50048]	Loss: 1.3086
Training Epoch: 18 [25088/50048]	Loss: 1.4099
Training Epoch: 18 [25216/50048]	Loss: 1.5247
Training Epoch: 18 [25344/50048]	Loss: 1.5815
Training Epoch: 18 [25472/50048]	Loss: 1.4297
Training Epoch: 18 [25600/50048]	Loss: 1.1838
Training Epoch: 18 [25728/50048]	Loss: 1.3930
Training Epoch: 18 [25856/50048]	Loss: 1.6703
Training Epoch: 18 [25984/50048]	Loss: 1.3355
Training Epoch: 18 [26112/50048]	Loss: 1.5594
Training Epoch: 18 [26240/50048]	Loss: 1.3847
Training Epoch: 18 [26368/50048]	Loss: 1.5154
Training Epoch: 18 [26496/50048]	Loss: 1.5751
Training Epoch: 18 [26624/50048]	Loss: 1.7279
Training Epoch: 18 [26752/50048]	Loss: 1.3855
Training Epoch: 18 [26880/50048]	Loss: 1.3835
Training Epoch: 18 [27008/50048]	Loss: 1.4532
Training Epoch: 18 [27136/50048]	Loss: 1.4552
Training Epoch: 18 [27264/50048]	Loss: 1.3127
Training Epoch: 18 [27392/50048]	Loss: 1.3869
Training Epoch: 18 [27520/50048]	Loss: 1.3942
Training Epoch: 18 [27648/50048]	Loss: 1.6436
Training Epoch: 18 [27776/50048]	Loss: 1.5340
Training Epoch: 18 [27904/50048]	Loss: 1.4192
Training Epoch: 18 [28032/50048]	Loss: 1.3672
Training Epoch: 18 [28160/50048]	Loss: 1.4811
Training Epoch: 18 [28288/50048]	Loss: 1.5396
Training Epoch: 18 [28416/50048]	Loss: 1.7184
Training Epoch: 18 [28544/50048]	Loss: 1.4958
Training Epoch: 18 [28672/50048]	Loss: 1.2309
Training Epoch: 18 [28800/50048]	Loss: 1.3214
Training Epoch: 18 [28928/50048]	Loss: 1.2401
Training Epoch: 18 [29056/50048]	Loss: 1.5374
Training Epoch: 18 [29184/50048]	Loss: 1.3507
Training Epoch: 18 [29312/50048]	Loss: 1.6591
Training Epoch: 18 [29440/50048]	Loss: 1.4470
Training Epoch: 18 [29568/50048]	Loss: 1.3868
Training Epoch: 18 [29696/50048]	Loss: 1.5130
Training Epoch: 18 [29824/50048]	Loss: 1.3836
Training Epoch: 18 [29952/50048]	Loss: 1.3895
Training Epoch: 18 [30080/50048]	Loss: 1.4326
Training Epoch: 18 [30208/50048]	Loss: 1.5011
Training Epoch: 18 [30336/50048]	Loss: 1.4315
Training Epoch: 18 [30464/50048]	Loss: 1.5529
Training Epoch: 18 [30592/50048]	Loss: 1.3617
Training Epoch: 18 [30720/50048]	Loss: 1.5902
Training Epoch: 18 [30848/50048]	Loss: 1.7705
Training Epoch: 18 [30976/50048]	Loss: 1.5565
Training Epoch: 18 [31104/50048]	Loss: 1.3971
Training Epoch: 18 [31232/50048]	Loss: 1.5394
Training Epoch: 18 [31360/50048]	Loss: 1.4173
Training Epoch: 18 [31488/50048]	Loss: 1.1400
Training Epoch: 18 [31616/50048]	Loss: 1.2241
Training Epoch: 18 [31744/50048]	Loss: 1.6475
Training Epoch: 18 [31872/50048]	Loss: 1.3578
Training Epoch: 18 [32000/50048]	Loss: 1.5333
Training Epoch: 18 [32128/50048]	Loss: 1.5143
Training Epoch: 18 [32256/50048]	Loss: 1.5423
Training Epoch: 18 [32384/50048]	Loss: 1.3206
Training Epoch: 18 [32512/50048]	Loss: 1.3457
Training Epoch: 18 [32640/50048]	Loss: 1.3983
Training Epoch: 18 [32768/50048]	Loss: 1.6307
Training Epoch: 18 [32896/50048]	Loss: 1.1933
Training Epoch: 18 [33024/50048]	Loss: 1.1088
Training Epoch: 18 [33152/50048]	Loss: 1.4613
Training Epoch: 18 [33280/50048]	Loss: 1.4697
Training Epoch: 18 [33408/50048]	Loss: 1.6614
Training Epoch: 18 [33536/50048]	Loss: 1.5453
Training Epoch: 18 [33664/50048]	Loss: 1.4467
Training Epoch: 18 [33792/50048]	Loss: 1.4639
Training Epoch: 18 [33920/50048]	Loss: 1.6419
Training Epoch: 18 [34048/50048]	Loss: 1.4468
Training Epoch: 18 [34176/50048]	Loss: 1.3962
Training Epoch: 18 [34304/50048]	Loss: 1.5105
Training Epoch: 18 [34432/50048]	Loss: 1.5478
Training Epoch: 18 [34560/50048]	Loss: 1.5151
Training Epoch: 18 [34688/50048]	Loss: 1.4867
Training Epoch: 18 [34816/50048]	Loss: 1.4159
Training Epoch: 18 [34944/50048]	Loss: 1.3531
Training Epoch: 18 [35072/50048]	Loss: 1.3130
Training Epoch: 18 [35200/50048]	Loss: 1.6057
Training Epoch: 18 [35328/50048]	Loss: 1.4942
Training Epoch: 18 [35456/50048]	Loss: 1.6181
Training Epoch: 18 [35584/50048]	Loss: 1.5552
Training Epoch: 18 [35712/50048]	Loss: 1.5633
Training Epoch: 18 [35840/50048]	Loss: 1.4892
Training Epoch: 18 [35968/50048]	Loss: 1.5442
Training Epoch: 18 [36096/50048]	Loss: 1.6186
Training Epoch: 18 [36224/50048]	Loss: 1.4546
Training Epoch: 18 [36352/50048]	Loss: 1.5650
Training Epoch: 18 [36480/50048]	Loss: 1.7828
Training Epoch: 18 [36608/50048]	Loss: 1.4200
Training Epoch: 18 [36736/50048]	Loss: 1.4381
Training Epoch: 18 [36864/50048]	Loss: 1.5108
Training Epoch: 18 [36992/50048]	Loss: 1.3746
Training Epoch: 18 [37120/50048]	Loss: 1.4865
Training Epoch: 18 [37248/50048]	Loss: 1.5606
Training Epoch: 18 [37376/50048]	Loss: 1.3113
Training Epoch: 18 [37504/50048]	Loss: 1.3763
Training Epoch: 18 [37632/50048]	Loss: 1.3267
Training Epoch: 18 [37760/50048]	Loss: 1.4374
Training Epoch: 18 [37888/50048]	Loss: 1.5165
Training Epoch: 18 [38016/50048]	Loss: 1.5496
Training Epoch: 18 [38144/50048]	Loss: 1.5841
Training Epoch: 18 [38272/50048]	Loss: 1.4910
Training Epoch: 18 [38400/50048]	Loss: 1.3839
Training Epoch: 18 [38528/50048]	Loss: 1.4357
Training Epoch: 18 [38656/50048]	Loss: 1.4560
Training Epoch: 18 [38784/50048]	Loss: 1.3899
Training Epoch: 18 [38912/50048]	Loss: 1.3555
Training Epoch: 18 [39040/50048]	Loss: 1.4023
Training Epoch: 18 [39168/50048]	Loss: 1.4744
Training Epoch: 18 [39296/50048]	Loss: 1.3460
Training Epoch: 18 [39424/50048]	Loss: 1.4613
Training Epoch: 18 [39552/50048]	Loss: 1.6017
Training Epoch: 18 [39680/50048]	Loss: 1.5474
Training Epoch: 18 [39808/50048]	Loss: 1.4633
Training Epoch: 18 [39936/50048]	Loss: 1.5324
Training Epoch: 18 [40064/50048]	Loss: 1.5425
Training Epoch: 18 [40192/50048]	Loss: 1.2288
Training Epoch: 18 [40320/50048]	Loss: 1.5140
Training Epoch: 18 [40448/50048]	Loss: 1.6408
Training Epoch: 18 [40576/50048]	Loss: 1.3313
Training Epoch: 18 [40704/50048]	Loss: 1.6465
Training Epoch: 18 [40832/50048]	Loss: 1.5410
Training Epoch: 18 [40960/50048]	Loss: 1.3357
Training Epoch: 18 [41088/50048]	Loss: 1.2999
Training Epoch: 18 [41216/50048]	Loss: 1.5293
Training Epoch: 18 [41344/50048]	Loss: 1.5232
Training Epoch: 18 [41472/50048]	Loss: 1.5725
Training Epoch: 18 [41600/50048]	Loss: 1.3599
Training Epoch: 18 [41728/50048]	Loss: 1.4314
Training Epoch: 18 [41856/50048]	Loss: 1.6478
Training Epoch: 18 [41984/50048]	Loss: 1.4517
Training Epoch: 18 [42112/50048]	Loss: 1.6198
Training Epoch: 18 [42240/50048]	Loss: 1.2908
Training Epoch: 18 [42368/50048]	Loss: 1.3363
Training Epoch: 18 [42496/50048]	Loss: 1.5718
Training Epoch: 18 [42624/50048]	Loss: 1.3177
Training Epoch: 18 [42752/50048]	Loss: 1.4419
Training Epoch: 18 [42880/50048]	Loss: 1.4790
Training Epoch: 18 [43008/50048]	Loss: 1.5429
Training Epoch: 18 [43136/50048]	Loss: 1.3229
Training Epoch: 18 [43264/50048]	Loss: 1.5971
Training Epoch: 18 [43392/50048]	Loss: 1.4345
Training Epoch: 18 [43520/50048]	Loss: 1.4313
Training Epoch: 18 [43648/50048]	Loss: 1.3594
Training Epoch: 18 [43776/50048]	Loss: 1.3298
Training Epoch: 18 [43904/50048]	Loss: 1.5866
Training Epoch: 18 [44032/50048]	Loss: 1.4221
Training Epoch: 18 [44160/50048]	Loss: 1.1549
Training Epoch: 18 [44288/50048]	Loss: 1.4414
Training Epoch: 18 [44416/50048]	Loss: 1.2148
Training Epoch: 18 [44544/50048]	Loss: 1.3112
Training Epoch: 18 [44672/50048]	Loss: 1.6526
Training Epoch: 18 [44800/50048]	Loss: 1.4311
Training Epoch: 18 [44928/50048]	Loss: 1.3308
Training Epoch: 18 [45056/50048]	Loss: 1.3591
Training Epoch: 18 [45184/50048]	Loss: 1.4382
Training Epoch: 18 [45312/50048]	Loss: 1.2708
Training Epoch: 18 [45440/50048]	Loss: 1.3817
Training Epoch: 18 [45568/50048]	Loss: 1.4510
Training Epoch: 18 [45696/50048]	Loss: 1.3919
Training Epoch: 18 [45824/50048]	Loss: 1.5008
Training Epoch: 18 [45952/50048]	Loss: 1.4972
Training Epoch: 18 [46080/50048]	Loss: 1.4931
Training Epoch: 18 [46208/50048]	Loss: 1.5350
Training Epoch: 18 [46336/50048]	Loss: 1.2550
Training Epoch: 18 [46464/50048]	Loss: 1.3899
Training Epoch: 18 [46592/50048]	Loss: 1.4791
Training Epoch: 18 [46720/50048]	Loss: 1.4044
Training Epoch: 18 [46848/50048]	Loss: 1.7290
Training Epoch: 18 [46976/50048]	Loss: 1.3926
Training Epoch: 18 [47104/50048]	Loss: 1.4664
Training Epoch: 18 [47232/50048]	Loss: 1.3299
Training Epoch: 18 [47360/50048]	Loss: 1.4993
Training Epoch: 18 [47488/50048]	Loss: 1.4411
Training Epoch: 18 [47616/50048]	Loss: 1.3035
Training Epoch: 18 [47744/50048]	Loss: 1.5804
Training Epoch: 18 [47872/50048]	Loss: 1.4406
Training Epoch: 18 [48000/50048]	Loss: 1.3820
Training Epoch: 18 [48128/50048]	Loss: 1.3404
Training Epoch: 18 [48256/50048]	Loss: 1.6230
Training Epoch: 18 [48384/50048]	Loss: 1.4968
Training Epoch: 18 [48512/50048]	Loss: 1.3456
Training Epoch: 18 [48640/50048]	Loss: 1.4497
Training Epoch: 18 [48768/50048]	Loss: 1.4242
Training Epoch: 18 [48896/50048]	Loss: 1.3630
Training Epoch: 18 [49024/50048]	Loss: 1.4574
Training Epoch: 18 [49152/50048]	Loss: 1.4792
Training Epoch: 18 [49280/50048]	Loss: 1.4666
Training Epoch: 18 [49408/50048]	Loss: 1.5853
Training Epoch: 18 [49536/50048]	Loss: 1.3984
Training Epoch: 18 [49664/50048]	Loss: 1.5083
Training Epoch: 18 [49792/50048]	Loss: 1.3263
Training Epoch: 18 [49920/50048]	Loss: 1.4521
Training Epoch: 18 [50048/50048]	Loss: 1.5809
Validation Epoch: 18, Average loss: 0.0126, Accuracy: 0.5608
Training Epoch: 19 [128/50048]	Loss: 1.3707
Training Epoch: 19 [256/50048]	Loss: 1.4486
Training Epoch: 19 [384/50048]	Loss: 1.4466
Training Epoch: 19 [512/50048]	Loss: 1.3986
Training Epoch: 19 [640/50048]	Loss: 1.4050
Training Epoch: 19 [768/50048]	Loss: 1.4208
Training Epoch: 19 [896/50048]	Loss: 1.1349
Training Epoch: 19 [1024/50048]	Loss: 1.2267
Training Epoch: 19 [1152/50048]	Loss: 1.4884
Training Epoch: 19 [1280/50048]	Loss: 1.2974
Training Epoch: 19 [1408/50048]	Loss: 1.2823
Training Epoch: 19 [1536/50048]	Loss: 1.3010
Training Epoch: 19 [1664/50048]	Loss: 1.2938
Training Epoch: 19 [1792/50048]	Loss: 1.2800
Training Epoch: 19 [1920/50048]	Loss: 1.5501
Training Epoch: 19 [2048/50048]	Loss: 1.4515
Training Epoch: 19 [2176/50048]	Loss: 1.4677
Training Epoch: 19 [2304/50048]	Loss: 1.2820
Training Epoch: 19 [2432/50048]	Loss: 1.3102
Training Epoch: 19 [2560/50048]	Loss: 1.5280
Training Epoch: 19 [2688/50048]	Loss: 1.4595
Training Epoch: 19 [2816/50048]	Loss: 1.2488
Training Epoch: 19 [2944/50048]	Loss: 1.4246
Training Epoch: 19 [3072/50048]	Loss: 1.2335
Training Epoch: 19 [3200/50048]	Loss: 1.2113
Training Epoch: 19 [3328/50048]	Loss: 1.3311
Training Epoch: 19 [3456/50048]	Loss: 1.4056
Training Epoch: 19 [3584/50048]	Loss: 1.4639
Training Epoch: 19 [3712/50048]	Loss: 1.4656
Training Epoch: 19 [3840/50048]	Loss: 1.3033
Training Epoch: 19 [3968/50048]	Loss: 1.5510
Training Epoch: 19 [4096/50048]	Loss: 1.4779
Training Epoch: 19 [4224/50048]	Loss: 1.2567
Training Epoch: 19 [4352/50048]	Loss: 1.5553
Training Epoch: 19 [4480/50048]	Loss: 1.3602
Training Epoch: 19 [4608/50048]	Loss: 1.5065
Training Epoch: 19 [4736/50048]	Loss: 1.5448
Training Epoch: 19 [4864/50048]	Loss: 1.5622
Training Epoch: 19 [4992/50048]	Loss: 1.3302
Training Epoch: 19 [5120/50048]	Loss: 1.2257
Training Epoch: 19 [5248/50048]	Loss: 1.5237
Training Epoch: 19 [5376/50048]	Loss: 1.4673
Training Epoch: 19 [5504/50048]	Loss: 1.2745
Training Epoch: 19 [5632/50048]	Loss: 1.3023
Training Epoch: 19 [5760/50048]	Loss: 1.5021
Training Epoch: 19 [5888/50048]	Loss: 1.0663
Training Epoch: 19 [6016/50048]	Loss: 1.1877
Training Epoch: 19 [6144/50048]	Loss: 1.4592
Training Epoch: 19 [6272/50048]	Loss: 1.3289
Training Epoch: 19 [6400/50048]	Loss: 1.3318
Training Epoch: 19 [6528/50048]	Loss: 1.4040
Training Epoch: 19 [6656/50048]	Loss: 1.1594
Training Epoch: 19 [6784/50048]	Loss: 1.4556
Training Epoch: 19 [6912/50048]	Loss: 1.4107
Training Epoch: 19 [7040/50048]	Loss: 1.3078
Training Epoch: 19 [7168/50048]	Loss: 1.2923
Training Epoch: 19 [7296/50048]	Loss: 1.4950
Training Epoch: 19 [7424/50048]	Loss: 1.4357
Training Epoch: 19 [7552/50048]	Loss: 1.3559
Training Epoch: 19 [7680/50048]	Loss: 1.2393
Training Epoch: 19 [7808/50048]	Loss: 1.3156
Training Epoch: 19 [7936/50048]	Loss: 1.2600
Training Epoch: 19 [8064/50048]	Loss: 1.5329
Training Epoch: 19 [8192/50048]	Loss: 1.5593
Training Epoch: 19 [8320/50048]	Loss: 1.1840
Training Epoch: 19 [8448/50048]	Loss: 1.5668
Training Epoch: 19 [8576/50048]	Loss: 1.5404
Training Epoch: 19 [8704/50048]	Loss: 1.5329
Training Epoch: 19 [8832/50048]	Loss: 1.1566
Training Epoch: 19 [8960/50048]	Loss: 1.7257
Training Epoch: 19 [9088/50048]	Loss: 1.1909
Training Epoch: 19 [9216/50048]	Loss: 1.5036
Training Epoch: 19 [9344/50048]	Loss: 1.4293
Training Epoch: 19 [9472/50048]	Loss: 1.2066
Training Epoch: 19 [9600/50048]	Loss: 1.3146
Training Epoch: 19 [9728/50048]	Loss: 1.4282
Training Epoch: 19 [9856/50048]	Loss: 1.1276
Training Epoch: 19 [9984/50048]	Loss: 1.3053
Training Epoch: 19 [10112/50048]	Loss: 1.1771
Training Epoch: 19 [10240/50048]	Loss: 1.4238
Training Epoch: 19 [10368/50048]	Loss: 1.4956
Training Epoch: 19 [10496/50048]	Loss: 1.1957
Training Epoch: 19 [10624/50048]	Loss: 1.3216
Training Epoch: 19 [10752/50048]	Loss: 1.2662
Training Epoch: 19 [10880/50048]	Loss: 1.4117
Training Epoch: 19 [11008/50048]	Loss: 1.5091
Training Epoch: 19 [11136/50048]	Loss: 1.6534
Training Epoch: 19 [11264/50048]	Loss: 1.3756
Training Epoch: 19 [11392/50048]	Loss: 1.4397
Training Epoch: 19 [11520/50048]	Loss: 1.1977
Training Epoch: 19 [11648/50048]	Loss: 1.3431
Training Epoch: 19 [11776/50048]	Loss: 1.3995
Training Epoch: 19 [11904/50048]	Loss: 1.4509
Training Epoch: 19 [12032/50048]	Loss: 1.3460
Training Epoch: 19 [12160/50048]	Loss: 1.3368
Training Epoch: 19 [12288/50048]	Loss: 1.3679
Training Epoch: 19 [12416/50048]	Loss: 1.2588
Training Epoch: 19 [12544/50048]	Loss: 1.3877
Training Epoch: 19 [12672/50048]	Loss: 1.4197
Training Epoch: 19 [12800/50048]	Loss: 1.6272
Training Epoch: 19 [12928/50048]	Loss: 1.5946
Training Epoch: 19 [13056/50048]	Loss: 1.4851
Training Epoch: 19 [13184/50048]	Loss: 1.1565
Training Epoch: 19 [13312/50048]	Loss: 1.4579
Training Epoch: 19 [13440/50048]	Loss: 1.3841
Training Epoch: 19 [13568/50048]	Loss: 1.3969
Training Epoch: 19 [13696/50048]	Loss: 1.4334
Training Epoch: 19 [13824/50048]	Loss: 1.6913
Training Epoch: 19 [13952/50048]	Loss: 1.1780
Training Epoch: 19 [14080/50048]	Loss: 1.4782
Training Epoch: 19 [14208/50048]	Loss: 1.3518
Training Epoch: 19 [14336/50048]	Loss: 1.3886
Training Epoch: 19 [14464/50048]	Loss: 1.4368
Training Epoch: 19 [14592/50048]	Loss: 1.3974
Training Epoch: 19 [14720/50048]	Loss: 1.3349
Training Epoch: 19 [14848/50048]	Loss: 1.1378
Training Epoch: 19 [14976/50048]	Loss: 1.4585
Training Epoch: 19 [15104/50048]	Loss: 1.3621
Training Epoch: 19 [15232/50048]	Loss: 1.2728
Training Epoch: 19 [15360/50048]	Loss: 1.5188
Training Epoch: 19 [15488/50048]	Loss: 1.3950
Training Epoch: 19 [15616/50048]	Loss: 1.3939
Training Epoch: 19 [15744/50048]	Loss: 1.1273
Training Epoch: 19 [15872/50048]	Loss: 1.2442
Training Epoch: 19 [16000/50048]	Loss: 1.1939
Training Epoch: 19 [16128/50048]	Loss: 1.4194
Training Epoch: 19 [16256/50048]	Loss: 1.3587
Training Epoch: 19 [16384/50048]	Loss: 1.2579
Training Epoch: 19 [16512/50048]	Loss: 1.4525
Training Epoch: 19 [16640/50048]	Loss: 1.1210
Training Epoch: 19 [16768/50048]	Loss: 1.2839
Training Epoch: 19 [16896/50048]	Loss: 1.4228
Training Epoch: 19 [17024/50048]	Loss: 1.1905
Training Epoch: 19 [17152/50048]	Loss: 1.6770
Training Epoch: 19 [17280/50048]	Loss: 1.5373
Training Epoch: 19 [17408/50048]	Loss: 1.3422
Training Epoch: 19 [17536/50048]	Loss: 1.5485
Training Epoch: 19 [17664/50048]	Loss: 1.2756
Training Epoch: 19 [17792/50048]	Loss: 1.3058
Training Epoch: 19 [17920/50048]	Loss: 1.4937
Training Epoch: 19 [18048/50048]	Loss: 1.3785
Training Epoch: 19 [18176/50048]	Loss: 1.3477
Training Epoch: 19 [18304/50048]	Loss: 1.2114
Training Epoch: 19 [18432/50048]	Loss: 1.6606
Training Epoch: 19 [18560/50048]	Loss: 1.5715
Training Epoch: 19 [18688/50048]	Loss: 1.3072
Training Epoch: 19 [18816/50048]	Loss: 1.5720
Training Epoch: 19 [18944/50048]	Loss: 1.2395
Training Epoch: 19 [19072/50048]	Loss: 1.2727
Training Epoch: 19 [19200/50048]	Loss: 1.7403
Training Epoch: 19 [19328/50048]	Loss: 1.4434
Training Epoch: 19 [19456/50048]	Loss: 1.1567
Training Epoch: 19 [19584/50048]	Loss: 1.4721
Training Epoch: 19 [19712/50048]	Loss: 1.3881
Training Epoch: 19 [19840/50048]	Loss: 1.2594
Training Epoch: 19 [19968/50048]	Loss: 1.3286
Training Epoch: 19 [20096/50048]	Loss: 1.3145
Training Epoch: 19 [20224/50048]	Loss: 1.3308
Training Epoch: 19 [20352/50048]	Loss: 1.6460
Training Epoch: 19 [20480/50048]	Loss: 1.4047
Training Epoch: 19 [20608/50048]	Loss: 1.2974
Training Epoch: 19 [20736/50048]	Loss: 1.2790
Training Epoch: 19 [20864/50048]	Loss: 1.4770
Training Epoch: 19 [20992/50048]	Loss: 1.2652
Training Epoch: 19 [21120/50048]	Loss: 1.3954
Training Epoch: 19 [21248/50048]	Loss: 1.3245
Training Epoch: 19 [21376/50048]	Loss: 1.4778
Training Epoch: 19 [21504/50048]	Loss: 1.4002
Training Epoch: 19 [21632/50048]	Loss: 1.3928
Training Epoch: 19 [21760/50048]	Loss: 1.1900
Training Epoch: 19 [21888/50048]	Loss: 1.5946
Training Epoch: 19 [22016/50048]	Loss: 1.4110
Training Epoch: 19 [22144/50048]	Loss: 1.4465
Training Epoch: 19 [22272/50048]	Loss: 1.4478
Training Epoch: 19 [22400/50048]	Loss: 1.5643
Training Epoch: 19 [22528/50048]	Loss: 1.5283
Training Epoch: 19 [22656/50048]	Loss: 1.4214
Training Epoch: 19 [22784/50048]	Loss: 1.3930
Training Epoch: 19 [22912/50048]	Loss: 1.3158
Training Epoch: 19 [23040/50048]	Loss: 1.3371
Training Epoch: 19 [23168/50048]	Loss: 1.4146
Training Epoch: 19 [23296/50048]	Loss: 1.2795
Training Epoch: 19 [23424/50048]	Loss: 1.4253
Training Epoch: 19 [23552/50048]	Loss: 1.1312
Training Epoch: 19 [23680/50048]	Loss: 1.5929
Training Epoch: 19 [23808/50048]	Loss: 1.3409
Training Epoch: 19 [23936/50048]	Loss: 1.2708
Training Epoch: 19 [24064/50048]	Loss: 1.3423
Training Epoch: 19 [24192/50048]	Loss: 1.3169
Training Epoch: 19 [24320/50048]	Loss: 1.3634
Training Epoch: 19 [24448/50048]	Loss: 1.3520
Training Epoch: 19 [24576/50048]	Loss: 1.4306
Training Epoch: 19 [24704/50048]	Loss: 1.4819
Training Epoch: 19 [24832/50048]	Loss: 1.4468
Training Epoch: 19 [24960/50048]	Loss: 1.3747
Training Epoch: 19 [25088/50048]	Loss: 1.7285
Training Epoch: 19 [25216/50048]	Loss: 1.3046
Training Epoch: 19 [25344/50048]	Loss: 1.3429
Training Epoch: 19 [25472/50048]	Loss: 1.2829
Training Epoch: 19 [25600/50048]	Loss: 1.4128
Training Epoch: 19 [25728/50048]	Loss: 1.5226
Training Epoch: 19 [25856/50048]	Loss: 1.5345
Training Epoch: 19 [25984/50048]	Loss: 1.4553
Training Epoch: 19 [26112/50048]	Loss: 1.4757
Training Epoch: 19 [26240/50048]	Loss: 1.3742
Training Epoch: 19 [26368/50048]	Loss: 1.4480
Training Epoch: 19 [26496/50048]	Loss: 1.4628
Training Epoch: 19 [26624/50048]	Loss: 1.3470
Training Epoch: 19 [26752/50048]	Loss: 1.4262
Training Epoch: 19 [26880/50048]	Loss: 1.2749
Training Epoch: 19 [27008/50048]	Loss: 1.2475
Training Epoch: 19 [27136/50048]	Loss: 1.4143
Training Epoch: 19 [27264/50048]	Loss: 1.1540
Training Epoch: 19 [27392/50048]	Loss: 1.5328
Training Epoch: 19 [27520/50048]	Loss: 1.4160
Training Epoch: 19 [27648/50048]	Loss: 1.2287
Training Epoch: 19 [27776/50048]	Loss: 1.5919
Training Epoch: 19 [27904/50048]	Loss: 1.5095
Training Epoch: 19 [28032/50048]	Loss: 1.3876
Training Epoch: 19 [28160/50048]	Loss: 1.3236
Training Epoch: 19 [28288/50048]	Loss: 1.3508
Training Epoch: 19 [28416/50048]	Loss: 1.6168
Training Epoch: 19 [28544/50048]	Loss: 1.1259
Training Epoch: 19 [28672/50048]	Loss: 1.7049
Training Epoch: 19 [28800/50048]	Loss: 1.3254
Training Epoch: 19 [28928/50048]	Loss: 1.5066
Training Epoch: 19 [29056/50048]	Loss: 1.3361
Training Epoch: 19 [29184/50048]	Loss: 1.4162
Training Epoch: 19 [29312/50048]	Loss: 1.4347
Training Epoch: 19 [29440/50048]	Loss: 1.3515
Training Epoch: 19 [29568/50048]	Loss: 1.1817
Training Epoch: 19 [29696/50048]	Loss: 1.1508
Training Epoch: 19 [29824/50048]	Loss: 1.5443
Training Epoch: 19 [29952/50048]	Loss: 1.4612
Training Epoch: 19 [30080/50048]	Loss: 1.5866
Training Epoch: 19 [30208/50048]	Loss: 1.1694
Training Epoch: 19 [30336/50048]	Loss: 1.2921
Training Epoch: 19 [30464/50048]	Loss: 1.4779
Training Epoch: 19 [30592/50048]	Loss: 1.1805
Training Epoch: 19 [30720/50048]	Loss: 1.4124
Training Epoch: 19 [30848/50048]	Loss: 1.5754
Training Epoch: 19 [30976/50048]	Loss: 1.2956
Training Epoch: 19 [31104/50048]	Loss: 1.4392
Training Epoch: 19 [31232/50048]	Loss: 1.3917
Training Epoch: 19 [31360/50048]	Loss: 1.2707
Training Epoch: 19 [31488/50048]	Loss: 1.2962
Training Epoch: 19 [31616/50048]	Loss: 1.2457
Training Epoch: 19 [31744/50048]	Loss: 1.4928
Training Epoch: 19 [31872/50048]	Loss: 1.6158
Training Epoch: 19 [32000/50048]	Loss: 1.2788
Training Epoch: 19 [32128/50048]	Loss: 1.3308
Training Epoch: 19 [32256/50048]	Loss: 1.2964
Training Epoch: 19 [32384/50048]	Loss: 1.6284
Training Epoch: 19 [32512/50048]	Loss: 1.7601
Training Epoch: 19 [32640/50048]	Loss: 1.2321
Training Epoch: 19 [32768/50048]	Loss: 1.3182
Training Epoch: 19 [32896/50048]	Loss: 1.4369
Training Epoch: 19 [33024/50048]	Loss: 1.6831
Training Epoch: 19 [33152/50048]	Loss: 1.5125
Training Epoch: 19 [33280/50048]	Loss: 1.4345
Training Epoch: 19 [33408/50048]	Loss: 1.3669
Training Epoch: 19 [33536/50048]	Loss: 1.3685
Training Epoch: 19 [33664/50048]	Loss: 1.2688
Training Epoch: 19 [33792/50048]	Loss: 1.4037
Training Epoch: 19 [33920/50048]	Loss: 1.3082
Training Epoch: 19 [34048/50048]	Loss: 1.5047
Training Epoch: 19 [34176/50048]	Loss: 1.7947
Training Epoch: 19 [34304/50048]	Loss: 1.6762
Training Epoch: 19 [34432/50048]	Loss: 1.4705
Training Epoch: 19 [34560/50048]	Loss: 1.3356
Training Epoch: 19 [34688/50048]	Loss: 1.3313
Training Epoch: 19 [34816/50048]	Loss: 1.2846
Training Epoch: 19 [34944/50048]	Loss: 1.2631
Training Epoch: 19 [35072/50048]	Loss: 1.4553
Training Epoch: 19 [35200/50048]	Loss: 1.5801
Training Epoch: 19 [35328/50048]	Loss: 1.3205
Training Epoch: 19 [35456/50048]	Loss: 1.6419
Training Epoch: 19 [35584/50048]	Loss: 1.4308
Training Epoch: 19 [35712/50048]	Loss: 1.4532
Training Epoch: 19 [35840/50048]	Loss: 1.4623
Training Epoch: 19 [35968/50048]	Loss: 1.5960
Training Epoch: 19 [36096/50048]	Loss: 1.5832
Training Epoch: 19 [36224/50048]	Loss: 1.5104
Training Epoch: 19 [36352/50048]	Loss: 1.3031
Training Epoch: 19 [36480/50048]	Loss: 1.2165
Training Epoch: 19 [36608/50048]	Loss: 1.4472
Training Epoch: 19 [36736/50048]	Loss: 1.1088
Training Epoch: 19 [36864/50048]	Loss: 1.3917
Training Epoch: 19 [36992/50048]	Loss: 1.3129
Training Epoch: 19 [37120/50048]	Loss: 1.3762
Training Epoch: 19 [37248/50048]	Loss: 1.2206
Training Epoch: 19 [37376/50048]	Loss: 1.2388
Training Epoch: 19 [37504/50048]	Loss: 1.4745
Training Epoch: 19 [37632/50048]	Loss: 1.1636
Training Epoch: 19 [37760/50048]	Loss: 1.6219
Training Epoch: 19 [37888/50048]	Loss: 1.2948
Training Epoch: 19 [38016/50048]	Loss: 1.3420
Training Epoch: 19 [38144/50048]	Loss: 1.4509
Training Epoch: 19 [38272/50048]	Loss: 1.2147
Training Epoch: 19 [38400/50048]	Loss: 1.4197
Training Epoch: 19 [38528/50048]	Loss: 1.5380
Training Epoch: 19 [38656/50048]	Loss: 1.6520
Training Epoch: 19 [38784/50048]	Loss: 1.3369
Training Epoch: 19 [38912/50048]	Loss: 1.4616
Training Epoch: 19 [39040/50048]	Loss: 1.4375
Training Epoch: 19 [39168/50048]	Loss: 1.4408
Training Epoch: 19 [39296/50048]	Loss: 1.4022
Training Epoch: 19 [39424/50048]	Loss: 1.3655
Training Epoch: 19 [39552/50048]	Loss: 1.2973
Training Epoch: 19 [39680/50048]	Loss: 1.3391
Training Epoch: 19 [39808/50048]	Loss: 1.5227
Training Epoch: 19 [39936/50048]	Loss: 1.4759
Training Epoch: 19 [40064/50048]	Loss: 1.2489
Training Epoch: 19 [40192/50048]	Loss: 1.3527
Training Epoch: 19 [40320/50048]	Loss: 1.2505
Training Epoch: 19 [40448/50048]	Loss: 1.4796
Training Epoch: 19 [40576/50048]	Loss: 1.3347
Training Epoch: 19 [40704/50048]	Loss: 1.5247
Training Epoch: 19 [40832/50048]	Loss: 1.3793
Training Epoch: 19 [40960/50048]	Loss: 1.3483
Training Epoch: 19 [41088/50048]	Loss: 1.3828
Training Epoch: 19 [41216/50048]	Loss: 1.2766
Training Epoch: 19 [41344/50048]	Loss: 1.2355
Training Epoch: 19 [41472/50048]	Loss: 1.1325
Training Epoch: 19 [41600/50048]	Loss: 1.0866
Training Epoch: 19 [41728/50048]	Loss: 1.5211
Training Epoch: 19 [41856/50048]	Loss: 1.5046
Training Epoch: 19 [41984/50048]	Loss: 1.6401
Training Epoch: 19 [42112/50048]	Loss: 1.7314
Training Epoch: 19 [42240/50048]	Loss: 1.1516
Training Epoch: 19 [42368/50048]	Loss: 1.2483
Training Epoch: 19 [42496/50048]	Loss: 1.5944
Training Epoch: 19 [42624/50048]	Loss: 1.4110
Training Epoch: 19 [42752/50048]	Loss: 1.1871
Training Epoch: 19 [42880/50048]	Loss: 1.4263
Training Epoch: 19 [43008/50048]	Loss: 1.4510
Training Epoch: 19 [43136/50048]	Loss: 1.3105
Training Epoch: 19 [43264/50048]	Loss: 1.5777
Training Epoch: 19 [43392/50048]	Loss: 1.4266
Training Epoch: 19 [43520/50048]	Loss: 1.4215
Training Epoch: 19 [43648/50048]	Loss: 1.3273
Training Epoch: 19 [43776/50048]	Loss: 1.5596
Training Epoch: 19 [43904/50048]	Loss: 1.4471
Training Epoch: 19 [44032/50048]	Loss: 1.3307
Training Epoch: 19 [44160/50048]	Loss: 1.4040
Training Epoch: 19 [44288/50048]	Loss: 1.4788
Training Epoch: 19 [44416/50048]	Loss: 1.2524
Training Epoch: 19 [44544/50048]	Loss: 1.3411
Training Epoch: 19 [44672/50048]	Loss: 1.4802
Training Epoch: 19 [44800/50048]	Loss: 1.2646
Training Epoch: 19 [44928/50048]	Loss: 1.2983
Training Epoch: 19 [45056/50048]	Loss: 1.5163
Training Epoch: 19 [45184/50048]	Loss: 1.5756
Training Epoch: 19 [45312/50048]	Loss: 1.1561
Training Epoch: 19 [45440/50048]	Loss: 1.2517
Training Epoch: 19 [45568/50048]	Loss: 1.3164
Training Epoch: 19 [45696/50048]	Loss: 1.4329
Training Epoch: 19 [45824/50048]	Loss: 1.4975
Training Epoch: 19 [45952/50048]	Loss: 1.6192
Training Epoch: 19 [46080/50048]	Loss: 1.5818
Training Epoch: 19 [46208/50048]	Loss: 1.4230
Training Epoch: 19 [46336/50048]	Loss: 1.2417
Training Epoch: 19 [46464/50048]	Loss: 1.3378
Training Epoch: 19 [46592/50048]	Loss: 1.3493
Training Epoch: 19 [46720/50048]	Loss: 1.3433
Training Epoch: 19 [46848/50048]	Loss: 1.5062
Training Epoch: 19 [46976/50048]	Loss: 1.4831
Training Epoch: 19 [47104/50048]	Loss: 1.5523
Training Epoch: 19 [47232/50048]	Loss: 1.3086
Training Epoch: 19 [47360/50048]	Loss: 1.5006
Training Epoch: 19 [47488/50048]	Loss: 1.5032
Training Epoch: 19 [47616/50048]	Loss: 1.2944
Training Epoch: 19 [47744/50048]	Loss: 1.5601
Training Epoch: 19 [47872/50048]	Loss: 1.3058
Training Epoch: 19 [48000/50048]	Loss: 1.2338
Training Epoch: 19 [48128/50048]	Loss: 1.3741
Training Epoch: 19 [48256/50048]	Loss: 1.2481
Training Epoch: 19 [48384/50048]	Loss: 1.4760
Training Epoch: 19 [48512/50048]	Loss: 1.5211
Training Epoch: 19 [48640/50048]	Loss: 1.4940
Training Epoch: 19 [48768/50048]	Loss: 1.2507
Training Epoch: 19 [48896/50048]	Loss: 1.6829
Training Epoch: 19 [49024/50048]	Loss: 1.5361
Training Epoch: 19 [49152/50048]	Loss: 1.3766
Training Epoch: 19 [49280/50048]	Loss: 1.4031
Training Epoch: 19 [49408/50048]	Loss: 1.2861
Training Epoch: 19 [49536/50048]	Loss: 1.3050
Training Epoch: 19 [49664/50048]	Loss: 1.0639
Training Epoch: 19 [49792/50048]	Loss: 1.2777
Training Epoch: 19 [49920/50048]	Loss: 1.3327
Training Epoch: 19 [50048/50048]	Loss: 1.0535
Validation Epoch: 19, Average loss: 0.0126, Accuracy: 0.5684
Training Epoch: 20 [128/50048]	Loss: 1.2049
Training Epoch: 20 [256/50048]	Loss: 1.2481
Training Epoch: 20 [384/50048]	Loss: 1.0931
Training Epoch: 20 [512/50048]	Loss: 1.2540
Training Epoch: 20 [640/50048]	Loss: 1.5980
Training Epoch: 20 [768/50048]	Loss: 1.2339
Training Epoch: 20 [896/50048]	Loss: 1.2137
Training Epoch: 20 [1024/50048]	Loss: 1.1880
Training Epoch: 20 [1152/50048]	Loss: 1.6672
Training Epoch: 20 [1280/50048]	Loss: 1.1636
Training Epoch: 20 [1408/50048]	Loss: 1.2443
Training Epoch: 20 [1536/50048]	Loss: 1.3849
Training Epoch: 20 [1664/50048]	Loss: 1.4171
Training Epoch: 20 [1792/50048]	Loss: 1.3274
Training Epoch: 20 [1920/50048]	Loss: 1.4193
Training Epoch: 20 [2048/50048]	Loss: 1.3248
Training Epoch: 20 [2176/50048]	Loss: 1.1035
Training Epoch: 20 [2304/50048]	Loss: 1.1955
Training Epoch: 20 [2432/50048]	Loss: 1.5663
Training Epoch: 20 [2560/50048]	Loss: 1.3512
Training Epoch: 20 [2688/50048]	Loss: 1.4027
Training Epoch: 20 [2816/50048]	Loss: 1.1193
Training Epoch: 20 [2944/50048]	Loss: 1.2773
Training Epoch: 20 [3072/50048]	Loss: 1.3205
Training Epoch: 20 [3200/50048]	Loss: 1.5697
Training Epoch: 20 [3328/50048]	Loss: 1.2104
Training Epoch: 20 [3456/50048]	Loss: 1.0710
Training Epoch: 20 [3584/50048]	Loss: 1.5037
Training Epoch: 20 [3712/50048]	Loss: 1.2907
Training Epoch: 20 [3840/50048]	Loss: 1.3038
Training Epoch: 20 [3968/50048]	Loss: 1.5451
Training Epoch: 20 [4096/50048]	Loss: 1.2712
Training Epoch: 20 [4224/50048]	Loss: 1.2338
Training Epoch: 20 [4352/50048]	Loss: 1.1643
Training Epoch: 20 [4480/50048]	Loss: 1.3796
Training Epoch: 20 [4608/50048]	Loss: 1.3342
Training Epoch: 20 [4736/50048]	Loss: 1.6457
Training Epoch: 20 [4864/50048]	Loss: 1.2608
Training Epoch: 20 [4992/50048]	Loss: 1.3953
Training Epoch: 20 [5120/50048]	Loss: 1.5674
Training Epoch: 20 [5248/50048]	Loss: 1.2962
Training Epoch: 20 [5376/50048]	Loss: 1.3141
Training Epoch: 20 [5504/50048]	Loss: 1.1824
Training Epoch: 20 [5632/50048]	Loss: 1.2727
Training Epoch: 20 [5760/50048]	Loss: 1.4530
Training Epoch: 20 [5888/50048]	Loss: 1.6973
Training Epoch: 20 [6016/50048]	Loss: 1.4401
Training Epoch: 20 [6144/50048]	Loss: 1.3113
Training Epoch: 20 [6272/50048]	Loss: 1.3768
Training Epoch: 20 [6400/50048]	Loss: 1.2265
Training Epoch: 20 [6528/50048]	Loss: 1.1668
Training Epoch: 20 [6656/50048]	Loss: 1.3303
Training Epoch: 20 [6784/50048]	Loss: 1.3987
Training Epoch: 20 [6912/50048]	Loss: 1.1935
Training Epoch: 20 [7040/50048]	Loss: 1.3070
Training Epoch: 20 [7168/50048]	Loss: 1.5777
Training Epoch: 20 [7296/50048]	Loss: 1.4197
Training Epoch: 20 [7424/50048]	Loss: 1.4387
Training Epoch: 20 [7552/50048]	Loss: 1.1713
Training Epoch: 20 [7680/50048]	Loss: 1.2903
Training Epoch: 20 [7808/50048]	Loss: 1.1460
Training Epoch: 20 [7936/50048]	Loss: 1.1352
Training Epoch: 20 [8064/50048]	Loss: 1.2937
Training Epoch: 20 [8192/50048]	Loss: 1.4906
Training Epoch: 20 [8320/50048]	Loss: 1.3942
Training Epoch: 20 [8448/50048]	Loss: 1.3006
Training Epoch: 20 [8576/50048]	Loss: 1.3725
Training Epoch: 20 [8704/50048]	Loss: 1.0072
Training Epoch: 20 [8832/50048]	Loss: 1.4485
Training Epoch: 20 [8960/50048]	Loss: 1.4374
Training Epoch: 20 [9088/50048]	Loss: 1.5519
Training Epoch: 20 [9216/50048]	Loss: 1.3092
Training Epoch: 20 [9344/50048]	Loss: 1.1686
Training Epoch: 20 [9472/50048]	Loss: 1.3002
Training Epoch: 20 [9600/50048]	Loss: 1.1769
Training Epoch: 20 [9728/50048]	Loss: 1.3534
Training Epoch: 20 [9856/50048]	Loss: 1.5139
Training Epoch: 20 [9984/50048]	Loss: 1.3768
Training Epoch: 20 [10112/50048]	Loss: 1.2433
Training Epoch: 20 [10240/50048]	Loss: 1.1665
Training Epoch: 20 [10368/50048]	Loss: 1.2078
Training Epoch: 20 [10496/50048]	Loss: 1.2829
Training Epoch: 20 [10624/50048]	Loss: 1.1512
Training Epoch: 20 [10752/50048]	Loss: 1.3831
Training Epoch: 20 [10880/50048]	Loss: 1.5054
Training Epoch: 20 [11008/50048]	Loss: 1.0793
Training Epoch: 20 [11136/50048]	Loss: 1.2631
Training Epoch: 20 [11264/50048]	Loss: 1.3503
Training Epoch: 20 [11392/50048]	Loss: 1.3843
Training Epoch: 20 [11520/50048]	Loss: 1.2299
Training Epoch: 20 [11648/50048]	Loss: 1.2805
Training Epoch: 20 [11776/50048]	Loss: 1.4298
Training Epoch: 20 [11904/50048]	Loss: 1.2878
Training Epoch: 20 [12032/50048]	Loss: 1.3057
Training Epoch: 20 [12160/50048]	Loss: 1.3053
Training Epoch: 20 [12288/50048]	Loss: 1.1869
Training Epoch: 20 [12416/50048]	Loss: 1.3799
Training Epoch: 20 [12544/50048]	Loss: 1.1705
Training Epoch: 20 [12672/50048]	Loss: 1.4453
Training Epoch: 20 [12800/50048]	Loss: 1.5357
Training Epoch: 20 [12928/50048]	Loss: 1.3246
Training Epoch: 20 [13056/50048]	Loss: 1.3574
Training Epoch: 20 [13184/50048]	Loss: 1.1994
Training Epoch: 20 [13312/50048]	Loss: 1.5035
Training Epoch: 20 [13440/50048]	Loss: 1.1053
Training Epoch: 20 [13568/50048]	Loss: 1.5901
Training Epoch: 20 [13696/50048]	Loss: 1.3636
Training Epoch: 20 [13824/50048]	Loss: 1.3089
Training Epoch: 20 [13952/50048]	Loss: 1.3554
Training Epoch: 20 [14080/50048]	Loss: 1.4191
Training Epoch: 20 [14208/50048]	Loss: 1.2356
Training Epoch: 20 [14336/50048]	Loss: 1.2493
Training Epoch: 20 [14464/50048]	Loss: 1.1486
Training Epoch: 20 [14592/50048]	Loss: 1.5293
Training Epoch: 20 [14720/50048]	Loss: 1.4132
Training Epoch: 20 [14848/50048]	Loss: 1.2935
Training Epoch: 20 [14976/50048]	Loss: 1.2731
Training Epoch: 20 [15104/50048]	Loss: 1.4249
Training Epoch: 20 [15232/50048]	Loss: 1.4310
Training Epoch: 20 [15360/50048]	Loss: 1.2497
Training Epoch: 20 [15488/50048]	Loss: 1.4688
Training Epoch: 20 [15616/50048]	Loss: 1.1110
Training Epoch: 20 [15744/50048]	Loss: 1.1486
Training Epoch: 20 [15872/50048]	Loss: 1.2977
Training Epoch: 20 [16000/50048]	Loss: 1.3881
Training Epoch: 20 [16128/50048]	Loss: 1.2803
Training Epoch: 20 [16256/50048]	Loss: 1.3318
Training Epoch: 20 [16384/50048]	Loss: 1.4208
Training Epoch: 20 [16512/50048]	Loss: 1.4184
Training Epoch: 20 [16640/50048]	Loss: 1.6634
Training Epoch: 20 [16768/50048]	Loss: 1.4597
Training Epoch: 20 [16896/50048]	Loss: 1.1108
Training Epoch: 20 [17024/50048]	Loss: 1.2610
Training Epoch: 20 [17152/50048]	Loss: 1.2857
Training Epoch: 20 [17280/50048]	Loss: 1.5609
Training Epoch: 20 [17408/50048]	Loss: 1.3338
Training Epoch: 20 [17536/50048]	Loss: 1.1521
Training Epoch: 20 [17664/50048]	Loss: 1.5683
Training Epoch: 20 [17792/50048]	Loss: 1.3719
Training Epoch: 20 [17920/50048]	Loss: 1.3684
Training Epoch: 20 [18048/50048]	Loss: 1.4957
Training Epoch: 20 [18176/50048]	Loss: 1.1611
Training Epoch: 20 [18304/50048]	Loss: 1.2224
Training Epoch: 20 [18432/50048]	Loss: 1.4488
Training Epoch: 20 [18560/50048]	Loss: 1.2941
Training Epoch: 20 [18688/50048]	Loss: 1.5063
Training Epoch: 20 [18816/50048]	Loss: 1.3044
Training Epoch: 20 [18944/50048]	Loss: 1.2991
Training Epoch: 20 [19072/50048]	Loss: 1.1846
Training Epoch: 20 [19200/50048]	Loss: 1.3737
Training Epoch: 20 [19328/50048]	Loss: 1.2402
Training Epoch: 20 [19456/50048]	Loss: 1.3446
Training Epoch: 20 [19584/50048]	Loss: 1.1656
Training Epoch: 20 [19712/50048]	Loss: 1.2904
Training Epoch: 20 [19840/50048]	Loss: 1.7605
Training Epoch: 20 [19968/50048]	Loss: 1.4181
Training Epoch: 20 [20096/50048]	Loss: 1.2587
Training Epoch: 20 [20224/50048]	Loss: 1.2012
Training Epoch: 20 [20352/50048]	Loss: 1.4608
Training Epoch: 20 [20480/50048]	Loss: 1.4394
Training Epoch: 20 [20608/50048]	Loss: 1.2820
Training Epoch: 20 [20736/50048]	Loss: 1.3460
Training Epoch: 20 [20864/50048]	Loss: 1.5019
Training Epoch: 20 [20992/50048]	Loss: 1.3261
Training Epoch: 20 [21120/50048]	Loss: 1.1818
Training Epoch: 20 [21248/50048]	Loss: 1.5163
Training Epoch: 20 [21376/50048]	Loss: 1.2287
Training Epoch: 20 [21504/50048]	Loss: 1.1937
Training Epoch: 20 [21632/50048]	Loss: 1.2847
Training Epoch: 20 [21760/50048]	Loss: 1.3371
Training Epoch: 20 [21888/50048]	Loss: 1.3993
Training Epoch: 20 [22016/50048]	Loss: 1.2055
Training Epoch: 20 [22144/50048]	Loss: 1.2847
Training Epoch: 20 [22272/50048]	Loss: 1.3256
Training Epoch: 20 [22400/50048]	Loss: 1.3664
Training Epoch: 20 [22528/50048]	Loss: 1.5293
Training Epoch: 20 [22656/50048]	Loss: 1.5921
Training Epoch: 20 [22784/50048]	Loss: 1.3830
Training Epoch: 20 [22912/50048]	Loss: 1.5868
Training Epoch: 20 [23040/50048]	Loss: 1.0940
Training Epoch: 20 [23168/50048]	Loss: 1.1571
Training Epoch: 20 [23296/50048]	Loss: 1.4859
Training Epoch: 20 [23424/50048]	Loss: 1.0716
Training Epoch: 20 [23552/50048]	Loss: 1.3634
Training Epoch: 20 [23680/50048]	Loss: 1.2742
Training Epoch: 20 [23808/50048]	Loss: 1.4604
Training Epoch: 20 [23936/50048]	Loss: 1.4838
Training Epoch: 20 [24064/50048]	Loss: 1.4626
Training Epoch: 20 [24192/50048]	Loss: 1.2588
Training Epoch: 20 [24320/50048]	Loss: 1.2440
Training Epoch: 20 [24448/50048]	Loss: 1.2643
Training Epoch: 20 [24576/50048]	Loss: 1.5749
Training Epoch: 20 [24704/50048]	Loss: 1.1585
Training Epoch: 20 [24832/50048]	Loss: 1.2153
Training Epoch: 20 [24960/50048]	Loss: 1.1760
Training Epoch: 20 [25088/50048]	Loss: 1.1817
Training Epoch: 20 [25216/50048]	Loss: 1.0069
Training Epoch: 20 [25344/50048]	Loss: 1.3437
Training Epoch: 20 [25472/50048]	Loss: 1.4426
Training Epoch: 20 [25600/50048]	Loss: 1.2709
Training Epoch: 20 [25728/50048]	Loss: 1.3400
Training Epoch: 20 [25856/50048]	Loss: 1.7044
Training Epoch: 20 [25984/50048]	Loss: 1.3792
Training Epoch: 20 [26112/50048]	Loss: 1.5439
Training Epoch: 20 [26240/50048]	Loss: 1.1946
Training Epoch: 20 [26368/50048]	Loss: 1.6300
Training Epoch: 20 [26496/50048]	Loss: 1.2969
Training Epoch: 20 [26624/50048]	Loss: 1.3912
Training Epoch: 20 [26752/50048]	Loss: 1.1667
Training Epoch: 20 [26880/50048]	Loss: 1.3754
Training Epoch: 20 [27008/50048]	Loss: 1.2136
Training Epoch: 20 [27136/50048]	Loss: 1.2940
Training Epoch: 20 [27264/50048]	Loss: 1.3357
Training Epoch: 20 [27392/50048]	Loss: 1.1447
Training Epoch: 20 [27520/50048]	Loss: 1.2897
Training Epoch: 20 [27648/50048]	Loss: 1.3711
Training Epoch: 20 [27776/50048]	Loss: 1.6455
Training Epoch: 20 [27904/50048]	Loss: 1.4598
Training Epoch: 20 [28032/50048]	Loss: 1.5808
Training Epoch: 20 [28160/50048]	Loss: 1.2890
Training Epoch: 20 [28288/50048]	Loss: 1.1381
Training Epoch: 20 [28416/50048]	Loss: 1.3808
Training Epoch: 20 [28544/50048]	Loss: 1.2067
Training Epoch: 20 [28672/50048]	Loss: 1.2280
Training Epoch: 20 [28800/50048]	Loss: 1.3165
Training Epoch: 20 [28928/50048]	Loss: 1.1573
Training Epoch: 20 [29056/50048]	Loss: 1.3306
Training Epoch: 20 [29184/50048]	Loss: 1.1730
Training Epoch: 20 [29312/50048]	Loss: 1.5399
Training Epoch: 20 [29440/50048]	Loss: 1.4684
Training Epoch: 20 [29568/50048]	Loss: 1.3527
Training Epoch: 20 [29696/50048]	Loss: 1.3525
Training Epoch: 20 [29824/50048]	Loss: 1.3941
Training Epoch: 20 [29952/50048]	Loss: 1.5568
Training Epoch: 20 [30080/50048]	Loss: 1.4233
Training Epoch: 20 [30208/50048]	Loss: 1.4220
Training Epoch: 20 [30336/50048]	Loss: 1.3349
Training Epoch: 20 [30464/50048]	Loss: 1.4417
Training Epoch: 20 [30592/50048]	Loss: 1.5551
Training Epoch: 20 [30720/50048]	Loss: 1.3347
Training Epoch: 20 [30848/50048]	Loss: 1.4828
Training Epoch: 20 [30976/50048]	Loss: 1.5168
Training Epoch: 20 [31104/50048]	Loss: 1.4230
Training Epoch: 20 [31232/50048]	Loss: 1.5981
Training Epoch: 20 [31360/50048]	Loss: 1.4051
Training Epoch: 20 [31488/50048]	Loss: 1.6531
Training Epoch: 20 [31616/50048]	Loss: 1.2255
Training Epoch: 20 [31744/50048]	Loss: 1.3355
Training Epoch: 20 [31872/50048]	Loss: 1.3954
Training Epoch: 20 [32000/50048]	Loss: 1.4022
Training Epoch: 20 [32128/50048]	Loss: 1.4844
Training Epoch: 20 [32256/50048]	Loss: 1.5446
Training Epoch: 20 [32384/50048]	Loss: 1.4788
Training Epoch: 20 [32512/50048]	Loss: 1.2344
Training Epoch: 20 [32640/50048]	Loss: 1.1762
Training Epoch: 20 [32768/50048]	Loss: 1.3618
Training Epoch: 20 [32896/50048]	Loss: 1.5494
Training Epoch: 20 [33024/50048]	Loss: 1.1558
Training Epoch: 20 [33152/50048]	Loss: 1.4325
Training Epoch: 20 [33280/50048]	Loss: 1.3152
Training Epoch: 20 [33408/50048]	Loss: 1.5846
Training Epoch: 20 [33536/50048]	Loss: 1.5180
Training Epoch: 20 [33664/50048]	Loss: 1.5152
Training Epoch: 20 [33792/50048]	Loss: 1.4615
Training Epoch: 20 [33920/50048]	Loss: 1.2216
Training Epoch: 20 [34048/50048]	Loss: 1.2994
Training Epoch: 20 [34176/50048]	Loss: 1.3640
Training Epoch: 20 [34304/50048]	Loss: 1.5947
Training Epoch: 20 [34432/50048]	Loss: 1.6556
Training Epoch: 20 [34560/50048]	Loss: 1.4096
Training Epoch: 20 [34688/50048]	Loss: 1.2813
Training Epoch: 20 [34816/50048]	Loss: 1.2240
Training Epoch: 20 [34944/50048]	Loss: 1.5046
Training Epoch: 20 [35072/50048]	Loss: 1.2938
Training Epoch: 20 [35200/50048]	Loss: 1.6155
Training Epoch: 20 [35328/50048]	Loss: 1.3481
Training Epoch: 20 [35456/50048]	Loss: 1.1491
Training Epoch: 20 [35584/50048]	Loss: 1.3733
Training Epoch: 20 [35712/50048]	Loss: 1.5506
Training Epoch: 20 [35840/50048]	Loss: 1.4104
Training Epoch: 20 [35968/50048]	Loss: 1.4652
Training Epoch: 20 [36096/50048]	Loss: 1.2835
Training Epoch: 20 [36224/50048]	Loss: 1.2914
Training Epoch: 20 [36352/50048]	Loss: 1.2058
Training Epoch: 20 [36480/50048]	Loss: 1.3134
Training Epoch: 20 [36608/50048]	Loss: 1.1333
Training Epoch: 20 [36736/50048]	Loss: 1.3767
Training Epoch: 20 [36864/50048]	Loss: 1.5106
Training Epoch: 20 [36992/50048]	Loss: 1.3734
Training Epoch: 20 [37120/50048]	Loss: 1.3932
Training Epoch: 20 [37248/50048]	Loss: 1.5061
Training Epoch: 20 [37376/50048]	Loss: 1.0701
Training Epoch: 20 [37504/50048]	Loss: 1.4357
Training Epoch: 20 [37632/50048]	Loss: 1.2742
Training Epoch: 20 [37760/50048]	Loss: 1.4834
Training Epoch: 20 [37888/50048]	Loss: 1.4437
Training Epoch: 20 [38016/50048]	Loss: 1.3746
Training Epoch: 20 [38144/50048]	Loss: 1.4543
Training Epoch: 20 [38272/50048]	Loss: 1.1575
Training Epoch: 20 [38400/50048]	Loss: 1.1955
Training Epoch: 20 [38528/50048]	Loss: 1.2514
Training Epoch: 20 [38656/50048]	Loss: 1.5758
Training Epoch: 20 [38784/50048]	Loss: 1.2999
Training Epoch: 20 [38912/50048]	Loss: 1.2641
Training Epoch: 20 [39040/50048]	Loss: 1.5886
Training Epoch: 20 [39168/50048]	Loss: 1.3345
Training Epoch: 20 [39296/50048]	Loss: 1.4410
Training Epoch: 20 [39424/50048]	Loss: 1.3765
Training Epoch: 20 [39552/50048]	Loss: 1.3018
Training Epoch: 20 [39680/50048]	Loss: 1.3525
Training Epoch: 20 [39808/50048]	Loss: 1.4026
Training Epoch: 20 [39936/50048]	Loss: 1.5313
Training Epoch: 20 [40064/50048]	Loss: 1.1866
Training Epoch: 20 [40192/50048]	Loss: 1.3633
Training Epoch: 20 [40320/50048]	Loss: 1.0988
Training Epoch: 20 [40448/50048]	Loss: 1.3090
Training Epoch: 20 [40576/50048]	Loss: 1.2987
Training Epoch: 20 [40704/50048]	Loss: 1.4173
Training Epoch: 20 [40832/50048]	Loss: 1.3127
Training Epoch: 20 [40960/50048]	Loss: 1.1964
Training Epoch: 20 [41088/50048]	Loss: 1.4954
Training Epoch: 20 [41216/50048]	Loss: 1.5004
Training Epoch: 20 [41344/50048]	Loss: 1.2748
Training Epoch: 20 [41472/50048]	Loss: 1.5505
Training Epoch: 20 [41600/50048]	Loss: 0.9222
Training Epoch: 20 [41728/50048]	Loss: 1.5321
Training Epoch: 20 [41856/50048]	Loss: 1.5320
Training Epoch: 20 [41984/50048]	Loss: 1.3343
Training Epoch: 20 [42112/50048]	Loss: 1.5378
Training Epoch: 20 [42240/50048]	Loss: 1.3632
Training Epoch: 20 [42368/50048]	Loss: 1.3754
Training Epoch: 20 [42496/50048]	Loss: 1.4040
Training Epoch: 20 [42624/50048]	Loss: 1.1526
Training Epoch: 20 [42752/50048]	Loss: 1.5906
Training Epoch: 20 [42880/50048]	Loss: 1.3108
Training Epoch: 20 [43008/50048]	Loss: 1.4893
Training Epoch: 20 [43136/50048]	Loss: 1.1294
Training Epoch: 20 [43264/50048]	Loss: 1.3474
Training Epoch: 20 [43392/50048]	Loss: 1.5634
Training Epoch: 20 [43520/50048]	Loss: 1.3627
Training Epoch: 20 [43648/50048]	Loss: 1.1631
Training Epoch: 20 [43776/50048]	Loss: 1.3994
Training Epoch: 20 [43904/50048]	Loss: 1.5537
Training Epoch: 20 [44032/50048]	Loss: 1.4221
Training Epoch: 20 [44160/50048]	Loss: 1.3951
Training Epoch: 20 [44288/50048]	Loss: 1.3174
Training Epoch: 20 [44416/50048]	Loss: 1.3469
Training Epoch: 20 [44544/50048]	Loss: 1.6635
Training Epoch: 20 [44672/50048]	Loss: 1.4798
Training Epoch: 20 [44800/50048]	Loss: 1.5821
Training Epoch: 20 [44928/50048]	Loss: 1.3490
Training Epoch: 20 [45056/50048]	Loss: 1.1654
Training Epoch: 20 [45184/50048]	Loss: 1.3331
Training Epoch: 20 [45312/50048]	Loss: 1.6032
Training Epoch: 20 [45440/50048]	Loss: 1.3241
Training Epoch: 20 [45568/50048]	Loss: 1.1978
Training Epoch: 20 [45696/50048]	Loss: 1.3479
Training Epoch: 20 [45824/50048]	Loss: 1.1942
Training Epoch: 20 [45952/50048]	Loss: 1.3604
Training Epoch: 20 [46080/50048]	Loss: 1.4255
Training Epoch: 20 [46208/50048]	Loss: 1.2915
Training Epoch: 20 [46336/50048]	Loss: 1.5064
Training Epoch: 20 [46464/50048]	Loss: 1.4278
Training Epoch: 20 [46592/50048]	Loss: 1.2878
Training Epoch: 20 [46720/50048]	Loss: 1.3546
Training Epoch: 20 [46848/50048]	Loss: 1.2629
Training Epoch: 20 [46976/50048]	Loss: 1.1596
Training Epoch: 20 [47104/50048]	Loss: 1.2812
Training Epoch: 20 [47232/50048]	Loss: 1.1270
Training Epoch: 20 [47360/50048]	Loss: 1.2227
Training Epoch: 20 [47488/50048]	Loss: 1.1909
Training Epoch: 20 [47616/50048]	Loss: 1.3537
Training Epoch: 20 [47744/50048]	Loss: 1.4119
Training Epoch: 20 [47872/50048]	Loss: 1.3736
Training Epoch: 20 [48000/50048]	Loss: 1.3442
Training Epoch: 20 [48128/50048]	Loss: 1.4378
Training Epoch: 20 [48256/50048]	Loss: 1.2884
Training Epoch: 20 [48384/50048]	Loss: 1.2419
Training Epoch: 20 [48512/50048]	Loss: 1.4904
Training Epoch: 20 [48640/50048]	Loss: 1.2454
Training Epoch: 20 [48768/50048]	Loss: 1.2699
Training Epoch: 20 [48896/50048]	Loss: 1.2661
Training Epoch: 20 [49024/50048]	Loss: 1.3743
Training Epoch: 20 [49152/50048]	Loss: 1.1903
Training Epoch: 20 [49280/50048]	Loss: 1.4331
Training Epoch: 20 [49408/50048]	Loss: 1.2809
Training Epoch: 20 [49536/50048]	Loss: 1.2822
Training Epoch: 20 [49664/50048]	Loss: 1.4405
Training Epoch: 20 [49792/50048]	Loss: 1.3488
Training Epoch: 20 [49920/50048]	Loss: 1.5410
Training Epoch: 20 [50048/50048]	Loss: 1.4658
Validation Epoch: 20, Average loss: 0.0124, Accuracy: 0.5689
Training Epoch: 21 [128/50048]	Loss: 1.3502
Training Epoch: 21 [256/50048]	Loss: 1.1616
Training Epoch: 21 [384/50048]	Loss: 1.3558
Training Epoch: 21 [512/50048]	Loss: 1.4116
Training Epoch: 21 [640/50048]	Loss: 1.3221
Training Epoch: 21 [768/50048]	Loss: 1.3827
Training Epoch: 21 [896/50048]	Loss: 1.0000
Training Epoch: 21 [1024/50048]	Loss: 1.1918
Training Epoch: 21 [1152/50048]	Loss: 1.3046
Training Epoch: 21 [1280/50048]	Loss: 1.5669
Training Epoch: 21 [1408/50048]	Loss: 1.5005
Training Epoch: 21 [1536/50048]	Loss: 1.3412
Training Epoch: 21 [1664/50048]	Loss: 1.3749
Training Epoch: 21 [1792/50048]	Loss: 1.1990
Training Epoch: 21 [1920/50048]	Loss: 1.1985
Training Epoch: 21 [2048/50048]	Loss: 1.2941
Training Epoch: 21 [2176/50048]	Loss: 1.1320
Training Epoch: 21 [2304/50048]	Loss: 1.2316
Training Epoch: 21 [2432/50048]	Loss: 1.1100
Training Epoch: 21 [2560/50048]	Loss: 1.2061
Training Epoch: 21 [2688/50048]	Loss: 1.1810
Training Epoch: 21 [2816/50048]	Loss: 1.1964
Training Epoch: 21 [2944/50048]	Loss: 1.0878
Training Epoch: 21 [3072/50048]	Loss: 1.1719
Training Epoch: 21 [3200/50048]	Loss: 1.2492
Training Epoch: 21 [3328/50048]	Loss: 1.3001
Training Epoch: 21 [3456/50048]	Loss: 1.2786
Training Epoch: 21 [3584/50048]	Loss: 1.4514
Training Epoch: 21 [3712/50048]	Loss: 1.2564
Training Epoch: 21 [3840/50048]	Loss: 1.5914
Training Epoch: 21 [3968/50048]	Loss: 1.3719
Training Epoch: 21 [4096/50048]	Loss: 1.5237
Training Epoch: 21 [4224/50048]	Loss: 1.1141
Training Epoch: 21 [4352/50048]	Loss: 1.2728
Training Epoch: 21 [4480/50048]	Loss: 1.1683
Training Epoch: 21 [4608/50048]	Loss: 1.4402
Training Epoch: 21 [4736/50048]	Loss: 1.2964
Training Epoch: 21 [4864/50048]	Loss: 1.2615
Training Epoch: 21 [4992/50048]	Loss: 1.2995
Training Epoch: 21 [5120/50048]	Loss: 1.0343
Training Epoch: 21 [5248/50048]	Loss: 1.3617
Training Epoch: 21 [5376/50048]	Loss: 1.2719
Training Epoch: 21 [5504/50048]	Loss: 1.4464
Training Epoch: 21 [5632/50048]	Loss: 1.2244
Training Epoch: 21 [5760/50048]	Loss: 1.2520
Training Epoch: 21 [5888/50048]	Loss: 1.2197
Training Epoch: 21 [6016/50048]	Loss: 1.3149
Training Epoch: 21 [6144/50048]	Loss: 1.2625
Training Epoch: 21 [6272/50048]	Loss: 1.1916
Training Epoch: 21 [6400/50048]	Loss: 1.3834
Training Epoch: 21 [6528/50048]	Loss: 1.2773
Training Epoch: 21 [6656/50048]	Loss: 1.2693
Training Epoch: 21 [6784/50048]	Loss: 1.3980
Training Epoch: 21 [6912/50048]	Loss: 1.3709
Training Epoch: 21 [7040/50048]	Loss: 1.2079
Training Epoch: 21 [7168/50048]	Loss: 1.1328
Training Epoch: 21 [7296/50048]	Loss: 1.2688
Training Epoch: 21 [7424/50048]	Loss: 1.3626
Training Epoch: 21 [7552/50048]	Loss: 1.3269
Training Epoch: 21 [7680/50048]	Loss: 1.5226
Training Epoch: 21 [7808/50048]	Loss: 1.4383
Training Epoch: 21 [7936/50048]	Loss: 1.4720
Training Epoch: 21 [8064/50048]	Loss: 1.1865
Training Epoch: 21 [8192/50048]	Loss: 1.2516
Training Epoch: 21 [8320/50048]	Loss: 1.3154
Training Epoch: 21 [8448/50048]	Loss: 1.2802
Training Epoch: 21 [8576/50048]	Loss: 1.1791
Training Epoch: 21 [8704/50048]	Loss: 1.1489
Training Epoch: 21 [8832/50048]	Loss: 1.4157
Training Epoch: 21 [8960/50048]	Loss: 1.2642
Training Epoch: 21 [9088/50048]	Loss: 1.2530
Training Epoch: 21 [9216/50048]	Loss: 1.3742
Training Epoch: 21 [9344/50048]	Loss: 1.3254
Training Epoch: 21 [9472/50048]	Loss: 1.3595
Training Epoch: 21 [9600/50048]	Loss: 1.0946
Training Epoch: 21 [9728/50048]	Loss: 1.7883
Training Epoch: 21 [9856/50048]	Loss: 1.4068
Training Epoch: 21 [9984/50048]	Loss: 1.2472
Training Epoch: 21 [10112/50048]	Loss: 1.2319
Training Epoch: 21 [10240/50048]	Loss: 1.2547
Training Epoch: 21 [10368/50048]	Loss: 1.3065
Training Epoch: 21 [10496/50048]	Loss: 1.1606
Training Epoch: 21 [10624/50048]	Loss: 1.1506
Training Epoch: 21 [10752/50048]	Loss: 1.3777
Training Epoch: 21 [10880/50048]	Loss: 1.2088
Training Epoch: 21 [11008/50048]	Loss: 1.3425
Training Epoch: 21 [11136/50048]	Loss: 1.1816
Training Epoch: 21 [11264/50048]	Loss: 1.4699
Training Epoch: 21 [11392/50048]	Loss: 1.1486
Training Epoch: 21 [11520/50048]	Loss: 1.4280
Training Epoch: 21 [11648/50048]	Loss: 1.2934
Training Epoch: 21 [11776/50048]	Loss: 1.3198
Training Epoch: 21 [11904/50048]	Loss: 1.1797
Training Epoch: 21 [12032/50048]	Loss: 1.2619
Training Epoch: 21 [12160/50048]	Loss: 1.2571
Training Epoch: 21 [12288/50048]	Loss: 1.1404
Training Epoch: 21 [12416/50048]	Loss: 1.1227
Training Epoch: 21 [12544/50048]	Loss: 1.2160
Training Epoch: 21 [12672/50048]	Loss: 1.3875
Training Epoch: 21 [12800/50048]	Loss: 1.2977
Training Epoch: 21 [12928/50048]	Loss: 1.1089
Training Epoch: 21 [13056/50048]	Loss: 1.2333
Training Epoch: 21 [13184/50048]	Loss: 1.2778
Training Epoch: 21 [13312/50048]	Loss: 1.2681
Training Epoch: 21 [13440/50048]	Loss: 1.2752
Training Epoch: 21 [13568/50048]	Loss: 1.3337
Training Epoch: 21 [13696/50048]	Loss: 1.5038
Training Epoch: 21 [13824/50048]	Loss: 1.3184
Training Epoch: 21 [13952/50048]	Loss: 1.2668
Training Epoch: 21 [14080/50048]	Loss: 0.9932
Training Epoch: 21 [14208/50048]	Loss: 1.2650
Training Epoch: 21 [14336/50048]	Loss: 1.3171
Training Epoch: 21 [14464/50048]	Loss: 1.2351
Training Epoch: 21 [14592/50048]	Loss: 1.3453
Training Epoch: 21 [14720/50048]	Loss: 1.2601
Training Epoch: 21 [14848/50048]	Loss: 1.1837
Training Epoch: 21 [14976/50048]	Loss: 1.5280
Training Epoch: 21 [15104/50048]	Loss: 1.3510
Training Epoch: 21 [15232/50048]	Loss: 1.5956
Training Epoch: 21 [15360/50048]	Loss: 1.3577
Training Epoch: 21 [15488/50048]	Loss: 1.2981
Training Epoch: 21 [15616/50048]	Loss: 1.0117
Training Epoch: 21 [15744/50048]	Loss: 1.2766
Training Epoch: 21 [15872/50048]	Loss: 1.4277
Training Epoch: 21 [16000/50048]	Loss: 1.3417
Training Epoch: 21 [16128/50048]	Loss: 1.2787
Training Epoch: 21 [16256/50048]	Loss: 1.1079
Training Epoch: 21 [16384/50048]	Loss: 1.5018
Training Epoch: 21 [16512/50048]	Loss: 1.4799
Training Epoch: 21 [16640/50048]	Loss: 1.4558
Training Epoch: 21 [16768/50048]	Loss: 1.3949
Training Epoch: 21 [16896/50048]	Loss: 1.2855
Training Epoch: 21 [17024/50048]	Loss: 1.2148
Training Epoch: 21 [17152/50048]	Loss: 1.3832
Training Epoch: 21 [17280/50048]	Loss: 1.3053
Training Epoch: 21 [17408/50048]	Loss: 1.3109
Training Epoch: 21 [17536/50048]	Loss: 1.3390
Training Epoch: 21 [17664/50048]	Loss: 1.1940
Training Epoch: 21 [17792/50048]	Loss: 1.0885
Training Epoch: 21 [17920/50048]	Loss: 1.3847
Training Epoch: 21 [18048/50048]	Loss: 1.5020
Training Epoch: 21 [18176/50048]	Loss: 1.3909
Training Epoch: 21 [18304/50048]	Loss: 1.1613
Training Epoch: 21 [18432/50048]	Loss: 1.2548
Training Epoch: 21 [18560/50048]	Loss: 1.3673
Training Epoch: 21 [18688/50048]	Loss: 1.2343
Training Epoch: 21 [18816/50048]	Loss: 1.5268
Training Epoch: 21 [18944/50048]	Loss: 1.2807
Training Epoch: 21 [19072/50048]	Loss: 1.1054
Training Epoch: 21 [19200/50048]	Loss: 1.4858
Training Epoch: 21 [19328/50048]	Loss: 1.5057
Training Epoch: 21 [19456/50048]	Loss: 1.2575
Training Epoch: 21 [19584/50048]	Loss: 1.1581
Training Epoch: 21 [19712/50048]	Loss: 1.3440
Training Epoch: 21 [19840/50048]	Loss: 1.4931
Training Epoch: 21 [19968/50048]	Loss: 1.5151
Training Epoch: 21 [20096/50048]	Loss: 1.5775
Training Epoch: 21 [20224/50048]	Loss: 1.2038
Training Epoch: 21 [20352/50048]	Loss: 1.5280
Training Epoch: 21 [20480/50048]	Loss: 1.2596
Training Epoch: 21 [20608/50048]	Loss: 1.1940
Training Epoch: 21 [20736/50048]	Loss: 1.2783
Training Epoch: 21 [20864/50048]	Loss: 1.3243
Training Epoch: 21 [20992/50048]	Loss: 1.4428
Training Epoch: 21 [21120/50048]	Loss: 1.1363
Training Epoch: 21 [21248/50048]	Loss: 1.4048
Training Epoch: 21 [21376/50048]	Loss: 1.3457
Training Epoch: 21 [21504/50048]	Loss: 1.6297
Training Epoch: 21 [21632/50048]	Loss: 1.3305
Training Epoch: 21 [21760/50048]	Loss: 1.3680
Training Epoch: 21 [21888/50048]	Loss: 1.4417
Training Epoch: 21 [22016/50048]	Loss: 0.9624
Training Epoch: 21 [22144/50048]	Loss: 1.4175
Training Epoch: 21 [22272/50048]	Loss: 1.1846
Training Epoch: 21 [22400/50048]	Loss: 1.2390
Training Epoch: 21 [22528/50048]	Loss: 1.3853
Training Epoch: 21 [22656/50048]	Loss: 1.2905
Training Epoch: 21 [22784/50048]	Loss: 0.9997
Training Epoch: 21 [22912/50048]	Loss: 1.3295
Training Epoch: 21 [23040/50048]	Loss: 1.3448
Training Epoch: 21 [23168/50048]	Loss: 1.3614
Training Epoch: 21 [23296/50048]	Loss: 1.5018
Training Epoch: 21 [23424/50048]	Loss: 1.5843
Training Epoch: 21 [23552/50048]	Loss: 1.2692
Training Epoch: 21 [23680/50048]	Loss: 1.3822
Training Epoch: 21 [23808/50048]	Loss: 1.0963
Training Epoch: 21 [23936/50048]	Loss: 1.3323
Training Epoch: 21 [24064/50048]	Loss: 1.2314
Training Epoch: 21 [24192/50048]	Loss: 1.2136
Training Epoch: 21 [24320/50048]	Loss: 1.3506
Training Epoch: 21 [24448/50048]	Loss: 1.3509
Training Epoch: 21 [24576/50048]	Loss: 1.8442
Training Epoch: 21 [24704/50048]	Loss: 1.3701
Training Epoch: 21 [24832/50048]	Loss: 1.3304
Training Epoch: 21 [24960/50048]	Loss: 1.3559
Training Epoch: 21 [25088/50048]	Loss: 1.1616
Training Epoch: 21 [25216/50048]	Loss: 1.2042
Training Epoch: 21 [25344/50048]	Loss: 1.4451
Training Epoch: 21 [25472/50048]	Loss: 1.2769
Training Epoch: 21 [25600/50048]	Loss: 1.3731
Training Epoch: 21 [25728/50048]	Loss: 1.4966
Training Epoch: 21 [25856/50048]	Loss: 1.2786
Training Epoch: 21 [25984/50048]	Loss: 1.5557
Training Epoch: 21 [26112/50048]	Loss: 1.1271
Training Epoch: 21 [26240/50048]	Loss: 1.2749
Training Epoch: 21 [26368/50048]	Loss: 1.4426
Training Epoch: 21 [26496/50048]	Loss: 1.5450
Training Epoch: 21 [26624/50048]	Loss: 1.3946
Training Epoch: 21 [26752/50048]	Loss: 1.2305
Training Epoch: 21 [26880/50048]	Loss: 1.2484
Training Epoch: 21 [27008/50048]	Loss: 1.4008
Training Epoch: 21 [27136/50048]	Loss: 1.2394
Training Epoch: 21 [27264/50048]	Loss: 1.3501
Training Epoch: 21 [27392/50048]	Loss: 1.2939
Training Epoch: 21 [27520/50048]	Loss: 1.2540
Training Epoch: 21 [27648/50048]	Loss: 1.2677
Training Epoch: 21 [27776/50048]	Loss: 1.2067
Training Epoch: 21 [27904/50048]	Loss: 1.3616
Training Epoch: 21 [28032/50048]	Loss: 1.3796
Training Epoch: 21 [28160/50048]	Loss: 1.2123
Training Epoch: 21 [28288/50048]	Loss: 1.3525
Training Epoch: 21 [28416/50048]	Loss: 1.1237
Training Epoch: 21 [28544/50048]	Loss: 1.3217
Training Epoch: 21 [28672/50048]	Loss: 1.4412
Training Epoch: 21 [28800/50048]	Loss: 1.3353
Training Epoch: 21 [28928/50048]	Loss: 1.3393
Training Epoch: 21 [29056/50048]	Loss: 1.2309
Training Epoch: 21 [29184/50048]	Loss: 1.5066
Training Epoch: 21 [29312/50048]	Loss: 1.1980
Training Epoch: 21 [29440/50048]	Loss: 1.3443
Training Epoch: 21 [29568/50048]	Loss: 1.1810
Training Epoch: 21 [29696/50048]	Loss: 1.3014
Training Epoch: 21 [29824/50048]	Loss: 1.4334
Training Epoch: 21 [29952/50048]	Loss: 1.1998
Training Epoch: 21 [30080/50048]	Loss: 1.3481
Training Epoch: 21 [30208/50048]	Loss: 1.0276
Training Epoch: 21 [30336/50048]	Loss: 1.3334
Training Epoch: 21 [30464/50048]	Loss: 1.0654
Training Epoch: 21 [30592/50048]	Loss: 1.3009
Training Epoch: 21 [30720/50048]	Loss: 1.4511
Training Epoch: 21 [30848/50048]	Loss: 1.2545
Training Epoch: 21 [30976/50048]	Loss: 1.1497
Training Epoch: 21 [31104/50048]	Loss: 1.3404
Training Epoch: 21 [31232/50048]	Loss: 1.0830
Training Epoch: 21 [31360/50048]	Loss: 1.3028
Training Epoch: 21 [31488/50048]	Loss: 1.1485
Training Epoch: 21 [31616/50048]	Loss: 1.2041
Training Epoch: 21 [31744/50048]	Loss: 1.3573
Training Epoch: 21 [31872/50048]	Loss: 1.4673
Training Epoch: 21 [32000/50048]	Loss: 1.3206
Training Epoch: 21 [32128/50048]	Loss: 1.2376
Training Epoch: 21 [32256/50048]	Loss: 1.5714
Training Epoch: 21 [32384/50048]	Loss: 1.5228
Training Epoch: 21 [32512/50048]	Loss: 1.5242
Training Epoch: 21 [32640/50048]	Loss: 1.2163
Training Epoch: 21 [32768/50048]	Loss: 1.5277
Training Epoch: 21 [32896/50048]	Loss: 1.2357
Training Epoch: 21 [33024/50048]	Loss: 1.2050
Training Epoch: 21 [33152/50048]	Loss: 1.1586
Training Epoch: 21 [33280/50048]	Loss: 1.2552
Training Epoch: 21 [33408/50048]	Loss: 1.2100
Training Epoch: 21 [33536/50048]	Loss: 1.2215
Training Epoch: 21 [33664/50048]	Loss: 1.2692
Training Epoch: 21 [33792/50048]	Loss: 1.2224
Training Epoch: 21 [33920/50048]	Loss: 1.2672
Training Epoch: 21 [34048/50048]	Loss: 1.3504
Training Epoch: 21 [34176/50048]	Loss: 1.2328
Training Epoch: 21 [34304/50048]	Loss: 1.1128
Training Epoch: 21 [34432/50048]	Loss: 1.8126
Training Epoch: 21 [34560/50048]	Loss: 1.4406
Training Epoch: 21 [34688/50048]	Loss: 1.2474
Training Epoch: 21 [34816/50048]	Loss: 1.2958
Training Epoch: 21 [34944/50048]	Loss: 1.2588
Training Epoch: 21 [35072/50048]	Loss: 1.5309
Training Epoch: 21 [35200/50048]	Loss: 1.2816
Training Epoch: 21 [35328/50048]	Loss: 1.2813
Training Epoch: 21 [35456/50048]	Loss: 1.4000
Training Epoch: 21 [35584/50048]	Loss: 1.2382
Training Epoch: 21 [35712/50048]	Loss: 1.2534
Training Epoch: 21 [35840/50048]	Loss: 1.4782
Training Epoch: 21 [35968/50048]	Loss: 1.3310
Training Epoch: 21 [36096/50048]	Loss: 1.3227
Training Epoch: 21 [36224/50048]	Loss: 1.1985
Training Epoch: 21 [36352/50048]	Loss: 1.5307
Training Epoch: 21 [36480/50048]	Loss: 1.2984
Training Epoch: 21 [36608/50048]	Loss: 1.2771
Training Epoch: 21 [36736/50048]	Loss: 1.3190
Training Epoch: 21 [36864/50048]	Loss: 1.1655
Training Epoch: 21 [36992/50048]	Loss: 1.3441
Training Epoch: 21 [37120/50048]	Loss: 1.3886
Training Epoch: 21 [37248/50048]	Loss: 1.3227
Training Epoch: 21 [37376/50048]	Loss: 1.3064
Training Epoch: 21 [37504/50048]	Loss: 1.3920
Training Epoch: 21 [37632/50048]	Loss: 1.2558
Training Epoch: 21 [37760/50048]	Loss: 1.1539
Training Epoch: 21 [37888/50048]	Loss: 1.4219
Training Epoch: 21 [38016/50048]	Loss: 1.4249
Training Epoch: 21 [38144/50048]	Loss: 1.4628
Training Epoch: 21 [38272/50048]	Loss: 1.2842
Training Epoch: 21 [38400/50048]	Loss: 1.2491
Training Epoch: 21 [38528/50048]	Loss: 1.1742
Training Epoch: 21 [38656/50048]	Loss: 1.3108
Training Epoch: 21 [38784/50048]	Loss: 1.3532
Training Epoch: 21 [38912/50048]	Loss: 1.3522
Training Epoch: 21 [39040/50048]	Loss: 1.5137
Training Epoch: 21 [39168/50048]	Loss: 1.3701
Training Epoch: 21 [39296/50048]	Loss: 1.4320
Training Epoch: 21 [39424/50048]	Loss: 1.4736
Training Epoch: 21 [39552/50048]	Loss: 1.3598
Training Epoch: 21 [39680/50048]	Loss: 1.3024
Training Epoch: 21 [39808/50048]	Loss: 1.0259
Training Epoch: 21 [39936/50048]	Loss: 1.3632
Training Epoch: 21 [40064/50048]	Loss: 1.3606
Training Epoch: 21 [40192/50048]	Loss: 1.4500
Training Epoch: 21 [40320/50048]	Loss: 1.2867
Training Epoch: 21 [40448/50048]	Loss: 1.2471
Training Epoch: 21 [40576/50048]	Loss: 1.2527
Training Epoch: 21 [40704/50048]	Loss: 1.3248
Training Epoch: 21 [40832/50048]	Loss: 1.4385
Training Epoch: 21 [40960/50048]	Loss: 1.1496
Training Epoch: 21 [41088/50048]	Loss: 1.3754
Training Epoch: 21 [41216/50048]	Loss: 1.4042
Training Epoch: 21 [41344/50048]	Loss: 1.3327
Training Epoch: 21 [41472/50048]	Loss: 1.1680
Training Epoch: 21 [41600/50048]	Loss: 1.2049
Training Epoch: 21 [41728/50048]	Loss: 1.3730
Training Epoch: 21 [41856/50048]	Loss: 1.4971
Training Epoch: 21 [41984/50048]	Loss: 1.2999
Training Epoch: 21 [42112/50048]	Loss: 1.2193
Training Epoch: 21 [42240/50048]	Loss: 1.1671
Training Epoch: 21 [42368/50048]	Loss: 1.1956
Training Epoch: 21 [42496/50048]	Loss: 1.6131
Training Epoch: 21 [42624/50048]	Loss: 1.3444
Training Epoch: 21 [42752/50048]	Loss: 1.4844
Training Epoch: 21 [42880/50048]	Loss: 1.5368
Training Epoch: 21 [43008/50048]	Loss: 1.3642
Training Epoch: 21 [43136/50048]	Loss: 1.2448
Training Epoch: 21 [43264/50048]	Loss: 1.1281
Training Epoch: 21 [43392/50048]	Loss: 1.1151
Training Epoch: 21 [43520/50048]	Loss: 1.4733
Training Epoch: 21 [43648/50048]	Loss: 1.5482
Training Epoch: 21 [43776/50048]	Loss: 1.2912
Training Epoch: 21 [43904/50048]	Loss: 1.5568
Training Epoch: 21 [44032/50048]	Loss: 1.3795
Training Epoch: 21 [44160/50048]	Loss: 1.4220
Training Epoch: 21 [44288/50048]	Loss: 1.4253
Training Epoch: 21 [44416/50048]	Loss: 1.3856
Training Epoch: 21 [44544/50048]	Loss: 1.4037
Training Epoch: 21 [44672/50048]	Loss: 1.2507
Training Epoch: 21 [44800/50048]	Loss: 1.2861
Training Epoch: 21 [44928/50048]	Loss: 1.3251
Training Epoch: 21 [45056/50048]	Loss: 1.3221
Training Epoch: 21 [45184/50048]	Loss: 1.3685
Training Epoch: 21 [45312/50048]	Loss: 1.4605
Training Epoch: 21 [45440/50048]	Loss: 1.3309
Training Epoch: 21 [45568/50048]	Loss: 1.4035
Training Epoch: 21 [45696/50048]	Loss: 1.3138
Training Epoch: 21 [45824/50048]	Loss: 1.4166
Training Epoch: 21 [45952/50048]	Loss: 1.3920
Training Epoch: 21 [46080/50048]	Loss: 1.3373
Training Epoch: 21 [46208/50048]	Loss: 1.3649
Training Epoch: 21 [46336/50048]	Loss: 1.4825
Training Epoch: 21 [46464/50048]	Loss: 1.4100
Training Epoch: 21 [46592/50048]	Loss: 1.2474
Training Epoch: 21 [46720/50048]	Loss: 1.4976
Training Epoch: 21 [46848/50048]	Loss: 1.1309
Training Epoch: 21 [46976/50048]	Loss: 1.1798
Training Epoch: 21 [47104/50048]	Loss: 1.3782
Training Epoch: 21 [47232/50048]	Loss: 1.1601
Training Epoch: 21 [47360/50048]	Loss: 1.2806
Training Epoch: 21 [47488/50048]	Loss: 1.4010
Training Epoch: 21 [47616/50048]	Loss: 1.2687
Training Epoch: 21 [47744/50048]	Loss: 1.2745
Training Epoch: 21 [47872/50048]	Loss: 1.2749
Training Epoch: 21 [48000/50048]	Loss: 1.2735
Training Epoch: 21 [48128/50048]	Loss: 1.0892
Training Epoch: 21 [48256/50048]	Loss: 1.1877
Training Epoch: 21 [48384/50048]	Loss: 1.4426
Training Epoch: 21 [48512/50048]	Loss: 1.1650
Training Epoch: 21 [48640/50048]	Loss: 1.5044
Training Epoch: 21 [48768/50048]	Loss: 1.2326
Training Epoch: 21 [48896/50048]	Loss: 1.2039
Training Epoch: 21 [49024/50048]	Loss: 0.9953
Training Epoch: 21 [49152/50048]	Loss: 1.2588
Training Epoch: 21 [49280/50048]	Loss: 1.4102
Training Epoch: 21 [49408/50048]	Loss: 1.2915
Training Epoch: 21 [49536/50048]	Loss: 1.3086
Training Epoch: 21 [49664/50048]	Loss: 1.3100
Training Epoch: 21 [49792/50048]	Loss: 1.4812
Training Epoch: 21 [49920/50048]	Loss: 1.4273
Training Epoch: 21 [50048/50048]	Loss: 1.6260
Validation Epoch: 21, Average loss: 0.0125, Accuracy: 0.5712
Training Epoch: 22 [128/50048]	Loss: 1.2021
Training Epoch: 22 [256/50048]	Loss: 1.4896
Training Epoch: 22 [384/50048]	Loss: 1.0512
Training Epoch: 22 [512/50048]	Loss: 1.3647
Training Epoch: 22 [640/50048]	Loss: 1.2641
Training Epoch: 22 [768/50048]	Loss: 1.2049
Training Epoch: 22 [896/50048]	Loss: 1.4167
Training Epoch: 22 [1024/50048]	Loss: 1.0534
Training Epoch: 22 [1152/50048]	Loss: 1.0873
Training Epoch: 22 [1280/50048]	Loss: 1.1218
Training Epoch: 22 [1408/50048]	Loss: 1.4733
Training Epoch: 22 [1536/50048]	Loss: 1.3289
Training Epoch: 22 [1664/50048]	Loss: 1.0883
Training Epoch: 22 [1792/50048]	Loss: 1.2954
Training Epoch: 22 [1920/50048]	Loss: 1.3968
Training Epoch: 22 [2048/50048]	Loss: 1.1991
Training Epoch: 22 [2176/50048]	Loss: 1.2440
Training Epoch: 22 [2304/50048]	Loss: 1.0535
Training Epoch: 22 [2432/50048]	Loss: 1.1255
Training Epoch: 22 [2560/50048]	Loss: 1.3883
Training Epoch: 22 [2688/50048]	Loss: 1.3222
Training Epoch: 22 [2816/50048]	Loss: 1.3626
Training Epoch: 22 [2944/50048]	Loss: 1.2660
Training Epoch: 22 [3072/50048]	Loss: 1.5089
Training Epoch: 22 [3200/50048]	Loss: 1.2280
Training Epoch: 22 [3328/50048]	Loss: 1.3261
Training Epoch: 22 [3456/50048]	Loss: 1.2718
Training Epoch: 22 [3584/50048]	Loss: 1.2127
Training Epoch: 22 [3712/50048]	Loss: 1.2786
Training Epoch: 22 [3840/50048]	Loss: 1.4624
Training Epoch: 22 [3968/50048]	Loss: 1.5764
Training Epoch: 22 [4096/50048]	Loss: 1.2409
Training Epoch: 22 [4224/50048]	Loss: 0.9760
Training Epoch: 22 [4352/50048]	Loss: 1.2433
Training Epoch: 22 [4480/50048]	Loss: 1.2759
Training Epoch: 22 [4608/50048]	Loss: 1.4632
Training Epoch: 22 [4736/50048]	Loss: 1.2747
Training Epoch: 22 [4864/50048]	Loss: 1.0435
Training Epoch: 22 [4992/50048]	Loss: 1.1965
Training Epoch: 22 [5120/50048]	Loss: 1.3478
Training Epoch: 22 [5248/50048]	Loss: 1.2233
Training Epoch: 22 [5376/50048]	Loss: 1.1189
Training Epoch: 22 [5504/50048]	Loss: 1.3081
Training Epoch: 22 [5632/50048]	Loss: 1.3107
Training Epoch: 22 [5760/50048]	Loss: 1.3598
Training Epoch: 22 [5888/50048]	Loss: 1.3063
Training Epoch: 22 [6016/50048]	Loss: 1.2617
Training Epoch: 22 [6144/50048]	Loss: 0.9274
Training Epoch: 22 [6272/50048]	Loss: 1.2001
Training Epoch: 22 [6400/50048]	Loss: 1.3832
Training Epoch: 22 [6528/50048]	Loss: 1.1891
Training Epoch: 22 [6656/50048]	Loss: 1.2882
Training Epoch: 22 [6784/50048]	Loss: 1.1861
Training Epoch: 22 [6912/50048]	Loss: 1.4254
Training Epoch: 22 [7040/50048]	Loss: 1.2295
Training Epoch: 22 [7168/50048]	Loss: 1.1504
Training Epoch: 22 [7296/50048]	Loss: 1.4079
Training Epoch: 22 [7424/50048]	Loss: 1.2348
Training Epoch: 22 [7552/50048]	Loss: 1.1576
Training Epoch: 22 [7680/50048]	Loss: 1.2465
Training Epoch: 22 [7808/50048]	Loss: 1.2139
Training Epoch: 22 [7936/50048]	Loss: 1.1878
Training Epoch: 22 [8064/50048]	Loss: 1.2317
Training Epoch: 22 [8192/50048]	Loss: 1.3203
Training Epoch: 22 [8320/50048]	Loss: 1.2869
Training Epoch: 22 [8448/50048]	Loss: 1.2833
Training Epoch: 22 [8576/50048]	Loss: 1.3066
Training Epoch: 22 [8704/50048]	Loss: 1.1849
Training Epoch: 22 [8832/50048]	Loss: 1.2276
Training Epoch: 22 [8960/50048]	Loss: 1.1239
Training Epoch: 22 [9088/50048]	Loss: 1.0967
Training Epoch: 22 [9216/50048]	Loss: 1.3821
Training Epoch: 22 [9344/50048]	Loss: 1.3324
Training Epoch: 22 [9472/50048]	Loss: 1.3623
Training Epoch: 22 [9600/50048]	Loss: 1.2276
Training Epoch: 22 [9728/50048]	Loss: 1.2359
Training Epoch: 22 [9856/50048]	Loss: 1.5161
Training Epoch: 22 [9984/50048]	Loss: 1.3536
Training Epoch: 22 [10112/50048]	Loss: 1.2562
Training Epoch: 22 [10240/50048]	Loss: 1.2115
Training Epoch: 22 [10368/50048]	Loss: 1.4393
Training Epoch: 22 [10496/50048]	Loss: 1.1795
Training Epoch: 22 [10624/50048]	Loss: 1.2918
Training Epoch: 22 [10752/50048]	Loss: 1.2003
Training Epoch: 22 [10880/50048]	Loss: 1.3326
Training Epoch: 22 [11008/50048]	Loss: 1.3270
Training Epoch: 22 [11136/50048]	Loss: 1.3135
Training Epoch: 22 [11264/50048]	Loss: 1.0603
Training Epoch: 22 [11392/50048]	Loss: 1.3508
Training Epoch: 22 [11520/50048]	Loss: 1.0231
Training Epoch: 22 [11648/50048]	Loss: 1.0872
Training Epoch: 22 [11776/50048]	Loss: 1.1245
Training Epoch: 22 [11904/50048]	Loss: 0.9492
Training Epoch: 22 [12032/50048]	Loss: 1.3082
Training Epoch: 22 [12160/50048]	Loss: 1.2266
Training Epoch: 22 [12288/50048]	Loss: 1.4300
Training Epoch: 22 [12416/50048]	Loss: 1.3125
Training Epoch: 22 [12544/50048]	Loss: 1.2898
Training Epoch: 22 [12672/50048]	Loss: 1.2755
Training Epoch: 22 [12800/50048]	Loss: 1.2881
Training Epoch: 22 [12928/50048]	Loss: 1.2651
Training Epoch: 22 [13056/50048]	Loss: 1.2576
Training Epoch: 22 [13184/50048]	Loss: 1.1344
Training Epoch: 22 [13312/50048]	Loss: 1.1674
Training Epoch: 22 [13440/50048]	Loss: 1.1735
Training Epoch: 22 [13568/50048]	Loss: 1.2939
Training Epoch: 22 [13696/50048]	Loss: 1.1257
Training Epoch: 22 [13824/50048]	Loss: 1.1895
Training Epoch: 22 [13952/50048]	Loss: 1.1805
Training Epoch: 22 [14080/50048]	Loss: 1.4652
Training Epoch: 22 [14208/50048]	Loss: 1.2218
Training Epoch: 22 [14336/50048]	Loss: 1.4765
Training Epoch: 22 [14464/50048]	Loss: 1.1908
Training Epoch: 22 [14592/50048]	Loss: 1.1275
Training Epoch: 22 [14720/50048]	Loss: 1.3192
Training Epoch: 22 [14848/50048]	Loss: 1.3832
Training Epoch: 22 [14976/50048]	Loss: 1.2221
Training Epoch: 22 [15104/50048]	Loss: 1.3570
Training Epoch: 22 [15232/50048]	Loss: 1.3992
Training Epoch: 22 [15360/50048]	Loss: 1.7216
Training Epoch: 22 [15488/50048]	Loss: 1.2286
Training Epoch: 22 [15616/50048]	Loss: 1.1595
Training Epoch: 22 [15744/50048]	Loss: 1.2501
Training Epoch: 22 [15872/50048]	Loss: 1.2386
Training Epoch: 22 [16000/50048]	Loss: 1.2681
Training Epoch: 22 [16128/50048]	Loss: 1.3605
Training Epoch: 22 [16256/50048]	Loss: 1.4618
Training Epoch: 22 [16384/50048]	Loss: 1.1584
Training Epoch: 22 [16512/50048]	Loss: 1.2851
Training Epoch: 22 [16640/50048]	Loss: 1.3988
Training Epoch: 22 [16768/50048]	Loss: 1.4748
Training Epoch: 22 [16896/50048]	Loss: 1.0595
Training Epoch: 22 [17024/50048]	Loss: 1.2724
Training Epoch: 22 [17152/50048]	Loss: 1.2326
Training Epoch: 22 [17280/50048]	Loss: 1.3230
Training Epoch: 22 [17408/50048]	Loss: 1.0896
Training Epoch: 22 [17536/50048]	Loss: 1.2075
Training Epoch: 22 [17664/50048]	Loss: 1.3048
Training Epoch: 22 [17792/50048]	Loss: 1.2574
Training Epoch: 22 [17920/50048]	Loss: 1.5919
Training Epoch: 22 [18048/50048]	Loss: 1.1368
Training Epoch: 22 [18176/50048]	Loss: 1.2613
Training Epoch: 22 [18304/50048]	Loss: 1.2673
Training Epoch: 22 [18432/50048]	Loss: 1.2487
Training Epoch: 22 [18560/50048]	Loss: 1.1166
Training Epoch: 22 [18688/50048]	Loss: 1.1501
Training Epoch: 22 [18816/50048]	Loss: 1.4120
Training Epoch: 22 [18944/50048]	Loss: 1.3009
Training Epoch: 22 [19072/50048]	Loss: 1.3565
Training Epoch: 22 [19200/50048]	Loss: 1.1325
Training Epoch: 22 [19328/50048]	Loss: 1.3557
Training Epoch: 22 [19456/50048]	Loss: 1.3958
Training Epoch: 22 [19584/50048]	Loss: 1.3200
Training Epoch: 22 [19712/50048]	Loss: 1.2602
Training Epoch: 22 [19840/50048]	Loss: 1.2326
Training Epoch: 22 [19968/50048]	Loss: 1.1042
Training Epoch: 22 [20096/50048]	Loss: 1.1901
Training Epoch: 22 [20224/50048]	Loss: 1.1034
Training Epoch: 22 [20352/50048]	Loss: 1.3283
Training Epoch: 22 [20480/50048]	Loss: 1.2006
Training Epoch: 22 [20608/50048]	Loss: 1.2975
Training Epoch: 22 [20736/50048]	Loss: 1.2332
Training Epoch: 22 [20864/50048]	Loss: 1.3517
Training Epoch: 22 [20992/50048]	Loss: 1.5416
Training Epoch: 22 [21120/50048]	Loss: 1.4457
Training Epoch: 22 [21248/50048]	Loss: 1.1759
Training Epoch: 22 [21376/50048]	Loss: 1.4154
Training Epoch: 22 [21504/50048]	Loss: 1.2141
Training Epoch: 22 [21632/50048]	Loss: 1.0723
Training Epoch: 22 [21760/50048]	Loss: 1.1870
Training Epoch: 22 [21888/50048]	Loss: 1.3386
Training Epoch: 22 [22016/50048]	Loss: 1.1508
Training Epoch: 22 [22144/50048]	Loss: 1.4376
Training Epoch: 22 [22272/50048]	Loss: 1.2408
Training Epoch: 22 [22400/50048]	Loss: 1.2438
Training Epoch: 22 [22528/50048]	Loss: 1.3364
Training Epoch: 22 [22656/50048]	Loss: 1.3160
Training Epoch: 22 [22784/50048]	Loss: 1.2794
Training Epoch: 22 [22912/50048]	Loss: 1.1127
Training Epoch: 22 [23040/50048]	Loss: 1.1470
Training Epoch: 22 [23168/50048]	Loss: 1.2928
Training Epoch: 22 [23296/50048]	Loss: 1.2397
Training Epoch: 22 [23424/50048]	Loss: 1.3458
Training Epoch: 22 [23552/50048]	Loss: 1.2913
Training Epoch: 22 [23680/50048]	Loss: 1.2570
Training Epoch: 22 [23808/50048]	Loss: 1.2295
Training Epoch: 22 [23936/50048]	Loss: 1.1045
Training Epoch: 22 [24064/50048]	Loss: 0.9611
Training Epoch: 22 [24192/50048]	Loss: 1.3097
Training Epoch: 22 [24320/50048]	Loss: 1.4477
Training Epoch: 22 [24448/50048]	Loss: 1.0436
Training Epoch: 22 [24576/50048]	Loss: 1.3443
Training Epoch: 22 [24704/50048]	Loss: 1.4689
Training Epoch: 22 [24832/50048]	Loss: 1.2764
Training Epoch: 22 [24960/50048]	Loss: 1.3467
Training Epoch: 22 [25088/50048]	Loss: 1.1898
Training Epoch: 22 [25216/50048]	Loss: 1.5366
Training Epoch: 22 [25344/50048]	Loss: 1.2748
Training Epoch: 22 [25472/50048]	Loss: 1.2951
Training Epoch: 22 [25600/50048]	Loss: 1.1444
Training Epoch: 22 [25728/50048]	Loss: 1.6006
Training Epoch: 22 [25856/50048]	Loss: 1.2547
Training Epoch: 22 [25984/50048]	Loss: 1.2213
Training Epoch: 22 [26112/50048]	Loss: 1.2054
Training Epoch: 22 [26240/50048]	Loss: 1.3794
Training Epoch: 22 [26368/50048]	Loss: 1.3515
Training Epoch: 22 [26496/50048]	Loss: 1.0639
Training Epoch: 22 [26624/50048]	Loss: 1.2725
Training Epoch: 22 [26752/50048]	Loss: 1.2884
Training Epoch: 22 [26880/50048]	Loss: 1.2066
Training Epoch: 22 [27008/50048]	Loss: 1.1824
Training Epoch: 22 [27136/50048]	Loss: 1.0875
Training Epoch: 22 [27264/50048]	Loss: 1.0894
Training Epoch: 22 [27392/50048]	Loss: 1.0025
Training Epoch: 22 [27520/50048]	Loss: 1.1925
Training Epoch: 22 [27648/50048]	Loss: 1.3499
Training Epoch: 22 [27776/50048]	Loss: 1.4726
Training Epoch: 22 [27904/50048]	Loss: 1.3186
Training Epoch: 22 [28032/50048]	Loss: 1.4236
Training Epoch: 22 [28160/50048]	Loss: 1.1047
Training Epoch: 22 [28288/50048]	Loss: 1.0837
Training Epoch: 22 [28416/50048]	Loss: 1.3470
Training Epoch: 22 [28544/50048]	Loss: 1.0965
Training Epoch: 22 [28672/50048]	Loss: 1.2833
Training Epoch: 22 [28800/50048]	Loss: 1.1363
Training Epoch: 22 [28928/50048]	Loss: 1.3180
Training Epoch: 22 [29056/50048]	Loss: 1.3848
Training Epoch: 22 [29184/50048]	Loss: 1.0801
Training Epoch: 22 [29312/50048]	Loss: 1.2213
Training Epoch: 22 [29440/50048]	Loss: 1.3903
Training Epoch: 22 [29568/50048]	Loss: 1.4710
Training Epoch: 22 [29696/50048]	Loss: 1.2377
Training Epoch: 22 [29824/50048]	Loss: 1.4416
Training Epoch: 22 [29952/50048]	Loss: 1.0932
Training Epoch: 22 [30080/50048]	Loss: 1.1134
Training Epoch: 22 [30208/50048]	Loss: 1.2257
Training Epoch: 22 [30336/50048]	Loss: 1.2131
Training Epoch: 22 [30464/50048]	Loss: 1.3918
Training Epoch: 22 [30592/50048]	Loss: 1.3895
Training Epoch: 22 [30720/50048]	Loss: 1.3272
Training Epoch: 22 [30848/50048]	Loss: 1.3014
Training Epoch: 22 [30976/50048]	Loss: 1.2480
Training Epoch: 22 [31104/50048]	Loss: 1.4610
Training Epoch: 22 [31232/50048]	Loss: 1.3142
Training Epoch: 22 [31360/50048]	Loss: 1.1304
Training Epoch: 22 [31488/50048]	Loss: 1.4791
Training Epoch: 22 [31616/50048]	Loss: 1.4179
Training Epoch: 22 [31744/50048]	Loss: 1.1154
Training Epoch: 22 [31872/50048]	Loss: 1.3301
Training Epoch: 22 [32000/50048]	Loss: 1.2696
Training Epoch: 22 [32128/50048]	Loss: 1.3017
Training Epoch: 22 [32256/50048]	Loss: 1.3944
Training Epoch: 22 [32384/50048]	Loss: 1.1700
Training Epoch: 22 [32512/50048]	Loss: 1.3906
Training Epoch: 22 [32640/50048]	Loss: 1.2773
Training Epoch: 22 [32768/50048]	Loss: 1.4040
Training Epoch: 22 [32896/50048]	Loss: 1.1504
Training Epoch: 22 [33024/50048]	Loss: 1.3241
Training Epoch: 22 [33152/50048]	Loss: 1.2975
Training Epoch: 22 [33280/50048]	Loss: 1.4953
Training Epoch: 22 [33408/50048]	Loss: 1.3462
Training Epoch: 22 [33536/50048]	Loss: 1.4593
Training Epoch: 22 [33664/50048]	Loss: 1.2256
Training Epoch: 22 [33792/50048]	Loss: 1.1857
Training Epoch: 22 [33920/50048]	Loss: 1.3749
Training Epoch: 22 [34048/50048]	Loss: 1.3765
Training Epoch: 22 [34176/50048]	Loss: 1.4467
Training Epoch: 22 [34304/50048]	Loss: 1.3140
Training Epoch: 22 [34432/50048]	Loss: 1.3494
Training Epoch: 22 [34560/50048]	Loss: 1.2531
Training Epoch: 22 [34688/50048]	Loss: 1.1449
Training Epoch: 22 [34816/50048]	Loss: 1.3523
Training Epoch: 22 [34944/50048]	Loss: 1.2677
Training Epoch: 22 [35072/50048]	Loss: 1.5510
Training Epoch: 22 [35200/50048]	Loss: 1.3657
Training Epoch: 22 [35328/50048]	Loss: 1.4845
Training Epoch: 22 [35456/50048]	Loss: 1.4266
Training Epoch: 22 [35584/50048]	Loss: 1.2699
Training Epoch: 22 [35712/50048]	Loss: 1.2365
Training Epoch: 22 [35840/50048]	Loss: 1.4473
Training Epoch: 22 [35968/50048]	Loss: 1.3699
Training Epoch: 22 [36096/50048]	Loss: 1.4899
Training Epoch: 22 [36224/50048]	Loss: 1.2865
Training Epoch: 22 [36352/50048]	Loss: 1.3370
Training Epoch: 22 [36480/50048]	Loss: 1.1054
Training Epoch: 22 [36608/50048]	Loss: 1.1710
Training Epoch: 22 [36736/50048]	Loss: 1.3167
Training Epoch: 22 [36864/50048]	Loss: 1.1339
Training Epoch: 22 [36992/50048]	Loss: 1.2499
Training Epoch: 22 [37120/50048]	Loss: 1.3396
Training Epoch: 22 [37248/50048]	Loss: 1.1224
Training Epoch: 22 [37376/50048]	Loss: 1.4257
Training Epoch: 22 [37504/50048]	Loss: 1.4272
Training Epoch: 22 [37632/50048]	Loss: 1.0137
Training Epoch: 22 [37760/50048]	Loss: 1.1962
Training Epoch: 22 [37888/50048]	Loss: 1.1881
Training Epoch: 22 [38016/50048]	Loss: 1.1440
Training Epoch: 22 [38144/50048]	Loss: 1.1047
Training Epoch: 22 [38272/50048]	Loss: 1.3258
Training Epoch: 22 [38400/50048]	Loss: 1.3427
Training Epoch: 22 [38528/50048]	Loss: 1.4198
Training Epoch: 22 [38656/50048]	Loss: 1.4097
Training Epoch: 22 [38784/50048]	Loss: 1.1388
Training Epoch: 22 [38912/50048]	Loss: 1.3361
Training Epoch: 22 [39040/50048]	Loss: 1.4085
Training Epoch: 22 [39168/50048]	Loss: 1.1371
Training Epoch: 22 [39296/50048]	Loss: 1.3396
Training Epoch: 22 [39424/50048]	Loss: 1.4554
Training Epoch: 22 [39552/50048]	Loss: 1.3773
Training Epoch: 22 [39680/50048]	Loss: 1.2534
Training Epoch: 22 [39808/50048]	Loss: 1.1956
Training Epoch: 22 [39936/50048]	Loss: 1.4810
Training Epoch: 22 [40064/50048]	Loss: 1.1529
Training Epoch: 22 [40192/50048]	Loss: 1.2508
Training Epoch: 22 [40320/50048]	Loss: 1.5468
Training Epoch: 22 [40448/50048]	Loss: 1.1721
Training Epoch: 22 [40576/50048]	Loss: 1.3247
Training Epoch: 22 [40704/50048]	Loss: 1.2646
Training Epoch: 22 [40832/50048]	Loss: 1.3690
Training Epoch: 22 [40960/50048]	Loss: 1.2522
Training Epoch: 22 [41088/50048]	Loss: 1.3061
Training Epoch: 22 [41216/50048]	Loss: 1.1477
Training Epoch: 22 [41344/50048]	Loss: 1.1999
Training Epoch: 22 [41472/50048]	Loss: 1.1506
Training Epoch: 22 [41600/50048]	Loss: 1.2843
Training Epoch: 22 [41728/50048]	Loss: 1.2765
Training Epoch: 22 [41856/50048]	Loss: 1.3839
Training Epoch: 22 [41984/50048]	Loss: 1.2149
Training Epoch: 22 [42112/50048]	Loss: 1.3084
Training Epoch: 22 [42240/50048]	Loss: 1.4907
Training Epoch: 22 [42368/50048]	Loss: 1.3557
Training Epoch: 22 [42496/50048]	Loss: 1.4082
Training Epoch: 22 [42624/50048]	Loss: 1.1695
Training Epoch: 22 [42752/50048]	Loss: 1.1751
Training Epoch: 22 [42880/50048]	Loss: 1.3878
Training Epoch: 22 [43008/50048]	Loss: 1.1831
Training Epoch: 22 [43136/50048]	Loss: 1.1853
Training Epoch: 22 [43264/50048]	Loss: 1.5957
Training Epoch: 22 [43392/50048]	Loss: 1.2295
Training Epoch: 22 [43520/50048]	Loss: 1.1699
Training Epoch: 22 [43648/50048]	Loss: 1.3310
Training Epoch: 22 [43776/50048]	Loss: 1.2078
Training Epoch: 22 [43904/50048]	Loss: 1.2350
Training Epoch: 22 [44032/50048]	Loss: 1.2864
Training Epoch: 22 [44160/50048]	Loss: 1.2838
Training Epoch: 22 [44288/50048]	Loss: 1.2880
Training Epoch: 22 [44416/50048]	Loss: 1.3591
Training Epoch: 22 [44544/50048]	Loss: 1.1246
Training Epoch: 22 [44672/50048]	Loss: 1.4185
Training Epoch: 22 [44800/50048]	Loss: 1.2321
Training Epoch: 22 [44928/50048]	Loss: 1.3511
Training Epoch: 22 [45056/50048]	Loss: 1.3858
Training Epoch: 22 [45184/50048]	Loss: 1.2662
Training Epoch: 22 [45312/50048]	Loss: 1.2017
Training Epoch: 22 [45440/50048]	Loss: 1.4344
Training Epoch: 22 [45568/50048]	Loss: 1.2833
Training Epoch: 22 [45696/50048]	Loss: 1.0771
Training Epoch: 22 [45824/50048]	Loss: 1.2222
Training Epoch: 22 [45952/50048]	Loss: 1.0721
Training Epoch: 22 [46080/50048]	Loss: 1.2736
Training Epoch: 22 [46208/50048]	Loss: 1.3320
Training Epoch: 22 [46336/50048]	Loss: 1.5387
Training Epoch: 22 [46464/50048]	Loss: 1.2537
Training Epoch: 22 [46592/50048]	Loss: 1.2074
Training Epoch: 22 [46720/50048]	Loss: 1.2123
Training Epoch: 22 [46848/50048]	Loss: 1.3856
Training Epoch: 22 [46976/50048]	Loss: 1.2685
Training Epoch: 22 [47104/50048]	Loss: 1.0666
Training Epoch: 22 [47232/50048]	Loss: 1.3887
Training Epoch: 22 [47360/50048]	Loss: 1.3484
Training Epoch: 22 [47488/50048]	Loss: 1.4092
Training Epoch: 22 [47616/50048]	Loss: 1.3072
Training Epoch: 22 [47744/50048]	Loss: 1.2644
Training Epoch: 22 [47872/50048]	Loss: 1.2617
Training Epoch: 22 [48000/50048]	Loss: 1.4743
Training Epoch: 22 [48128/50048]	Loss: 1.2417
Training Epoch: 22 [48256/50048]	Loss: 1.2236
Training Epoch: 22 [48384/50048]	Loss: 1.2287
Training Epoch: 22 [48512/50048]	Loss: 1.4207
Training Epoch: 22 [48640/50048]	Loss: 1.4623
Training Epoch: 22 [48768/50048]	Loss: 1.4561
Training Epoch: 22 [48896/50048]	Loss: 1.4517
Training Epoch: 22 [49024/50048]	Loss: 1.1683
Training Epoch: 22 [49152/50048]	Loss: 1.3618
Training Epoch: 22 [49280/50048]	Loss: 1.3353
Training Epoch: 22 [49408/50048]	Loss: 1.3173
Training Epoch: 22 [49536/50048]	Loss: 1.3919
Training Epoch: 22 [49664/50048]	Loss: 1.4163
Training Epoch: 22 [49792/50048]	Loss: 1.3762
Training Epoch: 22 [49920/50048]	Loss: 1.2093
Training Epoch: 22 [50048/50048]	Loss: 1.0463
Validation Epoch: 22, Average loss: 0.0122, Accuracy: 0.5789
Training Epoch: 23 [128/50048]	Loss: 1.2133
Training Epoch: 23 [256/50048]	Loss: 1.3323
Training Epoch: 23 [384/50048]	Loss: 0.8825
Training Epoch: 23 [512/50048]	Loss: 1.1328
Training Epoch: 23 [640/50048]	Loss: 1.2756
Training Epoch: 23 [768/50048]	Loss: 1.0086
Training Epoch: 23 [896/50048]	Loss: 1.0066
Training Epoch: 23 [1024/50048]	Loss: 1.2096
Training Epoch: 23 [1152/50048]	Loss: 0.8603
Training Epoch: 23 [1280/50048]	Loss: 1.0592
Training Epoch: 23 [1408/50048]	Loss: 1.2572
Training Epoch: 23 [1536/50048]	Loss: 1.3175
Training Epoch: 23 [1664/50048]	Loss: 1.1646
Training Epoch: 23 [1792/50048]	Loss: 1.3726
Training Epoch: 23 [1920/50048]	Loss: 1.0515
Training Epoch: 23 [2048/50048]	Loss: 1.2354
Training Epoch: 23 [2176/50048]	Loss: 1.3431
Training Epoch: 23 [2304/50048]	Loss: 1.3589
Training Epoch: 23 [2432/50048]	Loss: 1.0753
Training Epoch: 23 [2560/50048]	Loss: 1.4742
Training Epoch: 23 [2688/50048]	Loss: 1.1868
Training Epoch: 23 [2816/50048]	Loss: 1.2639
Training Epoch: 23 [2944/50048]	Loss: 1.2450
Training Epoch: 23 [3072/50048]	Loss: 1.2716
Training Epoch: 23 [3200/50048]	Loss: 1.1041
Training Epoch: 23 [3328/50048]	Loss: 1.2611
Training Epoch: 23 [3456/50048]	Loss: 1.1374
Training Epoch: 23 [3584/50048]	Loss: 1.2134
Training Epoch: 23 [3712/50048]	Loss: 1.1418
Training Epoch: 23 [3840/50048]	Loss: 0.9593
Training Epoch: 23 [3968/50048]	Loss: 0.9963
Training Epoch: 23 [4096/50048]	Loss: 1.0430
Training Epoch: 23 [4224/50048]	Loss: 1.0614
Training Epoch: 23 [4352/50048]	Loss: 1.1124
Training Epoch: 23 [4480/50048]	Loss: 1.2592
Training Epoch: 23 [4608/50048]	Loss: 1.3192
Training Epoch: 23 [4736/50048]	Loss: 1.4137
Training Epoch: 23 [4864/50048]	Loss: 1.2390
Training Epoch: 23 [4992/50048]	Loss: 1.2956
Training Epoch: 23 [5120/50048]	Loss: 1.1167
Training Epoch: 23 [5248/50048]	Loss: 1.2413
Training Epoch: 23 [5376/50048]	Loss: 1.1832
Training Epoch: 23 [5504/50048]	Loss: 1.2087
Training Epoch: 23 [5632/50048]	Loss: 1.1907
Training Epoch: 23 [5760/50048]	Loss: 1.3118
Training Epoch: 23 [5888/50048]	Loss: 1.2833
Training Epoch: 23 [6016/50048]	Loss: 1.1061
Training Epoch: 23 [6144/50048]	Loss: 1.0225
Training Epoch: 23 [6272/50048]	Loss: 1.1547
Training Epoch: 23 [6400/50048]	Loss: 1.2372
Training Epoch: 23 [6528/50048]	Loss: 1.1751
Training Epoch: 23 [6656/50048]	Loss: 1.3300
Training Epoch: 23 [6784/50048]	Loss: 1.1946
Training Epoch: 23 [6912/50048]	Loss: 0.9591
Training Epoch: 23 [7040/50048]	Loss: 1.0915
Training Epoch: 23 [7168/50048]	Loss: 1.0841
Training Epoch: 23 [7296/50048]	Loss: 0.9675
Training Epoch: 23 [7424/50048]	Loss: 1.0901
Training Epoch: 23 [7552/50048]	Loss: 1.1238
Training Epoch: 23 [7680/50048]	Loss: 1.5285
Training Epoch: 23 [7808/50048]	Loss: 1.3899
Training Epoch: 23 [7936/50048]	Loss: 1.2606
Training Epoch: 23 [8064/50048]	Loss: 1.2588
Training Epoch: 23 [8192/50048]	Loss: 1.2265
Training Epoch: 23 [8320/50048]	Loss: 1.0568
Training Epoch: 23 [8448/50048]	Loss: 1.3066
Training Epoch: 23 [8576/50048]	Loss: 1.1103
Training Epoch: 23 [8704/50048]	Loss: 1.2311
Training Epoch: 23 [8832/50048]	Loss: 1.1934
Training Epoch: 23 [8960/50048]	Loss: 1.3074
Training Epoch: 23 [9088/50048]	Loss: 0.9293
Training Epoch: 23 [9216/50048]	Loss: 1.1159
Training Epoch: 23 [9344/50048]	Loss: 1.2481
Training Epoch: 23 [9472/50048]	Loss: 1.3094
Training Epoch: 23 [9600/50048]	Loss: 1.1202
Training Epoch: 23 [9728/50048]	Loss: 1.1699
Training Epoch: 23 [9856/50048]	Loss: 1.2361
Training Epoch: 23 [9984/50048]	Loss: 1.4913
Training Epoch: 23 [10112/50048]	Loss: 1.1237
Training Epoch: 23 [10240/50048]	Loss: 1.2211
Training Epoch: 23 [10368/50048]	Loss: 1.3464
Training Epoch: 23 [10496/50048]	Loss: 1.2893
Training Epoch: 23 [10624/50048]	Loss: 1.1067
Training Epoch: 23 [10752/50048]	Loss: 1.4656
Training Epoch: 23 [10880/50048]	Loss: 1.2455
Training Epoch: 23 [11008/50048]	Loss: 1.0290
Training Epoch: 23 [11136/50048]	Loss: 1.2676
Training Epoch: 23 [11264/50048]	Loss: 1.1672
Training Epoch: 23 [11392/50048]	Loss: 1.4986
Training Epoch: 23 [11520/50048]	Loss: 1.0962
Training Epoch: 23 [11648/50048]	Loss: 1.0275
Training Epoch: 23 [11776/50048]	Loss: 1.3573
Training Epoch: 23 [11904/50048]	Loss: 1.3595
Training Epoch: 23 [12032/50048]	Loss: 1.5098
Training Epoch: 23 [12160/50048]	Loss: 1.1912
Training Epoch: 23 [12288/50048]	Loss: 1.2366
Training Epoch: 23 [12416/50048]	Loss: 1.3342
Training Epoch: 23 [12544/50048]	Loss: 1.7230
Training Epoch: 23 [12672/50048]	Loss: 1.0585
Training Epoch: 23 [12800/50048]	Loss: 1.2043
Training Epoch: 23 [12928/50048]	Loss: 1.4323
Training Epoch: 23 [13056/50048]	Loss: 1.1580
Training Epoch: 23 [13184/50048]	Loss: 1.3714
Training Epoch: 23 [13312/50048]	Loss: 1.1354
Training Epoch: 23 [13440/50048]	Loss: 1.2785
Training Epoch: 23 [13568/50048]	Loss: 1.2154
Training Epoch: 23 [13696/50048]	Loss: 1.3410
Training Epoch: 23 [13824/50048]	Loss: 1.2093
Training Epoch: 23 [13952/50048]	Loss: 1.1792
Training Epoch: 23 [14080/50048]	Loss: 1.2863
Training Epoch: 23 [14208/50048]	Loss: 1.1202
Training Epoch: 23 [14336/50048]	Loss: 1.1323
Training Epoch: 23 [14464/50048]	Loss: 1.0510
Training Epoch: 23 [14592/50048]	Loss: 1.5487
Training Epoch: 23 [14720/50048]	Loss: 1.2564
Training Epoch: 23 [14848/50048]	Loss: 1.0094
Training Epoch: 23 [14976/50048]	Loss: 1.2347
Training Epoch: 23 [15104/50048]	Loss: 1.4496
Training Epoch: 23 [15232/50048]	Loss: 1.2504
Training Epoch: 23 [15360/50048]	Loss: 1.2729
Training Epoch: 23 [15488/50048]	Loss: 1.0974
Training Epoch: 23 [15616/50048]	Loss: 1.1689
Training Epoch: 23 [15744/50048]	Loss: 1.0822
Training Epoch: 23 [15872/50048]	Loss: 1.1661
Training Epoch: 23 [16000/50048]	Loss: 1.1788
Training Epoch: 23 [16128/50048]	Loss: 1.3058
Training Epoch: 23 [16256/50048]	Loss: 1.2258
Training Epoch: 23 [16384/50048]	Loss: 1.3955
Training Epoch: 23 [16512/50048]	Loss: 1.0158
Training Epoch: 23 [16640/50048]	Loss: 1.1979
Training Epoch: 23 [16768/50048]	Loss: 1.1025
Training Epoch: 23 [16896/50048]	Loss: 0.9566
Training Epoch: 23 [17024/50048]	Loss: 1.2178
Training Epoch: 23 [17152/50048]	Loss: 1.2580
Training Epoch: 23 [17280/50048]	Loss: 1.3281
Training Epoch: 23 [17408/50048]	Loss: 1.3033
Training Epoch: 23 [17536/50048]	Loss: 1.3770
Training Epoch: 23 [17664/50048]	Loss: 1.3523
Training Epoch: 23 [17792/50048]	Loss: 1.3393
Training Epoch: 23 [17920/50048]	Loss: 1.0611
Training Epoch: 23 [18048/50048]	Loss: 1.1721
Training Epoch: 23 [18176/50048]	Loss: 1.2517
Training Epoch: 23 [18304/50048]	Loss: 1.1030
Training Epoch: 23 [18432/50048]	Loss: 1.4718
Training Epoch: 23 [18560/50048]	Loss: 1.3560
Training Epoch: 23 [18688/50048]	Loss: 1.4576
Training Epoch: 23 [18816/50048]	Loss: 1.1138
Training Epoch: 23 [18944/50048]	Loss: 1.2206
Training Epoch: 23 [19072/50048]	Loss: 1.2259
Training Epoch: 23 [19200/50048]	Loss: 1.2186
Training Epoch: 23 [19328/50048]	Loss: 1.1749
Training Epoch: 23 [19456/50048]	Loss: 1.2316
Training Epoch: 23 [19584/50048]	Loss: 1.1503
Training Epoch: 23 [19712/50048]	Loss: 1.3670
Training Epoch: 23 [19840/50048]	Loss: 1.1334
Training Epoch: 23 [19968/50048]	Loss: 1.1384
Training Epoch: 23 [20096/50048]	Loss: 1.2287
Training Epoch: 23 [20224/50048]	Loss: 1.3131
Training Epoch: 23 [20352/50048]	Loss: 1.1866
Training Epoch: 23 [20480/50048]	Loss: 1.4254
Training Epoch: 23 [20608/50048]	Loss: 1.5337
Training Epoch: 23 [20736/50048]	Loss: 1.1416
Training Epoch: 23 [20864/50048]	Loss: 1.4835
Training Epoch: 23 [20992/50048]	Loss: 1.0964
Training Epoch: 23 [21120/50048]	Loss: 1.2612
Training Epoch: 23 [21248/50048]	Loss: 1.0933
Training Epoch: 23 [21376/50048]	Loss: 1.2007
Training Epoch: 23 [21504/50048]	Loss: 1.3140
Training Epoch: 23 [21632/50048]	Loss: 1.3736
Training Epoch: 23 [21760/50048]	Loss: 1.1951
Training Epoch: 23 [21888/50048]	Loss: 1.1134
Training Epoch: 23 [22016/50048]	Loss: 1.4469
Training Epoch: 23 [22144/50048]	Loss: 1.1856
Training Epoch: 23 [22272/50048]	Loss: 1.2493
Training Epoch: 23 [22400/50048]	Loss: 1.2480
Training Epoch: 23 [22528/50048]	Loss: 1.4554
Training Epoch: 23 [22656/50048]	Loss: 1.1207
Training Epoch: 23 [22784/50048]	Loss: 1.2181
Training Epoch: 23 [22912/50048]	Loss: 1.4730
Training Epoch: 23 [23040/50048]	Loss: 1.2203
Training Epoch: 23 [23168/50048]	Loss: 1.5146
Training Epoch: 23 [23296/50048]	Loss: 1.4774
Training Epoch: 23 [23424/50048]	Loss: 1.1439
Training Epoch: 23 [23552/50048]	Loss: 1.1630
Training Epoch: 23 [23680/50048]	Loss: 1.2137
Training Epoch: 23 [23808/50048]	Loss: 1.1761
Training Epoch: 23 [23936/50048]	Loss: 1.2112
Training Epoch: 23 [24064/50048]	Loss: 1.1932
Training Epoch: 23 [24192/50048]	Loss: 1.2029
Training Epoch: 23 [24320/50048]	Loss: 1.2379
Training Epoch: 23 [24448/50048]	Loss: 1.2023
Training Epoch: 23 [24576/50048]	Loss: 1.2297
Training Epoch: 23 [24704/50048]	Loss: 1.2652
Training Epoch: 23 [24832/50048]	Loss: 1.0972
Training Epoch: 23 [24960/50048]	Loss: 1.4686
Training Epoch: 23 [25088/50048]	Loss: 1.1937
Training Epoch: 23 [25216/50048]	Loss: 1.1811
Training Epoch: 23 [25344/50048]	Loss: 1.5227
Training Epoch: 23 [25472/50048]	Loss: 1.1667
Training Epoch: 23 [25600/50048]	Loss: 1.2067
Training Epoch: 23 [25728/50048]	Loss: 1.3400
Training Epoch: 23 [25856/50048]	Loss: 1.2911
Training Epoch: 23 [25984/50048]	Loss: 1.2856
Training Epoch: 23 [26112/50048]	Loss: 1.1400
Training Epoch: 23 [26240/50048]	Loss: 1.2228
Training Epoch: 23 [26368/50048]	Loss: 1.1489
Training Epoch: 23 [26496/50048]	Loss: 1.2301
Training Epoch: 23 [26624/50048]	Loss: 1.4010
Training Epoch: 23 [26752/50048]	Loss: 1.2821
Training Epoch: 23 [26880/50048]	Loss: 1.2179
Training Epoch: 23 [27008/50048]	Loss: 1.2911
Training Epoch: 23 [27136/50048]	Loss: 1.6351
Training Epoch: 23 [27264/50048]	Loss: 1.3627
Training Epoch: 23 [27392/50048]	Loss: 1.1961
Training Epoch: 23 [27520/50048]	Loss: 1.2192
Training Epoch: 23 [27648/50048]	Loss: 1.2570
Training Epoch: 23 [27776/50048]	Loss: 1.1048
Training Epoch: 23 [27904/50048]	Loss: 1.3509
Training Epoch: 23 [28032/50048]	Loss: 1.3013
Training Epoch: 23 [28160/50048]	Loss: 1.3545
Training Epoch: 23 [28288/50048]	Loss: 1.4697
Training Epoch: 23 [28416/50048]	Loss: 1.2503
Training Epoch: 23 [28544/50048]	Loss: 1.1672
Training Epoch: 23 [28672/50048]	Loss: 1.4300
Training Epoch: 23 [28800/50048]	Loss: 1.2798
Training Epoch: 23 [28928/50048]	Loss: 1.0816
Training Epoch: 23 [29056/50048]	Loss: 1.1073
Training Epoch: 23 [29184/50048]	Loss: 1.3199
Training Epoch: 23 [29312/50048]	Loss: 1.3303
Training Epoch: 23 [29440/50048]	Loss: 1.2060
Training Epoch: 23 [29568/50048]	Loss: 1.1466
Training Epoch: 23 [29696/50048]	Loss: 1.0653
Training Epoch: 23 [29824/50048]	Loss: 1.1968
Training Epoch: 23 [29952/50048]	Loss: 1.0783
Training Epoch: 23 [30080/50048]	Loss: 1.1706
Training Epoch: 23 [30208/50048]	Loss: 1.2065
Training Epoch: 23 [30336/50048]	Loss: 1.0135
Training Epoch: 23 [30464/50048]	Loss: 1.5689
Training Epoch: 23 [30592/50048]	Loss: 1.3524
Training Epoch: 23 [30720/50048]	Loss: 1.1326
Training Epoch: 23 [30848/50048]	Loss: 1.3529
Training Epoch: 23 [30976/50048]	Loss: 1.2439
Training Epoch: 23 [31104/50048]	Loss: 1.3671
Training Epoch: 23 [31232/50048]	Loss: 1.3629
Training Epoch: 23 [31360/50048]	Loss: 1.3028
Training Epoch: 23 [31488/50048]	Loss: 0.9741
Training Epoch: 23 [31616/50048]	Loss: 1.4998
Training Epoch: 23 [31744/50048]	Loss: 1.1225
Training Epoch: 23 [31872/50048]	Loss: 1.0502
Training Epoch: 23 [32000/50048]	Loss: 0.9861
Training Epoch: 23 [32128/50048]	Loss: 1.3437
Training Epoch: 23 [32256/50048]	Loss: 1.3579
Training Epoch: 23 [32384/50048]	Loss: 1.3172
Training Epoch: 23 [32512/50048]	Loss: 1.1074
Training Epoch: 23 [32640/50048]	Loss: 1.2112
Training Epoch: 23 [32768/50048]	Loss: 1.0668
Training Epoch: 23 [32896/50048]	Loss: 1.0581
Training Epoch: 23 [33024/50048]	Loss: 1.2037
Training Epoch: 23 [33152/50048]	Loss: 1.3375
Training Epoch: 23 [33280/50048]	Loss: 1.4361
Training Epoch: 23 [33408/50048]	Loss: 1.5312
Training Epoch: 23 [33536/50048]	Loss: 1.1165
Training Epoch: 23 [33664/50048]	Loss: 1.2855
Training Epoch: 23 [33792/50048]	Loss: 0.9903
Training Epoch: 23 [33920/50048]	Loss: 1.2803
Training Epoch: 23 [34048/50048]	Loss: 1.4674
Training Epoch: 23 [34176/50048]	Loss: 1.2103
Training Epoch: 23 [34304/50048]	Loss: 1.2829
Training Epoch: 23 [34432/50048]	Loss: 1.1667
Training Epoch: 23 [34560/50048]	Loss: 1.1935
Training Epoch: 23 [34688/50048]	Loss: 1.2809
Training Epoch: 23 [34816/50048]	Loss: 1.1126
Training Epoch: 23 [34944/50048]	Loss: 1.7549
Training Epoch: 23 [35072/50048]	Loss: 1.2150
Training Epoch: 23 [35200/50048]	Loss: 1.4659
Training Epoch: 23 [35328/50048]	Loss: 1.3093
Training Epoch: 23 [35456/50048]	Loss: 1.2244
Training Epoch: 23 [35584/50048]	Loss: 1.5066
Training Epoch: 23 [35712/50048]	Loss: 1.3286
Training Epoch: 23 [35840/50048]	Loss: 1.1712
Training Epoch: 23 [35968/50048]	Loss: 1.3530
Training Epoch: 23 [36096/50048]	Loss: 1.3850
Training Epoch: 23 [36224/50048]	Loss: 1.4611
Training Epoch: 23 [36352/50048]	Loss: 1.1291
Training Epoch: 23 [36480/50048]	Loss: 1.4005
Training Epoch: 23 [36608/50048]	Loss: 1.1397
Training Epoch: 23 [36736/50048]	Loss: 1.1982
Training Epoch: 23 [36864/50048]	Loss: 1.1767
Training Epoch: 23 [36992/50048]	Loss: 1.2585
Training Epoch: 23 [37120/50048]	Loss: 1.4897
Training Epoch: 23 [37248/50048]	Loss: 1.1833
Training Epoch: 23 [37376/50048]	Loss: 1.2903
Training Epoch: 23 [37504/50048]	Loss: 1.3559
Training Epoch: 23 [37632/50048]	Loss: 1.0772
Training Epoch: 23 [37760/50048]	Loss: 1.2509
Training Epoch: 23 [37888/50048]	Loss: 1.1963
Training Epoch: 23 [38016/50048]	Loss: 1.2264
Training Epoch: 23 [38144/50048]	Loss: 1.1957
Training Epoch: 23 [38272/50048]	Loss: 1.4230
Training Epoch: 23 [38400/50048]	Loss: 1.2603
Training Epoch: 23 [38528/50048]	Loss: 1.2576
Training Epoch: 23 [38656/50048]	Loss: 1.3234
Training Epoch: 23 [38784/50048]	Loss: 1.2357
Training Epoch: 23 [38912/50048]	Loss: 1.2541
Training Epoch: 23 [39040/50048]	Loss: 1.3425
Training Epoch: 23 [39168/50048]	Loss: 1.1476
Training Epoch: 23 [39296/50048]	Loss: 0.9847
Training Epoch: 23 [39424/50048]	Loss: 1.4726
Training Epoch: 23 [39552/50048]	Loss: 1.4577
Training Epoch: 23 [39680/50048]	Loss: 1.3554
Training Epoch: 23 [39808/50048]	Loss: 1.2861
Training Epoch: 23 [39936/50048]	Loss: 1.3199
Training Epoch: 23 [40064/50048]	Loss: 1.4427
Training Epoch: 23 [40192/50048]	Loss: 1.1734
Training Epoch: 23 [40320/50048]	Loss: 1.1798
Training Epoch: 23 [40448/50048]	Loss: 1.3277
Training Epoch: 23 [40576/50048]	Loss: 1.1235
Training Epoch: 23 [40704/50048]	Loss: 1.2791
Training Epoch: 23 [40832/50048]	Loss: 1.3665
Training Epoch: 23 [40960/50048]	Loss: 1.2490
Training Epoch: 23 [41088/50048]	Loss: 1.5133
Training Epoch: 23 [41216/50048]	Loss: 1.2382
Training Epoch: 23 [41344/50048]	Loss: 1.1824
Training Epoch: 23 [41472/50048]	Loss: 1.1457
Training Epoch: 23 [41600/50048]	Loss: 1.3022
Training Epoch: 23 [41728/50048]	Loss: 1.2013
Training Epoch: 23 [41856/50048]	Loss: 1.1110
Training Epoch: 23 [41984/50048]	Loss: 1.3551
Training Epoch: 23 [42112/50048]	Loss: 1.1086
Training Epoch: 23 [42240/50048]	Loss: 1.0638
Training Epoch: 23 [42368/50048]	Loss: 1.2984
Training Epoch: 23 [42496/50048]	Loss: 1.4464
Training Epoch: 23 [42624/50048]	Loss: 1.0105
Training Epoch: 23 [42752/50048]	Loss: 1.1402
Training Epoch: 23 [42880/50048]	Loss: 1.2333
Training Epoch: 23 [43008/50048]	Loss: 1.1756
Training Epoch: 23 [43136/50048]	Loss: 1.2921
Training Epoch: 23 [43264/50048]	Loss: 1.3380
Training Epoch: 23 [43392/50048]	Loss: 1.4007
Training Epoch: 23 [43520/50048]	Loss: 1.2924
Training Epoch: 23 [43648/50048]	Loss: 1.1069
Training Epoch: 23 [43776/50048]	Loss: 1.3977
Training Epoch: 23 [43904/50048]	Loss: 1.3435
Training Epoch: 23 [44032/50048]	Loss: 1.5060
Training Epoch: 23 [44160/50048]	Loss: 1.1174
Training Epoch: 23 [44288/50048]	Loss: 1.3708
Training Epoch: 23 [44416/50048]	Loss: 1.3516
Training Epoch: 23 [44544/50048]	Loss: 1.1620
Training Epoch: 23 [44672/50048]	Loss: 1.4253
Training Epoch: 23 [44800/50048]	Loss: 1.4664
Training Epoch: 23 [44928/50048]	Loss: 1.2932
Training Epoch: 23 [45056/50048]	Loss: 1.1975
Training Epoch: 23 [45184/50048]	Loss: 1.1506
Training Epoch: 23 [45312/50048]	Loss: 1.2110
Training Epoch: 23 [45440/50048]	Loss: 1.1885
Training Epoch: 23 [45568/50048]	Loss: 1.3758
Training Epoch: 23 [45696/50048]	Loss: 1.2916
Training Epoch: 23 [45824/50048]	Loss: 1.2592
Training Epoch: 23 [45952/50048]	Loss: 1.0363
Training Epoch: 23 [46080/50048]	Loss: 1.2795
Training Epoch: 23 [46208/50048]	Loss: 1.4499
Training Epoch: 23 [46336/50048]	Loss: 1.4544
Training Epoch: 23 [46464/50048]	Loss: 1.0295
Training Epoch: 23 [46592/50048]	Loss: 1.0715
Training Epoch: 23 [46720/50048]	Loss: 1.2183
Training Epoch: 23 [46848/50048]	Loss: 1.1696
Training Epoch: 23 [46976/50048]	Loss: 1.1511
Training Epoch: 23 [47104/50048]	Loss: 1.0351
Training Epoch: 23 [47232/50048]	Loss: 1.3965
Training Epoch: 23 [47360/50048]	Loss: 1.1045
Training Epoch: 23 [47488/50048]	Loss: 1.2023
Training Epoch: 23 [47616/50048]	Loss: 1.1258
Training Epoch: 23 [47744/50048]	Loss: 1.3901
Training Epoch: 23 [47872/50048]	Loss: 1.1265
Training Epoch: 23 [48000/50048]	Loss: 1.4080
Training Epoch: 23 [48128/50048]	Loss: 1.2438
Training Epoch: 23 [48256/50048]	Loss: 1.1214
Training Epoch: 23 [48384/50048]	Loss: 1.2459
Training Epoch: 23 [48512/50048]	Loss: 1.3366
Training Epoch: 23 [48640/50048]	Loss: 1.4506
Training Epoch: 23 [48768/50048]	Loss: 1.1704
Training Epoch: 23 [48896/50048]	Loss: 1.0315
Training Epoch: 23 [49024/50048]	Loss: 1.2262
Training Epoch: 23 [49152/50048]	Loss: 1.1219
Training Epoch: 23 [49280/50048]	Loss: 1.3767
Training Epoch: 23 [49408/50048]	Loss: 1.2062
Training Epoch: 23 [49536/50048]	Loss: 1.2058
Training Epoch: 23 [49664/50048]	Loss: 1.0867
Training Epoch: 23 [49792/50048]	Loss: 1.3890
Training Epoch: 23 [49920/50048]	Loss: 1.3087
Training Epoch: 23 [50048/50048]	Loss: 1.1805
Validation Epoch: 23, Average loss: 0.0126, Accuracy: 0.5701
Training Epoch: 24 [128/50048]	Loss: 1.0700
Training Epoch: 24 [256/50048]	Loss: 1.4402
Training Epoch: 24 [384/50048]	Loss: 1.1623
Training Epoch: 24 [512/50048]	Loss: 1.3037
Training Epoch: 24 [640/50048]	Loss: 1.2523
Training Epoch: 24 [768/50048]	Loss: 1.1967
Training Epoch: 24 [896/50048]	Loss: 1.0729
Training Epoch: 24 [1024/50048]	Loss: 1.0011
Training Epoch: 24 [1152/50048]	Loss: 1.0447
Training Epoch: 24 [1280/50048]	Loss: 1.2736
Training Epoch: 24 [1408/50048]	Loss: 1.2833
Training Epoch: 24 [1536/50048]	Loss: 1.1996
Training Epoch: 24 [1664/50048]	Loss: 0.9967
Training Epoch: 24 [1792/50048]	Loss: 1.2419
Training Epoch: 24 [1920/50048]	Loss: 1.0382
Training Epoch: 24 [2048/50048]	Loss: 1.0409
Training Epoch: 24 [2176/50048]	Loss: 1.1856
Training Epoch: 24 [2304/50048]	Loss: 1.2043
Training Epoch: 24 [2432/50048]	Loss: 1.1920
Training Epoch: 24 [2560/50048]	Loss: 1.2832
Training Epoch: 24 [2688/50048]	Loss: 1.1370
Training Epoch: 24 [2816/50048]	Loss: 1.1929
Training Epoch: 24 [2944/50048]	Loss: 1.0301
Training Epoch: 24 [3072/50048]	Loss: 0.9980
Training Epoch: 24 [3200/50048]	Loss: 1.2882
Training Epoch: 24 [3328/50048]	Loss: 0.9945
Training Epoch: 24 [3456/50048]	Loss: 1.0958
Training Epoch: 24 [3584/50048]	Loss: 1.3819
Training Epoch: 24 [3712/50048]	Loss: 0.9957
Training Epoch: 24 [3840/50048]	Loss: 1.1835
Training Epoch: 24 [3968/50048]	Loss: 1.2175
Training Epoch: 24 [4096/50048]	Loss: 1.1273
Training Epoch: 24 [4224/50048]	Loss: 1.3469
Training Epoch: 24 [4352/50048]	Loss: 1.2874
Training Epoch: 24 [4480/50048]	Loss: 1.2914
Training Epoch: 24 [4608/50048]	Loss: 1.1898
Training Epoch: 24 [4736/50048]	Loss: 1.1855
Training Epoch: 24 [4864/50048]	Loss: 1.3204
Training Epoch: 24 [4992/50048]	Loss: 1.2989
Training Epoch: 24 [5120/50048]	Loss: 1.2036
Training Epoch: 24 [5248/50048]	Loss: 1.1909
Training Epoch: 24 [5376/50048]	Loss: 0.8999
Training Epoch: 24 [5504/50048]	Loss: 1.3414
Training Epoch: 24 [5632/50048]	Loss: 1.0587
Training Epoch: 24 [5760/50048]	Loss: 1.1892
Training Epoch: 24 [5888/50048]	Loss: 1.1467
Training Epoch: 24 [6016/50048]	Loss: 1.3736
Training Epoch: 24 [6144/50048]	Loss: 1.2031
Training Epoch: 24 [6272/50048]	Loss: 1.3597
Training Epoch: 24 [6400/50048]	Loss: 1.3068
Training Epoch: 24 [6528/50048]	Loss: 1.0971
Training Epoch: 24 [6656/50048]	Loss: 1.1616
Training Epoch: 24 [6784/50048]	Loss: 1.1723
Training Epoch: 24 [6912/50048]	Loss: 1.2489
Training Epoch: 24 [7040/50048]	Loss: 1.1870
Training Epoch: 24 [7168/50048]	Loss: 1.1097
Training Epoch: 24 [7296/50048]	Loss: 1.3071
Training Epoch: 24 [7424/50048]	Loss: 1.0929
Training Epoch: 24 [7552/50048]	Loss: 1.1234
Training Epoch: 24 [7680/50048]	Loss: 1.3121
Training Epoch: 24 [7808/50048]	Loss: 1.2124
Training Epoch: 24 [7936/50048]	Loss: 1.1763
Training Epoch: 24 [8064/50048]	Loss: 1.2965
Training Epoch: 24 [8192/50048]	Loss: 1.2749
Training Epoch: 24 [8320/50048]	Loss: 1.4329
Training Epoch: 24 [8448/50048]	Loss: 1.2543
Training Epoch: 24 [8576/50048]	Loss: 1.2296
Training Epoch: 24 [8704/50048]	Loss: 1.1456
Training Epoch: 24 [8832/50048]	Loss: 1.2943
Training Epoch: 24 [8960/50048]	Loss: 1.1761
Training Epoch: 24 [9088/50048]	Loss: 1.1463
Training Epoch: 24 [9216/50048]	Loss: 1.2221
Training Epoch: 24 [9344/50048]	Loss: 1.1053
Training Epoch: 24 [9472/50048]	Loss: 1.0502
Training Epoch: 24 [9600/50048]	Loss: 1.2559
Training Epoch: 24 [9728/50048]	Loss: 1.2973
Training Epoch: 24 [9856/50048]	Loss: 1.1183
Training Epoch: 24 [9984/50048]	Loss: 1.2848
Training Epoch: 24 [10112/50048]	Loss: 1.1949
Training Epoch: 24 [10240/50048]	Loss: 1.2395
Training Epoch: 24 [10368/50048]	Loss: 1.1560
Training Epoch: 24 [10496/50048]	Loss: 1.0779
Training Epoch: 24 [10624/50048]	Loss: 1.2154
Training Epoch: 24 [10752/50048]	Loss: 1.1473
Training Epoch: 24 [10880/50048]	Loss: 1.1804
Training Epoch: 24 [11008/50048]	Loss: 1.2489
Training Epoch: 24 [11136/50048]	Loss: 1.0762
Training Epoch: 24 [11264/50048]	Loss: 1.3478
Training Epoch: 24 [11392/50048]	Loss: 1.1888
Training Epoch: 24 [11520/50048]	Loss: 1.1574
Training Epoch: 24 [11648/50048]	Loss: 1.1197
Training Epoch: 24 [11776/50048]	Loss: 1.2677
Training Epoch: 24 [11904/50048]	Loss: 1.1859
Training Epoch: 24 [12032/50048]	Loss: 1.2441
Training Epoch: 24 [12160/50048]	Loss: 1.1183
Training Epoch: 24 [12288/50048]	Loss: 1.2598
Training Epoch: 24 [12416/50048]	Loss: 1.0965
Training Epoch: 24 [12544/50048]	Loss: 1.2389
Training Epoch: 24 [12672/50048]	Loss: 1.0187
Training Epoch: 24 [12800/50048]	Loss: 1.2289
Training Epoch: 24 [12928/50048]	Loss: 1.0597
Training Epoch: 24 [13056/50048]	Loss: 1.3907
Training Epoch: 24 [13184/50048]	Loss: 1.1671
Training Epoch: 24 [13312/50048]	Loss: 1.0359
Training Epoch: 24 [13440/50048]	Loss: 1.2046
Training Epoch: 24 [13568/50048]	Loss: 1.1886
Training Epoch: 24 [13696/50048]	Loss: 1.1074
Training Epoch: 24 [13824/50048]	Loss: 1.3201
Training Epoch: 24 [13952/50048]	Loss: 1.1582
Training Epoch: 24 [14080/50048]	Loss: 1.0548
Training Epoch: 24 [14208/50048]	Loss: 0.8707
Training Epoch: 24 [14336/50048]	Loss: 0.9577
Training Epoch: 24 [14464/50048]	Loss: 1.1526
Training Epoch: 24 [14592/50048]	Loss: 1.1022
Training Epoch: 24 [14720/50048]	Loss: 1.1747
Training Epoch: 24 [14848/50048]	Loss: 1.1397
Training Epoch: 24 [14976/50048]	Loss: 1.2075
Training Epoch: 24 [15104/50048]	Loss: 1.3402
Training Epoch: 24 [15232/50048]	Loss: 1.5238
Training Epoch: 24 [15360/50048]	Loss: 1.2482
Training Epoch: 24 [15488/50048]	Loss: 1.3652
Training Epoch: 24 [15616/50048]	Loss: 1.1899
Training Epoch: 24 [15744/50048]	Loss: 1.1810
Training Epoch: 24 [15872/50048]	Loss: 1.0916
Training Epoch: 24 [16000/50048]	Loss: 1.1187
Training Epoch: 24 [16128/50048]	Loss: 1.3262
Training Epoch: 24 [16256/50048]	Loss: 1.0977
Training Epoch: 24 [16384/50048]	Loss: 1.2224
Training Epoch: 24 [16512/50048]	Loss: 1.1451
Training Epoch: 24 [16640/50048]	Loss: 1.0502
Training Epoch: 24 [16768/50048]	Loss: 1.3686
Training Epoch: 24 [16896/50048]	Loss: 1.1798
Training Epoch: 24 [17024/50048]	Loss: 1.3468
Training Epoch: 24 [17152/50048]	Loss: 1.3248
Training Epoch: 24 [17280/50048]	Loss: 1.3993
Training Epoch: 24 [17408/50048]	Loss: 1.1706
Training Epoch: 24 [17536/50048]	Loss: 1.2468
Training Epoch: 24 [17664/50048]	Loss: 1.1137
Training Epoch: 24 [17792/50048]	Loss: 1.1750
Training Epoch: 24 [17920/50048]	Loss: 1.3297
Training Epoch: 24 [18048/50048]	Loss: 1.2591
Training Epoch: 24 [18176/50048]	Loss: 1.1993
Training Epoch: 24 [18304/50048]	Loss: 1.1417
Training Epoch: 24 [18432/50048]	Loss: 1.1819
Training Epoch: 24 [18560/50048]	Loss: 1.1972
Training Epoch: 24 [18688/50048]	Loss: 1.4617
Training Epoch: 24 [18816/50048]	Loss: 1.0671
Training Epoch: 24 [18944/50048]	Loss: 1.4318
Training Epoch: 24 [19072/50048]	Loss: 1.0751
Training Epoch: 24 [19200/50048]	Loss: 1.2327
Training Epoch: 24 [19328/50048]	Loss: 1.1899
Training Epoch: 24 [19456/50048]	Loss: 0.9980
Training Epoch: 24 [19584/50048]	Loss: 1.0188
Training Epoch: 24 [19712/50048]	Loss: 1.2322
Training Epoch: 24 [19840/50048]	Loss: 1.1733
Training Epoch: 24 [19968/50048]	Loss: 1.1623
Training Epoch: 24 [20096/50048]	Loss: 1.1775
Training Epoch: 24 [20224/50048]	Loss: 1.1536
Training Epoch: 24 [20352/50048]	Loss: 1.1657
Training Epoch: 24 [20480/50048]	Loss: 1.4466
Training Epoch: 24 [20608/50048]	Loss: 1.2917
Training Epoch: 24 [20736/50048]	Loss: 1.3474
Training Epoch: 24 [20864/50048]	Loss: 1.1236
Training Epoch: 24 [20992/50048]	Loss: 1.1010
Training Epoch: 24 [21120/50048]	Loss: 1.1827
Training Epoch: 24 [21248/50048]	Loss: 1.4303
Training Epoch: 24 [21376/50048]	Loss: 1.4450
Training Epoch: 24 [21504/50048]	Loss: 1.4294
Training Epoch: 24 [21632/50048]	Loss: 1.2924
Training Epoch: 24 [21760/50048]	Loss: 1.1288
Training Epoch: 24 [21888/50048]	Loss: 1.1651
Training Epoch: 24 [22016/50048]	Loss: 1.2394
Training Epoch: 24 [22144/50048]	Loss: 1.3678
Training Epoch: 24 [22272/50048]	Loss: 0.9970
Training Epoch: 24 [22400/50048]	Loss: 1.5610
Training Epoch: 24 [22528/50048]	Loss: 1.2520
Training Epoch: 24 [22656/50048]	Loss: 1.4951
Training Epoch: 24 [22784/50048]	Loss: 0.9870
Training Epoch: 24 [22912/50048]	Loss: 1.2946
Training Epoch: 24 [23040/50048]	Loss: 1.2032
Training Epoch: 24 [23168/50048]	Loss: 1.1190
Training Epoch: 24 [23296/50048]	Loss: 0.9416
Training Epoch: 24 [23424/50048]	Loss: 1.0958
Training Epoch: 24 [23552/50048]	Loss: 1.0744
Training Epoch: 24 [23680/50048]	Loss: 1.2010
Training Epoch: 24 [23808/50048]	Loss: 1.0138
Training Epoch: 24 [23936/50048]	Loss: 1.1052
Training Epoch: 24 [24064/50048]	Loss: 1.2361
Training Epoch: 24 [24192/50048]	Loss: 1.4428
Training Epoch: 24 [24320/50048]	Loss: 1.2014
Training Epoch: 24 [24448/50048]	Loss: 1.4007
Training Epoch: 24 [24576/50048]	Loss: 1.1668
Training Epoch: 24 [24704/50048]	Loss: 1.1555
Training Epoch: 24 [24832/50048]	Loss: 1.0673
Training Epoch: 24 [24960/50048]	Loss: 1.1965
Training Epoch: 24 [25088/50048]	Loss: 1.0119
Training Epoch: 24 [25216/50048]	Loss: 1.0839
Training Epoch: 24 [25344/50048]	Loss: 1.2001
Training Epoch: 24 [25472/50048]	Loss: 1.3621
Training Epoch: 24 [25600/50048]	Loss: 1.1680
Training Epoch: 24 [25728/50048]	Loss: 1.1073
Training Epoch: 24 [25856/50048]	Loss: 1.2413
Training Epoch: 24 [25984/50048]	Loss: 1.0452
Training Epoch: 24 [26112/50048]	Loss: 1.1564
Training Epoch: 24 [26240/50048]	Loss: 1.0389
Training Epoch: 24 [26368/50048]	Loss: 1.2595
Training Epoch: 24 [26496/50048]	Loss: 1.1915
Training Epoch: 24 [26624/50048]	Loss: 1.3226
Training Epoch: 24 [26752/50048]	Loss: 1.1204
Training Epoch: 24 [26880/50048]	Loss: 1.2490
Training Epoch: 24 [27008/50048]	Loss: 1.3707
Training Epoch: 24 [27136/50048]	Loss: 1.3147
Training Epoch: 24 [27264/50048]	Loss: 1.3595
Training Epoch: 24 [27392/50048]	Loss: 1.2684
Training Epoch: 24 [27520/50048]	Loss: 1.1985
Training Epoch: 24 [27648/50048]	Loss: 1.3098
Training Epoch: 24 [27776/50048]	Loss: 1.3134
Training Epoch: 24 [27904/50048]	Loss: 0.9554
Training Epoch: 24 [28032/50048]	Loss: 1.2174
Training Epoch: 24 [28160/50048]	Loss: 1.2738
Training Epoch: 24 [28288/50048]	Loss: 1.0143
Training Epoch: 24 [28416/50048]	Loss: 1.1409
Training Epoch: 24 [28544/50048]	Loss: 1.2372
Training Epoch: 24 [28672/50048]	Loss: 1.2101
Training Epoch: 24 [28800/50048]	Loss: 1.1839
Training Epoch: 24 [28928/50048]	Loss: 1.5887
Training Epoch: 24 [29056/50048]	Loss: 1.0181
Training Epoch: 24 [29184/50048]	Loss: 1.1242
Training Epoch: 24 [29312/50048]	Loss: 1.2063
Training Epoch: 24 [29440/50048]	Loss: 1.0383
Training Epoch: 24 [29568/50048]	Loss: 1.2294
Training Epoch: 24 [29696/50048]	Loss: 1.2877
Training Epoch: 24 [29824/50048]	Loss: 1.1621
Training Epoch: 24 [29952/50048]	Loss: 1.1732
Training Epoch: 24 [30080/50048]	Loss: 1.0812
Training Epoch: 24 [30208/50048]	Loss: 1.6170
Training Epoch: 24 [30336/50048]	Loss: 1.2126
Training Epoch: 24 [30464/50048]	Loss: 0.9961
Training Epoch: 24 [30592/50048]	Loss: 1.3340
Training Epoch: 24 [30720/50048]	Loss: 1.2625
Training Epoch: 24 [30848/50048]	Loss: 1.0772
Training Epoch: 24 [30976/50048]	Loss: 1.1540
Training Epoch: 24 [31104/50048]	Loss: 1.2813
Training Epoch: 24 [31232/50048]	Loss: 1.1003
Training Epoch: 24 [31360/50048]	Loss: 1.1203
Training Epoch: 24 [31488/50048]	Loss: 1.6133
Training Epoch: 24 [31616/50048]	Loss: 1.1849
Training Epoch: 24 [31744/50048]	Loss: 1.2851
Training Epoch: 24 [31872/50048]	Loss: 1.1681
Training Epoch: 24 [32000/50048]	Loss: 1.5489
Training Epoch: 24 [32128/50048]	Loss: 0.9798
Training Epoch: 24 [32256/50048]	Loss: 1.5033
Training Epoch: 24 [32384/50048]	Loss: 1.2391
Training Epoch: 24 [32512/50048]	Loss: 1.0665
Training Epoch: 24 [32640/50048]	Loss: 1.0977
Training Epoch: 24 [32768/50048]	Loss: 1.3276
Training Epoch: 24 [32896/50048]	Loss: 1.2618
Training Epoch: 24 [33024/50048]	Loss: 1.3595
Training Epoch: 24 [33152/50048]	Loss: 1.0530
Training Epoch: 24 [33280/50048]	Loss: 1.4317
Training Epoch: 24 [33408/50048]	Loss: 1.2942
Training Epoch: 24 [33536/50048]	Loss: 1.0733
Training Epoch: 24 [33664/50048]	Loss: 1.5521
Training Epoch: 24 [33792/50048]	Loss: 1.2696
Training Epoch: 24 [33920/50048]	Loss: 1.2614
Training Epoch: 24 [34048/50048]	Loss: 0.9988
Training Epoch: 24 [34176/50048]	Loss: 1.1539
Training Epoch: 24 [34304/50048]	Loss: 1.5013
Training Epoch: 24 [34432/50048]	Loss: 1.1162
Training Epoch: 24 [34560/50048]	Loss: 1.1083
Training Epoch: 24 [34688/50048]	Loss: 1.2168
Training Epoch: 24 [34816/50048]	Loss: 1.0583
Training Epoch: 24 [34944/50048]	Loss: 1.1409
Training Epoch: 24 [35072/50048]	Loss: 1.2658
Training Epoch: 24 [35200/50048]	Loss: 1.3308
Training Epoch: 24 [35328/50048]	Loss: 0.9557
Training Epoch: 24 [35456/50048]	Loss: 1.1588
Training Epoch: 24 [35584/50048]	Loss: 1.3871
Training Epoch: 24 [35712/50048]	Loss: 1.3360
Training Epoch: 24 [35840/50048]	Loss: 1.0711
Training Epoch: 24 [35968/50048]	Loss: 1.1040
Training Epoch: 24 [36096/50048]	Loss: 1.2727
Training Epoch: 24 [36224/50048]	Loss: 1.3078
Training Epoch: 24 [36352/50048]	Loss: 1.2120
Training Epoch: 24 [36480/50048]	Loss: 1.2301
Training Epoch: 24 [36608/50048]	Loss: 1.0543
Training Epoch: 24 [36736/50048]	Loss: 1.3160
Training Epoch: 24 [36864/50048]	Loss: 1.0562
Training Epoch: 24 [36992/50048]	Loss: 1.0673
Training Epoch: 24 [37120/50048]	Loss: 1.3285
Training Epoch: 24 [37248/50048]	Loss: 1.2143
Training Epoch: 24 [37376/50048]	Loss: 1.3141
Training Epoch: 24 [37504/50048]	Loss: 1.1415
Training Epoch: 24 [37632/50048]	Loss: 1.4393
Training Epoch: 24 [37760/50048]	Loss: 0.9696
Training Epoch: 24 [37888/50048]	Loss: 1.2830
Training Epoch: 24 [38016/50048]	Loss: 1.1351
Training Epoch: 24 [38144/50048]	Loss: 1.2480
Training Epoch: 24 [38272/50048]	Loss: 1.5809
Training Epoch: 24 [38400/50048]	Loss: 1.1637
Training Epoch: 24 [38528/50048]	Loss: 1.1407
Training Epoch: 24 [38656/50048]	Loss: 1.2308
Training Epoch: 24 [38784/50048]	Loss: 1.2665
Training Epoch: 24 [38912/50048]	Loss: 1.2875
Training Epoch: 24 [39040/50048]	Loss: 1.5278
Training Epoch: 24 [39168/50048]	Loss: 1.1686
Training Epoch: 24 [39296/50048]	Loss: 1.1676
Training Epoch: 24 [39424/50048]	Loss: 1.0303
Training Epoch: 24 [39552/50048]	Loss: 1.2384
Training Epoch: 24 [39680/50048]	Loss: 1.2354
Training Epoch: 24 [39808/50048]	Loss: 1.2213
Training Epoch: 24 [39936/50048]	Loss: 1.3682
Training Epoch: 24 [40064/50048]	Loss: 1.1625
Training Epoch: 24 [40192/50048]	Loss: 1.2065
Training Epoch: 24 [40320/50048]	Loss: 1.1912
Training Epoch: 24 [40448/50048]	Loss: 1.2101
Training Epoch: 24 [40576/50048]	Loss: 1.3042
Training Epoch: 24 [40704/50048]	Loss: 1.2800
Training Epoch: 24 [40832/50048]	Loss: 1.1109
Training Epoch: 24 [40960/50048]	Loss: 1.1809
Training Epoch: 24 [41088/50048]	Loss: 1.3304
Training Epoch: 24 [41216/50048]	Loss: 1.3011
Training Epoch: 24 [41344/50048]	Loss: 1.1156
Training Epoch: 24 [41472/50048]	Loss: 1.5353
Training Epoch: 24 [41600/50048]	Loss: 1.1442
Training Epoch: 24 [41728/50048]	Loss: 1.1222
Training Epoch: 24 [41856/50048]	Loss: 1.5029
Training Epoch: 24 [41984/50048]	Loss: 1.4630
Training Epoch: 24 [42112/50048]	Loss: 0.9787
Training Epoch: 24 [42240/50048]	Loss: 1.1341
Training Epoch: 24 [42368/50048]	Loss: 1.1934
Training Epoch: 24 [42496/50048]	Loss: 1.0784
Training Epoch: 24 [42624/50048]	Loss: 1.2140
Training Epoch: 24 [42752/50048]	Loss: 1.3011
Training Epoch: 24 [42880/50048]	Loss: 1.1843
Training Epoch: 24 [43008/50048]	Loss: 1.1307
Training Epoch: 24 [43136/50048]	Loss: 1.4670
Training Epoch: 24 [43264/50048]	Loss: 1.1175
Training Epoch: 24 [43392/50048]	Loss: 1.1872
Training Epoch: 24 [43520/50048]	Loss: 1.2679
Training Epoch: 24 [43648/50048]	Loss: 1.1839
Training Epoch: 24 [43776/50048]	Loss: 1.1849
Training Epoch: 24 [43904/50048]	Loss: 1.3750
Training Epoch: 24 [44032/50048]	Loss: 1.2338
Training Epoch: 24 [44160/50048]	Loss: 1.1609
Training Epoch: 24 [44288/50048]	Loss: 1.4012
Training Epoch: 24 [44416/50048]	Loss: 1.2356
Training Epoch: 24 [44544/50048]	Loss: 1.2333
Training Epoch: 24 [44672/50048]	Loss: 1.1084
Training Epoch: 24 [44800/50048]	Loss: 1.3373
Training Epoch: 24 [44928/50048]	Loss: 1.1134
Training Epoch: 24 [45056/50048]	Loss: 1.2483
Training Epoch: 24 [45184/50048]	Loss: 1.1081
Training Epoch: 24 [45312/50048]	Loss: 1.4024
Training Epoch: 24 [45440/50048]	Loss: 1.1700
Training Epoch: 24 [45568/50048]	Loss: 1.2339
Training Epoch: 24 [45696/50048]	Loss: 1.0135
Training Epoch: 24 [45824/50048]	Loss: 1.2690
Training Epoch: 24 [45952/50048]	Loss: 1.3042
Training Epoch: 24 [46080/50048]	Loss: 1.0254
Training Epoch: 24 [46208/50048]	Loss: 1.1834
Training Epoch: 24 [46336/50048]	Loss: 1.1436
Training Epoch: 24 [46464/50048]	Loss: 1.0418
Training Epoch: 24 [46592/50048]	Loss: 1.1465
Training Epoch: 24 [46720/50048]	Loss: 1.1085
Training Epoch: 24 [46848/50048]	Loss: 1.3609
Training Epoch: 24 [46976/50048]	Loss: 1.1520
Training Epoch: 24 [47104/50048]	Loss: 1.1686
Training Epoch: 24 [47232/50048]	Loss: 1.2617
Training Epoch: 24 [47360/50048]	Loss: 1.0335
Training Epoch: 24 [47488/50048]	Loss: 1.1071
Training Epoch: 24 [47616/50048]	Loss: 1.3096
Training Epoch: 24 [47744/50048]	Loss: 1.2696
Training Epoch: 24 [47872/50048]	Loss: 1.3331
Training Epoch: 24 [48000/50048]	Loss: 1.3920
Training Epoch: 24 [48128/50048]	Loss: 1.2969
Training Epoch: 24 [48256/50048]	Loss: 1.1740
Training Epoch: 24 [48384/50048]	Loss: 1.3564
Training Epoch: 24 [48512/50048]	Loss: 1.0910
Training Epoch: 24 [48640/50048]	Loss: 1.3872
Training Epoch: 24 [48768/50048]	Loss: 1.3542
Training Epoch: 24 [48896/50048]	Loss: 1.2323
Training Epoch: 24 [49024/50048]	Loss: 1.2183
Training Epoch: 24 [49152/50048]	Loss: 1.0886
Training Epoch: 24 [49280/50048]	Loss: 1.1079
Training Epoch: 24 [49408/50048]	Loss: 1.3527
Training Epoch: 24 [49536/50048]	Loss: 1.3152
Training Epoch: 24 [49664/50048]	Loss: 0.9869
Training Epoch: 24 [49792/50048]	Loss: 1.2734
Training Epoch: 24 [49920/50048]	Loss: 1.0525
Training Epoch: 24 [50048/50048]	Loss: 1.0458
Validation Epoch: 24, Average loss: 0.0123, Accuracy: 0.5808
Training Epoch: 25 [128/50048]	Loss: 0.9703
Training Epoch: 25 [256/50048]	Loss: 0.9857
Training Epoch: 25 [384/50048]	Loss: 1.0830
Training Epoch: 25 [512/50048]	Loss: 0.9517
Training Epoch: 25 [640/50048]	Loss: 1.1531
Training Epoch: 25 [768/50048]	Loss: 1.1011
Training Epoch: 25 [896/50048]	Loss: 1.1214
Training Epoch: 25 [1024/50048]	Loss: 1.1512
Training Epoch: 25 [1152/50048]	Loss: 1.3176
Training Epoch: 25 [1280/50048]	Loss: 1.1548
Training Epoch: 25 [1408/50048]	Loss: 1.1121
Training Epoch: 25 [1536/50048]	Loss: 1.1999
Training Epoch: 25 [1664/50048]	Loss: 1.1531
Training Epoch: 25 [1792/50048]	Loss: 1.0891
Training Epoch: 25 [1920/50048]	Loss: 1.0953
Training Epoch: 25 [2048/50048]	Loss: 1.2138
Training Epoch: 25 [2176/50048]	Loss: 1.3022
Training Epoch: 25 [2304/50048]	Loss: 1.2057
Training Epoch: 25 [2432/50048]	Loss: 1.2403
Training Epoch: 25 [2560/50048]	Loss: 1.1982
Training Epoch: 25 [2688/50048]	Loss: 1.2561
Training Epoch: 25 [2816/50048]	Loss: 1.1053
Training Epoch: 25 [2944/50048]	Loss: 1.2168
Training Epoch: 25 [3072/50048]	Loss: 1.1781
Training Epoch: 25 [3200/50048]	Loss: 1.0137
Training Epoch: 25 [3328/50048]	Loss: 1.1986
Training Epoch: 25 [3456/50048]	Loss: 1.0121
Training Epoch: 25 [3584/50048]	Loss: 0.8918
Training Epoch: 25 [3712/50048]	Loss: 1.0259
Training Epoch: 25 [3840/50048]	Loss: 1.2574
Training Epoch: 25 [3968/50048]	Loss: 1.0543
Training Epoch: 25 [4096/50048]	Loss: 1.3635
Training Epoch: 25 [4224/50048]	Loss: 1.0713
Training Epoch: 25 [4352/50048]	Loss: 1.0717
Training Epoch: 25 [4480/50048]	Loss: 1.2968
Training Epoch: 25 [4608/50048]	Loss: 1.4410
Training Epoch: 25 [4736/50048]	Loss: 1.0100
Training Epoch: 25 [4864/50048]	Loss: 1.1435
Training Epoch: 25 [4992/50048]	Loss: 1.2852
Training Epoch: 25 [5120/50048]	Loss: 1.2351
Training Epoch: 25 [5248/50048]	Loss: 1.3167
Training Epoch: 25 [5376/50048]	Loss: 1.0516
Training Epoch: 25 [5504/50048]	Loss: 0.9512
Training Epoch: 25 [5632/50048]	Loss: 1.1760
Training Epoch: 25 [5760/50048]	Loss: 1.4425
Training Epoch: 25 [5888/50048]	Loss: 1.3555
Training Epoch: 25 [6016/50048]	Loss: 1.2085
Training Epoch: 25 [6144/50048]	Loss: 1.2189
Training Epoch: 25 [6272/50048]	Loss: 1.2508
Training Epoch: 25 [6400/50048]	Loss: 1.1875
Training Epoch: 25 [6528/50048]	Loss: 1.1361
Training Epoch: 25 [6656/50048]	Loss: 1.1697
Training Epoch: 25 [6784/50048]	Loss: 1.2283
Training Epoch: 25 [6912/50048]	Loss: 1.3681
Training Epoch: 25 [7040/50048]	Loss: 1.1615
Training Epoch: 25 [7168/50048]	Loss: 1.2958
Training Epoch: 25 [7296/50048]	Loss: 0.8812
Training Epoch: 25 [7424/50048]	Loss: 1.3012
Training Epoch: 25 [7552/50048]	Loss: 1.1537
Training Epoch: 25 [7680/50048]	Loss: 1.4427
Training Epoch: 25 [7808/50048]	Loss: 1.1897
Training Epoch: 25 [7936/50048]	Loss: 1.2223
Training Epoch: 25 [8064/50048]	Loss: 1.0413
Training Epoch: 25 [8192/50048]	Loss: 0.9645
Training Epoch: 25 [8320/50048]	Loss: 1.0940
Training Epoch: 25 [8448/50048]	Loss: 1.2483
Training Epoch: 25 [8576/50048]	Loss: 1.0369
Training Epoch: 25 [8704/50048]	Loss: 1.3206
Training Epoch: 25 [8832/50048]	Loss: 1.2008
Training Epoch: 25 [8960/50048]	Loss: 0.9398
Training Epoch: 25 [9088/50048]	Loss: 1.1653
Training Epoch: 25 [9216/50048]	Loss: 1.2668
Training Epoch: 25 [9344/50048]	Loss: 1.1336
Training Epoch: 25 [9472/50048]	Loss: 0.9665
Training Epoch: 25 [9600/50048]	Loss: 1.1290
Training Epoch: 25 [9728/50048]	Loss: 1.3558
Training Epoch: 25 [9856/50048]	Loss: 1.1948
Training Epoch: 25 [9984/50048]	Loss: 1.0167
Training Epoch: 25 [10112/50048]	Loss: 1.1586
Training Epoch: 25 [10240/50048]	Loss: 1.1108
Training Epoch: 25 [10368/50048]	Loss: 1.0556
Training Epoch: 25 [10496/50048]	Loss: 1.2140
Training Epoch: 25 [10624/50048]	Loss: 1.1843
Training Epoch: 25 [10752/50048]	Loss: 1.2185
Training Epoch: 25 [10880/50048]	Loss: 1.0455
Training Epoch: 25 [11008/50048]	Loss: 1.2249
Training Epoch: 25 [11136/50048]	Loss: 1.0177
Training Epoch: 25 [11264/50048]	Loss: 1.0856
Training Epoch: 25 [11392/50048]	Loss: 1.1279
Training Epoch: 25 [11520/50048]	Loss: 1.1236
Training Epoch: 25 [11648/50048]	Loss: 1.3257
Training Epoch: 25 [11776/50048]	Loss: 1.1236
Training Epoch: 25 [11904/50048]	Loss: 1.1733
Training Epoch: 25 [12032/50048]	Loss: 1.1162
Training Epoch: 25 [12160/50048]	Loss: 1.3489
Training Epoch: 25 [12288/50048]	Loss: 1.1613
Training Epoch: 25 [12416/50048]	Loss: 1.1037
Training Epoch: 25 [12544/50048]	Loss: 1.3549
Training Epoch: 25 [12672/50048]	Loss: 1.0816
Training Epoch: 25 [12800/50048]	Loss: 0.9379
Training Epoch: 25 [12928/50048]	Loss: 0.9989
Training Epoch: 25 [13056/50048]	Loss: 0.9895
Training Epoch: 25 [13184/50048]	Loss: 1.0927
Training Epoch: 25 [13312/50048]	Loss: 1.2841
Training Epoch: 25 [13440/50048]	Loss: 1.1843
Training Epoch: 25 [13568/50048]	Loss: 1.3873
Training Epoch: 25 [13696/50048]	Loss: 1.1448
Training Epoch: 25 [13824/50048]	Loss: 1.2196
Training Epoch: 25 [13952/50048]	Loss: 1.0711
Training Epoch: 25 [14080/50048]	Loss: 1.1720
Training Epoch: 25 [14208/50048]	Loss: 1.0404
Training Epoch: 25 [14336/50048]	Loss: 1.1822
Training Epoch: 25 [14464/50048]	Loss: 1.0337
Training Epoch: 25 [14592/50048]	Loss: 1.1981
Training Epoch: 25 [14720/50048]	Loss: 1.2128
Training Epoch: 25 [14848/50048]	Loss: 1.0765
Training Epoch: 25 [14976/50048]	Loss: 1.3324
Training Epoch: 25 [15104/50048]	Loss: 1.0468
Training Epoch: 25 [15232/50048]	Loss: 1.2286
Training Epoch: 25 [15360/50048]	Loss: 1.1284
Training Epoch: 25 [15488/50048]	Loss: 1.1511
Training Epoch: 25 [15616/50048]	Loss: 1.1959
Training Epoch: 25 [15744/50048]	Loss: 1.3186
Training Epoch: 25 [15872/50048]	Loss: 1.1311
Training Epoch: 25 [16000/50048]	Loss: 1.0383
Training Epoch: 25 [16128/50048]	Loss: 1.0546
Training Epoch: 25 [16256/50048]	Loss: 1.2937
Training Epoch: 25 [16384/50048]	Loss: 1.2142
Training Epoch: 25 [16512/50048]	Loss: 1.1446
Training Epoch: 25 [16640/50048]	Loss: 1.0162
Training Epoch: 25 [16768/50048]	Loss: 1.2030
Training Epoch: 25 [16896/50048]	Loss: 0.9947
Training Epoch: 25 [17024/50048]	Loss: 1.2831
Training Epoch: 25 [17152/50048]	Loss: 1.4432
Training Epoch: 25 [17280/50048]	Loss: 1.1109
Training Epoch: 25 [17408/50048]	Loss: 1.2171
Training Epoch: 25 [17536/50048]	Loss: 1.0880
Training Epoch: 25 [17664/50048]	Loss: 1.2100
Training Epoch: 25 [17792/50048]	Loss: 0.9693
Training Epoch: 25 [17920/50048]	Loss: 1.0684
Training Epoch: 25 [18048/50048]	Loss: 1.0897
Training Epoch: 25 [18176/50048]	Loss: 1.1615
Training Epoch: 25 [18304/50048]	Loss: 1.1384
Training Epoch: 25 [18432/50048]	Loss: 1.2205
Training Epoch: 25 [18560/50048]	Loss: 1.0273
Training Epoch: 25 [18688/50048]	Loss: 1.1981
Training Epoch: 25 [18816/50048]	Loss: 1.2578
Training Epoch: 25 [18944/50048]	Loss: 0.9357
Training Epoch: 25 [19072/50048]	Loss: 1.2929
Training Epoch: 25 [19200/50048]	Loss: 1.1800
Training Epoch: 25 [19328/50048]	Loss: 1.1700
Training Epoch: 25 [19456/50048]	Loss: 1.2901
Training Epoch: 25 [19584/50048]	Loss: 1.2009
Training Epoch: 25 [19712/50048]	Loss: 1.2210
Training Epoch: 25 [19840/50048]	Loss: 1.3082
Training Epoch: 25 [19968/50048]	Loss: 1.0020
Training Epoch: 25 [20096/50048]	Loss: 1.2284
Training Epoch: 25 [20224/50048]	Loss: 1.1300
Training Epoch: 25 [20352/50048]	Loss: 1.2603
Training Epoch: 25 [20480/50048]	Loss: 1.1267
Training Epoch: 25 [20608/50048]	Loss: 1.0657
Training Epoch: 25 [20736/50048]	Loss: 1.3491
Training Epoch: 25 [20864/50048]	Loss: 1.2021
Training Epoch: 25 [20992/50048]	Loss: 1.2183
Training Epoch: 25 [21120/50048]	Loss: 1.1729
Training Epoch: 25 [21248/50048]	Loss: 1.0861
Training Epoch: 25 [21376/50048]	Loss: 1.2800
Training Epoch: 25 [21504/50048]	Loss: 1.1884
Training Epoch: 25 [21632/50048]	Loss: 1.2355
Training Epoch: 25 [21760/50048]	Loss: 1.1684
Training Epoch: 25 [21888/50048]	Loss: 1.3616
Training Epoch: 25 [22016/50048]	Loss: 1.0756
Training Epoch: 25 [22144/50048]	Loss: 1.1627
Training Epoch: 25 [22272/50048]	Loss: 1.2179
Training Epoch: 25 [22400/50048]	Loss: 1.2735
Training Epoch: 25 [22528/50048]	Loss: 1.0670
Training Epoch: 25 [22656/50048]	Loss: 0.9230
Training Epoch: 25 [22784/50048]	Loss: 1.1698
Training Epoch: 25 [22912/50048]	Loss: 1.1820
Training Epoch: 25 [23040/50048]	Loss: 1.0686
Training Epoch: 25 [23168/50048]	Loss: 1.0595
Training Epoch: 25 [23296/50048]	Loss: 1.1380
Training Epoch: 25 [23424/50048]	Loss: 1.3428
Training Epoch: 25 [23552/50048]	Loss: 1.4261
Training Epoch: 25 [23680/50048]	Loss: 0.9364
Training Epoch: 25 [23808/50048]	Loss: 1.1848
Training Epoch: 25 [23936/50048]	Loss: 1.1010
Training Epoch: 25 [24064/50048]	Loss: 1.3311
Training Epoch: 25 [24192/50048]	Loss: 1.2430
Training Epoch: 25 [24320/50048]	Loss: 1.0583
Training Epoch: 25 [24448/50048]	Loss: 1.2595
Training Epoch: 25 [24576/50048]	Loss: 1.2244
Training Epoch: 25 [24704/50048]	Loss: 1.2293
Training Epoch: 25 [24832/50048]	Loss: 1.0191
Training Epoch: 25 [24960/50048]	Loss: 1.1579
Training Epoch: 25 [25088/50048]	Loss: 1.2273
Training Epoch: 25 [25216/50048]	Loss: 1.2030
Training Epoch: 25 [25344/50048]	Loss: 1.1257
Training Epoch: 25 [25472/50048]	Loss: 1.1007
Training Epoch: 25 [25600/50048]	Loss: 1.5647
Training Epoch: 25 [25728/50048]	Loss: 1.0389
Training Epoch: 25 [25856/50048]	Loss: 1.1236
Training Epoch: 25 [25984/50048]	Loss: 1.2754
Training Epoch: 25 [26112/50048]	Loss: 0.9099
Training Epoch: 25 [26240/50048]	Loss: 1.3391
Training Epoch: 25 [26368/50048]	Loss: 1.3474
Training Epoch: 25 [26496/50048]	Loss: 1.0488
Training Epoch: 25 [26624/50048]	Loss: 1.2441
Training Epoch: 25 [26752/50048]	Loss: 1.0827
Training Epoch: 25 [26880/50048]	Loss: 1.0241
Training Epoch: 25 [27008/50048]	Loss: 1.2379
Training Epoch: 25 [27136/50048]	Loss: 1.1509
Training Epoch: 25 [27264/50048]	Loss: 1.2357
Training Epoch: 25 [27392/50048]	Loss: 1.2907
Training Epoch: 25 [27520/50048]	Loss: 1.1126
Training Epoch: 25 [27648/50048]	Loss: 1.1806
Training Epoch: 25 [27776/50048]	Loss: 1.2771
Training Epoch: 25 [27904/50048]	Loss: 1.0399
Training Epoch: 25 [28032/50048]	Loss: 1.1934
Training Epoch: 25 [28160/50048]	Loss: 1.0779
Training Epoch: 25 [28288/50048]	Loss: 1.0951
Training Epoch: 25 [28416/50048]	Loss: 1.3659
Training Epoch: 25 [28544/50048]	Loss: 1.3774
Training Epoch: 25 [28672/50048]	Loss: 0.9807
Training Epoch: 25 [28800/50048]	Loss: 1.0159
Training Epoch: 25 [28928/50048]	Loss: 1.3433
Training Epoch: 25 [29056/50048]	Loss: 1.1661
Training Epoch: 25 [29184/50048]	Loss: 1.0855
Training Epoch: 25 [29312/50048]	Loss: 1.3466
Training Epoch: 25 [29440/50048]	Loss: 1.1796
Training Epoch: 25 [29568/50048]	Loss: 1.2462
Training Epoch: 25 [29696/50048]	Loss: 1.1530
Training Epoch: 25 [29824/50048]	Loss: 1.3462
Training Epoch: 25 [29952/50048]	Loss: 1.1774
Training Epoch: 25 [30080/50048]	Loss: 1.1548
Training Epoch: 25 [30208/50048]	Loss: 1.2290
Training Epoch: 25 [30336/50048]	Loss: 1.2202
Training Epoch: 25 [30464/50048]	Loss: 1.0997
Training Epoch: 25 [30592/50048]	Loss: 1.4089
Training Epoch: 25 [30720/50048]	Loss: 1.1535
Training Epoch: 25 [30848/50048]	Loss: 1.0825
Training Epoch: 25 [30976/50048]	Loss: 1.1768
Training Epoch: 25 [31104/50048]	Loss: 1.3020
Training Epoch: 25 [31232/50048]	Loss: 1.0744
Training Epoch: 25 [31360/50048]	Loss: 1.0564
Training Epoch: 25 [31488/50048]	Loss: 1.0601
Training Epoch: 25 [31616/50048]	Loss: 1.1665
Training Epoch: 25 [31744/50048]	Loss: 1.0976
Training Epoch: 25 [31872/50048]	Loss: 1.2240
Training Epoch: 25 [32000/50048]	Loss: 1.3073
Training Epoch: 25 [32128/50048]	Loss: 1.2365
Training Epoch: 25 [32256/50048]	Loss: 1.3968
Training Epoch: 25 [32384/50048]	Loss: 1.1203
Training Epoch: 25 [32512/50048]	Loss: 1.1305
Training Epoch: 25 [32640/50048]	Loss: 1.1831
Training Epoch: 25 [32768/50048]	Loss: 1.2190
Training Epoch: 25 [32896/50048]	Loss: 1.2546
Training Epoch: 25 [33024/50048]	Loss: 1.2078
Training Epoch: 25 [33152/50048]	Loss: 1.2236
Training Epoch: 25 [33280/50048]	Loss: 1.2190
Training Epoch: 25 [33408/50048]	Loss: 1.0047
Training Epoch: 25 [33536/50048]	Loss: 1.1681
Training Epoch: 25 [33664/50048]	Loss: 1.2526
Training Epoch: 25 [33792/50048]	Loss: 1.2071
Training Epoch: 25 [33920/50048]	Loss: 1.3892
Training Epoch: 25 [34048/50048]	Loss: 1.3250
Training Epoch: 25 [34176/50048]	Loss: 1.2310
Training Epoch: 25 [34304/50048]	Loss: 1.0873
Training Epoch: 25 [34432/50048]	Loss: 1.2112
Training Epoch: 25 [34560/50048]	Loss: 1.3812
Training Epoch: 25 [34688/50048]	Loss: 1.2156
Training Epoch: 25 [34816/50048]	Loss: 1.0974
Training Epoch: 25 [34944/50048]	Loss: 1.1698
Training Epoch: 25 [35072/50048]	Loss: 1.0320
Training Epoch: 25 [35200/50048]	Loss: 1.0377
Training Epoch: 25 [35328/50048]	Loss: 1.1885
Training Epoch: 25 [35456/50048]	Loss: 1.0298
Training Epoch: 25 [35584/50048]	Loss: 1.0303
Training Epoch: 25 [35712/50048]	Loss: 1.2883
Training Epoch: 25 [35840/50048]	Loss: 1.0250
Training Epoch: 25 [35968/50048]	Loss: 1.2327
Training Epoch: 25 [36096/50048]	Loss: 1.1951
Training Epoch: 25 [36224/50048]	Loss: 1.3397
Training Epoch: 25 [36352/50048]	Loss: 0.9982
Training Epoch: 25 [36480/50048]	Loss: 1.1575
Training Epoch: 25 [36608/50048]	Loss: 1.2486
Training Epoch: 25 [36736/50048]	Loss: 1.1399
Training Epoch: 25 [36864/50048]	Loss: 1.1782
Training Epoch: 25 [36992/50048]	Loss: 1.1730
Training Epoch: 25 [37120/50048]	Loss: 1.1794
Training Epoch: 25 [37248/50048]	Loss: 1.0764
Training Epoch: 25 [37376/50048]	Loss: 1.1126
Training Epoch: 25 [37504/50048]	Loss: 1.3346
Training Epoch: 25 [37632/50048]	Loss: 1.0657
Training Epoch: 25 [37760/50048]	Loss: 1.2546
Training Epoch: 25 [37888/50048]	Loss: 1.0752
Training Epoch: 25 [38016/50048]	Loss: 1.0998
Training Epoch: 25 [38144/50048]	Loss: 1.1495
Training Epoch: 25 [38272/50048]	Loss: 1.1491
Training Epoch: 25 [38400/50048]	Loss: 1.1276
Training Epoch: 25 [38528/50048]	Loss: 1.2071
Training Epoch: 25 [38656/50048]	Loss: 1.0368
Training Epoch: 25 [38784/50048]	Loss: 1.2215
Training Epoch: 25 [38912/50048]	Loss: 1.1257
Training Epoch: 25 [39040/50048]	Loss: 0.9876
Training Epoch: 25 [39168/50048]	Loss: 1.1112
Training Epoch: 25 [39296/50048]	Loss: 1.3821
Training Epoch: 25 [39424/50048]	Loss: 0.9515
Training Epoch: 25 [39552/50048]	Loss: 1.2236
Training Epoch: 25 [39680/50048]	Loss: 1.4761
Training Epoch: 25 [39808/50048]	Loss: 1.3254
Training Epoch: 25 [39936/50048]	Loss: 1.1255
Training Epoch: 25 [40064/50048]	Loss: 1.2270
Training Epoch: 25 [40192/50048]	Loss: 1.0773
Training Epoch: 25 [40320/50048]	Loss: 1.1189
Training Epoch: 25 [40448/50048]	Loss: 1.2353
Training Epoch: 25 [40576/50048]	Loss: 1.3214
Training Epoch: 25 [40704/50048]	Loss: 1.3943
Training Epoch: 25 [40832/50048]	Loss: 1.1513
Training Epoch: 25 [40960/50048]	Loss: 1.1126
Training Epoch: 25 [41088/50048]	Loss: 1.3579
Training Epoch: 25 [41216/50048]	Loss: 1.2414
Training Epoch: 25 [41344/50048]	Loss: 1.2737
Training Epoch: 25 [41472/50048]	Loss: 1.2644
Training Epoch: 25 [41600/50048]	Loss: 1.3680
Training Epoch: 25 [41728/50048]	Loss: 1.2946
Training Epoch: 25 [41856/50048]	Loss: 1.3254
Training Epoch: 25 [41984/50048]	Loss: 1.3713
Training Epoch: 25 [42112/50048]	Loss: 1.2828
Training Epoch: 25 [42240/50048]	Loss: 1.1734
Training Epoch: 25 [42368/50048]	Loss: 1.1185
Training Epoch: 25 [42496/50048]	Loss: 1.0891
Training Epoch: 25 [42624/50048]	Loss: 1.0665
Training Epoch: 25 [42752/50048]	Loss: 1.1497
Training Epoch: 25 [42880/50048]	Loss: 1.2139
Training Epoch: 25 [43008/50048]	Loss: 1.2526
Training Epoch: 25 [43136/50048]	Loss: 1.2043
Training Epoch: 25 [43264/50048]	Loss: 1.1039
Training Epoch: 25 [43392/50048]	Loss: 1.3075
Training Epoch: 25 [43520/50048]	Loss: 1.2907
Training Epoch: 25 [43648/50048]	Loss: 1.2391
Training Epoch: 25 [43776/50048]	Loss: 1.2712
Training Epoch: 25 [43904/50048]	Loss: 1.1300
Training Epoch: 25 [44032/50048]	Loss: 1.1367
Training Epoch: 25 [44160/50048]	Loss: 1.2333
Training Epoch: 25 [44288/50048]	Loss: 1.0321
Training Epoch: 25 [44416/50048]	Loss: 1.0789
Training Epoch: 25 [44544/50048]	Loss: 1.4520
Training Epoch: 25 [44672/50048]	Loss: 1.2384
Training Epoch: 25 [44800/50048]	Loss: 1.2149
Training Epoch: 25 [44928/50048]	Loss: 1.2585
Training Epoch: 25 [45056/50048]	Loss: 1.1136
Training Epoch: 25 [45184/50048]	Loss: 1.1185
Training Epoch: 25 [45312/50048]	Loss: 1.3194
Training Epoch: 25 [45440/50048]	Loss: 0.9687
Training Epoch: 25 [45568/50048]	Loss: 0.9827
Training Epoch: 25 [45696/50048]	Loss: 1.0819
Training Epoch: 25 [45824/50048]	Loss: 1.0965
Training Epoch: 25 [45952/50048]	Loss: 1.2764
Training Epoch: 25 [46080/50048]	Loss: 1.0961
Training Epoch: 25 [46208/50048]	Loss: 1.1845
Training Epoch: 25 [46336/50048]	Loss: 0.8241
Training Epoch: 25 [46464/50048]	Loss: 1.1048
Training Epoch: 25 [46592/50048]	Loss: 0.9777
Training Epoch: 25 [46720/50048]	Loss: 0.9262
Training Epoch: 25 [46848/50048]	Loss: 1.2139
Training Epoch: 25 [46976/50048]	Loss: 1.2590
Training Epoch: 25 [47104/50048]	Loss: 1.2336
Training Epoch: 25 [47232/50048]	Loss: 1.1681
Training Epoch: 25 [47360/50048]	Loss: 1.1460
Training Epoch: 25 [47488/50048]	Loss: 1.2630
Training Epoch: 25 [47616/50048]	Loss: 1.1627
Training Epoch: 25 [47744/50048]	Loss: 0.9743
Training Epoch: 25 [47872/50048]	Loss: 1.1919
Training Epoch: 25 [48000/50048]	Loss: 1.1821
Training Epoch: 25 [48128/50048]	Loss: 1.2512
Training Epoch: 25 [48256/50048]	Loss: 1.3414
Training Epoch: 25 [48384/50048]	Loss: 1.2288
Training Epoch: 25 [48512/50048]	Loss: 1.1742
Training Epoch: 25 [48640/50048]	Loss: 0.9493
Training Epoch: 25 [48768/50048]	Loss: 1.1820
Training Epoch: 25 [48896/50048]	Loss: 1.2840
Training Epoch: 25 [49024/50048]	Loss: 1.0780
Training Epoch: 25 [49152/50048]	Loss: 1.3441
Training Epoch: 25 [49280/50048]	Loss: 1.3683
Training Epoch: 25 [49408/50048]	Loss: 1.1231
Training Epoch: 25 [49536/50048]	Loss: 0.9487
Training Epoch: 25 [49664/50048]	Loss: 1.0305
Training Epoch: 25 [49792/50048]	Loss: 1.1590
Training Epoch: 25 [49920/50048]	Loss: 1.2957
Training Epoch: 25 [50048/50048]	Loss: 1.0048
Validation Epoch: 25, Average loss: 0.0121, Accuracy: 0.5856
Training Epoch: 26 [128/50048]	Loss: 0.9934
Training Epoch: 26 [256/50048]	Loss: 1.2775
Training Epoch: 26 [384/50048]	Loss: 1.0954
Training Epoch: 26 [512/50048]	Loss: 1.0506
Training Epoch: 26 [640/50048]	Loss: 0.9098
Training Epoch: 26 [768/50048]	Loss: 0.9635
Training Epoch: 26 [896/50048]	Loss: 0.8954
Training Epoch: 26 [1024/50048]	Loss: 1.2351
Training Epoch: 26 [1152/50048]	Loss: 1.0907
Training Epoch: 26 [1280/50048]	Loss: 1.0764
Training Epoch: 26 [1408/50048]	Loss: 1.0197
Training Epoch: 26 [1536/50048]	Loss: 0.9072
Training Epoch: 26 [1664/50048]	Loss: 1.0120
Training Epoch: 26 [1792/50048]	Loss: 1.1977
Training Epoch: 26 [1920/50048]	Loss: 1.5573
Training Epoch: 26 [2048/50048]	Loss: 1.0454
Training Epoch: 26 [2176/50048]	Loss: 1.1451
Training Epoch: 26 [2304/50048]	Loss: 1.3317
Training Epoch: 26 [2432/50048]	Loss: 1.1480
Training Epoch: 26 [2560/50048]	Loss: 1.0609
Training Epoch: 26 [2688/50048]	Loss: 1.1099
Training Epoch: 26 [2816/50048]	Loss: 1.0382
Training Epoch: 26 [2944/50048]	Loss: 0.7844
Training Epoch: 26 [3072/50048]	Loss: 1.0586
Training Epoch: 26 [3200/50048]	Loss: 1.1328
Training Epoch: 26 [3328/50048]	Loss: 1.1019
Training Epoch: 26 [3456/50048]	Loss: 1.1728
Training Epoch: 26 [3584/50048]	Loss: 1.2128
Training Epoch: 26 [3712/50048]	Loss: 1.1468
Training Epoch: 26 [3840/50048]	Loss: 1.0914
Training Epoch: 26 [3968/50048]	Loss: 0.9234
Training Epoch: 26 [4096/50048]	Loss: 0.9612
Training Epoch: 26 [4224/50048]	Loss: 1.2781
Training Epoch: 26 [4352/50048]	Loss: 1.2515
Training Epoch: 26 [4480/50048]	Loss: 1.0735
Training Epoch: 26 [4608/50048]	Loss: 0.9556
Training Epoch: 26 [4736/50048]	Loss: 1.2094
Training Epoch: 26 [4864/50048]	Loss: 0.9323
Training Epoch: 26 [4992/50048]	Loss: 1.3355
Training Epoch: 26 [5120/50048]	Loss: 0.9555
Training Epoch: 26 [5248/50048]	Loss: 1.0894
Training Epoch: 26 [5376/50048]	Loss: 1.1671
Training Epoch: 26 [5504/50048]	Loss: 1.1460
Training Epoch: 26 [5632/50048]	Loss: 0.9607
Training Epoch: 26 [5760/50048]	Loss: 1.2017
Training Epoch: 26 [5888/50048]	Loss: 0.9280
Training Epoch: 26 [6016/50048]	Loss: 1.1850
Training Epoch: 26 [6144/50048]	Loss: 1.3810
Training Epoch: 26 [6272/50048]	Loss: 1.0628
Training Epoch: 26 [6400/50048]	Loss: 1.1469
Training Epoch: 26 [6528/50048]	Loss: 0.9393
Training Epoch: 26 [6656/50048]	Loss: 1.1196
Training Epoch: 26 [6784/50048]	Loss: 1.0988
Training Epoch: 26 [6912/50048]	Loss: 1.1839
Training Epoch: 26 [7040/50048]	Loss: 1.1390
Training Epoch: 26 [7168/50048]	Loss: 1.1830
Training Epoch: 26 [7296/50048]	Loss: 1.0724
Training Epoch: 26 [7424/50048]	Loss: 1.1204
Training Epoch: 26 [7552/50048]	Loss: 1.0942
Training Epoch: 26 [7680/50048]	Loss: 1.1614
Training Epoch: 26 [7808/50048]	Loss: 1.0520
Training Epoch: 26 [7936/50048]	Loss: 1.1788
Training Epoch: 26 [8064/50048]	Loss: 0.9552
Training Epoch: 26 [8192/50048]	Loss: 1.1612
Training Epoch: 26 [8320/50048]	Loss: 0.9993
Training Epoch: 26 [8448/50048]	Loss: 1.0871
Training Epoch: 26 [8576/50048]	Loss: 1.2033
Training Epoch: 26 [8704/50048]	Loss: 0.9551
Training Epoch: 26 [8832/50048]	Loss: 0.9869
Training Epoch: 26 [8960/50048]	Loss: 1.1320
Training Epoch: 26 [9088/50048]	Loss: 1.2356
Training Epoch: 26 [9216/50048]	Loss: 1.2504
Training Epoch: 26 [9344/50048]	Loss: 0.9025
Training Epoch: 26 [9472/50048]	Loss: 1.0866
Training Epoch: 26 [9600/50048]	Loss: 1.1425
Training Epoch: 26 [9728/50048]	Loss: 1.2295
Training Epoch: 26 [9856/50048]	Loss: 1.0581
Training Epoch: 26 [9984/50048]	Loss: 1.0955
Training Epoch: 26 [10112/50048]	Loss: 1.0731
Training Epoch: 26 [10240/50048]	Loss: 1.1555
Training Epoch: 26 [10368/50048]	Loss: 1.0086
Training Epoch: 26 [10496/50048]	Loss: 1.0294
Training Epoch: 26 [10624/50048]	Loss: 1.2469
Training Epoch: 26 [10752/50048]	Loss: 1.4885
Training Epoch: 26 [10880/50048]	Loss: 1.1768
Training Epoch: 26 [11008/50048]	Loss: 1.1084
Training Epoch: 26 [11136/50048]	Loss: 1.1807
Training Epoch: 26 [11264/50048]	Loss: 1.0024
Training Epoch: 26 [11392/50048]	Loss: 0.9187
Training Epoch: 26 [11520/50048]	Loss: 1.0197
Training Epoch: 26 [11648/50048]	Loss: 1.2388
Training Epoch: 26 [11776/50048]	Loss: 1.1399
Training Epoch: 26 [11904/50048]	Loss: 1.0536
Training Epoch: 26 [12032/50048]	Loss: 1.0284
Training Epoch: 26 [12160/50048]	Loss: 1.1031
Training Epoch: 26 [12288/50048]	Loss: 1.2739
Training Epoch: 26 [12416/50048]	Loss: 1.0752
Training Epoch: 26 [12544/50048]	Loss: 1.1626
Training Epoch: 26 [12672/50048]	Loss: 1.1208
Training Epoch: 26 [12800/50048]	Loss: 0.9138
Training Epoch: 26 [12928/50048]	Loss: 1.2483
Training Epoch: 26 [13056/50048]	Loss: 1.1916
Training Epoch: 26 [13184/50048]	Loss: 1.0534
Training Epoch: 26 [13312/50048]	Loss: 1.0764
Training Epoch: 26 [13440/50048]	Loss: 1.0305
Training Epoch: 26 [13568/50048]	Loss: 1.2279
Training Epoch: 26 [13696/50048]	Loss: 1.3471
Training Epoch: 26 [13824/50048]	Loss: 1.3422
Training Epoch: 26 [13952/50048]	Loss: 1.1911
Training Epoch: 26 [14080/50048]	Loss: 1.1479
Training Epoch: 26 [14208/50048]	Loss: 0.9189
Training Epoch: 26 [14336/50048]	Loss: 1.0431
Training Epoch: 26 [14464/50048]	Loss: 1.1018
Training Epoch: 26 [14592/50048]	Loss: 1.0553
Training Epoch: 26 [14720/50048]	Loss: 1.1986
Training Epoch: 26 [14848/50048]	Loss: 1.1195
Training Epoch: 26 [14976/50048]	Loss: 0.9265
Training Epoch: 26 [15104/50048]	Loss: 0.9778
Training Epoch: 26 [15232/50048]	Loss: 1.2828
Training Epoch: 26 [15360/50048]	Loss: 1.0019
Training Epoch: 26 [15488/50048]	Loss: 1.3920
Training Epoch: 26 [15616/50048]	Loss: 1.1071
Training Epoch: 26 [15744/50048]	Loss: 1.1161
Training Epoch: 26 [15872/50048]	Loss: 1.2740
Training Epoch: 26 [16000/50048]	Loss: 1.2455
Training Epoch: 26 [16128/50048]	Loss: 0.9824
Training Epoch: 26 [16256/50048]	Loss: 1.0032
Training Epoch: 26 [16384/50048]	Loss: 1.2843
Training Epoch: 26 [16512/50048]	Loss: 1.0014
Training Epoch: 26 [16640/50048]	Loss: 0.9388
Training Epoch: 26 [16768/50048]	Loss: 0.9979
Training Epoch: 26 [16896/50048]	Loss: 1.1676
Training Epoch: 26 [17024/50048]	Loss: 1.3275
Training Epoch: 26 [17152/50048]	Loss: 1.3073
Training Epoch: 26 [17280/50048]	Loss: 1.2116
Training Epoch: 26 [17408/50048]	Loss: 1.0836
Training Epoch: 26 [17536/50048]	Loss: 1.2551
Training Epoch: 26 [17664/50048]	Loss: 1.1642
Training Epoch: 26 [17792/50048]	Loss: 1.2062
Training Epoch: 26 [17920/50048]	Loss: 1.1138
Training Epoch: 26 [18048/50048]	Loss: 1.1329
Training Epoch: 26 [18176/50048]	Loss: 0.9118
Training Epoch: 26 [18304/50048]	Loss: 1.1029
Training Epoch: 26 [18432/50048]	Loss: 1.2368
Training Epoch: 26 [18560/50048]	Loss: 1.0959
Training Epoch: 26 [18688/50048]	Loss: 0.8147
Training Epoch: 26 [18816/50048]	Loss: 1.1143
Training Epoch: 26 [18944/50048]	Loss: 1.2613
Training Epoch: 26 [19072/50048]	Loss: 1.1262
Training Epoch: 26 [19200/50048]	Loss: 1.0803
Training Epoch: 26 [19328/50048]	Loss: 1.1139
Training Epoch: 26 [19456/50048]	Loss: 1.3119
Training Epoch: 26 [19584/50048]	Loss: 0.9970
Training Epoch: 26 [19712/50048]	Loss: 1.2221
Training Epoch: 26 [19840/50048]	Loss: 1.2287
Training Epoch: 26 [19968/50048]	Loss: 0.9584
Training Epoch: 26 [20096/50048]	Loss: 1.0899
Training Epoch: 26 [20224/50048]	Loss: 1.3551
Training Epoch: 26 [20352/50048]	Loss: 1.2042
Training Epoch: 26 [20480/50048]	Loss: 1.1975
Training Epoch: 26 [20608/50048]	Loss: 1.3001
Training Epoch: 26 [20736/50048]	Loss: 0.9787
Training Epoch: 26 [20864/50048]	Loss: 1.3532
Training Epoch: 26 [20992/50048]	Loss: 1.0949
Training Epoch: 26 [21120/50048]	Loss: 1.3056
Training Epoch: 26 [21248/50048]	Loss: 1.0420
Training Epoch: 26 [21376/50048]	Loss: 1.2056
Training Epoch: 26 [21504/50048]	Loss: 1.2148
Training Epoch: 26 [21632/50048]	Loss: 1.1917
Training Epoch: 26 [21760/50048]	Loss: 1.1264
Training Epoch: 26 [21888/50048]	Loss: 1.0811
Training Epoch: 26 [22016/50048]	Loss: 1.0239
Training Epoch: 26 [22144/50048]	Loss: 1.0967
Training Epoch: 26 [22272/50048]	Loss: 1.0290
Training Epoch: 26 [22400/50048]	Loss: 1.1971
Training Epoch: 26 [22528/50048]	Loss: 1.0151
Training Epoch: 26 [22656/50048]	Loss: 1.1588
Training Epoch: 26 [22784/50048]	Loss: 1.1731
Training Epoch: 26 [22912/50048]	Loss: 1.1818
Training Epoch: 26 [23040/50048]	Loss: 1.1594
Training Epoch: 26 [23168/50048]	Loss: 1.3759
Training Epoch: 26 [23296/50048]	Loss: 1.2608
Training Epoch: 26 [23424/50048]	Loss: 1.0652
Training Epoch: 26 [23552/50048]	Loss: 1.3068
Training Epoch: 26 [23680/50048]	Loss: 1.1126
Training Epoch: 26 [23808/50048]	Loss: 1.1147
Training Epoch: 26 [23936/50048]	Loss: 1.1001
Training Epoch: 26 [24064/50048]	Loss: 1.2835
Training Epoch: 26 [24192/50048]	Loss: 1.0113
Training Epoch: 26 [24320/50048]	Loss: 1.1816
Training Epoch: 26 [24448/50048]	Loss: 0.9174
Training Epoch: 26 [24576/50048]	Loss: 1.0869
Training Epoch: 26 [24704/50048]	Loss: 1.0608
Training Epoch: 26 [24832/50048]	Loss: 1.0088
Training Epoch: 26 [24960/50048]	Loss: 1.2485
Training Epoch: 26 [25088/50048]	Loss: 1.2558
Training Epoch: 26 [25216/50048]	Loss: 1.0233
Training Epoch: 26 [25344/50048]	Loss: 1.1632
Training Epoch: 26 [25472/50048]	Loss: 0.9715
Training Epoch: 26 [25600/50048]	Loss: 1.4106
Training Epoch: 26 [25728/50048]	Loss: 0.8711
Training Epoch: 26 [25856/50048]	Loss: 1.2066
Training Epoch: 26 [25984/50048]	Loss: 1.0386
Training Epoch: 26 [26112/50048]	Loss: 1.3234
Training Epoch: 26 [26240/50048]	Loss: 1.3133
Training Epoch: 26 [26368/50048]	Loss: 1.1001
Training Epoch: 26 [26496/50048]	Loss: 1.4233
Training Epoch: 26 [26624/50048]	Loss: 1.1038
Training Epoch: 26 [26752/50048]	Loss: 1.1198
Training Epoch: 26 [26880/50048]	Loss: 1.1998
Training Epoch: 26 [27008/50048]	Loss: 1.1734
Training Epoch: 26 [27136/50048]	Loss: 1.0943
Training Epoch: 26 [27264/50048]	Loss: 1.0178
Training Epoch: 26 [27392/50048]	Loss: 0.9752
Training Epoch: 26 [27520/50048]	Loss: 1.1789
Training Epoch: 26 [27648/50048]	Loss: 1.0479
Training Epoch: 26 [27776/50048]	Loss: 0.9062
Training Epoch: 26 [27904/50048]	Loss: 0.8960
Training Epoch: 26 [28032/50048]	Loss: 1.2000
Training Epoch: 26 [28160/50048]	Loss: 1.1570
Training Epoch: 26 [28288/50048]	Loss: 1.0700
Training Epoch: 26 [28416/50048]	Loss: 1.1816
Training Epoch: 26 [28544/50048]	Loss: 1.0543
Training Epoch: 26 [28672/50048]	Loss: 1.1558
Training Epoch: 26 [28800/50048]	Loss: 1.1452
Training Epoch: 26 [28928/50048]	Loss: 1.1535
Training Epoch: 26 [29056/50048]	Loss: 1.1799
Training Epoch: 26 [29184/50048]	Loss: 1.4723
Training Epoch: 26 [29312/50048]	Loss: 1.0517
Training Epoch: 26 [29440/50048]	Loss: 1.1715
Training Epoch: 26 [29568/50048]	Loss: 1.4290
Training Epoch: 26 [29696/50048]	Loss: 1.1214
Training Epoch: 26 [29824/50048]	Loss: 1.1130
Training Epoch: 26 [29952/50048]	Loss: 1.0109
Training Epoch: 26 [30080/50048]	Loss: 1.1013
Training Epoch: 26 [30208/50048]	Loss: 1.0957
Training Epoch: 26 [30336/50048]	Loss: 1.2285
Training Epoch: 26 [30464/50048]	Loss: 1.1758
Training Epoch: 26 [30592/50048]	Loss: 1.0850
Training Epoch: 26 [30720/50048]	Loss: 1.3452
Training Epoch: 26 [30848/50048]	Loss: 0.9443
Training Epoch: 26 [30976/50048]	Loss: 1.0549
Training Epoch: 26 [31104/50048]	Loss: 1.1943
Training Epoch: 26 [31232/50048]	Loss: 0.8933
Training Epoch: 26 [31360/50048]	Loss: 1.1197
Training Epoch: 26 [31488/50048]	Loss: 1.1230
Training Epoch: 26 [31616/50048]	Loss: 1.2372
Training Epoch: 26 [31744/50048]	Loss: 1.0794
Training Epoch: 26 [31872/50048]	Loss: 1.2726
Training Epoch: 26 [32000/50048]	Loss: 1.4660
Training Epoch: 26 [32128/50048]	Loss: 1.0574
Training Epoch: 26 [32256/50048]	Loss: 1.0463
Training Epoch: 26 [32384/50048]	Loss: 1.1018
Training Epoch: 26 [32512/50048]	Loss: 1.1065
Training Epoch: 26 [32640/50048]	Loss: 1.1025
Training Epoch: 26 [32768/50048]	Loss: 1.0598
Training Epoch: 26 [32896/50048]	Loss: 0.9713
Training Epoch: 26 [33024/50048]	Loss: 1.0521
Training Epoch: 26 [33152/50048]	Loss: 1.0583
Training Epoch: 26 [33280/50048]	Loss: 1.2908
Training Epoch: 26 [33408/50048]	Loss: 1.1619
Training Epoch: 26 [33536/50048]	Loss: 1.0617
Training Epoch: 26 [33664/50048]	Loss: 1.1530
Training Epoch: 26 [33792/50048]	Loss: 1.2244
Training Epoch: 26 [33920/50048]	Loss: 1.3319
Training Epoch: 26 [34048/50048]	Loss: 1.3108
Training Epoch: 26 [34176/50048]	Loss: 1.0307
Training Epoch: 26 [34304/50048]	Loss: 1.1784
Training Epoch: 26 [34432/50048]	Loss: 1.1533
Training Epoch: 26 [34560/50048]	Loss: 1.3795
Training Epoch: 26 [34688/50048]	Loss: 1.0245
Training Epoch: 26 [34816/50048]	Loss: 1.1361
Training Epoch: 26 [34944/50048]	Loss: 1.1668
Training Epoch: 26 [35072/50048]	Loss: 1.2060
Training Epoch: 26 [35200/50048]	Loss: 1.0938
Training Epoch: 26 [35328/50048]	Loss: 0.9012
Training Epoch: 26 [35456/50048]	Loss: 1.1110
Training Epoch: 26 [35584/50048]	Loss: 1.0043
Training Epoch: 26 [35712/50048]	Loss: 0.9752
Training Epoch: 26 [35840/50048]	Loss: 1.1522
Training Epoch: 26 [35968/50048]	Loss: 0.9780
Training Epoch: 26 [36096/50048]	Loss: 1.1585
Training Epoch: 26 [36224/50048]	Loss: 1.0140
Training Epoch: 26 [36352/50048]	Loss: 1.2161
Training Epoch: 26 [36480/50048]	Loss: 0.9499
Training Epoch: 26 [36608/50048]	Loss: 1.2394
Training Epoch: 26 [36736/50048]	Loss: 1.3093
Training Epoch: 26 [36864/50048]	Loss: 1.3074
Training Epoch: 26 [36992/50048]	Loss: 1.2133
Training Epoch: 26 [37120/50048]	Loss: 0.9284
Training Epoch: 26 [37248/50048]	Loss: 1.1273
Training Epoch: 26 [37376/50048]	Loss: 1.1798
Training Epoch: 26 [37504/50048]	Loss: 1.2391
Training Epoch: 26 [37632/50048]	Loss: 1.1614
Training Epoch: 26 [37760/50048]	Loss: 1.1016
Training Epoch: 26 [37888/50048]	Loss: 0.9971
Training Epoch: 26 [38016/50048]	Loss: 1.4743
Training Epoch: 26 [38144/50048]	Loss: 1.3550
Training Epoch: 26 [38272/50048]	Loss: 1.2430
Training Epoch: 26 [38400/50048]	Loss: 1.0283
Training Epoch: 26 [38528/50048]	Loss: 1.2267
Training Epoch: 26 [38656/50048]	Loss: 1.1257
Training Epoch: 26 [38784/50048]	Loss: 0.9630
Training Epoch: 26 [38912/50048]	Loss: 1.0633
Training Epoch: 26 [39040/50048]	Loss: 0.9934
Training Epoch: 26 [39168/50048]	Loss: 1.1698
Training Epoch: 26 [39296/50048]	Loss: 1.2790
Training Epoch: 26 [39424/50048]	Loss: 1.0980
Training Epoch: 26 [39552/50048]	Loss: 1.0946
Training Epoch: 26 [39680/50048]	Loss: 1.1525
Training Epoch: 26 [39808/50048]	Loss: 1.1005
Training Epoch: 26 [39936/50048]	Loss: 1.2073
Training Epoch: 26 [40064/50048]	Loss: 1.2265
Training Epoch: 26 [40192/50048]	Loss: 1.1923
Training Epoch: 26 [40320/50048]	Loss: 1.1678
Training Epoch: 26 [40448/50048]	Loss: 1.1767
Training Epoch: 26 [40576/50048]	Loss: 1.1728
Training Epoch: 26 [40704/50048]	Loss: 0.9310
Training Epoch: 26 [40832/50048]	Loss: 1.2358
Training Epoch: 26 [40960/50048]	Loss: 0.9630
Training Epoch: 26 [41088/50048]	Loss: 1.0703
Training Epoch: 26 [41216/50048]	Loss: 1.3666
Training Epoch: 26 [41344/50048]	Loss: 1.1267
Training Epoch: 26 [41472/50048]	Loss: 1.4127
Training Epoch: 26 [41600/50048]	Loss: 0.9557
Training Epoch: 26 [41728/50048]	Loss: 1.2475
Training Epoch: 26 [41856/50048]	Loss: 1.2396
Training Epoch: 26 [41984/50048]	Loss: 1.2149
Training Epoch: 26 [42112/50048]	Loss: 1.3195
Training Epoch: 26 [42240/50048]	Loss: 1.3488
Training Epoch: 26 [42368/50048]	Loss: 1.0846
Training Epoch: 26 [42496/50048]	Loss: 1.3189
Training Epoch: 26 [42624/50048]	Loss: 1.2633
Training Epoch: 26 [42752/50048]	Loss: 1.1824
Training Epoch: 26 [42880/50048]	Loss: 0.9800
Training Epoch: 26 [43008/50048]	Loss: 1.2045
Training Epoch: 26 [43136/50048]	Loss: 1.0125
Training Epoch: 26 [43264/50048]	Loss: 1.1502
Training Epoch: 26 [43392/50048]	Loss: 1.0800
Training Epoch: 26 [43520/50048]	Loss: 1.2287
Training Epoch: 26 [43648/50048]	Loss: 1.1470
Training Epoch: 26 [43776/50048]	Loss: 1.3349
Training Epoch: 26 [43904/50048]	Loss: 1.3710
Training Epoch: 26 [44032/50048]	Loss: 1.0930
Training Epoch: 26 [44160/50048]	Loss: 1.2400
Training Epoch: 26 [44288/50048]	Loss: 1.2904
Training Epoch: 26 [44416/50048]	Loss: 1.4021
Training Epoch: 26 [44544/50048]	Loss: 1.1148
Training Epoch: 26 [44672/50048]	Loss: 1.0772
Training Epoch: 26 [44800/50048]	Loss: 1.1061
Training Epoch: 26 [44928/50048]	Loss: 1.1907
Training Epoch: 26 [45056/50048]	Loss: 1.1730
Training Epoch: 26 [45184/50048]	Loss: 0.8246
Training Epoch: 26 [45312/50048]	Loss: 1.1293
Training Epoch: 26 [45440/50048]	Loss: 1.1265
Training Epoch: 26 [45568/50048]	Loss: 1.4291
Training Epoch: 26 [45696/50048]	Loss: 1.0886
Training Epoch: 26 [45824/50048]	Loss: 1.2064
Training Epoch: 26 [45952/50048]	Loss: 1.2505
Training Epoch: 26 [46080/50048]	Loss: 1.3751
Training Epoch: 26 [46208/50048]	Loss: 1.0051
Training Epoch: 26 [46336/50048]	Loss: 1.1102
Training Epoch: 26 [46464/50048]	Loss: 1.2258
Training Epoch: 26 [46592/50048]	Loss: 1.0846
Training Epoch: 26 [46720/50048]	Loss: 1.2577
Training Epoch: 26 [46848/50048]	Loss: 1.1873
Training Epoch: 26 [46976/50048]	Loss: 1.1558
Training Epoch: 26 [47104/50048]	Loss: 1.2063
Training Epoch: 26 [47232/50048]	Loss: 1.2457
Training Epoch: 26 [47360/50048]	Loss: 0.8533
Training Epoch: 26 [47488/50048]	Loss: 1.3369
Training Epoch: 26 [47616/50048]	Loss: 0.7966
Training Epoch: 26 [47744/50048]	Loss: 1.1896
Training Epoch: 26 [47872/50048]	Loss: 1.2076
Training Epoch: 26 [48000/50048]	Loss: 1.3564
Training Epoch: 26 [48128/50048]	Loss: 1.1358
Training Epoch: 26 [48256/50048]	Loss: 1.1983
Training Epoch: 26 [48384/50048]	Loss: 1.0994
Training Epoch: 26 [48512/50048]	Loss: 1.3187
Training Epoch: 26 [48640/50048]	Loss: 0.9865
Training Epoch: 26 [48768/50048]	Loss: 1.3243
Training Epoch: 26 [48896/50048]	Loss: 1.2605
Training Epoch: 26 [49024/50048]	Loss: 1.1309
Training Epoch: 26 [49152/50048]	Loss: 1.2178
Training Epoch: 26 [49280/50048]	Loss: 1.4550
Training Epoch: 26 [49408/50048]	Loss: 1.1441
Training Epoch: 26 [49536/50048]	Loss: 1.1203
Training Epoch: 26 [49664/50048]	Loss: 1.2016
Training Epoch: 26 [49792/50048]	Loss: 1.0312
Training Epoch: 26 [49920/50048]	Loss: 1.4062
Training Epoch: 26 [50048/50048]	Loss: 1.2384
Validation Epoch: 26, Average loss: 0.0122, Accuracy: 0.5848
Training Epoch: 27 [128/50048]	Loss: 0.9376
Training Epoch: 27 [256/50048]	Loss: 0.8093
Training Epoch: 27 [384/50048]	Loss: 0.9331
Training Epoch: 27 [512/50048]	Loss: 0.9366
Training Epoch: 27 [640/50048]	Loss: 0.9771
Training Epoch: 27 [768/50048]	Loss: 1.1751
Training Epoch: 27 [896/50048]	Loss: 0.9648
Training Epoch: 27 [1024/50048]	Loss: 1.1003
Training Epoch: 27 [1152/50048]	Loss: 0.9528
Training Epoch: 27 [1280/50048]	Loss: 1.3785
Training Epoch: 27 [1408/50048]	Loss: 0.8615
Training Epoch: 27 [1536/50048]	Loss: 0.9802
Training Epoch: 27 [1664/50048]	Loss: 1.0656
Training Epoch: 27 [1792/50048]	Loss: 1.1863
Training Epoch: 27 [1920/50048]	Loss: 1.0228
Training Epoch: 27 [2048/50048]	Loss: 1.2570
Training Epoch: 27 [2176/50048]	Loss: 1.1284
Training Epoch: 27 [2304/50048]	Loss: 1.1457
Training Epoch: 27 [2432/50048]	Loss: 1.2260
Training Epoch: 27 [2560/50048]	Loss: 0.8040
Training Epoch: 27 [2688/50048]	Loss: 1.1848
Training Epoch: 27 [2816/50048]	Loss: 1.1113
Training Epoch: 27 [2944/50048]	Loss: 1.2217
Training Epoch: 27 [3072/50048]	Loss: 0.9932
Training Epoch: 27 [3200/50048]	Loss: 1.0503
Training Epoch: 27 [3328/50048]	Loss: 1.2561
Training Epoch: 27 [3456/50048]	Loss: 1.1935
Training Epoch: 27 [3584/50048]	Loss: 1.2461
Training Epoch: 27 [3712/50048]	Loss: 1.2692
Training Epoch: 27 [3840/50048]	Loss: 0.9470
Training Epoch: 27 [3968/50048]	Loss: 1.0792
Training Epoch: 27 [4096/50048]	Loss: 1.1946
Training Epoch: 27 [4224/50048]	Loss: 1.0797
Training Epoch: 27 [4352/50048]	Loss: 1.0506
Training Epoch: 27 [4480/50048]	Loss: 0.9053
Training Epoch: 27 [4608/50048]	Loss: 1.1299
Training Epoch: 27 [4736/50048]	Loss: 0.9808
Training Epoch: 27 [4864/50048]	Loss: 1.2526
Training Epoch: 27 [4992/50048]	Loss: 1.1505
Training Epoch: 27 [5120/50048]	Loss: 1.1168
Training Epoch: 27 [5248/50048]	Loss: 0.9255
Training Epoch: 27 [5376/50048]	Loss: 1.1183
Training Epoch: 27 [5504/50048]	Loss: 1.0010
Training Epoch: 27 [5632/50048]	Loss: 1.0203
Training Epoch: 27 [5760/50048]	Loss: 0.9817
Training Epoch: 27 [5888/50048]	Loss: 0.8779
Training Epoch: 27 [6016/50048]	Loss: 1.1181
Training Epoch: 27 [6144/50048]	Loss: 1.1036
Training Epoch: 27 [6272/50048]	Loss: 1.0803
Training Epoch: 27 [6400/50048]	Loss: 1.0663
Training Epoch: 27 [6528/50048]	Loss: 1.1968
Training Epoch: 27 [6656/50048]	Loss: 1.2263
Training Epoch: 27 [6784/50048]	Loss: 1.0019
Training Epoch: 27 [6912/50048]	Loss: 0.8670
Training Epoch: 27 [7040/50048]	Loss: 1.1721
Training Epoch: 27 [7168/50048]	Loss: 1.0523
Training Epoch: 27 [7296/50048]	Loss: 1.0174
Training Epoch: 27 [7424/50048]	Loss: 1.2578
Training Epoch: 27 [7552/50048]	Loss: 1.1664
Training Epoch: 27 [7680/50048]	Loss: 0.8839
Training Epoch: 27 [7808/50048]	Loss: 1.0979
Training Epoch: 27 [7936/50048]	Loss: 1.2428
Training Epoch: 27 [8064/50048]	Loss: 1.1357
Training Epoch: 27 [8192/50048]	Loss: 1.0605
Training Epoch: 27 [8320/50048]	Loss: 1.0277
Training Epoch: 27 [8448/50048]	Loss: 1.0341
Training Epoch: 27 [8576/50048]	Loss: 0.9889
Training Epoch: 27 [8704/50048]	Loss: 1.1294
Training Epoch: 27 [8832/50048]	Loss: 1.2725
Training Epoch: 27 [8960/50048]	Loss: 0.9120
Training Epoch: 27 [9088/50048]	Loss: 0.9584
Training Epoch: 27 [9216/50048]	Loss: 0.8599
Training Epoch: 27 [9344/50048]	Loss: 1.0678
Training Epoch: 27 [9472/50048]	Loss: 1.1643
Training Epoch: 27 [9600/50048]	Loss: 1.0443
Training Epoch: 27 [9728/50048]	Loss: 1.1616
Training Epoch: 27 [9856/50048]	Loss: 0.9716
Training Epoch: 27 [9984/50048]	Loss: 0.9493
Training Epoch: 27 [10112/50048]	Loss: 0.8657
Training Epoch: 27 [10240/50048]	Loss: 1.1559
Training Epoch: 27 [10368/50048]	Loss: 0.9634
Training Epoch: 27 [10496/50048]	Loss: 0.9449
Training Epoch: 27 [10624/50048]	Loss: 1.1350
Training Epoch: 27 [10752/50048]	Loss: 0.9662
Training Epoch: 27 [10880/50048]	Loss: 1.2897
Training Epoch: 27 [11008/50048]	Loss: 0.9971
Training Epoch: 27 [11136/50048]	Loss: 1.1992
Training Epoch: 27 [11264/50048]	Loss: 0.9261
Training Epoch: 27 [11392/50048]	Loss: 1.3080
Training Epoch: 27 [11520/50048]	Loss: 1.0390
Training Epoch: 27 [11648/50048]	Loss: 1.0659
Training Epoch: 27 [11776/50048]	Loss: 1.0967
Training Epoch: 27 [11904/50048]	Loss: 1.0094
Training Epoch: 27 [12032/50048]	Loss: 1.0857
Training Epoch: 27 [12160/50048]	Loss: 0.9740
Training Epoch: 27 [12288/50048]	Loss: 1.0642
Training Epoch: 27 [12416/50048]	Loss: 1.3473
Training Epoch: 27 [12544/50048]	Loss: 1.0328
Training Epoch: 27 [12672/50048]	Loss: 1.2434
Training Epoch: 27 [12800/50048]	Loss: 1.0996
Training Epoch: 27 [12928/50048]	Loss: 0.9033
Training Epoch: 27 [13056/50048]	Loss: 0.8559
Training Epoch: 27 [13184/50048]	Loss: 1.1901
Training Epoch: 27 [13312/50048]	Loss: 0.9502
Training Epoch: 27 [13440/50048]	Loss: 0.9492
Training Epoch: 27 [13568/50048]	Loss: 0.8457
Training Epoch: 27 [13696/50048]	Loss: 1.1146
Training Epoch: 27 [13824/50048]	Loss: 1.0620
Training Epoch: 27 [13952/50048]	Loss: 1.1202
Training Epoch: 27 [14080/50048]	Loss: 1.1041
Training Epoch: 27 [14208/50048]	Loss: 1.1722
Training Epoch: 27 [14336/50048]	Loss: 1.0090
Training Epoch: 27 [14464/50048]	Loss: 1.3521
Training Epoch: 27 [14592/50048]	Loss: 0.9908
Training Epoch: 27 [14720/50048]	Loss: 1.3452
Training Epoch: 27 [14848/50048]	Loss: 1.3062
Training Epoch: 27 [14976/50048]	Loss: 1.2160
Training Epoch: 27 [15104/50048]	Loss: 1.1335
Training Epoch: 27 [15232/50048]	Loss: 0.8261
Training Epoch: 27 [15360/50048]	Loss: 1.1727
Training Epoch: 27 [15488/50048]	Loss: 1.0474
Training Epoch: 27 [15616/50048]	Loss: 0.9637
Training Epoch: 27 [15744/50048]	Loss: 1.1795
Training Epoch: 27 [15872/50048]	Loss: 1.3448
Training Epoch: 27 [16000/50048]	Loss: 1.0407
Training Epoch: 27 [16128/50048]	Loss: 1.2641
Training Epoch: 27 [16256/50048]	Loss: 0.9913
Training Epoch: 27 [16384/50048]	Loss: 0.9347
Training Epoch: 27 [16512/50048]	Loss: 1.2677
Training Epoch: 27 [16640/50048]	Loss: 1.2307
Training Epoch: 27 [16768/50048]	Loss: 1.1095
Training Epoch: 27 [16896/50048]	Loss: 1.2528
Training Epoch: 27 [17024/50048]	Loss: 1.1818
Training Epoch: 27 [17152/50048]	Loss: 1.1140
Training Epoch: 27 [17280/50048]	Loss: 1.1300
Training Epoch: 27 [17408/50048]	Loss: 1.1602
Training Epoch: 27 [17536/50048]	Loss: 1.0169
Training Epoch: 27 [17664/50048]	Loss: 1.0497
Training Epoch: 27 [17792/50048]	Loss: 1.2183
Training Epoch: 27 [17920/50048]	Loss: 1.1023
Training Epoch: 27 [18048/50048]	Loss: 1.1209
Training Epoch: 27 [18176/50048]	Loss: 1.0040
Training Epoch: 27 [18304/50048]	Loss: 1.0328
Training Epoch: 27 [18432/50048]	Loss: 1.2519
Training Epoch: 27 [18560/50048]	Loss: 1.3303
Training Epoch: 27 [18688/50048]	Loss: 1.3090
Training Epoch: 27 [18816/50048]	Loss: 0.8567
Training Epoch: 27 [18944/50048]	Loss: 1.1441
Training Epoch: 27 [19072/50048]	Loss: 1.0555
Training Epoch: 27 [19200/50048]	Loss: 1.0295
Training Epoch: 27 [19328/50048]	Loss: 1.0761
Training Epoch: 27 [19456/50048]	Loss: 1.1681
Training Epoch: 27 [19584/50048]	Loss: 1.2778
Training Epoch: 27 [19712/50048]	Loss: 1.2861
Training Epoch: 27 [19840/50048]	Loss: 1.0293
Training Epoch: 27 [19968/50048]	Loss: 0.9823
Training Epoch: 27 [20096/50048]	Loss: 1.0924
Training Epoch: 27 [20224/50048]	Loss: 1.1225
Training Epoch: 27 [20352/50048]	Loss: 0.9872
Training Epoch: 27 [20480/50048]	Loss: 0.9186
Training Epoch: 27 [20608/50048]	Loss: 1.0801
Training Epoch: 27 [20736/50048]	Loss: 1.1839
Training Epoch: 27 [20864/50048]	Loss: 1.0148
Training Epoch: 27 [20992/50048]	Loss: 1.1868
Training Epoch: 27 [21120/50048]	Loss: 1.0412
Training Epoch: 27 [21248/50048]	Loss: 0.8588
Training Epoch: 27 [21376/50048]	Loss: 1.2993
Training Epoch: 27 [21504/50048]	Loss: 1.1191
Training Epoch: 27 [21632/50048]	Loss: 1.1118
Training Epoch: 27 [21760/50048]	Loss: 0.9678
Training Epoch: 27 [21888/50048]	Loss: 0.8763
Training Epoch: 27 [22016/50048]	Loss: 1.0787
Training Epoch: 27 [22144/50048]	Loss: 1.1368
Training Epoch: 27 [22272/50048]	Loss: 0.8561
Training Epoch: 27 [22400/50048]	Loss: 0.9885
Training Epoch: 27 [22528/50048]	Loss: 1.1915
Training Epoch: 27 [22656/50048]	Loss: 1.1554
Training Epoch: 27 [22784/50048]	Loss: 1.0043
Training Epoch: 27 [22912/50048]	Loss: 1.0113
Training Epoch: 27 [23040/50048]	Loss: 1.0935
Training Epoch: 27 [23168/50048]	Loss: 1.0532
Training Epoch: 27 [23296/50048]	Loss: 1.0694
Training Epoch: 27 [23424/50048]	Loss: 1.2593
Training Epoch: 27 [23552/50048]	Loss: 1.0688
Training Epoch: 27 [23680/50048]	Loss: 1.3811
Training Epoch: 27 [23808/50048]	Loss: 1.1706
Training Epoch: 27 [23936/50048]	Loss: 1.0110
Training Epoch: 27 [24064/50048]	Loss: 1.1556
Training Epoch: 27 [24192/50048]	Loss: 1.1634
Training Epoch: 27 [24320/50048]	Loss: 0.9055
Training Epoch: 27 [24448/50048]	Loss: 1.2789
Training Epoch: 27 [24576/50048]	Loss: 0.9048
Training Epoch: 27 [24704/50048]	Loss: 1.2370
Training Epoch: 27 [24832/50048]	Loss: 0.8782
Training Epoch: 27 [24960/50048]	Loss: 1.1272
Training Epoch: 27 [25088/50048]	Loss: 0.9203
Training Epoch: 27 [25216/50048]	Loss: 1.0196
Training Epoch: 27 [25344/50048]	Loss: 1.0695
Training Epoch: 27 [25472/50048]	Loss: 0.8786
Training Epoch: 27 [25600/50048]	Loss: 1.1154
Training Epoch: 27 [25728/50048]	Loss: 1.0888
Training Epoch: 27 [25856/50048]	Loss: 1.1378
Training Epoch: 27 [25984/50048]	Loss: 1.1656
Training Epoch: 27 [26112/50048]	Loss: 1.1658
Training Epoch: 27 [26240/50048]	Loss: 1.0379
Training Epoch: 27 [26368/50048]	Loss: 1.1856
Training Epoch: 27 [26496/50048]	Loss: 1.1062
Training Epoch: 27 [26624/50048]	Loss: 1.2324
Training Epoch: 27 [26752/50048]	Loss: 0.9620
Training Epoch: 27 [26880/50048]	Loss: 1.1909
Training Epoch: 27 [27008/50048]	Loss: 1.1173
Training Epoch: 27 [27136/50048]	Loss: 1.1081
Training Epoch: 27 [27264/50048]	Loss: 1.0287
Training Epoch: 27 [27392/50048]	Loss: 1.1182
Training Epoch: 27 [27520/50048]	Loss: 1.2746
Training Epoch: 27 [27648/50048]	Loss: 1.0693
Training Epoch: 27 [27776/50048]	Loss: 1.2505
Training Epoch: 27 [27904/50048]	Loss: 1.1263
Training Epoch: 27 [28032/50048]	Loss: 1.5835
Training Epoch: 27 [28160/50048]	Loss: 1.0412
Training Epoch: 27 [28288/50048]	Loss: 1.1609
Training Epoch: 27 [28416/50048]	Loss: 1.1499
Training Epoch: 27 [28544/50048]	Loss: 0.9919
Training Epoch: 27 [28672/50048]	Loss: 1.2720
Training Epoch: 27 [28800/50048]	Loss: 0.9267
Training Epoch: 27 [28928/50048]	Loss: 1.1293
Training Epoch: 27 [29056/50048]	Loss: 0.9827
Training Epoch: 27 [29184/50048]	Loss: 1.1180
Training Epoch: 27 [29312/50048]	Loss: 1.0985
Training Epoch: 27 [29440/50048]	Loss: 0.9950
Training Epoch: 27 [29568/50048]	Loss: 1.0402
Training Epoch: 27 [29696/50048]	Loss: 1.1852
Training Epoch: 27 [29824/50048]	Loss: 1.1697
Training Epoch: 27 [29952/50048]	Loss: 1.0080
Training Epoch: 27 [30080/50048]	Loss: 1.0802
Training Epoch: 27 [30208/50048]	Loss: 1.1537
Training Epoch: 27 [30336/50048]	Loss: 1.0746
Training Epoch: 27 [30464/50048]	Loss: 1.2418
Training Epoch: 27 [30592/50048]	Loss: 1.0656
Training Epoch: 27 [30720/50048]	Loss: 1.0014
Training Epoch: 27 [30848/50048]	Loss: 1.0354
Training Epoch: 27 [30976/50048]	Loss: 1.1603
Training Epoch: 27 [31104/50048]	Loss: 1.1619
Training Epoch: 27 [31232/50048]	Loss: 1.1390
Training Epoch: 27 [31360/50048]	Loss: 1.3901
Training Epoch: 27 [31488/50048]	Loss: 1.2561
Training Epoch: 27 [31616/50048]	Loss: 1.0666
Training Epoch: 27 [31744/50048]	Loss: 1.1006
Training Epoch: 27 [31872/50048]	Loss: 1.3223
Training Epoch: 27 [32000/50048]	Loss: 1.0193
Training Epoch: 27 [32128/50048]	Loss: 1.2280
Training Epoch: 27 [32256/50048]	Loss: 1.0810
Training Epoch: 27 [32384/50048]	Loss: 1.1212
Training Epoch: 27 [32512/50048]	Loss: 0.9998
Training Epoch: 27 [32640/50048]	Loss: 1.3221
Training Epoch: 27 [32768/50048]	Loss: 1.1105
Training Epoch: 27 [32896/50048]	Loss: 1.0869
Training Epoch: 27 [33024/50048]	Loss: 0.9305
Training Epoch: 27 [33152/50048]	Loss: 0.9828
Training Epoch: 27 [33280/50048]	Loss: 1.3158
Training Epoch: 27 [33408/50048]	Loss: 1.0210
Training Epoch: 27 [33536/50048]	Loss: 1.2826
Training Epoch: 27 [33664/50048]	Loss: 1.2185
Training Epoch: 27 [33792/50048]	Loss: 1.1591
Training Epoch: 27 [33920/50048]	Loss: 1.2834
Training Epoch: 27 [34048/50048]	Loss: 1.2861
Training Epoch: 27 [34176/50048]	Loss: 0.9415
Training Epoch: 27 [34304/50048]	Loss: 1.1516
Training Epoch: 27 [34432/50048]	Loss: 1.1962
Training Epoch: 27 [34560/50048]	Loss: 1.2351
Training Epoch: 27 [34688/50048]	Loss: 1.1519
Training Epoch: 27 [34816/50048]	Loss: 1.0357
Training Epoch: 27 [34944/50048]	Loss: 1.1533
Training Epoch: 27 [35072/50048]	Loss: 1.1771
Training Epoch: 27 [35200/50048]	Loss: 1.0629
Training Epoch: 27 [35328/50048]	Loss: 1.1183
Training Epoch: 27 [35456/50048]	Loss: 1.0425
Training Epoch: 27 [35584/50048]	Loss: 1.2507
Training Epoch: 27 [35712/50048]	Loss: 1.2842
Training Epoch: 27 [35840/50048]	Loss: 1.2822
Training Epoch: 27 [35968/50048]	Loss: 0.8129
Training Epoch: 27 [36096/50048]	Loss: 1.1052
Training Epoch: 27 [36224/50048]	Loss: 1.1351
Training Epoch: 27 [36352/50048]	Loss: 1.2684
Training Epoch: 27 [36480/50048]	Loss: 0.9603
Training Epoch: 27 [36608/50048]	Loss: 1.2533
Training Epoch: 27 [36736/50048]	Loss: 0.9901
Training Epoch: 27 [36864/50048]	Loss: 1.1105
Training Epoch: 27 [36992/50048]	Loss: 1.0107
Training Epoch: 27 [37120/50048]	Loss: 1.0992
Training Epoch: 27 [37248/50048]	Loss: 1.1361
Training Epoch: 27 [37376/50048]	Loss: 1.2422
Training Epoch: 27 [37504/50048]	Loss: 0.9199
Training Epoch: 27 [37632/50048]	Loss: 1.0580
Training Epoch: 27 [37760/50048]	Loss: 1.1601
Training Epoch: 27 [37888/50048]	Loss: 0.9352
Training Epoch: 27 [38016/50048]	Loss: 0.9273
Training Epoch: 27 [38144/50048]	Loss: 1.2495
Training Epoch: 27 [38272/50048]	Loss: 1.3483
Training Epoch: 27 [38400/50048]	Loss: 1.1013
Training Epoch: 27 [38528/50048]	Loss: 1.1602
Training Epoch: 27 [38656/50048]	Loss: 0.9875
Training Epoch: 27 [38784/50048]	Loss: 1.2120
Training Epoch: 27 [38912/50048]	Loss: 1.0132
Training Epoch: 27 [39040/50048]	Loss: 1.2474
Training Epoch: 27 [39168/50048]	Loss: 1.0581
Training Epoch: 27 [39296/50048]	Loss: 1.1951
Training Epoch: 27 [39424/50048]	Loss: 1.1430
Training Epoch: 27 [39552/50048]	Loss: 1.0594
Training Epoch: 27 [39680/50048]	Loss: 1.0357
Training Epoch: 27 [39808/50048]	Loss: 1.2218
Training Epoch: 27 [39936/50048]	Loss: 1.0595
Training Epoch: 27 [40064/50048]	Loss: 1.0798
Training Epoch: 27 [40192/50048]	Loss: 1.1877
Training Epoch: 27 [40320/50048]	Loss: 1.0052
Training Epoch: 27 [40448/50048]	Loss: 1.1649
Training Epoch: 27 [40576/50048]	Loss: 0.9052
Training Epoch: 27 [40704/50048]	Loss: 1.1430
Training Epoch: 27 [40832/50048]	Loss: 0.9536
Training Epoch: 27 [40960/50048]	Loss: 1.2400
Training Epoch: 27 [41088/50048]	Loss: 1.0256
Training Epoch: 27 [41216/50048]	Loss: 1.1318
Training Epoch: 27 [41344/50048]	Loss: 1.0679
Training Epoch: 27 [41472/50048]	Loss: 0.8933
Training Epoch: 27 [41600/50048]	Loss: 1.3465
Training Epoch: 27 [41728/50048]	Loss: 1.1696
Training Epoch: 27 [41856/50048]	Loss: 1.0753
Training Epoch: 27 [41984/50048]	Loss: 1.1015
Training Epoch: 27 [42112/50048]	Loss: 1.1761
Training Epoch: 27 [42240/50048]	Loss: 1.3290
Training Epoch: 27 [42368/50048]	Loss: 1.0602
Training Epoch: 27 [42496/50048]	Loss: 1.0590
Training Epoch: 27 [42624/50048]	Loss: 0.9783
Training Epoch: 27 [42752/50048]	Loss: 1.1637
Training Epoch: 27 [42880/50048]	Loss: 1.2065
Training Epoch: 27 [43008/50048]	Loss: 1.1301
Training Epoch: 27 [43136/50048]	Loss: 1.4029
Training Epoch: 27 [43264/50048]	Loss: 1.1745
Training Epoch: 27 [43392/50048]	Loss: 1.2857
Training Epoch: 27 [43520/50048]	Loss: 1.2692
Training Epoch: 27 [43648/50048]	Loss: 1.0904
Training Epoch: 27 [43776/50048]	Loss: 1.3129
Training Epoch: 27 [43904/50048]	Loss: 1.0295
Training Epoch: 27 [44032/50048]	Loss: 1.3015
Training Epoch: 27 [44160/50048]	Loss: 1.0516
Training Epoch: 27 [44288/50048]	Loss: 0.9008
Training Epoch: 27 [44416/50048]	Loss: 1.2924
Training Epoch: 27 [44544/50048]	Loss: 1.2094
Training Epoch: 27 [44672/50048]	Loss: 1.1479
Training Epoch: 27 [44800/50048]	Loss: 1.1588
Training Epoch: 27 [44928/50048]	Loss: 1.0800
Training Epoch: 27 [45056/50048]	Loss: 1.0681
Training Epoch: 27 [45184/50048]	Loss: 1.0303
Training Epoch: 27 [45312/50048]	Loss: 0.9347
Training Epoch: 27 [45440/50048]	Loss: 1.0164
Training Epoch: 27 [45568/50048]	Loss: 1.3807
Training Epoch: 27 [45696/50048]	Loss: 1.1692
Training Epoch: 27 [45824/50048]	Loss: 1.1400
Training Epoch: 27 [45952/50048]	Loss: 0.9760
Training Epoch: 27 [46080/50048]	Loss: 1.1525
Training Epoch: 27 [46208/50048]	Loss: 1.2595
Training Epoch: 27 [46336/50048]	Loss: 1.0846
Training Epoch: 27 [46464/50048]	Loss: 1.1488
Training Epoch: 27 [46592/50048]	Loss: 1.1341
Training Epoch: 27 [46720/50048]	Loss: 1.1081
Training Epoch: 27 [46848/50048]	Loss: 1.1113
Training Epoch: 27 [46976/50048]	Loss: 1.0585
Training Epoch: 27 [47104/50048]	Loss: 1.0575
Training Epoch: 27 [47232/50048]	Loss: 1.2233
Training Epoch: 27 [47360/50048]	Loss: 1.1634
Training Epoch: 27 [47488/50048]	Loss: 1.0291
Training Epoch: 27 [47616/50048]	Loss: 1.2304
Training Epoch: 27 [47744/50048]	Loss: 1.1495
Training Epoch: 27 [47872/50048]	Loss: 1.0543
Training Epoch: 27 [48000/50048]	Loss: 1.1636
Training Epoch: 27 [48128/50048]	Loss: 1.0997
Training Epoch: 27 [48256/50048]	Loss: 1.0488
Training Epoch: 27 [48384/50048]	Loss: 1.1292
Training Epoch: 27 [48512/50048]	Loss: 1.0997
Training Epoch: 27 [48640/50048]	Loss: 1.1695
Training Epoch: 27 [48768/50048]	Loss: 1.0919
Training Epoch: 27 [48896/50048]	Loss: 1.2892
Training Epoch: 27 [49024/50048]	Loss: 1.0157
Training Epoch: 27 [49152/50048]	Loss: 1.2584
Training Epoch: 27 [49280/50048]	Loss: 0.9403
Training Epoch: 27 [49408/50048]	Loss: 1.1914
Training Epoch: 27 [49536/50048]	Loss: 1.1827
Training Epoch: 27 [49664/50048]	Loss: 1.1761
Training Epoch: 27 [49792/50048]	Loss: 1.1074
Training Epoch: 27 [49920/50048]	Loss: 1.2433
Training Epoch: 27 [50048/50048]	Loss: 1.2028
Validation Epoch: 27, Average loss: 0.0121, Accuracy: 0.5935
Training Epoch: 28 [128/50048]	Loss: 1.0951
Training Epoch: 28 [256/50048]	Loss: 1.0587
Training Epoch: 28 [384/50048]	Loss: 0.9105
Training Epoch: 28 [512/50048]	Loss: 1.0762
Training Epoch: 28 [640/50048]	Loss: 1.2054
Training Epoch: 28 [768/50048]	Loss: 1.0781
Training Epoch: 28 [896/50048]	Loss: 1.0806
Training Epoch: 28 [1024/50048]	Loss: 1.0738
Training Epoch: 28 [1152/50048]	Loss: 0.9867
Training Epoch: 28 [1280/50048]	Loss: 1.0141
Training Epoch: 28 [1408/50048]	Loss: 1.2156
Training Epoch: 28 [1536/50048]	Loss: 1.0474
Training Epoch: 28 [1664/50048]	Loss: 1.0733
Training Epoch: 28 [1792/50048]	Loss: 1.0894
Training Epoch: 28 [1920/50048]	Loss: 1.2167
Training Epoch: 28 [2048/50048]	Loss: 0.9966
Training Epoch: 28 [2176/50048]	Loss: 0.9027
Training Epoch: 28 [2304/50048]	Loss: 0.9060
Training Epoch: 28 [2432/50048]	Loss: 1.2682
Training Epoch: 28 [2560/50048]	Loss: 1.0942
Training Epoch: 28 [2688/50048]	Loss: 1.0767
Training Epoch: 28 [2816/50048]	Loss: 1.1309
Training Epoch: 28 [2944/50048]	Loss: 0.9406
Training Epoch: 28 [3072/50048]	Loss: 1.2042
Training Epoch: 28 [3200/50048]	Loss: 1.3447
Training Epoch: 28 [3328/50048]	Loss: 0.9755
Training Epoch: 28 [3456/50048]	Loss: 0.9854
Training Epoch: 28 [3584/50048]	Loss: 0.9621
Training Epoch: 28 [3712/50048]	Loss: 1.1230
Training Epoch: 28 [3840/50048]	Loss: 0.8381
Training Epoch: 28 [3968/50048]	Loss: 1.0480
Training Epoch: 28 [4096/50048]	Loss: 1.0352
Training Epoch: 28 [4224/50048]	Loss: 1.2416
Training Epoch: 28 [4352/50048]	Loss: 1.1898
Training Epoch: 28 [4480/50048]	Loss: 1.0413
Training Epoch: 28 [4608/50048]	Loss: 1.0877
Training Epoch: 28 [4736/50048]	Loss: 0.9876
Training Epoch: 28 [4864/50048]	Loss: 0.8703
Training Epoch: 28 [4992/50048]	Loss: 0.9706
Training Epoch: 28 [5120/50048]	Loss: 1.2463
Training Epoch: 28 [5248/50048]	Loss: 1.1690
Training Epoch: 28 [5376/50048]	Loss: 0.9941
Training Epoch: 28 [5504/50048]	Loss: 0.9311
Training Epoch: 28 [5632/50048]	Loss: 0.9810
Training Epoch: 28 [5760/50048]	Loss: 0.9736
Training Epoch: 28 [5888/50048]	Loss: 1.1636
Training Epoch: 28 [6016/50048]	Loss: 1.2446
Training Epoch: 28 [6144/50048]	Loss: 1.1055
Training Epoch: 28 [6272/50048]	Loss: 1.2506
Training Epoch: 28 [6400/50048]	Loss: 1.0954
Training Epoch: 28 [6528/50048]	Loss: 0.9661
Training Epoch: 28 [6656/50048]	Loss: 1.1589
Training Epoch: 28 [6784/50048]	Loss: 1.0060
Training Epoch: 28 [6912/50048]	Loss: 0.9170
Training Epoch: 28 [7040/50048]	Loss: 1.0386
Training Epoch: 28 [7168/50048]	Loss: 1.0183
Training Epoch: 28 [7296/50048]	Loss: 1.2729
Training Epoch: 28 [7424/50048]	Loss: 0.9619
Training Epoch: 28 [7552/50048]	Loss: 1.0842
Training Epoch: 28 [7680/50048]	Loss: 1.1184
Training Epoch: 28 [7808/50048]	Loss: 1.0076
Training Epoch: 28 [7936/50048]	Loss: 1.3016
Training Epoch: 28 [8064/50048]	Loss: 1.0625
Training Epoch: 28 [8192/50048]	Loss: 1.2248
Training Epoch: 28 [8320/50048]	Loss: 0.9575
Training Epoch: 28 [8448/50048]	Loss: 1.1380
Training Epoch: 28 [8576/50048]	Loss: 0.9184
Training Epoch: 28 [8704/50048]	Loss: 0.9527
Training Epoch: 28 [8832/50048]	Loss: 1.0196
Training Epoch: 28 [8960/50048]	Loss: 0.9639
Training Epoch: 28 [9088/50048]	Loss: 1.1621
Training Epoch: 28 [9216/50048]	Loss: 1.0136
Training Epoch: 28 [9344/50048]	Loss: 1.0772
Training Epoch: 28 [9472/50048]	Loss: 0.7606
Training Epoch: 28 [9600/50048]	Loss: 1.1278
Training Epoch: 28 [9728/50048]	Loss: 1.0627
Training Epoch: 28 [9856/50048]	Loss: 1.0449
Training Epoch: 28 [9984/50048]	Loss: 1.0708
Training Epoch: 28 [10112/50048]	Loss: 1.0847
Training Epoch: 28 [10240/50048]	Loss: 0.9637
Training Epoch: 28 [10368/50048]	Loss: 1.0251
Training Epoch: 28 [10496/50048]	Loss: 0.9232
Training Epoch: 28 [10624/50048]	Loss: 0.9168
Training Epoch: 28 [10752/50048]	Loss: 1.1149
Training Epoch: 28 [10880/50048]	Loss: 0.9191
Training Epoch: 28 [11008/50048]	Loss: 1.2583
Training Epoch: 28 [11136/50048]	Loss: 1.1038
Training Epoch: 28 [11264/50048]	Loss: 1.2852
Training Epoch: 28 [11392/50048]	Loss: 1.0594
Training Epoch: 28 [11520/50048]	Loss: 1.1198
Training Epoch: 28 [11648/50048]	Loss: 1.0279
Training Epoch: 28 [11776/50048]	Loss: 1.0136
Training Epoch: 28 [11904/50048]	Loss: 1.1747
Training Epoch: 28 [12032/50048]	Loss: 1.0680
Training Epoch: 28 [12160/50048]	Loss: 0.9523
Training Epoch: 28 [12288/50048]	Loss: 1.0174
Training Epoch: 28 [12416/50048]	Loss: 1.0999
Training Epoch: 28 [12544/50048]	Loss: 0.9280
Training Epoch: 28 [12672/50048]	Loss: 1.0240
Training Epoch: 28 [12800/50048]	Loss: 0.9598
Training Epoch: 28 [12928/50048]	Loss: 1.1231
Training Epoch: 28 [13056/50048]	Loss: 0.9259
Training Epoch: 28 [13184/50048]	Loss: 0.9631
Training Epoch: 28 [13312/50048]	Loss: 1.1514
Training Epoch: 28 [13440/50048]	Loss: 1.0911
Training Epoch: 28 [13568/50048]	Loss: 1.0463
Training Epoch: 28 [13696/50048]	Loss: 1.0022
Training Epoch: 28 [13824/50048]	Loss: 0.9565
Training Epoch: 28 [13952/50048]	Loss: 0.9938
Training Epoch: 28 [14080/50048]	Loss: 0.9988
Training Epoch: 28 [14208/50048]	Loss: 0.9805
Training Epoch: 28 [14336/50048]	Loss: 0.9060
Training Epoch: 28 [14464/50048]	Loss: 1.0958
Training Epoch: 28 [14592/50048]	Loss: 1.0364
Training Epoch: 28 [14720/50048]	Loss: 1.0424
Training Epoch: 28 [14848/50048]	Loss: 0.7493
Training Epoch: 28 [14976/50048]	Loss: 0.9475
Training Epoch: 28 [15104/50048]	Loss: 1.1477
Training Epoch: 28 [15232/50048]	Loss: 1.1044
Training Epoch: 28 [15360/50048]	Loss: 0.9764
Training Epoch: 28 [15488/50048]	Loss: 1.3509
Training Epoch: 28 [15616/50048]	Loss: 1.0891
Training Epoch: 28 [15744/50048]	Loss: 1.0202
Training Epoch: 28 [15872/50048]	Loss: 1.2088
Training Epoch: 28 [16000/50048]	Loss: 1.0942
Training Epoch: 28 [16128/50048]	Loss: 0.9911
Training Epoch: 28 [16256/50048]	Loss: 1.3493
Training Epoch: 28 [16384/50048]	Loss: 1.1662
Training Epoch: 28 [16512/50048]	Loss: 1.1125
Training Epoch: 28 [16640/50048]	Loss: 1.0701
Training Epoch: 28 [16768/50048]	Loss: 0.8698
Training Epoch: 28 [16896/50048]	Loss: 1.1328
Training Epoch: 28 [17024/50048]	Loss: 1.0931
Training Epoch: 28 [17152/50048]	Loss: 0.9950
Training Epoch: 28 [17280/50048]	Loss: 1.1991
Training Epoch: 28 [17408/50048]	Loss: 0.9794
Training Epoch: 28 [17536/50048]	Loss: 1.1410
Training Epoch: 28 [17664/50048]	Loss: 0.8189
Training Epoch: 28 [17792/50048]	Loss: 1.2655
Training Epoch: 28 [17920/50048]	Loss: 1.0690
Training Epoch: 28 [18048/50048]	Loss: 1.2420
Training Epoch: 28 [18176/50048]	Loss: 1.2503
Training Epoch: 28 [18304/50048]	Loss: 1.1090
Training Epoch: 28 [18432/50048]	Loss: 1.2363
Training Epoch: 28 [18560/50048]	Loss: 1.1357
Training Epoch: 28 [18688/50048]	Loss: 1.0976
Training Epoch: 28 [18816/50048]	Loss: 1.0709
Training Epoch: 28 [18944/50048]	Loss: 1.0132
Training Epoch: 28 [19072/50048]	Loss: 0.9951
Training Epoch: 28 [19200/50048]	Loss: 1.0719
Training Epoch: 28 [19328/50048]	Loss: 1.0513
Training Epoch: 28 [19456/50048]	Loss: 1.0653
Training Epoch: 28 [19584/50048]	Loss: 0.9748
Training Epoch: 28 [19712/50048]	Loss: 0.8335
Training Epoch: 28 [19840/50048]	Loss: 1.0885
Training Epoch: 28 [19968/50048]	Loss: 1.0154
Training Epoch: 28 [20096/50048]	Loss: 1.0509
Training Epoch: 28 [20224/50048]	Loss: 0.9276
Training Epoch: 28 [20352/50048]	Loss: 1.0422
Training Epoch: 28 [20480/50048]	Loss: 1.3011
Training Epoch: 28 [20608/50048]	Loss: 1.0360
Training Epoch: 28 [20736/50048]	Loss: 1.2452
Training Epoch: 28 [20864/50048]	Loss: 1.0052
Training Epoch: 28 [20992/50048]	Loss: 1.1339
Training Epoch: 28 [21120/50048]	Loss: 1.0206
Training Epoch: 28 [21248/50048]	Loss: 1.1832
Training Epoch: 28 [21376/50048]	Loss: 1.0950
Training Epoch: 28 [21504/50048]	Loss: 0.9613
Training Epoch: 28 [21632/50048]	Loss: 0.8620
Training Epoch: 28 [21760/50048]	Loss: 0.9971
Training Epoch: 28 [21888/50048]	Loss: 1.1417
Training Epoch: 28 [22016/50048]	Loss: 1.0956
Training Epoch: 28 [22144/50048]	Loss: 0.9814
Training Epoch: 28 [22272/50048]	Loss: 1.1108
Training Epoch: 28 [22400/50048]	Loss: 0.9106
Training Epoch: 28 [22528/50048]	Loss: 1.0196
Training Epoch: 28 [22656/50048]	Loss: 1.0174
Training Epoch: 28 [22784/50048]	Loss: 1.1281
Training Epoch: 28 [22912/50048]	Loss: 1.2044
Training Epoch: 28 [23040/50048]	Loss: 0.9366
Training Epoch: 28 [23168/50048]	Loss: 1.2880
Training Epoch: 28 [23296/50048]	Loss: 1.0581
Training Epoch: 28 [23424/50048]	Loss: 0.9841
Training Epoch: 28 [23552/50048]	Loss: 0.8318
Training Epoch: 28 [23680/50048]	Loss: 1.2330
Training Epoch: 28 [23808/50048]	Loss: 1.1288
Training Epoch: 28 [23936/50048]	Loss: 0.9938
Training Epoch: 28 [24064/50048]	Loss: 1.0267
Training Epoch: 28 [24192/50048]	Loss: 1.2242
Training Epoch: 28 [24320/50048]	Loss: 1.0154
Training Epoch: 28 [24448/50048]	Loss: 1.0491
Training Epoch: 28 [24576/50048]	Loss: 1.1685
Training Epoch: 28 [24704/50048]	Loss: 0.9335
Training Epoch: 28 [24832/50048]	Loss: 1.3057
Training Epoch: 28 [24960/50048]	Loss: 1.1415
Training Epoch: 28 [25088/50048]	Loss: 1.0805
Training Epoch: 28 [25216/50048]	Loss: 1.0883
Training Epoch: 28 [25344/50048]	Loss: 1.0602
Training Epoch: 28 [25472/50048]	Loss: 1.1595
Training Epoch: 28 [25600/50048]	Loss: 1.0568
Training Epoch: 28 [25728/50048]	Loss: 1.0500
Training Epoch: 28 [25856/50048]	Loss: 1.1148
Training Epoch: 28 [25984/50048]	Loss: 1.2776
Training Epoch: 28 [26112/50048]	Loss: 0.9115
Training Epoch: 28 [26240/50048]	Loss: 1.1414
Training Epoch: 28 [26368/50048]	Loss: 1.0763
Training Epoch: 28 [26496/50048]	Loss: 1.2094
Training Epoch: 28 [26624/50048]	Loss: 1.1869
Training Epoch: 28 [26752/50048]	Loss: 1.0909
Training Epoch: 28 [26880/50048]	Loss: 1.0713
Training Epoch: 28 [27008/50048]	Loss: 0.9262
Training Epoch: 28 [27136/50048]	Loss: 1.0918
Training Epoch: 28 [27264/50048]	Loss: 1.0641
Training Epoch: 28 [27392/50048]	Loss: 1.0148
Training Epoch: 28 [27520/50048]	Loss: 1.1047
Training Epoch: 28 [27648/50048]	Loss: 1.0202
Training Epoch: 28 [27776/50048]	Loss: 1.1010
Training Epoch: 28 [27904/50048]	Loss: 0.9279
Training Epoch: 28 [28032/50048]	Loss: 1.2386
Training Epoch: 28 [28160/50048]	Loss: 0.9381
Training Epoch: 28 [28288/50048]	Loss: 0.9264
Training Epoch: 28 [28416/50048]	Loss: 0.9702
Training Epoch: 28 [28544/50048]	Loss: 0.9224
Training Epoch: 28 [28672/50048]	Loss: 1.0654
Training Epoch: 28 [28800/50048]	Loss: 1.1494
Training Epoch: 28 [28928/50048]	Loss: 1.2543
Training Epoch: 28 [29056/50048]	Loss: 1.0007
Training Epoch: 28 [29184/50048]	Loss: 1.0323
Training Epoch: 28 [29312/50048]	Loss: 1.1827
Training Epoch: 28 [29440/50048]	Loss: 0.8952
Training Epoch: 28 [29568/50048]	Loss: 0.9993
Training Epoch: 28 [29696/50048]	Loss: 1.0562
Training Epoch: 28 [29824/50048]	Loss: 1.1002
Training Epoch: 28 [29952/50048]	Loss: 1.0508
Training Epoch: 28 [30080/50048]	Loss: 0.8478
Training Epoch: 28 [30208/50048]	Loss: 1.1880
Training Epoch: 28 [30336/50048]	Loss: 1.0278
Training Epoch: 28 [30464/50048]	Loss: 0.9981
Training Epoch: 28 [30592/50048]	Loss: 1.3922
Training Epoch: 28 [30720/50048]	Loss: 1.1326
Training Epoch: 28 [30848/50048]	Loss: 1.1783
Training Epoch: 28 [30976/50048]	Loss: 1.0597
Training Epoch: 28 [31104/50048]	Loss: 1.0776
Training Epoch: 28 [31232/50048]	Loss: 0.7962
Training Epoch: 28 [31360/50048]	Loss: 1.0363
Training Epoch: 28 [31488/50048]	Loss: 1.2444
Training Epoch: 28 [31616/50048]	Loss: 1.0418
Training Epoch: 28 [31744/50048]	Loss: 1.1910
Training Epoch: 28 [31872/50048]	Loss: 1.0781
Training Epoch: 28 [32000/50048]	Loss: 1.2843
Training Epoch: 28 [32128/50048]	Loss: 1.1106
Training Epoch: 28 [32256/50048]	Loss: 1.1985
Training Epoch: 28 [32384/50048]	Loss: 1.0203
Training Epoch: 28 [32512/50048]	Loss: 1.0527
Training Epoch: 28 [32640/50048]	Loss: 1.0072
Training Epoch: 28 [32768/50048]	Loss: 0.9298
Training Epoch: 28 [32896/50048]	Loss: 0.9722
Training Epoch: 28 [33024/50048]	Loss: 1.2517
Training Epoch: 28 [33152/50048]	Loss: 1.0774
Training Epoch: 28 [33280/50048]	Loss: 0.8318
Training Epoch: 28 [33408/50048]	Loss: 1.0087
Training Epoch: 28 [33536/50048]	Loss: 1.1049
Training Epoch: 28 [33664/50048]	Loss: 1.0748
Training Epoch: 28 [33792/50048]	Loss: 1.1367
Training Epoch: 28 [33920/50048]	Loss: 1.1369
Training Epoch: 28 [34048/50048]	Loss: 1.1009
Training Epoch: 28 [34176/50048]	Loss: 1.2411
Training Epoch: 28 [34304/50048]	Loss: 1.3513
Training Epoch: 28 [34432/50048]	Loss: 1.0469
Training Epoch: 28 [34560/50048]	Loss: 1.0219
Training Epoch: 28 [34688/50048]	Loss: 1.2338
Training Epoch: 28 [34816/50048]	Loss: 1.1526
Training Epoch: 28 [34944/50048]	Loss: 1.3007
Training Epoch: 28 [35072/50048]	Loss: 1.1592
Training Epoch: 28 [35200/50048]	Loss: 1.0541
Training Epoch: 28 [35328/50048]	Loss: 1.0242
Training Epoch: 28 [35456/50048]	Loss: 1.1756
Training Epoch: 28 [35584/50048]	Loss: 1.0592
Training Epoch: 28 [35712/50048]	Loss: 1.0955
Training Epoch: 28 [35840/50048]	Loss: 1.0669
Training Epoch: 28 [35968/50048]	Loss: 1.0467
Training Epoch: 28 [36096/50048]	Loss: 1.1943
Training Epoch: 28 [36224/50048]	Loss: 1.2905
Training Epoch: 28 [36352/50048]	Loss: 1.1649
Training Epoch: 28 [36480/50048]	Loss: 0.9618
Training Epoch: 28 [36608/50048]	Loss: 1.0400
Training Epoch: 28 [36736/50048]	Loss: 1.0912
Training Epoch: 28 [36864/50048]	Loss: 1.0542
Training Epoch: 28 [36992/50048]	Loss: 1.1392
Training Epoch: 28 [37120/50048]	Loss: 1.0140
Training Epoch: 28 [37248/50048]	Loss: 1.0325
Training Epoch: 28 [37376/50048]	Loss: 1.1362
Training Epoch: 28 [37504/50048]	Loss: 0.9761
Training Epoch: 28 [37632/50048]	Loss: 0.7327
Training Epoch: 28 [37760/50048]	Loss: 1.2072
Training Epoch: 28 [37888/50048]	Loss: 1.0791
Training Epoch: 28 [38016/50048]	Loss: 1.0729
Training Epoch: 28 [38144/50048]	Loss: 1.1626
Training Epoch: 28 [38272/50048]	Loss: 0.9178
Training Epoch: 28 [38400/50048]	Loss: 1.1217
Training Epoch: 28 [38528/50048]	Loss: 1.1896
Training Epoch: 28 [38656/50048]	Loss: 1.1193
Training Epoch: 28 [38784/50048]	Loss: 1.1138
Training Epoch: 28 [38912/50048]	Loss: 1.0006
Training Epoch: 28 [39040/50048]	Loss: 1.0105
Training Epoch: 28 [39168/50048]	Loss: 1.2558
Training Epoch: 28 [39296/50048]	Loss: 1.2852
Training Epoch: 28 [39424/50048]	Loss: 1.1789
Training Epoch: 28 [39552/50048]	Loss: 1.0419
Training Epoch: 28 [39680/50048]	Loss: 1.1336
Training Epoch: 28 [39808/50048]	Loss: 1.3246
Training Epoch: 28 [39936/50048]	Loss: 1.2462
Training Epoch: 28 [40064/50048]	Loss: 1.2187
Training Epoch: 28 [40192/50048]	Loss: 1.2350
Training Epoch: 28 [40320/50048]	Loss: 0.9361
Training Epoch: 28 [40448/50048]	Loss: 0.8570
Training Epoch: 28 [40576/50048]	Loss: 1.2735
Training Epoch: 28 [40704/50048]	Loss: 0.9443
Training Epoch: 28 [40832/50048]	Loss: 1.0335
Training Epoch: 28 [40960/50048]	Loss: 1.0397
Training Epoch: 28 [41088/50048]	Loss: 0.9690
Training Epoch: 28 [41216/50048]	Loss: 1.0005
Training Epoch: 28 [41344/50048]	Loss: 1.0639
Training Epoch: 28 [41472/50048]	Loss: 1.4224
Training Epoch: 28 [41600/50048]	Loss: 0.7938
Training Epoch: 28 [41728/50048]	Loss: 1.1155
Training Epoch: 28 [41856/50048]	Loss: 0.9383
Training Epoch: 28 [41984/50048]	Loss: 1.0538
Training Epoch: 28 [42112/50048]	Loss: 0.8655
Training Epoch: 28 [42240/50048]	Loss: 1.1264
Training Epoch: 28 [42368/50048]	Loss: 1.1655
Training Epoch: 28 [42496/50048]	Loss: 0.8979
Training Epoch: 28 [42624/50048]	Loss: 1.3558
Training Epoch: 28 [42752/50048]	Loss: 1.1739
Training Epoch: 28 [42880/50048]	Loss: 1.1851
Training Epoch: 28 [43008/50048]	Loss: 1.3020
Training Epoch: 28 [43136/50048]	Loss: 0.9979
Training Epoch: 28 [43264/50048]	Loss: 1.0604
Training Epoch: 28 [43392/50048]	Loss: 1.0818
Training Epoch: 28 [43520/50048]	Loss: 0.9779
Training Epoch: 28 [43648/50048]	Loss: 1.0638
Training Epoch: 28 [43776/50048]	Loss: 1.1430
Training Epoch: 28 [43904/50048]	Loss: 1.3897
Training Epoch: 28 [44032/50048]	Loss: 1.1876
Training Epoch: 28 [44160/50048]	Loss: 1.1293
Training Epoch: 28 [44288/50048]	Loss: 0.9925
Training Epoch: 28 [44416/50048]	Loss: 0.9285
Training Epoch: 28 [44544/50048]	Loss: 1.0931
Training Epoch: 28 [44672/50048]	Loss: 1.4166
Training Epoch: 28 [44800/50048]	Loss: 1.1781
Training Epoch: 28 [44928/50048]	Loss: 0.9733
Training Epoch: 28 [45056/50048]	Loss: 1.0256
Training Epoch: 28 [45184/50048]	Loss: 1.0946
Training Epoch: 28 [45312/50048]	Loss: 1.1018
Training Epoch: 28 [45440/50048]	Loss: 1.0272
Training Epoch: 28 [45568/50048]	Loss: 1.1102
Training Epoch: 28 [45696/50048]	Loss: 1.0899
Training Epoch: 28 [45824/50048]	Loss: 0.8460
Training Epoch: 28 [45952/50048]	Loss: 0.9438
Training Epoch: 28 [46080/50048]	Loss: 1.2959
Training Epoch: 28 [46208/50048]	Loss: 1.0127
Training Epoch: 28 [46336/50048]	Loss: 1.1711
Training Epoch: 28 [46464/50048]	Loss: 1.0843
Training Epoch: 28 [46592/50048]	Loss: 1.0555
Training Epoch: 28 [46720/50048]	Loss: 1.0262
Training Epoch: 28 [46848/50048]	Loss: 1.2782
Training Epoch: 28 [46976/50048]	Loss: 1.1797
Training Epoch: 28 [47104/50048]	Loss: 1.0870
Training Epoch: 28 [47232/50048]	Loss: 0.8149
Training Epoch: 28 [47360/50048]	Loss: 0.9997
Training Epoch: 28 [47488/50048]	Loss: 1.2624
Training Epoch: 28 [47616/50048]	Loss: 1.1402
Training Epoch: 28 [47744/50048]	Loss: 1.2308
Training Epoch: 28 [47872/50048]	Loss: 1.0955
Training Epoch: 28 [48000/50048]	Loss: 1.1349
Training Epoch: 28 [48128/50048]	Loss: 1.1614
Training Epoch: 28 [48256/50048]	Loss: 1.1391
Training Epoch: 28 [48384/50048]	Loss: 1.2141
Training Epoch: 28 [48512/50048]	Loss: 1.1975
Training Epoch: 28 [48640/50048]	Loss: 1.2088
Training Epoch: 28 [48768/50048]	Loss: 1.1577
Training Epoch: 28 [48896/50048]	Loss: 1.0498
Training Epoch: 28 [49024/50048]	Loss: 1.1160
Training Epoch: 28 [49152/50048]	Loss: 1.1896
Training Epoch: 28 [49280/50048]	Loss: 0.9675
Training Epoch: 28 [49408/50048]	Loss: 1.0613
Training Epoch: 28 [49536/50048]	Loss: 0.7164
Training Epoch: 28 [49664/50048]	Loss: 1.0199
Training Epoch: 28 [49792/50048]	Loss: 1.1880
Training Epoch: 28 [49920/50048]	Loss: 0.9247
Training Epoch: 28 [50048/50048]	Loss: 1.0872
Validation Epoch: 28, Average loss: 0.0120, Accuracy: 0.5935
Training Epoch: 29 [128/50048]	Loss: 1.0728
Training Epoch: 29 [256/50048]	Loss: 0.8801
Training Epoch: 29 [384/50048]	Loss: 0.9233
Training Epoch: 29 [512/50048]	Loss: 1.0710
Training Epoch: 29 [640/50048]	Loss: 0.9300
Training Epoch: 29 [768/50048]	Loss: 0.9204
Training Epoch: 29 [896/50048]	Loss: 1.0420
Training Epoch: 29 [1024/50048]	Loss: 0.9785
Training Epoch: 29 [1152/50048]	Loss: 0.9999
Training Epoch: 29 [1280/50048]	Loss: 0.9457
Training Epoch: 29 [1408/50048]	Loss: 1.0343
Training Epoch: 29 [1536/50048]	Loss: 1.0166
Training Epoch: 29 [1664/50048]	Loss: 1.0542
Training Epoch: 29 [1792/50048]	Loss: 1.2231
Training Epoch: 29 [1920/50048]	Loss: 0.8868
Training Epoch: 29 [2048/50048]	Loss: 0.9037
Training Epoch: 29 [2176/50048]	Loss: 1.0266
Training Epoch: 29 [2304/50048]	Loss: 0.9163
Training Epoch: 29 [2432/50048]	Loss: 1.0777
Training Epoch: 29 [2560/50048]	Loss: 1.0251
Training Epoch: 29 [2688/50048]	Loss: 1.0796
Training Epoch: 29 [2816/50048]	Loss: 0.8246
Training Epoch: 29 [2944/50048]	Loss: 1.1127
Training Epoch: 29 [3072/50048]	Loss: 1.0309
Training Epoch: 29 [3200/50048]	Loss: 0.7770
Training Epoch: 29 [3328/50048]	Loss: 0.9089
Training Epoch: 29 [3456/50048]	Loss: 1.1061
Training Epoch: 29 [3584/50048]	Loss: 0.9499
Training Epoch: 29 [3712/50048]	Loss: 1.0110
Training Epoch: 29 [3840/50048]	Loss: 1.1160
Training Epoch: 29 [3968/50048]	Loss: 1.0074
Training Epoch: 29 [4096/50048]	Loss: 0.8748
Training Epoch: 29 [4224/50048]	Loss: 1.0790
Training Epoch: 29 [4352/50048]	Loss: 1.1322
Training Epoch: 29 [4480/50048]	Loss: 1.0822
Training Epoch: 29 [4608/50048]	Loss: 0.9081
Training Epoch: 29 [4736/50048]	Loss: 0.8987
Training Epoch: 29 [4864/50048]	Loss: 1.1955
Training Epoch: 29 [4992/50048]	Loss: 1.0750
Training Epoch: 29 [5120/50048]	Loss: 1.1424
Training Epoch: 29 [5248/50048]	Loss: 1.0036
Training Epoch: 29 [5376/50048]	Loss: 1.0550
Training Epoch: 29 [5504/50048]	Loss: 0.8950
Training Epoch: 29 [5632/50048]	Loss: 1.0081
Training Epoch: 29 [5760/50048]	Loss: 0.9912
Training Epoch: 29 [5888/50048]	Loss: 1.0267
Training Epoch: 29 [6016/50048]	Loss: 0.8413
Training Epoch: 29 [6144/50048]	Loss: 1.0392
Training Epoch: 29 [6272/50048]	Loss: 1.1934
Training Epoch: 29 [6400/50048]	Loss: 1.0057
Training Epoch: 29 [6528/50048]	Loss: 1.1524
Training Epoch: 29 [6656/50048]	Loss: 0.9340
Training Epoch: 29 [6784/50048]	Loss: 0.9428
Training Epoch: 29 [6912/50048]	Loss: 0.9590
Training Epoch: 29 [7040/50048]	Loss: 1.0037
Training Epoch: 29 [7168/50048]	Loss: 1.1872
Training Epoch: 29 [7296/50048]	Loss: 0.9993
Training Epoch: 29 [7424/50048]	Loss: 1.1272
Training Epoch: 29 [7552/50048]	Loss: 1.0421
Training Epoch: 29 [7680/50048]	Loss: 0.8756
Training Epoch: 29 [7808/50048]	Loss: 1.0867
Training Epoch: 29 [7936/50048]	Loss: 1.0545
Training Epoch: 29 [8064/50048]	Loss: 1.2415
Training Epoch: 29 [8192/50048]	Loss: 1.0396
Training Epoch: 29 [8320/50048]	Loss: 0.8466
Training Epoch: 29 [8448/50048]	Loss: 0.9752
Training Epoch: 29 [8576/50048]	Loss: 0.8227
Training Epoch: 29 [8704/50048]	Loss: 0.9383
Training Epoch: 29 [8832/50048]	Loss: 0.9987
Training Epoch: 29 [8960/50048]	Loss: 0.9480
Training Epoch: 29 [9088/50048]	Loss: 1.0234
Training Epoch: 29 [9216/50048]	Loss: 0.9245
Training Epoch: 29 [9344/50048]	Loss: 1.0869
Training Epoch: 29 [9472/50048]	Loss: 1.0010
Training Epoch: 29 [9600/50048]	Loss: 1.1514
Training Epoch: 29 [9728/50048]	Loss: 1.1933
Training Epoch: 29 [9856/50048]	Loss: 1.1860
Training Epoch: 29 [9984/50048]	Loss: 1.1955
Training Epoch: 29 [10112/50048]	Loss: 1.0227
Training Epoch: 29 [10240/50048]	Loss: 1.0289
Training Epoch: 29 [10368/50048]	Loss: 1.0477
Training Epoch: 29 [10496/50048]	Loss: 0.8252
Training Epoch: 29 [10624/50048]	Loss: 1.2828
Training Epoch: 29 [10752/50048]	Loss: 0.9866
Training Epoch: 29 [10880/50048]	Loss: 1.1088
Training Epoch: 29 [11008/50048]	Loss: 1.0573
Training Epoch: 29 [11136/50048]	Loss: 1.0430
Training Epoch: 29 [11264/50048]	Loss: 1.0550
Training Epoch: 29 [11392/50048]	Loss: 0.8713
Training Epoch: 29 [11520/50048]	Loss: 1.0374
Training Epoch: 29 [11648/50048]	Loss: 0.9405
Training Epoch: 29 [11776/50048]	Loss: 1.1901
Training Epoch: 29 [11904/50048]	Loss: 1.1943
Training Epoch: 29 [12032/50048]	Loss: 1.2241
Training Epoch: 29 [12160/50048]	Loss: 0.9074
Training Epoch: 29 [12288/50048]	Loss: 0.8833
Training Epoch: 29 [12416/50048]	Loss: 0.8948
Training Epoch: 29 [12544/50048]	Loss: 1.1091
Training Epoch: 29 [12672/50048]	Loss: 1.2324
Training Epoch: 29 [12800/50048]	Loss: 1.0707
Training Epoch: 29 [12928/50048]	Loss: 1.1017
Training Epoch: 29 [13056/50048]	Loss: 1.0637
Training Epoch: 29 [13184/50048]	Loss: 1.0256
Training Epoch: 29 [13312/50048]	Loss: 1.0731
Training Epoch: 29 [13440/50048]	Loss: 0.9917
Training Epoch: 29 [13568/50048]	Loss: 0.9670
Training Epoch: 29 [13696/50048]	Loss: 0.9842
Training Epoch: 29 [13824/50048]	Loss: 1.0741
Training Epoch: 29 [13952/50048]	Loss: 1.0497
Training Epoch: 29 [14080/50048]	Loss: 0.7837
Training Epoch: 29 [14208/50048]	Loss: 0.8852
Training Epoch: 29 [14336/50048]	Loss: 1.1759
Training Epoch: 29 [14464/50048]	Loss: 1.1993
Training Epoch: 29 [14592/50048]	Loss: 0.9401
Training Epoch: 29 [14720/50048]	Loss: 0.9182
Training Epoch: 29 [14848/50048]	Loss: 0.8597
Training Epoch: 29 [14976/50048]	Loss: 0.7828
Training Epoch: 29 [15104/50048]	Loss: 1.0432
Training Epoch: 29 [15232/50048]	Loss: 1.0992
Training Epoch: 29 [15360/50048]	Loss: 0.9909
Training Epoch: 29 [15488/50048]	Loss: 1.1278
Training Epoch: 29 [15616/50048]	Loss: 0.7786
Training Epoch: 29 [15744/50048]	Loss: 1.1846
Training Epoch: 29 [15872/50048]	Loss: 0.9027
Training Epoch: 29 [16000/50048]	Loss: 1.2401
Training Epoch: 29 [16128/50048]	Loss: 0.8881
Training Epoch: 29 [16256/50048]	Loss: 1.2345
Training Epoch: 29 [16384/50048]	Loss: 1.0851
Training Epoch: 29 [16512/50048]	Loss: 1.3127
Training Epoch: 29 [16640/50048]	Loss: 1.2372
Training Epoch: 29 [16768/50048]	Loss: 1.1192
Training Epoch: 29 [16896/50048]	Loss: 1.0798
Training Epoch: 29 [17024/50048]	Loss: 0.9038
Training Epoch: 29 [17152/50048]	Loss: 1.0805
Training Epoch: 29 [17280/50048]	Loss: 1.0795
Training Epoch: 29 [17408/50048]	Loss: 1.1356
Training Epoch: 29 [17536/50048]	Loss: 0.9182
Training Epoch: 29 [17664/50048]	Loss: 1.0435
Training Epoch: 29 [17792/50048]	Loss: 0.9767
Training Epoch: 29 [17920/50048]	Loss: 0.9857
Training Epoch: 29 [18048/50048]	Loss: 0.9082
Training Epoch: 29 [18176/50048]	Loss: 1.0280
Training Epoch: 29 [18304/50048]	Loss: 1.0191
Training Epoch: 29 [18432/50048]	Loss: 1.0440
Training Epoch: 29 [18560/50048]	Loss: 0.9797
Training Epoch: 29 [18688/50048]	Loss: 0.9755
Training Epoch: 29 [18816/50048]	Loss: 0.7235
Training Epoch: 29 [18944/50048]	Loss: 1.0943
Training Epoch: 29 [19072/50048]	Loss: 1.0266
Training Epoch: 29 [19200/50048]	Loss: 1.2367
Training Epoch: 29 [19328/50048]	Loss: 1.0617
Training Epoch: 29 [19456/50048]	Loss: 1.1639
Training Epoch: 29 [19584/50048]	Loss: 0.9236
Training Epoch: 29 [19712/50048]	Loss: 0.9920
Training Epoch: 29 [19840/50048]	Loss: 1.0222
Training Epoch: 29 [19968/50048]	Loss: 0.9917
Training Epoch: 29 [20096/50048]	Loss: 1.0156
Training Epoch: 29 [20224/50048]	Loss: 1.0172
Training Epoch: 29 [20352/50048]	Loss: 1.0572
Training Epoch: 29 [20480/50048]	Loss: 1.1170
Training Epoch: 29 [20608/50048]	Loss: 1.0671
Training Epoch: 29 [20736/50048]	Loss: 1.1345
Training Epoch: 29 [20864/50048]	Loss: 0.9337
Training Epoch: 29 [20992/50048]	Loss: 1.0411
Training Epoch: 29 [21120/50048]	Loss: 1.0124
Training Epoch: 29 [21248/50048]	Loss: 1.0207
Training Epoch: 29 [21376/50048]	Loss: 1.1417
Training Epoch: 29 [21504/50048]	Loss: 1.0523
Training Epoch: 29 [21632/50048]	Loss: 0.9387
Training Epoch: 29 [21760/50048]	Loss: 1.0845
Training Epoch: 29 [21888/50048]	Loss: 0.9913
Training Epoch: 29 [22016/50048]	Loss: 1.0760
Training Epoch: 29 [22144/50048]	Loss: 0.8357
Training Epoch: 29 [22272/50048]	Loss: 0.8523
Training Epoch: 29 [22400/50048]	Loss: 1.0903
Training Epoch: 29 [22528/50048]	Loss: 0.8812
Training Epoch: 29 [22656/50048]	Loss: 1.2104
Training Epoch: 29 [22784/50048]	Loss: 0.8283
Training Epoch: 29 [22912/50048]	Loss: 1.1581
Training Epoch: 29 [23040/50048]	Loss: 1.0708
Training Epoch: 29 [23168/50048]	Loss: 0.9506
Training Epoch: 29 [23296/50048]	Loss: 1.1384
Training Epoch: 29 [23424/50048]	Loss: 0.7537
Training Epoch: 29 [23552/50048]	Loss: 1.0219
Training Epoch: 29 [23680/50048]	Loss: 1.0540
Training Epoch: 29 [23808/50048]	Loss: 1.0912
Training Epoch: 29 [23936/50048]	Loss: 1.0126
Training Epoch: 29 [24064/50048]	Loss: 0.9038
Training Epoch: 29 [24192/50048]	Loss: 0.9492
Training Epoch: 29 [24320/50048]	Loss: 1.2764
Training Epoch: 29 [24448/50048]	Loss: 1.0273
Training Epoch: 29 [24576/50048]	Loss: 1.1619
Training Epoch: 29 [24704/50048]	Loss: 1.0978
Training Epoch: 29 [24832/50048]	Loss: 1.0550
Training Epoch: 29 [24960/50048]	Loss: 1.1148
Training Epoch: 29 [25088/50048]	Loss: 1.2210
Training Epoch: 29 [25216/50048]	Loss: 1.0342
Training Epoch: 29 [25344/50048]	Loss: 1.0307
Training Epoch: 29 [25472/50048]	Loss: 1.1686
Training Epoch: 29 [25600/50048]	Loss: 1.1751
Training Epoch: 29 [25728/50048]	Loss: 1.1710
Training Epoch: 29 [25856/50048]	Loss: 0.9413
Training Epoch: 29 [25984/50048]	Loss: 1.0774
Training Epoch: 29 [26112/50048]	Loss: 0.8974
Training Epoch: 29 [26240/50048]	Loss: 1.0525
Training Epoch: 29 [26368/50048]	Loss: 1.1648
Training Epoch: 29 [26496/50048]	Loss: 1.2156
Training Epoch: 29 [26624/50048]	Loss: 0.8485
Training Epoch: 29 [26752/50048]	Loss: 1.2607
Training Epoch: 29 [26880/50048]	Loss: 1.2073
Training Epoch: 29 [27008/50048]	Loss: 1.2236
Training Epoch: 29 [27136/50048]	Loss: 0.9935
Training Epoch: 29 [27264/50048]	Loss: 1.0221
Training Epoch: 29 [27392/50048]	Loss: 1.1105
Training Epoch: 29 [27520/50048]	Loss: 1.1968
Training Epoch: 29 [27648/50048]	Loss: 0.9436
Training Epoch: 29 [27776/50048]	Loss: 1.2834
Training Epoch: 29 [27904/50048]	Loss: 0.9986
Training Epoch: 29 [28032/50048]	Loss: 0.8506
Training Epoch: 29 [28160/50048]	Loss: 1.0551
Training Epoch: 29 [28288/50048]	Loss: 0.9634
Training Epoch: 29 [28416/50048]	Loss: 0.8684
Training Epoch: 29 [28544/50048]	Loss: 1.0593
Training Epoch: 29 [28672/50048]	Loss: 0.8647
Training Epoch: 29 [28800/50048]	Loss: 1.0286
Training Epoch: 29 [28928/50048]	Loss: 1.2152
Training Epoch: 29 [29056/50048]	Loss: 1.1496
Training Epoch: 29 [29184/50048]	Loss: 1.1848
Training Epoch: 29 [29312/50048]	Loss: 0.9597
Training Epoch: 29 [29440/50048]	Loss: 1.1154
Training Epoch: 29 [29568/50048]	Loss: 1.0563
Training Epoch: 29 [29696/50048]	Loss: 1.0398
Training Epoch: 29 [29824/50048]	Loss: 1.0834
Training Epoch: 29 [29952/50048]	Loss: 1.0893
Training Epoch: 29 [30080/50048]	Loss: 0.9824
Training Epoch: 29 [30208/50048]	Loss: 1.0213
Training Epoch: 29 [30336/50048]	Loss: 1.0881
Training Epoch: 29 [30464/50048]	Loss: 1.0205
Training Epoch: 29 [30592/50048]	Loss: 1.1264
Training Epoch: 29 [30720/50048]	Loss: 1.0670
Training Epoch: 29 [30848/50048]	Loss: 1.0292
Training Epoch: 29 [30976/50048]	Loss: 1.0886
Training Epoch: 29 [31104/50048]	Loss: 1.3094
Training Epoch: 29 [31232/50048]	Loss: 1.0528
Training Epoch: 29 [31360/50048]	Loss: 1.1856
Training Epoch: 29 [31488/50048]	Loss: 1.1720
Training Epoch: 29 [31616/50048]	Loss: 1.0776
Training Epoch: 29 [31744/50048]	Loss: 0.7400
Training Epoch: 29 [31872/50048]	Loss: 1.2366
Training Epoch: 29 [32000/50048]	Loss: 1.0267
Training Epoch: 29 [32128/50048]	Loss: 0.9466
Training Epoch: 29 [32256/50048]	Loss: 0.9458
Training Epoch: 29 [32384/50048]	Loss: 0.7853
Training Epoch: 29 [32512/50048]	Loss: 0.8943
Training Epoch: 29 [32640/50048]	Loss: 0.9868
Training Epoch: 29 [32768/50048]	Loss: 1.3000
Training Epoch: 29 [32896/50048]	Loss: 1.0491
Training Epoch: 29 [33024/50048]	Loss: 1.1157
Training Epoch: 29 [33152/50048]	Loss: 0.9199
Training Epoch: 29 [33280/50048]	Loss: 1.4032
Training Epoch: 29 [33408/50048]	Loss: 0.8866
Training Epoch: 29 [33536/50048]	Loss: 0.8776
Training Epoch: 29 [33664/50048]	Loss: 0.9592
Training Epoch: 29 [33792/50048]	Loss: 1.0839
Training Epoch: 29 [33920/50048]	Loss: 0.9139
Training Epoch: 29 [34048/50048]	Loss: 1.1922
Training Epoch: 29 [34176/50048]	Loss: 1.0524
Training Epoch: 29 [34304/50048]	Loss: 1.0511
Training Epoch: 29 [34432/50048]	Loss: 0.8968
Training Epoch: 29 [34560/50048]	Loss: 1.0535
Training Epoch: 29 [34688/50048]	Loss: 1.0130
Training Epoch: 29 [34816/50048]	Loss: 0.9867
Training Epoch: 29 [34944/50048]	Loss: 1.0394
Training Epoch: 29 [35072/50048]	Loss: 0.9294
Training Epoch: 29 [35200/50048]	Loss: 1.2402
Training Epoch: 29 [35328/50048]	Loss: 0.9543
Training Epoch: 29 [35456/50048]	Loss: 1.3178
Training Epoch: 29 [35584/50048]	Loss: 0.8355
Training Epoch: 29 [35712/50048]	Loss: 1.1305
Training Epoch: 29 [35840/50048]	Loss: 1.0749
Training Epoch: 29 [35968/50048]	Loss: 1.0443
Training Epoch: 29 [36096/50048]	Loss: 1.1866
Training Epoch: 29 [36224/50048]	Loss: 1.0062
Training Epoch: 29 [36352/50048]	Loss: 1.1830
Training Epoch: 29 [36480/50048]	Loss: 1.1347
Training Epoch: 29 [36608/50048]	Loss: 1.0055
Training Epoch: 29 [36736/50048]	Loss: 1.0303
Training Epoch: 29 [36864/50048]	Loss: 0.9232
Training Epoch: 29 [36992/50048]	Loss: 0.8208
Training Epoch: 29 [37120/50048]	Loss: 1.1247
Training Epoch: 29 [37248/50048]	Loss: 1.0947
Training Epoch: 29 [37376/50048]	Loss: 1.1448
Training Epoch: 29 [37504/50048]	Loss: 1.0333
Training Epoch: 29 [37632/50048]	Loss: 1.0339
Training Epoch: 29 [37760/50048]	Loss: 1.1583
Training Epoch: 29 [37888/50048]	Loss: 1.0515
Training Epoch: 29 [38016/50048]	Loss: 1.0439
Training Epoch: 29 [38144/50048]	Loss: 1.0114
Training Epoch: 29 [38272/50048]	Loss: 1.0499
Training Epoch: 29 [38400/50048]	Loss: 1.0271
Training Epoch: 29 [38528/50048]	Loss: 1.3212
Training Epoch: 29 [38656/50048]	Loss: 1.2548
Training Epoch: 29 [38784/50048]	Loss: 0.9979
Training Epoch: 29 [38912/50048]	Loss: 1.1198
Training Epoch: 29 [39040/50048]	Loss: 0.8266
Training Epoch: 29 [39168/50048]	Loss: 1.3407
Training Epoch: 29 [39296/50048]	Loss: 1.1192
Training Epoch: 29 [39424/50048]	Loss: 0.9998
Training Epoch: 29 [39552/50048]	Loss: 1.2331
Training Epoch: 29 [39680/50048]	Loss: 1.0848
Training Epoch: 29 [39808/50048]	Loss: 1.1824
Training Epoch: 29 [39936/50048]	Loss: 0.9527
Training Epoch: 29 [40064/50048]	Loss: 1.1435
Training Epoch: 29 [40192/50048]	Loss: 0.8466
Training Epoch: 29 [40320/50048]	Loss: 1.0294
Training Epoch: 29 [40448/50048]	Loss: 1.1343
Training Epoch: 29 [40576/50048]	Loss: 1.1405
Training Epoch: 29 [40704/50048]	Loss: 0.9743
Training Epoch: 29 [40832/50048]	Loss: 1.1157
Training Epoch: 29 [40960/50048]	Loss: 0.9569
Training Epoch: 29 [41088/50048]	Loss: 1.1481
Training Epoch: 29 [41216/50048]	Loss: 1.0963
Training Epoch: 29 [41344/50048]	Loss: 0.9511
Training Epoch: 29 [41472/50048]	Loss: 1.1908
Training Epoch: 29 [41600/50048]	Loss: 1.0226
Training Epoch: 29 [41728/50048]	Loss: 0.9571
Training Epoch: 29 [41856/50048]	Loss: 0.8989
Training Epoch: 29 [41984/50048]	Loss: 0.8516
Training Epoch: 29 [42112/50048]	Loss: 0.7158
Training Epoch: 29 [42240/50048]	Loss: 1.0852
Training Epoch: 29 [42368/50048]	Loss: 1.1398
Training Epoch: 29 [42496/50048]	Loss: 1.0648
Training Epoch: 29 [42624/50048]	Loss: 1.0947
Training Epoch: 29 [42752/50048]	Loss: 0.9784
Training Epoch: 29 [42880/50048]	Loss: 1.0615
Training Epoch: 29 [43008/50048]	Loss: 1.3159
Training Epoch: 29 [43136/50048]	Loss: 1.0284
Training Epoch: 29 [43264/50048]	Loss: 1.0941
Training Epoch: 29 [43392/50048]	Loss: 1.1943
Training Epoch: 29 [43520/50048]	Loss: 0.8724
Training Epoch: 29 [43648/50048]	Loss: 0.9196
Training Epoch: 29 [43776/50048]	Loss: 1.1357
Training Epoch: 29 [43904/50048]	Loss: 1.2051
Training Epoch: 29 [44032/50048]	Loss: 1.1961
Training Epoch: 29 [44160/50048]	Loss: 1.1412
Training Epoch: 29 [44288/50048]	Loss: 1.1825
Training Epoch: 29 [44416/50048]	Loss: 1.1223
Training Epoch: 29 [44544/50048]	Loss: 1.1295
Training Epoch: 29 [44672/50048]	Loss: 1.1273
Training Epoch: 29 [44800/50048]	Loss: 0.9871
Training Epoch: 29 [44928/50048]	Loss: 1.2014
Training Epoch: 29 [45056/50048]	Loss: 1.2376
Training Epoch: 29 [45184/50048]	Loss: 0.8915
Training Epoch: 29 [45312/50048]	Loss: 1.0171
Training Epoch: 29 [45440/50048]	Loss: 1.2431
Training Epoch: 29 [45568/50048]	Loss: 0.9552
Training Epoch: 29 [45696/50048]	Loss: 1.0027
Training Epoch: 29 [45824/50048]	Loss: 1.1639
Training Epoch: 29 [45952/50048]	Loss: 1.0559
Training Epoch: 29 [46080/50048]	Loss: 1.2422
Training Epoch: 29 [46208/50048]	Loss: 1.0832
Training Epoch: 29 [46336/50048]	Loss: 0.9707
Training Epoch: 29 [46464/50048]	Loss: 0.9723
Training Epoch: 29 [46592/50048]	Loss: 1.0505
Training Epoch: 29 [46720/50048]	Loss: 1.1566
Training Epoch: 29 [46848/50048]	Loss: 0.9994
Training Epoch: 29 [46976/50048]	Loss: 1.1098
Training Epoch: 29 [47104/50048]	Loss: 1.1093
Training Epoch: 29 [47232/50048]	Loss: 0.9741
Training Epoch: 29 [47360/50048]	Loss: 1.0797
Training Epoch: 29 [47488/50048]	Loss: 1.0876
Training Epoch: 29 [47616/50048]	Loss: 1.1470
Training Epoch: 29 [47744/50048]	Loss: 1.0728
Training Epoch: 29 [47872/50048]	Loss: 0.9294
Training Epoch: 29 [48000/50048]	Loss: 1.0298
Training Epoch: 29 [48128/50048]	Loss: 1.0977
Training Epoch: 29 [48256/50048]	Loss: 1.1192
Training Epoch: 29 [48384/50048]	Loss: 1.3409
Training Epoch: 29 [48512/50048]	Loss: 1.1176
Training Epoch: 29 [48640/50048]	Loss: 1.2737
Training Epoch: 29 [48768/50048]	Loss: 1.2605
Training Epoch: 29 [48896/50048]	Loss: 0.9833
Training Epoch: 29 [49024/50048]	Loss: 0.8244
Training Epoch: 29 [49152/50048]	Loss: 1.3810
Training Epoch: 29 [49280/50048]	Loss: 1.3462
Training Epoch: 29 [49408/50048]	Loss: 0.9602
Training Epoch: 29 [49536/50048]	Loss: 1.0333
Training Epoch: 29 [49664/50048]	Loss: 1.1418
Training Epoch: 29 [49792/50048]	Loss: 1.4678
Training Epoch: 29 [49920/50048]	Loss: 1.1183
Training Epoch: 29 [50048/50048]	Loss: 1.0288
Validation Epoch: 29, Average loss: 0.0123, Accuracy: 0.5896
Training Epoch: 30 [128/50048]	Loss: 1.0191
Training Epoch: 30 [256/50048]	Loss: 1.1464
Training Epoch: 30 [384/50048]	Loss: 0.9340
Training Epoch: 30 [512/50048]	Loss: 1.0486
Training Epoch: 30 [640/50048]	Loss: 0.9822
Training Epoch: 30 [768/50048]	Loss: 1.0350
Training Epoch: 30 [896/50048]	Loss: 0.9523
Training Epoch: 30 [1024/50048]	Loss: 1.1127
Training Epoch: 30 [1152/50048]	Loss: 0.8710
Training Epoch: 30 [1280/50048]	Loss: 0.8844
Training Epoch: 30 [1408/50048]	Loss: 0.8621
Training Epoch: 30 [1536/50048]	Loss: 0.8762
Training Epoch: 30 [1664/50048]	Loss: 1.0792
Training Epoch: 30 [1792/50048]	Loss: 0.8761
Training Epoch: 30 [1920/50048]	Loss: 0.8109
Training Epoch: 30 [2048/50048]	Loss: 1.0245
Training Epoch: 30 [2176/50048]	Loss: 0.9329
Training Epoch: 30 [2304/50048]	Loss: 0.9209
Training Epoch: 30 [2432/50048]	Loss: 0.7757
Training Epoch: 30 [2560/50048]	Loss: 0.9155
Training Epoch: 30 [2688/50048]	Loss: 1.0735
Training Epoch: 30 [2816/50048]	Loss: 1.0200
Training Epoch: 30 [2944/50048]	Loss: 0.9191
Training Epoch: 30 [3072/50048]	Loss: 1.1248
Training Epoch: 30 [3200/50048]	Loss: 1.1108
Training Epoch: 30 [3328/50048]	Loss: 0.9708
Training Epoch: 30 [3456/50048]	Loss: 1.0257
Training Epoch: 30 [3584/50048]	Loss: 1.0425
Training Epoch: 30 [3712/50048]	Loss: 0.8863
Training Epoch: 30 [3840/50048]	Loss: 0.9900
Training Epoch: 30 [3968/50048]	Loss: 1.0257
Training Epoch: 30 [4096/50048]	Loss: 1.3334
Training Epoch: 30 [4224/50048]	Loss: 1.0233
Training Epoch: 30 [4352/50048]	Loss: 1.0540
Training Epoch: 30 [4480/50048]	Loss: 1.0848
Training Epoch: 30 [4608/50048]	Loss: 1.1331
Training Epoch: 30 [4736/50048]	Loss: 1.0299
Training Epoch: 30 [4864/50048]	Loss: 1.1498
Training Epoch: 30 [4992/50048]	Loss: 0.8685
Training Epoch: 30 [5120/50048]	Loss: 0.8744
Training Epoch: 30 [5248/50048]	Loss: 1.0223
Training Epoch: 30 [5376/50048]	Loss: 0.8247
Training Epoch: 30 [5504/50048]	Loss: 0.9541
Training Epoch: 30 [5632/50048]	Loss: 0.9788
Training Epoch: 30 [5760/50048]	Loss: 0.9765
Training Epoch: 30 [5888/50048]	Loss: 1.0303
Training Epoch: 30 [6016/50048]	Loss: 0.9676
Training Epoch: 30 [6144/50048]	Loss: 1.0424
Training Epoch: 30 [6272/50048]	Loss: 1.2304
Training Epoch: 30 [6400/50048]	Loss: 0.8803
Training Epoch: 30 [6528/50048]	Loss: 0.9990
Training Epoch: 30 [6656/50048]	Loss: 1.0530
Training Epoch: 30 [6784/50048]	Loss: 1.1740
Training Epoch: 30 [6912/50048]	Loss: 1.0760
Training Epoch: 30 [7040/50048]	Loss: 1.0564
Training Epoch: 30 [7168/50048]	Loss: 1.0501
Training Epoch: 30 [7296/50048]	Loss: 0.8380
Training Epoch: 30 [7424/50048]	Loss: 0.9536
Training Epoch: 30 [7552/50048]	Loss: 1.0594
Training Epoch: 30 [7680/50048]	Loss: 0.9122
Training Epoch: 30 [7808/50048]	Loss: 1.0601
Training Epoch: 30 [7936/50048]	Loss: 0.7858
Training Epoch: 30 [8064/50048]	Loss: 1.1352
Training Epoch: 30 [8192/50048]	Loss: 1.0803
Training Epoch: 30 [8320/50048]	Loss: 0.8354
Training Epoch: 30 [8448/50048]	Loss: 0.9975
Training Epoch: 30 [8576/50048]	Loss: 0.9074
Training Epoch: 30 [8704/50048]	Loss: 0.9563
Training Epoch: 30 [8832/50048]	Loss: 0.8920
Training Epoch: 30 [8960/50048]	Loss: 1.0031
Training Epoch: 30 [9088/50048]	Loss: 0.8226
Training Epoch: 30 [9216/50048]	Loss: 0.8229
Training Epoch: 30 [9344/50048]	Loss: 0.8818
Training Epoch: 30 [9472/50048]	Loss: 1.1570
Training Epoch: 30 [9600/50048]	Loss: 0.9327
Training Epoch: 30 [9728/50048]	Loss: 0.9358
Training Epoch: 30 [9856/50048]	Loss: 1.1266
Training Epoch: 30 [9984/50048]	Loss: 0.9248
Training Epoch: 30 [10112/50048]	Loss: 1.1187
Training Epoch: 30 [10240/50048]	Loss: 0.8763
Training Epoch: 30 [10368/50048]	Loss: 0.9270
Training Epoch: 30 [10496/50048]	Loss: 0.8588
Training Epoch: 30 [10624/50048]	Loss: 1.1545
Training Epoch: 30 [10752/50048]	Loss: 0.9669
Training Epoch: 30 [10880/50048]	Loss: 0.9785
Training Epoch: 30 [11008/50048]	Loss: 0.9972
Training Epoch: 30 [11136/50048]	Loss: 1.0474
Training Epoch: 30 [11264/50048]	Loss: 1.2825
Training Epoch: 30 [11392/50048]	Loss: 1.1365
Training Epoch: 30 [11520/50048]	Loss: 0.9572
Training Epoch: 30 [11648/50048]	Loss: 0.9230
Training Epoch: 30 [11776/50048]	Loss: 0.9045
Training Epoch: 30 [11904/50048]	Loss: 1.0326
Training Epoch: 30 [12032/50048]	Loss: 1.0327
Training Epoch: 30 [12160/50048]	Loss: 1.1711
Training Epoch: 30 [12288/50048]	Loss: 0.9644
Training Epoch: 30 [12416/50048]	Loss: 0.9449
Training Epoch: 30 [12544/50048]	Loss: 0.9711
Training Epoch: 30 [12672/50048]	Loss: 1.0265
Training Epoch: 30 [12800/50048]	Loss: 1.0768
Training Epoch: 30 [12928/50048]	Loss: 1.1381
Training Epoch: 30 [13056/50048]	Loss: 0.9688
Training Epoch: 30 [13184/50048]	Loss: 0.8585
Training Epoch: 30 [13312/50048]	Loss: 1.1607
Training Epoch: 30 [13440/50048]	Loss: 1.1724
Training Epoch: 30 [13568/50048]	Loss: 0.9968
Training Epoch: 30 [13696/50048]	Loss: 1.3762
Training Epoch: 30 [13824/50048]	Loss: 1.1469
Training Epoch: 30 [13952/50048]	Loss: 1.0064
Training Epoch: 30 [14080/50048]	Loss: 1.0648
Training Epoch: 30 [14208/50048]	Loss: 0.8917
Training Epoch: 30 [14336/50048]	Loss: 1.1022
Training Epoch: 30 [14464/50048]	Loss: 1.1001
Training Epoch: 30 [14592/50048]	Loss: 0.9752
Training Epoch: 30 [14720/50048]	Loss: 1.2367
Training Epoch: 30 [14848/50048]	Loss: 1.0506
Training Epoch: 30 [14976/50048]	Loss: 0.9239
Training Epoch: 30 [15104/50048]	Loss: 1.0768
Training Epoch: 30 [15232/50048]	Loss: 1.0511
Training Epoch: 30 [15360/50048]	Loss: 1.1526
Training Epoch: 30 [15488/50048]	Loss: 0.9647
Training Epoch: 30 [15616/50048]	Loss: 1.1547
Training Epoch: 30 [15744/50048]	Loss: 1.0628
Training Epoch: 30 [15872/50048]	Loss: 1.2245
Training Epoch: 30 [16000/50048]	Loss: 0.9388
Training Epoch: 30 [16128/50048]	Loss: 1.1215
Training Epoch: 30 [16256/50048]	Loss: 0.9419
Training Epoch: 30 [16384/50048]	Loss: 1.0556
Training Epoch: 30 [16512/50048]	Loss: 0.9114
Training Epoch: 30 [16640/50048]	Loss: 1.1058
Training Epoch: 30 [16768/50048]	Loss: 0.8932
Training Epoch: 30 [16896/50048]	Loss: 1.1213
Training Epoch: 30 [17024/50048]	Loss: 0.9802
Training Epoch: 30 [17152/50048]	Loss: 1.0067
Training Epoch: 30 [17280/50048]	Loss: 0.8643
Training Epoch: 30 [17408/50048]	Loss: 1.0204
Training Epoch: 30 [17536/50048]	Loss: 1.2639
Training Epoch: 30 [17664/50048]	Loss: 1.0080
Training Epoch: 30 [17792/50048]	Loss: 1.0752
Training Epoch: 30 [17920/50048]	Loss: 1.0889
Training Epoch: 30 [18048/50048]	Loss: 0.9680
Training Epoch: 30 [18176/50048]	Loss: 0.9686
Training Epoch: 30 [18304/50048]	Loss: 1.1941
Training Epoch: 30 [18432/50048]	Loss: 1.0160
Training Epoch: 30 [18560/50048]	Loss: 1.0563
Training Epoch: 30 [18688/50048]	Loss: 0.9056
Training Epoch: 30 [18816/50048]	Loss: 1.0000
Training Epoch: 30 [18944/50048]	Loss: 1.0437
Training Epoch: 30 [19072/50048]	Loss: 1.0846
Training Epoch: 30 [19200/50048]	Loss: 0.8796
Training Epoch: 30 [19328/50048]	Loss: 1.0126
Training Epoch: 30 [19456/50048]	Loss: 0.7561
Training Epoch: 30 [19584/50048]	Loss: 1.1286
Training Epoch: 30 [19712/50048]	Loss: 1.1673
Training Epoch: 30 [19840/50048]	Loss: 0.9711
Training Epoch: 30 [19968/50048]	Loss: 0.9669
Training Epoch: 30 [20096/50048]	Loss: 1.1955
Training Epoch: 30 [20224/50048]	Loss: 1.0084
Training Epoch: 30 [20352/50048]	Loss: 1.0242
Training Epoch: 30 [20480/50048]	Loss: 0.9978
Training Epoch: 30 [20608/50048]	Loss: 1.0741
Training Epoch: 30 [20736/50048]	Loss: 0.9846
Training Epoch: 30 [20864/50048]	Loss: 1.0139
Training Epoch: 30 [20992/50048]	Loss: 1.0234
Training Epoch: 30 [21120/50048]	Loss: 1.0682
Training Epoch: 30 [21248/50048]	Loss: 1.1348
Training Epoch: 30 [21376/50048]	Loss: 1.0124
Training Epoch: 30 [21504/50048]	Loss: 0.8169
Training Epoch: 30 [21632/50048]	Loss: 0.9225
Training Epoch: 30 [21760/50048]	Loss: 1.0768
Training Epoch: 30 [21888/50048]	Loss: 1.1448
Training Epoch: 30 [22016/50048]	Loss: 1.1236
Training Epoch: 30 [22144/50048]	Loss: 0.7656
Training Epoch: 30 [22272/50048]	Loss: 1.0063
Training Epoch: 30 [22400/50048]	Loss: 0.9360
Training Epoch: 30 [22528/50048]	Loss: 0.9403
Training Epoch: 30 [22656/50048]	Loss: 0.9593
Training Epoch: 30 [22784/50048]	Loss: 1.0431
Training Epoch: 30 [22912/50048]	Loss: 0.8893
Training Epoch: 30 [23040/50048]	Loss: 0.8428
Training Epoch: 30 [23168/50048]	Loss: 1.0022
Training Epoch: 30 [23296/50048]	Loss: 1.2096
Training Epoch: 30 [23424/50048]	Loss: 1.1106
Training Epoch: 30 [23552/50048]	Loss: 0.9077
Training Epoch: 30 [23680/50048]	Loss: 1.1369
Training Epoch: 30 [23808/50048]	Loss: 0.8796
Training Epoch: 30 [23936/50048]	Loss: 0.9600
Training Epoch: 30 [24064/50048]	Loss: 0.8780
Training Epoch: 30 [24192/50048]	Loss: 0.9984
Training Epoch: 30 [24320/50048]	Loss: 1.0909
Training Epoch: 30 [24448/50048]	Loss: 0.8670
Training Epoch: 30 [24576/50048]	Loss: 0.7824
Training Epoch: 30 [24704/50048]	Loss: 0.9370
Training Epoch: 30 [24832/50048]	Loss: 0.9528
Training Epoch: 30 [24960/50048]	Loss: 0.8977
Training Epoch: 30 [25088/50048]	Loss: 0.9018
Training Epoch: 30 [25216/50048]	Loss: 0.8218
Training Epoch: 30 [25344/50048]	Loss: 1.1707
Training Epoch: 30 [25472/50048]	Loss: 1.1132
Training Epoch: 30 [25600/50048]	Loss: 1.1590
Training Epoch: 30 [25728/50048]	Loss: 1.0390
Training Epoch: 30 [25856/50048]	Loss: 1.1069
Training Epoch: 30 [25984/50048]	Loss: 0.8776
Training Epoch: 30 [26112/50048]	Loss: 0.8940
Training Epoch: 30 [26240/50048]	Loss: 1.0324
Training Epoch: 30 [26368/50048]	Loss: 0.9550
Training Epoch: 30 [26496/50048]	Loss: 1.0915
Training Epoch: 30 [26624/50048]	Loss: 0.9716
Training Epoch: 30 [26752/50048]	Loss: 1.1075
Training Epoch: 30 [26880/50048]	Loss: 1.0602
Training Epoch: 30 [27008/50048]	Loss: 1.0081
Training Epoch: 30 [27136/50048]	Loss: 0.9010
Training Epoch: 30 [27264/50048]	Loss: 1.1363
Training Epoch: 30 [27392/50048]	Loss: 1.1636
Training Epoch: 30 [27520/50048]	Loss: 1.1649
Training Epoch: 30 [27648/50048]	Loss: 0.8513
Training Epoch: 30 [27776/50048]	Loss: 1.2278
Training Epoch: 30 [27904/50048]	Loss: 0.9719
Training Epoch: 30 [28032/50048]	Loss: 0.9844
Training Epoch: 30 [28160/50048]	Loss: 1.1156
Training Epoch: 30 [28288/50048]	Loss: 1.0035
Training Epoch: 30 [28416/50048]	Loss: 1.2059
Training Epoch: 30 [28544/50048]	Loss: 1.0989
Training Epoch: 30 [28672/50048]	Loss: 0.8314
Training Epoch: 30 [28800/50048]	Loss: 0.7575
Training Epoch: 30 [28928/50048]	Loss: 0.9714
Training Epoch: 30 [29056/50048]	Loss: 0.8689
Training Epoch: 30 [29184/50048]	Loss: 1.1186
Training Epoch: 30 [29312/50048]	Loss: 1.0363
Training Epoch: 30 [29440/50048]	Loss: 0.9101
Training Epoch: 30 [29568/50048]	Loss: 1.0607
Training Epoch: 30 [29696/50048]	Loss: 0.8866
Training Epoch: 30 [29824/50048]	Loss: 1.2240
Training Epoch: 30 [29952/50048]	Loss: 0.9732
Training Epoch: 30 [30080/50048]	Loss: 0.9806
Training Epoch: 30 [30208/50048]	Loss: 0.8816
Training Epoch: 30 [30336/50048]	Loss: 1.0535
Training Epoch: 30 [30464/50048]	Loss: 1.0715
Training Epoch: 30 [30592/50048]	Loss: 1.0195
Training Epoch: 30 [30720/50048]	Loss: 1.2369
Training Epoch: 30 [30848/50048]	Loss: 0.9331
Training Epoch: 30 [30976/50048]	Loss: 1.0605
Training Epoch: 30 [31104/50048]	Loss: 0.9874
Training Epoch: 30 [31232/50048]	Loss: 0.7990
Training Epoch: 30 [31360/50048]	Loss: 1.0348
Training Epoch: 30 [31488/50048]	Loss: 0.9926
Training Epoch: 30 [31616/50048]	Loss: 1.1288
Training Epoch: 30 [31744/50048]	Loss: 1.0480
Training Epoch: 30 [31872/50048]	Loss: 1.0367
Training Epoch: 30 [32000/50048]	Loss: 1.1733
Training Epoch: 30 [32128/50048]	Loss: 1.2546
Training Epoch: 30 [32256/50048]	Loss: 1.0174
Training Epoch: 30 [32384/50048]	Loss: 1.0852
Training Epoch: 30 [32512/50048]	Loss: 1.1389
Training Epoch: 30 [32640/50048]	Loss: 1.0093
Training Epoch: 30 [32768/50048]	Loss: 0.9721
Training Epoch: 30 [32896/50048]	Loss: 1.3085
Training Epoch: 30 [33024/50048]	Loss: 1.0072
Training Epoch: 30 [33152/50048]	Loss: 1.1047
Training Epoch: 30 [33280/50048]	Loss: 1.2821
Training Epoch: 30 [33408/50048]	Loss: 1.0339
Training Epoch: 30 [33536/50048]	Loss: 0.9484
Training Epoch: 30 [33664/50048]	Loss: 0.9672
Training Epoch: 30 [33792/50048]	Loss: 1.1713
Training Epoch: 30 [33920/50048]	Loss: 1.0699
Training Epoch: 30 [34048/50048]	Loss: 0.9480
Training Epoch: 30 [34176/50048]	Loss: 0.9173
Training Epoch: 30 [34304/50048]	Loss: 1.0531
Training Epoch: 30 [34432/50048]	Loss: 1.1443
Training Epoch: 30 [34560/50048]	Loss: 0.9544
Training Epoch: 30 [34688/50048]	Loss: 1.0296
Training Epoch: 30 [34816/50048]	Loss: 1.1733
Training Epoch: 30 [34944/50048]	Loss: 0.9035
Training Epoch: 30 [35072/50048]	Loss: 1.2815
Training Epoch: 30 [35200/50048]	Loss: 1.0746
Training Epoch: 30 [35328/50048]	Loss: 1.0531
Training Epoch: 30 [35456/50048]	Loss: 1.1734
Training Epoch: 30 [35584/50048]	Loss: 0.7757
Training Epoch: 30 [35712/50048]	Loss: 0.9937
Training Epoch: 30 [35840/50048]	Loss: 0.8189
Training Epoch: 30 [35968/50048]	Loss: 0.7908
Training Epoch: 30 [36096/50048]	Loss: 0.8329
Training Epoch: 30 [36224/50048]	Loss: 0.9738
Training Epoch: 30 [36352/50048]	Loss: 0.9241
Training Epoch: 30 [36480/50048]	Loss: 1.0225
Training Epoch: 30 [36608/50048]	Loss: 1.0206
Training Epoch: 30 [36736/50048]	Loss: 1.0531
Training Epoch: 30 [36864/50048]	Loss: 1.0897
Training Epoch: 30 [36992/50048]	Loss: 1.1706
Training Epoch: 30 [37120/50048]	Loss: 1.1776
Training Epoch: 30 [37248/50048]	Loss: 1.2159
Training Epoch: 30 [37376/50048]	Loss: 1.0013
Training Epoch: 30 [37504/50048]	Loss: 1.1006
Training Epoch: 30 [37632/50048]	Loss: 1.1522
Training Epoch: 30 [37760/50048]	Loss: 1.0021
Training Epoch: 30 [37888/50048]	Loss: 1.0938
Training Epoch: 30 [38016/50048]	Loss: 1.0329
Training Epoch: 30 [38144/50048]	Loss: 1.1189
Training Epoch: 30 [38272/50048]	Loss: 1.1445
Training Epoch: 30 [38400/50048]	Loss: 0.9744
Training Epoch: 30 [38528/50048]	Loss: 1.1872
Training Epoch: 30 [38656/50048]	Loss: 0.9141
Training Epoch: 30 [38784/50048]	Loss: 0.9387
Training Epoch: 30 [38912/50048]	Loss: 0.8050
Training Epoch: 30 [39040/50048]	Loss: 0.9201
Training Epoch: 30 [39168/50048]	Loss: 0.9356
Training Epoch: 30 [39296/50048]	Loss: 1.1401
Training Epoch: 30 [39424/50048]	Loss: 0.9462
Training Epoch: 30 [39552/50048]	Loss: 1.1424
Training Epoch: 30 [39680/50048]	Loss: 0.9757
Training Epoch: 30 [39808/50048]	Loss: 1.0122
Training Epoch: 30 [39936/50048]	Loss: 1.0383
Training Epoch: 30 [40064/50048]	Loss: 0.9789
Training Epoch: 30 [40192/50048]	Loss: 1.0180
Training Epoch: 30 [40320/50048]	Loss: 0.9877
Training Epoch: 30 [40448/50048]	Loss: 0.9177
Training Epoch: 30 [40576/50048]	Loss: 1.0215
Training Epoch: 30 [40704/50048]	Loss: 0.7869
Training Epoch: 30 [40832/50048]	Loss: 1.1480
Training Epoch: 30 [40960/50048]	Loss: 1.2755
Training Epoch: 30 [41088/50048]	Loss: 0.9315
Training Epoch: 30 [41216/50048]	Loss: 1.1431
Training Epoch: 30 [41344/50048]	Loss: 0.8279
Training Epoch: 30 [41472/50048]	Loss: 0.9382
Training Epoch: 30 [41600/50048]	Loss: 1.0036
Training Epoch: 30 [41728/50048]	Loss: 0.9192
Training Epoch: 30 [41856/50048]	Loss: 1.1321
Training Epoch: 30 [41984/50048]	Loss: 1.3841
Training Epoch: 30 [42112/50048]	Loss: 1.1800
Training Epoch: 30 [42240/50048]	Loss: 1.0397
Training Epoch: 30 [42368/50048]	Loss: 0.9475
Training Epoch: 30 [42496/50048]	Loss: 1.0259
Training Epoch: 30 [42624/50048]	Loss: 1.1380
Training Epoch: 30 [42752/50048]	Loss: 0.9791
Training Epoch: 30 [42880/50048]	Loss: 0.8120
Training Epoch: 30 [43008/50048]	Loss: 0.9590
Training Epoch: 30 [43136/50048]	Loss: 0.9921
Training Epoch: 30 [43264/50048]	Loss: 1.1970
Training Epoch: 30 [43392/50048]	Loss: 1.1830
Training Epoch: 30 [43520/50048]	Loss: 1.1578
Training Epoch: 30 [43648/50048]	Loss: 1.0520
Training Epoch: 30 [43776/50048]	Loss: 1.1819
Training Epoch: 30 [43904/50048]	Loss: 1.0434
Training Epoch: 30 [44032/50048]	Loss: 0.8401
Training Epoch: 30 [44160/50048]	Loss: 0.9762
Training Epoch: 30 [44288/50048]	Loss: 0.9672
Training Epoch: 30 [44416/50048]	Loss: 0.8293
Training Epoch: 30 [44544/50048]	Loss: 0.9537
Training Epoch: 30 [44672/50048]	Loss: 0.9316
Training Epoch: 30 [44800/50048]	Loss: 0.9041
Training Epoch: 30 [44928/50048]	Loss: 1.4035
Training Epoch: 30 [45056/50048]	Loss: 1.0482
Training Epoch: 30 [45184/50048]	Loss: 1.2199
Training Epoch: 30 [45312/50048]	Loss: 1.1666
Training Epoch: 30 [45440/50048]	Loss: 1.2171
Training Epoch: 30 [45568/50048]	Loss: 0.9048
Training Epoch: 30 [45696/50048]	Loss: 1.0187
Training Epoch: 30 [45824/50048]	Loss: 1.1081
Training Epoch: 30 [45952/50048]	Loss: 1.0425
Training Epoch: 30 [46080/50048]	Loss: 1.1430
Training Epoch: 30 [46208/50048]	Loss: 0.9102
Training Epoch: 30 [46336/50048]	Loss: 1.2933
Training Epoch: 30 [46464/50048]	Loss: 0.9400
Training Epoch: 30 [46592/50048]	Loss: 0.9807
Training Epoch: 30 [46720/50048]	Loss: 1.0553
Training Epoch: 30 [46848/50048]	Loss: 1.0314
Training Epoch: 30 [46976/50048]	Loss: 0.8518
Training Epoch: 30 [47104/50048]	Loss: 0.9913
Training Epoch: 30 [47232/50048]	Loss: 1.0621
Training Epoch: 30 [47360/50048]	Loss: 0.9918
Training Epoch: 30 [47488/50048]	Loss: 0.9676
Training Epoch: 30 [47616/50048]	Loss: 1.1068
Training Epoch: 30 [47744/50048]	Loss: 1.0286
Training Epoch: 30 [47872/50048]	Loss: 0.7834
Training Epoch: 30 [48000/50048]	Loss: 1.1883
Training Epoch: 30 [48128/50048]	Loss: 1.1149
Training Epoch: 30 [48256/50048]	Loss: 1.2017
Training Epoch: 30 [48384/50048]	Loss: 1.1834
Training Epoch: 30 [48512/50048]	Loss: 1.0124
Training Epoch: 30 [48640/50048]	Loss: 1.0516
Training Epoch: 30 [48768/50048]	Loss: 1.1315
Training Epoch: 30 [48896/50048]	Loss: 0.9893
Training Epoch: 30 [49024/50048]	Loss: 1.1930
Training Epoch: 30 [49152/50048]	Loss: 1.1958
Training Epoch: 30 [49280/50048]	Loss: 1.0105
Training Epoch: 30 [49408/50048]	Loss: 0.9923
Training Epoch: 30 [49536/50048]	Loss: 1.1798
Training Epoch: 30 [49664/50048]	Loss: 0.9909
Training Epoch: 30 [49792/50048]	Loss: 1.1429
Training Epoch: 30 [49920/50048]	Loss: 0.9900
Training Epoch: 30 [50048/50048]	Loss: 1.1877
Validation Epoch: 30, Average loss: 0.0125, Accuracy: 0.5910
Training Epoch: 31 [128/50048]	Loss: 0.8952
Training Epoch: 31 [256/50048]	Loss: 1.0040
Training Epoch: 31 [384/50048]	Loss: 0.9455
Training Epoch: 31 [512/50048]	Loss: 0.9365
Training Epoch: 31 [640/50048]	Loss: 0.9279
Training Epoch: 31 [768/50048]	Loss: 0.8658
Training Epoch: 31 [896/50048]	Loss: 0.9935
Training Epoch: 31 [1024/50048]	Loss: 0.9431
Training Epoch: 31 [1152/50048]	Loss: 0.7438
Training Epoch: 31 [1280/50048]	Loss: 1.0238
Training Epoch: 31 [1408/50048]	Loss: 0.9064
Training Epoch: 31 [1536/50048]	Loss: 0.8207
Training Epoch: 31 [1664/50048]	Loss: 0.9945
Training Epoch: 31 [1792/50048]	Loss: 0.9345
Training Epoch: 31 [1920/50048]	Loss: 1.0972
Training Epoch: 31 [2048/50048]	Loss: 1.0146
Training Epoch: 31 [2176/50048]	Loss: 1.0090
Training Epoch: 31 [2304/50048]	Loss: 1.2653
Training Epoch: 31 [2432/50048]	Loss: 0.9656
Training Epoch: 31 [2560/50048]	Loss: 0.9428
Training Epoch: 31 [2688/50048]	Loss: 0.7478
Training Epoch: 31 [2816/50048]	Loss: 0.9627
Training Epoch: 31 [2944/50048]	Loss: 1.0137
Training Epoch: 31 [3072/50048]	Loss: 0.8210
Training Epoch: 31 [3200/50048]	Loss: 0.8091
Training Epoch: 31 [3328/50048]	Loss: 0.8953
Training Epoch: 31 [3456/50048]	Loss: 0.7572
Training Epoch: 31 [3584/50048]	Loss: 0.9266
Training Epoch: 31 [3712/50048]	Loss: 0.9852
Training Epoch: 31 [3840/50048]	Loss: 1.0433
Training Epoch: 31 [3968/50048]	Loss: 0.9633
Training Epoch: 31 [4096/50048]	Loss: 0.8639
Training Epoch: 31 [4224/50048]	Loss: 0.9642
Training Epoch: 31 [4352/50048]	Loss: 0.8315
Training Epoch: 31 [4480/50048]	Loss: 1.0063
Training Epoch: 31 [4608/50048]	Loss: 0.8643
Training Epoch: 31 [4736/50048]	Loss: 0.8592
Training Epoch: 31 [4864/50048]	Loss: 0.8927
Training Epoch: 31 [4992/50048]	Loss: 1.0875
Training Epoch: 31 [5120/50048]	Loss: 1.1312
Training Epoch: 31 [5248/50048]	Loss: 1.1569
Training Epoch: 31 [5376/50048]	Loss: 1.0597
Training Epoch: 31 [5504/50048]	Loss: 0.9054
Training Epoch: 31 [5632/50048]	Loss: 1.0451
Training Epoch: 31 [5760/50048]	Loss: 0.6730
Training Epoch: 31 [5888/50048]	Loss: 0.8780
Training Epoch: 31 [6016/50048]	Loss: 0.8136
Training Epoch: 31 [6144/50048]	Loss: 0.9833
Training Epoch: 31 [6272/50048]	Loss: 1.0156
Training Epoch: 31 [6400/50048]	Loss: 1.0600
Training Epoch: 31 [6528/50048]	Loss: 0.9648
Training Epoch: 31 [6656/50048]	Loss: 0.8527
Training Epoch: 31 [6784/50048]	Loss: 0.7134
Training Epoch: 31 [6912/50048]	Loss: 0.9497
Training Epoch: 31 [7040/50048]	Loss: 0.8295
Training Epoch: 31 [7168/50048]	Loss: 1.0098
Training Epoch: 31 [7296/50048]	Loss: 1.1001
Training Epoch: 31 [7424/50048]	Loss: 0.7776
Training Epoch: 31 [7552/50048]	Loss: 1.0905
Training Epoch: 31 [7680/50048]	Loss: 0.8694
Training Epoch: 31 [7808/50048]	Loss: 1.0844
Training Epoch: 31 [7936/50048]	Loss: 0.9833
Training Epoch: 31 [8064/50048]	Loss: 0.8315
Training Epoch: 31 [8192/50048]	Loss: 1.1641
Training Epoch: 31 [8320/50048]	Loss: 1.0622
Training Epoch: 31 [8448/50048]	Loss: 0.9489
Training Epoch: 31 [8576/50048]	Loss: 1.0378
Training Epoch: 31 [8704/50048]	Loss: 0.8686
Training Epoch: 31 [8832/50048]	Loss: 0.9548
Training Epoch: 31 [8960/50048]	Loss: 1.0893
Training Epoch: 31 [9088/50048]	Loss: 0.8290
Training Epoch: 31 [9216/50048]	Loss: 0.9659
Training Epoch: 31 [9344/50048]	Loss: 0.8969
Training Epoch: 31 [9472/50048]	Loss: 1.0497
Training Epoch: 31 [9600/50048]	Loss: 1.0592
Training Epoch: 31 [9728/50048]	Loss: 1.1338
Training Epoch: 31 [9856/50048]	Loss: 0.7664
Training Epoch: 31 [9984/50048]	Loss: 1.2046
Training Epoch: 31 [10112/50048]	Loss: 1.1199
Training Epoch: 31 [10240/50048]	Loss: 0.9202
Training Epoch: 31 [10368/50048]	Loss: 0.9162
Training Epoch: 31 [10496/50048]	Loss: 0.7718
Training Epoch: 31 [10624/50048]	Loss: 1.0403
Training Epoch: 31 [10752/50048]	Loss: 0.8396
Training Epoch: 31 [10880/50048]	Loss: 1.0225
Training Epoch: 31 [11008/50048]	Loss: 0.9837
Training Epoch: 31 [11136/50048]	Loss: 1.0708
Training Epoch: 31 [11264/50048]	Loss: 0.9338
Training Epoch: 31 [11392/50048]	Loss: 1.1436
Training Epoch: 31 [11520/50048]	Loss: 1.3154
Training Epoch: 31 [11648/50048]	Loss: 1.0397
Training Epoch: 31 [11776/50048]	Loss: 1.1287
Training Epoch: 31 [11904/50048]	Loss: 0.8879
Training Epoch: 31 [12032/50048]	Loss: 0.8081
Training Epoch: 31 [12160/50048]	Loss: 0.9036
Training Epoch: 31 [12288/50048]	Loss: 0.8869
Training Epoch: 31 [12416/50048]	Loss: 0.9815
Training Epoch: 31 [12544/50048]	Loss: 0.9846
Training Epoch: 31 [12672/50048]	Loss: 1.0788
Training Epoch: 31 [12800/50048]	Loss: 0.9041
Training Epoch: 31 [12928/50048]	Loss: 1.0850
Training Epoch: 31 [13056/50048]	Loss: 1.0388
Training Epoch: 31 [13184/50048]	Loss: 1.0313
Training Epoch: 31 [13312/50048]	Loss: 0.7837
Training Epoch: 31 [13440/50048]	Loss: 0.9999
Training Epoch: 31 [13568/50048]	Loss: 0.9747
Training Epoch: 31 [13696/50048]	Loss: 1.0444
Training Epoch: 31 [13824/50048]	Loss: 0.9373
Training Epoch: 31 [13952/50048]	Loss: 0.9175
Training Epoch: 31 [14080/50048]	Loss: 0.9122
Training Epoch: 31 [14208/50048]	Loss: 0.8978
Training Epoch: 31 [14336/50048]	Loss: 1.1002
Training Epoch: 31 [14464/50048]	Loss: 1.0059
Training Epoch: 31 [14592/50048]	Loss: 0.9558
Training Epoch: 31 [14720/50048]	Loss: 0.9376
Training Epoch: 31 [14848/50048]	Loss: 0.9482
Training Epoch: 31 [14976/50048]	Loss: 0.8496
Training Epoch: 31 [15104/50048]	Loss: 1.0165
Training Epoch: 31 [15232/50048]	Loss: 0.9763
Training Epoch: 31 [15360/50048]	Loss: 1.0640
Training Epoch: 31 [15488/50048]	Loss: 0.7933
Training Epoch: 31 [15616/50048]	Loss: 0.9143
Training Epoch: 31 [15744/50048]	Loss: 0.9624
Training Epoch: 31 [15872/50048]	Loss: 0.9021
Training Epoch: 31 [16000/50048]	Loss: 1.0071
Training Epoch: 31 [16128/50048]	Loss: 0.8526
Training Epoch: 31 [16256/50048]	Loss: 0.9720
Training Epoch: 31 [16384/50048]	Loss: 1.0902
Training Epoch: 31 [16512/50048]	Loss: 1.0168
Training Epoch: 31 [16640/50048]	Loss: 0.8436
Training Epoch: 31 [16768/50048]	Loss: 0.9127
Training Epoch: 31 [16896/50048]	Loss: 1.2002
Training Epoch: 31 [17024/50048]	Loss: 0.8414
Training Epoch: 31 [17152/50048]	Loss: 0.9332
Training Epoch: 31 [17280/50048]	Loss: 0.8546
Training Epoch: 31 [17408/50048]	Loss: 1.1284
Training Epoch: 31 [17536/50048]	Loss: 1.0317
Training Epoch: 31 [17664/50048]	Loss: 0.7826
Training Epoch: 31 [17792/50048]	Loss: 0.9640
Training Epoch: 31 [17920/50048]	Loss: 1.0618
Training Epoch: 31 [18048/50048]	Loss: 0.8081
Training Epoch: 31 [18176/50048]	Loss: 1.3167
Training Epoch: 31 [18304/50048]	Loss: 0.7546
Training Epoch: 31 [18432/50048]	Loss: 1.1021
Training Epoch: 31 [18560/50048]	Loss: 0.9298
Training Epoch: 31 [18688/50048]	Loss: 0.8935
Training Epoch: 31 [18816/50048]	Loss: 0.8104
Training Epoch: 31 [18944/50048]	Loss: 0.9211
Training Epoch: 31 [19072/50048]	Loss: 0.9498
Training Epoch: 31 [19200/50048]	Loss: 1.1804
Training Epoch: 31 [19328/50048]	Loss: 0.9717
Training Epoch: 31 [19456/50048]	Loss: 0.6781
Training Epoch: 31 [19584/50048]	Loss: 0.9533
Training Epoch: 31 [19712/50048]	Loss: 1.0590
Training Epoch: 31 [19840/50048]	Loss: 0.9388
Training Epoch: 31 [19968/50048]	Loss: 0.8999
Training Epoch: 31 [20096/50048]	Loss: 0.8719
Training Epoch: 31 [20224/50048]	Loss: 0.9656
Training Epoch: 31 [20352/50048]	Loss: 1.1752
Training Epoch: 31 [20480/50048]	Loss: 1.1228
Training Epoch: 31 [20608/50048]	Loss: 1.2258
Training Epoch: 31 [20736/50048]	Loss: 1.1254
Training Epoch: 31 [20864/50048]	Loss: 1.0811
Training Epoch: 31 [20992/50048]	Loss: 1.0141
Training Epoch: 31 [21120/50048]	Loss: 0.8173
Training Epoch: 31 [21248/50048]	Loss: 0.9286
Training Epoch: 31 [21376/50048]	Loss: 1.1547
Training Epoch: 31 [21504/50048]	Loss: 0.8482
Training Epoch: 31 [21632/50048]	Loss: 0.9445
Training Epoch: 31 [21760/50048]	Loss: 0.8828
Training Epoch: 31 [21888/50048]	Loss: 0.8703
Training Epoch: 31 [22016/50048]	Loss: 1.1465
Training Epoch: 31 [22144/50048]	Loss: 1.0301
Training Epoch: 31 [22272/50048]	Loss: 0.8523
Training Epoch: 31 [22400/50048]	Loss: 1.0101
Training Epoch: 31 [22528/50048]	Loss: 0.9973
Training Epoch: 31 [22656/50048]	Loss: 0.8481
Training Epoch: 31 [22784/50048]	Loss: 0.9723
Training Epoch: 31 [22912/50048]	Loss: 0.7055
Training Epoch: 31 [23040/50048]	Loss: 1.1214
Training Epoch: 31 [23168/50048]	Loss: 1.1452
Training Epoch: 31 [23296/50048]	Loss: 0.8712
Training Epoch: 31 [23424/50048]	Loss: 1.0322
Training Epoch: 31 [23552/50048]	Loss: 0.7291
Training Epoch: 31 [23680/50048]	Loss: 1.0271
Training Epoch: 31 [23808/50048]	Loss: 1.0164
Training Epoch: 31 [23936/50048]	Loss: 0.7884
Training Epoch: 31 [24064/50048]	Loss: 0.9904
Training Epoch: 31 [24192/50048]	Loss: 1.1199
Training Epoch: 31 [24320/50048]	Loss: 1.0810
Training Epoch: 31 [24448/50048]	Loss: 0.9796
Training Epoch: 31 [24576/50048]	Loss: 0.8762
Training Epoch: 31 [24704/50048]	Loss: 0.9159
Training Epoch: 31 [24832/50048]	Loss: 0.8006
Training Epoch: 31 [24960/50048]	Loss: 0.8535
Training Epoch: 31 [25088/50048]	Loss: 1.0216
Training Epoch: 31 [25216/50048]	Loss: 0.8335
Training Epoch: 31 [25344/50048]	Loss: 1.0797
Training Epoch: 31 [25472/50048]	Loss: 0.9059
Training Epoch: 31 [25600/50048]	Loss: 0.9978
Training Epoch: 31 [25728/50048]	Loss: 1.0312
Training Epoch: 31 [25856/50048]	Loss: 0.7075
Training Epoch: 31 [25984/50048]	Loss: 1.2476
Training Epoch: 31 [26112/50048]	Loss: 0.9152
Training Epoch: 31 [26240/50048]	Loss: 0.9863
Training Epoch: 31 [26368/50048]	Loss: 1.1218
Training Epoch: 31 [26496/50048]	Loss: 1.1036
Training Epoch: 31 [26624/50048]	Loss: 1.0077
Training Epoch: 31 [26752/50048]	Loss: 0.9311
Training Epoch: 31 [26880/50048]	Loss: 1.1484
Training Epoch: 31 [27008/50048]	Loss: 0.9707
Training Epoch: 31 [27136/50048]	Loss: 1.1437
Training Epoch: 31 [27264/50048]	Loss: 0.8022
Training Epoch: 31 [27392/50048]	Loss: 1.1935
Training Epoch: 31 [27520/50048]	Loss: 0.8931
Training Epoch: 31 [27648/50048]	Loss: 1.1732
Training Epoch: 31 [27776/50048]	Loss: 1.2272
Training Epoch: 31 [27904/50048]	Loss: 0.9537
Training Epoch: 31 [28032/50048]	Loss: 0.8085
Training Epoch: 31 [28160/50048]	Loss: 1.0894
Training Epoch: 31 [28288/50048]	Loss: 1.0829
Training Epoch: 31 [28416/50048]	Loss: 0.9033
Training Epoch: 31 [28544/50048]	Loss: 1.1756
Training Epoch: 31 [28672/50048]	Loss: 1.0015
Training Epoch: 31 [28800/50048]	Loss: 1.2232
Training Epoch: 31 [28928/50048]	Loss: 0.8548
Training Epoch: 31 [29056/50048]	Loss: 1.2276
Training Epoch: 31 [29184/50048]	Loss: 1.0672
Training Epoch: 31 [29312/50048]	Loss: 0.9782
Training Epoch: 31 [29440/50048]	Loss: 0.7991
Training Epoch: 31 [29568/50048]	Loss: 1.1356
Training Epoch: 31 [29696/50048]	Loss: 0.8930
Training Epoch: 31 [29824/50048]	Loss: 1.1090
Training Epoch: 31 [29952/50048]	Loss: 1.1597
Training Epoch: 31 [30080/50048]	Loss: 0.7796
Training Epoch: 31 [30208/50048]	Loss: 0.9800
Training Epoch: 31 [30336/50048]	Loss: 1.0087
Training Epoch: 31 [30464/50048]	Loss: 1.1514
Training Epoch: 31 [30592/50048]	Loss: 0.9897
Training Epoch: 31 [30720/50048]	Loss: 0.7959
Training Epoch: 31 [30848/50048]	Loss: 1.0560
Training Epoch: 31 [30976/50048]	Loss: 0.9628
Training Epoch: 31 [31104/50048]	Loss: 1.0679
Training Epoch: 31 [31232/50048]	Loss: 1.2347
Training Epoch: 31 [31360/50048]	Loss: 1.0365
Training Epoch: 31 [31488/50048]	Loss: 1.1168
Training Epoch: 31 [31616/50048]	Loss: 1.0521
Training Epoch: 31 [31744/50048]	Loss: 0.8963
Training Epoch: 31 [31872/50048]	Loss: 1.0756
Training Epoch: 31 [32000/50048]	Loss: 1.2669
Training Epoch: 31 [32128/50048]	Loss: 1.0296
Training Epoch: 31 [32256/50048]	Loss: 0.9084
Training Epoch: 31 [32384/50048]	Loss: 1.0476
Training Epoch: 31 [32512/50048]	Loss: 1.0229
Training Epoch: 31 [32640/50048]	Loss: 0.9771
Training Epoch: 31 [32768/50048]	Loss: 1.2263
Training Epoch: 31 [32896/50048]	Loss: 0.8820
Training Epoch: 31 [33024/50048]	Loss: 1.0191
Training Epoch: 31 [33152/50048]	Loss: 0.9453
Training Epoch: 31 [33280/50048]	Loss: 0.9463
Training Epoch: 31 [33408/50048]	Loss: 1.1284
Training Epoch: 31 [33536/50048]	Loss: 1.2843
Training Epoch: 31 [33664/50048]	Loss: 0.8625
Training Epoch: 31 [33792/50048]	Loss: 0.9920
Training Epoch: 31 [33920/50048]	Loss: 1.0554
Training Epoch: 31 [34048/50048]	Loss: 0.7948
Training Epoch: 31 [34176/50048]	Loss: 1.0017
Training Epoch: 31 [34304/50048]	Loss: 1.0880
Training Epoch: 31 [34432/50048]	Loss: 1.2358
Training Epoch: 31 [34560/50048]	Loss: 1.0757
Training Epoch: 31 [34688/50048]	Loss: 0.8774
Training Epoch: 31 [34816/50048]	Loss: 1.1712
Training Epoch: 31 [34944/50048]	Loss: 1.0532
Training Epoch: 31 [35072/50048]	Loss: 0.8968
Training Epoch: 31 [35200/50048]	Loss: 1.1633
Training Epoch: 31 [35328/50048]	Loss: 1.0420
Training Epoch: 31 [35456/50048]	Loss: 0.9156
Training Epoch: 31 [35584/50048]	Loss: 0.9464
Training Epoch: 31 [35712/50048]	Loss: 0.9802
Training Epoch: 31 [35840/50048]	Loss: 1.0685
Training Epoch: 31 [35968/50048]	Loss: 0.9655
Training Epoch: 31 [36096/50048]	Loss: 1.0808
Training Epoch: 31 [36224/50048]	Loss: 0.8858
Training Epoch: 31 [36352/50048]	Loss: 1.0862
Training Epoch: 31 [36480/50048]	Loss: 0.9093
Training Epoch: 31 [36608/50048]	Loss: 1.0615
Training Epoch: 31 [36736/50048]	Loss: 1.0986
Training Epoch: 31 [36864/50048]	Loss: 0.8070
Training Epoch: 31 [36992/50048]	Loss: 1.0207
Training Epoch: 31 [37120/50048]	Loss: 0.9566
Training Epoch: 31 [37248/50048]	Loss: 0.8848
Training Epoch: 31 [37376/50048]	Loss: 0.9478
Training Epoch: 31 [37504/50048]	Loss: 0.8811
Training Epoch: 31 [37632/50048]	Loss: 0.8265
Training Epoch: 31 [37760/50048]	Loss: 1.0026
Training Epoch: 31 [37888/50048]	Loss: 1.0993
Training Epoch: 31 [38016/50048]	Loss: 0.9938
Training Epoch: 31 [38144/50048]	Loss: 0.8759
Training Epoch: 31 [38272/50048]	Loss: 0.7420
Training Epoch: 31 [38400/50048]	Loss: 1.1467
Training Epoch: 31 [38528/50048]	Loss: 1.0289
Training Epoch: 31 [38656/50048]	Loss: 1.4101
Training Epoch: 31 [38784/50048]	Loss: 1.0127
Training Epoch: 31 [38912/50048]	Loss: 1.0356
Training Epoch: 31 [39040/50048]	Loss: 0.9170
Training Epoch: 31 [39168/50048]	Loss: 1.0121
Training Epoch: 31 [39296/50048]	Loss: 1.0146
Training Epoch: 31 [39424/50048]	Loss: 1.0492
Training Epoch: 31 [39552/50048]	Loss: 1.0518
Training Epoch: 31 [39680/50048]	Loss: 1.0263
Training Epoch: 31 [39808/50048]	Loss: 0.8372
Training Epoch: 31 [39936/50048]	Loss: 1.1517
Training Epoch: 31 [40064/50048]	Loss: 1.0991
Training Epoch: 31 [40192/50048]	Loss: 1.0184
Training Epoch: 31 [40320/50048]	Loss: 0.9710
Training Epoch: 31 [40448/50048]	Loss: 1.0983
Training Epoch: 31 [40576/50048]	Loss: 1.0178
Training Epoch: 31 [40704/50048]	Loss: 0.9973
Training Epoch: 31 [40832/50048]	Loss: 1.0204
Training Epoch: 31 [40960/50048]	Loss: 1.0040
Training Epoch: 31 [41088/50048]	Loss: 1.0481
Training Epoch: 31 [41216/50048]	Loss: 0.9309
Training Epoch: 31 [41344/50048]	Loss: 1.0069
Training Epoch: 31 [41472/50048]	Loss: 1.0576
Training Epoch: 31 [41600/50048]	Loss: 1.0723
Training Epoch: 31 [41728/50048]	Loss: 1.2537
Training Epoch: 31 [41856/50048]	Loss: 1.0368
Training Epoch: 31 [41984/50048]	Loss: 0.9282
Training Epoch: 31 [42112/50048]	Loss: 1.0236
Training Epoch: 31 [42240/50048]	Loss: 0.9938
Training Epoch: 31 [42368/50048]	Loss: 0.8481
Training Epoch: 31 [42496/50048]	Loss: 0.9002
Training Epoch: 31 [42624/50048]	Loss: 0.8870
Training Epoch: 31 [42752/50048]	Loss: 0.8784
Training Epoch: 31 [42880/50048]	Loss: 0.9938
Training Epoch: 31 [43008/50048]	Loss: 1.0337
Training Epoch: 31 [43136/50048]	Loss: 1.0962
Training Epoch: 31 [43264/50048]	Loss: 0.9936
Training Epoch: 31 [43392/50048]	Loss: 1.1286
Training Epoch: 31 [43520/50048]	Loss: 0.9196
Training Epoch: 31 [43648/50048]	Loss: 0.9301
Training Epoch: 31 [43776/50048]	Loss: 1.0054
Training Epoch: 31 [43904/50048]	Loss: 1.0344
Training Epoch: 31 [44032/50048]	Loss: 1.0930
Training Epoch: 31 [44160/50048]	Loss: 1.1249
Training Epoch: 31 [44288/50048]	Loss: 1.2717
Training Epoch: 31 [44416/50048]	Loss: 0.9275
Training Epoch: 31 [44544/50048]	Loss: 1.0557
Training Epoch: 31 [44672/50048]	Loss: 1.1771
Training Epoch: 31 [44800/50048]	Loss: 1.0175
Training Epoch: 31 [44928/50048]	Loss: 1.0281
Training Epoch: 31 [45056/50048]	Loss: 1.0667
Training Epoch: 31 [45184/50048]	Loss: 0.9427
Training Epoch: 31 [45312/50048]	Loss: 0.9383
Training Epoch: 31 [45440/50048]	Loss: 1.0012
Training Epoch: 31 [45568/50048]	Loss: 0.9293
Training Epoch: 31 [45696/50048]	Loss: 1.1265
Training Epoch: 31 [45824/50048]	Loss: 1.0140
Training Epoch: 31 [45952/50048]	Loss: 0.9814
Training Epoch: 31 [46080/50048]	Loss: 0.9372
Training Epoch: 31 [46208/50048]	Loss: 0.9400
Training Epoch: 31 [46336/50048]	Loss: 1.0877
Training Epoch: 31 [46464/50048]	Loss: 1.0037
Training Epoch: 31 [46592/50048]	Loss: 1.2769
Training Epoch: 31 [46720/50048]	Loss: 0.9923
Training Epoch: 31 [46848/50048]	Loss: 1.1736
Training Epoch: 31 [46976/50048]	Loss: 1.1792
Training Epoch: 31 [47104/50048]	Loss: 0.9178
Training Epoch: 31 [47232/50048]	Loss: 0.9688
Training Epoch: 31 [47360/50048]	Loss: 1.0740
Training Epoch: 31 [47488/50048]	Loss: 1.2028
Training Epoch: 31 [47616/50048]	Loss: 1.2875
Training Epoch: 31 [47744/50048]	Loss: 1.0655
Training Epoch: 31 [47872/50048]	Loss: 0.7726
Training Epoch: 31 [48000/50048]	Loss: 1.1372
Training Epoch: 31 [48128/50048]	Loss: 0.9623
Training Epoch: 31 [48256/50048]	Loss: 0.9402
Training Epoch: 31 [48384/50048]	Loss: 1.2404
Training Epoch: 31 [48512/50048]	Loss: 0.8873
Training Epoch: 31 [48640/50048]	Loss: 0.9262
Training Epoch: 31 [48768/50048]	Loss: 1.0284
Training Epoch: 31 [48896/50048]	Loss: 1.1214
Training Epoch: 31 [49024/50048]	Loss: 1.3529
Training Epoch: 31 [49152/50048]	Loss: 0.9756
Training Epoch: 31 [49280/50048]	Loss: 0.9266
Training Epoch: 31 [49408/50048]	Loss: 0.6917
Training Epoch: 31 [49536/50048]	Loss: 1.1300
Training Epoch: 31 [49664/50048]	Loss: 1.2276
Training Epoch: 31 [49792/50048]	Loss: 0.8521
Training Epoch: 31 [49920/50048]	Loss: 1.1079
Training Epoch: 31 [50048/50048]	Loss: 0.8871
Validation Epoch: 31, Average loss: 0.0124, Accuracy: 0.5905
Training Epoch: 32 [128/50048]	Loss: 0.8537
Training Epoch: 32 [256/50048]	Loss: 0.8981
Training Epoch: 32 [384/50048]	Loss: 0.9060
Training Epoch: 32 [512/50048]	Loss: 0.8924
Training Epoch: 32 [640/50048]	Loss: 0.9980
Training Epoch: 32 [768/50048]	Loss: 0.9486
Training Epoch: 32 [896/50048]	Loss: 0.9876
Training Epoch: 32 [1024/50048]	Loss: 0.8014
Training Epoch: 32 [1152/50048]	Loss: 0.7068
Training Epoch: 32 [1280/50048]	Loss: 0.8873
Training Epoch: 32 [1408/50048]	Loss: 1.0074
Training Epoch: 32 [1536/50048]	Loss: 0.8608
Training Epoch: 32 [1664/50048]	Loss: 1.0635
Training Epoch: 32 [1792/50048]	Loss: 0.7340
Training Epoch: 32 [1920/50048]	Loss: 0.5733
Training Epoch: 32 [2048/50048]	Loss: 0.8820
Training Epoch: 32 [2176/50048]	Loss: 0.8367
Training Epoch: 32 [2304/50048]	Loss: 0.9237
Training Epoch: 32 [2432/50048]	Loss: 0.7687
Training Epoch: 32 [2560/50048]	Loss: 0.9106
Training Epoch: 32 [2688/50048]	Loss: 0.8929
Training Epoch: 32 [2816/50048]	Loss: 0.8273
Training Epoch: 32 [2944/50048]	Loss: 0.8629
Training Epoch: 32 [3072/50048]	Loss: 0.8748
Training Epoch: 32 [3200/50048]	Loss: 0.9650
Training Epoch: 32 [3328/50048]	Loss: 0.9580
Training Epoch: 32 [3456/50048]	Loss: 0.8893
Training Epoch: 32 [3584/50048]	Loss: 0.8526
Training Epoch: 32 [3712/50048]	Loss: 0.7589
Training Epoch: 32 [3840/50048]	Loss: 0.7819
Training Epoch: 32 [3968/50048]	Loss: 0.8054
Training Epoch: 32 [4096/50048]	Loss: 1.0968
Training Epoch: 32 [4224/50048]	Loss: 0.8766
Training Epoch: 32 [4352/50048]	Loss: 0.6625
Training Epoch: 32 [4480/50048]	Loss: 0.6271
Training Epoch: 32 [4608/50048]	Loss: 1.0649
Training Epoch: 32 [4736/50048]	Loss: 1.0125
Training Epoch: 32 [4864/50048]	Loss: 0.9884
Training Epoch: 32 [4992/50048]	Loss: 0.8629
Training Epoch: 32 [5120/50048]	Loss: 1.0876
Training Epoch: 32 [5248/50048]	Loss: 0.7074
Training Epoch: 32 [5376/50048]	Loss: 0.9594
Training Epoch: 32 [5504/50048]	Loss: 1.0726
Training Epoch: 32 [5632/50048]	Loss: 0.8619
Training Epoch: 32 [5760/50048]	Loss: 0.8186
Training Epoch: 32 [5888/50048]	Loss: 0.9046
Training Epoch: 32 [6016/50048]	Loss: 1.1225
Training Epoch: 32 [6144/50048]	Loss: 0.9092
Training Epoch: 32 [6272/50048]	Loss: 0.9794
Training Epoch: 32 [6400/50048]	Loss: 0.9682
Training Epoch: 32 [6528/50048]	Loss: 0.8962
Training Epoch: 32 [6656/50048]	Loss: 0.9263
Training Epoch: 32 [6784/50048]	Loss: 1.0355
Training Epoch: 32 [6912/50048]	Loss: 0.8121
Training Epoch: 32 [7040/50048]	Loss: 0.9566
Training Epoch: 32 [7168/50048]	Loss: 0.7960
Training Epoch: 32 [7296/50048]	Loss: 0.9891
Training Epoch: 32 [7424/50048]	Loss: 0.9169
Training Epoch: 32 [7552/50048]	Loss: 0.9193
Training Epoch: 32 [7680/50048]	Loss: 0.9015
Training Epoch: 32 [7808/50048]	Loss: 0.8513
Training Epoch: 32 [7936/50048]	Loss: 0.8649
Training Epoch: 32 [8064/50048]	Loss: 0.9525
Training Epoch: 32 [8192/50048]	Loss: 0.8443
Training Epoch: 32 [8320/50048]	Loss: 1.0451
Training Epoch: 32 [8448/50048]	Loss: 0.8757
Training Epoch: 32 [8576/50048]	Loss: 0.9235
Training Epoch: 32 [8704/50048]	Loss: 0.8978
Training Epoch: 32 [8832/50048]	Loss: 0.8464
Training Epoch: 32 [8960/50048]	Loss: 1.0814
Training Epoch: 32 [9088/50048]	Loss: 1.0099
Training Epoch: 32 [9216/50048]	Loss: 0.9500
Training Epoch: 32 [9344/50048]	Loss: 0.9796
Training Epoch: 32 [9472/50048]	Loss: 1.0153
Training Epoch: 32 [9600/50048]	Loss: 0.8563
Training Epoch: 32 [9728/50048]	Loss: 0.9425
Training Epoch: 32 [9856/50048]	Loss: 0.8670
Training Epoch: 32 [9984/50048]	Loss: 1.1887
Training Epoch: 32 [10112/50048]	Loss: 1.0268
Training Epoch: 32 [10240/50048]	Loss: 0.8762
Training Epoch: 32 [10368/50048]	Loss: 0.9846
Training Epoch: 32 [10496/50048]	Loss: 0.7350
Training Epoch: 32 [10624/50048]	Loss: 0.9733
Training Epoch: 32 [10752/50048]	Loss: 1.0011
Training Epoch: 32 [10880/50048]	Loss: 0.9192
Training Epoch: 32 [11008/50048]	Loss: 0.9454
Training Epoch: 32 [11136/50048]	Loss: 0.7898
Training Epoch: 32 [11264/50048]	Loss: 0.8051
Training Epoch: 32 [11392/50048]	Loss: 0.8500
Training Epoch: 32 [11520/50048]	Loss: 1.0524
Training Epoch: 32 [11648/50048]	Loss: 1.1262
Training Epoch: 32 [11776/50048]	Loss: 0.9366
Training Epoch: 32 [11904/50048]	Loss: 0.8634
Training Epoch: 32 [12032/50048]	Loss: 0.8297
Training Epoch: 32 [12160/50048]	Loss: 0.8985
Training Epoch: 32 [12288/50048]	Loss: 0.8377
Training Epoch: 32 [12416/50048]	Loss: 0.9118
Training Epoch: 32 [12544/50048]	Loss: 0.9450
Training Epoch: 32 [12672/50048]	Loss: 1.0350
Training Epoch: 32 [12800/50048]	Loss: 0.9799
Training Epoch: 32 [12928/50048]	Loss: 1.1273
Training Epoch: 32 [13056/50048]	Loss: 1.0419
Training Epoch: 32 [13184/50048]	Loss: 0.8701
Training Epoch: 32 [13312/50048]	Loss: 0.8525
Training Epoch: 32 [13440/50048]	Loss: 0.8771
Training Epoch: 32 [13568/50048]	Loss: 0.9018
Training Epoch: 32 [13696/50048]	Loss: 0.9311
Training Epoch: 32 [13824/50048]	Loss: 0.8933
Training Epoch: 32 [13952/50048]	Loss: 0.9321
Training Epoch: 32 [14080/50048]	Loss: 0.8731
Training Epoch: 32 [14208/50048]	Loss: 0.8250
Training Epoch: 32 [14336/50048]	Loss: 0.9221
Training Epoch: 32 [14464/50048]	Loss: 0.9608
Training Epoch: 32 [14592/50048]	Loss: 0.7942
Training Epoch: 32 [14720/50048]	Loss: 0.9708
Training Epoch: 32 [14848/50048]	Loss: 0.9521
Training Epoch: 32 [14976/50048]	Loss: 0.9881
Training Epoch: 32 [15104/50048]	Loss: 1.0752
Training Epoch: 32 [15232/50048]	Loss: 0.9037
Training Epoch: 32 [15360/50048]	Loss: 0.9375
Training Epoch: 32 [15488/50048]	Loss: 0.9399
Training Epoch: 32 [15616/50048]	Loss: 0.9786
Training Epoch: 32 [15744/50048]	Loss: 1.0988
Training Epoch: 32 [15872/50048]	Loss: 0.9352
Training Epoch: 32 [16000/50048]	Loss: 1.0479
Training Epoch: 32 [16128/50048]	Loss: 0.9784
Training Epoch: 32 [16256/50048]	Loss: 0.8011
Training Epoch: 32 [16384/50048]	Loss: 0.8985
Training Epoch: 32 [16512/50048]	Loss: 0.9018
Training Epoch: 32 [16640/50048]	Loss: 0.9742
Training Epoch: 32 [16768/50048]	Loss: 0.9456
Training Epoch: 32 [16896/50048]	Loss: 1.1518
Training Epoch: 32 [17024/50048]	Loss: 0.9455
Training Epoch: 32 [17152/50048]	Loss: 0.9093
Training Epoch: 32 [17280/50048]	Loss: 0.7727
Training Epoch: 32 [17408/50048]	Loss: 1.0010
Training Epoch: 32 [17536/50048]	Loss: 0.9451
Training Epoch: 32 [17664/50048]	Loss: 1.0104
Training Epoch: 32 [17792/50048]	Loss: 1.0872
Training Epoch: 32 [17920/50048]	Loss: 0.8622
Training Epoch: 32 [18048/50048]	Loss: 0.9809
Training Epoch: 32 [18176/50048]	Loss: 0.9538
Training Epoch: 32 [18304/50048]	Loss: 1.0476
Training Epoch: 32 [18432/50048]	Loss: 0.7624
Training Epoch: 32 [18560/50048]	Loss: 0.8087
Training Epoch: 32 [18688/50048]	Loss: 0.9324
Training Epoch: 32 [18816/50048]	Loss: 0.8671
Training Epoch: 32 [18944/50048]	Loss: 0.8726
Training Epoch: 32 [19072/50048]	Loss: 1.1839
Training Epoch: 32 [19200/50048]	Loss: 0.9986
Training Epoch: 32 [19328/50048]	Loss: 1.0509
Training Epoch: 32 [19456/50048]	Loss: 1.0252
Training Epoch: 32 [19584/50048]	Loss: 0.9366
Training Epoch: 32 [19712/50048]	Loss: 0.8713
Training Epoch: 32 [19840/50048]	Loss: 1.1438
Training Epoch: 32 [19968/50048]	Loss: 0.8552
Training Epoch: 32 [20096/50048]	Loss: 0.9588
Training Epoch: 32 [20224/50048]	Loss: 0.8824
Training Epoch: 32 [20352/50048]	Loss: 0.6278
Training Epoch: 32 [20480/50048]	Loss: 0.9346
Training Epoch: 32 [20608/50048]	Loss: 0.8851
Training Epoch: 32 [20736/50048]	Loss: 1.1051
Training Epoch: 32 [20864/50048]	Loss: 0.9728
Training Epoch: 32 [20992/50048]	Loss: 0.8797
Training Epoch: 32 [21120/50048]	Loss: 0.8931
Training Epoch: 32 [21248/50048]	Loss: 0.8813
Training Epoch: 32 [21376/50048]	Loss: 1.1360
Training Epoch: 32 [21504/50048]	Loss: 0.9397
Training Epoch: 32 [21632/50048]	Loss: 1.0281
Training Epoch: 32 [21760/50048]	Loss: 0.8293
Training Epoch: 32 [21888/50048]	Loss: 0.9709
Training Epoch: 32 [22016/50048]	Loss: 0.7395
Training Epoch: 32 [22144/50048]	Loss: 0.8703
Training Epoch: 32 [22272/50048]	Loss: 1.0767
Training Epoch: 32 [22400/50048]	Loss: 0.9346
Training Epoch: 32 [22528/50048]	Loss: 0.9313
Training Epoch: 32 [22656/50048]	Loss: 1.1259
Training Epoch: 32 [22784/50048]	Loss: 1.1351
Training Epoch: 32 [22912/50048]	Loss: 1.0831
Training Epoch: 32 [23040/50048]	Loss: 1.1088
Training Epoch: 32 [23168/50048]	Loss: 0.9564
Training Epoch: 32 [23296/50048]	Loss: 0.8612
Training Epoch: 32 [23424/50048]	Loss: 1.0386
Training Epoch: 32 [23552/50048]	Loss: 1.0254
Training Epoch: 32 [23680/50048]	Loss: 1.2440
Training Epoch: 32 [23808/50048]	Loss: 1.1178
Training Epoch: 32 [23936/50048]	Loss: 1.0212
Training Epoch: 32 [24064/50048]	Loss: 1.0271
Training Epoch: 32 [24192/50048]	Loss: 0.8324
Training Epoch: 32 [24320/50048]	Loss: 0.9250
Training Epoch: 32 [24448/50048]	Loss: 0.8500
Training Epoch: 32 [24576/50048]	Loss: 0.8988
Training Epoch: 32 [24704/50048]	Loss: 1.1788
Training Epoch: 32 [24832/50048]	Loss: 1.0168
Training Epoch: 32 [24960/50048]	Loss: 1.1159
Training Epoch: 32 [25088/50048]	Loss: 0.9518
Training Epoch: 32 [25216/50048]	Loss: 0.9972
Training Epoch: 32 [25344/50048]	Loss: 0.9341
Training Epoch: 32 [25472/50048]	Loss: 0.9813
Training Epoch: 32 [25600/50048]	Loss: 0.9113
Training Epoch: 32 [25728/50048]	Loss: 0.8992
Training Epoch: 32 [25856/50048]	Loss: 0.9822
Training Epoch: 32 [25984/50048]	Loss: 0.8420
Training Epoch: 32 [26112/50048]	Loss: 0.9940
Training Epoch: 32 [26240/50048]	Loss: 0.9726
Training Epoch: 32 [26368/50048]	Loss: 0.8764
Training Epoch: 32 [26496/50048]	Loss: 1.0286
Training Epoch: 32 [26624/50048]	Loss: 0.8843
Training Epoch: 32 [26752/50048]	Loss: 0.8578
Training Epoch: 32 [26880/50048]	Loss: 0.9879
Training Epoch: 32 [27008/50048]	Loss: 0.8366
Training Epoch: 32 [27136/50048]	Loss: 0.9142
Training Epoch: 32 [27264/50048]	Loss: 0.9816
Training Epoch: 32 [27392/50048]	Loss: 0.9207
Training Epoch: 32 [27520/50048]	Loss: 1.0752
Training Epoch: 32 [27648/50048]	Loss: 0.8255
Training Epoch: 32 [27776/50048]	Loss: 1.0272
Training Epoch: 32 [27904/50048]	Loss: 1.0538
Training Epoch: 32 [28032/50048]	Loss: 1.0514
Training Epoch: 32 [28160/50048]	Loss: 0.8787
Training Epoch: 32 [28288/50048]	Loss: 0.8572
Training Epoch: 32 [28416/50048]	Loss: 0.9816
Training Epoch: 32 [28544/50048]	Loss: 0.9049
Training Epoch: 32 [28672/50048]	Loss: 1.1133
Training Epoch: 32 [28800/50048]	Loss: 0.8433
Training Epoch: 32 [28928/50048]	Loss: 0.7982
Training Epoch: 32 [29056/50048]	Loss: 0.9766
Training Epoch: 32 [29184/50048]	Loss: 0.9220
Training Epoch: 32 [29312/50048]	Loss: 1.2693
Training Epoch: 32 [29440/50048]	Loss: 0.8849
Training Epoch: 32 [29568/50048]	Loss: 0.9964
Training Epoch: 32 [29696/50048]	Loss: 0.8618
Training Epoch: 32 [29824/50048]	Loss: 0.8571
Training Epoch: 32 [29952/50048]	Loss: 0.9771
Training Epoch: 32 [30080/50048]	Loss: 0.8800
Training Epoch: 32 [30208/50048]	Loss: 0.9641
Training Epoch: 32 [30336/50048]	Loss: 0.9961
Training Epoch: 32 [30464/50048]	Loss: 0.9782
Training Epoch: 32 [30592/50048]	Loss: 1.0708
Training Epoch: 32 [30720/50048]	Loss: 0.8881
Training Epoch: 32 [30848/50048]	Loss: 0.8292
Training Epoch: 32 [30976/50048]	Loss: 0.8901
Training Epoch: 32 [31104/50048]	Loss: 1.1174
Training Epoch: 32 [31232/50048]	Loss: 0.9350
Training Epoch: 32 [31360/50048]	Loss: 0.8440
Training Epoch: 32 [31488/50048]	Loss: 0.9049
Training Epoch: 32 [31616/50048]	Loss: 1.1242
Training Epoch: 32 [31744/50048]	Loss: 0.9879
Training Epoch: 32 [31872/50048]	Loss: 1.1862
Training Epoch: 32 [32000/50048]	Loss: 1.2745
Training Epoch: 32 [32128/50048]	Loss: 1.0370
Training Epoch: 32 [32256/50048]	Loss: 0.9229
Training Epoch: 32 [32384/50048]	Loss: 1.0876
Training Epoch: 32 [32512/50048]	Loss: 1.0797
Training Epoch: 32 [32640/50048]	Loss: 0.9102
Training Epoch: 32 [32768/50048]	Loss: 0.8996
Training Epoch: 32 [32896/50048]	Loss: 0.9752
Training Epoch: 32 [33024/50048]	Loss: 1.1689
Training Epoch: 32 [33152/50048]	Loss: 1.0922
Training Epoch: 32 [33280/50048]	Loss: 1.1568
Training Epoch: 32 [33408/50048]	Loss: 0.9032
Training Epoch: 32 [33536/50048]	Loss: 0.8564
Training Epoch: 32 [33664/50048]	Loss: 0.8808
Training Epoch: 32 [33792/50048]	Loss: 0.8641
Training Epoch: 32 [33920/50048]	Loss: 0.8323
Training Epoch: 32 [34048/50048]	Loss: 1.0847
Training Epoch: 32 [34176/50048]	Loss: 1.0121
Training Epoch: 32 [34304/50048]	Loss: 0.8973
Training Epoch: 32 [34432/50048]	Loss: 0.9674
Training Epoch: 32 [34560/50048]	Loss: 0.9200
Training Epoch: 32 [34688/50048]	Loss: 1.0938
Training Epoch: 32 [34816/50048]	Loss: 0.7655
Training Epoch: 32 [34944/50048]	Loss: 0.9965
Training Epoch: 32 [35072/50048]	Loss: 1.0497
Training Epoch: 32 [35200/50048]	Loss: 1.0643
Training Epoch: 32 [35328/50048]	Loss: 0.7292
Training Epoch: 32 [35456/50048]	Loss: 0.9323
Training Epoch: 32 [35584/50048]	Loss: 1.0325
Training Epoch: 32 [35712/50048]	Loss: 1.1505
Training Epoch: 32 [35840/50048]	Loss: 0.9051
Training Epoch: 32 [35968/50048]	Loss: 0.9279
Training Epoch: 32 [36096/50048]	Loss: 0.8427
Training Epoch: 32 [36224/50048]	Loss: 0.9938
Training Epoch: 32 [36352/50048]	Loss: 0.9083
Training Epoch: 32 [36480/50048]	Loss: 1.0430
Training Epoch: 32 [36608/50048]	Loss: 0.8639
Training Epoch: 32 [36736/50048]	Loss: 0.8320
Training Epoch: 32 [36864/50048]	Loss: 1.0933
Training Epoch: 32 [36992/50048]	Loss: 0.9307
Training Epoch: 32 [37120/50048]	Loss: 0.9961
Training Epoch: 32 [37248/50048]	Loss: 1.1146
Training Epoch: 32 [37376/50048]	Loss: 1.0458
Training Epoch: 32 [37504/50048]	Loss: 1.1813
Training Epoch: 32 [37632/50048]	Loss: 1.0009
Training Epoch: 32 [37760/50048]	Loss: 0.8384
Training Epoch: 32 [37888/50048]	Loss: 1.0139
Training Epoch: 32 [38016/50048]	Loss: 0.9481
Training Epoch: 32 [38144/50048]	Loss: 0.8650
Training Epoch: 32 [38272/50048]	Loss: 0.9908
Training Epoch: 32 [38400/50048]	Loss: 0.9455
Training Epoch: 32 [38528/50048]	Loss: 0.8774
Training Epoch: 32 [38656/50048]	Loss: 0.9298
Training Epoch: 32 [38784/50048]	Loss: 0.9977
Training Epoch: 32 [38912/50048]	Loss: 0.9483
Training Epoch: 32 [39040/50048]	Loss: 0.8153
Training Epoch: 32 [39168/50048]	Loss: 1.1166
Training Epoch: 32 [39296/50048]	Loss: 1.0655
Training Epoch: 32 [39424/50048]	Loss: 0.9194
Training Epoch: 32 [39552/50048]	Loss: 1.0017
Training Epoch: 32 [39680/50048]	Loss: 0.8454
Training Epoch: 32 [39808/50048]	Loss: 0.9369
Training Epoch: 32 [39936/50048]	Loss: 1.0027
Training Epoch: 32 [40064/50048]	Loss: 0.9924
Training Epoch: 32 [40192/50048]	Loss: 0.8794
Training Epoch: 32 [40320/50048]	Loss: 0.9490
Training Epoch: 32 [40448/50048]	Loss: 0.8885
Training Epoch: 32 [40576/50048]	Loss: 0.9032
Training Epoch: 32 [40704/50048]	Loss: 0.9284
Training Epoch: 32 [40832/50048]	Loss: 1.0016
Training Epoch: 32 [40960/50048]	Loss: 0.8899
Training Epoch: 32 [41088/50048]	Loss: 0.9740
Training Epoch: 32 [41216/50048]	Loss: 0.7661
Training Epoch: 32 [41344/50048]	Loss: 1.0335
Training Epoch: 32 [41472/50048]	Loss: 0.9752
Training Epoch: 32 [41600/50048]	Loss: 0.8835
Training Epoch: 32 [41728/50048]	Loss: 0.9221
Training Epoch: 32 [41856/50048]	Loss: 0.8331
Training Epoch: 32 [41984/50048]	Loss: 1.1478
Training Epoch: 32 [42112/50048]	Loss: 1.0474
Training Epoch: 32 [42240/50048]	Loss: 1.0371
Training Epoch: 32 [42368/50048]	Loss: 0.9941
Training Epoch: 32 [42496/50048]	Loss: 0.8071
Training Epoch: 32 [42624/50048]	Loss: 1.0873
Training Epoch: 32 [42752/50048]	Loss: 0.9860
Training Epoch: 32 [42880/50048]	Loss: 0.8471
Training Epoch: 32 [43008/50048]	Loss: 0.9408
Training Epoch: 32 [43136/50048]	Loss: 1.0141
Training Epoch: 32 [43264/50048]	Loss: 0.9318
Training Epoch: 32 [43392/50048]	Loss: 0.8284
Training Epoch: 32 [43520/50048]	Loss: 1.0383
Training Epoch: 32 [43648/50048]	Loss: 1.0946
Training Epoch: 32 [43776/50048]	Loss: 0.9224
Training Epoch: 32 [43904/50048]	Loss: 0.8926
Training Epoch: 32 [44032/50048]	Loss: 0.9162
Training Epoch: 32 [44160/50048]	Loss: 1.1791
Training Epoch: 32 [44288/50048]	Loss: 0.8899
Training Epoch: 32 [44416/50048]	Loss: 1.0132
Training Epoch: 32 [44544/50048]	Loss: 0.9326
Training Epoch: 32 [44672/50048]	Loss: 0.9827
Training Epoch: 32 [44800/50048]	Loss: 0.9625
Training Epoch: 32 [44928/50048]	Loss: 1.0107
Training Epoch: 32 [45056/50048]	Loss: 1.1859
Training Epoch: 32 [45184/50048]	Loss: 0.9386
Training Epoch: 32 [45312/50048]	Loss: 1.0642
Training Epoch: 32 [45440/50048]	Loss: 1.2423
Training Epoch: 32 [45568/50048]	Loss: 1.1494
Training Epoch: 32 [45696/50048]	Loss: 1.1776
Training Epoch: 32 [45824/50048]	Loss: 1.1579
Training Epoch: 32 [45952/50048]	Loss: 0.9137
Training Epoch: 32 [46080/50048]	Loss: 1.1133
Training Epoch: 32 [46208/50048]	Loss: 1.0954
Training Epoch: 32 [46336/50048]	Loss: 0.9662
Training Epoch: 32 [46464/50048]	Loss: 1.0334
Training Epoch: 32 [46592/50048]	Loss: 0.8982
Training Epoch: 32 [46720/50048]	Loss: 1.0618
Training Epoch: 32 [46848/50048]	Loss: 0.9931
Training Epoch: 32 [46976/50048]	Loss: 0.9557
Training Epoch: 32 [47104/50048]	Loss: 1.0064
Training Epoch: 32 [47232/50048]	Loss: 1.0609
Training Epoch: 32 [47360/50048]	Loss: 0.9103
Training Epoch: 32 [47488/50048]	Loss: 1.0125
Training Epoch: 32 [47616/50048]	Loss: 0.8190
Training Epoch: 32 [47744/50048]	Loss: 0.9680
Training Epoch: 32 [47872/50048]	Loss: 0.8580
Training Epoch: 32 [48000/50048]	Loss: 0.9580
Training Epoch: 32 [48128/50048]	Loss: 0.9922
Training Epoch: 32 [48256/50048]	Loss: 1.0556
Training Epoch: 32 [48384/50048]	Loss: 1.0600
Training Epoch: 32 [48512/50048]	Loss: 0.8667
Training Epoch: 32 [48640/50048]	Loss: 0.8417
Training Epoch: 32 [48768/50048]	Loss: 1.0751
Training Epoch: 32 [48896/50048]	Loss: 1.0826
Training Epoch: 32 [49024/50048]	Loss: 1.2320
Training Epoch: 32 [49152/50048]	Loss: 0.8924
Training Epoch: 32 [49280/50048]	Loss: 1.0251
Training Epoch: 32 [49408/50048]	Loss: 1.0991
Training Epoch: 32 [49536/50048]	Loss: 0.7615
Training Epoch: 32 [49664/50048]	Loss: 1.0547
Training Epoch: 32 [49792/50048]	Loss: 1.0113
Training Epoch: 32 [49920/50048]	Loss: 1.1758
Training Epoch: 32 [50048/50048]	Loss: 1.0072
Validation Epoch: 32, Average loss: 0.0123, Accuracy: 0.6018
[Training Loop] Target accuracy 0.6 reached!
[Training Loop] Training done
Stopped Zeus monitor 0.
